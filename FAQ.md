## FAQ

	1. whats the need for mapping information problems to structure (math) problems? for example, isnt an information asymmetry already structural?
		- yes and no - the problemm is already captured on a layer of abstraction above the agent layer (what you could call 3-d space or physical reality), 
		which is what I sometimes mean when I say the structural/math layer though I should really say the physical information layer, 
		where most problems should be transformed to unless you have existing solutions or a complete interface map so you can query for a solution on other interfaces.
			- but the information asymmetry is an abstract problem has many solutions, and applying each solution would look different between different problem spaces.
			- one way to solve it is by distributing all information to all agents - another way is by splitting hte information and sharing it equally - another way is removing the information.
			- these solutions would look different depending on the problem space - distributing all information or removing it may not be possible depending on what resources you have.
			- but once you have the problem matched to these solution structures, you can apply the solution structures to 3-d space, looking for objects that could fulfill the definition of solution terms.
			- what does 'distribute' mean for a particular problem space? these are the questions that can be answered on the 3-d space layer.
				- if you have info tech, you can distribute information that way, at risk of the info being hacked
				- if you have social networks, you can distribute it that way, at risk of distorting it
			- different solutions comes with different intents & problems like risks, and these objects are also automatically identifiable once the solution is applied
			- distributing information may give conflicting agents power over each other - but only one of them may use it - thats an information problem as well, which can also be framed as an info asymmetry.
			- whats the solution that causes the fewest risks & subsequent info asymmetries? that depends on the problem space.

	2. does every object need to be mapped to a common shape (like a square or circle) with your system?
		- that strategy can be used to compare attribute sets that match these common structures, to find structural solutions that can then be applied to the original problem 
		- in the absence of other problem-solving methods, finding structures with problem-solution matches already indexed can be an efficient method of solving the original problem.
		- that doesnt mean there arent cases where finding a new structure (like a core function combination circle layer system or a function system) isnt useful for depicting information in ways that will reveal problem cause & other important info, even revealing solutions if the information is organized in the right structure

	3. is this too abstract to be useful? how would you implement this?
		- the fact that we can imagine what a concept is means it can have structure, & interfaces act like standardizing filters:
		  while they are abstract terms, they have intrinsic physical attributes & map to structures even when they are abstract enough to have few physical attributes
		- the docs for some implementation strategies are here:
			https://github.com/outdreamer/build-a-cure/blob/master/docs/core_analysis/derivation_methods.md
			https://github.com/outdreamer/build-a-cure/blob/master/docs/workflow/problem_solving_matching.md
		- most of my implementation strategies vary on:
			- the starting point of the analysis (which interface the query starts from)
			- the structures relevant (which structures or type of graphing method to use)
			- the intent (create a prediction function, reduce solution space, compare solutions, or match problem with solution)
			- the core abstract function represented (is it a find method, an apply method, a combination)
			- the models used (object model, interface query)
		- but they have in common:
			- using core objects & patterns
			- using info objects like problems/incentives/sub-systems/efficiencies & definitions & concepts like probability/relevance to create defined structures like prediction functions
			- applying structure to unstructured information

	4. can this really be used to automate math insights? that requires complex thought that cant be automated.
		- whoever told you that is full of
			Lattice multiplication method automation
				- https://github.com/outdreamer/build-a-cure/tree/master/docs/specific_problem_analysis/multiplication.md
			Integration method automation
				- https://github.com/outdreamer/build-a-cure/tree/master/docs/objects/problem_space.svg
			Eigenvector/eigenvalue relationship derivation automation
				- https://twitter.com/alienbot123/status/1154930391012167680
				- https://github.com/outdreamer/build-a-cure/tree/master/docs/specific_problem_analysis/automate_math_proof_example.md
			Set generation automation
				- using a similar method as this example of attribute/function combination, generate all possible sets:
				  https://twitter.com/alienbot123/status/1245950414278627328
				  <img src="https://github.com/outdreamer/build-a-cure/tree/master/docs/specific_problem_analysis/predict_pathogen_species.png" />
			- more evidence of damnation - come get your poison:
				- Problem solving automation workflow identifying structure implementing the concept of randomness, which can be used to generate functions with conceptual properties like high ratio of 'calculatability of answer' to 'verification of answer' (which can also be used to identify structure as an interface that can capture non-structured information like concepts)
					https://github.com/outdreamer/build-a-cure/tree/master/docs/workflow/problem_solving_matching.md
				- Linking relevant concepts to randomness such as average and balance/equality using definition routes as a method of identifying a probability distribution with randomness built-in (distribution with equal probability distributed across outcomes, or alternatively a distribution where each outcome has the same averaged probability value)
				- Generating the symmetry concept as a combination of objects/attributes/functions like 'reversible changes without losing information', using core component combination analysis
					https://github.com/outdreamer/build-a-cure/tree/master/find_existing_solutions/system_analysis/core_analysis.py
				- Identifying bases & other structures as an origin of a prediction function, rather than data sets alone, as alternate routes to a prediction function
					https://github.com/outdreamer/build-a-cure/tree/master/docs/tasks/problem_workflow_example.md
				- generate a function with certain attributes using net intent of structural component operations
					- to generate a function with ambiguity in input/output relationships (as in multiple inputs produce the same output), introduce an exponent in the dependent variable (like how x^2 + y^2 removes the concept of 'uniqueness' from the input/output relationship, given how exponents use repetition of the same base (x as a base, multiplied by itself), and using how combining different types of repetition can remove 'uniqueness' from the input/output relationship, and using how different pairs of inputs can generate the same outputs with a squaring operation (making squaring the unit operation to fulfills intents like "generate the 'ambiguity' attribute" or "remove the 'uniqueness' attribute")
				- kernel trick: 
					- the intent is to 'differentiate shapes on a graph with a straight line' (shapes indicating clusters belonging to different data categories)
					- in its standard definition routes, 'differentiating' can take the form of:
						- 'maximizing difference'
						- 'isolating difference'
						- 'producing difference'
					- 'maximizing difference' can take the form of 'adding a difference' rather than 'maximizing an existing difference'
					- if there is a difference, but it's not defined by a straight line, the difference boundary can be used to indicate a group of data that should have a different added attribute value (like height) than the other points
					- 'adding a difference' between shapes can translate to the structures:
						- 'adding a dimension'
						- 'changing the difference definition'
						- 'adding a difference of an existing type (scalar in current dimension)'
					- now that you have a specific structure ('add a difference in the form of a dimension') to achieve this general intent ('differentiate shapes'), apply that structure to the problem:
				      - structural intent: find a function that maximizes differences between shapes on a graph
				        - find the differentiating boundary on the current graph if there is one
					        1. identify a function that would create different values on either side of the boundary (minimizing values on one side, maximizing values on the other side)
					          - example: 
					          	- functions like x^2 have low outputs for low inputs and proportionately higher outputs for slightly higher inputs, so if you align the boundary with the position where the input/output proportion changes, you'll align low inputs with low outputs and slightly higher inputs with high outputs
					          	- 'aligning low inputs' means arranging the axes so the low axis values near zero overlap with the shape positions that should have low outputs
					          		- so 'alignment' here consists of centering/shifting the axes so that:
					          			- low/high values occur in the right positions
					          			- the difference where low outputs change into high outputs aligns with the differentiating boundary
					       	2. alternatively, find the direction of change (from one shape to another) that could be mapped to a direction of growth in a function
					        	- 'direction of change' = 'outward from center of shape', so growth in value (from zero up) should align with the outward direction (align origin with center of shape)
					        		- <img src="https://en.wikipedia.org/wiki/Kernel_method#/media/File:Kernel_trick_idea.svg"/>
					        3. alternatively, identify that the shape-differentiating boundary is the important object, and that this boundary should also be the separator in low/high outputs from whatever function is chosen
					        - 1, 2, & 3 are just different starting points/formats of the same trajectory ('aligning inputs/outputs across differentiating boundary', 'differentiating outputs for different group inputs', 'align direction of group change with direction of increasing change')
					      - then apply this differentiating function to add a dimension of change
					      - then test if the new low outputs & relatively higher inputs are different enough to clearly separate them with a line (the unit separator)
					      - if not, try another function to maximize differences in outputs between shapes, with other structures that definitions (like 'adding a difference' or 'maximizing difference') can map to
					    - if there isnt a differentiating boundary, find a differentiating attribute between data groups, such as numbers that are square roots/primes/integers
					    	- then apply the same procedure as above, to find a function that differentiates numbers with that attribute from numbers without it
					- so from the origin intent 'differentiate shapes', we:
						- pulled definitions relevant to that intent
						- iterated through definitions
						- applied definitions to a system to get their structure in that system (answering the question, "what form would 'maximize difference' take in the graph system"), with answers like ('add a change type' and 'maximize change')
						- applying the structures retrieved by that definition application to the system (apply 'add a change type' by pulling types of change, iterating & applying them) in a way that aligns with origin intent
							- apply 'add a change type' (specifically a dimension of change) in a way that aligns with intent 'differentiating shapes'
								- this application involves first pulling core or important change types in this system:
									- change type 'input difference', in group membership
									- change type 'output difference', across inputs of different types
									- change type 'output difference thresshold', where outputs begin to change from one change type (linear, like 1^2 = 1) to another (quadratic, like 2^2 = 4)
									- change type 'attribute difference', in attributes of a data point
									- change type 'value difference', in various values of an attribute across different data points
								- then mapping these as inputs generating the group differences, which have their own input/output relationship already defined (input data::output group label)
								- formatting/arranging the change types in a structure that generates the group difference implies a function linking inputs & outputs, across the difference trajectory:
									- origin group A: origin group B difference
									- origin position attribute similarity (low values of A are similar to low values of B)
									- target position attribute difference (low values of A are different from low values of B)
										- meaning converted low values of A are lower/higher than converted low values of B
										- to get a difference in an attribute (like position), you can apply a conversion to maximize differences within that attribute, or add an attribute that offers another type of position difference, so that the attribute as defined in another space/system (3-d as opposed to 2-d) is differentiated
								- the origin position attribute similarity can be converted into the target position attribute difference with a function:
									- if there is a similarity between the threshold structure within a function output, and the threshold structure differentiating groups, that could make the input-output relationship generating function align with the overall 'differentiating shapes' intent
										- inject a similarity in that position, taking advantage of the existing similarity in structures (output threshold & group boundary both being examples of the 'differentiating limit' structure), by aligning group membership and threshold side
									- now you can search for a function that would align inputs/outputs across this threshold, starting your search with functions having an attribute of volatility (similar inputs produce very different outputs)
										- with the restrictions that:
											- it should have one major change in output change type, like x^2 has one major change from semi-linear to very nonlinear change
											- this major change should occur at relatively low values, for standardization & the fact that there isnt much room in the center shape for growth types
										- other functions maximizing difference would include a wave where adjacent inputs produce positive/negative values, but that implies other groups or alternating groups beyond the two categories
						- you have various starting points to automate finding the solution:
							- find the structure missing the solution first (derive solution structure, then fill it in with a solution)
								- find the structure of the input/output relationship that needs to occur
								- then fill it in with a function producing that input/output relationship
							- combine solution components first & apply limits/tests/filters to check if it matches solution metrics (build & refine solution)
								- find functions likely to produce difference across inputs
								- then check if they produce the right difference, and refine it (by centering/scaling) until it matches the difference you need
							- this solution is an example implementation of the structure-intent interface combination, with a specific implementation of the 'change' interface within that interface combination
						- this method can be generalized to a method of finding functions for an intent like 'reduce computation' or 'differentiate with a line'

	5. what is a problem space
		- its the space where youd graph all the info relevant to a problem - the context allowing a problem to exist
		- I often use tech as a key determinant of a problem space bc which tech you have often determines which available resources like strategies you can use, but it includes all the other resources you might have access to (info, potential, energy, physical assets, etc).

	6. why improve problem-solving at all?
		- the problems with current solution methods:
			- solutions that are slow to implement, static, not shared, not organized, not generalized, & include repeated work
			- solutions often dont use prior knowledge (insights/patterns) to inform new solutions
			- known/discoverable systems with known/discoverable rules are treated as unknowns
			- errors are found with common known or easily derived rules ("change/remove assumptions") or causes ("misaligned attributes")
			- problems of the same type persist across systems
			- problems can be standardized to info/structural problems, which have associated solutions, and can be used as building blocks of solutions
			- work devoted to repeating a solution could be work devoted to innovating problem-solving
			- problem-solving isnt automated
		- current methods are focused heavily on information - if people become too focused on information (what is true at a given time), they'll never change again & time will end,
			they'll just calculate everything from the point that they find a way to do so
			- instead of focusing on information as the priority, they need to focus on preserving variance potential, so there are still questions to answer
			- outrunning the onset of the information calculation singularity involves:
				- creating self-sustaining variance sources & protecting existing ones (maintaining ambiguity/alternative options)
				- automating what can be & also automating the update of automation tools
				- evaluating information on the basis of change/potential
				- analyzing reason/cause rather than information
				- this means avoiding optimizing everything
				- there should always be at least two comparable alternatives so a decision is difficult & not certain
					- at least two systems, at least two perspectives, at least two metrics, at least two intents, etc - the ark requires differentiation to sustain potential

	7. what do you mean by 'using potential as a base rather than time'
		- time as a base for assessing change is useful in solving information problems
		- time occurs when there are no symmetries allowing for reversibility - in order for something to be reversible, symmetries have to align to allow for efficient organization of energy flow so a system can form to be a platform for the change
		- potential is the ability to change, time is the realization of change
		- im using potential as a proxy for the time variable, just like using the derivative rather than the function
		- potential is an important input of time - if there is potential for change, time can occur 
		- focusing on time over-focuses on information, which is the result of a measurement, and measurements have unintended side effects like over-dependence on the measurement
		- potential also captures a lot of potential information:
			- whether something is about to happen (whether a function is about to change)
			- whether something is possible or unverifiably possible
			- whether something deviates from or complies with known patterns (whether it's likely to be new or not)
			- how similar something is to output of known generators (adjacence to functions as a analytical metric, rather than the prediction function itself)
			- possibilities & ambiguities (where information is lost like in a black hole or uncalculatable like where there are too many alternatives) 
		- evaluating change with respect to potential measures whether you're increasing the number of possibilities (enabling information to occur as time passes) or decreasing them
		- if you make a decision that closes too many doors, potential, change, & time will be permanently lost, if the door goes with it (if it's irreversible)
		- other types of time are useful to evaluate change 
			- whether youve changed in conceptual time, causal time, potential time, or information time, & whether the change is absolute/specific - did you change everyone's time or just yours?
			- these metrics differ in how other types of time pass
		- rather than asking 'is this resource needed at a given location' - we can ask questions like 'did we enable people in that location to solve a resource deficit?'

	8. what is the actual workflow to use this?
		- a general program logic flow might include the following:
			1. check pre-existing output of the program (pattern indexes, concept definitions, etc) to see if it can be used as an input filter for a new problem (the system filters below are some of the outputs of this program) to break the problem into solved problems
			2. if it isn't composable with solved problems, but the problem type is still identifiable, then select a solution strategy & starting point
			3. then select threshold metrics to switch between strategies 
			4. then execute the solution strategies, checking at various threshold points for problem-solution match
			5. if no match found for one strategy, switch to other strategies
			6. if no matches found across all strategies, switch to uncertainty description patterns & methods
			7. output either insights found, problem-solution match, or uncertainties that need to be resolved (gather more data, answer this question, etc)
			8. store any info objects found that arent already in indexes (insights, patterns, problem-solution matches, interfaces, functions)

	9. is all value created? are there any problems left to chase? what problems cant be solved with this?
		- once certainty-generating/finding/deriving devices are created, uncertainty-generating/finding/deriving devices will be necessary, and we can stop evaluating questions based on 'what is real' and start evaluating questions based on 'what should/could be real' by creating reality
		- this system can solve problems where information is calculatable (like on the math interface) or where information is retrievable/testable (where you have data you can find & retrieve).
			- it can identify new interfaces on which problem-solving automation is possible, as they develop or become measurable
			- it can integrate new problems & new problem types into the system
		- it cant solve problems where information isnt measurable or calculatable - that means:
			- problems that are not solvable in this host system (a universe with these laws of physics):
				- problems that require more computation than we have computing power for
				- problems regarding information that is not retrievable or derivable (destroyed information, like historical information or information inside black holes)
				- problems regarding structures we dont have the understanding to organize information queries for, or retrieve information for (if there is a physics or math insight that is so foreign to our understanding that we don't even know to look for it, that may not be solvable with this tool, but it should be able to point understanding & information retrieval in that direction if not reach the destination structure). This would happen if the analysis isn't comprehensive enough when generating different perspectives, to identify new interfaces & new structures on them not adjacently derivable with existing interfaces.
					- maybe there's an object that generates so much randomness that we cant ever capture enough information about it to derive it
					- maybe there's a mechanism preventing necessary computation time to derive the mechanism from gathering around certain structures capable of hosting sentient life
					- maybe information has a built-in expiration in physics, and if it's not used, it decays - maybe this is how math develops, around efficiency energy organization that is allocated according to incentives & aligned with meaningful intent based on usage
			- these are examples I can come up with, which means my system can also come up with them - but you can see how non-standard assumptions can generate a high level of difference, to come up with alternative explanations originating from very different but still possible systems.