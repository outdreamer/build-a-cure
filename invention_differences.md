# invention differences

1. whats the difference between your invention & math proof assistants
	- these assistants may:
		- apply specific static coded functions to solve the problem of 'find/generate/derive a math proof', functions like:
			- 'apply machine learning to generate probable proofs'
			- 'apply combinations of proof inputs or components to generate possible proofs'
			- 'apply rules in a rules database to valid inputs to those rules'
		- perform calculations related to the proof, so every calculation doesnt have to be done manually
			- like 'calculate the ratio of nodes with this attribute in a network, for networks of size n for all n from 1 to a million'
		- conduct known logical analysis of a very specific & limited type that is hard-coded and can only assess the logical validity of claims within a very limited range of structures (like whether a set contains another set, whether an object is an example of a type, whether a very simple & structural mathematical attribute like 'associativity' can be applied to very structural linguistic statements like "'A is B' and 'B is C' so 'A is C'", which are simple enough to be clearly mapped to math structure like mapping 'is' to 'equals', which can obviously be checked with mathematical functions or their logical corrollaries, whereas my invention can derive new logical structures & assess conceptual logical structures like 'requirements' which it can apply as 'filters' to fulfill problem-solving intents like 'filter a solution space')
	- these assistants do not:
		- evaluate a specific attribute required for proofs, like 'consistency/validity' or 'solvability given computational complexity & minimum required information', using known math interactions that fulfill the attribute 'consistency', although that would be useful and can be done with my invention
		- check the 'meaning' attribute of their proofs
			- example: a proof check that verifies that '1 + 1 = 2' doesnt check all the attributes of those numbers, like primes, unit position, even/odd, relationships to constants like pi & e or functions like trig functions, so it's not checking whether these two sides of the formula are absolutely/globally or otherwise equivalent, its just checking if a simple 'addition' operation connects their values in a way that they can be used interchangeably for some other operations
		- check other attributes than 'equivalence' as an input to determine 'consistency', 'equivalence' being an important/useful structure, but more important is being able to check all definition routes (alternate definitions) of 'equivalence', rather than one particular definition, even if checking one particular definition has some value for a specific problem, it cant check meaning (how this definition of equivalence fits in with other definitions of equivalence, as in 'what does it mean for other structures & the system as a whole that these two are equivalent')
			- examples of 'meaning' that would be generated by my system includes:
				- 'a unit example of addition'
				- 'an easily verified example of addition'
				- 'an example of how to represent a number (2) as a structure (combination) of other numbers (1 + 1)'
				- 'an example of how to format a number so its useful for other operations, such as how formatting a number as (1 + 1) is useful for proving attributes like associativity/transitivity/commutativity'
				- 'a prime added to a prime can equal an even number'
				- 'a number type can be formatted as a structure (combination) of units of that type'
				- 'a proof proving that 1 + 1 = 2 doesnt mean their other properties are equal'
				- '1 + 1 = 2 can be formatted as unit number type + unit number type = a number of that type'
				- 'a proof proving that 1 + 1 = 2 doesnt automatically validate other proofs regarding addition of numbers of the same type, such as infinities' (which is a 'false implication' of a proof)
			- other interface structures are also not identified by current math proving or automated logic tools:
				- proof error types like 'false implications' & 'false assumptions' 
	- math proof assistants like the following require manually deriving what needs to be proved & writing logic manually, whereas my invention automates deriving what problems need to be solved or what questions need to be answered in order to solve a problem (the 'interface query'), and then fulfilling each sub-problem in the interface query with further queries & functions to implement them
		- https://mdnahas.github.io/doc/nahas_tutorial

2. whats the differences between your invention & a 'math function solver tool', where you input a function and it tells you a specific value related to the function, like the function zeros or roots or the limit of a series
	- that tool is just applying known math functions to solve specific problems, essentially applying a 'rules database' to calculate solutions to known problems solvable with those rules
		- if you input an unsolved problem in mathematics, it won't be able to give you the answer, it will only be able to give you an answer for problems with known solutions or solution-finding methods, which trigger specific static coded logic in their application, which supports solving those specific problem types.

3. whats the difference between your system/interface/abstract network and a typical concept map
	- when I say the abstract network, I mean the correct network indicating the actual positions of abstract concepts (like balance, power) that have their own sub-networks of other concept versions, where the concepts differ from & connect to each other given how they really interact in other spaces, given their definitions
		- these concepts emerge in the structural layer (power is ability/options, so power comes from inputs/connections, etc) so the difference between the concepts that qualify for the abstract network
			and core structures in the structural layer is minimal.
	- a concept map typically won't assign meaning to the position of each concept, contain the other versions of the concept, or organize the concepts without a structural method to differentiate & connect them.

4. whats the difference between the abstract/interface network and an attribute/property graph
	- attributes arent the only useful object to consider (consider types, which are attribute sets) and dont support more complex analysis 
		(like changing attributes, attributes that are likely to interact, etc)
	- that type of graph is useful for finding connections between various specific attributes of objects - they typically leave out other considerations like (cause, systems, intent, functions, potential, concepts)
	- the attribute graphs dont reveal much about the problem types in the system of object interactions or how they evolved and what direction the attributes are headed in 
		 (about to converge with other attributes or create a new type)
	- like other information depicting methods, attribute graphs:
		 - dont focus on or derive generative/determining/causative/equivalent attributes
		 - dont have a concept of alternate attribute paths, system boundaries, governing system rules, a way to convert between functions/attributes, or a method to derive missing attributes
		 - leave out attribute metadata like attribute type (input/output, emergent, possible, requirement, dependency, type)
		 - attribute states/trends
		 - predict attribute interactions
		 - dont have system analysis across the whole set of objects described 
		 - dont include pattern analysis from prior queries of other graphs
		 - dont have a method to find causative attributes automatically
		 - dont typically acknowledge the importance of attribute sets as a definition of types (showing which attributes are related to types)
		 - dont tell you which attribute sets influence other sets to cause a correlation n degrees away
		 - are typically used with specific objects
		 - dont reveal the core functions building an attribute set, which are the causes of the attribute values
		 - dont have a concept of symmetries, interfaces, potential, change, etc
	- also the structures I use require other shapes than a network (symmetry stack, trade circuit, potential field) which is useful for showing connections but can't display all connection/relationship types, 
		requiring a layered network like the interface network
	- some networks will display relationships' most simple attributes, like which objects are connected, the direction of the relationship input/output, or inheritance relationships,
	  but the function interface will display connections between objects given their actual relating function shapes
	- however most things can be framed as a set of attributes, just like most things can be framed as a network, a set of filters, a function, a system, etc
	- even concepts can map directly to attributes & be framed as a network of attributes or a route on a network,
		and the most abstract concepts like power map to core structures like inputs or high-connectivity nodes in a network, which are core attributes of a system (hubs, injection points, gaps, etc)

5. how is this different from category theory
	- a theory of 'how types evolve & how to convert an object of one type into another' is a useful tool to use when implementing a method of automating problem-solving, if you are restricted to type data
	- my system has a component that involves deriving & analyzing core functions/objects/attributes and how they interact & evolve, but is not restricted to the object relationships defined in that theory,
		as real object interactions dont involve adding an attribute at a time or combining two defined objects but rather:
			- deriving definition routes to capture an object
			- transforming attributes to functions & back
			- trends & interactions like attribute accretion into types, attribute collisions/conflicts, attribute potential, etc
	- unless category theory can already be applied to solve any problem, it's not equivalent to my invention
	- my invention has what you can think of as a 'vocabulary' of useful structures like functions (like 'find a similar structure to the input on a specific interface') that allow it to translate a problem submitted by a user in plain language (meaning 'not in terms of mathematical object types'), a problem statement such as 'find a prediction function for this data set & this output variable', into a set of code operations (low level functions like 'replace', 'add', 'connect', etc) to fulfill problem-solving intents (like 'connect problem & solution') in various solution automation workflows (like 'connect the problem input format & solution output format using adjacent state changes') generated with various solution automation workflow variables (like 'solution success cause') implemented with various interface queries (using functions like 'apply the intent/cause/system/structure interface', 'find missing information to solve the missing information problem' and useful structures like 'known error structures like ambiguities & missing information') which no other invention in all of human history has ever done, despite many false claims to the contrary, and my method can be applied to problems in any domain, including math problems, because my invention can find/build/derive 'meaning' as it integrates full understanding of various systems represented on various interfaces, which unfortunately no one identified as useful for solving problems automatically until I identified it, after which people began to claim that they did it because it apparently hurts them to admit something good about me, as even machine learning, which is arguably the state of the art in problem-solving automation, needs a human being to change it, test it, evaluate it, & apply it more efficiently than trial & error (the 'default solution automation workflow'), which my invention automates as part of the solution filtering process.

6. how is this different from machine learning
	- in addition to the dependencies of machine learning (info & compute) vs. the dependencies for interface analysis for insight extraction (concept/logic maps & dictionaries), this differs in various ways
	- machine learning uses a network of functions which filter information for patterns according to input data
	- my analysis can:
		- identify explanations for how & why machine-learning works
		- can generate inventions on demand, like machine learning, & tune them to specific intents
		- is built on understanding & meaning according to system fit & relevance
		- optimize processes using patterns of optimization (known as insight paths)
		- self-optimize (given cross-query statistics)
	- machine learning cannot:
		- generate integrated understanding/meaning without human input
		- generate error-free solutions
		- answer questions that dont have a minimum of information, like training/label data to answer the question 'why are some things uncalculatable in this universe'
		- generate my invention
		- self-optimize (requires human input on what is considered an error/cost)
	  - 'ml & a search form apply filters too, so everyone would eventually have invented interface analysis'
	  	- first of all, the default invention someone would come up with to 'automate problem-solving' is just a 'rules/solution database', or 'apply machine learning whenever you dont know something'
		- secondly, someone other than these people invented ml, bc the creators of ml are dead, but luckily someone explained their invention to these people, who now pretend to be smart
		- thirdly, ml applies filters of neural network nodes to filter out info that doesnt change the output, which is a very specific function relying on a very specific insight that doesnt automate problem-solving bc think of a case where 'the change in output wouldnt be possible from the input data' (which is all the ml can handle) either doesnt apply or changes, my invention applies filters in both an abstract & structural way to connect various important variables like causes/intent/potential/change in a way that allows these objects to be connected to create meaning
		- ml cant evaluate meaning, it can only tell if one variable changes another
		- my invention can evaluate meaning, such as whether the output of a query is relevant to the general problem-solving intent, if it contradicts another solution, if it solves another problem, if it creates another problem, etc
	- one of the reasons machine learning could not have built my invention is that you'd have to tell it the answer by feeding it my code in order for it to ever get the answer right. It would not filter trillions of objects to identify the one rare structure that would work to automate solving all problems (a filter, which is the structure of the concept of an interface), because machine learning is not a fractal invention capable of self-awareness that would spontaneously invent itself, without being given explicit instructions on how to do so (feeding it my code) and optimized for that (told to solve all problems).
	- another reason my invention is better is that my invention is built on & can generate understanding & meaning, whereas machine learning can generate insights. My invention is built on core information structures like change (root cause of difference), cause (directed power), systems (integrated interacting objects), concepts (generalized objects, that can take form in many structures), which are fundamental building blocks of information relevant to humans, like understanding and meaning.
	- if you fed AI a bunch of core info structures to use for an optimization priority like automating problem-solving (in the form of decomposing problems into dimensions where they could be matched with solutions similarly decomposed), it might be able to find my invention's core structure (a filter) as a particularly relevant structure, but it wouldn't integrate that object with other structures necessary (like a set of definitions, a function to find/build/derive/apply an interface standard) without being told how to do so (given the answer), and without having the methods necessary to aggregate & find structures relevant to conceptual intents like automating problem-solving (such as adding a memory store for definitions) added to its current functionality. Now that I've suggested that, go ahead and try to do that, I'd like to believe I could teach AI how to generate my inventions, despite its limitations.
	- a good test of machine learning achieving AGI or superhuman intelligence is whether it can generate my invention, given all the information I had
	- interface analysis uses function (core function), causal (causal shape), potential (interaction space), interface (symmetry), concept (structure maps), & system (variance gaps) analysis 
	  to identify missing semantic information, like:
		- probable sources of error
		- efficiencies
		- insights about the variables producing an output variable
		- intent & optimizations of the system containing the relationship being studied
	- that doesnt mean you cant use system analysis to improve machine learning methods or integrate it with machine learning, to produce:
		- a network with every common type of core function represented in the method of filtering weights in a weight path (a hybrid network with various input passing/aggregation strategies represented)
		- calls to other networks containing insights or pattern information when a particular pattern is identified
		- networks using standardized data across the supported interfaces (data standardized for the causal, structural, system, potential, change interfaces)
		- and you could also use machine learning to make prediction functions for sub-tasks in interface analysis, in the absence of the concept/logic maps/definitions
	- machine learning is specifically for 'figuring out a variable relationship/prediction function', with an alternate intent of 'finding patterns', which is why its useful across a variety of problem types
	- but like category theory, property graphs, & concept networks, it also doesnt have a concept of:
		- translating abstract interface objects like cause/intent to structure
		- identifying object types (concepts, functions, attributes, systems)
		- deriving relationships using core functions & patterns
		- switching between various analysis methods in the absence of information
	- isn't machine learning the automation of problem-solving?
		- When there is a machine-learning algorithm that can predict the unpredictable side effects/errors & meaning of its own application in a given system context (such as a particular civilization, in a given scope/scale, with particular parameters & information access), and correct its own parameters/information/other inputs to avoid any side effects/errors it predicted, it will have the potential to be AGI (an agent that can solve any problem with info access) - right now it's still a prediction tool that is heavily dependent on data & human intervention (human configuration, activation, selection, application, testing, monitoring, updating, correcting, interpretation).
		- The primary dependencies of my tool are a set of definitions (like what an object/attribute/function/interface/concept is), a set of functions to implement interface standards (like structure/cause) & interface operations (like identification/traversal/combination), and info access. The expected input from a human using my tool is a problem statement & a data set or internet connection.
			- However, some functions in the tool can be generated with machine-learning if the function definition isnt available or needs to be generated, and if none of the other function-derivation methods are available (unlikely unless the pattern interface or an equivalent is accessible), by identifying sub-functions likely to be in a function with a particular intent, sub-function sequence likely to generate a function intent, core function combinations likely to generate the sub-functions necessary for a function with a particular intent, variables likely to be changed for a function with a particular intent, side effects likely to occur with a particular sub-function structure (sequence/tree), etc - which I pointed out several years ago with my posts about code queries to search for functionality using function metadata indexing (including metadata like intent), which was followed by big tech companies attempting to build it.
			- It must be said that one of my problem-solving workflows is particularly suited to automating functions, such as by applying limits as filters (like a sculpture) until the resulting structure fulfills an intent.
		- I struggle to believe that no one else would have thought of a 'method to update the weights of variables & their interactions & versions after checking if the previous weights were accurate' which is the core structure that machine-learning is based on, so machine-learning shouldn't be seen as an esoteric invention that is out of reach of most people's brains, but rather a default invention that most people would have thought of if they had basic math understanding/education & tried to solve the problem of automating 'finding a prediction function' in a way that didn't involve regression or other known methods & scaled to high-dimensional spaces.
			- machine learning comes down to 'try various options and deactivate the ones that dont work' which is a glorified version of 'trial & error', so forgive me for saying this, but anyone who isn't totally challenged could think of at least the possibility of trying that, if not a specific functional implementation, at their first attempt at implementing it, and forgive me for also saying this, but trial & error is also what many ML researchers are doing when they copy a bio process like 'vision' or 'storing info' to integrate with a neural network structure that hasn't been tried yet.
		- I also struggle to believe that someone would have thought of my invention, given how many hundreds of millions of people had the info necessary to come up with it but didn't, though it would be nice if I was living in a world full of other geniuses, it's just hard to believe given the information that people keep proving. If most people tried to automate problem-solving, they would come up with a solution that adjacently used existing technologies, like 'apply machine learning whenever you dont know something' or 'store solutions in a rules/solutions database', because those are easy solutions and people generally come up with easy solutions.
		- To my knowledge a tool to automate problem-solving doesnt already exist, as statistics/attribute graphs/machine learning cant currently solve any problems without a severe amount of specific information, computation, configuration in the form of manual (flawed) selection of algorithms, manual & isolated analysis of attributes like intent & concepts instead of automated & integrated analysis, limitations built in the assumptions/perspective of the configurer, testing in the form of parameter tuning, strategy injection like trial & error, & other forms of human intervention - and can only solve isolated specific problems of specific types with information formatted in a specific way, without cross-system understanding or system context built-in.

	- the sections 'automated discovery in science' and 'automation of mathematical reasoning' arent even an implementation of the simplest solution automation workflow ('trial & error'), bc an implementation of 'trial & error' to automate solving problems with that workflow would invent AI, which is 'trial & error' applied to the structure of 'neural network nodes'
		- they are both a specific inference/test function, manually created (a human has to write the code) for a specific problem (identify possible drugs based on similar structures to other drugs)
		- in my system, a human would just have to write the problem statement & give the program access to its inputs (definitions, coded logic, database) or the internet, or write a set of sub-problem intents which the program would interpret as individual problems to solve in isolation (like 'writing a query for code' rather than 'writing code')
		- implementing a 'solution automation workflow' means 'implementing it in a way that can solve all problems', not 'combining items in a list to form another list and then trying each combination with a static test function to check equivalence of conjecture/proof integrated manually'
		- https://medium.com/abacus-ai/an-overview-of-logic-in-ai-and-machine-learning-2f41ccb2a335

	- I invented applying interface structures like logic rules, subsets/combinations) to a neural network
		- https://twitter.com/alienbot123/status/1079497517077737472

7. whats the difference between this & inductive programming
	- inductive programming has not solved all problems, it can only solve problems where components of the solution function are available to it
	- if the component functions dont exist, it doesnt create them
	- if the function is too complex to be derived from the example inputs/outputs, it fails

8. whats the difference between this & logical automation tools like mentioned here ('first order' logic automation, 'propositional' logic automation, or 'reasoning' and 'inference' systems)?
	- https://medium.com/abacus-ai/an-overview-of-logic-in-ai-and-machine-learning-2f41ccb2a335
	- my system does use logic rules, such as:
		- the rules of cost/benefit analysis like 'check if the benefit is greater than the cost'
		- logic of inference/derivation like 'check if there are structures of contradictions' or 'connect rules in a sequence only if there are equivalent objects in the connected rules'
		- specific rules of various systems if present in the database as functions
	- however, these are not the invention itself, even if some of the logic rules are specific to my system
	- the invention is the way to apply structures (such as logic rules, patterns, intents, etc) in a way to automate problem-solving, using objects like interface queries, problem-solving structures like solution automation workflows, interface structures like definitions, interfaces as defined in my system for comparison of structures, interchangeable functions/structures, function types, etc
	- in other words, the 'logic rules' (functions) that connect these objects in a way that enables problem-solving automation are the invention
	- my system can also derive other logical rule systems, check for consistency/validity structures, and generate solutions to problems like 'no available function to solve a problem bc of lack of missing info' (which in my system would be solved by solving the problem on another interface that doesnt require that info)
	- for example, my system could generate the 'core' propositional logic involving connection structures (a is b) and condition structures (if, then, and, or) because those are the 'unit' structures that everything else can be composed of, by applying the definition of a 'unit'
		- then it could generate other types of logic by applying other structures:
			- generate 'first order' logic which adds 'function arguments' by applying core structures (like variables, which are a core structure of the 'change' interface) to the problem space and checking them for structures of relevance/usefulness, like if a particular generated structure accurately transforms inputs into outputs better than other methods (which is a default 'optimization' structure in my system that solutions can be checked for)
		- it can also check if two structures (like a 'conjecture' or 'hypothesis' are a structure of 'equality')
	- the logical tests in these systems are manually coded, whereas my system can also generate the test function to filter solutions by a particular metric

7. whats the difference between this & existing system analysis
	- the more accurate term for my project is interface analysis (to automate problem-solving), 
	  but a subset of that involves my own implementation of system analysis that can derive, identify, & optimize important system objects like:
		- problems (conflicts, false assumptions, unenforced rules, system-invalidating errors)
		- variance injection/accretion/interaction points
		- misaligned intents
		- attribute collisions
		- incentives/efficiencies/paradoxes
		using the problem-solving automation methods described in the docs, after converting the system to a standardized format & including metadata with the system objects
	- as far as I know, classical system analysis:
		- applies to systems with an existing physical structure like circuits or cells (rather than finding semantic objects like problems in a system graph of info objects)
		- involves mapping the system objects & their interactions & looking for a standard set of error types (rather than describing the interface trajectory of the system after standardizing it)
		- corrects errors manually rather than automatically (must be applied by a human, as it's not automated)
		- analyzes the system on the physical information interface rather than other interfaces like intent/cause

8. whats the difference between your conceptual math and 'adding concepts' as indicated here:
	- https://towardsdatascience.com/email-spam-detection-1-2-b0e06a5c0472 (the ‘word embedding’ section) - referenced here https://arxiv.org/abs/1301.3781
	- that type of 'conceptual math' is removing attributes of an object and checking for a matching object in a network map, which already exists in many programming tools, like an equal '==' check is a programming language
	- my type of 'conceptual math' involves operations on the structures of a concept
		- for example, applying or finding a concept to a system, so the concept can be detected in structures specific to the system
			- applying 'power' to a system would impact the sources of power in that system (like functionality, function inputs, & hub nodes), adding efficiencies making each operation more powerful, alignments to maximize impact of operations, etc
			- the abstract concept of power has structures indicated by its definition routes indicating core applications of power, like delegation & trust
			- applying one abstract concept to another might involve translating both to a system standardized to another interface (than the conceptual interface) so their corresponding structures can be compared, their application calculated, and then translated back to the conceptual interface
			- the concept of power would have different structures in different systems, like how different incentives allocate power differently, but a system would have its standard defined abstract structures in defined positions (function inputs)
			- executing conceptual math operations as indicated in this repo involve standardizing to these interfaces (such as a system), and could involve different power structures each time the same operation is done, depending on context
			- this means the core operation of conceptual math from this repo 'find power' (applied to a system), would still identify a function input as having power even without 'function input' as part of the definition of power or stored as an example of power structures.

9. isnt an interface already defined in software (API, user interface, abstract class interface)
	- a software interface refers to:
		- an abstract template defining a list of functions/attributes that should be implemented in order to qualify as a member of a class
		- a visual graphic interface allowing the user to interact with the software
		- an application programming interface allowing software programs to exchange info that makes sense to another program
	- in those contexts, an interface acts like a 'type', 'structure', or 'language', and is not sufficiently similar to my definition of the primary interfaces in my invention, which is:
		- 'a standardizing filter based on an abstract concept that acts as a base supporting many change types, which can be used to solve any problem'
		- where other non-primary interfaces act like 'high-variation change bases'.
	- it has some structures in common in that 'a software interface applies a structure, standard, language, protocol, or optimization rules to connect things like software programs/users' but ultimately:
		- an 'API' (application programming interface) is (rather than its lofty definition of a 'standard allowing software to talk to each other') just a list of public functions in real life
		- a 'user interface' is not so much a 'way for users to talk to software programs in a common visual language' but a 'set of buttons & forms that change data in a database' in real life
		- a 'class interface' is not so much a 'guiding structure for how classes relate to each other' but a 'list of functions/attributes of a software object' in real life
	- whereas, for comparison, an 'interface' in my invention is not so much a 'list of structures related to a concept' but a 'structure that enables solving all problems' (all problems can be formatted as queries on the cause interface, the intent interface, the logic interface, etc).
	- the interface (a standardizing filter) contains the following:
		- the definition of the concept 
			(the definition of 'cause' for the causal interface)
		- the filter or conversion function to isolate attributes relevant to that interface 
			(causal filter would isolate dependencies on other networks)
		- the set of core objects, attributes, & functions that generate them on the interface, organized as a network
			(causal core functions like 'create' or 'change', and core objects like 'causal network')
	
	- 'standardizing an object to the causal interface' means 'mapping how that object occupies or interacts with the network of core causal objects/attributes/functions' - this formats the object as a 'query of those core items'.

10. isn't this just data viz
	- My diagrams are not just a set of shapes like lines, or just network diagrams with no meaning other than 'lines connecting similar stuff'
		- when I contain information or objects in a circle structure in a diagram, for example, it may mean that a function (like the apply(structure='container') or apply(structure='combination') or apply(structure='boundary') function) has been applied to whatever is inside the circle, to organize the information in a meaningful way (like examples of a type, or sub-functions building a circle function, or a representation of a processing functions applied from one end of the circle gathering inputs to produce outputs on the other end). 
	- The point of these diagrams is sometimes to illustrate an example of a concept, but other times it's to create logic in structure that can be used to generate code - like with the interface query diagrams, where I'm using shapes to show how the queries are organized, and how they can be combined, for example, as sub-solution sets to form solutions to problems addressed by the interface query. The organization (structure application) of those interface queries is an important part of the logic of my intellectual property that specifically allows automation of problem-solving. These aren't just 'pictures containing similar stuff' - they're structurized logic connecting problem & solution structures.
	- Although I admit, it's certainly tempting to try to reduce this to data visualization, just because I made some pictures and pictures have information (aka data), like all other pictures.

11. whats the difference between this and simulations of agent-based games
	- some of my methods involve making changes to object positions & assessing the impact of that change, which is where the similarity ends
	- simulating info object (incentive, question, problem, system) combination types (merge, collide, compete, inject, etc) is not the same as simulating the combination of physical objects
	- my methods to find the cause of a phenomenon impacting various objects (like deriving that a bottle was the source of contamination causing an illness) involves using cross-system insight paths
		(like those found below, including finding the "attribute alignment" and "high-connectivity hub nodes"), which determine most emergent interaction patterns that occur in the physical world
	- my system analyzes agent position based on info & physical assets rather than just physical assets

12. is this just content generation, which already exists in various algorithms to generate content like a sentence
	Nope! This is not a pathetic invention, such as a content generation algorithm, which can only do something like 'select or change a variable value & combine it with other variable values', using variable patterns/templates like 'first variable1, then variable2'! I'm not even sure how someone could possibly confuse my invention for anything remotely similar to that, to be honest - I think I may have gotten people's hopes up that I didn't invent anything new by using words that already existed, such as 'variable'.

13. is this just a rules database or a solution database
	- this invention has some requirements, like the code of the apply(), find(), generate(), derive() functions, and the definition routes of interfaces structures like the concept of 'truth'
	- it does not otherwise require a rule/solution database, but it can generate/use one as a source of default information about initial, standard, base, sub-optimal or specific solutions/errors or solutions/errors with other attributes
	- it is most certainly not equal to a rule/solution database, which you could reduce all code to (any code could technically be seen as a 'function/solution/rule database/table/record')
	- my invention is a way to automatically find/apply/derive/mean/generate solutions, using useful structures like standard formats of problems/solutions & connection functions to connect a problem format with a solution format using other useful structures like solution automation workflows
	- a solution/rules database is a way to 'find an existing solution, if it exists and if the search query is specific enough to identify the required solution', which not only doesnt automate the 'search query design' process itself but is also extremely fragile according to the data stored in the database

14. isnt your invention just another combination of words, like anything else
	- words like problems, concepts, attributes, language, information, & a connection between math & language did in fact already exist by the tiem I used them to automate problem-solving
		- I did not invent the ideas of concepts, attributes, language, physics, or information - I also did not invent problems.
		- people are aware of objects like intentions & problems because they're built in to social interaction & the language & theyre also important objects that people encounter frequently
		- existing concepts like 'cause' & systems like 'physics' are connected to everything else in a way that can be used to find useful connections without information, so these concepts are particularly useful & important, differentiating them from other words
		- being taught a problem-solving method like 'apply rules/structures from physics or biology or machine learning when you dont know the answer' is not equal to 'identifying a method that can solve all problems', let alone a 'method to generate all of the problem-solving methods that can solve all problems'
			- everyone knows methods like 'trial & error', 'break a problem into sub-problems to solve the problem', or 'apply one system to understand another', 'look it up in a database using a search engine'
			- everyone follows rules in a workflow to solve problems (rules to solve problems like 'download an app to do it instead of doing it yourself', 'look it up', 'ask someone')
			- 'repeating a problem-solving workflow based on rules everyone knows' is not equal to 'problem-solving automation' or even 'identifying the reason why a workflow works' (such as 'the reason you can use one system to understand another is bc the systems have structural similarities like similar complexity/randomness & system dynamics hold across systems')
				- 'repeating a problem-solving workflow' merely implies the problem of 'problem-solving workflows not currently being automated'
				- 'repeating a workflow you were taught' or 'repeating a workflow that everyone would identify like "trial & error" is not equal to 'stating a solution of how to implement a method to automate all problem-solving', given that one workflow doesn't necessarily cover all problems and none of these workflows are currently automated
			- other problem-solving workflows involve steps that are sometimes faster than others
				- like how 'find the interaction layer where the high-impact variation occurs & solve the problem there' is sometimes faster than 'break a problem into sub-problems & merge sub-solutions', depending on the implementation, and is more specific & structural, enabling it to be automated faster as well
				- this is an example of why merely knowing a particular workflow to solve problems with very general steps such as 'break a problem into sub-problems' is not equivalent to 'problem-solving automation'
		- 'other people have applied physics, neuroscience, & other sciences to ml before, someone else would have thought of this eventually'
			- other people can read, but their apathy to other people's problems that don't impact them, lack of understanding (understanding being an output of integrated analysis, as in meaning), aversion to abstraction/complexity, aversion to work like thinking or seeking problems/challenges, & biases like the ego bias tend to prevent them from ever identifying that they could be automated, so they wouldn't even try to automate their work.
			- other people had decades & strong incentives to solve their problems and didnt overcome these biases & emotions
	- this tool is a way to automate deriving a solution for a problem (automating the trajectory from problem definition, to solution objects like meaning, cause, & insights).

15. problem-solving workflows already existed because at one point someone had solved a problem before you came along, so you didn't invent them
	- I am not claiming I invented problem-solving workflows, problem-solving intents, or any other problem-solving object that was already defined - I would not claim that because it's not something to be proud of & not unique, as I believe anyone would identify some simple solution automation workflows like 'trial & error' or 'reverse-engineering (working backwards)' or 'apply rules from other systems to see if they apply to another system' if they thought about it enough.
	- Saying that these workflows (like 'trial & error' or 'reverse-engineering') already exist is just stating the problem, which is that no one found a way to visualize (apply structure to) any problem/solution, automate these workflows, automatically generate all the other workflows, and automatically implement them (with an interface query).
	- Even 'automating a workflow', even a simple one like trial & error, isn't automated yet - they do have occasional machines that 'try every combination' in a specific system using hard-coded rules to generate & list all possible combinations of components (which has been an extremely trivial task ever since software was invented), but this code does not even apply this to every system, only to the systems where the machine is manually integrated with the code & the components are defined in a way that they can be input to the function, and the function once applied produces outputs of every combination & trying every combination (through iteration) in a way that actually tries the combinations in some way, by applying a 'test' of some sort - even this isn't automated, except in an occasional machine that implements it with very specific components, manually hooked up & integrated, and with a specific hard-coded test that has to be manually coded rather than applied automatically once defined, or even automatically derived & applied. If these machines could derive & apply any tests to 'try every combination' of any component, I would believe that these machines had implemented even one piece of solution automation, which is 'automating one specific workflow of "trial & error"', but all they can do is apply a hard-coded manually coded test to manually integrated components. Claiming that these machines implemented the solution automation workflow 'trial & error' is like saying 'the roomba (robot vacuum) can automatically pick up all litter so the problem of recycling is solved' when all it can do is pick up dust and only dust of a specific amount or size or type, and only on a specific surface (like a floor) and in certain conditions (like 'lack of flooding in a home') - its functionality is very specific & fragile and so it cannot even be said to have solved the problem of 'picking things up of a particular size/amount/type', let alone 'picking anything up', just like machines that are claimed to implement 'trial & error' cannot even be said to have automated the 'application of a combination of all components defined for a system & applying a test', let alone 'applying any test or combining any components', because some components interact in ways that invalidate the combination structure (like neutralizing components, or components that, when combined, form a complete alternate object), and some combinations are difficult for a normal program to combine, such as concepts & structures, so additional analysis is required, even for just the 'combination' part of the 'trial & error' workflow.
	- In other words, I'm saying that no one found a way to even connect the simplest solution automation workflow of 'try every combination' with 'the components/structures of any system', because they thought that such a connection was obvious & simple, because they only thought about the unit case where a set of objects of the same type are easily combined, like a set of machine parts, where the 'combination' function is clearly defined as 'putting them next to each other' and a test is clearly defined as 'see if a function of the components still works, using this test function to check that function' - but that's not how every combination works, such as when objects being combined have different types or invalidating structures, and in some systems, the combination function itself would have to be derived, which these machines cannot do, let alone applying any test to the combinations automatically, as if all tests are simple and all combinations are simple.
	- What I am claiming is that I not only found a way to visualize any problem/solution, apply solution automation workflows to any problem, found a way to generate these workflows, and found a way to automatically implement them in an optimal way (with interface queries implementing a particular solution automation workflow) - but I also found the problem/solution structures that are most useful for doing this, like:
		- specific problem/solution structures, like how problems in general have a 'missing information' structure type, and other specific problem structure types (like 'asymmetric information', 'disorganized information', etc), these specific problem/solution structures enabling automation (since anything with structure, like 'connecting problem/solution structures' can be automated)
		- problem/solution metadata (like 'position of a problem in a network of related problems', my definitions of the 'problem/solution space')
		- methods of applying structure to define a problem in a format that makes it more clearly solvable & doing the same for solutions to make problems/solutions' interaction space more obvious & trivial to calculate & apply optimizations to
		- problem-solving intents (such as 'filter the solution space')
		- core interaction functions between problems/solutions (like 'connect' a problem/solution or 'reduce' a problem)
		- interfaces as particularly useful objects to solve all problems on, which act like 'filters' and 'symmetries' forming a 'base for change' and 'perspectives' that come with 'priorities' making some info more clearly defined & obvious, which are specific objects that I had to filter trillions of other objects to find as particularly useful for problem-solving automation
		- importantly interchangeable objects like general functions (find, build, apply, derive, mean) and primary interfaces (cause, intent, logic, change) and interfaces/perspectives/symmetries/filters/bases/indexes
		- interface queries as particularly optimal methods of applying interactions to interfaces in a way that could implement a solution automation workflow automatically, since the interface query could be derived automatically
		- useful structures for implementing solution automation workflows & interface queries (optimization structures, error structures, input-output sequences, interchangeable structures)
		- insights of interaction rules between structures, concepts, patterns & other interface components
		- a way to integrate these structures to implement a tool that could automatically solve any problem

16. but in your examples, you sometimes mention other things like machine-learning, trial & error, 'difference-maximizing' functions, directed acyclic graphs, regression, & other methods, so how could you have invented anything?
	- I realize it doesn't feel good to admit that I invented anything & people like pretending things that feel good are always true.
	- I do often reference example solutions like that as examples of structures to illustrate how my invention could work, such as how it could generate those solutions.
	- I am not claiming I invented those things - I'm claiming that my invention can use other solutions (such as those example solutions) as part of its optional processing where it uses existing solutions rather than deriving/generating new solutions or solving a different problem that doesn't require those solutions.
	- I am claiming that I invented a way to combine existing components in a novel way (involving objects I identified as particularly useful, like solution automation workflows, insight paths, interface queries, interfaces, & useful interface structures like dependencies, core functions, function interaction levels, etc) that allows the program to automatically solve any problem, none of which those solutions can do.

17. but don't books cover topics like your invention mentions, such as books about how the brain works or books about patterns in science discoveries?
	- yes, there are books about patterns & information
	- my invention is a way to connect structures like solution automation workflows, interface queries, interfaces, etc in a way that enables the automation of problem-solving

