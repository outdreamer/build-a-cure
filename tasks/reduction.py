from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.decomposition import PCA, LatentDirichletAllocation, TruncatedSVD
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.manifold import TSNE

def reduce_features(x_features, y_labels, components_count, reduction_name):
	''' 
	x_features is in a dataframe, output by pandas.read_csv()
	components_count is the number of features to reduce to
	apply svd, lda, pca, t-sne & other feature reduction methods once data is filtered to numeric variables 
	'''
	'''
	transform() & fit_transform():
		- apply dimensionality reduction to x, returns x_new, array-like shape(samples, components)
		- dirichlet.transform() returns doc_topic_distribution shape(samples, components)
		- lda.transform() projects data to maximize class separation
		- tsne.transform() doesnt exist
	'''
	reduction_method = None
	if reduction_name == 'pca':
		'''
		PCA (n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)

		alt: KernelPCA

		method:
			- find top features explaining variance with eigenvalues of eigenvectors
		'''
		reduction_method = PCA() if components_count is None else PCA(n_components=components_count) # svd_solver='full', 'arpack')
	elif reduction_name == 'svd':
		'''
		"TruncatedSVD/lsa (n_components=2, *, algorithm='randomized', n_iter=5, random_state=None, tol=0.0):
			- Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with sparse matrices efficiently
			- truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA)
			- supports two algorithms: a fast randomized SVD solver, and a “naive” algorithm that uses ARPACK as an eigensolver on X * X.T or X.T * X, whichever is more efficient
			- SVD suffers from a problem called “sign indeterminacy”, which means the sign of the components_ and the output from transform depend on the algorithm and random state.
			  To work around this, fit instances of this class to data once, then keep the instance around to do transformations."

		method:
			- select explanatory left features of output matrix, using:
				- singular-to-diagonal transform executed by multiplying:
				 	- orthogonal matrix U x * x, diagonal matrix D x * y, and orthogonal matrix V y * y that approximate original matrix A

		'''
		reduction_method = TruncatedSVD() if components_count is None else TruncatedSVD(n_components=components_count, algorithm='randomized') 
		# n_components = 100 for lsa, n_iter & random_state for randomized solver, tol for arpack
	elif reduction_name == 'dirichlet':
		''' 
		LatentDirichletAllocation (n_components=10, *, doc_topic_prior=None, topic_word_prior=None, learning_method='batch', learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, 
		evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=None, verbose=0, random_state=None)
		
		input: array-like or sparse matrix, [samples, features]

		output: doc_topic_distribution for X, shape= (samples, components)

		method: learns a model using variational bayes, then transforms to fit that model
		'''
		#X, _ = make_multilabel_classification(random_state=0)
		reduction_method = LatentDirichletAllocation() if components_count is None else LatentDirichletAllocation(n_components=components_count) # random_state=0)

	elif reduction_name == 'lda':
		'''
		LinearDiscriminantAnalysis (*, solver='svd', shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001) # solver = ‘svd’ (doesnt compute cov unless store_covariance is True), ‘lsqr’, ‘eigen’

		"A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.
		The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.
		The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the transform method."

		attributes:

		    coef_ - ndarray of shape (n_features,) or (n_classes, n_features) Weight vector(s).
		    intercept_ - ndarray of shape (n_classes,) Intercept term.
		    covariance_ - array-like of shape (n_features, n_features) Weighted within-class covariance matrix. 
		    	It corresponds to sum_k prior_k * C_k where C_k is the covariance matrix of the samples in class k. 
		    	The C_k are estimated using the (potentially shrunk) biased estimator of covariance. If solver is ‘svd’, only exists when store_covariance is True.
		    explained_variance_ratio_ - ndarray of shape (n_components,)
		    means_ - array-like of shape (n_classes, n_features) Class-wise means.
		    priors_ - array-like of shape (n_classes,) Class priors (sum to 1).
		    scalings_ - array-like of shape (rank, n_classes - 1) Scaling of the features in the space spanned by the class centroids. Only available for ‘svd’ and ‘eigen’ solvers.
		    xbar_ - array-like of shape (n_features,) Overall mean. Only present if solver is ‘svd’.
		    classes_ - array-like of shape (n_classes,) Unique class labels.

		method:
			- separate classes with line by mapping features into lower dimensional space (assuming means are far from each other)
				- calculate class means distance or inter-class variance 
					- calculate intra-class variance (distance between mean & sample)
						- construct lower-dimensional space maximizing inter-class variance
		'''

		reduction_method = LinearDiscriminantAnalysis()
		# reduction_method.predict([[-0.8, -1]])

	elif reduction_name == 'tsne':

		''' tsne (n_components=2, *, perplexity=30.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=1000, n_iter_without_progress=300, min_grad_norm=1e-07, 
			metric='euclidean', init='random', verbose=0, random_state=None, method='barnes_hut', angle=0.5, n_jobs=None)

			"converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.
			It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high"

			attributes:
				embedding_ - array-like, shape (n_samples, n_components) - Stores the embedding vectors 
	    		kl_divergence_ - float - Kullback-Leibler divergence after optimization.
				n_iter_ - int - number of iterations run

			output: Embedding of the training data in low-dimensional space

		'''
		reduction_method = TSNE(n_components=components_count) # reduction_method.shape
	else:
		print('unknown method', reduction_name)

	if reduction_method:
		result = {'reduction_method': reduction_method}
		result['features'] = reduction_method.fit_transform(x_features) # input is array-like shape(samples, features), returns X_new, ndarray array of shape(samples, components)
		result['y_labels'] = y_labels
		result['model'] = reduction_method
		result['components'] = reduction_method.components_ # principal axes, sorted by explained_variance_
		result['singular_values'] = reduction_method.singular_values_ # singular values of components (2-norms of components in lower dimensional space)
		result['explained_variance'] = reduction_method.explained_variance_ # amount of variance explained by each feature
		result['explained_variance_ratio'] = reduction_method.explained_variance_ratio_
		print('feature reduction', reduction_name, '\n\tOriginal feature #', x_features.shape[1], '\n\tReduced feature #', result['features'].shape[1])
		return result
	return False