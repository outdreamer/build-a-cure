# Concept Operations


## Applications
    - discovering functions between nodes, 
    - matching problem/solution types, 
    - identifying & leveraging conceptual behavior
        - use truth to apply pressure
        - express faith to motivate


## Conceptual Computer (Interface network)

  - look for mapping between the interface network and quantum particles
    - if this exists, its possible the interface network is stored in each superposition/quantum particle network, so that you can derive it at any point if you have the tools to measure it
    - however, by its nature, it may be designed to be unmeasurable - so youd have to use derivation methods to estimate its structure
    - this implies the structure of the interface network may also be changeable within the system implementing it
    - are the superpositions chaotic because theyre the interface where chaos leaks in from other universes?
    - if universes attract other universes by order/chaos, we would need to anticipate the impending adjacency to universes that need the order given by interface network understanding the most
    - interface network manipulation should be the next target once its current structure is understood
    - you would need to be prepared to handle unknowable system problem-solving (problem-solving in the absence of measurement tools/minimum of information to solve)
    - also maximizing the variance in this universe would attract refugees from other spaces chased out by the gravity of certainty, so they can take shelter in our variance - being the variance-generator would also allow us access to other universes as sources of error information
    

## Concept Derivation

  - identifying unique objects in a system that cant be defined in terms of standard operations on other objects

    example:
    - power cant be defined as a simple combination of other objects in the networks it participates in, because its an abstract property having many possible implementations, all having one thing in common, which is the role/behavior of "enabler/enabling", and power is therefore occupying more than one semantic layer, as it can be an object, role, function, input/output, depending on which structure is more relevant to the host system for this implementation of power

    - however, defining it in terms of these possible structures it occupies (input, role, function) is too simplistic - for example you cant define power as simply "an input" because while that is true, given its enabling functionality, it leaves out a lot of information and fails to distinguish it from other inputs, which may not be powerful since they are common or easily substituted with alternatives

    - even defining power by its core unifying function "enabling" is too simplistic because often power does more than just enable something, given the connectedness of systems - meaning that enable one thing often disables another thing in the system or an adjacent system, which creates effects other than enabling, as the disabled process may disable the original enabled process down the causal path

  - therefore we can conclude that abstractions are concepts that:

    - can take many structures (the concept of equivalence has many possible implementations)

    - can impact many systems varying by system attributes or system types (abstract, calculatable, variable, understood, types, functional, prioritized, optimized)

    - cannot be perfectly defined as a simple function of other objects, but rather are definable with a set of simple, core boundary rules that differentiate them from other concepts
      - these boundary rules do not involve other concepts on the same layer, but rather core components
      - for example, the core components of common shapes are: line, point, curve, corner
      - the set of common shapes are the uniquely identifiable combinations of these components (circle, square, triangle) that are not identifiable as simple transforms of other common shapes, but rather are composed of simple limit rules based on their core components (line, point, curve, corner)

      - "enablement" doesnt perfectly capture "power", and the concept of "enablement" also relies on the concept of "power", but it does differentiate power from other concepts & unite its possible implementations & meanings
      - "symmetry" doesnt perfectly capture "balance"
      - "similarity" or "substitutability" or "identity" doesnt perfectly capture "equivalence"

    - are uniquely identifiable compared to other concepts
      - balance is related to symmetry so these are not unique concepts but embedded/dependent/overlapping/hierarchical concepts
      - however balance is clearly differentiable from power, as balance inherently involves equivalence and power doesnt, whereas power inherently involves enablement and balance doesnt


## Specific Concepts


  ### Solvability

    - examine shape of universe nexus by which problems are solvable & which are not
    - unsolvable problems are dependent on a conclusion that is not constructable with known information or functions
    - for the most part, unsolvable problems related to objects that are not understood or not based in our universe, 
    - so we only see their impact on other objects as they cascade into our perceivable universe or theorize their construction by processes we can perceive
    - common types of problems usually considered unsolvable:
      - computationally expensive but not impossible (information-dense, like how many particles are in the universe or how many prime numbers there are)
      - imperceptible relationship/object considered impossible to verify (what is the structure of other universes' nexuses)
      - conceptual relationships not mapped to math operations


  ### Time

  - one reason time particles & potential particles are related is because time represents potential - if there's no potential for change (variance), time cannot pass
    https://twitter.com/remixerator/status/1082066543788216320

    - this is one reason its important to determine other possible universes before our variance runs out - if we waste all our time/potential confirming/checking the information & entanglement connections in every particle, we wont be ready for the next universe once we're ejected from this one because we used up all the variance in this one

    - the universe chains/trajectories should be predictable if theres a correlation in variables - if the next universe has more variables creating more complexity or less potential or less information, that would increase the level of difficulty in preserving variance

    - otherwise if we preserve all the variance in this one, we'll attract the excess energy from other universes that collapsed into certainty, where the excess variance has to go somewhere and it will probably be routed to any universe where variance is handled in a relatively stable way

    - so its important to use any remaining variance to preserve variance in this universe, until we determine a universe that is more optimal to go to & a way to get there or bring it here

    - any leftover variance after that calculation can be used to reduce variance in knowable systems (solve problems)

    - this means not collapsing all superpositions into a certain state, which is a danger of quantum physics in addition to accidental or intentional entangled pair chain triggers

    - the direction of cause from observer to superposition state can probably be reversed, which is a key to solving some time travel problems in addition to alternative time shapes (conceptual time, time loops)

  - time can be defined as:
    - how many systems are capable of interacting on the same interface layer & the trajectory across the manifold of possible interaction combinations of those systems
      - whether other layers of interaction are system-adjacent (possible) or whether interface physics prohibits interactions on those layers for this system interaction trajectory (timeline)

    - if a variable changes with time (possible variance), that doesnt mean time (possible variance) caused its variance, 
      but the system collisions enabling the set of interactions possible in that layer

    - the flow of system molecules follows a physics that should be able to explain system change rules
    - the key question is: what is the system (set of core functions) that could generate the set of possible systems we assume exist, 
      and what host system could enable these core functions to result in combinations explaining those systems?
    - what is the host system in which system molecules exist? its modelable as a network with different states (structural math layer) in a particular interface (core dimension set)
    - what is the path between the math layer & the others that allows the math layer to contain its own internal rules and also capture all other rules?
      - the math layer is so important bc its the rules of value & structure, & it's relevant when we can observe/measure other systems to the point of being able to assign value & structure
      - systems that cannot be measured can be guessed but not proven, unless the set of proofs leaves only one possibility for that system's position
      - if there is only one possibility left to the question that explains the variance in this universe, then time doesnt exist in this universe
      - the point wouldnt be to predict future behavior, but to derive source rules of the universe
        & then optimize them for a universe where time could exist (every question cannot be answered in that universe)
        & then detail the steps necessary to trace that universe to this one
      - if there is a universe where every question can be answered, it might poison adjacent universes with its certainty, so they might pool their variance to introduce chaos to that one
        if there are remaining problems to solve in those other universes (systems with non-determinable winners using info available inside the universe, in that state)
      - information has to leak to preserve variance in this system
    - when you standardize the other layers to the position interface, it can be captured in the math layer
    - what is the causal relationship between these layers? does math cascade into the others or just capture their structure, once it decays into information?
    - math is the unit system in the system layer, just like information is the unit object in the conceptual layer (unit/key object: out of which all other objects are built)

  - where does time get expanded in a distorted way when expanded to the fourth dimension after being compressed into the 3rd, like 2d representations of a sphere get expanded in a distorted way?

  - example of a different conceptual system:
    - if power favored centralization, another core concept like balance would have to favor a chaotic process or not exist at all, or another core concept would need to be added to the network


  ### Meaning (Relevance/Structure)

  - meaning comes from attributes like:
    - reduction of signals (relevance)
    - matching of signal structures (similarity)


  ### Entropy/variance

    - the reason entropy can evolve on its own is that if order was the default, it wouldnt allow order (in the form of alignments) to be constructed for a particular efficient purpose (like making a trade or cooperating) and then automatically deconstructed by entropy when no longer used. Efficiencies may be rare & temporary enough that order shouldnt be the default so disorder can allow more interactions to occur and more efficiencies can be found naturally. It's similar to the free market allowing companies to organize & operate without regulation to find efficiencies in demand/supply.

    
  ### Randomness

      High-randomness system properties:

      - consolidation of variance to an interface
      - interchangeable inputs
      - structural similarity across interacting systems (few variables in which they differ) on that interface
      - variables across different interfaces (system-level variable, intent-level variable, function-level variable)
      - change physics applies to every layer of the system, its components, and interacting/host systems
      - compounding stability in structure, compounding variance in content
      - centrality of random variable as a hub in a causal or interface network

      In a random (unpredictable) system, it's possible to derive patterns in structure much of the time (whether a function exists), but less feasible to predict patterns in content (how a function is implemented).

      Random system concepts:
      - symmetry
      - equivalence
      - uncertainty
      - variance
      - binding, change, & boundary physics

  #### Randomness Generator

    1. calculate maximal variance points in a system (variables most unrelated to all other variables), and equalize their contributions

      example:
        - "cloud size" is directly related to adjacent water, wind patterns, temperature & elements in adjacent air
        - "cloud size" is indirectly related to moon phase (influences wind patterns), sun exposure (influences temperature), pollution (influences elements in air)
        - "cloud size" is very indirectly related to astrology 
          (influences moods, emotions, subconscious, dreams, & market decisions, which influences market trends, which influence side effects of production like pollution)
        - cloud size is so indirectly related to astrology that it may be considered independent of astrology, despite the fact that every object is inherently related to all other objects
        - we can say that "cloud size" has a "maximal causative distance" or "minimal dependence" on "astrology"
        - other ways to find a variable with minimal dependence on some other variable include:
          - varying abstraction level:
            - the concept of 'balance' is indirectly related to everything but only specifically related to a small subset of things (justice, symmetry, etc), most of which are either conceptually abstract, or mathematically abstract (specific to mathematics, like a low-level operation or attribute that can be calculated numerically)

          - querying its dependent variables 
            (cloud size is caused by element distribution, so element distribution is independent of cloud size)
            - in reality this is not real independence, because many dependence relationships are circular, either 
              - directly (one circular loop between two nodes), or
              - indirectly (the output dependent node, cloud size, goes through many systems before returning some input requirement of the input independent node, element distribution)
              - this is because there are very few to zero ways to generate an output that has no side effects on input requirements (input inputs)
                - an example is "victimless crimes" like ejecting junk into space, which may not impact us immediately but definitely will return some causation (in the form of required inputs to some process) to our species eventually
        - another example is "corners of a square":
          - each side of the square is equal, so it's equally likely that the "square" system will generate a movement of balls within the square, that pushes a ball to one of its corners
          - the corners represent a maximal variance variable (corner), which are unique in that if a ball is in one corner, it necessarily cannot also be in some other corner
          - this is the foundation of identifying not just maximal variance-generating independent variables in a system, but also system nodes (gathering points of inputs/outputs) & interfaces (standards)

    2. design systems that optimize the number of independent max variance points
      - how do you design a system with maximal variance-generating independence points?
        - take the problem of a square - how would you generate the corners such that:
          - each corner is unique compared to other corners
          - each corner exerts the same influence on the ball movements within the square
          - each additional corner adds to the variance of the ball movements
        - eventually if you add too many corners, you get a circle - is this the maximal variance implementation of a square, or is there some point between a square and a circle with more variance-generating points than either?
          - it depends on the variables that youre trying to optimize the randomness of - if they can occupy any point on the circle, a circle may be more appropriate - if they can only occupy a corner, you need to find some combination of corners that is not a circle in order to maximize their variance

    - mentioned here:
      https://twitter.com/remixerator/status/1148816151125712896
      https://twitter.com/remixerator/status/1004578257637953537
      https://twitter.com/remixerator/status/1004578256820064257