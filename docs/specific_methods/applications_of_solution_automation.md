# Applying solution automation methods in repo

  - what can you use system analysis & interface math to build?

## Predicting Interface Evolution

  - given that we currently understand object & system definitions, the object model seems optimal to us right now, with current tech & information

  - what if the assumption that the object exists is flawed?
    - its not an entity that exists, unless its exerting influence to generate variance in some system & cant be controlled
    - it only exists according to its relationships to other entities (define an entity using other entities)

  - what if the structure assigned to the object (a separate entity) is flawed? 
    - should be a type stack, a subset of a network, a formula, etc, rather than a list of attributes & rules

  - what if the idea of a system is flawed?
    - the assignment of a host system can be so catastrophic to the accurate representation of a problem that it may be better to avoid this & use only rules that apply to all systems

  - what if the idea of a definition is flawed?
    - if definitions/facts begin to decay the more theyre assumed & depended on, if stretched beyond their capacity

  - what if the idea of interface selection is flawed?
    - what if there's an optimal interface to represent all information (interface-building formula), rather than a specific one (causal, object, type, function, priority, system, structure interface)
    - an interface is just a standardizing filter - a formula to generate the right standardizing filter for a particular task would be better than relying on interface definitions

  - this will involve the physics of scales, filters, and other structures relevant to interface networks 


## Post Problems (in System Context)

  - whats next after automating problem-solving (of problems within the context of a semi-knowable system)?

  - being able to solve a problem within a semi-knowable system enables other more complex applications of problem-solving, such as system generation & derivation of host systems from within, calculating problems that shouldnt be solved out of the set of solvable problems, etc

  - extensions of predictive capacity for decision chains, enhanced system design & optimization, & derivation of system metadata from inside the system, using analysis of solvable problems & variance sources (deriving universe system configuration & position, to achieve prioritized universe calculations & interact with other universes in a mutually beneficial way as well as preserving & exporting our useful knowledge to them)

  - designing systems that have required problem types to achieve some goal, beyond solving a problem within a system
    - the output of problem-solving is not just solutions, but also:
      - objects (insights, insight paths, patterns) used to build them
      - reduced computation demand
      - convergence of patterns
      - reduced distance between understanding & variance generators

    - post-problem-solving in isolation in the system context, the next gen of problem-solving can involve:
      - operations on the dimension object, to design dimension combinations for optimal next-gen problem-solving like system design, derivation of laws to generate known systems, & enhanced decision chain prediction
      - operations on the system object, to design systems having desired problem types to achieve other problem outputs than just known solutions reducing those problems
      - operations on the problem object, to solve many problems at once, across different systems, to generate a cross-system state
      - operations on the solution object, to create self-destructing solutions, solutions that ensure the prevention of their own disablement, solution that stabilize system objects, etc
      - operations on concept combinations: designing, fitting & applying protocols, rule types (change, governing, learning rules), solutions, problem-generators, variance-generators as needed to adapt an object set to better fit into other object sets

    - this type of analysis is to increase predictive capacity, so the emergent trends many iterations from now can be determined with existing tools

    - this will involve the math of infinities, to ensure the possibility of priority survival given the most extreme permutations of a potential harmful decision
      - a decision that limits the full understanding or optimization of the universe in a way that also harms other universes
      - if finding out some information will harm the calculation priority of this universe, that needs to be determined so we can prevent it from ever being known
      - out of all the problems which are solvable in this universe, there may be a subset that arent supposed to be solved 
        (implying the metric to calculate is whether life in this universe can protect that secret)

    - this means we'll be able to determine:
      
      I. decision chains to avoid

        - which operations on systems will produce absolute universal limits of operations between infinities & the systems, types & concepts generating them

        - such as those that will create species capable of destroying all other species (like machines or organisms that could destroy us)

        - if current choices will converge to an intersection/singularity point of no return that exerts a permanent limit

          - a limit such as a maximum on the potential growth/adaptation of the species, given a decision chain & the laws of physics, which we can avoid using this analysis

        - whether quantum physics may make the universe too computable to exist; the system seems very fragile/dependent on lack of specific entanglements

          - what if the particle of one bomb was entangled w/ a particle in another bomb, & was accidentally set off?
          - this is why (post-system context problem-solving) there's demand for methods to identify universe-invalidating decision chains
          - it's not too late to preserve enough variance to allow adaptation to the point of discovery of universal calculation priorities

      II. how to adapt to systems that don't fit existing systems or currently computationally derivable combinations of existing systems

        - such as communication with aliens that dont require information because they have their own problem-solving methods in each of their processing centers

      III. universe limits, priority, complexity & other metadata to determine its key vulnerabilities, inefficiencies, and position among possible universes

        - such as a universe where layers or points or variance are the fundamental structure, rather than dimensions or space-time
        - or universes with different configurations of interface networks
        - or universes with the same configuration of interface networks, but different structural assignments in information layers
        - or universes with more information layers than ours (potential information, verified information, prior information, information derivation methods, etc)

        - it's also important to build tools that can handle the priorities we identify as key to this universe so that our decisions from that point can fulfill those priorities & benefit other universes, so we have the required tools to do so when we reach the tech that can identify them

          - if our universe is supposed to solve a problem & it doesn't, that may have a chain reaction on other universes that we can predict, & then export our prediction mechanism so other universes can also prevent their inhabitants from making sub-optimal decisions for everyone

        - my methods of problem-solving automation in system-context problem-solving can identify the correct ratio of variance to aim for, the correct order of problems to solve (whether to solve encryption or optimal government structure first), which questions or other problem types should be maintained in which positions, etc

          - this translates into post-system context problem-solving spaces to the correct ratio & position of system types to avoid universe boundaries, which change rules to deploy across systems to avoid triggering harmful chain reactions, etc

      IV. calculating the set of systems that are capable of generating known systems

        - for instance, a system capable of generating known math & physics laws will be able to generate the same type networks, dimension relationships, function patterns, function metadata generating rules, conceptual structure assignments, etc for known systems

        - given this set of systems capable of generating math/physics rules, which system is likelier to be true, and which are possible, and can the configuration be changed?

      V. calculating existing positions of system structures in a system to generate the set of possible/likely/optimal positions

        - system structures include chains, routes, functions, interfaces, limits, & networks (such as variance generators)

        - given a set of observed rules, we'll be able to determine which configurations of variables are producing variance, and what positions those variables occupy on the network of determinating variables for that set of rules 

          - this should be able to narrow down the set of alternate explanations for variance patterns, such as whether potential particles exist on a subatomic level, or whether the pre-determination of events is changeable

        - identifying counteracting processes to determine/regulate/reduce variance in required inputs such as optimal resource distribution 
          (resources including resource metadata(position, count, neighbors, etc))
          https://phys.org/news/2020-01-diabolical-coupled-cavities-quantum-emitters.html
      
      VI. generating object-generators for a conceptual intent 

        - function-generators include systems, system networks, & functions

        - for example, intents like information security have sub intents: "high computational complexity", "minimal parameter information leakage"

        - calculating optimal systems for a function or calculating function candidates fulfilling a metric & generating new number/function/variable types when existing functions are exhausted
          - such as finding a new encryption function that leaks minimal information about its parameters but has a calculatable metadata property for verification
            - the limited supply of these functions implies that the number of good encryption functions in a space is calculatable given the metadata of that space
            - the information leakage ratio & other function metadata also allows the calculation of exploit potential of an encryption function