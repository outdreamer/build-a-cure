# Exploit Opportunity Identification


  - features as exploits & vice versa

    - everything can be used for malicious intents, but some structures are more exploitable than others, when there's a gap between logic & intent

    - in an online marketplace, allowing businesses to change price for a particular customer allows them to give preferential pricing based on relationship with that customer, but these relationships can provide an exploit, if businessperson is the powerless one in the relationship (being extorted for bribes from a gang), so that price-changing option can be used to get preferential pricing
      - if site didn't allow price changes, that would protect the business owner to a small degree but would prevent the preferential pricing option to build/maintain relationships

    - some structures are clear tradeoffs/dichotomies, and other structures seem like clear dis/advantages

    - what determines classification as a benefit (feature) or a cost (exploit opportunity) can be degrees of distortions/transforms

      - how many steps/requirements and integrations with other exploits are necessary before something is converted into an exploit

    - many tradeoffs are false dichotomies (a spectrum where opposite sides contradict each other & are mutually exclusive) because:
      - when either side is a type class, both types can often apply at the same time
      - when either side is a decision (as in a decision tree), one decision path may converge to the other decision path
      - either path may produce attributes that seem similar enough to be indistinguishable from the other
    
      - within the bias (hard-coding rules) vs. variance (lack of rule enforcement) dichotomy:
        - being adaptable to variance may be a form of bias, in that it may be unnecessary variance that should be hardcoded and is just creating problems by remaining variable
        - the opposite is also true, bias towards adaptability may be a form of variance, if the bias produces random change
        - bias doesnt inherently contradict variance as the definitions imply

  - type interface can act as a proxy for the intent interface, if types are clearly defined, unique & restrictive enough


  - non-standard exploit-finding patterns:
    - faked signals for unrelated purposes
    - output communication/processing chains mapped to intents
    - sub-intent combinations/chains as intents, assumptions as exploit opportunities (assume code functions are the primary exploit layer to focus on, rather than systems or processes using functions like 'garbage collection' or 'memory optimization')
    - indirect intents (actions not clearly benefiting any agent like 'transporting message', which form extra opportunities when combined with other actions not clearly benefiting any agent, like activities considered necessary or default)
    - matrix of compatible tech & possible alternate intent paths allowed between intent limits/filters

  - exploit opportunities involve a divergence between some expected legitimate input/output & actual malicious input/output

  - exploits -> relevant properties 
  - delegate cost -> efficiency from alignment that goes unregulated (no system rules) where exploit is gap in regulation

  - inputs providing exploit opportunities can involve input assumptions related to:
    - hardware (memory, CPU, threads, queries)
    - language (stack/heap implementation)
    - storage management (cache mechanism, garbage collection mechanism, optimization)
    - condition (limit, metric)
    - code (default tool, code, tool version, tool source, tool-management tool)
    - config (definitions)
    - permission (intended permissions vs. allowed permissions)
    - intents (user, dev, protocol)
    - actions (user (explicit decisions, implicit preferences), dev (auto/forced updates, data corruption fixes), automated (script running past its intended window of use), third party (browser, OS, anti-virus, isp))
    - functions (retrieved, generated, lack of assumption coverage of input space)
    - parameter values
    - outputs (info leaks)

  - exploit opportunity types:
    - unenforced expectations of rule implementation methods (protocols)
    - intent-expectation divergence
      - expectation: "input intents are legitimate"
        example: using legitimate input intents (login, use form, retrieve results) to build malicious output (info about data source/query engine/caching mechanism/filter used)
        (searching big/complex/varied queries to find limits of query engine & matching with known query engine limits)
      - expectation: "output intents are legitimate"
        example: using legitimate tools (database query, session, form) to build malicious output intent "retrieve info from unauthorized account"
        (searching for theorized terms used by other user to find out what other user is seeing in search results that could be used to derive other user's information exposure & habitual use)
      - expectation: "input content is legitimate"
        example: sending spam emails with target keywords designed to train theorized spam-detection AI model to target associated keywords in emails
      - expectation: "input use is legitimate"
        example: sending legitimate requests to establish pattern of use that can later be exploited 
          (login from many locations/devices simultaneously from beginning of use to avoid identification as hackers later)
      - expectation: "inputs cannot be used to get unauthorized info x"
        example: 
          - "using system stat/monitoring logs to identify readable folders by logger process"
          - "injecting rule to remove comment chars in regex filter to activate disabled code not evaluated by tests"
          - "separating submitted chars with delimiter to accumulate code chars in non-code files to make them eventually identified as code once full code char string is accumulated"


  ### Vuln potential of a solution

    1. identify conceptual/type interactions of the solution
      example: explore the interaction of random applied to random (or algorithms applied to themselves, like hash of a hash) for possible interference opportunities

  ### Hacking

    - assumption manipulation
      - threshold/metric/condition manipulation
      - input manipulation
      - verification gaps
