## Ideas


### To do

  - solution type: balance info asymmetry
  - matching


## Work distribution & incentivization

  - system users should always be system builders, so their incentives don't crystallize into an irreversibly static state
  - users should be builders by the end of each game
  - the questions of each problem space can be mapped to user tasks within a game, defined by a set of rules creating similar questions answerable in the game
  - when builders have taught enough users, builders should move on to being users of another game, in an alternating cycle
  https://twitter.com/remixerator/status/1217718371816329217


## Optimization

  - Determining which queries/calculations are optimal

    - given that certain calculations have known cost estimates, which calculations are optimal, in what order/frequency, and given what information?
    - example: when deriving a prediction function, when do you query for function & function generator patterns, when do you request more data, when do you continue assessing regression, when do you apply standardization?
    - is it optimal to solve this problem set or another problem set, or deploy resources to both?
    - solution distribution: should this solution be deployed at run time, in a specific system, should the solution be stored as its generator function, etc


## Bio System Analysis

  - nth iteration simulations: analysis that treats cyclical, recursive, cascading & iterative processes as trade loops between positions & systems
    - example: in addition to analyzing how a drug is metabolized:
      - how its structure will interact with other structures
      - how the resulting structures after the nth-interaction will interact with other resulting structures, etc
      - how the dna of a probiotic or other microorganism treatment could get re-purposed by microorganisms
      - how the activity of a gene could get accidentally impacted in other pathways & which of those pathways are possible given a treatment
      - how an interaction can be minimally modified to produce extremely different results
      - how nth-iteration interactions will be timed with other processes cycling in that time
      - how causal cascades can cycle to be interactive with the treatment
      - how functionality gaps or interactions at nth-iteration can provide opportunities for mutation & other randomness sources 

  - given the known vulnerabilities of the bio-system:
    - which problems are solvable (killing pathogen, deploying a substance to a position) with what methods (evolution, medicine, stressor distribution)
    - which problems are inevitable
    - which problems are solvable with auto-generated vs. external resources
    - which stressor patterns can prevent which problems


## Protocol Recommendations
  
  1. Auto-update crypto keys/algorithms to use constants that are always guaranteed to be below x% risk that they'll be hacked given common computational resources.
  

## Optimal Resource Trades

  - famine, farmland, mine field location data
  - cost of removing mines
  - cost of transporting people
  - which land is for sale & at what cost
  - how much land is necessary to support a person
  - set of resource trade sets across projected weather patterns for next 10 years to generate migration paths
  - which farmland is more optimally used for something else (harvesting other natural resources, water supply, city)
  - which cities have industries to support new workers moving there
  - what distribution of cities/farms is optimal for reducing resource transportation cost
  - how much does it cost to train someone to maintain a garden
  - what is the configuration of farmland & farm input supply chains that reduces supplies transportation cost
  - what is the import/export potential of each region (which crops & other resources can they produce)
  - what distribution of industries should occur by distance from ocean/water source
  - what infrastructure can optimize farmland at lowest cost
  - which laws are commonly reused across governments & are generally agreed to not be exploitative of citizens
  - what distribution of courts/police/lawyers is necessary to distribute fairness in markets
  - what compoments of this can be automated/updated automatically & which should be done with manual input
  - what tools do they need other than farmland 
    (water source, sun, cell phone, charger, electricity, portable wi-fi generator, app to track their assets, app to trade by phone, app to request supplies/loans)


### Crypto

  1. use predictive tools to predict transactions & calculate them in advance to speed up tx

    - this would assess people's known resources to build an index of global demand/supply, then calculating through these resource distributions, economic incentives for trades, social networks, platform dominance, & product availability & findability (search results rankings) - which tx were likely to happen where for which products, then calculate those tx in advance


### ML

  1. not only does ML have the potential to derive combinations like type paths & insight paths, it can derive other system/interface metadata such as core functions:
    - example:
      https://techxplore.com/news/2020-01-alphafold-protein.html

  2. you don't have to rely on disorder to detect order if you have an ordered way of generating disorder patterns

    https://techxplore.com/news/2020-01-brain-like-network-disorder.html


## Security

  1. one-time use stack (network, os, app) in extension of one-time password

    - when ISP tech is distributed (network drones/satellites), you can also switch network providers for each message, in an agreed-on stack-switching pattern determined at start of conversation, using agreed-on pattern decompression algorithm


## Bio
  
  1. Look for symmetries in nature because that's where the variance is likely to be routed
    https://phys.org/news/2020-01-lizard-snake-size-unrelated-climate.html

    The symmetry presented by stability provides a platform for variance (coronavirus) to develop.


## Computing/distribution

    - identifying servers that have resources optimized for various pieces of task
    - identify optimal server path to break down task into pieces with operation order that can be done at each communication step to operate under communication cost thresholds
    - storing metadata at definition time to make computation distribution/delegation/communication calculations pre-computed on some level
      - metadata like:
        - data computing request potential
        - data variance/patterns
        - matching data variance/patterns with sub-structures like value functions:
          - for compartmentalization & storage optimized for access:
            - if you can fill a math progression or other function having inherent position with data, you can store progression function & data map, and use that structure to find data quicker
          - for quick computation:
            - if you can fill a math progressionw with data, you can access computed values by position (if data is in the 3rd term, you know what the data will be before looking up the data)
        - data range/data type/data probability distribution & change patterns can be computed after definition time


    - error types

      - intersections/processes with obvious error opportunities:
        - data splitting
        - data merging
        - data indexing & caching
        - computation indexing & caching (false positions to answers, false answers)
        - computation timing
        - computational order error (non-commutative)
        - connectivity

      - non-obvious error opportunities or lack of optimization

        - organization method of distributed datasets allows for random or specific task optimization, rather than absolute optimization 
          (multiple indexes for same dataset or storage of metadata properties & locations in each dataset so many organization methods can be used on same dataset)

          - theres a lot of room for pre-computing, pre-indexing, & pre-combining within & across data set combinations

        - lack of relationship metadata between datasets 
          (which dataset contains more items of type c, etc)

        - identifying key vertices (factors on an analytical layer) to split the data set beforehand and creating multiple clusters split in different ways likeliest to be needed

          - identifying key splitting order & intervals at query time
          - identifying optimized stack components for computation tasks
            (servers/file systems/folder structures/databases/indexes/metadata/order/organization/languages/algorithm/data structure)

          - identifying mix of processes/computations that can be applied to dataset to optimize a task
            - one subset can be processed linearly, another can be processed in parallel, another can be pre-computed
            - another subset's handler can be evaluated at runtime if its likeliest to change
            - mixing these on the same dataset may improve performance

## Learning models

    - brain learns through various reward models:
    - short term rewards: storing useful information
    - long term rewards: storing useful functions
    - adaptive/reusable rewards: storing functions by relevance or abstraction
    - on-demand rewards: storing function-generating methods/interfaces
    - reward potential-maximization: storing derivation method core functions


## Election security

  - apply your other solution to election security:

    - given a range of expected votes in a category, how much does final result deviate from expected votes

      - if an attribute like intelligence is associated with a particular vote, and outcome count deviates from known attribute count to this degree, what set of ratios of deviation can be attributed to noise and what percentage to interference/fraud?

      - what is the path between the determining attribute (intelligence) & the output concept (interference/fraud)?
          - is lack of intelligence a few transforms away from the output concept
            - meaning, if stupidity is associated with voting for the x party, is interference actual fraud given that its still indicating the expected stupidity rates in population?
            - or if stupidity is associated with voting for the x party, is the result inevitable, regardless of how it's achieved (they wouldve voted the same way as the fraudster tricked them into voting or artificially chose their vote with data manipulation)


## Navigation model

    - predict variance sources & ratios:
      - type/shape/movement/interaction interface variance sources:
        - is an object that moves in restricted directions likely to be sentient life (delivery robot)
        - can sentient life move in restricted directions (drug addict, multi-tasking, drunk)
        - can interactions predict movement types (waving across street, looking at phone, predicting drivers' moves, checking street for cars/behind car before crossing/backing up)
        - are objects of a certain shape or size associated with different movement types (small objects are faster, larger objects have more obvious momentum physics, smaller objects likelier to be capable of flight)
      - assumption/hypothesis interface variance sources
        - are assumptions of a certain variance level given a certain minimum information associated with prediction accuracy
        - are multiple contradictory starting hypothesis associated with higher prediction accuracy, if so, with what degree of variance between hypotheses?


## Quantum physics

  - examine when randomness can masquerade as entanglement due to limited options of core function interactions due to the system development being in an initial phase
    https://phys.org/news/2020-01-supercomputers-link-quantum-entanglement-cold.html

  - information organized by relevance & efficiency

  - useful for computing attribute & variance flow as well as flow between flow of energy into measurement delivery 
    (optimize energy distribution between particle position/spin attributes according to measurement limits)
    https://en.wikipedia.org/wiki/Partial_differential_equation

  - look for new objects in inter-system physics - as an object occupies a current & target system as well as the space in between that isnt classifiable
    - what physics apply between systems (space-times here)
      https://arstechnica.com/science/2020/02/white-dwarf-causes-strange-relativity-effect-called-frame-dragging/

  - examine how info is being destroyed in black holes & in quantum physics - is this a process that can be used for encryption or is the info irretrievable?

  - scale transitions

      - give example of emergent effects of phase/scale transitions across threshold values that exert more variance than systems can hold

      - quantum scale transitions as delegation of information to optimal/efficient/low-energy positions
        - appears to be in multiple positions until it determines which position is more efficient or easier to maintain
      
  - quantum => info => energy dissipation:
    - when quantum superposition resolves into information, which resolves into efficiency & energy, how does it cycle back into enabling additional variance (more quantum uncertainties)
    - theres an element of dimensional collapse/compression/removal involved in these processes, releasing the variance from the structures that store it
    - the dimensions of potential in a superposition collapse into information about particle attributes
    - the dimensions of information collapse into energy, which can collapse into efficiency (path of least resistance that achieves an intent) if combined with enough other efficiency pathways
    - examine the pattern of high dimensional objects appearing to have lower dimensions at various points of measurement
      - this is similar to alternate paths producing the same output from the same input
      - as variance flows into various dimension sets, the overlap of output with other dimension sets is less likely but not impossible & may occur at predictable intervals,
        like how integers have more limiting attributes than numbers in between, so boundary physics applies to integers & they can act as a filter
    - examine when emerging attributes cant be contained within a space & leak into others

## Communication

  - tokenizing common content/content-generation functions on clients & in communications
  - calculating computable sub-components & delegating computation to nodes on network trajectory
    - this means if executing a process on content isnt efficient on the source system, calculate network trajectory to route communication toward servers that are better for that computation & start sending content unprocessed and process it on those servers
    - if you had a communication protocol that supported common content tokens, sending content to servers that are better at converting content to tokenized form would be better than a random or non-optimized server
    - sending converted tokenized content & the id of the tokenization map on different routes adds some interim security
  - using neutrinos as a way to speed up communication using them as jumping-off/charging points

