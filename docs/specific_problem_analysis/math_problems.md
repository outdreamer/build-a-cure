# Problems with Current Analysis Methods

## Problems with Set Theory

	- all useful & isolatable spaces/sets should be derived, as well as their attributes (core, emergent & otherwise)

	- spaces (sets with operators) should be auto-generated and intents should be calculated, so that spaces/sets can be used as functions, linked on a network, and chosen programmatically


## Problems with Calculus

	- should be founded on space-generating operations from a core space so that a trajectory between spaces to solve a particular calculus problem like AUC can be calculated from that space-generating operation set, which has intents mapped, rather than looking for calculation spaces to transform to from the function or from the origin space


## Problems with Linear Algebra

	- has inherent limitations due to structure assigned to intersecting concepts of position, relevance, & difference


## Problems with Statistics


### Evaluating variables in isolation

	- the variable interactions usually studied aren't normally indexed by causal metadata like causal adjacency (direct vs. indirect cause) or causal shape

	- same problem as making risk assessment in isolation & evaluating transactions in isolation
		- the risk presented to an insurer is not calculatable from metadata of the insured but from the system the insured exists in
		- the risk in a transaction is calculatable using financial system metadata

	- causal shape of the relationship is assumed to be independent/input vars => dependent/output vars, where in reality the dependence between the output & inputs is likelier to be nonzero & the framework for specifying causal distance is not formalized

	- causal shape analysis is crucial for:
		- identifying variables that can be collapsed into other variables
		- which variables that can be replaced with others
		- if there are likely sets of variables that influence the target variable
		- if a variable is an end leaf rather than a causal branch variable
		- if variation is about to converge or expand
		- if variation is concentrated in a sub-system (trade loop) that is largely independent from the host system and can be ignored in many cases


### Checking variables for predictive power instead of predicting variable metadata & sets first

	- system metadata should have a minimum of information that can enable identifying probable variable sets

	- then predictive functions should be quicker to build, because the solution space is drastically reduced by the system analysis identifying probable variable sets

	- first create an interface where the filters allow multiple variable sets & metadata, then once you narrow down which variable sets/metadata are possible, narrow it down by likelihood given system config

	- example of how variables are examined almost at the level of trial & error, when predicting the likely sets of variable sets would drastically reduce computation requirements
	  https://phys.org/news/2020-01-simple-sequence.html
		

### Treating non-random processes as random

	- rather than deriving core functions and observing the trajectory of variance from core functions to combinations of them & other normal system rules like variance accretion patterns, they assume each variable is so unrelated to other variables or so impacted by many other variables that it's random, when system analysis would identify it as a clear combination of system functions, gaps, filters, & other components

	- equality is not a default position so it shouldnt be treated as the standard from which to explore distortions
	- other standards like 'path of least resistance' indicating the importance of core functions should be used as defaults to distort instead of random equality

	- the trajectory of heat through a system is predictable like the trajectory of variance through a system is predictable
	  https://phys.org/news/2020-01-supercomputers-link-quantum-entanglement-cold.html

	- whether either is the common currency tradeable between system components is another question:
		- is heat or energy or charge the common factor explaining the trajectory?
		- is variance or adjacency or direction the common factor explaining the trajectory?

	- which mechanics explain why other variables accrete into these interfaces
		- variance/energy are both input forces that move into unrestricted spaces (interface/filter gaps) unless artificially restricted
		- does the original position of the input explain its maintenance as an input (early inputs like energy have influence in formation of systems and remain inputs traded in that system)
		- variance/heat are difficult to maintain & retain in the system without the right system structure to handle them, despite being a common output of system inputs
		- if the system isnt closed/independent and doesnt have handlers for variance that allow it to maintain its closure, variance will leak into adjacent systems
		- one way for a system to be independent in a way that doesnt leak in harmful ways to other systems is to have outputs that are inputs or outputs for other systems