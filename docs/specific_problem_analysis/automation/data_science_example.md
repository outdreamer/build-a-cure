
    - generating false data to 'balance racially biased datasets' removes the embedded info that some races are oppressed
      https://www.zdnet.com/article/this-startup-wants-to-fix-your-biased-ai-one-dataset-at-at-time/
      
      - how would you derive that info (derive the concept of 'racial oppression') from a data set with 'racially biased data' that reflects real problems, without erasing the information of those problems, information which might be relevant to what youre predicting?
        - if you only have race/location columns in your data set:
          - if races are differentiable by location:
            - why are they isolated? is one race being targeted by the other? is there another conflict, like previous oppression, creating economic disparity (an alternate cause of separation by location)?
          - if races arent differentiable by location:
            - check for crime data by location:
              - is crime higher in locations with integration? why might that be the case?
            - check for other types of location separation (granular separation, like by neighborhood rather than zipcode)
      
      - the concept of 'oppression' can be inferred in many ways, depending on the data set
        - resource access
        - health metrics (image & consumption habit data will show signs of stress)
        - state changes:
          - if the data is new, it will reflect different problems based on the different state of power distribution
            - previously powerful oppressive groups experiencing a backlash, reflected in stressors
          - if the data is old, it will reflect problems like:
            - clear differences in health/culture from severe segregation of groups
        - 'group' changes
          - group dynamics between oppressors/oppressed through phases over time
        - 'tech' changes
          - other variables, with info tech & science applied (science to improve health metrics, info tech to magnify & integrate group dynamics)
        - the concept of a 'health standard' ascribed to racially-associated traits that doesnt correlate with actual health, but allows variation in social choices, given other metrics like social skills
      
      - real 'balancing' of a data set is meaningfully balanced, not just variable value-balanced:
        - a data set with exclusively minority criminals could be balanced by adding real data of majority criminals (not generated data)
        - this doesnt remove the information implying oppression, but it does include a standard to compare different causes of crime
          - the oppressors who are poor are likely to be poor for different reasons (intellectual poverty, drug addiction) than oppressed minorities (systemic/compounding inequality)
            - how would you identify this difference in cause in the data set?
              - the poverty of an oppressor is likelier to be a choice, so the key factors would be:
                - signs of intellectual poverty (facial tattoos, signs of aggression, copying pop culture, lack of social skills)
                - signs of addiction (lack of habit variation, lack of other consumption habits)
                - signs of agency (lack of optional health habits, access to expensive health options)
                - signs of power (lack of self-modification, lack of stress)
              - the poverty of an oppressed minority is less likely to be a choice, so the key factors would be:
                - signs of group membership/strong relationships (gang signs, embedded cultural symbols, similarity of habits/consumption within groups, signs of social skills)
                - signs of lack of power (self-modification, signs of poor health/high stress, lack of planning for future)
                - signs of lack of agency (lack of health habits, modifications are likely to be cheaper, a/antisocial behavior)
                - visible differentiating factors (enabling others to identify them as similar/different)
        - a data set that correlates gingers with crime would be balanced with data where that trait is not targeted for oppression (is it associated with more crime in Mexico too where theyre given more power than average - what about where its not noticed/targeted at all, like a community of gingers)
        
      - a good AI would identify relevant causal concepts (depending on the data set) like:
        - positive concepts of health, social skills, intelligence, strong relationships as factors in success that can offset the negative concepts associated with oppression
        - negative concepts like stress, poverty, & gangs
        - neutral relevant concepts like tech & science, which can be used to oppress or liberate

      - a good AI would infer that this was a complicated problem requiring conceptual analysis by the complexity of language & variation in social interactions or language progression, which would give it information about how new terms develop (to categorize & identify oppressive behaviors, often using comedy, so those behaviors can be stopped) and how they propagate (used by oppressive groups for profit, leading to self-awareness), giving the AI information sufficient to infer the concept of oppression

      - a really good AI would identify causal structures like causal loops in the data:

        - example: 
          - oppressing a group by an irrelevant factor like hair may upset members of that group, which may either cause members of that group to develop social skills like emotional regulation, or do crimes
          - this creates a self-fulfilling prophecy in the form of a causal loop: the hair used to be irrelevant, until they were oppressed for it and reacted to that oppression in a negative way, which made the oppressors think they were right, and may make oppression continue
        - it would also isolate strong causative anti-crime factors like emotional regulation, which will have some different signs & problems (lack of emotional expression, signs of intelligence, signs of stress to correct other people's problems)

        - it would also generate its own standardized language to generate/infer/associate concepts:
          - 'emotional regulation' would be translated to 'irrelevant status change signal response regulation' by a very good AI that can process concepts & apply standards
          - once that concept is formatted that way, it can infer structures to look for like facial expressions & chosen status signals, without knowing about them
          - the concept of 'emotional regulation' can be inferred:
            - 'status change signal responses' that are identifiable by other agents

          - 'status change signal responses' (emotions) can be inferred from core concepts like 'status', applying 'change', and 'information processing' or 'communication', as well as 'stress' and the associated concept 'stress handler', identifying a 'status change signal' as a stressor

          - once concepts are generated & standardized in this way, other concepts (like 'intelligence' or 'oppression') can be fit into this standard & the meaning of each concept identified quicker
