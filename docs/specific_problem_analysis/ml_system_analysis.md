## Standard Neural Network Design for initial complex problem factor identification reduction
  
  - identify problem types that a neural network algorithm & structure is best equipped to handle with intent-matching, then once you have components matched with core intents (aggregate, select, average, etc), you can reverse engineer network structure/algorithm given input intent, once intent is deconstructed into core intents with known components

  - given that neural networks apply "apply, aggregate, & filter" functions to sort causative information into a standard shape like a tensor or vector set 
    (representing function variables generating an output variable)

    to identify sources of causation (like feature position or feature shape),
    
    how can neural network structure & algorithms be designed to generate a network & algorithm that is likeliest to be able to identify causative factors in the largest range of problem types?

  - there is an optimal network structure & algorithm that can handle the derivation of most causation shapes, since features aggregate in sets that have patterns between input features & output sets

  - the goal of optimizing neural network use is to identify the prediction function from as little data as possible

  - once you can identify an accurate prediction function using one data point (so its robust to changes in causation), the field of machine learning would be invalid

  - in order to do this, you need to identify:
      - candidate variables (cant verify which are actual variables without more data points)
      - variance patterns in the data point (one candidate variable leading to other candidate variables with net impact on overall variance)
      - causation shapes related to those variance patterns (causal loop, causal vector, causal network, etc)
      - priorities (structural priorities like aggregate, distribute, balance - functional priorities like align incentives, produce variance, optimize - conceptual priorities like change)
      - patterns, structures, generative functions, etc - all derivable objects on all interface layers
    - or any subset of these which can explain the complexity of the data point

  - given the profile of these derived interface objects, you can design a neural network that can identify:
    - the minimum information of future data points necessary to accurately categorize a new data point as belonging to the class of the training data point
    - the change patterns objects of this class are likely to display

  - so a neural network that can identify any prediction function takes these interface object derivation functions as input, and if they are above a threshold, 
    integrates them into a final decision of class & other metrics like 
      - change patterns
      - minimum information
      - optimal network design for this complexity level (which will probably involve fewer calculations than the 
        original classification calculation because not all information from derived interface objects 
        provided enough variance compression to be included in the final classification)

  - given that existing networks identify feature contribution, using metrics like feature position, 
    more accuracy in generating prediction functions can be added without extra computation using other interface object metrics like:
      - feature type
      - feature variance
      - feature priority
      - feature distortion
      - feature uniqueness
      - feature causation shapes
      - feature change patterns

  - this means a set of networks evaluating the contributions to final classification for a given complexity level or other system metric made by:
    - causal shape
    - change patterns
    - type stack
    - conceptual query

  - can produce answers to questions like:
    - which interface object combinations can be used to generate an accurate prediction function?
    - which interface object combinations map to which prediction functions?
    - which interface object prediction-function generating networks should be used first on a problem, 
      given that interface layer's higher independence/causation/variance-generation?
    - which problem types (conflict, alignment, asymmetry, lack) map to which interface objects?

  - so instead of doing problem-solving operations like:
    - get data
    - apply standard DNN or neural network structure designed by auto ML
    - use prediction function until no longer valid

  - you can run problem-solving automation operations like:
    - get data point
    - apply interface object derivation function
    - check if problem is solved
    - if not, apply interface-based neural network design function to generate optimal neural network for this problem until problem is solved, 
      at which point, store this interface object combination in index of solved problems
      and to make each new prediction, first apply interface object derivation function to each data point to format it 
      in ways that the neural network trained to identify contribution of interface objects to final classification can interpret
    - if problem changes:
      - generate new prediction function, according to previously identified change patterns of derived interface objects if they exist
      - re-apply whole process to generate the new prediction function or a neural network architecture to generate the new prediction function


  - if weight path patterns are a wave or other function - extra nodes between them add to the curvature, which is why adding extra nodes can add complexity

  - neural network nodes as facets of additional complexity/dimensions - calculating the complexity of a prediction function-determining problem (based on which distortions are likely from prediction function patterns) would allow selection of the necessary nodes
    - higher complexity requires that more combinations of weight path patterns be experimented on, and more nodes allow room for more experimentation
    - auto-configuring the network with common or data pattern-specific weight path patterns rather than random or equal could speed up training
    - reverse logic can be beneficial here - given the complexity of a problem, which features with which differences would have to occur for the training to be useful (or for the problem to be solvable)? check the data for those variable types/differences
    - use prior knowledge of patterns (insights like 'differentiating variables tend to cluster') as a way to organize analysis of the corresponding feature data pointed to by the insight (adjacent features are passed in to higher weighted node sets or node sets equipped to handle subtler differences)
    - different network or node sets can represent different problem-solving automation insights (one node can represent a 'differentiating variables' filter, another node can represent a 'cluster' verb/function filter)
      - if none of these node sets finds a pattern, the node sets can be recombined into new options (like another outer layer of a core function diagram) and re-trained
    - adjacent networks are created for stacked variables (like symmetry stacks, where features differ on a symmetry and symmetries are layered (like the hand-limb-spine symmetry stack)
    - phase shift points are identified first & the data is standardized around them - so when one type has extreme/compounding attribute values emerging as another type, that threshold is built in to the data (data near to that threshold is transformed to be higher to differentiate the types, or you add a third output category like 'transforming' or 'interim type')
    - possible causal shapes are identified first & the network is organized to fit them
      - for example, weight paths trained to highlight one attribute set are applied to other alternative equivalent attribute sets (an alternate causal route to the output)

    - what is the ratio of coverage of all possible feature interactions that is fulfilled by a standard neural network? what does the standard architecture reward in terms of clustering attribute sets for experimentation?
      - it rewards attribute sets that appear correlated in the data, with generalizations applied

    - should you use position as a determinant of feature importance? should position be removed from the data & another network trained on position-removed data?

    - would it be better to frame features in terms of system analysis (attributes, functions & routes)
      - an ear definition route can be framed as:
        - take other dimensions of change (than vision or taste) & assume methods to detect them (sound), then design a system (ear) based on efficiency as a priority to detect that change
        - stack a symmetry on top of the spine to make calculations & choose priorities, then stack another symmetry (face) on that symmetry (head) to host multiple change-detecting methods (facial features) to guide calculations
      - once you frame features in this way, training to find prediction functions should be trivial

    - can you partially reset the neural network mid-training to help improve generalization in addition to existing methods, so it doesnt tend too far in the data-dependence direction?
      - identify features that are likely to be data-specific, given whether the categories share that feature with different attribute values

    - training prediction functions on system data rather than data for objects within the system would add other gains
    ` - by knowing the structure of a system, you can infer insights like:
        - which object shapes are most compatible with the system
        - which forces/interactions are likely to evolve in the system
        - system metadata (stability, priorities, potential)
        - system info objects (assumptions, inputs, efficiencies, incentives, phase shifts, ambiguities, core functions, boundaries)
      - for the dog vs. cat classification, training on causal system data (the evolution system, the bio system, or the DNA system) might be a better target for the prediction function than training on images of the outputs of those systems
      - this analysis would help predict ambiguities (dogs & cats will have very similar features sometimes given how DNA & evolution works) and how to differentiate them (check for specific attribute sets in data, otherwise indicate that data is insufficient but here's the data you should gather for this problem type)
      - you might also be able to identify a sub-system that has the most valuable data for this prediction (mutation sub-system as a sub-system with a distortion function applied to the DNA system)

  - add to ml explanation

    - in a fully connected network, every weight path from the previous layer in passed on to each node in the next layer, meaning that if a set of weight paths with additional weights applied cant produce a 'feature found' or 'yes' value above the threshold, those weight paths & additional weight combinations arent passed on as candidates for the prediction function or building blocks of it

    - the aggregation of weights & input values creates a tree of trees on each weight path, where leaves are the input values, which are multiplied by a weight as they hit the branch, and the branch represents addition creating a new hub or joined branch, and each tree is sent to every other tree branch hub in the next node layer
      - can you calculate anything faster by changing this structure, like adding memory in between each tree, arranging them around the memory like in a circle, so changes, similarities, etc are synced with the memory before proceeding (so the nodes would be computing things like change rate to check if its worth mentioning to the hub, rather than checking value compared to a threshold, which would be the responsibility of the hub memory)

  - prediction model trained on conversations as encryption key/alg parameters, updated with new messages

  - how to check if a data set is similar to one that has already been trained, to avoid re-training to save CPU

    - store metadata about the data set like shape (groups/clusters, linear, random) and the metadata for those shapes (radius & overlap for clusters, distortion patterns & outliers for linear, starting point for random)

    - derive info metadata like type, cause, change patterns & check if determining/generative structures (core functions, symmetries, false similarities) match across the two data sets

    - identify patterns of variation once a similarity has been found, to avoid checking the whole data set
      - example: once you identify that both data sets have two output categories, what are the patterns of difference in the internal points of those categories (patterns in overlaps, misidentifications, corrupt/incomplete data, differentiating variables) - usually youre applying a categorization model to two categories that are very different (so its important to identify them correctly) but have some illusory similarities or features in common, making the categorization task non-trivial - so you'd look for patterns of differentiation within categories, to check if the data sets are approximately equivalent so you can use the same model without training - either specifically, storing patterns of differentiation for specific categories, in the model metadata, or generally, for general category differentiation patterns

    - store adjacent functions or specify a parameter range generated from the original function to identify functions that can be generated with accessible transforms or functions that are usually generated for similar variable sets (using common function patterns), to identify similarity in parameter values (using various types of parameters, at various layers on the causal stack, such as preceding functions like the function producing this function as its derivative, alternate functions like the series sum, or descriptive parameters like moment-generating functions)

    - identify function/data vertices, which are determining points like maxima/minima/inflection points as well as the minimum number of points necessary to identify the momentum of the curve, or points that indicate phase shifts in general

    - store semi-trained models and use them as vectors to create a complete trained model, without training


## neural network notes

    - preventing consecutive extreme examples could prevent the need for some corrections during training

    - how to identify false correlations (irrelevant features, similar features with no direct connection, interactive features like external agents)

      - example: dogs wearing a collar more than cats, so collar is identified as a feature predicting species, but other data sets have more cats with collars

      - output/emergent qualities of dogs are:
        - domestication
        - lack of aggression/docility (personality can be identified bc they smile more & bare their teeth less) 

        which could position the collar feature as an output of:
          - being a dog (its easier to get collars on them)
          - external interactions/agents (like culture, education, & DNA are sources/expressors of agency that can appear to cause similar output or chosen features)

        and therefore not causative of the dog type (instead associated with the domesticated animal type)    

      - identify that any domestic animal exhibiting any aggressiveness at all would also have a collar, so its not a type indicator 
        (concept trajectory: animal.domestic + animal.aggressive = animal.has['collar'])

        - identify the concept of the controlling species in photo background, or infer their existence by the fact that the collar couldnt be made by the dog given its biological limitations

      - organizing features by intent (collar is to control/track identity, and external agents are likely to control other animal species in a similar way)

    - reducing noise (both in data so less training is required, & weight path patterns, so fewer variants need to be checked & variants outside of probable ranges can be eliminated early)

      - reducing noise or identifying cause with decision objects 

        - identify decisions interfering with or causing data (decision chains & other shapes, including variants like incentivized decisions, efficient decisions, etc)

          - examples:

            - decision to interact with an object (causes future options or limitations)
            - decision of short-term thinking/efficiency-prioritizing species to pollute environment (causes dangerous chemicals in water)

          - rather than trying to infer the cause of the polluting chemical from physics/bio rules, you can infer starting from a decision chain given agent rules, because not every data pattern will be an accident, and agents interfering with/causing data can produce noise or legitimate data variation

            - you can start by looking for signs of agency, in objects like 'required technology to build something' (fingers necessary to build collar) or 'extreme variation not found in nature' (hats)

      - similar to reducing noise from the concept of agency, noise from other irrelevant interactions can be reduced by identifying:

        - adjacent objects that have interaction potential (weather system is likely to interact and can change how an animal looks)

        - objects that are likely to interact, either by:

          - incentives (anyone who takes their toy is going to be interacted with)
          - probability (anyone walking around the neighborhood could be interacted with)
          - enforced rules (the mailman is required to interact with unless window/sound are blocked)

      - probable ranges & vertices of a prediction function can be estimated better once you know the reason the function emerged, which is a key function metadata attribute linking the causes & the output intents/priorities

        - reasons on interfaces such as 'clustering rules' or 'boundary rules' or 'interaction rules'

        - a function's different emergence reasons can be used as a way to generate probable vertex/range sets, given a particular reason

          - example: is a function shaped like a random cloud bc several clusters are overlapping/colliding/converging, or bc theres noise in the data, or bc the type clusters just hit a variance injection point at similar times

        - the 'emergence reason' concept is related to cause but is more specifically definable as 'a rule interface directly preceding the function, generating a trajectory'

          - 'because they hit a point' is a reason for the function's current state, and it also describes the trajectory of the function's future behavior, so it can be called a priority/intent as well as a cause, since it relates the origin & the target of the function

    - objects in the ml system need to make sense together, not just being valid or functional according to their definitions, but changing in ways that benefit system intents under change/interaction conditions (contribute feature contribution information to prediction function) and dont contradict each other under certain cases

      - for example:

        - with two related system parameters having different bases:

          - learning rate (improving with respect to time, with value based on change ratio)
          - activation function (based on threshold)

        - should the activation function change across the concept of time as it occurs in the system (meaning 'a training cycle', 'a traversal of network weight paths'), given that activation and learning are related (should they use the same base and change according to the same base)?

            - should it be different if its the first or last cycle in training, or the first or last layer of a type, given that adjacence to the final category output makes the final decisions more important, just like highly differentiating weight combinations are important?
            - should weights be adjusted according to this difference in importance

        - should system limits like learning rate & activation threshold have attribute values in common, or do they occupy system positions where similarity isnt productive for system intents

        - should possible weight paths have gaps in between them which are not reachable with learning?

        - when estimating corrective measures to adjust the learning rate, high-cost error types should be identified & prioritized:

          - one error type scenario is that another global minimum exists thats severely different from identified minimum
          - another error type scenario is that the global minima steadily decrease or decrease in an unpredictable way bc the function has many peaks, so theorizing the existence of minima will be high-cost without semantic information

        - when an answer is guaranteed/determinable in a weight path is when it should be checked, which may be earlier than the final layer, so calls to nodes on the same layer may be justified

        - some alignments in the system shouldnt occur (like function aligning perfectly with data) bc the intents dont match (function intent 'predict other data' and data intent 'to represent a sample')

      - error cascades should be evaluated for system design

        - if learning rate is above a range of correctness, or weights/bias are slightly too high & the activation threshold is just below what it would take to deactivate that node
          - the error ranges that can happen from parameters should be looked for in differences between category examples that are the most similar (or likely to be similar given system analysis of problem space like 'how species typically differ')

        - features that are too similar for the system parameters to catch can be accounted for 
          - by magnifying the differences before training time, or accounting for error types at training time, when weight paths are found to have a gap overlooking a weight path that wont catch a particular similarity

      - high cost errors can be evaluated for alternate outcomes with distortions (a system parameter determining the range of distortions that can be used to correct outcomes, that is fewer for example than the distortions that are typically required to turn one category into another)


    - add to explanation: 

      - fully connected network passes weighted versions of different weight sets from all previous inputs 

        - 3rd layer will have weighted sum of weighted sums of input features, so each weighted sum in layer 2 is contributing to a weighted sum with the other layer 2 nodes
        - the fully connected network is like a reverse decision tree, where decisions represent sums rather than path selections & all lower inputs are passed up to each node above it
        - if A, B, C, and D are the second layer nodes representing a particular weight set of the original input features, the third layer nodes is composed of weighted sets of A, B, C, & D, and so on

        - another way to visualize a particular node path is as the successive progression of that path toward a central origin (the target label) where each layer is the weighted summation of all nodes on that circular layer (to do: make a diagram)

    - whats the value of identifying weight path patterns that can generate a clear answer vs. the patterns that can generate a clear 'not' answer or a lack of clarity between alternatives? should you train to identify an alternate type that can be used to determine the other, given that they are alternates in a set?

    - similarities & other system filter objects between weight paths can help predict errors in pooling layers later in the network

    - what happens when one variable is at a different causal level than the others? 

      - example: 

        - proxy variable: the tail of a cat is more constant than that of a dog, so it can be used as a proxy for a categorization function of multiple variables

        - higher cause variable: DNA vertices are a higher-cause variable than any particular feature set, because they generate identification features

          - how do you keep your data at the same level of cause, so DNA data isnt mixed with image data for categorization? 
          - physical conditions can also be higher cause and could interact with image data by default if the condition is visible 
          - a physical condition could override genetic influences to determine differentiating features (if the dog was in fights, that will override their DNA to determine their feature shapes)
          - its especially important to keep variables at similar causal levels because algorithms will aggregate them, and if you can skip analyzing a causal variable set and instead analyze the causal output variable of that variable set, that will reduce training time
          - in this case there should be alternate paths in the network to handle edge cases, or you could restore the overridden data to its original beforehand (restore a torn ear to its original shape)
          - this means different weight path patterns for false similarities, false differences (a torn ear isnt really a species-specific feature, unless cats are in far fewer fights)
          - you can generate default weight paths to handle different cases like that by:
            - ignoring features that are corrupted (distorted) or that override the real causal variables
            - building in paths that lead to deactivated nodes or zero-weighted features in the case of multiple distortions applied to a feature that dont appear to be from the real causal variable level (output distortions like tearing, which arent from the DNA causal layer)
            - allocating these weight paths that found distortions or different causal level variables to other causal structures
              - looking for DNA causal variables in the ear distortion would mean looking for causal concepts like aggression & then looking for signs of that concept in the data, for features that would cause or indicate aggression, like bigger size or sharper teeth - then the output of this causal search, which is the on the target DNA causal level, could be integrated with the original data set & training could continue
              - or it could be determined to be an output variable rather than a causal variable, and restored to its original value & integrated with the original data set to continue training (and all nodes from that point on could apply a weight transform for that feature if the distortion level is found in other weight paths - installing a memory of weight transforms)


    - error types:

      - example: 'training an ai model by remote controlling a drone to pick coconuts' will have some errors with outputs like 'changing direction to account for wind blowing target in other direction', when the inputs were a monkey pushing the leaf to the side which made the drone's model think that a gust of wind moved it and it should turn to maintain its trajectory

      - how would you design an ai algorithm to:

        - account for these error types in algorithm design
          - identify error types in data like 'assign weight to leaf motion to create a direction change' and convert them to the right combination of factors like 'check for other sources of motion first'
          - apply activation functions to node sets to identify patterns that are error types which shouldnt be passed on as valuable information to determine when to change direction
          - look for similarities (mapping to attribute alignments) in weight path patterns and other system filter objects that could be error types
          - error type deactivation in a hybrid network of networks mapped to sub-problem type (solving a sub-conflict between similar alternatives with a standard categorization network), where combinations of objects like an info asymmetry + assumption dependence lead to error types, so activations can be applied to limit the passage of data after these combinations in the hybrid network

        - predict these error types
          - look for attribute alignments (direction changes produced by both monkey pushing leaf & wind but appearing to be similar enough to be difficult to differentiate given that the monkey may not be visible) and other system filters

    - what would the value be of keeping some parameters randomized, some constant, some locally determined or ambiguous until training/run time (parameter or weight superposition)?

      - determining threshold values & aggregation/grouping methods when particular value sets or weight paths are determined to be causative or require disambiguation:

        - to provide a clear distinction in a categorization or feature selection problem

        - to adjust for errors in the algorithm-problem type match or the algorithm-complexity match if they become significant, so changing data is handled better

          - to find the optimal position of a particular function in the network by training
          - to allow for node clusters/paths to solve sub-problems
          - to allow for iteration of a node & other causal shapes to be applied locally 

    - when a decision is increasingly clearly ambiguous/indeterminable during training, what is the sequence of strategies to follow before returning an 'unknown' prediction
      - navigate to previous nodes when decision wasnt clearly ambiguous & distort data to check for adjacent alternate versions that would be clearly differentiable
      - check for randomness (found with corrupted data, false similarities, and other system objects)
      - check for different causal route to features 
        (species with similar features will have different routes to those features, and the route would leave traces, if not in that data point, then in others, so integrate data set statistics)
      
    - certainty networks vs. isolated predictions

      - a network of predictions with certainty rates would be more flexible as an output than a clear answer, where the network could be applied to protect against unforeseen cascading data changes
        - if an edge case value is in a certainty network, the prediction functions will be able to handle cases where edge cases cascade better than storing isolated predictions

        - this would handle a range of 'potential cause' rather than just 'known cause', for variables that are likely to be causative in high degrees (in conditions like if the system is similar to known systems or has a randomness injection point or other system features)

    - when you organize a network so that each node is only doing one task (executing a task function without any other analysis, like a dev creating a service rather than first or regularly analyzing if the service will actually help the business), its less flexible than a network of nodes that can do multiple tasks

      - a node that can answer 'yes/no' should be a node that can answer 'yes/no if' (given a threshold condition) and 'aggregated yes/no if' (asking adjacent nodes a question before deciding) and 'averaged aggregated yes/no if' and so on, to mine differences between weight paths for insights


    - identify vertices such as cases where individual nodes or subsets can totally change the outcome of the training or produce phase shifts or other important system objects and make decisions about thresholds for those cases before training (what do you do when adding a node adds error 60% of the time and more accuracy 10% of the time and neutral impact the rest, given the data (delegate to different network architecture, gather data, use system objects/patterns to make predictions in those cases)

    - how do you check for optimal combinations, causation, feature sets, and system objects reached before the end of training at a particular node, or a feature/weight set that matches the correct prediction function bc its a high-impact feature/weight combination (weight x applied to weight path 1, weight y applied to weight path 2) that happens to identify an important system object (efficiency, interaction, variance gap, causal structure) - you can include a test of original data in each node for predictive potential once you determine the level of complexity needed to create a prediction function (number of terms or level of variation, which can be used as a filter before running tests for system objects or correct prediction function similarity at each node)

    - how do you identify different types of relevance & nodes that supply them for an intent like categorization/prediction?

      - different types of relevance like direct causation (is causative), imminent causation (will be causative, or may be if imminent conditions occur), alternate causation (can be causative)

        - for categorization, direct causation may be identifying/differentiating features - imminent causation may be converging or adjacent features

        - if all a node does is answer yes/no for a particular question, that will be sufficient for that question but not other important questions that could be answerable with the data

        - how to tell which questions are answerable given that one question is answerable

          - meaning you can reach answer node B given a starting point of attribute values, a function set, and a question intent direction identifying an information gap in attribute values, where the answer node may not fully answer the question but it will move toward the question intent direction

            - for example, asking 'is ingredient A toxic' can be answered with filters (probability filter such as 'not likely in moderation') or mapping a trajectory between nodes agent starting position & agent death

            - so the question ('is substance A toxic') can be formmatted like: 
              - a path between nodes agent position and agent death
              - a movement in the direction of node clusters with the high-toxicity attribute (implying toxic intent of the substance)
              
              where the question is the information gap: 
                - will event 'ingest substance A' move in direction of high-toxicity clusters, an adjacent intent direction or state, or toward agent death

            - so other answerable questions would involve a subset of that information (a subset of the path of the answer) or other related object (alternate, opposing, etc)

          - involve a subset or same set of attributes
          - involve opposing/complementary attributes
          - involve direct causative or caused attributes
          - involve abstract attributes

      - node sets/weight paths that arent important for direct causation may be important for imminent causation

  - given that certain algorithms can only add so much certainty for a particular problem type, that should be integrated into output (trust shouldnt be default handler for algorithm output)

  - behavior data, search data, purchase data can be used to link health conditions and train AI to predict conditions from a symptom search

  - statistical tests & hypotheses should be standardized for false objects (false similarities), errors in assumptions (non-normal distribution) or data collection/measurement, change types (about to become another distribution/in the process of being converted)
    
    - 'a hypothesis assuming random independent variables is more likely to include m/n non-random variables or x causal shapes'

    - these insights can be used to adjust test critical values in the absence of data on correctness to use instead:
      - 'a z-value of 1 for a data set like this is likely to be accurate 5% of the time and is likelier to be 1.2 most of the time'

  - structural mismatches of solutions & problems

    1. algorithm structures: theres an inherent structural mismatch between some algorithms (decision tree, neural net) and some problem types (prediction) given the intent & abstraction layer 

      - example: 

        - some algorithms are too specific or have a structural mismatch (decision tree has structural split, specify & direction intents) with the problem type (predicting a set of variables that is about to change)

        - the decision tree occupies a very low-level, neutral, granular, abstract/structural position - that means it can be used for many intents 

        - the decision tree can be hacked with various edge case, phase shift & boundary manipulations

          - if two variables on different layers are about to converge, they should have been on the same layer or in the opposite causal direction or treated as one variable - the model will be increasingly wrong until it's updated or until these ambiguities and likely relationships are accounted for in the design

          - the reason algorithms can be hacked is bc of the structural mismatch between the algorithm and the problem type

        - the intents resulting from the decision tree's layers, direction, and thresholds can only contain so much variation in the data 

        - generated varied data with permutations of data objects (loops, sets, alternates) can be used to make the tree more robust to adjacent/likely changes

    2. test structures: a similar problem is when the test/method will identify false similarities or other objects & return the false version, limiting the potential for the correct version to be identified

      - the structure of the test can be a barrier to the truth, if its over simplified or excludes too much information

      - a test that's done iteratively on new data without changing the function according to new information would fail if the data changes more than the function can handle

    3. brain analysis structures: structural biases in human brains prevent us from seeing the truth - we're biased toward objects we understand or which are simple to derive

    - algorithms should identify mismatches (in complexity, variance, completeness & other metrics) & other problem types between the data/algorithm/problem type

    4. prediction method structures: 

      - bayesian probability has the incorrect structure for solving a problem of predicting the dependent variable if its not actually the dependent variable

      - meaning if the input is actually an output, like a southern dialect is an output of location, so predicting criminal activity for a location based on whether they have a southern dialect may correlate with some location-crime data bc of hot weather increasing emotions, but southerners can move while retaining their manner of speech & can change their emotion-based behavior from the new cold weather

      - probability of (criminal activity | dialect) should be probability of (criminal activity | weather), since weather is the more causative reason why location is a significant factor in criminal activity


  - algorithms should produce a set of solutions (an obvious/simple answer, a pattern-compliant answer, a common answer, a robust answer)

    - example of why youd want an 'obvious solution' tag on the output:

      - how could you build an algorithm that wouldnt overidentify a race attribute as being a potential criminal indicator?

      - query maps & causal shapes:
        - query for data on related word sentiment (if theres a negative association with a word related to that race)
        - query for data on intent ('identifying potential criminals' is the intent of the training process, which needs to be an input to the algorithm)
          - why would identifying an individual by a related attribute to a negative related term be useful for the 'identify potential criminal' intent?
          - if the intent is to 'identify potential criminals', which is a high-stakes intent
          - the algorithm should identify that a causal loop would be dangerous to treat as an input, given that a causal loop (like mistreatment or persecution of minorities leading to poverty which leads to crime) can hide original inputs (root cause being mistreatment)

      - alternatively, it could use interface analysis:

        - use inference & intent (to predict) to arrive at insights like:

          - 'if the answer is obvious, it must not be true bc otherwise I wouldnt have been asked to predict it, unless this is a test situation'
          - 'if the attribute value is common across the population, it must not be a predictor bc there are many people with that attribute value who are not in this data set bc they didnt commit a crime'

        - hard-code insights like above to be consulted if a variable is mistakenly identified as significant

        - treating one involuntary variable as high-impact belies the complexity of social games, which involve learning competitions (who has the best manners, whos the smartest) that would mean there is high potential for economic status variation within nodes having that attribute

        - given the changeability of that variable (easily changed with structural tools & also frequently changed in gene pool), it should not be treated as a predictor

        - it should identify location & economic status as an indicator of criminal activity given the health/drug addictions symptoms of the people whose data is used for training

        - it should also identify social games that are used for criminal activity & skill at those games (making intimidating or emotionless facial expressions)

        - it should identify culture as a key factor in variation in criminal activity, which can be specific to an attribute but has high variation within that attribute (producing gang violence, govt corruption, or a culture of karma/street justice) with low variation in outcomes (kill or be killed) - and identify that if it doesnt have cultural information, it cant make predictions

      - most algorithms dont have the complexity to identify complex sub-systems or related systems like social information games, communities, economies, or cultures

      - a really smart algorithm would immediately identify a few insights like:

        - example of deriving a social game like bullying:
          - with a priority of 'avoiding criticism'
            - most nodes would deserve criticism, because avoiding it is easy, especially at scale
            - this incentivizes criticizing nodes who dont deserve criticism

        - 'there are different reasons people do crimes'
        - 'some reasons they do crimes include need (resource acquisition, asset/reputation protection), goal attainment (enable a career), culture (avoiding crimes isnt a priority), social games (dares, threats, bullying, corruption, group dynamics), enjoyment (test if anyones paying attention, test how fragile the system is, rebel against authority)'
        - 'randomness is the biggest distributor of those reasons' (luck)
        - 'randomness can lead to lack of justice or other types of meaning'
        - 'people who have lack of justice are likelier to do crimes'
        - 'which people dont have justice - unfairly persecuted people'
        - 'which people are unfairly persecuted - different people, excellent people'
        - 'some privileged people with justice/meaning do crimes anyway just for fun to see how much they can get away with' (counterpoint - other reasons to do crimes than need, culture, social games)
        - 'which facial expressions are associated with the criminal reasons we're trying to prevent'
        - 'which factors do we need to complete the prediction function'
        - 'which expressions are often false signals despite being good predictors in subsets'

        - how would you build an algorithm to identify those insights just from maps & from mugshot data? 

          - the point is invalid once you have those insights, bc the answer is clearly not predicting who will do crimes but figuring out:
            - a system of social rules to prevent those situations from happening in society
            - at what point people start trying to have fun rather than contribute
            - deriving intent (either mechanically or with a prediction tool)

          - the algorithm would identify a high-variance data set and derive that its not the obvious visible attributes that will be predictors (except a few like visible signs of crack addiction, which will only predict the ratio that are crack addicts) but the subtle visible attributes, which will necessarily be incomplete without data about each person's specific traversal of the maps

          - the algorithm would identify that certain facial expressions are associated with criminal activity, such as:
            - dead eyed hopeless expression (lost hope of good treatment)
            - crazy expression (went on a rant after putting up with some stressor)
            - shocked expression (cant believe this happened to them)

          - most of them would also have signs of stress destructuring their faces, except resilient criminals, bosses, or some first-time offenders

          - the different routes to each expression should show up in traversals of social game, community, & culture maps as their status & decisions change, and cross concepts like randomness (in which ideas people encounter, which skills they learn, which social games they play), equivalence (in resource distribution), justice (as a counter object to randomness), & meaning (group belonging, goal attainment, success)

          - the output would be a set of paths that leave traces, some of which would show up in mugshots, some of which would require questions

        - identifying a concept (like 'reason to do crime') can be as trivial as exploring combinations of core functions

          - once the algorithm identifies the concept of language (from input data or insight maps) as 'information trades formatted as paths', it should be able to identify the concept of lies, as it will know that 'information can be inaccurately described' from its own errors

          - once it identifies lies, it can identify the concept of social games (reward from coordinating a lie with another node, reward from a successful lie, reward from adding a distant transform to another node's information)

          - once it identifies social games, it can derive their intent (the point of social games is to control other people or to get resources like enjoyment or funds)

          - then it can identify specific social games as anti-societal behavior that hurts the group

          - the counter-object is pro-societal rule compliance that helps the group

          - then it can move on to identifying specific social games (initial crimes, bullying) that remove inputs of pro-societal decisions (complying with rules), creating a 'reason to do crime'

          - then it can look for outputs of those games (stress, difference) and make an attempt to translate that to the data features (facial expressions, signs of addiction, signs of aging)

        - this requires that society-wide data needs to be integrated into the algorithms (like genetic variance) as well as data on criminal activity


  - rather than using data & the training process as an indicator of consensus, they can use patterns (patterns of data, patterns of change, patterns of variables) as an indicator of consensus

  - algorithm based on problematic adaptive systems like cancer bc theyre learning faster than the host system


## alliance opportunity identification algorithm

      - example of an alliance opportunity: two groups have an incentive to say the same thing, for different reasons & intents
        - oppressed person has a group incentive to condemn a biased person against their oppressed group
        - privileged person has a group incentive to condemn a biased person, even if the biased person is a member of a group they belong to

        - this is because of the attribute/function/object alignment

          - they have an incentive to say the same thing condemning that person, but for different reasons & intents

          - the oppressed person has:
            - incentive to condemn the biased person
            - incentive to condemn the biased person, as a group
            - reason to protect people in their group
            - reason to protect themselves
            - intent to find other people willing to condemn the biased person

          - the privileged person has:
            - incentive to condemn the biased person, if they havent done the same biased thing on the record
            - incentive to condemn the biased person, as a group
            - incentive to condemn biased people against oppressed minorities, with oppressed minorities
            - reason to protect people in their group, by seeming unbiased
            - reason to protect people in other groups or oppressed groups, to establish good will, make peace, avoid criticism, or earn future favors
            - reason to protect themselves, by seeming unbiased
            - intent to cooperate with other people looking to condemn the biased person
            - intent to establish good will, relationships, make peace, avoid criticism, or earn future favors from oppressed minorities

          - the reasons to say it are different, but the content of the action ('condemn the biased person') is the same, and other objects like intent also differ between those positions


## stereotype nn algorithm

    - log objects of bias like:

      - false similarity between rule & data
      - correct contextual rules, that are incorrect in new/most/unexperienced/possible/common contexts
      - correct rule component/input
      - similarity between biased rule and correct rule structure/input
      - lack of concept of alternative possibilities
      - priorities that mimic or create error types like over-prioritizing safety can look like prioritizing efficiency or being biased
      - incorrect rules applied to other data like other biased rules producing strong bias functions rather than correction functions
      - structures that incentivize bias like lack of update rules or adjacent destruction of correction rules or lack of protection for connections preventing bias
      - mismatch between time to required decision and time to make change or query for new rules
      - concepts:
        - stereotype rule dynamics (how they form, are destroyed/changed/replaced)
        - scope/specificity/generalization
        - stupidity which is related to bias
        - reasons not to look for contradictory/other information types
      - misidentify output (wearing a similar outfit to criminal for different reason than crime) or cyclical output (wearing similar thing as an activist given culture progression who is known to be against the police can increase potential for police violence) as a cause
      - misassigning the concept of responsibility (the source of root cause), such as misidentifying the aggressor/victim as being at fault
      - misassigning the burden of the prevention function (burden is on the aggressor), according to power dynamics (the aggressor has the information about their own violent intents, the power to check information & the responsibility to use it)
      - structural gaps allowing bias to gather/develop (gaps in rule enforcement/validation checks or over-prioritizing priorities known to create bias if not restricted, like efficiency/safety, or processes that have the same output like fear/caution/scanning for threats)
      - structural components that form the inputs or components of bias, making it likelier to develop if those components are allowed to interact
      - structural components that make bias more likely, like frequently requiring fast survival decisions


## voting for algorithm of resource distribution

    - example algorithm input/output: 
      - 'this algorithm will guarantee these resources at x intervals if you do y work, and provide these possible resource streams if z work is available & a technology is accessible'

    - example algorithm:
      - 'invest in diverse research projects to fulfill basic intents first (researching number of evolution steps you can skip when gene editing before you create toxic plants, researching whether plants should be optimized or a composite plant should be designed with multiple intents like reducing water intake & maximizing oxygen output'
      - 'invest in solutions so that cost/reward ratio of solutions is kept above ratio x to avoid exploits unless the system is closed and optimized, and below ratio y to avoid high-cost solutions unless theyre abstract & fulfill basic intents'

    - the number of people required to participate to get to that resource distribution could be a filter, so only realistic algorithm participation (algorithms that people are likely to vote for) is an option
    - sets of realistic algorithms (each algorithm set covering income for a population) make it to the voting ballet, and the set that is voting for the most above the realistic threshold is the default winner
    - iterations of voting can apply subsequent filters of resulting algorithm sets like optional/potential work agreed to, and allow debate and lobbying to get higher rates by promoting or educating people for different types of work (they can vote for an algorithm requiring more investment if they have x signatures & if the work will produce solutions to reduce costs overall for all or other groups, otherwise an income rate isnt guaranteed)
    - monitoring, fixing, & enforcement agencies can be used as necessary (when new drugs are discovered, systemic failures occur, etc)


## question algorithms

  - question algorithm: use question patterns (usable in insight paths) to skip ahead by asking questions expected given a conclusion
      - example: once a type label is somewhat clear but still not completely determined, whats a question normally asked that will find the right deciding factors to determine the answer without further questions?
        - asking the right question will let you 'skip ahead' in weight training after asking the question & finding/applying the answer to update the weights
        - an example is asking the question 'is it a hybrid' once you arrive at a conclusion like 'this data point has features from two different types' (a reducing question rather than a completing question)

  - add diagram for question derivation for service list to automate chat bot entries

    - deriving the questions customers will ask for a set of services

      - which processes are complicated or not optimized (need to be in person for certain transactions that people would rather do online)
      - which processes involve changing information (account balance, transaction approval)
      - which processes are likely to have errors (auth)
      - which processes people will likely be interested in using the most

    - what percent of changes are just from finding efficiencies, using those as a foundation for common distortion types (random change, directed change, connecting change, etc)

    - what system of core objects/functions/attributes generate a space where:
      - circles & squares are fundamental or standard objects
      - there is a continuous spectrum of values (real numbers) around which alternate number types rotate (complex numbers, etc)
      - comparing change generated by two variables (one independent value function determining the dependent value) has patterns of measurement potential
      - isolating by attributes (like isolating direction & scale to transform to a vector space) or framing information in different structures (sets, matrixes, sequences) allows patterns that are calculatable (implying the framing filter is determining, so matrix attributes can be determined by its definition)

    - what objects describe lack of information like ambiguities or lost information, other than randomness (difficult to identify randomness), variance (lack of patterns), and infinities (lack of information being difficulty of computing the sequence except in terms of other infinities if it doesnt converge, or lack of guarantees that the sequence can be maintained/stabilized to continue)

  - question-answering algorithm

    - example: 'what is the definition of this word', 'what is the best route to destination', 'why does evolution occur', 'how to implement an intent with this tool'

      - question is the set of nodes that should be connected

      - answer is the path between nodes
        - set of steps to take
        - set of possible alternate equivalent routes
        - network query if other networks are needed
        - ordered combination of words & sub-definitions to form a definition

        - the answer can have various forms:
          - moving in the question intent direction 
          - approaching an interim answer to the final destination node or answering a sub-question
          - answering on another node layer (cause) or answering the reason for the question
          - arriving at a non-answer (there is no right answer) or a conclusion of ambiguity or immeasurability
          - taking a sub-optimal route to the final destination node for one metric (accuracy) to fulfill another metric (understandability)
