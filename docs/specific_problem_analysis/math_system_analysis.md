# physical link between 3-d & interface network

  - if there's a physical link to the interface network, its probably activated by or related to symmetry stacks, given that this object keeps appearing in the generated system
    - fractals are a symmetry stack, as the change ratio applies to each successive layer of change
    - this could mean equalizing symmetries (resource distribution) or building a structure that can stack them (independence machine)
    

# questions

  - do concepts like cause emerge logically from a system where interactions (through forces creating adjacence) & time (state changes are persistent & stable till next iteration of time unit) are possible

  - how does a topology of a number's attribute sets (related complex numbers, adjacent functions/relevant formulas, core generating objects, factors, etc) collapse to a number?

  - are there gaps in an incomplete topology depicting numbers, as dictated by physics rules?

  - do components that can be used to build real numbers map to conceptual network (complex numbers), just like structural-conceptual objects have clear mappings, or are complex numbers more like perspective facets of a number? which types of alternate numbers are missing from the type set given combinable attributes?

  - is math causative (rules to describe information of certain types, structures, & potential), or is physics causative of which information can be generated with stability, enabling measurements & therefore allowing it to be described?

    - why are there numerical symmetries that attract information & values? this implies they're true or continue to be true, if they keep generating information

    - which physics filters (forces, particles, states) beyond the set of constants could generate the math system?

    - if the math system is causative, is it also changeable (can you generate different physics rules by changing it)

    - make a number type/attribute layer graph to fill in gaps in combinations

    - if physics generates information like fuel exhaust, and math can decrypt the exhaust into useful patterns, and those patterns have potential to predict & control physics movements, the math system could have power over the physics system

    - can the math system capture every different type of distance, time, value, etc that the physics system can generate? if one lags behind the other that implies some level of causation

      - what definitions of distance, position, value are missing in the network of definitions

    - do physics concepts like cause, time, & mutually exclusive alternatives (true/false, of an attribute captured in a superposition) leak into the math system as sequence/adjacence, position/change, or valid/invalid for that space

      - these can be classified as physics concepts bc logic has rules/physics describing it, and is a unit interface of other change interfaces

      - the physics/rule sets of a system describe its potential to generate information - the physics of math determine what information is describable in the set of system-generated information
      
      - example with the math system:
        - the physics/rule sets of the math system can generate information
        - the physics of math determine what information is describable in the set of math-generatable information (generated information is input to generation functions)
        - there's a logical loop between generatable information rules & describable information rules

      - explore physics of math determining spaces (sets that can be coverted, spaces that are possible) that evolve around symmetries/interfaces enabling those spaces

      - what other outer layers of spaces can be useful/generated with known operations on known spaces?

      - what system would need to be kept in place by these universal constants (which can be inputs or interim tools used to connect/stabilize the other elements of the math space)

      - apply system analysis to math

        - try swapping value definitions (quantity, position, scale, direction) in different spaces to test for symmetries & patterns that can be used as predictors

        - look for system patterns (attribute alignments, etc) and objects (efficiencies, incentives) such as where something is more calculatable

        - intents of function types:

          - wave functions to model a spectrum of stable alts

          - log functions to model a boolean decision

            - if a log function represents each fork and the aggregate output function amounts to the decision tree, are composite log functions reflective of common patterns in aggregated decision tree output functions
            - or the other way around, so a decision tree can usually be modeled with a logistic function once the variables are formatted the right way, like in sets of attribute sets/paths for 0/1 values

          - decision trees 
            - are a good frame for symmetry stacks
            - need dead-end nodes that represent unanswerable decisions (where information doesnt contain minimum required features, is unclear/corrupted, or points to all possible categories with an ambiguous level of equivalence)

          - support vector machine
            - can be applied to causes on various causal layers, after clustering them to differentiate them 
            - similar to the problem of finding a symmetry, also finding an average

          - clustering
            - in addition to centroids/SVMs, you can infer the inherited group data set, & the core functions to get to the original data set groups, and check for variables used in/output by those core functions

          - combining these methods could be used to allocate methods by complexity & type 

            - start with decision trees, use clustering to differentiate between remaining type group features (attribute sets), use inherited type inference & core function derivation to model any remaining features

            - start on the data layer, navigate up to infer types, trace causal functions connecting them as well as output branches that dont become types

            - start with the core objects, combination types, limits, layers, & filters that are most powerful in explaining interactions & design a custom algorithm based on those or other common interaction objects

          - use new data & change patterns to improve the model prediction method - if two clusters tend to evolve sideways (forming a square) and then converge to other corners, that can be used to improve the predictor early on with a small amount of new data once you know what direction the changes are headed in, vs. waiting for a lot of new data to confirm its not just noise

          - noise patterns can be used to reduce data variance to probable isolatable relationships

          - specify a definition of range of degrees of dependence that is classifiable as 'independent' from another variable

          - use probability in selecting assumptions - how often are variables independent (in two separate sub-systems that are isolated & only interact indirectly up to n degrees of interactions away)

          - is there a better standard assumption set that describes more relationships between variables that create complex relationships justifying analysis?

          - is there a variable type that clarifies relationships, which can be applied before gathering data, or which can be removed to generate complexity?

          - univariate polynomials to model isolated motion (same independent variable applied to itself rather than just a constant like linear functions)

            - when does isolated motion (parabola like x^2) look like group motion (normal distribution), and what does this mean?

              - if the motion of the x variable looks like the group motion, either the x variable has the group's information as an input, or its a false similarity, or the motion of an isolated variable can mimic the group motion under certain circumstances, like when physics applies (physics of an isolated variable like throwing a ball up in a system with gravity, and the physics of group decisions differing from a standard averaage decision, where gravity/forces and lack of restrictions both exert a tempering influence on the motion as with a lack of restrictions, trends tend to gravitate toward the average with minor deviations from it in either direction)


    - algorithms

      - when sorting or finding, use vertices & probability to split the list to traverse - a list of length 20 can have vertices at 3 - 5 different points where the value is likely to be within one traversal with x% likelihood - this is related to minimum information to solve

        - 'minimum information to detect pattern' is another key metric - how many nodes do you need to check in a list of length n before you can be reasonably certain if there's a function generating it or if that function applies within subsets or other structures?

