Python rules

	- methods like insert, remove or sort that only modify the list have no return value printed – they return the default None. This is a design principle for all mutable data structures in Python
	
	- procedure: function with no return value (in python returns None even without a return value)

	- function:

		- docstring: first, string literal statement of the function body
		- execution: introduces a new symbol table used for the local variables of the function
			- when a function calls another function, a new local symbol table is created for that call
		- variable assignments in a function store the value in the local symbol table
		- variable references look in the local table, then local tables of enclosing functions, then the global symbol table, then the built-in names table

		- global: 
			- global & enclosing function variables cannot be directly assigned a value within a function 
			- unless a global var is named in a global statement, or an enclosing function var named in a nonlocal statement

		- parameters are passed using call by value (where the value is always an object reference, not the value of the object)

		- optional args have default values to the right of the required args
		- non-keyword arguments cant be passed after a keyword/named argument
		- arg order doesnt matter even with required args
		- defaults are executed once in consecutive calls, unless assigned a value of None as default
		- *args is a tuple of the positional optional arguments after the required argument or can store any arguments after the preceding formal parameters
		- **keywords generates a dict of the keyword/named arguments, in the order they were passed
		- *args must be before **keywords
		- named args cant be keys in **keywords dict without positional delimiter /
		- def f(positional, /, positional_or_keyword, *, keyword): # separates positional args, positional & keyword args, and keyword args for clarity

		- pass sequence of positional arguments to a function: range(*args)
		- pass dict of keyword arguments to a function: print_dict(**d)

		- function annotations about types:
			def f(var1: str, var2: str = 'default') -> str:

	- operators

		- integer numbers (e.g. 2, 4, 20) have type int, the ones with a fractional part (e.g. 5.0, 1.6) have type float
		- operators with mixed type operands convert the integer operand to floating point
		- division always returns a floating point number: 17 / 3 = 5.6667
			- floor division to get an integer result: 17 // 3 = 5
			- calculate the remainder: 17 % 3 = 2
		- exponents: x ** 2 is x squared

		- while and if statements can contain any operators, not just comparisons

		- numerical operators: <, >, <=, >=

			- comparison operators: is (check if same object), in (check if value in sequence)

				- boolean operators: and, or, not 
					- 'not' has the highest priority and 'or' the lowest: A and not B or C == (A and (not B)) or C

				- 'and' and 'or' are short-circuit operators, whose evaluation stops as soon as the outcome is determined
					- if A and C are true but B is false, A and B and C does not evaluate the expression C

		- assignment inside expressions must be done with the walrus operator :=

		- sequences are compared based on left to right ordering, one pair at a time:
			- [1, 3] is less than [1, 4]
			- [1, 3] is less than [1, 3, -1] because of the length

	- control flow

		- break: breaks out of the innermost enclosing for or while loop

			- loop statements may have an else clause executed when the loop terminates through exhaustion of the iterable (with for) or when the condition becomes false (with while), but not when the loop is terminated by a break statement

			- a try statement’s else clause runs when no exception occurs, and a loop’s else clause runs when no break occurs

		- continue: continues with the next iteration of the loop

		- pass: does nothing; can be used when a statement is required syntactically but the program requires no action

	- functions

		- lambda: creates small anonymous functions, where params are comma-separated and logic is after the colon, used to return functions or pass in functions or use functions in expressions
			lambda params: logic
			- pairs.sort(key=lambda pair: pair[1])

		- list comprehensions
			list(map(lambda x: x+1, range(10))) == [x+1 for x in range(10)]

		- zip
			- zip() in conjunction with the * operator can be used to unzip a list

			# transpose a matrix
			matrix = [[1, 2, 3], [5, 6, 7]]
			list(zip(*matrix)) = [(1,5), (2,6), (3,7)]

			questions = ['name', 'quest', 'favorite color']
			answers = ['lancelot', 'the holy grail', 'blue']
			for q, a in zip(questions, answers):
				print('What is your {0}?  It is {1}.'.format(q, a))

		- del
			del a[0] # remove first item
			del a[2:4] # remove slice
			del a[:] # empty list a = []
			del a # remove variable

		- range(start, end, increment)

		- run module as a script: if __name__ == "__main__":

		- time a function execution:
			from timeit import Timer
			Timer('expr1', 'expr2').timeit()

		- test a function: include sample call & output in interactive mode in docstring
			"""
			>>> print(average([20, 30, 70]))
		    40.0
		    """

		  test embedded tests in docstrings with doctest.testmod
		  	import doctest
			doctest.testmod()

		class TestStatisticalFunctions(unittest.TestCase):

		    def test_average(self):
				self.assertEqual(expr1, output)
		        with self.assertRaises(ZeroDivisionError):
		            average([])
				with self.assertRaises(TypeError):
		            average(20, 30, 70) # should be list passed in

		  import unittest
		  unittest.main()

	- repr(object): generates str representation of an object or one that would generate the same object if passed to eval()

	- random.randrange(start, stop, step), random.random() for random float between 0 inclusive and 1, choice(sequence) for random item in sequence

	- round(float, number_digits)

	- call function by name:

		import module_name
		result = getattr(module_name, 'function_name')()


	- exceptions

		try:
			x = int(input("enter a number: "))
			f = open(filename, 'r')
			# with open("myfile.txt") as f: # has pre-defined clean up action that automatically closes file using with keyword
			break
		except ValueError:
			print("not a number")
		except OSError as err:
			print("OS error: {0}".format(err))
			print(err.args) # same as print(err)
		except (RuntimeError, TypeError, NameError):
			raise CustomGroupException
			# raise throws a specific exception
		else:
			print('if no exception raised, execute this')
			print('successful computation result', result)
			f.close()
		finally:
			print('necessary code to clean up resources, regardless of if an exception is thrown')
			# finally executed right before try (break, continue, or return)
			# (finally return) returned instead of (try return)
			# re-raised after this:
			# 	unhandled try exceptions
			# 	except/else exceptions

	- threads
		- threading allow simultaneous execution of tasks that are not sequentially dependent, like running I/O in parallel with computations in another thread
		- synchronization primitives (locks, events, condition variables, and semaphores) help coordinate sharing data/resources across threads
		- best practices for task coordination involve concentrating access to a resource in one thread & use queue module to feed that thread with requests from other threads
		- threads are cheaper than processes
		- kernel threads are part of the os, as opposed to user space threads
		- locks let you synchronize threads, specifying whether they should be blocking or not

	- memory management
		- python does automatic memory management:
			- reference counting for most objects
			- garbage collection to eliminate cycles
		- memory is freed shortly after the last reference to it has been eliminated
		- to track objects only during usage, use weakref to avoid creating a reference, so its automatically removed from the weakref table & calls the weakref callback (ie, for caching)

	- accessing web resources

		from urllib.request import urlopen
		with urlopen(url) as response:
			for line in response:

	- check variable

		- var1 is var2:
			if var1 points to the same object as var2

		- exists: 
			if 'var1' in locals() or 'var1' in globals():

		- is none:
			if var1 is None:

		- is true: 
			if var1 is True:

		- is not false or 0: 
			if var1:

		- object has an attribute: 
			if hasattr(obj, 'attr1'):
			'attr1' in dir(obj)	

		- if dict has key: 
			if 'key1' in dict1:

		- isalnum(), isdigit(), isnumeric(), isdecimal()

		- wrap a var expr in a try throws NameError if var does not exist:
			try:
			    myVar
			except NameError:
			    myVar = None

	- index gives first occurrence position by default

	- list.insert(index, obj) 

	- list.remove(obj)

	- cmp(list1, list2) to compare two lists

- data structures

	- primitives
		- float
		- int
		- str
			- raw string (no special chars like \n): r'C:\some\name'
			- slice: 
				- 'test'[0:2] = 'te' # the end char at position 2 is excluded
				- s[:i] + s[i:] == s
				- an omitted start index defaults to zero, an omitted end index defaults to the size of the string
				- negative indexes start at -1

			- format:
				
				- format(input, format)
					format(math.pi, '.2f') = '3.14'

				- "%(var1)s" % dict
					'%(var1)s' % {'var1': "Python"} # s specifies string type

				- str.format(expr_or_vars)
					"{int1}".format(expression_for_n)
					"{0}".format(1+2)
					'{0} = {1}'.format(var1, var2)

				- f""
					f"{var1!expression_key}" == f"{expression(var1)}"
					f"{var1!r}." == f"{repr(var1)}."

				- f"{expr}"
					f"{var1:{type_arg1}.{type_arg2}}" 
					var1 = decimal.Decimal("12.34567")
					f" {var1:{width}.{precision}}" # nested field example

				- f"{var1:format}"
					var1 = 1024
					f"{var1:#0x}" = '0x400' # uses format specifier
		- bool

	- sequence data type: list, tuple, range, str

	- tuple (immutable) = ()

		- tuple1 = 1,

		- can contain mutable objects like lists

		- accessed with sequence unpacking rather than iterating
			t = 1, 2, 3
			x, y, z = t # fetches 1, 2, and 3 values from t in order

			- any sequence supports this but item count must be equivalent

		- support nesting
			t = 1, 2, 'a'
			u = t, (1, 2, 3, 4, 5) = ((1, 2, 'a!'), (1, 2, 3, 4, 5))

	- set (unique) = set()
		- set1 = {1}
		- unordered collection with no duplicates
			a - b                              # letters in a but not in b
			a | b                              # letters in a or b or both
			a & b                              # letters in both a and b
			a ^ b                              # letters in a or b but not both

		- supports set comprehensions

		- ordered set
		- frozen set

	- array = arr.array()
		- requires same data type, for memory improvements
		- array of numbers stored as two byte unsigned binary numbers (typecode "H") vs. the usual 16 bytes per entry for lists of ints:
			a = array('H', [4000, 10, 700, 22222])

	- list = []
		- lists are order-retaining sequences
		- represented internally as an array
		- the largest costs come from growing beyond the current allocation size (because everything must move), or from inserting/deleting near the beginning (because everything after that must move)
		- good for stacks using append/pop to modify the last item, but not for queues bc inserting/appending/removing at the first position requires a shift of all the others
		- deque is good for queues where the both ends will be changed or when first item needs to be changed (from collections import deque, queue = deque(some_list), queue.append, queue.popleft)
		- concatenation: result.append(a) = result = result + [a]
		- use for random access & stacks
		- optimized for fast fixed-length operations
		- incur O(n) memory movement costs for pop(0) & insert(0, v) operations which change size & position

	- collections.deque() list-like object:
		- deques support thread-safe, memory efficient appends & pops from either side of the deque with about the same O(1) performance in either direction
		- represented internally as a doubly linked list
		- faster appends & pops from left end than lists (O(1) for deques, O(n) for lists), slower lookups in the middle (O(n) for deques, O(k) for lists)
		- use for queues & breadth first tree searches
		- indexed access is O(1) at both ends but slows to O(n) in the middle

			listvar = deque([starting_node])

			def breadth_first_search(sequence):
			    node = sequence.popleft()
			    for m in node:
			    	if m == target_val:
			        	return m
			    listvar.append(node)

	- heap:
		- priority queue
		- heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property
			- in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C
			- in a min heap, the key of P is less than or equal to the key of C

		- heapq module provides functions for implementing heaps based on regular lists. The lowest valued entry is always kept at position zero for frequent access of smallest value

	- bisect has tools for manipulating sorted lists, like insertions that maintain sort

	- complexity

		- list:
			- O(1)
				get/set/append/pop last/len
			- O(k)
				get slice/pop intermediate/extend 
			- O(n)
				insert/delete/iterate/delete slice/in/min/max
			- O(k + n)
				set slice
			- O(n log n)
				sort
			- O(nk)
				multiply

		- deque:
			- O(1)
				- append, appendleft, pop, popleft
			- O(k)
				- extend, extendleft, rotate
			- O(n)
				- remove, copy

		- dict:
			- O(1)
				- get, set, delte
			- O(n)
				- iterate, copy

		- set:
			- O(1)
				- in
			- O(added len of both sets)
				- union
			- O(lower len of sets)
				- intersection

		- visual:
			- strange similarities/differences should be grouped together in a table where possible
			- an expectation table would show only surprising similarities/differences in performance for structures expected to be similar/different given position

	- dict = {}
		- dict class can now remember insertion order
		- keys can be any immutable type (strings, numbers, or tuples with no mutable objects)
		- list(d) returns a list of keys used, in insertion order
		- dict([('k1', 1), ('k2', 2), ('k3', 3)]) = dict(k1=1, k2=2, k3=3)
		- supports dict comprehensions

	- linked list
	
	- hash map

	- stack (LIFO)
		- list type makes a decent stack data structure as it supports push & pop operations in amortized O(1) time, and random access is O(1)
			- given that theyre based on arrays, they occasionally need to resize the storage space with append/insert/deletes
		- linked list has more stable O(1) inserts/deletes

	- queue (FIFO)
		- collections.deque offers optimized end insert/append/delete

	- non linear:
		- tree
		- graph

	- ideas:

		- mixed data structures: a mix of constants, generative functions, and shapes to store data in a way that allocates attributes as needed (flexibility where it's changed, quick retrieval where it's needed)
		
		- computing metadata like the ordered form of the sequence, storing successive value differences between adjacent pairs as a quicker way to find maxima/minima in a data set requiring comparisons, the optimal split points for approved intents, or the sequence statistics at insertion time (local/global average, probability distribution)
		
		- storing common sequences & only storing the deviations from a common sequence with the common sequence id, or the composition of sequences required to generate it, so the position of a value can be computed from composing the metadata of the other sequences, or executing the generation process & computing it when needed

			- example: finding the maximum value can be as simple as storing the maximum or average value of a subset, and then storing other values in that subset as versions of it (m, m + 1, m - 3) where highest distortion is also stored as subset metadata, so finding the highest global value is a subset query of metadata like the value of m in each subset and the value of the greatest distortion - for applications where more than just the maximum are needed, otherwise the maximum can be stored at insertion time

		- storing frequently requested values at vertices that are stored in metadata (first/last or first value in subsets created with search/sort/split operations)
		
		- storing the sequence in a way that maximizes the chance of finding unique values (creating subsets where variation in subset values is maximized, subsets of equal values are clustered & attributes are attached, or a repeated value has a count attribute)
		
		- organizing the network of values given their attributes as paths, so all the values with a 'repeated' attribute would be in a similar network location
			- storing multiple attribute networks given relevant attributes (type attribute is used when retrieving data, unique/repeated attribute is used when finding one value existence/position or sorting)
		
			- similarly, storing frequently requested sets of data in a network path (first requested data stored adjacent to origin, second requested data usually requested after that first request stored in the next node on the next layer)

		- custom data structure optimized for a data set

		- storing probability distributions of averages, sets, inflection points, functions, & transformation functions & using those to select storage methods, as an alternative to storing data with a constant method
		
		- storing traversal patterns and skipping steps needed based on request success (if the first operation output is the end of the request sequence, its considered successful, otherwise extra operations would be stored as a traversal pattern with the final operation output considered the successful value given the first request)

		- embedding metadata in data structure (storing the count of maximums as the number of tree layers, storing each maximum at first position checked in breadth-first search if maximum is a target metric)

		- identifying sub-optimal paths for a sequence and adding attributes to replace traversals/calculations (sub-optimal step sequences given a set of values, where losses compound or gains arent guaranteed)
			- example: in a sequence with subsets of length 5, traversing more than 3 nodes from a direction without finding a local maximum is a sub-optimal step sequence - so subsets in the sequence matching this sub-optimal path can be marked with a recommended traversal method ('skip') or supported intents ('looking for a minimum') at the start/finish, and traversal can check the positions with these attributes first


- programming

	- stochastic programming: improvise a policy to maximize expected value, based on available data & probability distributions

	- dynamic programming: breaking a problem solved over multiple periods into sub problems, which are solvable given state & optimization function
		example: dijkstra algorthm for shortest path involves the assumption that knowing distance from origin to interim node is a pre-condition for knowing distance from origin to target node

	- stochastic dynamic programming: compute a policy to maximize expected value, assuming randomness


- algorithms

	- traversals

	- divide & conquer

	- breadth-first search (layer search)
		- queue (fifo)
		- checks whether a vertex has been discovered before enqueueing the vertex
		- good for shortest path analysis & analyzing node relationships

	- depth-first search (path search)
		- stack (lifo)
		- delays checking whether a vertex has been discovered until the vertex is popped from the stack
	
	- operation: 
		- runtimes
		- memory use
		- limitations, worst/best case scenarios, & trade-offs
		- example usage strategies 
		- advantages vs. other methods

	- ideas:

		- algorithm for testing which algorithm is the best traversal method

			- each algorithm is best in a certain variety of cases, which may have overlaps - determining if the sequence matches a standard case of each type or is within the range optimized by those cases is a trivial computation that can be done before traversal of the entire sequence

				- minimizing cost with distortions from a standard traversal, at traversal time - given knowledge of sequence like randomness, ranges, patterns, or subsets: 
					'starting from a standard best traversal method, evaluating metrics indicating more optimal methods, and pivoting to those methods'

				- find worst-case scenario for a particular algorithm and check if that scenario or adjacent states apply

					- example: a worst-case scenario for a greedy algorithm is one where the function is a wave with many peaks and you start at a minimum point in a cluster of the lowest peaks

				- alternatively, find rules that reduce the possible scenarios the most and apply those first (its faster to rule out an ordered sequence than it is to check for randomness)

			- determing whether existing algorithms optimize search for a particular sequence or if a modification of an algorithm would improve the search time for a sequence or sequence type may also be trivial, such as testing for randomness in a sequence

			- optimizing algorithms for known/computable error types

				- example: for a greedy algorithm, it would ideally check values after an alternate is determined to be the lower value, to prevent a false branch choice
					- if exponential growth is common, it would check values n-nodes away to account for slow growth pattern
					- if many peaks are common, it would check values within common width ranges away to account for that pattern

				- how to compute this error type for the greedy algorithm:
					- assumption as a constant priority: adjacent values are prioritized as indicators of global values
					- assumption implication: the values beyond an adjacent value continue the trend (of increasing or decreasing)
					- unenforced rule: global values arent required to match adjacent values or the pattern implied by adjacent values (a lowering sequence of values)

		- storing low-cost computations with a particular algorithm & seeing if a sequence or problem can be framed as a direct combination/transform of those low-cost computations

			- example: 

				- for bfs, storing the concept of node layer & checking the value of adjacent layer nodes is low-cost, so using that is making a bet that the value is within n layers of your origin below the cost threshold

			- storing different concepts (position subsets, value pairs, subset split values, value layer, completion rate, sequence metadata) as the default/base object rather than the explicit objects stored (position, value, count, sequence) may be more efficient for certain intents & requests


- oop: 
	
	- design patterns

	- best practices

	- ideas:

		- mixed

			- events/use cases/workflows/intents as objects or code update processes ('identify a new important object to users, like a search filter type') with a goal of reducing user needs as they use the application more
				- if users' goal is to retrieve data to filter, the application should be training a prediction model (on the best filters to use, common queries & query optimization recommendations, on the data patterns & change patterns) they can use to opt-out of some queries, and should be regularly checking for alternate data sources in permitted data sets

			- converting attributes to functions & generating attributes from functions or vice versa so objects are just a set of functions or attributes

			- when is it best to use nodes/objects, connections/functions, object attribute sets, metadata, queries, structures, processes, & values - and combinations or components of those - as a base object?
			
		- queries
			- application designed as a set of code queries

		- filters
			- application designed as a set of filters

		- metadata

			- intent: matching intents of functions with user requests in a system organized by intent, so that intents leave info traces in the form of metadata or usage that can be checked & used to grant permission
				- dev intents as composable functions (like 'define an event-handling function') with a goal of reducing code written by pre-computing all objects (task, task list) & functions needed to code various business applications ('optimize task sequence', 'train ai model to predict time needed for a task') for a particular application intent ('build a task-scheduler app with access to calendar')

				- granular functions like split or non-standard API calls shouldnt be manually-coded or standard, but automatically selected/generated (given high-level application intents & application usage) - so no code is written that isnt used for an approved intent


- database: database concepts, tradeoffs between data storage methods, dynamoDB

	- acid

		- atomicity: each tx fails or succeeds completely, vs. some steps in the tx succeeding, even with loss of power, process interruption, etc
		- consistency: each tx is a link between valid states
		- isolation: concurrent tx have an equal impact as sequential tx
		- durability: completed tx are in non-volatile memory (retained if system crashes)

	- transactions:
		- support start, lock object, execution, atomic commit or roll back of operation(s) in the transaction object
		- other transaction types or applications: distributed transactions across multiple network nodes, multi-layer/nested transactions, object transactions, file system transactions

	- types:
		- relational
		- key-value
		- graph
		- orm

	- ideas:

		- process-based application structure, rather than event/code/data-based application, so that processes' context (input-output pairs) can be re-used & passed into or used to build functions (given function metadata stored) to reduce process repetition
			- if an initial process just queried a table and the output of the query is still cached & includes the very reduced set of fields from a second query, use that process i/o to build a function at runtime (query cached version of the table subset rather than query the table)

		- multi-variable key-value store indexes
			- store commonly requested variables as keys with at least one identifying variable, sorted by most requested first

		- deriving core functions/symmetries/variables of data & storing combination of those or combination of averages/other metadata rather than the data itself
			- example: storing 'cluster 1 origin with distortion function 3' rather than a record for records that are not commonly requested

		- graph database as sequence storage structure (with values on a map and the sequence represented as a query)

		- alternate structures for databases like trees/filters/functions

		- databases with intent (store more sensitive data in separate tables/servers that require more access credentials)

		- storing minimal data
			- if there is a center around which data rotates or a sequence/function fulfilled by data, store all records except the missing one like one-hot encoding, which can have a placeholder to indicate it should be computed as well
			- storing proxy variables determinable from other variables (not storing codes that are mappable like city & state or determinable like the output of a function & the function)


- resource optimization

	- service-oriented architectures
	- map-reduce
	- distributed caching
	- load balancing

	- ideas:

		- tokenization
			- creating a map of identifiers and common value sets to reduce sending/querying time

		- server function optimization
			- optimizing servers for certain functions
			- optimizing server locations for common function paths

		- integrating info derivation methods with info storage methods through data structures & algorithms
			- assessing whether something needs to be re-computed or retrieved at request time
			- predicting requests by request patterns and known user need/intent structures like trees or paths

- os 
	- memory management
	- processes
	- threads
	- synchronization
	- paging
	- multithreading

- web
	- DNS lookups
	- TCP/IP
	- socket connections

- ml
	- data-driven modeling

	- train/test protocols
		- add missing data to training set
		- train & test sets should have no overlap
		- 80/20 is a normal split
		- validation set is from training data
		- add cross-validation to split the data set in multiple ways 
		
	- error analysis

	- statistical significance
		- problem-solution: data sources, annotation, modeling approaches, & pitfalls

		- reduce overfitting

			- feature selection
				- pca: reduce variables to linear components so linear combinations can be used to represent variable sets
				- regularization + ANOVA
				- random forest (nodes as feature conditions so similar outputs are in the same set, where feature importance is based on tree entropy reduction)

		- ROC curve
			- plots how a binary classifier's ability changes based on its threshold value
			- plots the true positive rate (true's predicted correctly) against the false positive rate at various thresholds

		- correlation matrix to isolate variables with low correlation for independence

		- confusion matrix to display precision & recall of various values, comparing prediction & true value counts

		- accuracy (total correct prediction ratio)
		- AUC (ability to distinguish +/- observations)
		- precision (confidence in predicting a value): relevant/retrieved
		- recall (values predicted correctly): retrieved/total relevant
		- F1-score (measures test accuracy) = combine precision & recall with a harmonic mean (rate average) = 2 * (precision * recall) / (precision + recall)

		- lime explainer to show factors for a given prediction

	- basic AI/ML methods & algorithms
		- dense network
		- convulational network
		- reinforcement learning

		- random forest
			- ensemble learning method to weight decision tree output

		- xgboost
			- similar to a Random Forest with the difference that every tree is fitted on the error of the previous one
