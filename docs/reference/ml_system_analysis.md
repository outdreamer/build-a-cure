
  - neural network nodes as facets of additional complexity/dimensions - calculating the complexity of a prediction function-determining problem (based on which distortions are likely from prediction function patterns) would allow selection of the necessary nodes
    - higher complexity requires that more combinations of weight path patterns be experimented on, and more nodes allow room for more experimentation
    - auto-configuring the network with common or data pattern-specific weight path patterns rather than random or equal could speed up training
    - reverse logic can be beneficial here - given the complexity of a problem, which features with which differences would have to occur for the training to be useful (or for the problem to be solvable)? check the data for those variable types/differences
    - use prior knowledge of patterns (insights like 'differentiating variables tend to cluster') as a way to organize analysis of the corresponding feature data pointed to by the insight (adjacent features are passed in to higher weighted node sets or node sets equipped to handle subtler differences)
    - different network or node sets can represent different problem-solving automation insights (one node can represent a 'differentiating variables' filter, another node can represent a 'cluster' verb/function filter)
      - if none of these node sets finds a pattern, the node sets can be recombined into new options (like another outer layer of a core function diagram) and re-trained
    - adjacent networks are created for stacked variables (like symmetry stacks, where features differ on a symmetry and symmetries are layered (like the hand-limb-spine symmetry stack)
    - phase shift points are identified first & the data is standardized around them - so when one type has extreme/compounding attribute values emerging as another type, that threshold is built in to the data (data near to that threshold is transformed to be higher to differentiate the types, or you add a third output category like 'transforming' or 'interim type')
    - possible causal shapes are identified first & the network is organized to fit them
      - for example, weight paths trained to highlight one attribute set are applied to other alternative equivalent attribute sets (an alternate causal route to the output)

    - what is the ratio of coverage of all possible feature interactions that is fulfilled by a standard neural network? what does the standard architecture reward in terms of clustering attribute sets for experimentation?
      - it rewards attribute sets that appear correlated in the data, with generalizations applied

    - should you use position as a determinant of feature importance? should position be removed from the data & another network trained on position-removed data?

    - would it be better to frame features in terms of system analysis (attributes, functions & routes)
      - an ear definition route can be framed as:
        - take other dimensions of change (than vision or taste) & assume methods to detect them (sound), then design a system (ear) based on efficiency as a priority to detect that change
        - stack a symmetry on top of the spine to make calculations & choose priorities, then stack another symmetry (face) on that symmetry (head) to host multiple change-detecting methods (facial features) to guide calculations
      - once you frame features in this way, training to find prediction functions should be trivial

    - can you partially reset the neural network mid-training to help improve generalization in addition to existing methods, so it doesnt tend too far in the data-dependence direction?
      - identify features that are likely to be data-specific, given whether the categories share that feature with different attribute values

    - training prediction functions on system data rather than data for objects within the system would add other gains
    ` - by knowing the structure of a system, you can infer insights like:
        - which object shapes are most compatible with the system
        - which forces/interactions are likely to evolve in the system
        - system metadata (stability, priorities, potential)
        - system info objects (assumptions, inputs, efficiencies, incentives, phase shifts, ambiguities, core functions, boundaries)
      - for the dog vs. cat classification, training on causal system data (the evolution system, the bio system, or the DNA system) might be a better target for the prediction function than training on images of the outputs of those systems
      - this analysis would help predict ambiguities (dogs & cats will have very similar features sometimes given how DNA & evolution works) and how to differentiate them (check for specific attribute sets in data, otherwise indicate that data is insufficient but here's the data you should gather for this problem type)
      - you might also be able to identify a sub-system that has the most valuable data for this prediction (mutation sub-system as a sub-system with a distortion function applied to the DNA system)


  - prediction model trained on conversations as encryption key/alg parameters, updated with new messages

  - how to check if a data set is similar to one that has already been trained, to avoid re-training to save CPU

    - store metadata about the data set like shape (groups/clusters, linear, random) and the metadata for those shapes (radius & overlap for clusters, distortion patterns & outliers for linear, starting point for random)

    - derive info metadata like type, cause, change patterns & check if determining/generative structures (core functions, symmetries, false similarities) match across the two data sets

    - identify patterns of variation once a similarity has been found, to avoid checking the whole data set
      - example: once you identify that both data sets have two output categories, what are the patterns of difference in the internal points of those categories (patterns in overlaps, misidentifications, corrupt/incomplete data, differentiating variables) - usually youre applying a categorization model to two categories that are very different (so its important to identify them correctly) but have some illusory similarities or features in common, making the categorization task non-trivial - so you'd look for patterns of differentiation within categories, to check if the data sets are approximately equivalent so you can use the same model without training - either specifically, storing patterns of differentiation for specific categories, in the model metadata, or generally, for general category differentiation patterns

    - store adjacent functions or specify a parameter range generated from the original function to identify functions that can be generated with accessible transforms or functions that are usually generated for similar variable sets (using common function patterns), to identify similarity in parameter values (using various types of parameters, at various layers on the causal stack, such as preceding functions like the function producing this function as its derivative, alternate functions like the series sum, or descriptive parameters like moment-generating functions)

    - identify function/data vertices, which are determining points like maxima/minima/inflection points as well as the minimum number of points necessary to identify the momentum of the curve, or points that indicate phase shifts in general

    - store semi-trained models and use them as vectors to create a complete trained model, without training

