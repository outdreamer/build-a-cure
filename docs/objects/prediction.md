# Prediction

  - pattern matching/anomaly detection/noise reduction in finding relevant information eliminates information based on:
      - commonness/similarity to patterns (focus on common patterns)
      - similarity to other data (isolate anomalies)
      - reducing equivalence from randomness (isolate non-random processes that follow rules)
      
  - randomness

    - initial automated tests of a data set (to apply an associated architecture/algorithm to) can answer questions like "can a set of variables produce the level of variance needed to contribute information to complete a task (like prediction of an output variable), determined by matching the level of variance in the output (like the most different examples in a category)"
      - additional layers of analysis: 
        - can a distortion of the variables provide the needed variance
        - can an extra variable provide the needed variance
        - can this architecture/algorithm form structures needed to explain (connect) these causal links (describe/derive the causal network) or hold the relevant concepts (like how the structure of 'evolution from a common ancestor' has a corresponding structure in the divergence of cat/dog graph clusters when you add an extreme value of the time variable), given the complexity & variation & change potential involved

    - examine how normally distributed change (extreme change rate at tails manifesting as asymptotes/inflection points, approaching average change rate in middle) produces non-linearity
      - given the sum of independent random variables producing normal distribution (central limit theorem), 
        - as when independent random variables interact in an 'aligned combination' structure going the same or converging direction, 
          - and the compounded aggregate effect is a normal distribution collecting minimal extreme values as most of the random variable sums gravitate around the average in the middle, 
            - given the range of values they tend to have in common that when added produces values in a similar range as the normal distribution's average +/- a standard deviation 
      - example:
        - a relevant example structure is a circular cell with equally distributed openings that many different cell types can pass through, and the probability distribution of cell types is relatively even
          - when these cell types are in the circular cell, they interact with other randomly selected cell types
            - these cell types are all similar in a certain range of certain variables (cell size)
              - this similarity allows them to interact in a particular way, occupying a particular interaction level (colliding in the circular cell)
                - a compounding sum of the cell types would take the form of additive damage (cell surface disruption & smoothing) from collisions, which is spread relatively evenly across cell types, but would be concentrated on a range of any surfaces, and any other cell metadata (spin, velocity, size, position) that tend to be involved in collisions more
                  - interactions of this type tend to attract additional interactions of this type, but additional interactions have decreasing impact on cell surface change, stabilizing at a relatively smooth shape most of the time
        - questions
          - does the shared interaction layer interfere with the definition of independence, or change independence of the variables, or change a different type of independence
            - the original definition is lack of impact, implying that neither is an input/output of the other (such as outputs on a similar layer of a distant root cause in a tree)
          - is the cell surface shape resulting from additive interactions the output variable of these combined random variables (cell types, possibly having different surface shapes, sizes, and other variables) that has or stabilizes at a normal distribution
          - what is the input change trigger that produces an increase in output randomness after this stabilization
          - do similar structures also tend to have similar systems/incentives driving their similarity
            - what are the most dissimilar systems that can produce similar sub-structures?
          - does most change occur in a particular change range
            - there is an incentivized range by the host system, but is there incentivized centrality & stabilization of change
            - does a normal distribution accompany stabilized change around an optimal average value
            - is the compounding of random variables with normalized output ranges an output of systems with normalized value range incentives
            - do independent change ranges in general just happen to gravitate around a change range around an average determined by the difference degree represented by extreme values, or is the root cause that variables in general happen to have averages where most of their values occur so when there is no enforcement of limits applied to inputs, the added variable tends to be average, or that certain interaction types or interactions in general necessarily neutralize randomness of the output/combined variables
            - is it relevant that the distribution of change output ranges in the added variables is random (where each independent variable output range is equally likely)
            - what other variable type(s) could be combined to produce a normal distribution, and is their output added variable distribution driven by similar reasons
            - what would be necessary to keep the added variable distribution random instead of normal
        - how to generate the above questions
          - how to change the system to fulfill different intents
          - how to change the structures of the system without changing the relevant interaction of the system
          - what could also produce the system (other causes)
          - what are other interactions/structures (stabilization of change, change range, similarities such as common overlaps of change ranges) that could be related to the relevant interaction, and in what way
          - what are the assumptions, rules & other requirements

    - example prediction using different bases
      - need-based vs. time-based analysis
        - pick up medicine when you run out vs. picking it up according to a schedule using time-series data
        - randomness is injected in dose & timing of usage, that can interfere with regularity of schedule to pick up 
        - this may not show up in a data set, but if it does, a time-based prediction may not capture the randomness of a pickup schedule where a person varies their dose/timing (accidentally taking two, meaning pickup is a day earlier than expected)
        - the need to pick up is accurately predicted given whether theyve run out (if they have taken all doses)
          - unless there's additional randomness injected in:
            - their schedule around pickup day (they have a distraction preventing pickup when they need to refill their supplies)
            - their method/consistency of testing/determining when supplies are low (using an automated reminder, relying on memory to check, etc)
            - habit consistency/compliance, memory & tendency to form/change habits
        - if you tried to predict based on timing of previous pickups, youd get it right only part of the time, if they make a habit out of accidentally taking more than one, or if they have a routine method they call to make up for a missed dose, or other functions they consistently execute with regard to dose/timing variation
          - unless your algorithm handles the randomness injections present, which can be injected to the prediction function or data set as composing/alternate/base/time-staggered functions/variables allowing these distortions to occur, siphoning randomness into developing functions called consistently to handle that randomness
          - to increase algorithm accuracy, bases should be built-in to the analysis as a variable, and developing functions to handle randomness as its identified (person building a habit to handle dose variation, or switching habits) can be injected as they occur with that metadata (level/type/directness) of randomness
      - randomness capture often involves overlapping developing functions, which "hand-off" randomness to each other, as one develops a more efficient randomness-capture method & handles the stress on previous developing functions, taking over more responsibility according to the variation they can handle
        - as developing randomness-capturing functions overlap & interact, they tend to either stabilize at an efficiency (increasing order through coordination between functions or optimization of a function) or diverge into chaos (compounding randomness)
        - the cycles of developing function overlap, compounding, & other types of interaction can increase the accuracy of absolute time-based prediction functions (as opposed to the more accurate event/need-based prediction function in this case), given the concentration of cause present in local time variables (right after the event determining a need, such as when supplies are checked & found to be low)
      - how can you integrate this structure into an absolute time-based prediction function
        - determine distortion vectors of different bases, developing randomness-capturing functions, & other related components (alternate/composing functions), & the metadata of these vectors (patterns, prediction functions of the distortion vectors, probability distributions related to distortion vectors & relationships between probability distributions)
          - example: a randomly sampled rotating set of distortion vectors of these magnitudes/directions accurately predicts an event in absolute time when applied to a particular regression line from a robust/augmented/imputed/normalized data set in this way, with this type/count of randomness injections & having this demand/supply causal structure, integrated with the vertex 'need' variable metadata (is the medicine necessary, what impact does it have on decisions & functions like memory)
        - organize the function as a structure (like a network or tree) of components once theyre derived (bases, randomness injections/aggregations, composing functions & variable subsets, prioritized/organized distortion vectors, metadata of components like probability distributions, component interactions like developing function overlap according to the time-base, or centrality of variation around local event sequence variables) & test it on subsets of the data set
          - example: if this structure of components explains the changes across subset data sets, its likelier to be true & capable of handling additional randomness aggregations or other variables, which are likely to be injected or accrue at these positions in the structure
      - deriving relevant data set concepts with core structures & priorities (like commonness/organization/aggregation/base/causation to determine relevance):
        - resource:
          - core input/output type of relevant interaction types (trade interaction type, like a pickup)
          - generate resource metadata: 
            - supply: unit count :: quantity, minimum/maximum quantity limits, acquisition limits
            - demand: trigger :: usage, current vs. capacity quantity difference, usage limits
        - habit (developing a function to handle randomness, like storing reminders in a scheduling app)
          - repeated agent function with varying agency/intentionality (intentional vs. natural habit, vs. habit with randomness injected from vertex/interface variables like attention/discipline)
        - event
          - combination/alternate structure applied to output data type (pickup event)
          - isolated/unit structure applied to input base (isolated units of time as events)
          - pivot points of cause (once an event occurs, changes are triggered in other directions)
        - need
          - apply structure of imbalance between resource metadata (supply/demand)
          - apply structure of limit to functions (requirement/assumption)
          - apply structures of cause to functions (dependency)
        - the structures of conceptual variables like resource/habit/event/need can be replaced with the structures of interval & lack of interaction enforcement, if the interacting system/function data is available
      - which variables apply or give the illusion of randomness, for what reasons:
        - high-impact, root-cause, high-variation, multi-function variables that act like interfaces often have cascading side effects that interfere with a high variation of systems, across interaction structures like interaction layers, and if multiple interface variables exist, they may interact in a way that seems like randomness or generate randomness in their side effects
        - homogeneity in system structure tends to generate highly similar variable sets (shapes that develop in corners of a shape) which tend to generate randomness (equivalence of probability of a shape occurring in a particular corner)
        - what structures of randomness are more consistent/stable than others
          - randomness that occurs on layers of a system built on a stable base whose foundations/limits can handle high variation or increasing variation up to a certain degree of increase, allowing unrestricted interaction between structures that can hold similar/equivalent randomness so there is a guaranteed interchange of randomness where overflows occur
          - artificial/enforced randomness, in equally likely variable sets whose possible output sets are constant (possible values of dice)

  - surprising patterns often come in the form of:
  
    - compounding patterns that go unmeasured (black swan pattern)
    - compounding patterns that are measured in ways that they dont vary from expected patterns (different dimension as a host of variance)
    - compounding patterns that are measured in ways that vary from expected patterns but not at point of measurement (wave function & line intersection)

  - expectation vacillation is optimized when neither extreme is expected permanently & expectations gravitate toward local inflection or threshold points
    - expecting evil & expecting sainthood are both sub-optimal in most situations, whereas expecting moderation is usually more useful bc it allows more freedom, and more freedom allows more self-optimization than using forced optimization rules, which change slower than local (self) optimization rules

  - prediction functions

    - its not important to just identify the best-fit balancing bias/variance for a model, its also important to identify:
      - the adjacent models that exist with common distortion functions applied to the best fit model
      - the change patterns applied to the best fit model

  - identify maps between:

    - machine learning: ai algorithms & probability distributions
      - which algorithms output which probability distributions (both in output and in input var sets)

    - semantic: filtering gaps, variable sets, & problem types
      - which filter sets (filters to capture cascading variance across feature levels) map to which variable sets/variable types/problem types

    - so that you can use the map in reverse, using probability distributions of inputs to identify which algorithm to use, etc

    - predictions can be generated from radiating layer graphs

      - whats after time-based change assessment? interface, variance/potential, default-based change assessment

      - whats after system derivation & management? predicting systems from a boundary, the core input (boundary/limit management) - the goal is filter management, as filters can generate other objects

      - whats after humiliation/criticism & problem avoidance markets? achievement, solution automation, and solution-sharing markets

      - whats after dependence market (value in exploiting people's positions)? 

        - an independence market (value in setting others free of their positions)

      - whats after a love/emotion-based society? 

        - a problem-solving society based on potential, independence, and understanding, where everyone can self-motivate

      - whats after an information market (value in certainty)? 

        - a potential market
          
          - value in managing/arranging/creating uncertainty, manipulating/protecting time/change/potential, ensuring time/change enables information that benefits people

      - whats after a chaotic society with voluntary organization that allows corruption to gather & hide to advance other governments (feelings)?

        - a society organized by intelligence (as a proxy for potential), with alternatives to:
        
      - use stressors to create intelligence
      - generate intelligence using system/interface analysis
      - teach/share intelligence
      - find intelligence in a data source like the internet
      - install intelligence (learning models/maps, regression tools, logic tools like decision trees, prediction tools like a simulation/imagination engine)


### Invention Prediction

  1. Reverse-engineer: apply known useful functions (combine, reduce, standardize, compare, duplicate, randomize) 
    to fulfill common useful intents (predict, verify, find, etc) & assess value of output product in problem space

  2. Conceptual query: apply structure to conceptual combinations & check matching problem spaces if the output product has value for an agent in that space

  3. Identify necessary structure and identify interface combinations/trajectories that can generate that structure


### Interaction Predictions

    - before buying a product, after scanning the objects you own, this tool would be able to tell you how the product might negatively interact with the other objects

    - example: when buying essential oil, it would answer questions like:
      - 'how will this interact with the furniture in my house, if used as directed?' (diffuser)
      - 'how will this interact with the furniture in my house, if used as people use it for alternative purposes than directed' (medical)
      - 'how effective will it be for a particular problem/use case' (this is simulated product testing using object model queries like in the previous section)

    - it would also scan the commonly used & potential use cases for possible intentions with the product
      - 'if youre planning on using it for use case "self-treatment", only take x amount for y period of time if youre otherwise healthy'

    - also generate which attributes coordinate with which rule sets (& their combination output spaces)

      - given a set of rules, you can generate the possible impact of rule combinations in a space with known objects
      - the output space should be usable as an input to derive the set of attributes generated by/emerging from the rules

      - example:

        - given that a chess piece can move according to certain rules & given certain input chess pieces with position, the set of all possible moves is calculatable
        - given the set of all possible moves (and resulting paths), you should be able to calculate emerging attributes of the game of chess, such as:

            - ratio between optimal paths and possible paths
            - maximum path-limiting potential of a non-final move
            - optimal intent & intent combinations of varying abstraction (diversify, get to opposing end first, guard king)
            - key determining variables (number of pawns sacrificed, number of maximum-square moves, number of nth-hop moves)
            - incentive structures (have backup strategy, calculate limiting activity of moves)
            - number of equivalent alternative paths
            - variance distribution across system
            - variance added by each piece type and each repeated piece of a type
            - variance added by each rule, allocated to a type or across types
            - value added by planning/deriving opponents' plans
            - value added by calculating all options or probable options during game
            - value added by preparing for probable options given opponent experience assessment
            - value added by assumptions (aggressiveness, experience of opponent, strategies not clearly mapped to intent or other system objects)
            - optimal board coverage at different phases
            - function sets needed to beat any opponent
            - variable sensitivity
            - variable substitutability
            - related variables
            - unenforced rules (creating variable spaces) vs. enforced rules
            - types/cause/intent objects
            - potential impact of agent creativity
            - relevant interfaces
            - path metadata topology & navigation methods to get from one state to another

          - abstract attributes (concepts, properties):

            - system attributes:
              - complexity
              - randomness
              - order (moves occur in chains where order matters)
              - types (different types of pieces have different rules)
              - ratio (different ratios of type populations)
              - limits (moves limit the remaining possible moves)
              - games
              
            - game attributes:
              - fairness

    - the matrix of rules/attribute configurations & interactions is an important object for emergent effect/concept & interaction prediction
      - this stores the full set of interactions & variations possible within the type/object definition
      - the variations are representable with a topology
      - the interactions may vary by more than one distance measure, so may require multiple topologies (metric topology of variation topologies)
      - representing the emergent properties may involve networks, paths across the topology & between topologies for emergent properties (like relevant interfaces or potential impact of agent creativity)
      - representing abstract attributes may involve subset topologies (a topology using a subset of vars or another transform like a type/interface/concept transform), networks, paths, layers, cross-sections, circuits & slices for abstract attributes (like power, or balance)
      - abstract attributes can be inputs, outputs, or organization/combination/logic attributes
      - since abstract concepts usually have their own internal network of meanings, the concept itself usually has a field of relevance generated by this network, rather than a clear boundary on its relevance

    - what other networks generate fields?
      - networks with enough variance have fields of potential in addition to the defined objects & functions, since variance accretes & variance potentials collide
      - these fields of potential have multiple types/layers:
        - potential variations of existing system objects (position, spin)
        - potential changes in system rules (switch between change rule sets, type rule sets, boundary rule sets, standard rule sets, or interaction rule sets at different scales, when joined with other molecules to form different-scaled object)
        - potential breakdown/transformation of system (charge rules, change rules)
        - potential interactions with other systems (sharing electrons & other types of combination)
      - the collision & overlap of these fields of potential generates areas of states varying by probability

    - emergent effect prediction:

      - example: 
        - if you bend your knee fast enough that you eventually hit a limit, a reflex will kick in (based on speed if you miscalculate your speed based on the limit) to push you up without effort, retaining some momentum in the process, exerting a synergistic effect with standing up
        - given that the spectrums of variance overlap (vertical motion) between variables (reflex motion & bend motion), you can predict that they will either have synergistic/compounding or neutralizing impact, depending on direction of output force

      - factors of potential for emergent effects
        - alignment between function metadata (intents, variables, variable metadata)
        - boundaries/limits 
        - variance handlers (boundary protection rule - reflex instinct)

    - state/type change point prediction
      - states & types follow patterns in the proportion & distribution of variance across variable sets
        - example: several variables have to be diverging from average in order for a type change to occur, and the output vector & other metadata of these changes will have a point where it becomes clear which type they are leading toward and what probability the transition will continue
      - the change points will be discernible from these patterns & pattern variables
        - the patterns in variables, variable sets, variable values, variable types, variable metadata (importance, dependency network) will be able to predict type change points at various variable configurations


### Structural objects as prediction tools

  - set
  - filter
  - network
  - system (network with boundary)
  - structures of change (direction/distance)


### Information objects as prediction tools

#### Facts

  - provide a useful starting point for predictions

#### Expectations/Assumptions

  - provide a useful starting point for distortions or combinations of assumptions

#### Perspectives

  - use biased structures as a starting point for filtering solution space
  - perspectives are a filtered version of truth that reveals information by what it focuses on & filters out
    - examples: religious, logical, probability perspectives
  - the functions generating perspectives are a foundation for truth-distortion methods

#### Intents/Priorities/Reasons/Motivations

  - use purpose as an interface for filtering solution space

#### Plans/Intentions/Strategies

  - like the other objects, plans are an attempt to create structure in an unstructured space, starting from the agent perspective
  - plans differ from intents bc intents can be a direction or a function, whereas plans are usually an ordered combination of functions to achieve a move in a direction

#### Insights

  - connecting rules (include core functions determining connecting rules)

#### Incentives

  - unlike plans, incentives create structure from the system perspective

#### Opportunities

  - use opportunities (intersections of unenforced rules) as a way to reduce solution space

#### Questions

  - focus predictions on a particular information asymmetry/direction/layer or other structure that can support information

#### Patterns

  - patterns of common system objects (combination/filter functions, function/attribute objects) are another good starting point, 
    either standardized across systems or contextual patterns within the system to predict

#### Solutions/Problems

  - maps between solution & problem structures are another useful tool for predictions or reducing solution spaces

#### Lies

  - unlike jokes, lies use realistic rather than extreme distortion functions

#### Jokes

  - similar to lies, jokes are often built with multiple core distortion functions to build an extreme possibility that is built with some true information & is relevant in some way
  - this makes joke patterns possible tools for determining likelihoods
    - combinations of extreme possibilities occasionally return to fields of probable outcomes
    - reality follows patterns of extremes & relevance to current knowledge

  - common joke (reality distortion) functions:
    - switch objects with extreme versions
    - remove or add assumption
    - extend assumption to extreme

  - joke patterns by intent:

      - to find overlooked assumptions:
        - create false question (a question no one would ask) 

      - to find hidden relevance:
        - vary stakes (to find actually important things among things considered irrelevant/unimportant)

      - to find alternate path:

        - functions to explain relationship between 'stereotypes & behavior that complies with stereotypes'

          - simple answer: 
            - 'stereotype is true'

          - complex answer:

            - 'stereotype creates comically false expectations'

              - which has output paths for agent positions, given example using 'race' 
                (but only the redneck & clown races, which I hate for valid reasons which Im required to keep to myself - Ill let you figure out who Im talking about - hint: its rednecks who are all clowns though they try to pretend to do computers - how sad that I have to clarify this at all):

                - victim:

                  - complying with stereotype expectations to trigger racist comments to:

                    - seem like a victim

                    - choose compliance with expectations
                      - to free themselves from making a decision
                      - display self-awareness/control

                    - create opportunity to make a point that: 
                      - only racists would criticize them
                      - they expect other groups to be offended, which is racist

                    - trigger a fight to: 
                      - enable criticizing someone of a different group
                      - deliver a joke about group of attacker

                - attacker:

                  - create opportunity to make a point that:

                    - victim is making racist assumption by:
                      - expecting other groups to be racist rather than just stating facts
                      - assuming that stereotype isnt true about most group members even though they havent met every group member
                      - interpreting jokes built on stereotypes as serious/factual statements (which implies racial inferiority in IQ)
                    
                    - victim has lower IQ & is similar to other racists:
                      - prioritizing truth over comedy is having priorities wrong (and guess who else had their priorities wrong)

                    - victim's group has the worst stereotype jokes & only the smart races are racist bc they have the option of thinking and theyve figured out that racism is optimal
                    - dehumanizing people is a mark of civilization & the only thing humans have in common, so if they dont do it theyre subhuman
                    - being offended implies the victim's group is inferior & incapable of figuring out when a joke is good, so clearly the joke teller is innovating their field rather than trying to offend anyone
                    - racism motivates people to achieve things and if you dont contribute to racist comedy, youre crippling global markets
                    - by making racist jokes, you are offering free motivation to achieve things out of a charitable spirit, which group was clearly lacking
                    - group needs a villain to persecute so only a selfless hero would tell racist jokes to provide a villain
                    - if they werent busy villainizing people:
                      - they would have more time to ask the right questions about why they all act the same way
                      - they would have time to come up with better jokes mocking other races
                      - they would have time to change their behavior
                      - they would have time to think it through, realize there are some good points there, and start mocking their own race better than attacker bc they know more about themselves
                    - realize that the person who makes racism the funniest wins and therefore theyre not rooting for their own group by failing to come up with the best racist jokes
                    - racism is flattering bc provoking a large group of people means you think (or rather wish) theres a possibility they can beat you as a group, if they all work together for many years

        - which are alternate paths to explain the original relationship between 'stereotypes & behavior that complies with stereotypes' using distortion functions:

          - remove assumptions:
            - victim is really the victim
            - victim is not racist
            - victim is superior
            - stereotype is false
            - people are good
            - people arent motivated by racism
            - victim cant control their behavior

          - extend assumption:
            - given starting assumption, extend it:
              - 'victim is not racist'
                - find attributes of someone victim would call a racist
                  - find similarities between victim's attributes & racists' attributes
                    - find path to portraying victim's attributes as racist
                      - complete the circle using functions to build a false relationship 'victim is racist' out of facts or realistic logic

      - the reason this is useful for predictions is:
        - joke patterns can help find complex reasons (alternative paths) to explain a seemingly simple relationship
        - joke patterns can help find assumptions that may be false, especially if removing them opens the possibilities for extremely logical alternative paths
        - joke patterns can help reduce solution space (possible paths to explain relationship between A & B) while finding complex relationships, using distortion functions commonly used to build complex relationships