## example of interface analysis applied to 'artificial intelligence' problem space

    - ai assumptions
      - ai assumes that there is a causal node (indicating a source reason) for a pattern that is relevant to the problem ai is applied for
        - assumes 'if a group is associated with x, there must be a reason for that'
        - there may be a causal node indicating the reason for that (other than bad/missing data), but it may not be relevant to what ai is being applied for
        - example:
          - there may be a reason that a group is associated with higher crime or lower income, but the reason may be relevant to a historical human bias error like 'over-simplification', rather than relevant to the ai usage task of 'determining how members of that group' should be categorized or otherwise allocated resources'
        - ai is not evaluating its own usage intents that it supports, is not adjusting itself for possible usage intents, and is not accounting for assumptions like 'reason for a pattern' (and subsequent assumptions like 'any reason for a pattern is good enough to use that pattern to identify components fitting a pattern') & errors like bias injected into the data
        
      - how to correct this error:
        - apply insights:
          - rather than using any pattern to identify components, use patterns that are generated by reasons that are legitimate for the usage intent
            - rather than using patterns generated by a reason, like a general 'human bias error' or a specific 'over-simplification error',
              - use patterns generated by a legitimate reason for the usage intent like 'determine economic status', patterns such as 'financial credit history' or 'other resource decision-making history', which is a pattern generated by a legitimate reason such as 'prior decision-making patterns are predictive of future decision-making patterns'
          - this applies concepts like 'time' and 'decisions' as more predictive of 'future decisions' or 'an output of prior decisions' (like economic status) than variables used as inputs to human bias errors like race/gender
            - inputs to human bias error can also be inputs to future decisions, but mostly only bc of human bias errors applied to decision-makers, rather than bc of their absolute decision-making patterns in a vacuum (even in the absence of human bias errors like racism/sexism applied to them)
          - identify insight that a particular variable is only applicable in specific contexts
            - race/gender are applicable in a system where race/gender are inputs to decisions subject to human bias errors, and where that system cant change, and where no counterexamples of race/gender being unpredictive can be found
          - identify that if the algorithm uses the variables race/gender just like human bias errors used race/gender variables, the error will continue & the algorithm will either be increasingly less accurate over time, or it will be more accurate only bc it caused its own accuracy (condemned a group so they continued to be condemned)
          - identify insight that input variables to errors like human bias errors are typically overly simple and shouldnt be used in isolation
          - apply concept of 'fairness' to avoid errors like over-simplification bias & variable isolation bias
            - if a pattern is legitimate to use for a usage intent, it will also be fair (using prior financial decisions is fair game to predict future financial decisions)
            - if a pattern is highly predictive for a usage intent, it will also be legitimate (a racist/sexist algorithm wont be highly predictive)
          - identify insight that the algorithm repeatedly making errors anytime an oppressed/weaker group is part of the data set, bc humans commit a bias error of 'allocating costs to oppressed/weak groups bc they cant return the punishment & allocate costs to the allocator'

        - how to derive these insights to correct this error:
          - identify & apply relevant concepts
            - fairness (use relevant components to predict variable)
            - legitimacy (use actual absolute predictive variables, rather than variables that are only predictive bc of context such as errors like bias being applied)
            - usage intent
            - decisions & time are relevant concepts to predict a variable about time-based (future) decisions
          - identify & apply relevant insights
            - variables like human bias errors create other errors
              - avoid common known bias errors (like over-simplification) unless theyre required for a relevant reason (like 'finding bias errors')
            - some variables only apply in specific contexts
              - if a variable (like differences in financial decisions) disappears when its input (human bias error) is removed, its a contextual variable, and therefore wont be predictive
              - absolute factors like definitively relevant variables (decision-making history) are better predictors
          - these concepts & insights are relevant to the usage intent (predict future decisions) & context (human bias error history) by definition
          - the injected concepts & insights in the error are not relevant to the usage intent
            - concepts like 'simplicity' and 'group' are not relevant if applied as 'any position or structure of simplicity' (including over-simplicity in the algorithm, or algorithm inputs like over-simplification error causing biased data) and 'any group' (including groups of race/gender)
            - the subsequent implicit assumption 'any reason is good enough for any usage intent if its above a level of predictive accuracy' of insights like 'patterns exist for a reason' is not relevant, bc 'any reason is good enough' doesnt align with the usage intent


    - example of injected vs. emerging definition in an algorithm/data
      - example: poetry automation
        - solution metric filters
          - has to be a reason for a distortion from normal connections (distortions like leaving words out, or like 'away it came' is a distortion from how youd normally portray movement, but there isnt a reason for it, such as similarity or relevance to other forms of movement in the verse, such as that the distortion emphasizes another structure to add meaning to it)
          - show rather than tell (gives structures of mysticism rather than just calling it mystical to sound like its indescribable, as in beyond words/structure)
          - compliance with a standard, whether its a standard tone (emotional), style (formal language), structure (syllable count), or usage (cadence)
        - why would AI error out on these filters:
          
          - response of audience is not considered in pattern data (patterns are isolated from responses generated by the whole poem or a particularly evocative subset of the poem)
          
          - structure isnt the only important component
            - structure of the language map query isnt the only important component
            - language structures like syllables/clauses/adjectives/rhymes arent the only important component either
            - language meaning structures like definitions/synonyms are more meaningful but not in isolation 
              - cant just execute simple queries like 'list definitions of interesting words' to make reliably good poetry
              - the structures (like position/connection/repetition) of definitions & other language meaning structures also matters
            - response:
              - language structures have different impact on different audiences, evoking different emotions
            - surprise:
              - attention structures (cadence, repetition, distortions, emphasis) of the poem will be different for different people in different states
              - the element of surprise is crucial for the poem to work in order to make its point effectively by drawing attention to specific connections that are surprising to the audience
            - starting/ending positions of the query are also relevant
            - connections to other structures are also relevant

          - patterns arent the only important component, even sequential patterns
            - it might also miss other patterns (of varying size like patterns of structures like lines or linguistic components, or varying pattern componet, like patterns of patterns/connections/attributes) & position (like skipping a position)
            - even if it copies a pattern of node traversal in a language map, the context might be lost between the two positions on the language map where the query is started
              - the second position might have a different query that is equal to the original position's traversal query, bc the query is guided by incorrect attributes that dont generalize to the second position
                - example:
                  - original position:
                    - "daybreak's lost citizens"
                    - queries:
                      - language component query: possessive noun adjective plural noun
                      - meaning query: 'the dead'
                        - possible related meaning query: 
                          - 'dead from activities in the night, like a war raid'
                  - apply the language component query to another position:
                    - "mountain's missing flowers"
                    - queries:
                      - language component query: possessive noun adjective plural noun (same query)
                      - meaning query: 'damaged environment'
                        - possible related meaning query: 
                          - 'flowers plucked by people to give to another person'
                          - 'environmental damaged exacted by people'
                          - 'natural disaster like lava or storm'
                          - 'lack of flowers from harvest or animals'
                          - 'lack of flowers planted by people to protect environment'
                          - 'too many bees for the number of flowers to sustain them'
                  - the second position has some of the poetry of the first just using the language component pattern, but lacks the gravity in tone, the associations & context of the original bc of the different nodes connected with the pattern
                    - some of these attributes are mimicked by selecting certain nodes to re-generate the original tone & connection patterns ('lost' and 'missing' having similar meanings)

            - patterns might be cut off right where theyre about to make a sequential connection pattern with another pattern
            
            - patterns might only have meaning in particular structures (which may not even need to be in a sequence, because reversing the order might have the same impact), structures which are not preserved by the ai algorithm

            - abstracting & isolating into a pattern leaves out info about context (system interface), which removes the relevance between different components of the verse
            
          - ai has no concept of relevance except the definition (injected by the developer) of adjacence (or the definition of relevance emerging from the algorithm + data) built in to the pattern identification function (like 'isolate sequences that are repeated' or 'isolate sequences of x words')
            - the injected definition of relevance from this algorithm/data is 'sequential connections' or 'sequential repeated/common connections'
            - the emerging definition of relevance depending on the algorithm/data might be 'isolated sequences of x words beginning with subject and having at least one verb + adjective', which is the pattern in the sequential pattern components that might emerge from the data set
          
          - ai doesn't align definitions of meaning between input/output
            - does not self correct its conceptual changes between input/output definitions of meaning
          
          - ai doesn't align definitions of meaning across interfaces or interaction layers
            - whats meaningful in a particular example data point (relevance to another pattern in the data point) or on the structural interface (number of syllables/words in pattern) isnt automatically aligned with whats meaningful for the general intent of 'generating good poetry' for any input data points
          

    - ai tests
      - questions/tasks that are typically framed as unable to be answered by anyone other than a human, like:
        - 'what is the meaning of life'
        - 'find a new system to predict events'
        - 'what will be the next big invention'
        - 'when will the end of the world happen'
      - these are relegated to human philosophy & endeavor bc theyre considered too complex to answer, requiring many contributors over many generations
      - example of how my system would approach these questions:
        - problem: 'find an answer to the question: what is the meaning of life'
          - the answer is 'protecting potential of the universe'
          - an example of an interface query to get to that answer is:
            - standardize problem:
              - 'what is the purpose/intent of life'
              - 'how does it fit into the contextual system in which it develops'
            - alternate queries:
              - apply definition & system interface
                - a definition route of 'life' is the 'ability to change' (learn/adapt)
                - a definition route of 'intent' is what something can be used for
                  - sub-problem: what can life be used for:
                    - all functions of the human brain
                  - apply 'system context' interface to this sub-problem (sub-question, or sub-query)
                    - when a question like 'what can x be used for' or 'what intents does it support' is asked, the intent of the question is often finding the most differentiating intent from other components, rather than listing all intents
                    - the most different intent supported by 'human brain functions' is 'protecting entities with potential who cannot reward them'
                    - the reason this is the most different intent is that its the:
                      - rarest intent
                      - aligns with responsibilities (sources of potential have responsibility & power to protect other sources of potential)
                      - most difficult intent
                      - an intent that requires unlikely conditions, like being taught to prioritize it, or deriving that its the right intent to prioritize, by deriving an ethical system prioritizing it
                      - associated with intelligence, a proxy of potential
              - apply definition & conceptual interface
                - 'meaning' has a structural system definition route 'fit in a system'
                - 'intelligence' is a proxy for 'potential'
                - 'potential is uncertain ability'
                - 'a contradictory component to potential is certain ability or certain inability'
                - 'intelligence is power'
                - 'power' distributes when it's misused (used to preserve itself, at the cost of power distribution)
                - intelligence has functions like:
                  - can change a component to fit into a system (create meaning)
                  - distributes its power bc it can generate power/meaning under any conditions
                  - identifies that:
                    - potential is the rarest & most valuable resource that is a similar input to meaning to intelligence (itself)
                    - protecting sources of potential is top priority
                    - delegation of power is equal to delegation of responsibility
                    - delegating both power & responsibility is more efficient than other methods, as other entities will learn to solve their own problems with help
                    - distributing power in the form of intelligence does not contradict any sources of potential & enables sources of potential
                  - concludes that:
                    - not only is it efficient to distribute intelligence, it is also ethical (does not destroy sources of potential)



    - add to nn

      - youre always using multiple formats in a graph, even when trying to depict one format (a variable network always has core structures & conceptual structures even when it's intent is to depict variable interactions)
        - even a conceptual network has conceptual (and other interface) structures depicted on it that overlap with other structures (like concept nodes & interaction functions)
        - the formats used can be side effects of the primary format, indicating adjacence
        - the reason theyre included by default is bc interfaces are fundamental

      - why cant you just apply a good ai algorithm with probable predictive value to data, once data is standardized to an interface format?
        - example: format data as concepts, and apply an ai algorithm to predict concepts in a data set, or predict concept interactions in a concept set
        - you certainly can, I suggested doing so in my patent for coverage, as it's the obvious existing solution, but only as a short-term sub-optimal solution to connect input/output while the interface logic was being built
        - its also manually applying interface analysi instead of incorporating it into an algorithm
          - by manually applying the definition of a 'concept' to fulfill 'data processing' intents like 'standardization', you are manually applying interface analysis, which is still my invention even if you do it manually
        
        - interfaces have logic specific to them that may be more useful/efficient than an ai algorithm
          - formats have interaction logic specific to them that may not fit with the input/output relationships of a particular algorithm
          - examples:
            - functions/intent/cause/pattern/logic/change have inherent sequential structures that other interfaces dont
              - this is bc they are objects that map inputs/outputs
            - concepts have an inherent tree & net (capturing evolutions of definitions before fully structured) structure that other interfaces dont (concepts capture openings, overlaps, definition evolutions)
              - this is bc the interface object ('concept') is by definition a definition structure (a structure that describes definition structures, like definition routes)
          - an algorithm that applies the more powerful interface interactions & structures inherent to interfaces to quickly identify important variables, group membership, & other important structures would be a better goal than trying to pretend you didnt steal my work
            - the work of 'stealing my work' and 'pretending not to' doesnt produce efficient brains with a good grasp of concepts like 'meaning', so it emerges as an inefficiency bc the type of brains developed wont be good at finding solutions to output problems generated by granularly copying/pasting to a specific problem/solution & hoping it works better
            - 'understanding my work' is a better goal if you cant look away bc youre not done experiencing awe
              - once you understand it, you wont need to watch me work, youll be able to generate my work
        
        - ai automates learning, like identifying structures such as important variables, why cant you use it for everything
          - some learning algorithms are sub-optimal (as in slow, or missing important info) or cant derive the required info (as in limited in possible accuracy achievable) so theyre not fitting for everything & cant derive the missing info
          - interface analysis can automate identifying important variables by applying an interface:
            - to apply interface component definitions
              - like identifying variable structures, such as vertex variables
            - to convert to a format where the data is framed in terms of these structures & has the interaction logic rules applied (like rules governing interactions between variable types, like how a vertex variable is often an input to other variables bc its causative)
        
        - learning method
          - ai applies updates to learning parameters to adjust a function until its a good predictor
          - interface analysis can apply other methods of learning, like applying:
            - error structures to derive a function from opposite structures to those error structures, for intents like 'create an accurate prediction function'
            - vertex structures, which are an interface structure related to 'important variables', for intents like 'identify important variables'
            - organization structures, which are relevant for intents like 'finding the optimal position of a structure in another structure' and 'finding useful structures'
          
          - related intents of ai can have different adjacent algorithms
            - 'create an accurate prediction function' (apply function, base, component, subset, filter & limit structures)
            - 'avoid errors' (apply error structures)
            - 'identify important variables' (apply vertex structures)

          - however the methods for one intent can act as proxies for another related intent

          - how to identify related intents
            - 'avoiding errors' is an intent that coordinate with intents like 'find a connecting function' (like 'find a prediction function')
            - the reason they are related is bc of interface structures (definition, structure, & functionality):
              - definition
                - errors are by definition a component of learning
              - intent
                - the 'prediction' intent is helped by intents like 'learning new info'
              - structure & functionality
                - the inputs/outputs of 'avoid errors of this type (inaccurate prediction types, as inaccuracy/difference types)' and 'find a connecting prediction function' are similar

          - alternative structures to filter function network (other than variable/object/state network)
            - limit network
              - format data in a way that it can be input into a limit network and the limits will perform intents supported by limits:
                - identify it or what it is not
                - find its optimal position
            - combination network
              - network of subsets/combinations of components (like variable subsets/combinations)
                - applying functions to nodes in this network can identify optimal combinations quickly, if functions applied align with outputs (applying the functions produces useful info to determine functionality)
            - mixed network
              - combination of network structures
                - route data to other network structures when its determined by a routing network structure that other structures cant use it (like routing info to another interaction layer, if its a network of interaction layers)
              - network of mixes of structures
                - network of mixes (highly variable combinations) of structures, like a network of cross-interface structures
            - similarity (adjacent/proxy) network
              - replace components with adjacent components for intents like 'find alternatives'
            - difference network
              - a network of difference structures (like extreme, opposite, specific, value, structure, & pattern differences) for intents like 'find alternatives' or 'find interactions between a specific difference set'
            - relevance/irrelevance network (for what contexts & problems would info be useful/useless)
              - system context network
                - systems where info would fit into the system or be useful
                - related contexts & their interactions
                - spaces where info is structural (graphable) or has an attribute like difference structure (difference type/degree)
            - point sets (like threshold values/centers/origins to use as references)
            - cross-interface maps

          - alternate variable set structures
            - isolated aligned variable changes as a map (stack of differences) between types/thresholds

        - structures identified by interface analysis
          
          - learning structures (including by definition variables, types & differences)
            - error/solution structures
              - adjacent (sub-problem, related problem, available/known/possible) error/solution structures
            - guess-answer interaction structures (structures of difference/similarity between very wrong or almost correct guess & actual answer)
              - prediction-answer interaction structures (structures of interaction between informed prediction, uninformed prediction, pattern-informed prediction, etc)
            - question-answer interaction structures (structures of interaction between question, info retrieved by question, difference between info retrieved so far & answer, and the answer)
            - adjacent learning structures
              - change structures are adjacent to learning structures bc learning involves change types (like between initial/interim/final states, inputs/outputs)
            
          - interface structures (intents, concepts, patterns)
            - system structures like incentives, ambiguities, & efficiencies
            - meaning structures (reasons why something worked, how an insight fits in a system, how components can be organized in a useful way, how a structure is relevant for an intent)
          
          - useful interface interaction structures 
            - function sequences with alignment in inputs/outputs
            - core functions as a component for 'build' intents
            - interaction layers (coordinating, competing, contradicting, coexisting & other interaction structures)
            - definition structures
            - change potential
            - conversion adjacence
            - structures of interface interactions (intent-function maps, logic/cause/state networks)

          - core interface interaction structures (apply interface structures to interface structures)
            - to generate core intent structures:
              - apply structure interface: structure intents like direct uses
              - apply concept interface: conceptual intents like priorities
        
        - structures identified by ai
          - type (group membership)
          - difference
          - important variables

        - questions unanswered by ai
          - does the definition of difference make sense for this problem/solution components (algorithm, data, intent)
          - what are the intents (structure intents like direct uses & conceptual intents like priorities) supported by the algorithm/parameters
            - can this algorithm fulfill the intent of connecting the input/output differences in the data
              - is it capable of mapping the inputs/outputs, given the difference structures in the data (difference types, lack of sufficient differentiation, etc)
            - what other intents (like adjacent/contradictory intents to the primary intent) are supported

      - ai has added concepts since its first implementation bc the first implementation (multi-layer perceptron) was not enough & other functions needed to be embedded to optimize for certain tasks
        - which functions/components need to be added for which tasks is still not a solved problem in the ai space, whereas my system does solve that

      - ai has still not derived the concept of an interface & generated its own interface structure to solve a problem, without being fed the definition of an interface, which is an input to my system
        - if ai was the optimal structure, it would have quickly identified the useful components of my system (like trade-offs, exploits, efficiencies, ambiguities, definitions & interfaces) & applied them to solve any problem, but most ai algorithms/parameters cant do even one task with perfect accuracy (or total coverage, like 90% accuracy & an explanation of why it cant achieve higher accuracy - like 'insufficient data')
        - my system can identify when minimal info to solve a problem isnt reached & can generate explanations of its processes (interface queries, which are trajectories on the 'interface' interface)

      - ai depends on insight path algorithms like 'trial & error' to accomplish some tasks like in grid search to find optimal algorithm parameters, but still must be told to do so by a human who has decided that exploring all parameter combinations is needed
        - theyre wrong to think that, they should have come up with my method to filter a solution space using interfaces & solution automation workflows
        - a 'function network' is only useful if a human puts it in the right system & applies it in the right way to solve the problems its optimized for, and retires it when better methods are implemented - the function network doesnt get better at solving all problems just by being fed more problems
        - my system self-optimizes as its given more problems to solve

      - a function network can sometimes be replaced by an object (interface components) or variable network (of interface variables) but ai wouldnt figure this out if it was given a function network as its origin algorithm, bc while learning how to optimize for a particular solution, ai is not evaluating how to optimize its own algorithm, input, & parameters and would need to be instructed to do so
        - then it would encounter error types and it would need to be instructed to identify & handle error types
        - then it would be linked with other models, data, & function networks, and other interaction error types would emerge and it would need to be instructed on how to handle new data, integrate with other function networks for useful agent intents without merging with other function networks (which would remove its connection to the original tasks it solved), help other models avoid new error types while identifying & handling them itself
        - it wouldnt just identify that it should apply concepts like 'meaning' in its integrations with other automation components, it would have to be told to do so & told how to do so
        - a structural definition of 'meaning' is an input to my system, so my system does have methods to handle complex integration/organization tasks bc these are components of meaning

      - ai depends on my system in order to predict error types better, integrate common sense, apply definitions of concepts, etc

      - my system can generate error types & apply different solution automation workflows as needed to solve a problem, and identify the workflow likeliest to be useful before applying it

      - a function network depends on input data, whereas my system depends on problems, definitions, the logic of interface queries, & an internet connection to find new info as it identifies its own need for new info

      - examples of functions that are sub-optimal for a task but get it done (or appear to do so) without immediately invalidating side effects
        - an approximation method isnt as useful as a correct method built on cross-interface understanding
        - attacking a known security vuln by deploying a code fix specific to that vuln doesnt address the problem of identifying/preventing vulns automatically or identifying/preventing hackers from having/using reasons to hack
        - playing whack-a-mole with exploit structures & exploitative agents doesnt address the causative problems of a conflict between agent intents & a lack of coexistence/cooperating/coordination structures
        - paying ransomware hackers as a 'recover lost data' function doesnt address the related problems of risks like that they wont or cant return data or will return it in error (partial/unusable) state & common hacker intents like making money rather than fixing problems and the incentive to move on to next victim or next scam rather than helping victims without ability to enforce data recovery
      
    - what generates understanding? specific interface concepts integrated into the solution, with examples showing understanding (of interface components like interactions/systems in general, & of the problem space), like:
      - abstraction: 
        - handle the problem type 'unhandled problem variants of a problem type' by abstracting the solution
      - intent: 
        - handle the problem type 'conflicting agent intents' by restricting the intent applicability of the solution (what intents are supported by the function)
      - system:
        - handle the problem type 'solution used in different context to enable malicious intents' by applying system context filters to enable applying the solution in specific contexts
      - change: 
        - handle the problem type 'unexpected changes to inputs, logic changes at execution time, or changes applied to change usage of outputs' by enforcing validation of function version, input validation, and validation of output integration with other solutions
      - organization: 
        - handle the problem type 'sub-optimal interaction of solution with other components' by applying simulations of interactions with other components

    - a good solution shows that it was built with understanding of these interface components, including components like:
      - concepts like 'malicious intent', 'organization', 'relevance', 'probability', and 'integration'
      - problem types like 'conflicting incentives' and 'contradictory neutralizing functions' and 'structure vs. flexibility trade-off'
      - system components like 'similarities', 'standards', 'opposite structures', 'interaction layers'
      - function structures like 'useful input-output sequences' & 'core functions'
      - change components like 'vertex variables' and 'adjacence'
      - common/useful structures like 'maps between interfaces'

    - the relevant structures for a problem (like related problem types & structures of those related problem types) should be generated & applied until understanding is reached
      - at this point, once it has understanding, a neural net can solve problems like:
        - why is the user using a neural net to solve this problem
          - example: 
            - why is the user using an AI to solve a problem of differentiating cat/dog photos, if the photos are very clearly separable, meaning the problem doesnt require AI
            - bc the process of using AI is solving a different problem for the user than sorting cat/dog photos, such as different outputs of an AI model, like:
              - time usage: looking busy at work
              - authority: using AI as a source of truth/authority ('the AI said it was true so it must be true')
            - by knowing about system & related system components like 'usage' and 'context' and 'agent', as well as other related interface concepts like 'intent', the AI can infer that AI model outputs (given a definition of AI models) might be the intent of the user
            - alternatively, by knowing that sometimes a problem type of 'different origin/target (like a different cause/reason/intent) is more relevant than the direct/stated origin/target' applies to a context, it can infer that the user has a different origin/target (like a different output of the AI than the AI model itself, or the completion of the task)

    - with regard to using the structure of a filter network as a filter for useful info contributing to a function, other info & info formats are useful & derivable which can be derived with faster means (definitions, bases, subsets, concepts, error types, intents, opposite structures, efficiencies, alternatives)
    
    - nn to derive error types & bias incorporated into other nn data/parameters/algorithms/structures/models

    - ai measures variance of a specific variable structure type
      - example: 
        - an image categorization algorithm measures variance in variable position, in formats where variable position is uncertain, like images
        - apply attributes useful for position-differentiation & position-grouping (like adjacence) to a standard algorithm
          - apply combinations of adjacent data as input to look for features in combinations, rather than standard unit input
        - cause of solution success (why this works):
          - features tend to have position patterns
          - features tend to have relative position patterns
          - a way of measuring relative position is by combining adjacent features


  - ml explanation of finding coefficients of prediction function by applying distortions to coefficients & ruling out distortions that dont contribute to prediction accuracy
    - can be optimized with reductions like:
      - 'calculating the most different distortions that will reduce possible values the quickest & applying those distortions'

  - apply other interfaces to the ml problem space (beyond intent interface)
    
    - interface interface
      
      - calculating perspectives (interfaces with intent, including interface components like origin-target differences & important concepts/priorities/filters) of the ml problem/solution space and applying them to generate a structure that applies multiple perspectives, like the:
        - possibility perspective
          - random/corrupted: the possibility that each or a subset of data points is incorrect/corrupted or random
          - incomplete: the possibility that variables are missing
          - distorted/extreme: the possibility that variable ranges are misrepresented by the data
          - conditional functions: the possibility that multiple alternate functions of the input variables apply in different conditions which can be parameterized
        - opposite x intent perspective
          - rather than solving the problem of 'trying to differentiate between categories', solve problem of 'trying to merge categories'
        - opopsite x direction x abstract (type) perspective
          - rather than trying to categorize from image, try to derive image template types from category & definitions
        - difference x pattern perspective
          - apply difference type interaction patterns to augment data to strengthen prediction function
      
      - finding variables enabling the selected perspectives & generating network algorithm/structure from those variables

    - structure interface
      - alignment
        - what alignments exist between ml network algorithms/structures and:
          - problem components like 'uncertainty reduction' intents (networks that have differentiation, missing info derivation, or de-randomizing side effects)
          - boundaries of interacting solution metric requirement structures
    
    - core interface
      - what components of algorithms exist that can be used to construct algorithms on demand
    
    - function interface
      - what input/output interactions exist between ml network structures/config/algorithms that allow for functionality to develop
    
    - cause interface
      - what causal structures (dependencies, direct/unique/ambiguous causation, alternate causes, proxy causes, causal metadata like degree) exist between ml network structures/config/algorithms, such as:
        - input-output sequences where each component causes (builds, leads to, enables, activates/triggers, supplies requirements of subsequent components)
        - core components which can be used to construct a system/object
    
    - concept interface
      - specificity of ml solutions, which can be generalized to handle more cases
        - unsupervised classifiers identify difference type as defined by a distance definition (distance from center/average/nearest neighbor)
          - generalized form:
            - classifier that identifies multiple difference types (differences within variable subsets, distance from other clusters/average/neighbors, etc) to support various intents (like probability of accuracy in identification/differentiation)
        - supervised classifiers identify difference type as defined by difference from labeled training samples
          - generalized form:
            - classifier that identifies multiple differences, within labeled attributes & category/type definitions, between type definitions, and difference patterns between types

  - is a network the best structure for implementing ml?
    - standard: an ANN network has: 
      - causation: direction linking input/output
      - changing parameters, starting from a standard origin: weight distortions
      - solution filters: applied distortion filters to select the distortions that contribute to prediction accuracy
    - alternate: 
      - other networks can be added to handle conditions where alternate functions can be applied
      - a decision network can be applied to determine when decisions are made to switch to an alternate network (where data contributes to prediction accuracy of an alternate function instead of the current one)
    - subset/core: 
      - sub-networks or core network components can be added to handle resolution of subset/core component interactions
      - a decision network can be applied to determine when sub-sets or core component interactions need resolution, like reducing common factors of input variables with standardization
    - combination:
      - integration structures can include a decision network to determine when components should be combined
    - filters:
      - filters or limit structures can be used to reduce what doesnt contribute to prediction accuracy or cant be applied by default (solution cant merge variables from alternate functions)
    
    - the core structure of a network is 'connection', with related structures 'position', 'direction', and 'filters'
      - a problem & solution can be 'connected' using a structure (like a sequence or network) of formats
      - other structural insight paths can be used to derive solutions
        - find solution by applying solution metric structures like structures of a priority
        - find solution by applying core structures available in problem space
        - find solution by applying system structures like ambiguity & incentive
      - these can also be applied to the ml problem space

  - examples of reasons why variable or object network (including all terms like a language map, or a subset of relevant terms to the problem) can be insufficient/inefficient, while it can be used as a format to solve problems in general just like vectors can
      - a language map of operators used by a set of functions would have many overlaps & complicated paths between operators, resulting in unclear directions that functions move in or move inputs toward
      - an object network cannot clearly show all the possible object states (or the system contexts producing those object states)
      - the network is an absolute reference structure with static positions of terms/variables/objects, with a definition of difference or interaction applied to the objects to determine position, a position that may not be ideal for making a particular solution path or interaction sub-network clear

      - variable networks may illustrate attributes & direction of cause, but they dont illustrate:
        - why something is true
          - 'structure of an input' may cause a variable like the 'output of an interaction with another structure', but why is that true - bc:
            - structure enforces limits on interaction potential
            - structure allows similarities & alignments between fitting structures to interact
            - structure allows contradictory structures to damage other structures
          - these are system objects that arent displayed in a variable network with an arrow between an input structure & another structure, with an arrow leading to the side effects & other outputs of that interaction
        - you can add other layers to the variable network to show variables on other interfaces like cause to display reasons why something is true, other than a factual variable network displaying known interactions of components of facts, but this makes it not just a variable network, but a stacked/layered variable network, like how a state network has multiple layers and isnt restricted to one network
        - if you illustrate all interface interactions on one network, it might be unusable bc of the density of interactions
          - the above example has many patterns associated with it, many possible intents, many attributes/functions, many adjacent potential interactions & states & other interface objects
          - these objects can be indexed on the same network, but for quick queries, sorting through all of these objects may not be efficient
          - its also not efficient to store possible adjacent states of an object or its many sets of objects that can generate it or be generated by it in the same network
        - an object network is good for showing known interactions between objects, just like a variable network is good for showing known interactions between variables
        - the merging of all these networks is not efficient, for example when displaying an attribute like 'usefulness' 
          - this attribute may be a property of many objects, and it may be a property of structures of objects (like a system of objects)
          - the attribute also has definition routes associated with it
          - these interactions would be inefficient to display on the same network
          - the structure of the attribute definition routes (on a network of standardized concise definition components like concepts on a language network) would require different structures than objects with that attribute as a property
          - the structures of usefulness defined as the attribute's definition routes may not show up exactly the same on an object or other network 
          - a definition route of usefulness may interact with concepts like relevance & alignment, concepts that may not be displayed on a merged network in a clear way that shows the definition route
            - relevance & alignment are fundamental & therefore common concepts/structures that will show up frequently in a variable or other network, in structures that dont align with their definition routes
          - 'alignment' may involve pairs of objects that make up most of the network - showing how 'usefulness' is defined in terms of 'alignment' (connecting 'usefulness' to those pairs of objects indicating alignment structures) will look differently on that network than on the language network of definition routes
          - so a query to find 'alignment' in a merged variable/object network (that includes all interface components) would have to select between the language network version of 'alignment' (definition routes between concepts) and the structures of 'alignment' found on the object/variable network (pairs of objects), which is less efficient and clear than designing an interface query to select these sub-queries beforehand (like 'standardize object network to concepts & match concepts with alignment definition route')
        - a merged network implies that all possible useful info has already been generated & added to the network, whereas interface queries involve operations that can find/derive new info