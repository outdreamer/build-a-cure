- add to solution automation workflows

  - apply core interaction functions & core structures to useful structures to identify generate & apply other useful structures as solution structures
    - example: apply the concept of 'probability' to generate useful structures, which would produce a query like 'identify probability structures like patterns of useful structures'
    - workflow fit: this may have the same output as other workflows but it involves applying the insight that 'core structures of useful structures' which benefits from the usefulness of the concept of 'adjacence' in that core structures (like combination of useful structures, or patterns of useful structures) will probably also be useful structures
    - generalization: other useful concepts & interface components (like 'adjacence') can be used to generate insights (like 'core structures of useful structures are also likely to be useful') that can be used to generate solution automation workflows (like 'apply useful structures like the concept of probability to identify/generate other useful structures that can be used as solution structures')

  - identify structures of useful interface components that can be used to generate/identify other useful structures
    - example: identify useful structures (like patterns) of useful structures 
      - example pattern of useful structures: 'structures having a higher number of useful concepts it can function as'
        - example of a structure having a high number of useful concepts it can function as:
          - tangent: useful bc it acts like an 'interaction', 'intersection', 'approximation' or 'similarity' structure
    - workflow fit: this applies useful structures to themselves to generate useful identification/generation structures of those structures, given that theyre by definition useful
      - this is similar to workflows involving 'apply useful structures to find useful structures' or a variant of it, but specifically applies useful structures to create functions to identify/generate other useful structures
        - example: the function created above is 'identify structures matching the useful structure that is the structure of a pattern of useful structures "structures having a higher number of useful concepts it can function as"'
        - the 'structure' of useful structures is the 'pattern' of useful structures
      - this translates structures (like patterns or combinations or input-output sequences) of useful structures into specific structures (like 'structures with a higher number of useful concepts it can function as') that can be used for effective identification/generation of specific useful structures (once formatted in a useful format, like the 'structure-function-concept' format) & applied as solution structures in a specific problem space
      - this is generating what amounts to 'insight paths' to connect general useful structures with a function to identify specific useful structures, using connecting rules like 'standardize structures to various interfaces or interface structures, like combinations of interfaces, and then identify useful structures like patterns of useful structures'
      - it also adds the 'standardization to various interfaces' step, which adds variation in the solution space of possible useful structures to search

  - identify useful structures for depicting interface components (like change types), particularly useful structures (like 'adjacent structures in different formats'), and apply these structures to create solution structures (like structures of clarity/obviousness, where the info made clear by that structure is useful for problem-solving intents)
      - example: a graph of a topology of properties indicating the change types that break properties or produce phase shifts in properties to show change structures
      - solution success cause: 
        - why is it easier to solve problems when they are graphed? bc of structures of truth like clarity/obviousness given focus/implications produced/prioritized by certain formats
          - connections are implied with default adjacent or otherwise obvious structures, like a line that completes a triangle when added to an angle structure formed by two lines where the missing line is the same length as the other lines
          - some structures are clear in different formats, like differences or patterns being more visible from one angle than another
        - so a structure should be used to graph a component when the particular structures made obvious/clear by that graph format are required or otherwise useful
        - example:
          - a 'unit case' structure is useful to answer the question of 'whether a possible connecting rule is true at all'
          - a 'contradiction' structure is useful to answer the question of 'whether a connecting rule is absolutely true'

  - identify solution success cause of why a structure is useful (or specifically why its useful for an intent) & apply that to identify other useful structures as needed to solve a problem
      - example:  
        - some structures are useful by definition bc they 'fulfill a requirement', which is a definition route of 'relevance' which involves 'usefulness' as one of its definition routes
        - others are 'probably useful' bc they tend to fulfill a general solution or optimization metric like 'reducing complexity' or 'increasing coordination/organization'
        - some structures are only useful in a particular context, like how structures of 'counterintuition' are useful for intents like 'prevent errors' in 'complex system' contexts
      - why is a structure made obvious by a particular format? bc its standardized in some way that allows for clear implied connections between differences to be made
        - example: the example of 'two lines of equal length connected at one endpoint to form an angle' has a clear implied connection between the 'unconnected endpoints' which has another 'degree of implication' added by the 'equal distance between the unconnected endpoints' which has another 'equivalence' structure in the form of "equal length to the other two sides' length", to resolve the implied 'difference structure' that is the 'distance between the unconnected endpoints'
          - these equivalences between side lengths form an implied pattern structure:
            - two line lengths are equal
            - the distance between unconnected endpoints is a third equal side, forming a pattern of equivalences in side lengths, given the addition in the pattern sequence of another line object having an equal length attribute value, after applying a 'sequence' structure to the first two lines
          - a difference structure offers a connection between this initial equivalence (equivalence between 'existing line lengths') and the next equivalence (equivalence between 'existing & missing line lengths') by identifying the missing line as a relevant difference
            - this structure can be identified as 'missing' or a 'relevant difference' in the first place by applying:
              - the identified 'pattern structure' to generate implications like the third value in the pattern sequence
              - the 'similarity structure' in 'change type' between the endpoints and their starting point (they have a similar position away from the connected endpoints forming the angle, and this similarity in position to that connected endpoint structure makes them comparable, and comparing two endpoints of an angle structure leads to a connection between them, as in a third line connecting them)
          - these two equivalences form another equivalence:
            - the equivalence between 'equal line lengths' and 'missing line length' of the implied third line in the pattern sequence

      - workflow fit: this is similar to the 'identify problem/solution structures (like solution success cause) of problem/solution structures (like useful solution structures)' workflow, but adds the connection to 'system contexts' where a particular structure is relevant

  - identify patterns in differences in useful structures in various systems (like structures of 'requirements' in various systems) to identify variables of how a structure should be changed to become useful in a particular system, as well as the differences between useful & non-useful structures in a particular system compared to each other & compared to the system structure, to identify probable useful structures given a system structure
      - generalization: this can be generalized to 'identify the differentiating variables of the most useful structures to compare (like useful/non-useful structures, or useful structures vs. system structure, or error structures vs. solution structures) and identify the most useful structures to compare, to enable optimal identification of these structures which are directly relevant for problem-solving intents'
      - workflow fit: this is similar to the 'identify useful structures (like patterns) in useful structures to identify/generate them, & apply them as solution structures' workflow, but generalizes it to all problem/solution structures, and adds a core interaction function of 'comparison' to 'connect the structures that are most useful to differentiate', an intent that is particularly useful for problem-solving intents when applied to problem/solution structures

  - apply variance structures like 'approximation' using useful structures like 'adjacence' to other required or otherwise useful structures like 'input output sequences' to generate sources of variation to apply to problem/solution structures like workflows to generate different structures, such as 'apply adjacent transforms to input-output sequences to find adjacent input-output sequences to use as solution structures as well as the structures required to solve the sub-problems of converting to those adjacent structures' instead of 'find all currently possible input-output sequences to find solutions using only existing resources'
    - workflow fit: this is a variant of the 'apply useful structures to find probable structures of certainty/usefulness' workflow, but is specifically applying useful structures with intent to create variations of useful structures which may be more possible or more useful for some intent than using the exact/existing useful structures
    - solution success cause: this works bc existing useful structures may fulfill some high priority intents, but it may fulfill them suboptimally, so only filtering the solution space by these structures will restrict the solution space to sub-optimal solutions, ignoring alternatives that may be accessible with adjacent structures like existing conversion functions
      - this also works bc variants of useful structures may benefit from their similarity to useful structures, in that they have similar outputs (being similarly useful) while allowing for differences in intents fulfilled
    - generalization: apply useful structures like 'similarity/adjacence' to create other useful structures like 'differences' in inputs to allow for useful structures like 'differences' (useful for problem-solving intents) like an 'increase in optimization' in outputs (like 'solution metrics') - 'differences' being a specifically useful structure to apply to sub-optimal existing solutions, which are a default/input structure justifying usage of this workflow

  - identify which solution automation workflows or interface queries would produce solutions according to an optimization metric of solutions determined to be sufficient (like computational complexity, speed, accuracy or efficiency) and use this as a filter of possible solution workflows/queries generated by default methods like 'trial & error' (search all possibilities) or by applying other workflows to generate solution structures
    - if a solution method is slow, its unlikely to be an effective application of a workflow, so avoid that and exclude it from initial rounds of solutions generated/applied
    - example: in the 'find a prediction function' problem, applying 'adjacent changes' as a useful system structure is a component of various workflows/queries, but it is only useful in some positions of the problem space
      - if you apply 'adjacent changes' to the function itself, it will produce many versions of the function that require testing & further adjustments, and you would need to apply an effective search/filter method to reduce that solution space, like applying 'adjacent changes' to known positions of limits of the function where it wouldnt likely have structure
      - if you apply 'adjacent changes' to other problem space components, like 'variable structures' or 'data sets', you may generate useful output for various intents like 'identifying variable structure causation' and 'data augmentation'
      - if you apply other useful structures like 'base' to the 'adjacent change', to find 'adjacent changes that produce a base' to the function structure, that could be useful bc then youre looking for a conversion function between functions, which is a quicker task to solve & more optimizable
      - by definition, 'methods that use a lot of resources' are not usually optimal problem-solving methods, so that can be used to filter the set of generated solutions created by applying useful structures or by other methods
    - applying 'adjacent changes' to known solution structures is one way of creating efficient methods, bc by definition 'adjacence' requires less resources, so these are likelier to be efficient solutions, if they qualify as solution structures according to other metrics

    - workflow fit: this is similar to 'apply useful structures of solutions like efficiencies to generate/filter solutions' but is specifically prioritizing low-cost optimization metrics as a required input to generating/filtering possible solution structures (including workflows/queries) to avoid applying any queries/workflows that are unlikely to be optimal in some metric, which is particularly useful when many possible solution structures are generated & need to be filtered, which applies the definition of efficiency structures like 'adjacence' to generate the insight that 'workflows/queries that arent adjacent are by definition unlikely to be efficient' and applying this insight to determine a starting point for the workflow & applying it as a solution structure filter (solution automation workflows & interface queries, in addition to solutions)
    - simplification: this can be simplified to 'if its not quick, its not a shortcut' given the definition of a shortcut, which is the structure that makes some solution-finding methods more powerful/optimal than others
    - generalization: other structures that are required inputs given the definition of solution automation workflows & other problem/solution structures can be used as a method of filtering/generating the solution space or other solution structure

  - identify alternate causes of solution success as generators of solutions:
    - the reason a 'function network' or 'state simulator' or a 'stacked alignment' may be successful as predicting interactions may not be bc of the structure itself but bc of one of its outputs, such as:
      - a 'dependency' or 'interaction' structure caused by a 'function network' structure 
      - a 'state sequence' caused by a 'state simulator' structure
      - a 'phase shift bc of a threshold crossing' caused by a 'stacked alignment' structure
    - these useful output structures can be caused by other structures:
      - a 'dependency' structure can be caused by a 'containing' structure forcing that specific 'input-output sequence' to develop, caused by for example a 'lack of functionality' rather than 'connected functions in a function network'
      - a 'state sequence' can be caused by an 'interactive, adjacent, & probable structure filter set' structure
      - a 'phase shift bc of a threshold crossing' can be caused by a 'lack of input validation' structure
    - identifying variables of the solution success cause allows different solution causes to be identified & applied as solution generators
    - workflow fit: this is similar to 'identify & apply solution success causes as solution generators' but abstracts it & applies a higher causal node as input to 'solution success cause' as the 'causes of solution success cause'
    - generalization: this can be generalized to: 'identify generators of useful solution structures or identify alternate routes to useful solution structures & apply those alternate routes or generators as generators of solutions'
    - solution success cause: this works bc there isnt usually one input to the success of a solution, allowing for the usefulness of alternate inputs to solution success, and these inputs can be generated, and there are ether useful solution structures than solution success cause that this rule can be applied to as well, to generate solutions using other useful solution structures

  - identify differences in interface queries, workflows & other problem/solution components that can be used to solve the same problem & apply those variables to generate possible/adjacent solution queries or solutions given a base solution or just given the variables
    - the output of this would include workflows like:
      - input-output sequence & 'build' core interaction function structures: 'find the "input-output sequences" that build/produce the same output solution from the same input problem, with a given range of difference in input context (robust solutions), to build/produce/process the solution structure info from the problem structure info'
        - note: this is particularly useful bc it abstracts away causality & functionality, leaving just 'available resources' (inputs) and 'outputs' as the factors to analyze, so it could be specified as 'causal input-output sequences' or 'function input-output sequences' but that may be unnecessary in some circumstances where that info is irrelevant or captured in the abstraction of the 'input-output' structures
      - format/state sequence & 'connect' core interaction function structure: 'find adjacent "format sequences" that can "connect" a problem/solution'
      - generative variable structures: 'find "generative variables" of a problem and reduce those instead'
      - interactive structures & 'build' core interaction function structure: 'find interactive/cooperative structures in a problem space and use those to build a solution'
    - a workflow that can generate all the relevant structures used to build these workflows could be:
      - identify attributes/structures/components that are interchangeable ('format', 'input/output', 'interaction') and apply them to fulfill a structural requirement (a structure that can be 'connected') to solve a problem ('connect' problem/solution)
      - solution success cause: this works bc if components are interchangeable, they can be changed to generate differences that dont alter the input/output/functionality of the original structure
      - generalization: this can be generalized to the workflow 'identify variables of workflows that dont invalidate functionality of workflows & apply variables to generate different workflows'
      - workflow fit: this is similar to 'identify differences between solution automation workflows & apply variables to generate different workflows' but is a specific variant that identifies differences in specific queries/workflows used to solve the same problem which are standardized in some way for comparison (all of the above compared workflows use 'connect' or 'build' interaction functions and a 'sequence' structure in some position), and applies useful interface components such as concepts like 'interchangeability' to generate insights like 'use interchangeables to generate different workflows' to apply these insights as generators of workflow-generating functions

  - example of multiple ways to solve a problem
    - multiple ways to solve the 'find if a particular number is in a random sequence & find its position' problem
      - iterate through the sequence & check
      - filter the solution space of all possible sequences & approximate an answer by checking various points with solution filters
        - apply the definition of generative function's 'random' definition if available to identify error types possible/expected
        - apply the general definition of 'random' and 'probability' to find filter structures of likely sequences, which has probability-based filter structures when applied to a 'random number sequence' structure, like:
          - the sequence structure is likely to not have clear patterns in it (patterns that imply that it can be reduced to a function other than a random function)
          - the sequence structure is likely to not have a pattern that applies to the entire sequence (a pattern may occur, but is likely to only occur in a subset, the larger the sequence is)
          - if most positions do not have the number, its likely that remaining positions do have the number given possible patterns not ruled out by most positions (rather than finding a long sequence that doesnt contain the number, which is less likely than most numbers appearing in a long sequence at least once)
        - apply patterns of probability patterns as filters
          - how likely is it that 'if most positions do not have the number, its likely that remaining positions do have the number' produces an error?

    - multiple ways to solve the 'answer a question automatically' problem, with the following interface query:
      - if you have question-answer pattern info:
        - apply those patterns of language map routes or other info structures in the pattern to produce the answer
      - if you have error type & solution format or known solution info:
        - identify error type & apply solution formats or known solutions for that error type
          - apply the solution format as a template with a 'missing' error type or 'identification' problem:
            - for the question 'how did they get from A to B', the 'missing' structure is a 'missing path connecting the points', and the identification problem is selecting the correct route from all possible routes connecting the points
          - apply known solutions for the 'missing' error type (like 'matching structures that fit missing structure') or the 'identification' problem (like 'apply probability-based filters')
      - if you have concept identification, system structure identification, & derivation functions & access to info like agent decision histories but dont have specific info to filter possibilities of the optimal solution format of 'who drove it'
        - derive understanding of 'decisions' objects (to drive a car) & give proxy/approximate/abstract answers based on understanding
          - for the question 'who drove the car', derive relevant concepts such as 'inputs' to 'driving a car', like 'incentives', 'functions', 'access', 'requirements', 'intents' to 'drive a car' and produce a set of abstract descriptions of the types of people with those intents/incentives & other related objects

  - apply structures of interface components as filters when predicting unknown systems

    - example: using just info about inputs/outputs of a cell and one of its components like DNA, predict all the other interactions/components/functions occurring inside a cell

      - apply higher-certainty interfaces like insights, system, core, & potential structures to generate a set of possible default components (like sub-systems or sub-components that can exist in the cell) based on insights about:
        
        - system structures like efficiencies in the form of energy usage
        - potential-impacting structures like limits in the form of contradicting rules preventing a structure from occurring
        - given info like the cell's attributes like size, the inputs available given the cell's inputs & the outputs of components & their interactions given the cell's outputs

      - once this set of default components/functions is generated, filter it by applying other interface components (like intent) to add extra certainty
        
        - interface component combination structure (intent + cause + change as sequential filters):
          - intent: this set of components/functions is possible (given a sub-system that could stabilize given how efficiency works in the form of energy usage in biological systems), but is there an intent associated with it? what agent would benefit from its existence? is there a reason for it to exist? does its outputs benefit any other system?
            - these questions can filter the set of possible components given structural system insights & core structures by intent, bc structures dont usually exist persistently if they are not useful to some other system
            - this filter could be replaced with a 'relevance' filter
          - cause: this set of components/functions is possible & there is a reason for them to exist, but is there a cause that could produce them (and would produce them, given its intent to do so)? 
          - change: this set of components/functions is possible, there's a reason for them to exist, and there is a cause that could produce them, but can it be changed in a way that would invalidate it and are these changes likely, or are there changes that would produce more optimal functionality for some higher priority intent and are those changes likely?

        - interface component contradiction structure (error as a contradictory structure of the above interface component sequential combination filter structure)
          - error: is there a way for a component/system to stabilize without these attributes or in contradiction of these attributes? (is it possible for a component to be independent of the other components but still exist within a system like a cell, like an output of evolution that isnt necessary but hasnt been removed yet, or a particularly efficient/useful component, or a component that causes problems but hasnt been handled yet bc they are lower priority or relatively lower in impact/scale of problems caused)

    - workflow fit: this applies the concept of 'multi-interface alignment' to fulfill the problem-solving intent of 'filtering the solution space', which is similar to 'applying interfaces in an interface query' but is different in that it specifically applies combinations of 'cooperative/aligning' interface components as 'filter' structures to apply when identifying 'probable solution structures' based on 'multi-interface alignment' as a source of certainty, given that 'cooperative/aligning structures' tend to be likelier to be true/real bc they add value such as 'efficiency' to a structure ('sharing resources through cooperation') so theres a reason to use those structures over other structures, and this is also similar to 'applying certainty structures as a source of solutions' but acts as a variant of both workflows that also combines them
      - applies specific certainty structures like attributes such as 'alignment' from the 'applying certainty structures' workflow
        - applies the insight that structures with attributes like stable/cooperative/efficient are likelier to be true, and applying this insight & related certainty structures to 'interface components'
      - applies 'apply interfaces in an interface query' workflow, but specifically using interfaces that are organized by certainty structures:
        - "combine interface components in a way that fulfills the 'multi-interface alignment' or 'cooperative' structure"
        - a specific highly useful interface query like this can function as a solution automation workflow
          - not all interface queries are useful for all problems, but some are, like this one, so it qualifies as a solution automation workflow
      - combines the two workflows (with a standard problem-solving workflow of 'filtering the solution space'):
        - apply the interface query of "filter the solution space by applying 'structures of multi-interface alignment composed of interface components'"
      - this may be the same output as the output generated by another interface query or solution automation workflow, but it differs in route applied to generate it (the 'combination' structure of solution automation workflows), and this route can be used to generate other workflows/queries

  - output difference:

    - this workflow's output interface queries differ from other interface queries

      - example: applying the 'break a problem into sub-problems & merge sub-solutions' workflow generates an interface query like:
        - sub-problem: identify solution space of possible components
        - sub-problem: reduce solution space (identify probable components)
          - sub-problem: identify components that can be used as filters or components of solutions
            - sub-problem: apply 'concept' interface, specifically the 'probability' concept, to identify common components in other structures of bio system
            - sub-problem: apply 'core' interface to identify core components in other structures of bio system
            - sub-problem: apply 'pattern' interface identify patterns in differences between containing structure & component structures, from other structures of bio system
          - sub-problem: apply filters to possible structures to identify remaining probable solutions or apply components as possible structures to build probable solutions
        - sub-problem: check if solutions pass solution metrics

        - note: the indentation indicates the integration/merging level of sub-solutions

      - this interface query also identifies filters of possible components, but using a different route, bc it implemented a different solution automation workflow ('break a problem into sub-problems & merge sub-solutions') as opposed to the other workflow ('apply structures (like combinations) of interface components fulfilling metrics like 'cooperation/alignment' to build/identify structures that fulfill metrics like multi-interface alignment as a source of certainty')
      - this workflow can be generalized to:
        - 'apply structures of interface components fulfilling useful metrics to build/identify solution structures that fulfill any useful (like 'certainty') structures involving interfaces'

  - apply general useful structures to find probably relevant useful structures in the problem space, as a way of filtering problem-solving intent fulfillment components, like solution inputs or solution components
    - this is a proxy for knowledge, to replace rules like 'augment the data set with variants of it' with general structures like 'apply core structures like difference to all problem inputs like data sets' and 'find error types like false similarities with alternate causes' (in relevant problem structures like 'equivalent data sets with different causes of their patterns')
    - an example is applying 'difference' & 'average' to find 'difference from average' as a relevant structure that may be an input to the 'prediction function solution'
    - solution success cause: this works bc these general structures are known to occur & be useful across systems for variable problem-solving intents and are therefore likely to interact, meaning they are relevant for finding & connecting interactive structures of a problem/solution, by applying the general structures to the problem space to find specific relevant structures to connect
    - workflow fit: this is an extension of the 'apply useful structures' workflows, with the application of additional filters for relevance of structures, like 'contradicting error structures' and 'position' of the useful structures once specified for that problem space
      - for example, positioning a useful structure in the input data may not be correct for the 'find a prediction-function-finding algorithm' problem bc that may not be a meaningful position where it is useful to other structures

  - example of finding useful structures like 'average' to find relevant variables like 'difference from average' in a 'find a prediction function' problem
    - if a simple algorithm identifies 'difference from an average' as a variable of a prediction function for 'health problems', other useful structures may be applied to align the meaning of the structure, like 'abstraction', so the 'difference from an average' is not applied to the 'specific' average of that initial training data set, but rather from the average of 'any' data set (calculated dynamically according to the data set average value), if thats relevant for the specific problem, and if error types of that structure are not relevant
      - this would indicate that its any difference from the average that leads to differences in 'health', rather than a specific difference from the average of a particular data set 
        - but if the specific average happens to be the same as the absolute average or is a common average across data sets, this adjustment may not be necessary to the simplistic 'difference from average' prediction variable
        - adjustments to the simple structure should be made where necessary to improve problem-solving intent fulfillment (improve the prediction function)
      - often a problem wont be as simple as finding one particular important structure
      - this simplistic algorithm that just checks for 'difference from average' to predict 'health' would have errors like missing relevant structures such as:
        - 'difference from average in an interactive community'
        - 'difference from absolute average'
        - 'skewed data filled with outliers from many communities, which seem like the average in the skewed data set'
    - how would it identify 'average' as an important object for the 'find a prediction function' problem?
      - this problem has sub-intents:
        - 'find differences between data points', which has sub-intents:
          - 'compare data points'
            - which has related objects like 'average' or 'standard'
              - these related objects enable comparison, similar to 'centering' a line between points or 'normalizing' a variable value, which remove the common factor by creating an average in the form of a common distance from the regression line or average value (0.5 between 0 and 1), allowing the remaining non-common difference (which the two data points dont have in common) to be clearly identified (difference from regression line or difference from 0.5, rather than difference from 0), bc the average value has meaning in the form of being more common so difference from it also means a 'lower probability', not just a 'difference in value'
      - the 'average' can be important for the 'find a prediction function' problem bc it allows for comparison between data points to find differences that can be used as variables to build the prediction function
      - in standard terms, the 'average' can be useful bc its a possible relevant object/input/sub-intent to a sub-intent (compare) of a sub-intent (find differences as variables) to implement a method (build) of fulfilling the original problem (find prediction function)
      - so we know the 'average' can be a useful structure - how do we know 'difference from average' is a relevant variable of the prediction function?
        - we are looking for variables, which by definition involve changes to values, so applying 'differences' to already identified useful structures is a way of finding possible variables
        - then, applying standard methods, 'difference from average' is a relevant variable if it has predictive value across data sets
    - we are making assumptions:
      - the data set involves interactive data point sources
      - other useful structures dont apply, such as 'alternate causes'
        - the reason an outlier may have differences in 'health' may not be bc of 'differences from average' but another cause like 'randomness' or 'interactivity between similar data points' (meaning interactions with similar data point sources is an alternate cause of health differences)
    - so applying known general useful structures:
      - alternate causes
      - abstraction
      - assumptions
      - core structures like 'average' & 'difference'
    - to a problem structure & input (data set) creates relevant structures to the 'find a prediction function' problem, like:
      - relevant data set differences (variables)
      - differences between data sets (alternate samples)
      - 'difference from average' as a predictive structure
        - variants of 'difference from average' by applying:
          - relevant attributes like 'interactivity'
          - variants of input structures like a 'community data set' rather than a 'data set distributed across non-interactive communities'
      - error types
        - 'false similarities' in data sets
          - two data sets may be equivalent for different reasons ('alternate causes'), such as one group is healthy in one data set bc of missing variables, like proximity to lab testing center, and is healthy in the other data set bc theyre more average, and healthy in another data set bc they interact more & share medicine
        - 'many outliers in a data set' indicating a 'false signal' error type of the 'absolute average'
        - 'missing variables' from the data set
        - 'missing variation' in different data sets
    - these structures are necessary for creating a robust algorithm to create a prediction function, bc a typical algorithm will not infer all of these structures bc they are not directly in the input data set points, variables, or their direct interaction spaces containing their connections
    - these structures can be embedded with pre-processing in the input format, but that can lead to problems if processed/embedded incorrectly according to the processed structures' actual meaning
      - using '0.5' as input instead of the 'average health data point', which may or may not be a predictive structure ('difference from average') and may be calculated incorrectly (should be a difference from a different average) so embedding it with a manual standardization may lead to other errors
        - this would inject a certainty in the input data that may be an error or an uncertainty or may not be resolvable with the existing data
      - also, pre-processing for one of these structures may contradict pre-processing for another structure
    - an algorithm would identify & apply these general useful structures to create relevant useful structures for solving the problem, adjusting the structures as needed to improve the problem-solving intent fulfillment

  - example of identifying useful info formats for intents & apply to fulfill those intents given the metric those formats fulfill (like 'minimizing cost' or 'reusing resources')
    - rather than a language map, other useful info formats may include:
      - a map on a different interaction level, like 'core components':
        - a component language map (common core components of other terms) with words graphed as queries of the map
          - this would be useful for intents like the following, bc these involve adjacent attributes/functions/structures of this info format:
            - 'build a language map query like a particular word query'
            - 'find similarity between words'
            - 'generate a new word'
        - a network of common query structures on the language map (what are the words used for, like common sentence patterns such as questions)
    - rather than an attribute graph, containing a network of grouped attributes as objects:
      - a network of attributes as nodes
      - a network of individual attribute networks
    - rather than a causal network diagram:
      - a causal diagram with specifically n-degrees of causal inputs mapped
      - a set of vectors pointing in the same direction of other caused nodes (as vectors) with magnitude indicating input directness degree (root cause being shorter to indicate earlier position in the causal sequence)
      - clustered nodes by the outputs they have in common
    - rather than a system network diagram:
      - a network of functions that can generate/describe it
    - rather than a variable network:
      - a set of example system networks that include those variables as attributes of objects as network nodes

  - apply difference patterns between assumed/implied/actual meaning as a way of correcting false implications/assumptions to connect structures without explicit known connections

  - apply differences that cause errors in the fewest possible cases as a way of changing a problem into a solution or changing a solution to a similar problem or changing other problem/solution structures for problem-solving intents

  - identify & apply non-standard intents as ways of finding useful structures for problem/solution intents like 'core interaction function' intents
    - example: apply alternate definition routes of 'power' & 'truth' to fulfill core interaction functions like 'connect' to fulfill a non-standard intent of 'power structures' (intents such as 'finding truth structures', like facts)
      - alternate definition routes of 'power' include:
        - 'ability to produce changes' bc power indicates an input to functions, where functions produce changes like new/altered components
      - alternate definition routes of 'truth' include:
        - 'stability/reliability'
      - implied related power/truth insights given connections of definition routes
        - if a structure is powerful, it can produce changes
        - an absolute or input truth cant be changed, even by powerful structures
        - if many powerful structures cant change a particular structure, that indicates the structure is true (true in the form of 'stability;reliability')
      - so 'power structures' have a non-standard intent of 'finding truth' by applying 'power structures' to produce changes (an output of power) to see if some structure can be changed
        - this intent connects alternate definition routes & implications of 'power' and 'truth'
          - these concepts are by default connected in the insight 'truth is a form of power' (applying this insight is not required but it is relevant)
      - how would you know to use 'power structures' to fulfill the intent 'find truth structures'
        - bc the two concepts are connected by their alternate definition routes:
          - power produces changes
          - truth cant be changed
          - power can used to determine if something can be changed
            - if it cant be changed, its true
        - the important part is identifying 'change' as a connecting structure between the two concepts, and identifying that one (power) produces an opposite structure (change) of the other (truth, which is stable/reliable), so it can be used to find 'not truths', and therefore can be used to find 'truths'
        - so 'power structures' have a non-standard intent of 'finding truth' which is a relevant intent to problem/solution intents like 'filtering the solution space'
      - this can be generalized to identify & apply useful structures with non-standard intents that produce other useful structures like 'truths' relevant to problem-solving intents
      - workflow fit: this involves identifying useful structures for problem-solving intents (like truths) as a problem to solve ('find useful structures for problem-solving like truths'), in addition to other general problems to solve when problem-solving (like 'filtering solution space'), which differentiates it from other workflows, bc these useful structures are inputs to general problem-solving intents so they can act as a new general problem-solving intent

  - once problem/solution structures are combined & filtered for workflow structures/types like difference-maximizing workflows or base workflows, and further filtered by which combinations fulfill problem-solving intents, further filter the set by which combinations fulfill differences that are also structures of relevance for those problem-solving intents
    - workflow fit: this is an extension of the workflow 'combine problem/solution structures (like core interaction functions & error structures & workflows) or change workflow input variables to create a set of all possible workflows & filter the possible workflows by metrics like relevance to problem-solving intents & interactivity', extending it by an additional filter, to filter out workflows that differ by structures that are not relevant to some problem-solving intent
      - if a workflow differs from other workflows, it should differ in a way that is relevant to fulfilling a problem-solving intent (some workflows may differ in ways that are not useful for a problem-solving intent, so this filters them out)

  - identify which interface structures fulfill problem/solution structures on different interfaces & apply those structures to fulfill those problem/solution structures
    - example: identify which intents & intent structures (like which sub-intent structure solution metrics help fulfill the general solution metric intent) fulfill solution metric intents (like 'keep this variable below this threshold value') & apply these intent structures as solution filters or solution components
    - this applies across interfaces:
      - identify intent metrics that help fulfill a problem/solution structure metric (like a core interaction function or a solution metric)
      - the general version of which is 'identify attributes that help fulfill a problem/solution structure attribute'
    - solution success cause: this works bc the same format is being applied to fulfill core interaction functions for problem/solution structures, and some formats can be used to fulfill core interaction functions on their own
      - this should allow structures that are not standardized to the same format to be connected as well, using the version of a format in that structure
        - example: you dont have to standardize every structure to the 'intent' or 'solution metric' attribute interface if you know the corresponding structures of those structures in the current formats
    - workflow fit: this is similar to 'standardize to the same format by applying an interface and apply interaction rules of that interface to determine structure interactions' but is specific to the interface structures that help fulfill problem/solution structures & allows them to be mixed with other formats using corresponding equivalents rather than forcing all structures to be standardized, and involves matching structures of problem/solution structures with those of other problem/solution structures (matching 'intents of sub-problem solutions' with 'original problem solution metric intents'), given how the fact that a solution metric of a sub-problem may be relevant to the solution metric of the original problem, which is a possible structural relevance that allows these components to be relevant in a way that allows one (sub-problem solution) to predict the other (original problem solution), so that structure (sub-solution/solution metric) is a useful format to use to fulfill interaction intents (like 'connect') between these structures (sub-solution metric & solution metric)
    
  - derive & apply interaction rules of useful cross-interface or useful interface components (efficiencies/alignments)
    - to answer questions like:
      - with what 'frequency' can a 'random combination' of 'alignments' create an 'efficiency' in a given 'system'?
    - having variables:
      - solution formats
        - certainty (probability, frequency, average) 
          - format solution as 'certainty of a particular structure given inputs' rather than other formats like 'method to generate a particular structure given inputs'
      - solution filters
        - requirements
        - intents
      - problem space
        - system context
      - interactions
        - applicable connecting logic rules
        - attributes (random, adjacent, difference-maximizing, similar, opposite)
        - structures (combination, subset)
      - interacting components
        - useful components (alignments, efficiency)

    - identifying useful cross-interface interaction rules
      - examples:
        - 'merge difference types' function
        - 'common relevant efficiencies' structures
        - 'find simplifying/generative/change-generating function' function

      - how to identify useful cross-interface interactions:
        - structures of useful components from one interface applied to another interface
          - example: useful structure interface components (combinations) of useful system components (incentives, ambiguities) applied to other interfaces
        - useful interaction structures of relevant problem-solving structures (like core interface structures or problem interface structures like errors)
          - 'merge' is a useful interaction structure of error structures like 'difference types'
          - 'common relevant efficiencies' combines multiple useful interface structures/attributes to create a useful structure for 'fulfilling an intent', which is relevant to 'problem-solving'

  - identify & apply generative functions of cross-interface interaction rules (like structure-concept interactions) & useful cross-interface interactions & cross-interface interactions between useful structures
    
    - example:
      
      - structures that generate function to 'merge difference types'
      
      - structures with conceptual attribute 'relevance':

        - a 'database index' structure gathers & searches just the information useful for 'find' intents based on structures of standards like 'what is commonly searched for' and 'what varies across data' and 'what combination of fields uniquely identifies/differentiates data', so it can be used to fulfill intents like:
          - intent: 'identify unique records'
            - fulfills 'unique combination' standard
          - intent: 'find records quickly by only searching relevant data'
            - fulfills 'what is commonly searched for' standard
          - intent: 'differentiate records'
            - fulfills 'what varies across data' standard
        
        - applying the above structures of standards as solution filters can find the 'database index' structure in the 'search database' problem space

        - the solution creates an efficiency for the 3 'search database' intents, while creating inefficiencies for the 'minimize storage' & 'generalize solution' intent
          - these intents could be fulfilled with modifications to the 'database index' solution:
            - 'minimize storage': store static values in a nested index
            - 'generalize solution': apply 'database index' solution dynamically or for other queries or query types

        - interface query to generate implementation of this cross-interface interaction
          
          - identify variables
            - core variables
              - data set format
              - data set stores
              - data set store format
              - data set store fields
              - data set store records
              - search method
              - sort method
              - storage method
              - query
            - variable combinations
              - data set/store-query relation
              - search-sort relation

          - apply requirement filters to solution space:
            - iterating data of some format is required
          
          - apply variable expansions to solution space:
            - apply probability interface to 'input/output formats' structure: 
              - data doesnt have to be in original input format or output format of 'common' queries
            - apply change interface to 'input/output formats' structure: 
              - data doesnt have to be in a static format
            - apply structural interface: 
              - data can be in 'multiple' formats
              - data 'subsets' can be stored in addition to the 'complete' data set

          - apply structure filters: 
            - what structure in the 'search database' problem space can identify unique field combinations, enable quick searching for common searches, and differentiate data (even if it differentiates them incompletely, or to a contextual definition of 'differentiate')

            - apply differences to variable values to generate core adjacent combinations of variable values
              - store subset of rows for a query type (search only these rows for queries involving these fields/operations/conditions/values)
              - store subset of fields (search only these fields)
              - store subset of example rows with allowed variation (search for these examples for these queries & anything adjacent to the example)
              - store data patterns to search for a query (search for these patterns for these queries)
            
            - filter combinations by usefulness for 'search database' problem intents
              - apply filter with 'system optimization' ('only add required functionality')
                - which of the combinations fulfills the above 3 intents without fulfilling extra unnecessary intents (aligned demand/supply of intents)


    - identify & apply interaction structures (like core interaction functions such as 'connect' or core interaction structures like 'combination') to solution structures (like solution automation workflows or known solutions) to solve general related/sub-problems of problem-solving ('filter solution space', 'select solution metrics') to generate solution automation workflows
      - related component (workflow) fit (position in network of related components): this formulates the task of 'generate solution automation workflows' as a set of problem structures (like related/sub-problems such as 'filter solution space') to solve with queries (like 'find solutions to the problem of filtering the solution space' or 'apply interaction structures of known solution structures to create new solution structures to fulfill general problem-solving intents like filtering the solution space')

    - identify 'difference' structures between known problems/solutions & apply them to possible solutions to see if it becomes more like a solution/problem structure, to test if it fulfills a solution format
      - if you apply 'differences' known to differentiate problems/solutions to a possible solution, it should become more like a problem if its originally a solution, so this is a way of identifying solutions

    - add 'isolate/separate' to core interaction functions in problem/solution interaction space to create solution automation workflows, since creating barriers to separate a problem from the problem space or separate a problematic structure from its required inputs is a possible solution format

    - apply 'implications' as local units of interaction & meaning, since an 'implication' is a logical extension of a rule system that makes sense ('fits the system' or 'has meaning in the system'), unless contradicted by some interaction
      - any structure (like conclusions/insights/facts) can be units of interaction/meaning as well, but implications fill in the gap when there isnt a known factual rule to predict interactions between structures
      - example: if a function fulfills a specific 'reduction' intent (like 'reduce a sequence'), that implies (but doesnt mean) it can fulfill a general 'reduction' intent (like 'reduce any input'), but the implication means this function can be tried as an initial possible probable solution, before trying other functions that dont fulfill any 'reduction' intents, and its likelier to fulfill the general intent, unless other solution structures/workflows identify more optimal functions for that general intent
      - implications are a useful structure to specifically fulfill the 'connect' core interaction function on the problem/solution interaction layer
      - this can be generalized to find other structures that are useful to fulfill other problem/solution core interaction functions, or other intents in general, and the structures that fit the best for a core interaction function or another intent can be derived rather than configured in a database (stored in a static known rule like 'select implications to fulfill connect intents')
      - workflow fit: this is similar to 'applying solution structures as components of a solution or to fulfill a core interaction function' or 'apply relevant structures for problem-solving intents' but specifically identifies the structural alignment between 'implications' and the 'connect' core interaction function as a useful way of selecting useful structures (implications) to fulfill that intent when other common problems (like lack of information) to problem-solving in general are present

    - identify general structures associated with solutions to general/core problem formats (efficient filters, optimal routes, cost-minimizing structures), even if theyre not directly relevant to the original problem format, and apply them as default or component structures to build a solution out of or apply them to fulfill a core interaction function of the problem/solution structures (like 'reducing' the problem)
      - workflow fit: this is different from the 'apply solution structures' workflow bc it abstracts the solution structures (applying general solution structures) & connects them to the structural interface (the system-structure interface structure of 'efficient routes', as opposed to the system interface structure of 'efficiencies', or the structure interface structure of 'shortest route') & applies all of them as components of the solution, rather than filtering solution structures to select directly relevant structures to the problem format
      - this workflow can be generalized to include other cross-interface structures to connect structures in a useful way across interfaces, given that the problem/solution structures may not be in a particular format like the structural format or the abstract format, so cross-interface structures help fulfill intents like 'connect' without applying the interface to all the problem/solution structures
        - example of applying other interface structures:
          - adding other general system optimization structures like 'stability' can fulfill intents like 'filter' better, like to 'filter/reduce the solution space' for general intents like 'finding solutions quickly'
            - a 'stable efficient route' is likelier to be optimal bc the solution will be robust to change as well as minimizing cost & fulfilling the original problem intent of 'connecting source/destination nodes'
          - adding other interface structures like 'cause':
            - a 'highly causative efficient route' indicates the route is a powerful input to other structures, which can be useful for solving related similar problems like 'connecting adjacent source/destination nodes' given that the route has a high usage rate which implies it may be used to connect other nodes, so this structure is useful on the problem/solution interaction layer (useful for other related similar problems)
      - solution success cause: this works bc cross-interface structures are useful for fulfilling intents like 'connect' without applying the interface to all structures, and bc the cross-interface structure is likelier to be interactive with any given interaction layer than either of the one-interface formats

    - apply structures of meaning to find relevant (useful, interactive) problem/solution structures (like core interaction functions such as 'connect' that fit with structures like 'causal sequences') & apply those to fulfill problem-solving intents, solution automation workflow intents, interface query design or sub-query intents, or core interaction function intents
      - there are many different structures of relevance in the problem/solution interaction space, such as inputs (problem cause, error types) and outputs (solution metric filters, solutions)
      - this is why there are many different solution automation workflows, and many differences in applicable workflows even between equivalent problem/solution formats
      - the relevant structures are 'meaningful' to find a 'solution' for the 'problem', sometimes meaningful by 'connecting' or 'merging' the problem/solution, by 'building' one or the other, 'deriving' one from the other, 'invalidating' one or the other, etc
      - structures of meaning could be a 'causal input/output function sequence' for a 'connect' core interaction function, or a 'filter function sequence' for a 'reduce' core interaction function applied to the 'solution space' in the workflow
        - these structures are meaningful bc they are useful to each other in some way, such as interacting bc they fit together (like how an input interacts with a function bc it fits its input requirements)
      - the workflow would involve deriving structures that fit together in a meaningful way to fulfill a core interaction function intent & using those as building blocks of solutions, or that fulfills a solution automation workflow intent
      - workflow fit: this workflow generalizes other workflows & adds applicability to multiple layers of the problem-solving intent stack

    - reduce problem/solution structures into known error structures & check the produced variants of those structure to see if they have known solutions, if they generate a solution, reduce/change the problem or the need to solve it
      - example: for the 'find a prediction function' problem, reduce the data set by 'human error patterns', the output of which may overlap with the output of other commonly applied methods like 'data augmentation', 'data reduction' or 'data subset sampling for cross-validation'
        - reducing the data set in this way may reduce the problem of 'finding a prediction function', or the data set may not have much variation once 'human error patterns' are removed from the data set, reducing the need to solve the problem with a more complex method (the resulting data with 'human error patterns' removed may have a clear pattern that simple methods can output a prediction function for)

        - if a complex/noisy data set can be reduced into a straight line by reducing it by 'human error patterns', 'outlier patterns', 'noise patterns' or with operations on variables to correct variable error types (like removing redundant variables or combining variables into a type), the problem structures may have been distorted and may be inaccurate
          - this applies an insight like 'simpler connections are likelier bc they require less work (are more adjacent) and stable systems tend to minimize cost to maintain stability' (Occam's razor)
          - if an insight indicates the possible presence of an error structure (like a structure of improbability) in a problem/solution (such as an improbably complex/noisy data set), that error structure should be addressed & the problem should be attempted to be reduced by/broken into/converted/connected to (depending on the workflow & core interaction function applied) that error structure, before trying to solve the original problem
          - the first step of this workflow can include a query for insights to detect errors in the original problem/solution structures
      
      - other structures of 'human error patterns' can show up in other problem/solution structures than the 'problem input variables/formats' (like problem/solution assumptions, the problem statement, the solution metrics selected) which may distort the structures, leading to a pointless query bc the problem statement or solution metric is inaccurate

      - workflow fit: 

        - this is similar to the 'break a problem into sub-problems' workflow, but its specifically trying to break down the problem into sub-problems that are 'known error types', to see if the problem is created by (or equal to) a structure like a combination of errors which act as components constructing the problem
          - this can be generalized to other error structures than 'combinations' and matched with the appropriate workflows:
            - check if 'combination of errors' = 'problem': uses 'break problem into sub-problems' workflow, bc a 'combination of errors' can be used to build a 'problem', so a problem can be broken into those errors
            - check if 'sequence of errors' = 'problem': uses 'find root cause of problem & solve the cause instead' workflow, bc an 'error sequence = problem' allows analyzing 'problem cause'
        - this is a variant of the 'apply error structures to problem structures' workflow, except it includes a function to select a solution automation workflow ('break problem into sub-problems') based on the error structure ('combination of component errors') to connect the problem/error structures, and the injection of insights that can identify error structures in problem/solution structures

    - identify solution input variables that maximize differences in solution output or identify solution output types & weight them by some metric to integrate/filter them into a solution
      - example: for the 'find a prediction function' problem, find the most differentiating variables (number/value of constants/exponents, number of conditional/sub-range functions) producing the most differences in solutions ('constant linear average function', a 'highly variable function intersecting with the most data points', a 'piece-wise function describing threshold-based phases of the function') and weight these solutions to integrate/filter them according to a metric (like 'probability of the prediction function occurring with these input variable types/dependencies/correlations & data set patterns') to create an output solution
        - the metric to weight them needs to be relevant to the problem/solution structures
          - probability of a particular solution function is relevant to the operation of 'weighting' that solution function
            - 'weighting' function involves assigning a ratio to an input's power to imapct the output, based on how likely it is to be equal to the output (given that we're comparing the same data type, an input solution function and an output solution function)
            - a 'probability' is a 'ratio' data type, so its relevant to the 'weighting' function which requires identifying & assigning a 'ratio' to an input
            - what inputs are used to calculate the probability is a different problem, but the inputs to the probability should also be relevant to problem/solution structures
              - using 'function/variable patterns' is relevant to predict the probability of the occurrence of a function having 'equal/different patterns'
      - workflow fit: this is similar to 'find the maximally different filtering structures that will filter solution space the most efficiently', but is applied to 'input solution variables' rather than 'solution space filters'

    - apply error cause once an error is identified as a way of determining understanding of the problem system & decide whether to solve that error type or adjusting solution based on error cause
      - example:
        - an outlier would create an error in inaccuracy of predictions if incorporated into a prediction function, but the reason for the outlier ('error cause') may be that the connections between function variables are changing and the outlier is an early warning sign, or the outlier is the result of a set of interacting conditions creating an edge case that is expected occasionally but doesnt have to be included in a prediction function of other points & can be separated into its own conditional prediction function
      - workflow fit: this is similar to workflows analyzing 'solution success/failure cause' but identifying error cause in this case is to specifically improve understanding of the problem system so the connections between problem/solution structures are more identifiable & optimizable

    - apply error structures to problems as a neutralization structure and as a way of generating related problems (sub-problems, component problems) to solve instead or as part of the workflow
      - example: apply 'missing' error structure to problem components like data points or variables for the 'find a prediction function' problem, to solve the problem for a subset of the data set/variables instead
        - you can also apply correcting solution structures for each error type applied to get to the original solution format
          - to correct the 'missing' error structure that manifests as missing data points or variables, add an offsetting solution structure of 'iterate' & 'average' to repeat the process for other 'missing' error applications & average them to get back to the original solution format, a prediction function for the complete data set
            - data_set => missing(data set points) * iterate(missing, data_set) * average(iterations) => data set prediction function
        - other example: apply 'opposite' structure to the 'find a prediction function' problem, as in 'find functions that are not predictive' & differentiate from them to find the original solution format of the prediction function
          - apply('opposite', 'prediction function') => 'non-predictive functions' => apply ('differences', 'non-predictive functions') => 'set of possible prediction functions'
      - workflow fit: this is a variant of 'apply solution structures to solutions to optimize them' workflow
        - this specific process is similar to that produced by applying other analysis, like core analysis, which would apply a 'subset' function to various problem structures since thats a core structure, and then integrate those structures in a structure that would produce the original solution format, but its another route to produce that process

    - identify systems where a particular solution/error structure will be relevant (an error that causes damage by neutralizing required (or all alternative optional) functions, or a solution that neutralizes many error types) as a way of filtering relevant solution/error structures from the set of all possible structures, to find structures that are relevant to avoid for the input problem system, & add as a solution metric filter or solution component structure, in case the solution metric filters are incomplete/missing or solution components are not already identified by other methods
      - error type of 'assuming simplicity (failing to identify complexity)' can be caused by an error type of 'failing to identify the impact on complexity of the output of assuming simplicity'
        - when you assume simplicity, it reduces the incentive/benefit for an agent to be complex, and reduces the probability that they will be complex, causing a reduction in complexity or a reduction in showing complexity, if they were initially complex
        - the assumption causes itself to be likelier to be true (self-fulfilling prophecy) if the assumer is more powerful than the assumed & if they are connected by a mutual/common investment
        - this assumption can cause cascading errors in a system with a conceptual-probability interface structure like 'powerful outliers'
        - so solving a problem in a system with this condition ('powerful outliers' or the 'potential for powerful outliers') will require applying a solution/optimization structure to offset that error type, and a solution without this structure may not be optimal unless other structures of the solution provide a substitute/alternate for it
      - find error/solution structures that can replace other structures and identify the most-reduced set of structures that can fulfill the most functions without neutralizing any relevant solution structures in a system
    - workflow fit: this is similar to the 'find/derive error/solution structures & apply them as filters/components of solutions' workflow, but includes a function of 'finding useful alternate reduced error/solution structure sets' to fulfill relevant error type filters or solution structures, and 'finding systems where these structures are particularly relevant' to avoid applying irrelevant structures
    - why this is useful: many solution/error structures are possible, and its useful to filter them to reduce the operations required or reduce the solution space
    - solution success cause: this works as a filter bc some structures are only relevant for some systems

    - apply interaction (connection functions, structure-building) & neutralizing (structure-invalidating) structures & apply them to generate the sets of possible systems to allowing identifying the true structure of a problem system (how a problem system works)
      - once the sets are generated, filter the possible system structures by other structures like probabilities, patterns & prediction tests ('can a known system connection function be predicted by assuming this system structure')
        - prediction tests can be filtered by relevance ('predict important system connection functions, such as functions that are inputs to many other functions or required functions')
      - why this is useful: misunderstanding a problem system is a major source of error types and solving that problem can solve other problems like 'finding a specific connection function', just by knowing how structures can interact given relevant neutralizing structures, which allows identifying possible & probable problem system structures
      - workflow fit: this is a specific variant of the 'apply useful structures like interaction structures to problem/solution structures' but is applied to the 'problem system' structure as a particularly useful way of drastically reducing other errors, given rules about how interaction/neutralizing structures can create systems, indicating its an important input to the problem system, meaning its particularly useful relative to other problem/solution structures, & solving for this error type in understanding the problem system can drastically reduce other errors to the point of not needing to solve them
      - solution success cause: this works bc if an interaction structure cant exist in a system given other neutralizing structures, its not relevant and doesnt need to be included in the solution space, allowing reduction of the solution space

    - apply rules/patterns of solution connections to identify when a more optimal solution is adjacent to a sub-optimal solution (to offset the error type 'failing to pursue a direction when a more optimal solution is adjacent bc of prior cost from that pursuit')
      - just like the 'identify absolute minima/maxima' problem solved by methods like 'gradient descent', optimal solutions are often adjacent to the selected sub-optimal solution, but arent found bc of lack of understanding of solution patterns, connections & probabilities
      - solutions follow patterns revealing rules in their connections
        - some solutions are connected by an adjacent function, like an 'opposite' function or 'adding a variable/constant'
          - applying common differences determining the difference between sub-optimal & optimal solutions is a good way to find an optimal solution quickly once you have an initial sub-optimal solution with methods like 'approximation' or 'subset prediction'
        - applying 'standard function subset difference' structures to a standard/initial solution (like a function predicting a data subset) to produce 'maximally different' or 'highly probable given function patterns' possible alternative functions & checking which of these functions are good predictors for randomly selected data subsets is a good way to reduce the solution space
        - assuming that a consistent lack of value added by pursuing a particular direction will never produce value is a fallacy
          - just like assuming that bc you havent found a local maxima in a particular direction for a consistent period doesnt mean there isnt one, and bc functions & variables have patterns, maxima pattern probabilities can be used to predict their positions rather than applying random checks or applying a particular cost/benefit ratio to each pursued 'direction' variable value
        - solutions are rarely an idealized simple continuous function 
          - more often in reality a solution function for the 'find a prediction function' problem might be:
            - a set of subset functions, indicating emergent phase shifts at different threshold values of the independent variable
            - a set of conditional functions connected by a network organized by input conditions
      - applying these solution-connection rules to find adjacent optimal solutions to sub-optimal solutions is a way to work around the problem of 'trial & error (trying every combination)' to filter the solution space
      - workflow fit: this is similar to the 'apply changes to solutions to find other solutions, based on differences in intent/structure associated with those changes' but applies the pattern interface to these solution structure connections to inject the useful interface component 'probability' into the success of solutions, given the applied solution success cause 'patterns indicate a consistent connection that may be useful as a prediction rule to reduce uncertainty' (so applying connection patterns as a guide of queries is a way to increase the probability of finding certainties like true variable connections)

    - identify & apply other important structures that offset error types (that are important across problems/systems) to problem/solution structures as a way of improving solutions given an initial sub-optimal solution
      - example:
        - applying 'probability' structures like patterns & other probability structures reduces the error types related to not considering probability structures
          - such as the logical fallacy of 'assuming there isnt value in a high-cost, low-reward direction bc of prior costs' which doesnt account for outliers, threshold values & resulting phase shifts, & other probability structures indicating a complex system that isnt governed by just that cost/benefit
      - workflow fit: this is a general variant of the above workflow 'apply rules/patterns of solution connections' wrapped in the function of identifying other important structures to consider when solving a problem optimally where they are not already known & deriving structures to neutralize that error type where there arent already known structures to neutralize it & applying those neutralizing structures
        - this is also related to the 'apply certainty structures & other useful structures' workflows, but in this case its applying structures that are 'required' to apply in order to avoid known error types, rather than just useful structures or structures associated with solutions or optimization structures like certainty (like 'logic', 'understanding', 'info' for a prediction problem)
        - error structures can be generated by applying 'difference' structures to solution metric or optimization structures ('different from solution' = 'error' which may be relevant, like a solution-neutralizing error type)

    - find or generate structures of difference between multiple solutions & derive solution success cause of each, and apply those differences when one solution doesnt work, based on changes in solution success cause (conditionally generate different solutions based on changing inputs)
      - example: different solutions to the 'find a prediction function' problem involve differences like:
        - differences in variability between data set points
        - number of traversed data set points of a function
        - difference between the function & corresponding data points
      - applying these variables to a sub-optimal solution when a solution is determined to be sub-optimal for a particular input condition (which may not be known) allows a rotation of the available/generated solutions to find one that is successful for a different input condition

    - apply the useful 'maximum difference' structure to all problem/solution components, like find 'maximally different solutions' and use those as an initial solution space to filter/adjust quickly, or 'maximally different solution component sets' or other problem/solution structures to assemble/filter/connect problem/solution components
      - variant of the 'apply maximally different solution filters' workflow

    - find a solution & apply error types to a solution to identify if the error types produce the expected output of an error type
      - example: 
        - for the 'find a prediction function' problem, identify an approximate prediction function & apply error types to it
          - apply the 'incorrect assumption' error type
            - does the prediction function have the expected errors if you change the assumptions (input variables & variable values, problem statement, other assumptions like that the independent variables are causative at all & not coincidentally correlated)
          - apply the 'incorrect causal structure' error type:
            - does the prediction function have the expected errors if you change its causal structures?
      - if applying these error types produces the expected errors (rather than improving the accuracy of the solution), the solution is likelier to be correct or approximately correct

    - convert problem to standard problem, find standard/base solution, and map difference types to intent to apply when a different problem is input that can be solved with differences applied to the standard solution
      - example: 
        - when a function has multiple variables, apply difference type 'partial' to a derivative function and 'subset' to the 'partial derivative' input (find a partial derivative for one of the multiple variables)
        - when you need to solve a different problem 'find a derivative function for a function of x, y, & z', that differs from a standard/base problem (like 'find a derivative function for a function of x'), apply the differences that map with the intents of those differences
          - the relevant 'intent' of adding 'multiple' structure to the 'input variable' structure (the intent relevant to the problem) is 'find a derivative for each variable'
            - to fulfill this intent, apply the intent map: 'multiple': ['partial', 'subset', 'iterate']
              - apply the 'partial' and 'subset' structures to the standard solution once the 'multiple' structure is added to the standard problem
              - in other words:
                - to achieve a 'multiple' difference type applied to the input variable count of the original standard problem, apply 'partial' to the standard solution component ('derivative') of a 'subset' of each ('iterate') of the input variables
          - an adjacent/direct intent of adding 'multiple' structure to the 'input variable' structure is ('add a variable' or 'vary an existing variable', where the variable is 'number of input variables')

      - workflow fit: this is simlar to applying the intent interface but adds a starting position of a 'standard solution' mapped to a 'standard problem', for problems where a standard solution is available, rather than for example assembling structures fulfilling various relevant intents, such as intents for sub-problem or problem component structures

      - solution success cause:
        - this works bc a standard known solution for a related standard problem offers an 'interface' problem to use as a base to apply changes to
        - it uses the fact that 'similar problems will often have similar solutions'
        - it also uses the fact that a structure 'mapping differences to intents' is useful for 'mapping solution differences to intents' where intents can be derived from the problem, as 'different solutions' is a relevant object in this workflow

    - apply useful vertex rules from other systems after standardizing to a problem system
      - example: a useful physics rule is 'like attracts like' 
        - what is a causative structure:
          - applying this to other systems indicates the usefulness of 'similarities'
            - similar objects are often found together bc they are adjacent transforms of each other, and adjacent transforms are likelier than non-adjacent transforms
            - this means 'adjacent transforms' (like minor changes to a sub-optimal solution to fit it to a new problem) are a useful structure to apply as well, as an initial solution
          - applying the 'similarity' structure to a problem system would generate workflows such as: 
            - 'apply solutions to similar problems & adjust to create similarities to the original problem system so the solution/problem fit/interact'
            - 'a similar structure to a similarity is a difference (which are only a few changes away, as they are almost opposite structures), so differences are also a useful structure that can be applied'
            - 'change the problem until its similar to the solution (connect problem/solution by equating them'
      - why is it useful (solution success cause):
        - this rule is particularly useful bc its:
          - core/fundamental (that can be used to build other rules or be used as a foundation for other rules to develop)
          - powerful (impacts many interactions)
          - abstract (can be applied to many layers/systems bc it interacts with a core concept/structure like 'similarity')
      - other useful rules can be identified using these & other attributes that differentiate useful rules
      - the useful structures causing the useful rules (solution success cause) can also be identified & applied to problem/solution components
      - workflow fit: this is similar to 'apply insights to implement a solution automation workflow', but is different in that it applies generally 'useful rules' (which have attributes like causative power, core importance, abstract structures) rather than insights ('rules useful for specific intents like generating new understanding'), and also involves applying 'solution success cause' as an attribute of useful rules that is an alternative to applying the rule itself & allows abstracting/structuring the rule so it can interact with more systems

    - find a solution that is at least partially correct (like a solution that almost works) and determine solution success cause, then apply that as a solution metric filter or a solution structure like a solution component
      - example: for the 'find a prediction function' problem, a partially correct solution function might work bc it has a 'similarity' to an 'average function'
        - once you know that, you can use that as:
          - a solution component & apply other workflows to construct the rest of the solution, while including that component as an input requirement to other workflows
          - a solution metric filter & apply that as inputs to other solution automation workflows, like:
            - 'create a structure of solution metric filters as solution requirements acting as limit structures in a solution template'
      - workflow fit: this is similar to the workflows of:
        1. 'solving one component of the problem at a time, then solving the next in the sequence'
        2. 'find a solution that is sub-optimal and apply changes to optimize it'
        3. 'apply inputs of solutions (like causes of solution success) to create solutions'
        but combines other components of the other workflows, like the 'starting position' of workflow 2, a 'function' used in workflow 1, and the general 'structure' of workflow 3

    - identify possible solution structures that are the most testable (like 'solution components'), as in the most testably true/false & apply those structures when building a solution
      - example: for the 'find a prediction function' problem, find the probable subset functions that can be proved as true/false the quickest & assemble a prediction function out of those functions or adjustments to them to fit in or fit together with the other functions (like in a subset function sequence or fit together for continuity/curvature or fit into the same function with adjustments)
      - workflow fit: this is similar to the solutions or workflows:
        - 'find the most different possibilities to form a solution space and start filtering/testing/adjusting those to find a solution'
        - 'find the vertex variables as an approximation to a complete function' 
        - 'construct solution template using requirements as limit structures'
        but optimizing for 'testability' (rather than 'important variables' or 'maximum difference in possible solutions' or 'solution requirement limits') as a solution filter to speed up finding a solution

    - apply structural interface to derive & build structures that will solve problem structures of a particular solution automation workflow and an organizing structure to integrate them once solved (like a 'problem state sequence' or a 'solution metric filter')
      - example:
        - just like the 'prisonners dilemma' structure is a structure that solves the question of 'which choice is more often optimal, as in producing better outcomes more often', and a 'pinball machine' structure solves the question of 'which structures will a ball traverse given input variable values like direction/speed', certain structures can resolve questions more efficiently than others
        - build a structure (like a network, maze, game, or sequence of gaps) to represent variables to resolve that are organized in a way that solves the problem according to the selected workflow
          - if the problem is 'resolving order of a set of variables', build a sequence of gap structures & apply changes until each variable is resolved in the right input/output sequence, such as to solve the problem of 'determining variables that cause each other in an input/output sequence'
          - if the problem structure is a 'complexity', apply complexity-reduction structures or organizing structures like mapping/sorting/clustering/prediction algorithms to organize & resolve problem structures that are complexities
          - if the problem is 'finding out which set of path steps is optimal', organize problem states in a maze structure, where error paths end in a barrier so an agent cant continue to the destination
      - these structures assign structure to workflow-specific problem structures (like sub-problems or interim problem states), then apply change & test functions until the problem structure is solved
        - 'break a problem into sub-problems & merge sub-solutions'
        - 'solve sub-problems one at a time in a sequence to help solve later sub-problems'
        - 'filter the solution space by applying highly reductive filters'
        - 'connect problem/solution with a format sequence'
      - fit with other workflows:
        - rather than applying only a specific solution automation workflow to the problem, this workflow designs a structure to solve relevant problem structures (like 'sub-problems' or 'the problem of connecting sub-problems' or 'changing problem state in a way that makes progress toward a solution metric') by applying changes (like 'move a ball around the pinball machine' or 'decide which route to go in the maze') and testing if a solution metric is fulfilled ('like hitting the right structures' or 'selecting a route that can exit the maze')
        - this is a 'mix' structure applied to solution automation workflows:
          - it injects a workflow 'apply changes & filter output with solution metric tests' into the workflow of 'apply structural interface to design a structure that can solve problem structures with available change functions in those structures' which can be applied to workflows where problem structures like sub-problems or interim problem states exist (like 'reduce problem' and 'connect problem with solution') instead of another workflow
          - other solution automation workflows can be generated by applying interaction functions like apply/inject to solution automation workflows & their structures/components, to fulfill a solution automation workflow like 'connect problem/solution' (connect them using solution automation workflows & their structures/components/interaction functions)

    - identify workflow fit attribute (how a workflow fits with or relates to other workflows) to identify new variables of workflows or change types that can be applied to generate workflows using an input workflow or identify missing workflows

    - derive other interaction functions/structures between problem/solution components to derive solution automation workflows from those interactions
      - example:
        - 'break a problem into sub-problems and solve separately, then merge/integrate into solution' works bc of the solution success cause:
          - interaction function: component function between components of a problem & the complete problem
            - a problem can be broken into components (like core functions, or components of inputs like problem variables) like anything else, and 'solving a component of the problem' makes progress toward fulfilling the intent of 'solving the complete problem'

          - other solution success causes:
            - another reason a 'component' structure works when applied to a problem is that there may be existing/adjacent solutions for sub-problems rather than the original problem, so this structure offers a different way to connect problem/solution
            - this also works bc it applies a known core interaction function of problems/solutions ('reduce') to the problem, 'reducing' it into sub-problems, so it's already moving closer to a solution given problem/solution interaction functions & their patterns

        - 'generate solution space of possible solutions & filter by solution metrics' works bc of the solution success cause:
          - interaction function: subset function between solution & solution space
            - 'a solution ia necessarily a subset of all possibilities in the solution space'
            - 'the solution space is a subset of all possibilities'
        - 'apply changes to existing solutions to similar problems & test' works bc of the solution success cause:
          - interaction function: similarity sequence
            - 'similar objects, like similar problems, will have attributes in common, and so will objects related to them in a similar way (their solutions)'
        - 'apply changes to existing solutions known not to work & test' works bc of the solution success cause:
          - interaction function: difference structure
            - 'by definition a solution that works will be different from solutions that dont work'
        - 'start from system of certainties and apply certainty interaction functions to generate or reach a solution' works bc of the solution success cause:
          - interaction function: certainty interaction layer
            - the solution is the target certainty, and applying interaction (connecting/changing) functions specific to the certainty interaction layer to an initial certainty set will generate other certainties, like the solution structure, and/or reduce uncertainties like the unfiltered solution space of possible solutions
      - these interaction structures (subset, component, similarity sequence, difference) offer different ways to connect problem & solution components, offering different ways to connect problems/solutions        
        - other interaction structures/functions can be derived/generated to connect problem/solution components in new ways
          - any interaction function applied to problem/solution components that doesnt violate their definitions is a possible problem/solution interaction function
          - filter these by which functions interact in a way that can make progress toward or fulfill a problem-solving intent (like 'connect problem/solution' or 'reduce problem')

    - apply other component structure patterns (like attributes/functions) as another problem-solution connection format
      - change a problem (input) structure until it fulfills an attribute in a problem-solution connecting attribute sequence like the following, then fulfill the next attribute, etc
        - attribute sequences
          - 'complex, organized, filtered, isolated, simple'
          - 'abstract, random, grouped, relevant, matched, compared, equated'
        - function sequences

    - apply interface analysis to identify different versions of an insight or insight pattern in different interfaces, as insights are typically inputs to other insights so they can be used to generate them

    - rather than generating/deriving specific rules to implement a workflow, pull & apply known specific rules to implement a workflow or components of it
      - example: "occam's razor" is a known rule that can be used to fulfill the 'filter solutions' component of the 'generate & filter possible solutions' workflow
        - other known/optimal attribute filters can be used as solution filters
        - solution filters that filter the solution space the most optimally for a metric like speed/completeness, in the right sequence/structure can also be derived & applied

    - generate other variants of 'solve a different problem bc of optimizations fulfilled by solving the other problem (like using existing/adjacent resources)'
      - find adjacent structures (like approximations/alternates/interchangeables/simplifications/subsets) of problem/solution components and apply those as inputs to the solution automation workflow or interface query instead
      - find overlapping problems with various problem components and solve for overlapping problems instead
      - solve for the adjacent problem of 'preventing problem inputs/outputs' or 'enabling solution inputs/outputs' instead of the original problem

    - identify optimal problem to solve & solve that instead
        
        - the 'find a prediction function' problem has a default sub-problem of 'isolate each variable' and 'check if this variable impacts the dependent variable'
          
          - this is the wrong sub-problem to solve, bc:

            - it may not be possible to isolate a variable's contribution to cause bc of a causal structure between causal variables that is neither totally independent or dependent
              - if variables are independent, they can be isolated, and if theyre totally dependent, one can be used as a substitute of another
              - example of a variable with various dependence causal structure:
                - variables considered independent:
                  - shape of the earth and the latest news headline
                    - one is very constant and the other seems highly variable, but is becoming more constant as fundamental attributes of the earth become more relevant to news
                - variables considered dependent:
                  - the function caller requiring an output of a function, & the expected function output reliably created by the called function
                    - this isnt perfect dependence bc the outputs of a function may not be produced even if the code is correct, bc of other variables like hardware, but its a good approximation of dependence given the definition route of input as a causal structure
                - variables with mixed dependence:
                  - variables involving structures of ambiguity may not be able to be isolated such as mixed causal structures like similar alternatives with unmeasurable differences having a conditional dependency between alternatives
                    - for a problem of 'predict which equivalent route gets to the destination more optimally':
                      - routes that seem independent (unrelated) may have built-in dependencies, like:
                        - 'structural similarities'
                        - 'opposite structures'
                        - 'adjacent position'
                        - 'route selection alternation preference, caused by a variation preference'
                      - if you take the left/right route around an obstacle, it may not be measurable whether you made a better decision to get to your destination, because the other outputs of your route (leaning or looking one way more frequently) may be so negligible as to 
                        - resolve themselves
                        - never be measured in the first place
                        - offset by other decisions (taking a different route next time for variation)
                      - the routes may be equivalent or the differences may be immeasurable, and the output destination is the same, but the routes may also have a dependency that makes the input route variables (like left route frequency & right route frequency) of the output destination impossible to accurately isolate
                      - they also cant be combined accurately into one variable (like 'route structure' or 'route adjacence') bc this erases the info about their dependency & any conditions impacting one route or a route's selection
                          - whether you take a route may depend subconsciously on whether you took a different route previously (with a built-in preference for variation)
                          - the route frequencies and other route variables like route structure would both have to be incorporated in this case bc they cant be reduced
                        - one isolated component of the route variable might be sufficient to predict some variables (like whether a person develops a bias toward left/right), but they wont predict other variables (like vulnerability to natural disasters only impacting one route)
                        - if you cant identify/measure the structures like prediction potential or differences/connections in the route variable structures, you cant isolate them
                      - the dependency between alternatives may also be conditional
                        - 'the left route only influences the right route if theres a condition changing their interaction or if an agent creates a dependency by choosing one based on the other'

            - everything has some connection to everything else
              - example:
                - 'temperature' or 'collusion' in a particular industry like pharmaceuticals may seem unrelated to a problem like 'predict financial instrument markets', and variables like 'stock prices' might seem more relevant, but:
                  - temperature can influence emotions, which are an important input to the stock market & other financial markets, and tropical temperatures lead to more tropical plants like coffee, and higher caffeine intake also leads to changes in emotions, and temperature also influences many commodity prices & prices influence other prices
                  - collusion can create unequal stress distribution, where market participants who play fair arent making fair gains relative to malicious players, which has an impact on the stock market & regulatory environment, which can influence other regulatory environments, which is an input to financial markets
            
            - causal variables can have different causal structures like a causal loop or causal alignments between interaction layers, but that doesnt mean the causal relationship where they cause the dependent variable is incorrect, its just incomplete
              - example:
                - incomplete causal loop
                  - 'high temperate causes lack of work ethic, causing rise in temperature from pollution'
                - incomplete causal interaction layer alignment
                  - causative variables can be causative on a different interaction layer like an abstraction level of the problem
                  - 'power is a causative concept and input is a causative structure but that doesnt mean they contradict each other - theyre aligning variables on different interfaces, bc power takes the form of inputs in the function format'
                  - 'gravity can cause storms and so can electric charge, which are aligning causes on different interaction layers'

            - causative variables can have alternate causes (they might cause the dependent variable, but they might also be replaceable with alternate causes that also cause the dependent variable, with/out them)
              - example:
                - 'any source of energy can cause a storm, not just one like wind or gravity'

            - causative variables may seem interchangeable with other variable sets, while a hidden dependency exists, so theyre both required in order to predict the dependent variable
              - 'a plant can develop according to the input variable of human intervention or the variable of biology rules' is incomplete bc human intervention is in a causal loop structure with biology, and may be considered a subset or output of biology

          - so even if you find out that a variable is correlated with the dependent variable, & created a prediction function that seems to work at some level of accuracy for now, you still havent solved the problem bc youve applied inaccurate structures/definitions/connections rather than those based on understanding
            - example:
              - hiring based on biases like race/gender may seem to work well for a while, until social mobility & economic factors change, bc those may be the real determinants of success of particular groups, as certain groups had better education bc of better economic status, and the prediction function used an adjacent cause of 'bias' instead of the root causes of 'economic status' and 'education'
            - this makes the default sub-problem of 'isolating variables & determining impact on dependent variable' a shortcut to solving the problem, but it wont always have good results, bc of these inaccuracies in handling causal structures built in to the assumptions of solving the problem that way
          
          - so the right sub-problems to solve include:
            - 'how direct is the cause of this variable on the dependent variable'
            - 'how easy would it be to convert this variable into a causative variable' (how much work would you have to do like 'applying changes' to make the variable cause the dependent variable)
          
          - this is where useful interface components like 'filters' can be applied
            - filters reduce the solution space, just like reducing the set of possible variables by 'directness of causation' is useful
        
        - to find the optimal problem to solve in this specific case, you would need to apply the causal interface to identify the accurate structures of causation (like directness, uniqueness, inevitability of cause) to identify causal relationships, rather than proxy signals of causation like 'correlation' and 'sequence'
        
        - to generalize finding the optimal problem to solve, you would apply interface components to determine if the original problem & problem structures like sub-problems are capable of solving the problem (fulfill solution metric like 'accuracy') or if there is potential for optimization by applying interface analysis
        
        - solution automation workflow:
          - in order to find out if the original/default problem structures are sufficient to fulfill a solution metric, analyze the assumptions of the problem & problem structures to check for error types in those structures, implying that optimizations in the problem structures are possible, where there are default problem structures embedded in the problem statement or pulled from definitions or common solution workflows for a particular problem
          - error types in problem structures include:
            - contextually accurate structures (like connections, such as when conditional/proxy variables are used instead of root causes)
              - like fragile/forced conditions, such as whether a correlation or an adjacent cause like bias is used as a cause
            - missing/incomplete interface components like cause structures
              - bias is caused by over-prioritization of simplicity and by economic uncertainty (finite/unequal resources, leading to resource competition, leading to the development & use of filtering rules, such as hiring decisions)
              - bias is not the only cause, as the primary root cause is economic status & education, which has a causal loop structure with bias (the ultimate root cause being physics, an interim root cause being brain structure, and a more direct root cause being lack of information/testing tools to offset bias)
              - bias & economic uncertainty are both caused by economic status
              - other causal structures exist between these variables, bc they encapsulate a vast degree of information (like history, decisions, agency, culture, habits, patterns, brain structures, and priorities), so can be treated as important or possibly even vertex variables
          - the problem structures & their error structures can be compared/connected/reduced/combined until the solution structures (like the 'accuracy' metric) are reached, to check if they can adjacently fulfill the solution metric
            - the 'isolating variable impact' sub-problem can create an 'accurate prediction function' solution format, but not in all cases of different input variable causal structures, so if the interim solution structure of the sub-problem structure of the "isolated variables' causal structures" have an error type (are incorrectly mapped to causal structures), they wont create the optimal 'accurate prediction function', fulfilling general optimization metrics like 'robustness' of the solution

    - finish applying solution automation workflows to the problem of 'generate solution automation workflows'
      - example:
        - applying 'break problem into sub-problems & merge sub-solutions' takes the form of the following when applied to this problem:
          - sub-problem: generate solution automation workflow components
            - sub-solution: to generate solution automation workflow components, find structures that can be used to build solution automation workflows (variables, structures like connection/interaction/difference structures)
          - sub-problem: find alternate inputs of solution automation workflow components
            - sub-solution: apply different interfaces & interface components like abstraction & system contexts to find alternate inputs of solution automation workflows
          - sub-problem: generate new workflows
            - sub-solution: apply interface analysis (& associated interface queries) to generate new workflows
              - example: 
                - apply the solution automation workflow 'apply useful interface components (like useful structures or system objects) to connect problem/solution'
            - sub-solution: solve the alternate problems of 'find new difference types to apply or find conversion functions between interfaces & find new interfaces to apply to find new workflows'

    - identify the errors of the perspectives generating each set of workflow variables & remove the errors to change the perspective to a new perspective that can generate other variables
      - example:
        - the perspective of problem/solution components has an error of 'over-prioritizing the interaction layer involving those components' which is an error bc it reduces the chances of finding workflows involving other components like 'differences'
        - to change this perspective into another (like a perspective where 'differences' are a core component that is likelier to be identified as an input to problem-solving components like workflows), apply interfaces or conversion functions between them
      - solution success cause: 
        - this works bc there are alternative variable sets that can generate solution automation workflows, like 'interactive components', 'function sequences', 'core interaction functions', 'causes of solution success', 'insights to optimize problem-solving', etc - all of them are not required to be used to generate a workflow, even if you can find these structures in any system bc of their abstraction level
      - solution requirement cause:
        - this solution is necessary bc over-focusing on structures that are too certain/static will prevent difference types from being injected that can identify other variables
        - its also necessary bc a variable thats adjacently structural on one interface may not be on another, so different variables on different interfaces makes sense as a requirement

    - identify causes answering the question of 'why is one problem format easier to solve than another format for a problem/problem space' & other questions relevent to problem-solving & apply them to make a problem easier to solve (optimize solutions)
      - example:
        - 'breaking a problem into sub-problems' makes a problem easier to solve bc of the cause that 'separates variables causing the problem and they are easier to solve in isolation'
        - apply this cause to generate other solution optimizations:
          - apply this cause to the 'connect problem & solution' solution automation workflow to generate a new workflow or implement (specify) a workflow:
            - isolate the variables of connection & connect them separately

    - generate inputs to causes of simplifying problems & other relevant process to problem-solving
      - example:
        - 'adjacent structures for one problem format are more interactive than those of another format' (adjacent functions interact in a way that fulfills one problem-solution format connection better than another problem-solution format connection)
          - this is another cause of why a problem is easier to solve in a particular format
        - this cause has inputs/requirements:
          - there must be adjacent structures
          - the adjacent structures in one format must be interactive
          - their interactions must enable connection of the problem/solution in that format
        - derive & generate the inputs to problem-simplifying solution success causes:
          - determine the requirements of a problem-simplifying cause
          - generate & apply those requireemnts
        - example:
          - identify that in order to find & apply adjacent interactive structures, they must exist
          - in order for these to exist, in some cases the program will need to generate them
          - in some cases, this will involve converting between formats, and the problem can be simplifed to 'converting to a format where adjacent interactive structures already exist'
          - this amounts to 'applying an interface', which this insight path has derived as the solution

    - identify structures with input/output structures like sequences that can be used to connect problems/solutions for a generated set of problem/solution formats
      - example:
        - the solution automation workflow 'generate possible solutions and filter them' applies a 'filter' structure bc the problem format involves 'many solutions' and the solution format involves 'one solution', the problem format being to 'find one solution out of the many possible solutions', and a filter can reduce the number of an object that is output, so it fulfills a problem-solving intent to connect these formats, given that in order to find one solution out of many possible solutions, a program would have to reduce the number of possible solutions in some way, so 'reduce' functions/structures like 'filters' are useful

      - generate possible problem/solution formats to connect by applying error structures
        - the above 'finding' problem has an error structure of 'excess possible solutions' or 'lack of solution filters'
        - most structures would be problematic in particular contexts
          - some structures are especially errors when applied to problem/solution components, like a 'lack of solution filters' as opposed to 'any lack of filters'
        - other error structures involve structures that can be solved with core interaction functions
          - 'connect' solves the error structure of 'lack of connection between problem & solution'
          - 'mix' solves the error structure of 'find new solution' or 'find a random combination to solve obfuscation problem'
          - 'break & combine' solves the error structure of 'complexity added by combined problem components' (where sub-problems are simpler to solve)

      - this isnt the same as 'apply core interaction functions like reduce/connect & basic structures known to solve problems like input-output sequences', its saying 'generate & apply all core structures/functions that are relevant to solve problems, given that error structures are also fundamental structures connectible to solutions with core interaction functions' and also 'generate & apply all possible problem/solution formats and find structures that connect them to generate solution automation workflows'

    - identify insights that optimize problem-solving, identify their variables & generate them to apply them dynamically to generate solution automation workflows

      - example:

        - the workflow 'break problem into sub-problems & combine sub-solutions' applies the insight 'smaller/simpler problems are easier to solve'
      - how to identify this insight:
        - pull patterns from problem-solving and identify that problem-solvers often apply the workflow of 'use unit/basic/simple case to solve the problem, then check if it holds with other cases'
        - pull definitions & fit them in a way that makes sense (doesnt contradict any factual rules)
          - 'small' is an adjacent term to 'simple', 'unit', 'basic', 'low-cost' or 'adjacent' because they are all similar to the concept of 'easy', so it fits into the rule 'smaller problems are easier to solve'
          - 'simple' is a synonym of 'easy'
        - test if changes to a problem make it easier to solve, and filter which changes succeed in making it easier to solve
          - if a problem is 'climbing a ladder', test if changing the problem to 'climbing a step' makes it easier to solve - if so, identify that the change was 'reducing' or 'simplifying' the problem to its 'unit' case, and test if this change simplifies other problems as well

      - how to identify/generate other insights that make problems easier to solve:
        - any change that has an opposite effect ('change', 'reduce', 'neutralize', 'remove') on a component/cause/variable/structure of an error structure, or its generative system, without causing other error structures
        - any function that connects inputs/outputs more efficiently than another function can make a problem of a relevant structure easier to solve (a more efficient 'reduction' function may be a better solution like 'filter' than a less efficient 'reduction' function like 'sort then filter')

    - solve problem for one component/variable/structure of the problem, then add other components of the problem and check if solution holds or modify it to fit the new component
      - this is a specific case of the general workflow 'simplify the problem, solve the simpler version, then check if the simple solution holds when complexity is added, or adjust the simpler solution for complicating structures'
      - its also a variant of the 'break problem into sub-problems & combine sub-solutions' solution automation workflow

    - find any missing workflows by applying change functions to workflows to find variants like general/specific versions of a workflow
      - apply a workflow to various problems to find changes to apply to workflows to adapt them to specific problems, and add those changes to a general solution automation workflow

    - convert workflows to other workflows to find any missing variables to generate one workflow from another

    - identify & apply commonly useful structures by standardized structures of usefulness (like 'which structures have outputs that have common inputs for other functions', 'which are capable of generating many other components', 'which are inputs to structures of usefulness') to all functions/variables/components of problem/solution components

    - apply error structures to problem/solution components like solution automation workflows (like 'missing' error structure applied to 'worfklows') & known solutions to those error structures, and generate/identify new/specific error types in the problem-solving system & apply solutions to those error types

    - generate structures of difference (like 'difference sequences') and apply as components of workflows (similar to applying interaction structures, solution structures, optimization structures, relevance structures, or not-error structures)
      - solution success cause: this works bc in order to get from problem to solution, you have to apply differences to the problem/solution until theyre equivalent, bc they start as different, which is the problem
        - a difference can be an error type: 
          - a value is different from another value, like an expected/required value
        - similarly, a problem is different from a solution
          - the problem 'find a value' is different from the solution of 'a value'
      - in order to find error types ('problematic' differences), you can generate difference structures & find optimal routes between the inputs/outputs as a source of solution automation workflows
        - to find solution automation workflows, first generate & identify problems in a known system and find optimal routes between inputs/outputs (formatted as starting/ending positions)
          - then identify the interface components interacting with those routes

    - apply differences to solutions that are known not to work (can be calculated as definitely not solutions, or have been tried and are known not to work) bc a solution that works would have to be different from these in order to solve the problem

    - derive patterns of differences between solutions that definitely dont work and solutions occupying structures like areas of ambiguity where the solutions in the area might work but are more difficult to calculate, and reduce solution space to those areas, and apply those patterns of difference to calculate solutions that might work given solutions that definitely are known/calculatable as not solutions

    - derive structures of solution spaces that position/structure solutions in a way that adjacence indicates probability of working, so areas can be ruled out with threshold metrics representing boundaries
      - rather than applying a simplistic similarity metric, apply a metric that determines actual similarities based on relevance to the problem
        - example of grouping methods to determine adjacence in a solution space:
          - structural similarities can indicate similar functionality, or they can be insignificant to a particular problem and caused by an unrelated factor (like two similar structures created in different positions by similar boundary structures but having different functionality bc of the different position), so grouping solutions by structural similarities is one way that can contextually represent similarity of solution success for a particular problem
          - combinations of components of workflows/interface queries (interactions, differences) that can act in isolation (a workflow can be formatted as a set of interactions)
          - vertex variables
          - apply solution ranges where solution formats can be reduced to approximations of solutions or adjacent components to solutions (a theorem can be framed as adjacent to a proof)

    - apply pattern-identification methods of differences between solution automation workflows, isolate into difference types, & add to variables determining difference between workflows to generate them

      - example of applying differences to generate alternate solution automation workflows (different routes to connect problem & solution)

        - standard basic workflow: 
          - trial & error
        - alternate workflow: 
          - apply 'trial & error' to filtered solution space of 'adjacent' solutions
        
        - the differences between these workflows include:
          - container structure (one workflow contains the other)
          - different position of components (position of 'trial & error' in one is different from position of 'trial & error' in another)
          - one workflow has an attribute applied to filter solution space ('adjacent')
        
        - these can be reduced to known interface component variables, even if the variables interact in a new way thats different from other workflows:
          - 'structure' variable including structures like containers & positions
          - 'workflow component' variable including other workflows, solution spaces, solution metric filters
          - 'core component' variable including attributes/functions/objects (like 'adjacent' attribute, which is relevant to intents like 'finding solutions quickly' or 'finding feasible solutions with existing resources')
          - 'interaction function' variable including interaction functions like 'apply' & 'filter'
        
        - so an example of generating a workflow from another workflow using differences between these two example workflows would involve applying three logic rules that can be used to connect the two example workflows, which can presumably connect/generate other workflows:
          1. 'apply workflow components as inputs of core interaction functions'
            - example application of this rule:'inject one workflow into the other'
          2. 'apply relevant core components like attributes to workflow components like the solution space to generate a different workflow'
          - apply any remaining general logic rules once the workflows are generated:
            3. 'filter generated workflows by whether they connect components in a way that can connect problem input & solution output'

        - other differences between alternate workflows may identify other variables that can be used to generate one workflow from another

        - solution success cause: why does this method work to generate different workflows?
          - analyzing 'differences' between workflows is by definition relevant to identifying variables between workflows, which can by definition be used to generate them
          - one workflow is a more abstract version of the other, and varying abstraction level is by definition applicable to many contexts like inputs, within a range

        - given these solution success causes (inputs of success of the solution), we can derive other methods to generate workflows:
          - 'abstract a workflow within a certain range of abstraction' (so it doesnt lose its meaning)
          - "apply definitions of relevant components to workflows like 'differences' with an interaction function like 'generate' that is relevant given their definition like 'variables'"

    - derive & apply workflow template/structure to fill with workflow variable values once interface analysis is fully applied to workflows
      - this means once components like standard/base workflows, common workflows & workflow patterns, & workflow variables are identified
      - this is an alternative to writing static function logic to design interface queries

    - derived alternate merged interfaces (like the meaning interface) to avoid sub-optimal metrics inherent to each interface perspective, where the problem can be adjacently solved
      - the 'survival' and 'evolution' perspectives have their own disadvantages, so merge them into an interface to avoid these disadvantages
        - 'survival' disadvantages include errors like 'over-identifying threats', from survival functions like 'constantly checking for threats'
        - 'evolution' disadvantages include errors like 'excess change, incompatible with other changes', from evolutionary functions like 'gene modification/activation/addition/movement'
      - applying the survival function 'check for threats' to identify a threat of the change type that is 'incompatible changes with other changes' is one way to merge those components on these interfaces (using the error of one interface to fix an error in the other interface, assuming no other errors are adjacent in these merged positions)

    - apply definition of any other components of the workflow that havent been applied in other solution automation workflows or workflow-generating workflows
      - an 'insight path' is a 'shortcut to find new useful info' so apply the definition of 'shortcut'
        - by definition, its a method that requires less work
        - so generate methods requiring less work as an initial solution space
        - solution success cause: this works bc of the overlap between the definitions of adjacence and efficiency
          - paths are 'efficient' bc they require less work, meaning they may use 'adjacent' resources (nodes or methods)

    - identify the shortest, lowest-cost, most adjacent or otherwise most efficient/optimized route/function to known solutions from problem definitions and identify patterns in these routes or the variables/components/structures/formats enabling them to be optimized (sub-interfaces, definitions, interaction levels), and generate function to iterate through those patterns based on usefulness for a problem definition, and apply those patterns

    - apply trial & error except with the injection of the concept of 'solution progress' as a filter of multiple methods attempted in parallel, derived from maximizing difference types based on filter capacity (solution progress assessed similar to learning from error/cost)
      - identify the most different structures you can apply (like directions of motion) and apply them iteratively, checking for progress toward the solution metric based on solution patterns of progress (accept costs of these types up to a particular threshold or other structure), and stopping the pursuit of any differences that dont match solution progress patterns

    - identify & apply alternative inputs (variables) of solution automation workflows to create other workflow-generating workflows, given the definition of 'generative' meaning 'an input to', and given that this workflow for the default inputs (variables) of workflows is already stored elsewhere, so this applies 'alternative' as a transform
      - example of alternative inputs:
        - to identify that a method is especially useful out of all the possible methods, you can use alternate variable sets:
            - start with solution metrics as limits creating the structure/template of a solution, and fill it in or work backwards
            - common components of useful solutions, or components of commonly useful solutions
            - adjacent combinations of available resources at the origin state (problem position)
            - core interactive components
          - these are alternates bc they have equivalent/similar input/output when applied to this problem of 'identifying a useful method in a large set of possible methods'
        - a function (structure of connections between specific inputs & outputs) can have alternate formats like:
            - a set of filters
            - a set of differences
            - a set of intents
            - a set of requirements
          - these are alternate versions of the function that dont lose info expressed by the function, and they can serve as alternate inputs to the function outputs, since the function itself is also an input

    - iterate through optimization priorities & apply other optimizations to workflows, like 'find alternatives to optimize for robustness', which when applied to workflows would generate the previous 'apply alternative inputs to workflows' workflow-generating workflow

    - apply useful interface components (like 'interactivity', 'ambiguity', 'incentive', 'contradiction', 'requirement') to fulfill core interaction functions (like connect, complete, reduce, merge) with interface structures for optimized querying
      - fulfill optimization intent 'avoid full interface standardization'
        - map interactive structures across interfaces for queries that support avoiding full interface standardization
        - map corresponding structures across interfaces to avoid standardization to an interface in case an isolated operation like 'identify one object' is needed
          - this allows for an efficient interface query that executes only the conversions necessary & keeps the processing on one base interface, pulling in isolated structures from other interfaces with sub-queries as needed

    - standardize interface queries & solution automation workflows to other interfaces (to avoid converting problem system to a particular interface just to implement a workflow)
      - apply other interfaces like 'structure' interface to specify a query/workflow and interfaces like 'concept' to abstract a query/workflow
