- examples of other implementation methods than documented default methods (like the 'apply/find/build/derive' function set)

- how to implement the interface analysis framework as a set of simple functions for an initial version, involving alternate default constant simple structures to combine, such as:
    - symmetry structures applied as a default variable structure, rather than 'incremental change combinations', so that by default, symmetry combinations are sought, rather than incremental change combinations, to always frame change in terms of symmetries
    - combinations of useful intents (such as those that are relevant/realistic) as a default interface to base changes on, finding the functions to fulfill them at query time, as implementation variables
    - structures of relevant cross-interface structures as a default structure to apply changes to
        - for example, 'patterns of causal structures of structures of structures' (like the patterns of cause of 'a sequence of structures that develops from another structure')
	- functions that act like useful filters of relevant information, such as:
	    - a function that can determine an attribute like 'input change type' that reveals similarity/difference of a relevant structure, like 'output change type (integer)'
	    - more generally, a function that can determine attributes that reveal 'information about a lot of other information'
	    	- such as how an 'average data point' reveals 'a lot of information about other data points (in that most of them will be near the average)' and an 'extreme/limit of a range of data points' reveals 'a lot of information about other data points (in that most of them will be within that range)'
	- applying specific interface structures as a default constant set
		- applying 'information' interface interaction rules of the 'physical reality' interface such as:
			- 'truths can become false when over-depended on, beyond their appropriate context or meaning, or beyond their potential to illuminate or support other truths, or in an incorrect structure like a foundation for other truths'
			- 'truths can be so irrelevant to an intent as to be equivalent to false (example: citing the heat death as a reason not to try to do anything)'
			- 'truths can be so rarely/improbably true as to be equivalent to false (example: an error state that is so rare you basically dont have to plan for it, like where neutrinos would flip all the bits on a server at once)'
			- 'truths can be so unstable (difficult to maintain) as to be equivalent to false (example: a rare atomic state that degrades into another more stable state more frequently)'
			- 'truths can be so difficult to verify/calculate as to be equivalent to false (example: number of atoms in the universe)'
			- 'truths can be so non-adjacent to other probable/known truths as to be equivalent to false (example: future truth of a reality that is non-adjacent to current reality)'
			- 'truths can be so lacking in reasons as to be equivalent to false (example: there is no reason for a rare anomaly except random coincidence so it may as well be ignorable)'
			- 'truths often come with an opposing counterpoint as most truths are not absolutely true but are wrong in some way like in a particular context/usage, so that a truth without a counterpoint as how it might not be true is unlikely to be true'
			- 'truths are rarely the only truth explaining a variable interaction, as there is rarely a variable requirement requiring that specific combination and preventing any other combination from succeeding as there are more often many routes between two points than one route, and there are many alternate equivalent preceding/succeeding variable interactions that are similarly explanatory, bc errors are default in most systems and therefore differences applied to sequences/routes/components are likely and are less likely to be ruled out by some requirement if they still generate movement in the original direction, and similarly bc components are frequently unitary and can be combined in different ways to generate the same structures, and similarly bc real systems are often complicated and subject to errors from their high degree of interactivity with other systems in contrast to a theoretical high degree of isolation'
			- 'truths usually are not obvious (such as the wishes of agents as their default, and therefore also obvious/simple to them, so these wishes can be assumed to be not true) but if they are, a beginner without expert knowledge of rules is likelier to see them than an expert bound by their knowledge of rules'
		- applying interim connecting structures of useful structures representing the truth like 'rules databases', 'attribute networks', etc, such as 'common structures to both systems' such as 'constant attribute rules' (variable interaction rules)
		- applying useful reality representation structures such as 'metaphors (similar/relevant but different/inaccurate structures to format something differently in a useful way to achieve understanding)'
		- applying physics rules such as the 'fuzziness of physical reality' as a way to find other true structures ('symmetries', 'alternative definitions', 'patterns')
		- applying structures like the functions that can generate the highest variation (such as the 'Conway game of life') as a default structure to generate other high-variation structures like reality
		- applying specific mappings across interface structures (like concepts such as 'balance', physics structures like 'symmetry', and physical reality structures such as information agent structures such as 'justice' and 'economic equilibrium', or concepts like 'interface', structures like 'bases', information structures like 'metaphors' being variable implementations of the 'interface' concept) as default structures (such as default inputs of a neural network used to predict other mappings)
		- apply useful perspective structures (like combinations of perspectives such as the 'optimization perspective' and the 'religious perspective' to efficiently describe other useful structures like 'what agents want and what could be true using adjacent transforms of reality') as a default constant set
		- apply useful structure sets that should go together (like how 'dependence' is useful in the 'causal' interface but negative in the 'physical reality' interface)
		- apply useful structures (like 'direction or line connecting starting/goal points' and 'implementation structures to get to that point') to model useful structures that are useful when applied together, like 'intents/implementations of intents', as a useful input to neural networks to fulfill the task of 'adjacently combining them in a way that connects them'
		    - similarly apply patterns of implementation structures such as 'causal loops referencing nested sub-problems resolved with some structure and output to the host structure once solved' as default components of implementations of intents
		- fulfill a primary function of a particular interface that can solve most problems, such as 'evaluate meaning', and the useful functions to fulfill that function efficiently
		- apply structures iteratively to calculate the global/universal meaning to check if its obviously false at scale (in its extreme form which is more easily determined), such as applying a rule like 'its ok to violate someones rights if theyre a genius and youre fascinated by them' (after iterating to an extreme scale, its obvious that no, that couldnt be right, then geniuses wouldnt want to live if that rule is applied at scale, to guarantee that geniuses' rights will always be violated, and it couldnt be right for many other reasons, such as it is probably 'better/more sustainable/and therefore lower cost' to prioritize turning people into geniuses than to persecute geniuses with rights violations which will reduce our supply of geniuses to zero)
			- structures that make other structures obvious are obviously useful
		    - apply interactions of known structures at scale as default filters of other scaled structures, determining what else can exist at that scale, deriving core structures of truth from these scaled structures that could also exist at that scale
		- apply structures that can be used for multiple intents as default useful structures bc they can handle more variation than other structures and are therefore likelier to be compoundingly useful when applied in structures like combinations
		- apply structures that can offset neural network inadequacies such as 'limits of a series' being useful for solving the problem of modeling 'longer sequences of input/output connections' as a tool to determine when a sequence might converge, which is obviously useful for prediction intents, as well as other structures that allow the user to 'see far ahead', such as 'highly complex/different realistic/stable/efficient' structures, useful determining functions of extreme attribute similarities/differences (like absolute change type interactions, such as preserved change types when some function is applied) as 'extreme' structures and 'equivalent alternate structures (that can keep each other in check and identify invalid structures)', and other structures that allow you to efficiently calculate a lot from a little information
		    - you can see how useful functions (like calculating an adjacent attribute such as 'constance/addition' that reveals non-adjacent information like information about 'extreme change type preservation') act like a powerful information filter that reveals information that is difficult to calculate (non-adjacent)
		    - finding useful structures to determine attributes of other useful structures (useful structures like 'extremes') can be done by reverse-engineering 'input problem cases' where these revelatory 'filter' structures would be useful:
		        - finding highly different structures like extremes or complex manifolds (high-variation change based on an interface like a connected structure such as a manifold, which is a useful math structure adjacently mapped/corresponding to an interface)
		        - determining what is easily determined about that highly different structure once you know it
		        - determining what cases this information could be useful in
		        - determining whether those cases are useful
		    - similarly apply functions to 'calculate convexity' and 'filter possible functions given maximal differences (structurally similar change types that can look similar at first like exponential change, waves, hyperbolic functions, etc but which have adjacent filters to filter them out) detected by some point' as a way of improving 'gradient descent errors'
		    	- you can see how these 'interface structural similarities' (like an 'exponentially increasing subset') provide a useful constant base to apply changes to, to determine other possible structures and identify filters of those possible structures
		    	- this is an embedded application of the 'interface' concept, which is clear in the usefulness of this 'similarity to base changes on'
		- apply interaction levels ('adjacently connectible structures that interact/connect') as 'input information' or an 'information format' to neural networks (which fulfill the task of 'finding adjacent combinations to connect variables')
	- applying a function to find interaction levels where the problem is solvable with adjacent combinations and the function to find those adjacent combinations once transformed to that interaction level
	- iterating through filtered interface structure combinations and checking test cases of input/output pairs
	- create a network of solutions to use as bases to navigate
	- iterate constant/variable pairs to model the highest degree of certainty/uncertainty pairs
	- applying more adjacently determinable structures as an input
	    - attribute networks as an input and function networks as an output implementing those attribute networks
	- finding an 'input-output sequence' that would be useful and would probably be adjacently constructible in that sequence
	    - like a sequence of first building an 'attribute network' to describe all variables and then building a 'function network' describing that network and then a 'function generator network' to compress/generate that function network
	- applying default interface structures as the core functions of a machine learning network so that errors like 'not understanding abstract concepts' can be avoided by injecting abstract concepts as a default input or function or related structure, so that 'combinations of abstract concepts' required to 'solve a problem of understanding some conceptual combination' are adjacent
	- finding all high-variation variables/functions and applying that as a base to describe other high-variation variables/functions
	- apply optimization interaction structures (like a 'set of game strategies that can result in a tie') as being useful structures to model other useful structures like 'interchangeable equivalent alternates'
	- apply the 'biggest differences in between known problems/solutions' as a set of useful differences to apply to model other differences which are likely to be more connectible than those differences
	- apply combinations of workflows to find other workflows, to find solution-finding methods
	- apply structures guaranteed to be relevant (like 'changes within x causal degrees') as a default set of structures likely to be useful in solving a problem, then apply other useful structures like 'combinations/input-output sequences' to those structures to find the useful set of structures to solve the problem
	- apply a sequence of these other implementation methods as a way of designing a path from mvp to final product
    - apply useful structures that are more adjacent to solutions than problems are, such as a structure including 'interactive structures' and 'useful descriptions of the problem'
        - the better explained a problem is with the best representation of it, the easier it is to solve it
    - finding structures that represent the most interface structures (like a 'high variation structure that represents various core interface structures like interactions and requirements and generative functions')
    - finding structures that describe most problems/solutions in terms of interface structures such as 'a breaking of an interface' or 'misrouting of randomness' or a 'structure of resolution between ambiguous alternates' and other useful interface descriptions of useful structures, so these can be re-applied or applied as defaults or core structures
    - similar to how solving a maze can involve applying these structures, as 'equivalent alternate' structures that can provide the same or similarly useful information in solving the maze, deriving other useful possible solutions is possible by applying useful concepts like 'incentives' (to derive basic structures likely to occur)
       - applying regular 'direction of motion' checks to make sure the agent is still traveling toward the goal and not repeating routes
       - applying standard maze configurations as possible alternatives to select from
       - solving for standard tricks to check for in a maze that are incentivized and therefore likely to be encountered, and solving for the solutions to those tricks
       - solving for interactive sequences of paths that coordinate/cancel each other and can/cannot exist in the same maze
       - solving for 'maximal filters' that can filter out possible mazes the most efficiently
       - apply reverse-engineering to find indicators of various possible sequences of end paths nearest the exit
       - these structures involve solving other problems than 'apply any route using trial & error as a base solution, and change it as you go' which is a default solution to solving a maze
       - solving problems like mazes is a proxy for solving other problems bc of the high variation captured in a maze, if the maze reflects realistic randomness and other variable interaction patterns enough, similar to solving other games if they reflect realistic structures enough
    - a set of useful descriptions of reality that have reasons why they could be true (like a 'calculator of efficient methods of preserving energy cycles' and a 'structure to support maximal variation/uncertainty (time)' and a 'stable alternate coexisting sequence (time) finder') are useful to apply as defaults and efficient structures and truth structures, as well as the variables creating uncertainties (like 'unknown beneficiaries of calculations' and the 'maximally different structure' and the 'most stable system that supports the most variation') in these descriptions and the variables between them (such as interface structures like different priorities), as well as the differences between the description and related structures like 'problems solved with that description' and 'priorities fulfilled by that description' which make the meaning and reality of each description more calculatable
    - apply useful structures like 'clarifying structures' which make something obvious similar to how certain filters make some solutions obvious such as 'standardizing' and 'difference-maximizing' filters make differences more obvious
        - for example, finding standardizing structures like 'matrixes' which are a useful format that is also useful for other intents like 'mapping sets of sequential operations' and 'reducing some function to a set of adjacent combinations once in that format' such as 'solving linear systems of equations' 
    - applying useful methods such as methods of deriving information about other useful structures such as 'types' (all members of a type have this attribute that defines the type) and connectivity (constant lines are formed by a type of addition so it makes sense that adding them which is applying more addition doesnt change their shape, as their shape is the product of addition, as opposed to an operation that adds a dimension or restricts range, which would be required to create different shapes)
        - finding the 'type' of an object gives almost free (low-cost) information about that object, bc of the information stored in the type, where the type acts like an interface that can support some variation within the definition
        - other low-cost, high information-producing structures (like abstract concepts such as balance, alternate definitions, interaction levels, similarity to some other known structure, useful filters like standards to derive maximal information, useful networks to know a position in which is a high-information structure in useful networks) can be derived in a similar way and prioritized as default structures to find
    - useful formats of structures similar to a standard network but different in a useful way like 'gap networks of connected empty shapes' to represent related structures like intents to fulfill with implementations filling the shapes, or 'state networks to model useful sequences/queries' or 'maps to model useful connections' or 'map networks to model different analysis perspectives' or 'interface networks to model bases that capture high variation'
    - a set of certainty/uncertainty pairs to apply as default problem/solution structures capturing high variation
    - build sets of 'loosely related possible' associations (which arent guaranteed by definitions but which are allowed) using definition-adjacent connections, like how the connection between variables 'constant' and 'constant squared' involves a definition involving 'multiplication', but also has related attributes that are outputs like curvature which are required by the definition and other connections which are not as relevant like 'constant preservation of data type between input/output as a scalar' which is a loosely associated connections rather than tightly bound by the definition, using these loose associations to discover new possible connections not explicitly defined but also not definitely restricted
    - find component functions of data set subsets and iterate applying these until a non-matching point is found outside of the acceptable error range, adding terms or alternate functions to process in parallel to handle new points where found outside the range
    - find subsets of the data set that shouldnt be reduced to a function bc of complexity and randomness and other factors likely to predict insufficient information or variable injection points or other uncertainties that cant be resolved, where other subsets of the function are clearly mappable to functions
    - find more useful structures to describe variable interactions such as 'variables (such as squares) creating requirements (such as required growth, as in positive or nonzero growth)' and 'variables creating other useful structures like embedded change (change on change, like exponential growth leading to the accretion of matter)' and inputs to these structures like 'equivalences in factors creating self-similarity (in multiplication/area inputs) leading to multiple differences in outputs (of multiplication, compared to adjacent inputs)' which are useful in their capturing of high variation (similarities/equivalences creating differences like multiple differences between change rates of adjacent inputs)
      - the core unit of maximized potential of a variable in its interactions in isolation of other variables (self-interactions) is the area that can be created by the unit of core maximally different interaction (multiplication) with itself, representing its interaction space in the 'adjacent sides of a rectangle' operation (multiplication), leading to its potential (in its possible range of impact, as squaring it is maximizing its differentiability) to influence probabilities
      - finding the important alternate sets of functions that lead to these important structures like 'required growth (a relevant structure of reality)' can generate a 'limit scaffold' ('the model must not contradict required growth of some structure that has growth as a requirement or other form of certainty, as in some variable in the model must not grow in a way that contradicts known required growth of some other variable') that represents 'points of impossibility' that should be used as filters to avoid when modeling reality
      - the intersection of 'generative scaffolds' and 'limit scaffolds' is a useful place to start modeling the dichotomy between certainty/uncertainty to explore, format, & filter the space between them that is allowed by reality
    - finding other useful representations of a function (such as a 'stack of squares of increasing side length' as a useful alternate representation of x-squared to represent the value of y in a clearer way that reflects the equivalence in multiplication inputs and the multiple differences created by the equivalence in multiple dimensions) which increase the relevance and meaning of a function representation, as its probable interactions with other functions is more clear given the core relevant differentiating attribute of equivalent factors
      - similarly, the equivalence in factors leading to a line with slope 1 (1,1, 2,2, 3,3 as inputs) is significant and indicates the relevance of x-squared as different from other functions and more relevant as a unit function of change and indicates the relevance to the output by its squared area created by the difference between these input pair points and the origin
    - applying rules to find highly useful structures like interfaces such as by asking questions in a sequence like:
    	- 'what are the maximal difference-capturing variables like change' (such as 'functions vs. constants') or 'what contradictions exist in useful structures like change' such as 'what changes dont change' and 'what are variables of changes that dont change (limited change around a symmetry)' and 'what changes unchanging variables (constants, symmetries)'
    - rules like "'what are not inputs' are also a cause of structures in addition to inputs" (bc resources not invested determine states of alternate functions) which is another reason to derive missing information (not just to determine what a function will likely do bc of some input but also what other functions could stop that function given missing inputs devoted to alternate functions)
    - an alternate implementation of a 'blur' algorithm (using the compression/filters/other interface structures involved in vision) to quickly determine trends in a data set is to sample the data set and evaluate each subset quickly to benefit from the overall impression of the emergent pattern visible across a sequence of alternate subsets, where the impression is formed by easily differentiated structures like 'border angles', 'densities', 'ranges', etc which are common across multiple subsets in the sequence, or to align subset data sets as a sequence or network of subsets (organized by some similarity), to make trends more easily identified
        - similarly iterating through data set subsets quickly (to efficiently store memory by identifying the most obvious repeated patterns) is another way to implement an algorithm similar to 'blur', if the neural network has the ability to derive 'overall impressions of a shape' to compress the data set rather than just simply storing the whole data set
        - similarly, identifying a regression function is possible by finding which functions seem to be in the center/average of a data set (as a proxy for their summarizing capacity for longer when rotations/angle changes are applied (to view it from a lower or higher position), a transform that doesnt change the summarizing capacity of a function immediately if its a good summarizing function, 'summarizing capacity' of a function being easily determined from 'impressions' achieved by low memory (obvious feature retention) or fast processing times ('blur' effect)
        - similarly, identifying a data set or regression function that is 'most similar to the origin data set' is trivial by applying some transforms to some maximally different or standard or randomly selected data sets and positioning them in a way that aligns with the original data set (like by aligning its ranges in a separate graph that is close enough to easily identify differences but obviously separable) so it's obvious whether the added data set/line is similar to the original data set (and therefore the regression line of the added data set can be used as an approximation), which applies the 'comparison' functionality of a symmetry to the concept of 'data sets/regression lines'
        - where the 'summarizing capacity' of a function describes how much info the summary preserves (such as info about limits/averages/densities/vectors describing variation from base functions/patterns/possible interactivity/adjacencies/errors/ratios/probabilities, etc) which are metadata about the data set that can be variables summarized by the data set as well as subsets of data set points can be summarized as components of the data set, these variables of the function being possible input components or output solution metrics or interim sub-intents of a regression algorithm, which can be used to determine all other algorithms
        - as a metaphor, the "'definition routes' that when combined say the same thing (in a different way)" is a good set of nodes on a language network to use as an approximation of the variables of regression algorithms (and problem-solving in general), as a stable interface structure that allows maximal variation (it will be similar to my set of verbs like find/build/derive and structures I have identified as useful and the interfaces Ive identified and so on), as different regression algorithms indicate a general summary of the input, which varies somewhat around the interface of the input data set, while "saying the same thing" in the sense of preserving info about the input and "saying it in a different way" such as by preserving different variables and applying different functions
            - a query to find the 'summaries that are the most similar, where the inputs are highly different' is a good way to find these nodes
        - relatedly, finding out if a function is a straight line or a wave function with very small magnitude (or another type of polynomial with difficult-to-measure incremental changes) is a matter of testing a subset of the data set for variation and extrapolating/expanding that pattern to the rest of the data set (as opposed to checking the whole data set), but this isnt a good way to find out if those wave patterns are errors or if errors or error-similar structures like waves vacillating around an average are a default component of reality, which is related to the problem of determining the level of specificity and discreteness to assess incremental changes of integrals and polynomials (waves being a default difference structure), but this problem can be addressed to some degree by applying scalars to magnify the subset to make differences from constants/waves more obvious, checking it for robustness by applying randomness/errors, and other methods of identifying a stable function
            - just like the 'required component of a curve' is a 'subset with count greater than 2', 'subset with count greater than 2' can be used as an 'adjacent input format' to speed up an algorithm to assess non-linearity of a data set regression function, just like other algorithms have input formats that are more optimal for the algorithm than other formats, and 'subset with count 2' dont have as much info embedded that could relate to non-linearity as a 'subset with count greater than 2'
            - related questions to this problem are 'at what interaction level do you zoom out/in to in order to describe the variable interactions adjacently describing the most variation, or otherwise in a useful way (until there is a clear summary line, until the points are obviously differentiable between point types, until the point connections are obviously differentiable between connection types, etc)', as changes in scale and distance from data set can either erase or magnify differences like non-linearity, just like changes in scale/distance from the data set can make similarities such as patterns like lines more obvious
        - finding out 'non-adjacent similarities that are relevant', such as how finding 'non-adjacent inputs with equivalent outputs' is useful bc it implies a 'horizontal line' structure might be relevant, relevant such as 'being a useful base solution function to apply changes to in order to find variation based on that line, or to use as a simplistic summary function', which can be framed as a 'high density output' with importance bc of the repetition of that output in some pattern like an interval indicating cyclical patterns, or like a ratio indicating commonness, either way indicating usefulness of that 'high density output' as a base to apply changes to or use as a simplistic summary function
            - relatedly, vertical lines (indicating an input that can become any value) are so unlikely in a data set that isnt random that they can be ruled out as improbable in most functions except where required or made probable by another route
        - calculating the function from a set of symmetry structures (local subset averages (most similar point or change from most similar change rates to other change rates), a set of inflection points (a change in change structure like charge of change rates), a set of peaks (change in direction of change rate)) or symmetry-limiting structures (extremes of a probable range representing limits) is trivial once a ratio of these are known given some complexity (& other metadata like input ranges) of the function
            - relatedly, another useful problem-solving intent in regression is 'finding asymmetries in peaks which are useful to know about using adjacent info, bc peak symmetry is a useful assumption when true to determine more/all of the function from knowing a subset and relatedly useful to know about when false to identify when implications/similarities are contradicted and not absolute'
            - formatting the average as being more similar to points or being more similar to point connections (or being different from randomness or extreme points) is a useful set of alternatives when finding inputs that are common across useful structures or finding useful formats for algorithms that use averages
    - given that the complexity in regression is caused by non-linear shapes as opposed to constant lines, finding which variable interactions exist (which input variables are exponents) and which cause these complexity structures (exponents of an input variable, constant coefficients of exponents, addition/multiplication with other exponent terms, etc) would enable removing those complexity structures to reveal a simpler form of the data set that is more easily condensed into a regression line or a base solution function to apply changes to (like how removing parabolas that are symmetrical around a clear or probable average is a trivial task that makes the average more obvious), such as applying inputs of complexity structures to the base solution function to specify the more general simpler function
        - this is related to workflows like 'remove variables until a simpler component function emerges'
        - this is also related to workflows like 'find a simpler combination of inputs preceding the original function/data set' which can be applied in another direction such as 'find the function interaction level where maximal differences begin to emerge' such as how combining functions by type (like step functions, wave or other polynomial functions, sequence functions, discrete functions, closed shape functions) can produce functions that represent a 'combination of these maximally different function types' which may be more useful at filtering a function solution set than other interaction levels, so this interaction level of function types may be a more useful place to start when finding a regression function to summarize points
            - relatedly, finding 'sets of useful interface queries that decompose most variation' can be as simple as finding sequences of interface structures like 'find common/powerful/probable/high-variation variable interactions that match common system interactions like common system errors or common differences from incentives/defaults' and 'apply the function type interaction level to decompose the remaining unknown variable interactions, once these common/powerful/probable/high-variation variables are known', sequences which could be generally useful in 'decomposing most variation across problems', which is a useful problem-solving intent
    - given the possible set of interaction functions (direct connection, constant connection, side of an area representing a limit/border, error connection, random connection), use these interaction types as a variable to identify algorithms to classify connections between points based on their probability of being one of these interaction types and the line patterns that emerge given some set of interactions of a particular type pattern in some subset of the data set
        - for example, any two points in a data set are very unlikely to have a direct connection, so that should be used rarely
        - if a connection is confirmed or probable to be a direct connection, adjacent connections are less likely to be direct connections and other connections similar to the direct connection in other subsets are likelier to also be direct connections
        - if connections indicate low volatility, finding a connection with an extreme slope implying 'volatility' is less likely after some ratio of connections are tested/determined/assigned a probability of some interaction type
    - a function to find the simplest (or otherwise effective) polynomial to describe averages of local data set subsets (where angle of lower/upper borders and densities influence the average) is an example of a standard method that can be found with adjacent structures
    - changing the definition of useful structures like 'averages' to find alternate methods
    	- defining an average as a 'line that when changed the most compared to other functions, still fits within the boundaries described by upper/lower limits' points directly to a method to determine the average function fulfilling that definition 'find a subset of possible different functions to change, and changes that can be applied to these functions, and apply boundaries as limits to these changes'
    	- defining an average as a 'difference from extremes' or the 'usefulness of right triangles in finding average functions (and vectors applied to them to generate extremes) of a data set' points directly to methods like 'find angles applied to a possible average line that capture the highest variation in a data set, once possible extremes are known'
    - evaluating a function's 'differences from randomness' is another starting point to base changes on rather than basing them on an average bc its the 'opposite of the intended information' and is therefore similarly useful in that adjacency
        - other function bases include functions that 'connect non-adjacent subset averages', that 'connect adjacent subset averages', 'connect function upper/lower ranges', & other representative/summarizing functions
        - a random data set is not useful and is therefore useful to determine early on in calculations, just like function limits and patterns are useful to determine, and randomness may as well be an indicator of falsehood (as in 'something that needs to be changed in order to determine truth, like requirements/impossibilities') bc of this lack of usefulness
        - other known structures that are not useful are equally likely to use as bases for change, like how known useful structures like core components are, bc of their dichotomy in the certainty of their usefulness
        - applying 'common types/variables of functions that can form randomness' (such as 'contradicting/neutralizing change types that cancel each other out' or 'complementary opposite change types (like triangles which form a square)' or 'randomness-amplifying which doesnt change the randomness of the inputs' or 'symmetries like the equivalent weight of dice sides' or 'a high number of variables' or other functions that are likelier than average to create the requirement of randomness as 'even distribution of probable outputs') is useful as a way to 'determine probable randomness' or similarly to 'determine differences from randomness', 'remove/add randomness' and other intents related to randomness
    - building an interface structure of interacting rules to base changes on, like a 'set of requirements' (like how 'connecting components of a structure' is 'required' to 'form component connections to create that structure' by definition) or similarly a 'set of rules that are definitely impossible or not true' as a foundation for other changes (where possibilities exist between contradicting limits imposed by requirements) is a useful structure to start discovering new rules from
        - identifying rules that identify non-adjacent information required by a definition is similarly useful like definitions that identify adjacent/obvious information required by a definition are, like how numbers in a sequence like the set of integers are required to be one unit away from other integers and required to increase if its the set of positive integers, so knowing that a set is sorted in an order like this gives you information about all the numbers between two items in the set bc of the definition determining the set allowing functions like 'estimate where an item will be found to reduce the search space', or how the type of a number determines some of its known functions
    - a standard method to solve problems is framing them in terms of core structures like 'similarities and differences' (like 'similarities to constant representative/average lines', 'similarities to averages', or differences like 'difference-maximizing functions', 'highest angles connecting adjacent/similar subsets as a function that is almost guaranteed to be incorrect to base changes on'), then applying a function to 'determine which differences to resolve' (like 'differences between base functions like averages and alternate similarities like densities' or 'differences from highest angles connecting adjacent subsets and a base average/density-determining function')
        - this method finds the variables likeliest to be known/similar (or easily derived/predicted as adjacent to known variables) constants like 'averages' and 'local subsets (locality as an indicator of similarity)' and 'densities', and variables likeliest to be unknowns like 'alternate more complex functions with more variables' and applies similarities/differences to model those and find which differences are relevant to resolve (the differences that 'connect related similarities/differences', related by providing complementary information for intents like 'represent a data set', complementary information like 'base functions' and 'specifying differences customizing that base')
        - finding a function that models the 'maximally different local subsets of the data set' is a solution-finding method easily produced by this method
        - this is an implementation of the workflow involving 'finding matching structures based on common attributes' to model 'uncertainties within that structure' and connect these structures to problem/solution definitions
        - other differences to resolve could be the 'difference between the set of possible/probable functions and the set of best representative functions' or the 'difference between a set of probable determining variables and a probable representative function' or the 'difference between probable linear representative adjacent local function sets and the non-linear variants that are better representatives'
        - this is related to a structure like 'applying variables to a structure like a "set of maximally different angles" applied as a symmetry to find the angle set that hits (or alternatively/equivalently approaches) the most data points when the angle set is changed the least (like rotated, shifted, scaled, etc)', since a 'line that approaches the direction of or is adjacent to the data point densities' is similarly useful as a 'line that intersects some ratio of points', as a 'representative function' isnt required to intersect with any data points, so that intersection is a variable that can change in the solution-finding method, as well as other properties not required by the definition
        - this formats structures in a way that makes it adjacent to identify variables and sources of variation (like rules like 'sudden constants/similarities that enable other changes to begin by providing a foundation like a barrier/limit for differences are a good way to identify interface variables in systems'), which is why the primary interfaces are useful in the first place (they allow highlighting uncertain differences by applying certain similarities/differences through standardizing/similarizing to fundamental/core unchanging variables like 'cause' as in embedded variables of that variable like 'causal degree' that support/describe/limit other variation and otherwise fulfill intents related to variation the most completely)
            - relatedly, identifying the most powerful variables as the biggest sources of error in a particular format, such as 'causal position' being a source of error in the causal network format (such as how an input might be an output of the output but it could seem like an input in some false similarity errors) and the 'connection function' in the network format (such as how some changes can seem adjacent/probable in one network format but its a coincidence, where some other network format is more reliable at making those connections adjacent)
            - relatedly, the primary interfaces are also useful for being based on the 'reasons' why a structure may be relevant to another structure (it is caused/changed/allowed/required/intended by other structures, it is useful to or interactive with other structures, it is a variation (as in a definition route) of other structures like concepts, etc) which are united on the 'meaning' interface (determining relevance/usefulness of structures to each other)
            - calculating the structures that are not in a data set but which could be relevant based on other structures that filter structure combinations like probability/similarity/commonness is useful as an intent to predict possible real structures that will be found in future data sets (calculating uncertain differences before theyre a problem/before theyre real)
        - this is related to other workflows like 'find maximum differences (what something is not/find the opposite of something to find limits of what it is)' by asking questions like 'what is not cause' (with answers such as 'an event that follows another event is not necessarily a causal sequence bc they may be so indirectly connectible that they are effectively independent') to find useful structures like the limits of the causal interface and how it interacts with other interfaces like meaning (such as how 'events that follow each other in time may be causally separable and arent required to occur in the same system or detectably influence each other')
            - similarly, finding why structures would not interact (such as how 'some structures cant detect/measure other structures, and therefore cant use them as inputs') is useful as a filter of 'meaning' interface logic
    - a unifying function of the various representations of a function where the representations are variants supported by the unifying function is likelier to represent the function the best
        - similarly, a unifying structure that supports various representations (like a 'set of maximally different directions', a 'set of reflective mirrors as polygon sides capable of producing different variants of the same information', a 'network of foundation structures around which maximal changes are supported which can coordinate', a 'set of filters capable of filtering the highest ratio of solution sets with the highest similar degrees of accuracy', a 'set of overlapping shapes with a common center (of common components) that model reality with similar accuracy', a 'set of connections between common high variance-capturing structures like maps/filters/networks') of the interface network (in various perspectives that filter it) is likelier to best represent the interface network
        - finding a useful 'sequence of filters' is useful as a good way to avoid problems of assuming too much & other basic errors of bias, like by applying "possible, known, required, probable, computable, measurable/testable, usable, & realistic" structures early on in the filter sequence
        - similarly, a unifying function of solution metrics (efficiency, accuracy, generalizability, flexibility) is a useful base to apply changes to in order to determine variables of solution-finding methods
    - finding useful structures to combine as defaults is useful, such as how 'symmetries', 'fractals', 'randomness as a limiting counter-structure', and 'right angles' are useful as core structures to describe a high degree of changes bc of their definitions ('applying fractals to changes in the direction of a right angle based on a symmetry up to the limiting point where additional changes appear random in their accuracy at describing change' can describe probable changes around that symmetry) bc of the relations between their definitions ('symmetries' and 'fractals' both having a 'common base (of a "self") for change, and a limit on changes to that base' in common, so applying these in the same structure benefits from their common symmetry in their definition and applies changes to this symmetry in their definitions, and 'fractals' further fulfills other attributes of symmetries like a 'limited change, as fractals converge')
        - similarly other examples like 'why i is a relevant number to rotations' given its adjacent concepts which make this functionality probable or inevitable (not only its allowance by definitions, but also possibly its difference in ability to produce a difference in sign/direction from the origin, its core operation of 'multiplication' being a definition of the components of a type of n-dimensional change, 'multiplication' as relevant to angles through creation of closed cornered shapes, a unit of change that is relevant to rotation, the relevance of sign changes to wave functions which are relevant to circles, etc, which are useful changes for connecting more directly relevant structures to rotations like pi), in its function connecting two other structures of rotation such as e, a structure related to spirals (a structure with a useful equivalence in its change rate ratios of adjacent compounding) and pi, a structure that acts like an origin/symmetry of external/internal spirals (trending toward polynomials in the external direction and trending toward individual points in the other direction) and would be a spiral but is missing the 'change rate increase' to create a different equivalence than a spiral has (a change rate equivalence, that creates the property of 'closedness' in the circle as opposed to the spiral but is sufficiently different from a 'cornered shape' in either the internal/external direction so as to justify its own definition separate from spirals or cornered shapes, despite being equidistant from these and other and unknown structures in their definitions)
    - mapping problems to more defined fields like highly structural creative industries such as 'music' to find concept mappings that are more easily determined like how finding the rule 'intelligent goodness is more difficult than obvious/complicated wrongness or obvious goodness' by applying the clear definitions in music of "obvious rights/wrongs like compliance with major/minor chords/notes or compliance with patterns" and how finding the intelligent goodness requires knowing the obvious errors it avoids like incentives like 'cheap rewards from any difference, even wrong differences' as its easier to create a new minor song than to create a new complicated but good song, as the range of possible solutions is narrower but can still host complexity/variation that intelligence could survive in, and complicated goodness is more complicated than complicated wrongness bc of the additional problem of the limited range that requires creativity to sustain intelligence in, a rule that would help avoid errors of over-simplification, prioritizing any difference, and avoid obvious errors as well as errors that create changes that violate a solution structure like a range of good solutions, and would incentivize finding high-variation variables sooner (to stay within the limited range) than prioritizing wrongness would
    	- finding a corresponding physics version of a useful structure is a good filter to apply when determining useful structures or other structures to apply as defaults bc these are likelier to be functionally useful if not more plausible, realistic, or possible
    	- example metaphor: gravity between 'equivalent alternates' (such as alternative theories) is weak but enough to keep them in the same definition (allows 'aggregation' to occur)
    	- other example metaphors which are useful in the sense of being evocative or otherwise useful for calculating something: 'supersymmetry' and 'interface network', 'string theory' and 'variables as waves, as reality units (possibly related to twistors/spinors)', 'isomorphisms retaining histories/inputs' as some definition of a 'wormhole', the 'jacobi identity' (and other structures of inequality like asymmetries) as an example of a 'way of determining what structures will accrete in some definition of unidirectional time (like to determine what will become matter)', 'commutativity' and 'equivalences in quantum superposition probabilities', 'invariance attributes like associativity/commutativity being a useful structure as a base to apply other changes to, to base other more speculative variables on', 'non-orientability and CFT violation', 'whether representations of reality form reality, to the extent that it can collapse/expand into or be based on other representations as those become more energy efficient, like in a cycle of different representations', etc
    		- related questions: 
    			- are all measurable invariance attributes components of reality or are some of them the basis of some reality and others the basis of another like points on a lattice that can be a foundation of reality, where points in between are not, so travel between them could only involve motion in the transform producing either from the other
    			- which symmetries are foundational and absolute in the math interface? which symmetries (like abstraction as filter or map, connecting similar concepts like energy/variation) connect these absolute math symmetries to physics symmetries? which symmetries allow maximal differences to develop? what symmetries exist in reality (such as symmetries across space-time states preserving potential energy or entropy, so that we only have access to functions within a probable range area governed by that metric)?
    		- the usefulness of these metaphors depends on the variability between 'definitely possible' and 'definitely not possible' (plausible, logical, not definitely impossible, evocative, suggestive/implicative, conditional, etc), so that a tool to 'traverse similarities and apply maximal differences to find structures like requirements, symmetries, concepts, and limits' can benefit from embedded but not articulated useful variables/structures such as connection in language (superset of definitions containing a subset of math definitions)
    	- similarly, other metaphors include how the number-based spaces can be extended to apply to number types (like a 'prime-based space'), how a 'complex number-based space as a possible structure of reality' has a corrollary in the insight that "almost every fact has a counter-fact (similar to paradoxes with local contradictions across statements) where it is related to its equally legitimate opposite (where it's not true), where opposites are allowed by definitions (similar to manifolds that can seem like extremely different objects that contradict each other absolutely but are actually related consistently by some common structure)", where alternate possible spaces may represent/embed other possible insights as their generative/limiting/determining functions, and which may intersect/overlap in a space of these insights (which has a corrollary with quantum field theory), etc
    - finding solution functions for possible known errors to existing methods, like how 'standard neural networks' can have an error of 'finding a different function of different components for each input/output pair' or 'finding overly simple functions that re-use the same (or otherwise simple) components the most but dont handle extreme/new contradictory cases' or 'finding all possible incremental components of some size that could differentiate some outputs and removing some of the less useful/adjacent/common incremental components'
    - identifying interface query intents fulfilled by known useful structures (like how 'cellular automata' and similarly 'standard neural networks' are useful for interface queries such as 'finding maximal differences generatable by one logical/change unit on its own, to identify where a seemingly complex phenomenon can be identified by one variable (the logical unit), thereby reducing the complexity of high variation data sets or the complexity of finding variable interactions between variables of high variation data sets')
    - neural networks may have an error created by the 'sequence of training data', in which earlier training data influences the final solution function disproportionately to its utility value
        - finding the worst case scenario where each training algorithm could miss the most obvious (or otherwise useful) alternate/absolute optimal (such as with gradient descent) bc of the order of training data and how to correct these problems (such as 'start training multiple models at multiple different points and attempt to integrate/converge to an absolute optimal given their change types, like pursuing only those descents where lower values are clearly identified or otherwise where lower values continue to be possible')
        - abstracting this workflow to 'find common variables like "robustness to order changes" that are highly differentiating in math fields/functions, then apply these common variables to neural networks/regression algorithm to check for variation resulting from these variables'
    - 'minimize extreme errors of a solution function' and 'maximize data set coverage of a solution function' are both 'alternative contradictory' intents to solve the 'find a regression function' problem, which offer some degree of 'complementary info' rather than 'definitely overlapping or equivalent info', as the 'extreme errors like missing an entire variable or missing an outlier' and the 'maximum data set coverage' have no guaranteed overlap, as a 'maximum data set coverage' could easily exclude extreme values or other sources of extreme error like variables which are easily missed if some subset is selected
    - an example of specific simplifications to solving prediction problems can involve avoiding known suboptimals/errors/violations of requirements by identifying & applying differences to those and allowing all other variation to develop, such as predicting a subset of just the worst case scenarios by applying extremes to increasingly high variation variables (technological development in some direction like electricity/automation/chemical printing/speed/compression/computation) and identifying opposing variables (particle accelerators/generators which can modify components of or interactors with electricity, software likely to be used with electricity, & quantum technological development which can modify components of electricity can oppose a possible error possible with electricity technology in an extreme such as in a concentration of one position, 'if energy technology or energy technology variation (innovation) was concentrated in one position by some entity, what could oppose/change it'), and otherwise modifying high variation variables ('what are all the worst case combinations of high variation variables with other high variation variables like quantum computers and neural networks'), as if variables dont cause high variation, they can sometimes be ignored in some cases until they indicate change in the direction of causing high variation, as knowing the 'structures like patterns of variable development such as patterns of large-scale errors' is more useful than solving for every possible variation interaction in cases like with computation limits, similar to how 'limit change patterns' are also useful in decomposing all variation
        - this can generate possible innovation intents like 'encrypt physical molecules so they cant be read', 'inject variables in reality in a way that creates gravity, if uncertainties attract gravity to require interactivity to develop to handle the increase in variation without destroying the foundational structure (create a structure like a black hole, an isolated uncertainty cascade)', 'speed up information travel/acceleration technology so farther information can be read and used in predictions more quickly (such as by directing radiation/randomness wherever its not visibly/directly reflected back at us by hitting a structure, so that it hits something we cannot measure and therefore is likelier to encounter maximal differences if they exist, which are likely to respond back as intelligence sources and improve our rate of technological advancement, or similarly direct radiation at inputs to these sources which is likelier to be findable and is similarly likely to get a response as well as likelier to incentivize organization/connection and lead to an increase in the power of radiation to cover distances as those will be reduced by this connection and therefore increase its speed), or similarly direct radiation in known stable ways that create maximal differences here to attract other maximal differences as intelligence sources who require differences to solve their problems' and therefore the direction that changes (innovation) may be applied to fulfill those intents, as measurable intents are likelier to be focused on & fulfilled, and relatedly 'horizontal innovation' can also be incentivized to connect these innovations in different directions on a new interaction layer
    - 'solution automation workflows' can follow rules governing selection of structures like 'spaces' to position them in structures like 'sequences', so for instance by the time a query starting on the 'interface network' and moving to the 'math interface network' to a 'causal/neural network' gets to 'finding polynomials in euclidean space', it should be heavily filtered so that the extremely high ratio of 'possible functions allowed' relative to 'probable solution function variation' in that final space are mostly filtered by the point the workflow arrives there in the sequence (as in, a 'probable function range area' or 'probable primary function components' are identified by that point), or otherwise progresses from a space with more possibilities to fewer possibilities if no such filters are applied, so that the space itself can act like a filter on the possible solution functions
    - finding resolution functions for commonly useful connections/transforms, like connecting the 'densities to sparsities' or 'density patterns across densities' or 'densities to extremes' or the 'edge points to an edge line' or 'upper/lower/average edge lines' or the 'densities to regression lines' or the 'shapes (like graphs) formed by densities to regression lines' or the 'local subsets represented/connected and the subsets skipped/unconnected' and other useful connections in the data set regression problem space
        - other useful structure examples to apply as defaults given their higher probability for various reasons (adjacency to requirement, commonness, etc)
        	- identifying variable interaction functions that could not be true given some system of reality where those functions could exist that is not true, like identifying that a 'wishing reality system' is not an accurate model of reality by identifying that 'agents wish for problems to be easy to solve (they wish for types of freedom)' and 'problems agents have are not usually easy to solve' and 'some common wishes of agents contradict other wishes of their own and of other agents, and would also contradict a wishing reality system for other agents' and therefore a 'wishing' variable interaction to connect problems/solutions is not a valid problem-solving function applied absolutely, which is useful to rule out variable interaction functions to find variable interaction functions that are possible/legitimate as a way of finding a reality system model, though finding states that move in the direction of that system is possible using realistic rules (increasing agent intelligence makes problems easier to solve and reduces agents' contradictory wishes against other agents, which is a solution of removing the intents that make the 'wishing reality system' impossible to logically sustain)
            - similarly, identifying high-variation explanatory variables like 'incentives' and 'interaction levels' from a typical data set where structures like 'defaults' and 'input/output similarities, differences, and requirements enabling interactions' (as more probable than other variable values) and 'adjacencies' (in casual degree) and 'efficiencies' (in benefit/cost ratio and stability) and 'interactivities' (as default interactions between variables) and 'types' (as efficient captors of information) appear more common than others, 'incentives' being a common factor in these common variables, as these structures are incentivized compared to other structures that may be more complex/difficult in some way, so simple queries like 'find common high-variation variables in common high-variation variables to identify other common high-variation variables' are useful in typical problems like regression, as if a common variable seems high cost its likely that we just havent identified the incentive yet, as the incentive is the determining variable more often than not, just like randomness is not usually real equivalence of probability in outcomes, but is likelier to just be lack of information about variable interactions that makes a variable interaction falsely seem random
            - identifying the structures like 'shapes that when overlapped and rotated and viewed from a distance look like more common shapes like circles' as useful for describing reality in that they cover and explain more variation than other structures (this structure has a structure embedded in it that indicates 'equivalent alternates (having an equivalence in their center & a similarity in their rotation)', the 'incompleteness of any one alternate in this set when viewed in isolation', the 'usefulness of general trends once a process is repeated/scaled', the 'balance inherent to multiple equivalent alternate perspectives', the 'ever-changing nature of foundational structures leading to changes in interaction levels', and other fundamental structures that are descriptive if not generative of reality)
                - relatedly, viewing reality as a 'set of repeating processes applied to partial overlapping/embedded structures at varying intervals/magnitudes/scopes' may be more useful than some 'networks with unique nodes or just functions as nodes/queries or just individual usages as nodes/queries' bc the repeatability models useful interactions like 'aggregations at scale', 'net/emergent effects', 'in/stabilities', and other important structures created by the interactions of real systems, so this model of reality may simulate a more useful interaction level than abstractions like function networks tend to capture
                    - similarly, 'similarities (like patterns) in "differences from incentives"' and 'similarities in entropy/uncertainty/potential reduction structures' and 'structures of differences (such as attributes like unfulfilled/impossible/required intents) between differences/variables and difference-connection/solution structures' are related useful structures that captures high variation and formats differences in a minimally complex structure
                    - a network/field of these sets of useful structures that describe high variation in reality is useful as an alternative to a set of useful interface structures which can generate these adjacently
                - relatedly, viewing reality as a set of 'differences from required limits (impossibilities)' or alternately 'commonalities (similarities to probabilities)' (produced by adjacent/input/causative structures of commonalities like scale/aggregation/repetition/isolation/efficiency/incentives/investments/adjacencies/examples/usages as ways to produce commonalities) and differences from commonalities (like those that occur when "previously isolated commonalities interact after repeating enough to reach each other's position") can be more useful in its specificity as different from other useful formats like just any high ratio of all combinations of 'similarities/differences', this specificity being useful in the certainty it provides to base changes on
            - identifying the structures like combinations/ratios/networks/interaction levels of optimals, stabilities, requirements, errors, and other interface structures that occur in reality is important for solving the 'find a regression function' problem, as there will inevitably be something that a real system 'optimizes for incompletely', something that a real system 'should do but doesnt given this priority/requirement/input/opportunity', something that 'stabilized after a particular error type', and other combinations of interface structures like 'conditions' and 'causes' and 'optimals' (such as rules like 'apply high variation or high potential-variation variables first to decompose high variation'), which can explain most variable interactions but are not default/core structures already identified/required (these structures are more complex variants resulting from more interactions), which are obviously useful despite their complexity, as more useful to retain and use as defaults that re-generating them from core components every time, similar to how solution automation workflows can be more useful to retain than re-generate
                - these are useful for determining when a data set (or the system it reflects or the parameters of the analysis process like definitions) is incomplete/incorrect or otherwise suboptimal/erroneous in some way
                - relatedly, structures like 'monodromies' can be useful to apply existing math structures to indicate which points/lines/other structures that could represent 'limits which should not or need not be crossed' (which differences should not be resolved, such as where an ai program might 'resolve a difference' of a medical problem which is a 'state of difference from health' by allowing patients to die to create a 'difference from the requirement for health/health resolution to invalidate the problem' which is the difference to avoid except in extreme circumstances like where a 'health generator is adjacent, invaliding the requirement for health' and otherwise resolving unnecessary differences like 'resolving a difference that is already a known constant' or 'resolving a simple difference that is obvious' and 'resolving a difference that leads to an extreme such as hyperbolic change which is unhandled')
                - relatedly, retaining structures that are more useful to make constant (such as where its more useful to retain a map of inputs/outputs than to find a connecting function, like where extremely high variation is observed/possible and not reducible or where reduction is contradictory to other intents) and use as input defaults than to re-generate is a way of identifying the stable structures of reality that should not usually be changed/tested but rather applied as default inputs, combined in increasingly complex ways to describe more variation, rather than replaced with more descriptive variables, as these defaults are already the best at description
        	- identifying patterns/limits/useful representations (like areas)/other interface structures of useful 'false similarities' of functions (at some point, subset, in relation to some threshold, or range), such as when '(x^2) + 1' depicted as an area will look like a rectangle (when x is near 1) and when it will look like a square (as x approaches infinity), and identifying the point where these 'false similarities of areas' change (when x is sufficiently large that 1 looks trivial by comparison), and whether other changes are possible/defined/likely (whether those 'trivializing' changes will apply again at some point in that direction) or whether the pattern will continue, to identify 'different change types possible with a function', which is useful for determining the remaining sets of a function given a determined set of input/output relationships and in filtering functions that are equivalent or seem equivalent in some subsets but are not
        	    - similarly, finding 'standardizing' functions like to remove 'obviously non-impactful variables' such as the 'shift to move intercept value at y-axis to y = 0' that make the change type patterns of x^2 more obvious (the inevitability of the 'increase in area created by x' being exponential being more obvious once the 1 constant additive is removed as the impact of adding 1 at higher x values is more trivial which obscures the otherwise obviously exponential area increase, and the addition of 1 being increasingly trivial in changing this exponential increase in an invalidating direction)
        	    - similarly, other 'variable combinations with obvious/predictable impacts' exists like how a 'sum of constantly increasing and constantly decreasing variables' could easily be a horizontal line depending on their ratio
        	    - depicting these 'possible variable interactions' in a way that their commonalities are depictable as common points/overlaps or other obviously similar structures (such as combinations of inputs having an attribute in common crossing the same line/point or having the same shape or area) is useful for filtering probable interactions that are relevant to some other interaction (like an input/output interaction)
        	- similarly, finding a subset of points that represents an average/midpoint of some other set of points can replace the calculations required to connect the set of endpoints and instead just using the midpoint is acceptable as an approximation algorithm of the others, if enough different points are preserved to retain the general shape of the data set, just like 'removing extremely similar points' as 'redundancies' (but not redundant in all cases, as in the case of points around a density center, which preserve the weight of the density given its surrounding adjacent points) and removing 'removing non-adjacent (but non-maximally different) points connectible with local constant functions' as 'improbabilities' is another way to reduce the set of points required to connect in the same function
        	- finding useful formats like the 'set of angles/areas creating the components/features/products created by an input' which are summed to generate the output and which can be easily checked as un/applicable to other inputs by changing the angle of perspective in viewing these angles/areas so the impact of component areas/coefficients is obvious (viewing the interim products of operations in between x and y as a stacked set of areas, angles referring to the differences between area upper/lower limits compared to input x), rather than repeating the multiplication/sum operations on the other input, which is related to the format of a 'network of areas (representing products of variable pairs/sets) that is separated and aligned to make the output (sum) more obvious to avoid re-computing it', these formats being useful for approximating/prediction/avoiding computations by identifying similarities in component metadata like ratios of areas, and identifying obviously wrong 'product component sums', similar to filters like types that determine 'information about what else is also true' from a 'set of input facts'
        	    - this format is maximally useful when the interim components are few & large or otherwise obviously different, 'either very large or small' and 'very few' (meaning more adjacent to the final output)
        	    - once probable 'large components' (or otherwise simplified components) are identified as probable, filtering the sets of possible smaller input components of these components to resolve the ambiguity of 'which smaller inputs created these larger outputs' is a reduced problem compared to 'filter all possible solution functions' and possibly also 'apply incremental changes in a direction that reduces error' in some cases, which can be repeated up to the point where additional component-resolution is trivial (differences added by further component identification are trivial, or equivalent to other differences created by some other component set or simple layer like some function of randomness)
            - similarly, the 'equivalences in differences' from a particular 'possible solution function' are useful to identify 'probably useful summary functions', as functions that produce 'equivalent differences' are likelier to be more adjacent to the actual solution function (transformable to the solution function by some trivial transform to produce these equivalent differences, like a shift or scale change)
                - relatedly, finding these useful standardization functions that produce an 'equivalence in differences' from one function to another is useful to find these functions which have equivalent differences
                - relatedly, the '(patterns and other interface structures of) differences in functions that have equivalences at various subsets' are useful to identify, as having a common interface defined by their equivalent subsets which vary adjacently by some variables that generate the different functions united by those subsets, which make the problem of 'selecting between equivalent alternates such as ambiguous solution functions' more trivial
        	- identifying the 'maximally different subsets' that will stay under an error threshold for the same given general solution function is useful to solve for and apply once the 'maximally different subsets' of a data set are identified to filter the possible solution functions by taking a subset
        		- similarly finding the probability of a particular solution function by comparing the ratios of coverage of these 'different subsets, which are equivalent in their error range of a general solution function' (which function corresponds to a higher proportion of maximally different subsets under a minimum error range, this proportion representing an approximation of the probability of that function being the solution function)
        	- checking for set of 'points and directions' within a horizontal slice of a probable range (where the densities are, indicating where the solution function probably is within that range) is more trivial than checking for inputs corresponding to any outputs, allowing a method to skip points corresponding to outputs outside of that slice, as a subset of 'points with adjacent directions of change' is an alternate format of the solution function that can be used to determine the rest of the function, as finding the actual points in a horizontal slice of the data set and adjacent directions of change moving away from those points captures the value of a subset of determining points (in a high-variation slice of a probable range area), as an alternative to the usual determining points like extremes/inflections/averages, which are further determining of the rest of the function when paired with adjacent change directions, if the slice is in a high-variation section so that most variation of the function is captured in that slice (the same can be said for any horizontal slice of the data set but moving it to within the probable area range of the solution function and specifically to a high-variation subset makes it more useful, and pairing points with adjacent change directions increases their determinability of the rest of the solution function), so 'finding the most useful position of the slice' is the problem to solve, in addition to 'finding the point/adjacent direction pairs that determine a function, instead of the usual determining points/metadata'
            - identifying useful structures like different formats to handle cases where it's not possible to select between alternate solution functions, such as where a range of probable area is too large and functions within that range are equally or similarly possible, in which case every possible function in that range is more useful to format as a node on a function network, as every connection can be distorted to be every other connection type by applying some variable or error, these variables or errors being possible to apply generally across systems, so any data set could be the product of an error applied to some correct function and could therefore require some distortion to correct, so a set of weights indicating which function is likelier in a function network is more useful than a single function, or alternately formatted as a function generating a set of 'certainties' like angles (indicating a probably correct point, after which there is a divergence creating a probable area), and 'randomnesses' acting like 'ambiguities' where many possible functions exist
                - this is related to other methods involving handling alternate functions but applies the insight that 'every possible connection function between points could be valid and could be altered by excluded/hidden variables its likely to encounter until it becomes every other function, and having a function network where identifying the node where a function is gives useful information about adjacent/probable alternate functions, which is useful for identifying the probability of the function becoming other functions and the probability its an incorrect function, given some known correct input variables, this function network organized by probability of becoming adjacent functions'
            - identifying patterns of error feedback indicating obvious solutions to correct the errors is useful, such as where the errors of a function that is incorrect bc its shifted to another position follow obvious error patterns that can be easily identified and corrected
            - identifying points that, when some simple connection function is applied, can generate the most other points is useful to identify as simpler connections are likelier to be true (if the whole data set can be reduced to a set of center points and circular border points around these centers, these patterns are useful to identify as being significant in their repetition, structure, simplicity, and difference from simpler/core functions like lines, even if the effect of these overlapping circles seems like randomness or a linear function in some subset)
        	- identifying useful components to describe data set subsets like adjacent point sets such as 'filters/bottlenecks/convergences (connect only different surrounding directions) and random/circles (connect points in all surrounding directions) and extremes/limits/peaks (connect similar but opposite surrounding structures) as default structures of adjacent point connections to use as components'
        	- 'filtering structures similar to outputs to find the outputs' is a useful function such as how its useful to know a set of the most unique (maximally different) functions to filter maximal differences, and its useful to know the 'most similar function of the set of similar functions' to filter similar functions bc the representative/average of these functions is more similar to each item in the set and is likely to be relevant to other items in the set
        	- identifying new densities from a known density is often trivial bc identifying the midpoint (or other representation like density) of a one-dimensional set of points in some direction to discover densities in (from the perspective of the known density looking in that direction) is trivial compared to the task of graphing all points and finding all densities, where finding 'trivial to identify' densities is more efficient and can benefit from some transforms like angle of evaluation changing the problem space of 'filtering all points past this x-value' into a one-dimensional set (comprised of a subset of points in that direction) to identify a 'direction to move in', finding local averages in many subsets, densities being likelier to be less common and therefore more trivial to identify than the local subsets used to calculate local averages to select/create points to connect, and structures like 'equally distributed points in some direction which would make the task of identifying a density or average trivial' is less likely to occur in a realistic data set and calculations can be skipped in those cases
        	- identifying what point interactions (like high volatility, extremes in ranges, high slopes) can indicate possible sources of high variation, which are the most important points to identify, to identify whether an average line (or base solution regression line) needs to be adjusted, so that identifying these 'high variation-causing points' can be an approximation or alternate of a regression line-finding method
        	    - similarly, identifying which regression lines correspond to 'what types/sets/ratio/other metadata of points indicate the points that must be ignored in order to make the regression lines seem accurate', like how 'change rate changes' and 'difference from base function slope sign changes (positive/negative difference from base function slope)' have to be ignored to make a constant line seem accurate, etc, so that checking for these types of points once a base solution function is found which might contradict it is trivial
            - identify what variable interactions look like with various error type structures like combinations and filters of these possibilities to apply tests to input variables or gather more input info (expanding the workflow loop to include changes to test/data gathering variables)
            - identify at what point inferring that a 'more complex function with more peaks exists after how many negative indications indicating the opposite' is the wrong inference and can be contradicted 
            - identify filters to select/switch between function formats like 'probable function range areas' (useful when an area seems more random) or 'average functions with surrounding error vectors' (useful when an error or legitimate function range seems too ambiguous to resolve with current info) based on different complexity structures and cases in data sets
            - applying variables to create randomness from a non-random data set (or similarly linearity from a non-linear data set) and identifying whether some randomness/linearity-resolution functions also use those connections required to create it (if some system dynamic like 'system collisions or overlaps' is a randomness-resolution function and a collision/overlap variable created randomness, do the systems explain the original data set before the collision/overlap)
            - identifying error/legitimacy resolution structures (at what point does an outlier seem like a trend-change predicter rather than an error)
            - identifying gaps in uncertainty/certainty, randomness/linearity, difference/similarity, complexity/simplicity, and error/legitimacy spectrum variables that cant decompose some change type adjacently and the spectrums which can resolve them using some resolution structure (or the other structures using these spectrums which havent been identified yet)
            - identifying the most useful tests to apply (like 'a method that can identify new inventions as well as non-adjacent high-variation variables or sources of randomness in systems like "high-distance high-randomness variables like neutrinos" and "powerful/change-triggering variables like incentives as explaining most variable interactions"' as well as reversing the function to 'find system structures created with incentives/randomness' or 'a method that adjacently identifies useful interface structures from highly different as in non-interface structures') to check if a solution-finding method is successful at which point different solution-finding methods can be generated combinatorially and tested iteratively
            - identifying similarity structures between subsets of the data set (similarities such as 'similarity in intersection with some line type having some attribute', "similarity in a point's difference types from neighbors") which explain the data set the best, as points which are similarizable by these attributes are likelier to be related in that way in the system producing the data set, these similarities serving as inputs to probability of relatedness and therefore can be usable as a filter of points to incorporate in some algorithm to determine a representative line, compared to some difference set to generate a composite ratio of relevant similarity types compared to difference types
            - identifying inevitabilities of structures and the related structures like points/ratios/distances required to identify them (such as two slopes on either side of a peak making the peak inevitable, given the input variable interactions implied by the slopes and given the distance between them and the existence of other peaks already identified)
                - identifying evocative structures (which are similar enough to other structures to be useful in deriving them)
            - identifying relevant truths like how the 'extremes/borders/upper/lower limits of the probable area range' might be a better representative format of the function than a line bc different behavior at a higher vs. a lower value is a realistic possibility that occurs in real systems, or the 'extremes with another structure' (like an average or a probability distribution of a point being in a particular sub-area, as in a different probability distribution for each 'x-value' or 'local x-value subset')
            - identifying useful structures to add/remove such as 'areas of randomness' such as a cube of evenly distributed points, which its possible to filter into possible solution functions to connect it with more certain lines around it but is also possible to remove as a structure of randomness that indicates neither priority so can be removed as possible noise and added back in when new information might help filter the possible solution functions, in cases such as where the more certain lines around the random structure are equivalent and indicate none of the solutions in the random area as more probable
            - identifying useful alternatives (like whether to 'apply multiple average methods and merge the average outputs (like an ensemble of networks)', or 'whether there is room for improvement in the average methods and finding new variants of them given their variables is worth pursuing (like a function of method errors where a more optimal point is implied on the error curve)') in existing methods by applying interface structures like 'input-output sequences' applied to 'average methods' as an important component of the regression problem space
        	- identifying unit structures to apply as components of a data set range (such as a data set density or probable function range) include units like 'overlapping circles which are likelier to describe a probable function range of a typical data set which is usually an area rather than a clear line', 'semi-circles shifted around a linear average line', 'shapes that indicate exponential change and also some non-trivial area like curved rectangles or circles as tiles of the probable function range' to indicate variable interaction structures that could represent the legitimate variable interactions mixed with some error likely in a system of some probable complexity (local change tiles which are likely to be simple shapes distorted by errors in some degree/way), where a 'probable function area as opposed to a probable function line' indicates either a complex system, a system of maximally different or locally representative alternates indicated by edges/corners or local averages of these shapes where the interim points are errors, a lack of complete input variables, an error like randomness injected in the input data set, where a better (simpler, clearer & more accurate) representation of the data set would be a previous variable set on a prior causal node where the differences creating the area are graphed as vectors and the variable interaction can be graphed with a line
        	- just like some structures are useful in their simplifying and explanatory effect (like 'core components' leading to outer 'interaction levels' supporting 'maximal differences (like a function network but in every direction)' where 'interactions are adjacent between nodes'), other structures that combine important useful interface structures like 'core components' and 'interaction levels' to achieve useful intents like 'adjacent/linear interactions' are similarly useful, and can enhance the usefulness of these structures, such as 'cross interaction level errors' like what errors can happen when an output layer interacts with the core input layer, or a way to organize the structure so that frequently interactive functions are adjacent even across interaction levels
        	- identifying 'sequences of change sets' that are commonly seen across systems, to identify common useful functions found in sequences like a 'creativity/generation' step, a 'maximally different/uniqueness' step, a 'standardization/grouping' step, an 'abstraction' step, a 'contradiction/neutralization' step, a 'incentives/efficiency' step, a 'stress/competition' step (like to see which variables stay constant and which can maximize their variation), an 'explanatory/decomposition' step, a 'understanding/organization' step, a 'ambiguity or other error generation/identification' step, a 'randomization' step, in a way that reflects real change patterns, and applied to connect known certainties (probably certain trends in the data set) and uncertainties (the remaining variation), these 'sequences of change sets' being more useful than other function formats to identify commonly repeated useful functions, as well as other patterns in change sets like cycles and equivalent alternate change sets, some of which are simpler/more useful than the original connection between original inputs/outputs, and to identify different useful function formats like 'sequences of vector structures & maps' applied to inputs (which move in the direction of function networks and allow queries like 'which are the most commonly useful function networks' and 'which function network compresses all other function networks adjacently'), as structures of 'useful incompleteness' created by mixing cross-interface structures that leave irrelevant variation unhandled and are adjacently transformed into useful variation-capturing functions
        	- condensing variables as 'some or any change in this variable set of the same type (rather than specific changes in each variant of the type)' is also useful to quickly identify sources of variation on a different interaction level
        	- identifying structures like areas/slopes of maximum volatility (adjacent input-extreme output connections) and connecting them to areas of constance as a way of identifying the maximum volatility allowed by a filtered set of functions in a more constant subset of the data set, as connecting the filtered set of a function in a more determinable subset and the volatility allowed by that filtered set can determine other local subsets of the data set
        	- identifying useful similar structures like overlaps/convergences as indicators of similar but different types of change (overlaps being an indicator of robustness or probability, and convergence being an indicator of some limit structure or an average structure)
        	- given that embedded variables (meaning 'embedded on interface variables') are likelier to create 'maximal differences', testing those as a default filter to describe complex data sets that can easily look random in some subset is useful in this problem space
        	- given that there may be a transform of the data set (like a subset of input variables) may be a more efficient way to generate the probable data sets found in real life (as in, there is a subset of input variables around which most of the variation is clear and easily explained by more common functions, indicating the original input variables include random noise), its useful to identify subsets of variables with simpler variation as complex systems are less likely to be stable and are likelier to contain random noise from variance injections, similar to how identifying simple structures like 'evenly spaced repeated data set subsets' and 'densities' can identify common patterns that are simpler to model as a pattern or a component of the solution function if they represent a sufficient ratio of the data set
        	    - relatedly to the workflow involving removing simple structures, removing some subset of the 'points' that are connectible in 'straight lines' in a data set is useful to remove structures too simple to be useful (in the sense of relevance for the intent of capturing information beyond a general simple summary base function to use in finding the actual solution) in describing a more complex data set as is more commonly found than a simple linear data set
        	- some structures are more probable than others 'a range of data points around a pattern (as in an area of probable range of a function) which describes likelier structures like errors' is likelier than a 'highly variable specific function (like a function with many peaks)', so these likelier structures can be applied as more default than other structures
        	- function sets which are adjacently transformed into each other which have some base of similarity in common (like accuracy, variation, input/output patterns, or input variable subset ratio) are likelier to be equivalent alternates and probable solution function sets than other function sets
        	- 'maximally different data sets with known error types applied, mapped to solution functions' are another useful starting point for reducing the regression problem
        	- variables like 'dependence/connections between inputs' (such as a 'sequence using the previous adjacent input as an input', as opposed to a 'function of independent variables') that create dependent/independent variables are also useful to identify, as these variable differences are important to filter out, where independence of variables is not obvious but a sequence is easily detected
        	- identifying structures (like isolatable structures, including obvious components, such as 'anomalies') which are likely to explain some variation interaction patterns (like an 'occasional extra peak disrupting a more probable pattern') as common error structures to remove to find simpler functions and add to find error-handling functions or erroneous versions of data sets, similar to how modeling a 'system cascading to destruction' is useful to identify signals of these error structures in data sets, just like identifying 'probable new more stable states' of a data set is also useful
        	- increasing the 'directness' or 'adjacency' of variable interactions as being more explanatory, more likely to be linear/simple, and less likely to be subject to undetectable noise/errors is a useful intent rather than trying to identify distant connections between indirectly causally linked variables, as indirect connections are likelier to change (but also likelier to re-occur when removed so extremely distant effects are still useful to model)
        	- similarly, given how the problem of 'finding a function to summarize a data set' is a problem of 'finding missing variables (coefficients, powers, etc) that are not in the input', other formats of a problem are similarly useful to apply as defaults (what are the patterns of missing variables, are they more difficult to detect, are they less visible using data gathering techniques, do they have similar complexity like 'host dna' & 'pathogen dna' which allows inferring the 'existence of dna and pathogens in a host', and 'default function errors (like immune errors)'), similar formats like 'stable variables' to identify variables that are more stable than other variables so they will become 'high-variation'-causing variables, a useful structure to identify with 'high-variation variables'
        	    - for a more complex specific example, identifying people from their dna involves identifying 'missing variables' (like phenotype-determining genes, as well as 'genetic change variables' such as epigenetic changes, errors in dna-phenotype mappings, mutations, disorders in dna editing, etc and causes of these 'genetic change' variables) and filters of these missing variables (tests of these variables to help filter them or legitimate stable differences to filter/separate these), which are variables in between the inputs/outputs in different interaction levels than the inputs/outputs like the 'gene function' interaction level
        	    - similarly, the machine-learning problem involves finding 'missing in-between variables' (like a 'function to create differences (to find variations of structures like combinations of inputs)' and a 'function to attribute/connect useful differences to filter out (like connecting an error difference to a node or node structure)' and a 'function to calculate differences from correct values (calculate the error/loss from a particular input variation combination)')
        	    - these are highly complex sets of variables (input/output subset filters, high-variation change inputs, 'instruction' functions generating similarities, variable interaction functions, generative functions, difference-filters) which fits with highly complex systems, decomposing them to slightly less complex variables on different interaction levels, which can help with identifying missing variables (such as inferring 'alcohol' as a cause of 'genetic changes' when other complex sub-systems arent sufficient to decompose all variation in a data set, as a source of variation from another interactive complex system, given how complex systems usually are created by interactions between multiple complex sub-systems)
        	    - variable interaction functions like 'add/subtract/neutralize', 'limits/extremes', 'filter/differentiate', 'repeat (like as a default)', and 'interfaces/averages' are common (or even required) across systems so are likelier than other variables/functions to re-occur in unknown systems, which makes them useful to apply as default variable interaction structures
        	- similarly, applying known 'contradictory cases' of 'when known rules are wrong' such as when a more general function is wrong compared to a less general but more accurate function despite a common solution metric like generality
    	- similarly, identifying the 'core shape of a local subset representing a change unit of the more general function' and 'its interaction function (such as overlapping with other core units)' and its change functions (such as how it can 'rotate to some degree or vacillate in some way') as a way to identify the general function from a local subset that is sufficiently representative of the differences in the data set that it can be used to find the 'core change unit shape' that can be repeated/shifted/scaled/rotated/otherwise changed to find the rest of the function using some interaction function
    	    - relatedly, finding the network that filters the possible core change type combinations (rotate, scale, shift, vacillate/cycle, embed variables, repeat, abstract, connect, format, etc) in a maximally efficient way for most functions is a useful intent to fulfill as a default implementation strategy to start with
    	- similarly, identifying useful structures like 'maximally different directions of change such as the cardinal directions (or high/low left/right directions of change)' as useful structures to use as a filter to identify different change types that are common and highly different, as a 'maximally different unit of change' which is useful to find adjacent changes
        - similarly, identifying connections between variable interaction structures (like how a variable structure such as a 'variable upper bound and a constant lower bound' have a useful structure of "implying but not guaranteeing (making them probable and useful to test)" other variable structures like 'more change (either expansive/reductive) happening in the upper range' or 'fewer limits on change in the upper range' or 'more change incoming to the lower bound' or 'interface variables and/or constants relevant to the lower bound') which can be connected to possible filters/limits of those structures, such as whether other variable interaction structures (or specific known problem space system rules) filter/limit/prevent a possible implication
        - similarly, identifying local subset representation structures (like densities or average lines explaining the majority of differences in the subset/densities) for one local subset and then removing those probable components of various possible representations in another subset to find additional possible variables in maximally different subsets to filter the set of possible alternate representation structures (and their components) from the original subset, after identifying the maximally different subsets likeliest to have differences in probable representation structures & their components
        - similarly, identifying connections between 'uncertainties' (like subsets of the data set that are more uncertain/variable where other subsets are more easily determined) and 'uncertainty resolution functions'
            - resolution functions like 'intersecting with the most points in the subset with the simplest line' or 'the simplest line that remains some minimal distance away from the most points in the subset' or some balance of 'averageness' and 'intersection' which are useful variables of filtering/generating these 'alternate regression lines' in 'highly uncertain subsets'
        - similarly, identifying useful structures like the 'variation range' necessary to create a useful structure like an identified 'acceptable error range area' (with a trivially identified area of coverage that is likely to be useful in finding a maximum of points falling within that area when moved across the data set 'probable regression range') that can be moved across a 'probable regression range' and tested in various subsets to cover a 'ratio of the data set in that subset', this 'acceptable error range area' being useful to avoid calculating the error for a possible line at every input value and just checking if it falls within the range area centered in some 'probable regression range', either after calculating some 'probable regression line' or using the 'probable regression range' to fit it inside that range, or finding the trajectory of the 'maximum coverage direction' as the area is moved across subsets
        - similarly, identifying the relative usefulness of structures like 'intersecting/overlapping summarizing lines of data set subsets' as opposed to 'adjacent local subset summarizing lines with strict range limits' to allow for possible overlaps in local subset selections bc the borders of these ranges might not be guaranteed/required by data and to allow for known variable interaction patterns like 'multiple possible interaction types/states involving the same variables, variable interactions which can explain variation in outputs with similar/adjacent inputs
        - similarly, identifying alternates of useful structures like 'randomness' (such as how 'randomly dropping a ratio of data points can reveal robust variables') alternatives such as 'combinations/sequences of common/powerful functions across systems' can identify more probable structures to replace these less likely structures, using probability structures like 'commonness' to replace the less accurate/likely structure like 'just any randomness at all (a random selection of randomness)', which applies 'more probable & less random' randomness created by a higher degree of certainty through alternate structures like 'commonness'
            - similarly applying more relevant/useful structures of randomness like 'obvious structures like simple shapes like densities removed from the data set' which are more relevant to workaround (as simple-minded agents are likelier to intervene with obvious human error and likelier to be required to stop the interference of), as true randomness is unlikely to be easily verified like obvious randomness and is less relevant/useful to account for (above some ratio of expected noise), as more complex randomness could easily be hidden legitimate variables undeterminable from the original data set requiring more data/variables to identify such as 'incoming changes' to the data set
        - similarly, identifying a function to convert non-linear to various probable linear functions (more adjacently computable, or using fewer variables involving equivalent alternate variable subsets of the data set) is a useful intent to fulfill with structures like logarithms, topologies, & mappings to model 'interaction levels where interactions are adjacently computable with objects defined on that level'
    	- similarly, an 'index of pre-computed regression lines to compare with original data set subsets' to fulfill useful intents like 'avoiding computation' is useful for connecting original data set subsets and pre-computed regression lines for alternate subsets
    	- similarly, how clustering relevant structures (like inputs) by differences/similarities (such as organizing by the 'similarity/equivalence of output value') can identify useful structures like input patterns of similar outputs given assumptions like 'non-volatility' and 'non-randomness' and 'non-uniqueness of inputs for each output' (like input subsets that have clear patterns, like how a wave function organizing the inputs by similarity of output would have clear patterns of magnitudes/amplitudes in a few subsets that prevent requiring checking the whole input space for the pattern, as the output y-value would have a few data points associated with it such as 'input value points graphed vertically with the y-value on the x-axis' representing inputs having that output, where this vertical input pattern associated with a y-value would have clear patterns such as obvious differences in distance between points that are clear after a few points rather than many), which reduces the problem to 'find n y-values having the same value m times to check for obvious patterns in x-values for each y-value, if there are multiple x-values'
    	    - preemptively testing for validity of 'assumptions of algorithms' is an example of 'alternate equivalent structures' (like limits/requirements/intents) and an application of the 'input/output sequence' of optimal algorithm filters
    	    - this applies a useful function like 'sort' to the outputs rather than the inputs to get useful information about similar outputs once sorted that way such as how variable/cyclical/maximally different the function might be by comparing inputs of equivalent outputs
    	    - once outputs are sorted, it is possible to find highly different outputs easily, which is useful for intents like 'check for maximally different outputs' or 'find output range or output extremes'
    	- similarly, 'finding average magnitude/amplitude/count of peaks in a data set for some subset' is useful just like 'finding the general average value' and 'finding local subset averages' and 'finding reoccurring subsets' and 'finding highly different local subsets and their connecting functions' are useful
        - similarly, a 'adjacent points merging function (using some representation like an average midpoint, or using a similarity metric like distance from/angle to local densities, and using some ratio selection like merging n points at a time within some distance m of each other)' and a 'adjacent point non-merging function (leaving some points unmerged bc of their representativeness of legitimate differences)'
            - similarly, an algorithm to identify variables that can capture high variation when applied together (representation metrics, similarity metrics, ratio of input metadata like count/difference score) as useful complementary capturing variables of information, acknowledging differences in data sets like that averages are sometimes more useful than similarity metrics and sometimes the opposite is true and there is a useful input type like 'input count/ratio of the total count' that is optimal for some structures like high-variation data sets with some noise level
        - similarly, applying similarities between structures that have a reason why theyre useful for summarization/representation intents (like how 'big/simple' shapes like 'equilateral' shapes are particularly useful to identify to simplify the task of identifying a regression function bc finding their centers/averages/densities/patterns is more trivial and likelier to reflect reality as big/simple shapes are less likely to occur by accident in a data set and therefore likelier to summarize the data set, up to a certain point, like how identifying that a data set as a whole has a generally square shape makes it likely to be equivalent to random)
        - an example resolution function between 'densities and regression lines' is 'divide into subsets, then find one representative density for each local subset, then expand densities until an overlap/equivalence is reached with another expanded density' by applying the 'reason' for why it would be useful to connect 'representations (like density averages) of adjacent local subsets' (bc adjacent local subsets are connected in the original input data set, so the reason to connect them (or a variant of them like a representation of them) later is that connecting them later 'aligns with the original input' in a relevant way, relevant by 'preserving the information of the original data set' which is useful for the 'find a regression function' intent) and applying the structure of 'how' to connect them through 'expanding' them (and the reason why to use that, which is an adjacent transform applied to a density average and is therefore useful, where equivalents are also trivial to determine, and these operations in total can beat other regression algorithms in some solution metrics)
        - similarly, a function that connects the 'points that vary' and the 'points in common' across multiple probable regression lines is a useful function to solve for 
            - finding the sections of the regression line that would be variable in variations of the bias vs. variance tradeoff, to focus on finding functions to connect the 'points in common across probable regression lines' that should be optimized for in the final function, where the 'points that can vary across regression lines' can be averaged or otherwise represented by known probable points at discrete intervals rather than a continuous line, where a point not on those points can be approximated by adjacent points, where the 'points in common' can be approximated by a range that is narrower than the range for the 'points that vary' and the range representing a range of acceptable solutions, so finding a function to resolve the reduced solution set of the 'points in common' connecting functions by connecting these subsets with the functions describing the 'points that vary' is a useful function to solve for
	     - finding the connecting function between different sets of summarizing functions like the 'average' and a 'slope-standardized function (to find the useful standard to compare changes to, to find the core differentiating vectors from a straight/average line) and its scalar to scale it to the original' and the 'lines that describe local subsets to the points of extremes (similar to eigenvectors)' and the 'lines that connect averages of densities' is a useful function that connects these alternates which offer the same representation attribute but also capture different information in the data set, as connecting 'efficient representations' is more trivial than connecting 'every data point'
	         - finding the 'useful core function representing the most standardized (such as de-scaled) function' is useful to find a 'component function' to check against multiple subsets of the data set (do any known variable interactions create change types other than this component function or component function range or do they follow the structure of the component function/range) and look for variables that adjacently create/scale the core component function to check it for realistic probability of representation of variable interactions
	     - finding the most important structures to check for when filtering possible solution functions (such as how its important to check if an amplitude of a polynomial is different across different peaks to determine if a peak pattern can be applied/found, how its important to check multiple local subsets of the function input range, etc) can act like maximally differentiating filters of the solution set
    	- finding the useful ratios & other structures of inputs to an algorithm like 'find the common slopes of connection lines between points in local subsets of the data set', where the algorithm to find the 'useful ratio/count of slopes in common (a ratio compared to some standard, like the number of possible connections)' is the target to solve for, as the other structures that are useful are already known or easily determined and the uncertainty is in finding the threshold values or other values to optimize implementations of those structures
       - connecting alternate formats of the data set/regression functions like 'maximally different connectible shapes (like interfaces) that can be formed by a data set subset of some ratio' which can be used to indicate 'embedded variables' (like variations on that interface) is useful for determining one function format from another which may be more trivial than another method
        - other structures than standard regression structures (averages, connection lines, subsets) like 'maps' can be applied as a useful structure in the regression problem space bc of how mapping one subset to another through substitution can be an efficient way to decompose a more complex set of points into a more standardized or otherwise useful set that is likely to represent the original set and requires less memory to store, which are useful as components of solution-finding methods
        - finding useful metrics like 'degree of erroneous difference to ignore' between obvious average functions of local subsets is useful to find out what information to ignore when an average line of one subset differs to some degree from an average line of an adjacent subset, especially if the next subset confirms the original subset average line, applying the concept of 'data corruption' to describe some degree of error deviating from some implied metric, resolving these 'implication' structures (like the implication of a 'common subset average line') into 'conclusion' structures (like a degree of commonness of that line across subsets above some ratio), and finding useful tests of these differences, to find out when a difference may reflect a common or otherwise probable/implied structure (implied by adjacent inputs, common patterns, similarity to known implications, etc) rather than an erroneous anomaly to ignore
	- in the 'network (fuzzy space) of structures' fulfilling intents, interface structures like 'overlaps' exist between structures adjacent to or otherwise useful for multiple alternate intents, these interface structures indicating their usefulness for other intents like 'deriving alternate intents' and 'building a maximum ratio of structures'
        - in the space of useful structures, concepts like 'balance', 'alignment', 'simplicity', 'probability', 'composability', and 'uniqueness' will be obvious, which can be used as 'conceptual filters' of useful structures that are likelier to be useful than other structures
        - an example of this fuzzy space includes structures (like 'angles, partial closed shapes (like sides and corners), connection functions of partial closed shapes, higher-dimensional closed shapes, shapes that when combined can produce a closed shape in between them') as the set comprising the fuzzy space of a 'closed shape', this space being composed of components adjacent to or otherwise useful for fulfilling intents (forming/describing/differentiating a 'closed shape') related to a 'closed shape'
        - a network of similar/equivalent alternate spaces include a 'non-repeatable/unique component space (which is optimal for storage minimization)', a 'repeatable component space (which is more optimal for displaying usages/queries of components)', a 'usage adjacency component space where frequently co-used components are adjacent (that is useful for finding probably useful structures using a component)', a 'difference as adjacency space (where maximal differences are possible with adjacent queries)', a 'layered space with both intent/structures (where the fuzzy space of an intent contains maximally different structures fulfilling/adjacent to fulfilling that intent)' bc of the adjacency of these structures for these intents related to components
            - relatedly, a 'usage network (apply)' has adjacent corrollaries like a 'filter network (find)', a 'component network (build)', a 'difference-resolution network (derive)' due to the core functions it is an equivalent alternate to
        - graphing an intent by its 'surrounding related structures' such as by 'structures that use it' or 'structures that fulfill/build/create/cause it' or 'its input/output structures' or 'structures that filter out everything in some relevant subset but that intent' (or similarly 'structures that determine that intent') is another way to visualize structures like intents that are more useful when defined as a set of alternate definitions which can represent examples of them in some other system
    - framing common structures with relevant metadata like useful intents in standard terms to maximize the optimal positioning of these structures in queries
        - a network (which depicts uniqueness and similarity) is useful for 'finding new/different unique similarities by the gaps & other structures in the network' as well as 'identifying difference/similarity of two known structures'
        - a map is useful for 'finding unique connections & other metadata about connections' (like the commonness of connections having equivalent/similar connection/input/output) as well as 'translating a structure in one format (of a set that can be described by a network indicating uniqueness/similarity) to a corresponding position in another format (of a set that can be a network)'
        - a filter is useful for 'finding a similar subset of points in a network/set of points' (as in similarity to some attribute, like a solution structure/metric/requirement)
    - finding a good starting point to start applying interface structures is crucial for deriving adjacent solutions, like how a limited subset of 'logical rules such as definitions' or 'physics rules' or 'truth limiting rules (what is definitely not true)' might be useful as a starting constant input to start applying interface structures (like changes) to, to derive other rules that follow logically, are required to be true, are implied, are not contradicted, or have other structures of truth associated with them
        - similarly, finding a 'useful structure to describe common patterns in changes' is an example of a useful isolateable structure that can be a good approximation of a full implementation on its own, answering questions such as 'are most variables an adjacent combination (or other core structure) of some subset of interface structures', which is findable with iteration
        - similarly, finding a 'reason for similarities/differences' (reasons like 'its an efficient/useful combination of few inputs commonly available, so is often repeated across irrelevant systems') is another isolateable rule set that is a good approximation of a full implementation of all logic rules of interfaces
        - the differences between these 'equivalent alternate' isolateable rule sets that are good approximations of interface analysis make them useful to combine in an adjacent combination as offsetting 'ensemble' structures to weight the impact of their outputs against average outputs & other representations of outputs
        - this is like how everything can be framed as a component that can be added/multiplied to other components, but thats not always useful in terms of reducing computation requirements, such as how knowing 'addition' and 'multiplication' are capable of describing all other structures, but that doesnt capture a useful degree of complexity of the potential interactions of those two operations, where knowing concepts like 'self' and 'embedding' is more adjacent to the complex operations/functions possible with addition/multiplication (self-multiplication like 'powers' and embedding as in 'embedding of operations'), these two concepts being adjacently derivable with interface analysis through core structures like 'unit/identity (self)' and 'application/usage (embedding)', similar to how matrixes (aligned multiplication of ordered sets) and inner product spaces (spaces where some product is trivial to compute) and convolutions (complete multiplication product of sets) are not adjacent to just the functions add/multiply
             - this is a useful structure for tasks like 'encryption' as the 'set of concepts that are adjacent to a useful structure' is more difficult to guess (from the set of all possible concept sets) but is easy to verify
    - applying interface structures to optimize with ml, such as by applying 'input/output sequences' to position 'generative feature layers before filter feature layers' and other patterns that make sense to server as a useful contradiction to offset the irrelevant variation introduced by the preceding layer, or applying 'self' to apply neural networks to select preprocessing & algorithm functions/parameters
    - to handle common error structures like 'dead ends', apply structures like patterns to find the solution to (the 'way out of') traps using these errors, such as by applying insights like 'nothing is unconnected to everything' which means 'there are no real dead ends' as 'everything is both true and not true in some way' and find the distortion of the perspective that created the error of the 'dead end' and connect it to the balanced perspective (where interface structures exist or are adjacent), creating a difference that allows other differences to be embedded/connected/supported
        - if there are filters allowing for only one possibility (a 'dead end' error that leads away from a network), find the filters that represent the errors in those filters, reversing the perspective back to the balanced perspective and exiting the perspective creating the error of the lack of variation leading to that error, creating opposition to the incorrect difference
        - inject more variables to connect the 'dead end' position back to the balanced perspective, applying the interface metadata to create new differences (such as random differences through interactivity) where required to offset the incorrect differences of the over-reductive perspective, and use those differences to build differences on them and create change in another direction
        - where one structure seems to capture everything (like an attribute network), apply differences to identify other networks that capture alternate complementary information (usage networks, limit networks, difference networks, function networks, etc) to limit the limits of that over-reductive perspective
            - like how an attribute (such as 'criminal') can obscure info or over-simplify info, despite being an efficient way to store some relevant info (like the 'crime of jay-walking' being equated to all other crimes despite the fact that it is mostly only criminalized to collect fees to fund police stations and other far more harmful behaviors are not punished at all, such as stalking/copying inventors), whereas a network with contextual or functional info can more accurately depict that info to reflect its true variation
            - this is similar to how one infinity can be used to create other infinities (like the Banach-Tarski paradox) bc these graph structures are like topologies or fields in that they are capable of describing reality at every point but offer useful alternate advantages when starting from different points
        - this is like the structure of a 'mobius strip' or an 'isolated system describing the system containing it by identifying variables of observations of subsets of its own structures (regarding the incompleteness theorem)' where one structure seems ambiguously equivalent to different structures that seem to contradict each other but actually can co-exist in the same structure using the perspective interface, or like the structure of a 'rule set' occupying a point on a torus where what determines one error doesnt determine another error bc different 'rule sets' are supported and the 'rotation' and 'injection of new interaction levels (as different concentric circles allowed to be the base/core/stable level)' allowed in the torus shape enables endless (stable) balanced variation
        - a 'balance' of variation is a core structure driving interfaces, as there needs to be some variation-supporting structure like a ratio of change (like 'potential/kinetic energy' and 'momentum') and counter-change (like 'gravity', 'energy preservation/transfer limits'), or a structure like 'caring' ('connection to the most stable foundation, where this connections acts like an equivalence') that is supported and stable, otherwise the interface cant exist or similarly cant support any change, which can be used to find interfaces
        - on the other hand, filters may seem restrictive/limiting/reductive (like a trap that is a 'required error' such as a trap hiding a 'dead end' error) but may enable endless complexity/variation, like how a 'reality' filter may seem boring/reductive/simple until you identify how complex reality is, and that the 'reality' filter is powerful in that it empowers other structures to exist like 'clear descriptions/representations & measurements/experiments' as well as constants/variables and consistencies/contradictions to occur, enabling the pursuit and identification of truth and falsehood and the application of differences to both
        - "filters to identify errors such as common structures of paths leading to 'dead end' errors" are a way out of errors of 'constance' (like 'over-prioritizations' such as 'over-reductions') just like finding 'abstractions' or 'interaction levels' or 'equivalent alternates' or 'embedded perspectives' or 'connecting function of perspectives' are a way out of traps
        - finding a common perspective that hosts both a trap/error and also a solution/way out of it (or generally a 'perspective that can trap any trap' as in 'capture the variation of incorrect differences driving traps') is an example of a useful interface to apply to other problems
        - similarly, 'traps/errors may be more adjacent to a solution than another trap/error', so creating a 'path of differences leading to errors' is not just a way to find errors but also find out what is not a solution and therefore what is a solution, and value can be created from a trap if there is a way to convert it into a solution like by applying it to itself ('trap the trap')
        - the 'maximally different structure' that can support the most difference types is also the structure that can support adjacent structures of differences, like 'ambiguities' (like the ambiguity in an equivalent distance of one error from the center of this structure to the distance to another error, and like 'contradictions/paradoxes' and 'counterintuitions/complexities' and 'alternate representations' as different variations of 'maximally different structures producible with the same inputs, supportable in the same system')
    - finding a solution base that is optimal for different algorithms allows finding different possible solution bases, at which point finding the more optimal adjacent solution to those bases is possible with known/adjacent algorithms (as opposed to finding 'maximally different solution bases' to start from with one particular algorithm)
        - finding the maximally different functions that make an adjacent optimum findable with some algorithm helps 'filter out these more adjacent solutions if theyre incorrect' and 'find counterexamples of alternate possible solutions' or 'divide/filter the solution space' faster, such as by finding 'common overlaps in solution spaces generated by different algorithms' where these overlaps are more trivially calculated in some way than by applying either/both algorithms
        - deriving the 'solutions findable/verifiable with an algorithm' is a useful way to filter the solution space, find common solutions across algorithms, and find variables of algorithms to find other algorithms or match algorithms with metadata like intents/metrics optimized for
        - finding a 'network of algorithms with rules for switching between algorithms in certain cases like with certain input patterns or certain solution metrics' is also adjacent using these structures of connections between 'algorithms' and 'solutions/ranges/sets adjacently found/filtered with those algorithms'
    - finding the set of concepts that builds a solution (such as how 'adjacency' and 'generality' help build a 'regression' solution) which help to offset each other ('adjacency' in the form of 'adjacent/local subsets' or 'adjacent/local optima or adjacent/local density averages' offset by 'generality' in the form of 'representative subsets' or 'representative summaries like averages') can help form a common base set of solution structures to build on top of
        - these are useful specifications of more general spectrums like 'specific local variables, as opposed to general abstract patterns/summaries', 'adjacency' being a non-definite partial format of 'specificity' in the 'regression' problem space, having the additional concept of 'triviality' included in the subset of concepts driving 'specificity' to relevant variables like 'position (point, density, extreme)' in the 'regression' problem space
        - finding regression lines whose differences from data points frequently follow a common component pattern (like 'having the same distance from data points' or alternately 'use the fewest components (one component as in the same value)' which fulfills a 'simplicity' or 'linearity of combination' metric) is another useful intent to fulfill in the 'regression' problem space
        - similarly, finding a set of components that commonly connects extremely different points (like points from different non-adjacent subsets at different limits (upper vs. lower) connected to some representative subset like an average) is a useful intent to fulfill in a regression algorithm, the specificity of this intent making the algorithm trivial to find
        - similarly, applying rules of relevance such as 'removing variables with equivalent information coverage is acceptable if one variable remains to cover that information in an intent related to an information-preservation intent but its better to leave the variables/rules generating those variants in the data set rather than leaving in one variant or all known variants in some cases like when minimizing memory storage is prioritized'
    - apply common structures like filter/reduce/match/add/change/map/sort to format functions to identify the maximally different functions, then find the reason why those functions are useful as a way to identify 'rule sets that can act as limits of usefulness to identify the areas and boundaries of usefulness'
    	- example: 
	    	- bc its useful to have a 'map of a keyword to different variants of it', its possible to identify that "a unique signal has variations because of alternate definitions/formats it can take while still remaining unique (bc of 'definition routes')" (allowing this 'variable interaction' & 'variable interaction-structure rule' regarding usefulness to be derived: 'bc this variable interaction exists, keyword maps/definition routes are useful')
	    	- bc its useful to have filter functions in general, its possible to identify that 'not all differences are adjacently useful for every intent' and 'different variations of structures are not organized by default' and 'different structures can coexist'
	    	- bc maps (connections between 'user-assigned (relatively arbitrary)' rather than 'absolutely-defined' values) are useful, 'arbitrary connections' are also useful for intents related to randomness/uniqueness, like when connections dont matter absolutely but are still useful to assign (like organizing a filesystem to optimize common queries such as using symlinks or naming conventions, even though that organization doesnt reflect absolute truths of the universe, or such as how mapping several terms to one identifying term allows quicker identification of unique term usage while minimizing memory storage, or how an explicit map to identify a category of an attribute value set is useful when identifying the causal variables is non-trivial, to skip that causal analysis, or how substitution maps can create the appearance of randomness bc they are relatively arbitrary)
	    	- bc maximally different functions (filter, map, change) often co-occur indicating their usefulness, its possible to identify the 'common complexity of problems solved more frequently recently (bc of existing solutions to simpler problems)'
	    	- bc similar functions often co-occur (filter, sort) indicating their usefulness, its possible to identify that those functions are cooperative for various intents ('sort' speeds up some filters in some cases)
	- applying rules to identify when a regression function can be incorrect mapped directly to problem space structures
		- example: when a high ratio of incidental/random variables (like variables about a context that any species can exist in which is not relevant to identifying a species) are included in the function, or when an important variable is ignored to fulfill an incorrect priority like simplicity, these errors are mappable to differences between the incorrect function and data set subsets
		- when a more simple function is found but it contradicts the more complex function that would cover more cases bc of the data set subset chosen/available that makes the simpler function seem correct, its violating other known solution priorities like generality which can offset over-simplification errors
		- more importantly, structures of this error can be mapped to a set of example possibly relevant regression structures (data set subsets and regression lines)
		- for example, when there is a 'small pattern in a local subset indicating a more complex function' which is 'within the boundaries of a potential field of variable interactions' and 'not overlapping with probable error areas' but is ignored in favor of a simpler function, thats a structure that could be an error of 'ignoring a general trend bc of a data set subset selection (applied either before receiving available data or after)'
	- applying interface structures to known useful structure (like gradient descent) given their definition can identify relevant structures useful to those useful structures like optimizations to apply to inputs before applying the useful structure
		- the idea of 'gradient descent' is most useful when applied to a 'function that is adjacently connectible to the optimal solution' (meaning the inputs are already adjacent or equal to the actual optimal inputs) bc of the definition of 'gradient descent' which applies 'adjacent change combinations', assuming that 'adjacent change combinations' are capable of finding an optimal (which is best used when 'everything is adjacent' or 'maximally different examples are adjacent', etc)
        - this makes 'maximally different interaction levels' a useful structure to identify, to make most variable combinations or 'the most variable' variable combinations adjacent (which is what interface analysis does by default)
        - for example, to identify the structure of 'imaginary numbers (like i)' as a possible useful structure, some intents (like 'create a circle' or 'create common structures (like a circle)') make it obvious to try as a component of formulas and its not adjacent change combinations as theyre typically implemented ('addition/multiplication') but rather as an adjacent combination of interface structures (like identifying how addition/multiplication arent easily describing all structures and solving all problems, so try applying the 'opposite' of 'components of those core operations' to try to generate differences to use to describe different structures), for example to solve the problem of finding 'equivalent alternate maximally different descriptions/generators of a circle as a useful structure to describe/generate/apply', otherwise the idea of 'square root of negative one' is not adjacent to most intents but is obviously useful to identify, so identifying 'useful intents that would make such structures obviously useful' are useful to identify by applying interface analysis to fulfill probably useful intents like 'create common core components in different ways'
        - as another example, the way I discovered that circles are related to primes is by picturing multiplication in my head, in which I could easily generate squares/rectangles but I noticed there were gaps between these shapes' corners which were easily generated by integer factors, and these gap points were likely to be more describable as on a curve rather than as a product of integer factors
        	- https://twitter.com/remixerator/status/997394393471516672
        	- it also reminded me of the fibonacci sequence bc of its embedded growth between numbers in the sequence rather than clearly exponential or clearly linear growth, as a different type of default growth that was neither of the two simpler types
        	- the idea of 'gaps between integer non-1 factors (when graphed as pairs of factors)' is not actually definitive as indicating non-linearity but is still evocative and therefore still useful in adjacently deriving that non-linearity is relevant, these 'evocative' rather than 'implicative' ideas are still useful in deriving new probable structures to test
        	- this insight path would be easily identified with interface analysis 'which asks questions like "what something is not" and "what is not easily generated by these variables" and "what is not adjacent" and "what is different" and "where would similarities be expected (like with other integers) but differences are found instead (like with primes) and what interface unites these (like the dichotomy of curves vs. rectangles)" whereas machine-learning dumbly applies a few insights at a time such as 'adjacent change combinations eventually can find some useful functions if you have enough compute',
        	    - while applying other relevant structures, like filters to avoid irrelevant exceptions/rules like pieces of the definition that most closely overlap with other more relevant functions like the 'identity function (using multiplication)' being more relevant than 'addition/multiplication' as the identity function is less meaningful than non-1 factors)
        	    - identifying the alignment between 'numbers between non-1 integer products in 2-d space' and 'numbers between non-1 integer products on the number line (primes)'
        	    - other useful structures to apply are:
        	    	- 'equivalent alternate formats' of primes and their factors (the relevant default variables) such as 'sets of alternate factors and lines connecting sets'
        	    	- the basic 'interface' structure which connects the 'equivalent alternate formats (number sequence of primes and factor sets of primes)' using a common standard they both adhere to in their similarity of patterns/gaps
            - relatedly, deriving other useful structures like 'e' can be formed by maximizing output change types (differences between adjacent values) while minimizing input variables (the 'previous value' and the 'previous previous value' and the 'addition' operation) without using exponents and using a simple function (minimized differences necessary to create the extreme differences)
        - similarly, 'adjacent change combinations' like addition/multiplication are not frequently useful for useful intents like 'differentiate a wave from a circle' which a machine-learning algorithm typically would not adjacently achieve but interface analysis would by applying structures like circles by default as a common core structure on the structure interface
    - applying interface structures like 'causal sequences' to identify useful structures like 'alternate points where variation is useful to inject'
    	- for example, injecting variables at the point of data-creation/gathering rather than data-processing (pre-pre-processing) could influence the value of data (its reflectiveness of reality) such as how 'smiling' before taking pictures can make some variables more obvious/detectable, which is a 'reality change rule' that is useful for making variables less ambiguous/more obvious so a classification algorithm can be simpler, so an algorithm to identify these points/rules is useful to improve data quality, or to identify more useful problems to solve like 'if the subject was smiling, what would these hidden variable values probably be, based on similar expressions as smiling that are generatable with existing data' (similarly, 'identifying a dye to inject (input side solution) or a treatment to try (output side solution) that would differentiate cells more easily' is a useful structure for a classification algorithm to be able to identify to improve data quality and delay decision-making of the model structure until a data quality ratio/info minimum is reached), which applies the insight 'bc there are some differences between items in different classes, there will likely be other differences that are detectable'