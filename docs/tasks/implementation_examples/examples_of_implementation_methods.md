- examples of other implementation methods than documented default methods (like the 'apply/find/build/derive' function set is an 'implementation method' of interface analysis as its an independent set of structures that can be applied as defaults to implement it to fulfill some solution metric like comprehensiveness, minimal constants, variability, etc)
    - this document contains 'function sets which can act as implementations of workflows' (function sets which can implement a solution automation workflow) as well as 'solution automation workflows' (useful sequences of steps to solve a problem)

- how to implement the interface analysis framework as a set of simple functions for an initial version, involving alternate default constant simple structures to combine, such as:

    - identify useful structures like the 'useful structures found/almost found, but not used/connected/understood, interface structures' or 'useful structures not found' by a particular algorithm, and how it could be altered to find those interface structures and use them to improve its own solution metric values (like whether a particular algorithm has enough 'variation potential' to simulate itself and its errors, and enough built-in self-evaluation requirements like 'identify useful structures and abstractions of them found during training', that it can find those structures)
        - for example, the fact that an AI algorithm might frequently find a common solution structure like 'create an index of some answers rather than leaving those variables uncertain or re-calculating them, as they are unlikely to change (info interface) and pair it with adjacent change combinations of existing known solution functions (change/core interface) using some reward mechanism to incentivize additional changes of some type/direction' doesnt mean the AI will find other relevant useful structures, like the following, but it does mean that this structure is useful and that existing algorithms can find some similar/equivalent solution structures and that this solution structure is trivial to find for some algorithms
            - the 'cause of the solution success' as in 'why its useful', or identify that its another useful structure such as a 'cross-interface structure' (a info-change/core cross interface structure) as well as a 'cross-certainty structure' (variable + constant), or otherwise identify other useful structures like 'cause/logic/other interfaces' or other reality-covering variables which could replace the 'info index and the adjacent changes with a reward function'
            - relatedly, this solution is findable to this algorithm bc the solution is trivial to find with its available functionality (as in 'adjacent to inputs') and is a trivial change of existing solution structures
        - this is partly bc AI algorithms (even those tuned to find solutions) use solutions differently than a human brain does, as a 'possible useful output of some process or set of variables applied as inputs', in isolation of other info like other problems, other queries of the brain/model to solve problems, the solution's impact when applied across interfaces, patterns between solutions found, effective functions to describe all solutions, etc
        - a 'reinforcement learning algorithm with some feedback/reward mechanism' (related to specific loss evaluation methods like gradient descent), applies a core solution automation workflow like 'change a base solution (until its across some threshold of a solution metric)', which is not a required workflow to use, 'finding patterns/structures/variables of solutions or other useful structures and re-combining/varying these patterns/structures/variables is an alternative that doesnt necessarily involve using known solution structures'
        - its feedback evaluation function might not be able to adjacently detect subtle differences or hidden variables or be able to use alternate info as input, such as 'info about the network params and algorithm sub-functions' as input bc those will have to be simulated while using them to evaluate them, and the variation required for that simulation might not be supported by the network while also running the evaluation function
            - if its not adjacent, some algorithms will never find it
        - given that these are known errors, a good algorithm would have to combine algorithms like this in a set with a different algorithm tuned to handle this error, at least to provide a counterpoint or contradictory view (like a 'maximal difference' algorithm applied with an 'adjacent change combination' algorithm) if not the trivial improvement that is more useful than a counterpoint in some cases, where the exact opposite might be too extreme and thereby create errors, as the exact opposite implies there is nothing optimal about the original solution structure and it should be opposed completely to test its opposite
        - a good algorithm should be able to re-identify a missing interface structure, if one is missing, using the others
            - if you took the useful structure of the 'reward mechanism' away, what could replace it?
                - this idea of the 'reward/feedback mechanism' is a useful way to 'connect outputs with inputs' (to incentivize inputs that led to valuable outputs)
                - 'common variables/patterns of solutions' is another useful structure that could replace a 'reward mechanism' (like how a 'test' function to check every solution in a set can replace a 'filter' function to differentiate info likely to be a solution)
                - a 'function to make every input useful (derive info from any input, and connect any input to a useful structure)' (rather than finding useful inputs) is another alternative useful structure
                - what other connections between interface structures arent fully used or optimized yet? cross-interface structures are a good place to start looking, as well as other workflows and variables of them
            - if you didnt identify the 'info index + adjacent change combination' cross-interface structure, what else might you have identified?
        - similarly, connecting all solution-finding methods to 'incentives' and 'usages' is useful as another solution metric (does this workflow integrate the concept of 'incentives' and 'usages', which is required for not just finding solutions, but building them optimally, and making sure theyre used, and making sure theyre used correctly, to apply 'usage' structures that make some solution more likely to be used correctly, which are related functions that cant be replaced by each other - 'independent coordinating/complementary alternates' rather than 'equivalent alternates')
        - why might an algorithm identify an 'info index' adjacently without having that given as an input? bc its format allows/requires identifying 'input/output indexes' and 'interim indexes' in between those are a default structure in some networks, given some algorithms, so 'maps/indexes' might be adjacently identified as useful (implicitly, rather than explicitly) by various 'neural network' algorithms

    - identify useful structures in specific problem spaces and sets of useful info like 'sets of differences/starting points' that can be used to identify those useful structures
        - for example, in the 'find a medicine' problem space, identifying that the 'ocean is a harsher environment than land in many ways/cases' and that 'stress leads to evolution' means that 'compounds to resist pathogens are likely to be found in aquatic life, as they survived this harsh environment' and similarly the 'ocean is a high variation environment' and therefore 'new compounds that are useful in a new way or dangerous in a new way are likelier to occur in the ocean' and otherwise identifying efficient structures (like survival functions/compounds in a harsh environment), high variation structures (like where evolution occurs faster), common structures (like common points/paths of evolution), and other useful structures is possible and useful for other intents, like 'predict which organism might evolve an antibody to this pathogen quickest'
        - the set of information including 'starting points/inputs' and 'variables that are useful when applied from those starting points/inputs' is useful to identify in that set, to apply when identifying 'when an interface query will be most useful'
        - whats even more useful is when the starting point is not adjacent to the solution, but the query makes it adjacent to connect to the solution, across problems
        - the 'starting points' here are the 'insights', which when applied, make it trivial to solve these problems such as 'filter the solution space' (identify that aquatic life are a good solution set to test first as they are likelier to be more resilient bc of the harsh environment, which could be why algae and fungi are useful for many things as theyve developed many functions as theyre resilient to harsh environments, and similarly is why they can be more dangerous as well, theyre over-evolved)
        - finding those "'useful starting points/inputs' for a query (run on a particular problem-solving network) that make the query most useful" is a useful problem-solving intent, just like finding the functions of that problem-solving network is a useful intent, which is like workflows that involve a step to 'solve a different problem, or change the problem into another problem, or solve a sub-problem' but involves changing the inputs until theyre insights, which are useful inputs/starting points to most problem-solving queries, and finding useful connections to match an insight set with a query set that would make that insight most useful
        - similarly, adjacent changes to known insights can produce other insights (new insights which can be used as new inputs to queries which could make them more useful), like how the above insights adjacently produce the insight 'extreme systems/contexts (within survival limits) can produce useful structures like multi-use structures and other powerful survival structures like a high variation in antibodies', similarly an environment producing survival structures like 'high variation in antibodies' could produce other useful structures, like 'common compounds of structures (fish) having those useful structures (high variation in antibodies)' (which is why omega 3s might be useful for survival, bc the fish species evolved to have a high variation of antibodies, which led to the development of other useful compounds, or the omega 3's might be an input to the antibody creation rather than an output, where the omega 3's might be useful as an 'equivalent alternate' as the antibodies if they have such a causal dependency that they can be used as an approximation), similarly insights make most workflows more successful, so theyre useful to apply regardless of the structure of the workflow in many cases (answering questions like 'is the omega 3 compound only useful or only develop to be useful bc of the high variation in antibodies, or only useful bc it occurs in a resilient/surviving structure which indicates other compounds would be useful in the same species, or only useful bc of the harsh environment which would require any surviving structure to develop useful structures, or only bc the survival structure of the host uses other useful structures like "resilient multi-use/function" structures like algae as an input')
        - differences in the possible reasons for the usefulness of a structure indicates the usefulness of identifying differences in reason, as different reasons for usefulness reflect different functions, systems and variable interactions and other interface structures
        - similar to how cross-interface structures tend to be useful by default, 'opposing' structures (like a generative function and a filter function) tend to be useful as well, possibly bc theyre also cross-interface structures

    - identifying specific variants of known useful structures like known errors which arent formatted with sufficiently specific structures to be automatable which have potential for additional specficity and therefore an opportunity for optimizing the usefulness of those structures
        - for example, the 'loophole' structure is a useful general error structure concept, as a target structure to focus on/avoid/close, but its not specifically structured enough to be automatically identifiable/solvable, so adding specificity to this structure is useful (such as identifying a specific definition of it such as 'a gap in coverage of a solution of all possible inputs, where coverage of inputs prevents some error structure from occurring for those inputs'), which is a more useful variant of the error structure through its specificity, which is now possible to find automatically (find the gaps in possible inputs covered by a particular solution, given this function to identify coverage of inputs and this function to find all possible inputs) and also automatically fix (finding a function to increase the coverage of inputs for a particular solution)

    - identify structures that are common across solution-finding methods like solution automation workflows/interface queries that are particularly useful, and identify structures related to these common structures (such as conceptually similar as, or causal structures of these structures)
        - for example, a 'structure common across solution-finding methods' is 'high variation', 'usage of high-variation functions', 'high variation between functions used', and other forms of 'high variation' such as 'independence/orthogonality', where other structures can be used to connect these structures such as 'similarity between input/output (such as a protein/receptor similarity) that makes these high-variation structures interactive' and 'a clear difference/similarity created by the solution-finding method that is useful for some problem-solving intent (similar/different in the way that is specified as useful by that problem-solving intent) and similar/different to the solution/problem structures in some way', among other structures of the solution-finding method like how 'connect a sequence of similarities to create a clear similarity/difference' is a function that can describe many workflows as those workflows will fulfill an intent like 'connect problem/solution' that makes this structure a common implementation structure of a workflow
        - similarly, the solution-finding methods will vary in a new way to connect structures to fulfill some core function interface (like the 'find' or 'build' or 'derive' interface), so the solution-finding method would need to be a 'new/different connection between functions that still fulfills some core function intent (like find/build/derive)', where 'fulfilling some core function intent' might take the form of 'vacillating around the find interface applied as a symmetry, so that its similar enough to the find function to be an approximation/alternate of it'
        - finding 'independence' structures such as 'uncorrelated, indirectly related, distantly related, specifically but not generally related, relatable but not by default related, relatable with either the same unit/iterated function or with new variables, requiring a high degree of work to relate, or otherwise independent variables in the same system' is a way of finding useful variants of independence to apply in solution-finding methods
        - this is similar to how structures common to solution automation workflows are higher interactivity/connectivity/variation than other structures, they also have other attributes in common which are related to these attributes (independence being related to variation)
        - given that there are likely to be multiple reasons why a structure is useful, as useful structures are likely to be useful for multiple intents (an alignment between multi-intent and multi-reason utility), identifying the other reasons why a structure is useful (independence isnt just related to high variation, its also related to a known useful structure like a cross-interface structure, which is useful in that it connects different systems), and the connection between these reasons (high variation structures like 'independence' would likely be related to other high variation structures like 'connections between different systems (as in cross-interface structures)')

    - identifying functions like 'specific repetitions/iterations' that are less likely to create useful info, to identify structures likely to create useful info by applying differences to those structures, such as 'repetition functions with specific variables (like "variable position" and "variable application position (change input or change limit)")' that make it likelier to be useful for problem-solving intents like 'identifying new variables'
        - some repetitions (as in 'iterations') of a structure can create useful structures like additional dimensions (like 'repeating a square by stacking it can create a cube')
        - other repetitions of a structure can create less useful info (like how 'iterating across every possible point of a square' is not the same as 'adding/finding new useful variables')
        - the difference is that while both add new dimensions (a point creates a line when iterated through 'stacking', and a square creates a cube when iterated through 'stacking'), one can contain more variation as its higher dimensional and the iteration function can also contain more variation for the same reason
            - other functions to add a dimension include 'position a copy of the structure so it has a side/surface in common (an overlap on one unit of the border, such as a side) and fill in the implied structure by the endpoints/other sides of these overlapping structures, connecting their endpoints/corners' or 'position a copy of the structure, a distance away equal to one side of the structure with an overlap of the centers of the two structures, meaning a distance in one dimension only, and connect their corresponding endpoints/corners/sides of the two structures'
            - all of these involve 'positioning a copy of the structure in a different position from the original structure, and in a different position from other iteration functions, and then connecting the two structures'
        - to include relevant metadata such as a 'way for this error to be true/useful/a solution' (for the stated intent of 'identify new variables' that its an error for):
            - its possible to create high variation with a function that will definitely cross every point and is not a function like 'iterate every point crossing this constant line, then add an increment to the constant line and repeat the iteration'
        - what is different about a function like 'iterate every point crossing this constant line, then add an increment to the constant line and repeat the iteration' and some other iteration function that crosses every point but is less predictably uninformative and low-variation? 
            - the variables are applied at a 'position' where the variables can have 'compounding' value (such as 'integrating the feedback from one variable directly in another', similar to the fibonacci sequence), as opposed to being applied in a limit of variation
            - in the lower-variation iteration function, the variation is in the limit of variation, where the line intersecting with the square is used to guide (and therefore limit) the variation of the iteration is predictably and linearly changed every time, to produce the same result every iteration
            - there are some 'compounding/exponential/otherwise non-linear' functions that would produce predictable results, but this output is less common in that type of function
        - similarly, where might this iteration function be useless/useful? 
            - it might be useless in creating new variables, when the structure being iterated on (such as a cube) is already higher-dimensional than what the iteration function could create (such as a line), if the structure being iterated on isnt a 'dimension set' but rather a 'structure in a dimension set' (when the input problem structure is already a solution structure, more so than any change created by the solution-finding method could create)
            - this is an example of why its useful to be able to find the 'net/emergent effect of iterated changes' and 'metadata about those effects' (what 'degree of variation' is possible with this function, applied in this way, to these inputs, to this degree of iteration)

    - identifying high-variation structures like 'philosophies/perspectives' that are abstract or otherwise useful and also easily filtered
        - similar to how the golden rule has associated concepts like symmetry, generosity, justice/balance, reciprocity, and empathy, it also has associated errors (its not enforced, so could easily cost someone all their resources, and empathy doesnt always produce a requirement for generosity, and justice/reciprocity arent equal to generosity, and symmetry is different enough from balance to justify its own concept)
        - similarly, perspectives like 'magical thinking' are easily disproved without a test (whether a real-life test or an algorithmic simulation), just with finding info about 'common thoughts' (such as 'go to store', with related info like 'the store doesnt come to people, even though they think about it and visualize it')
        - relatedly, other high variation structures include combinations of existing high-variation structures like 'perspectives' (a 'filter with priorities that covers reality') and 'cultures' (a 'stack of iterated changes to create variable interactions within a perspective'), such as 'culture-perspective combinations that lead to the destruction of the society that applies that culture-perspective combination'
        - relatedly, its useful to connect perspectives with structures like useful structures (such as 'cross-interface structures adjacently created by that perspective', which are a useful output that these philosophies/perspectives can sometimes adjacently derive)
        - how do you find these errors from these philosophical rules?
            - 'intersections of scaled interactions, where one variable/structure arrives first and uses the resource first, or theres an overlap creating some resource conflict, or these required structures otherwise contradict each other when they interact' is a interface format description of a common error
            - 'apply the rule at extreme variable values, like at scale or at its most extreme interpretation or an extreme test like a test with measurable large structures like a building, to determine scale limits of the philosophy'
            - 'finding differences between concepts' of the philosophy/perspective, such as how 'justice is not equal to generosity' and errors resulting from those error structures like 'differences being equated as equal' (or 'equalities being differentiated as different'), although both are related concepts adjacent to the 'golden rule', and similarly connect the concepts to interface structures like 'requirements'
            - 'finding differences in the context that created a philosophy and useful contexts like current/modern problem spaces, where differences/gaps in this mapping across time contexts could indicate the irrelevance/in-absoluteness of that philosophy'
            - finding 'lack of connectivity to reality-covering structures (how does it connect truth/falsehood with abstraction/specificity) or interface structures (does it contain enough variation to be realistic)' and 'lack of handling for common problems (how does it handle "group interaction errors")' are other useful functions to find errors with these perspectives ('if you prioritize the same priorities as this perspective, can you avoid errors in some common problem space?')
        - why are these philosophies/perspectives useful?
            - given that 'philosophies/perspectives' are already extremely filtered rule sets, they themselves are easily filtered, which makes them useful
            - these 'philosophies/perspectives' are an attempt to find 'abstract efficient/limited rule sets that cover all of reality' (given the structure of 'perspectives' as a filter that prioritizes a subset of priorities over another subset, as in a usefully limited set), but are often incorrect, as they tend to over-simplify/over-limit/over-filter, use structures on an incorrect abstraction/interaction level, and also dont connect their rule sets to actual reality-covering variables (truth/falsehood, im/balance, power, un/certainty, simplicity/complexity, abstract/specific, solution/problem, useful/relevance/meaning, requirement/alternate, probable/impossible, etc)

    - identify solution metrics like how a falsehood should be accompanied with the corresponding true version of it and an error should be accompanied by the useful/solution version of it
        - a 'false similarity' (like a 'variable that seems correlated but is independent') can be a 'true similarity' and is useful to connect to a true similarity, even if the connection contains very little information
        - for example, two variable interactions may seem correlated (a 'false similarity'), but in real systems, those variable interactions might occur for the same reason (a 'true similarity', in the position of 'cause'), even if theyre technically independent of each other, they might be dependent on the system that creates the pattern in the same reason for those similarities in structure, and therefore not absolutely independent, a reason such as 'because of common limits and inputs, this structure is often adjacent and optimal for some common intent like "storing info", so it keeps re-occurring, even though the two processes are not directly interactive, or otherwise dependent or similar or connected, causally or otherwise, and the similarity they have in common is that reason of adjacency for why a structure keeps re-occurring, and another similarity in the available limits/inputs that are adjacent to that structure, and the similarity of the adjacency itself'
        - this may not seem like a lot of info, it may not be derivable (as in exactly knowable, having ruled out other possibilities, with just the variable correlation), and it may also not be useful in predicting the structure's functions/variables, as they may vary far more in uncorrelated variables than the correlated variables, but its generally useful to know 'system/input/use cases where a similarity/difference may not only occur, but also may be some other high-variation useful structure as well, such as false/true/causal/independent/similar/connected/adjacent/etc', as these 'cases connected to structures' provide a structure to look for/apply as a base structure, to describe the system in which the variable interactions occur

    - identify useful structures like 'patterns connecting useful structures' such as 'cause (backward in the sequence to find inputs)/predict (forward in the sequence to find outputs)' as different values of the same variable 'direction' applied to a core structure like a 'input/output sequence' to find useful structures like functions to find core functions or primary interfaces or other useful structures by applying this pattern and the reason for its success (involving 'interactivity', 'variability', and 'powerful' structures like 'core functions' or other useful structures like 'cross-interface structures')
        - similarly, 'filter/generate or reduce/combine or compress/expand or find/build' is related to different values of the same variable 'count' applied to a core structure like a 'set' (or a 'limit', as in 'what is not in the set', which acts like an 'equivalent alternate' storing similar information as the 'set')
        - 'embed/inject' is applicable to a core structure like a 'container' through values of the same variable 'position'
        - this is why 'sets' can be used as an alternate structure for 'predict' intents, they capture similar variation/information as 'sequences' and are an 'equivalent alternate' core structure, where the intents enabled by 'sequences' are similar to the intents enabled by 'sets', as 'sets' encode information about other interface structures like 'interaction levels' which are highly predictive of other variables (structures on the same interaction level can be used to predict other structures on the interaction level, more so than variables on other interaction levels)
        - finding these variables (direction) applied to interface structures (sequences) to get primary interfaces or other interface structures (cause as a core function and a primary interface, predict as a core function)
        - finding the 'set of similarities' and 'input/output sequences that determine a structure' can identify the 'variables of structures which can identify it'
            - for example, a set of slopes/starting points creates a 'set of similarities' in the 'set of intersections of these 'slopes/starting points', a 'set of similarities' which can create a 'set of limits' through the 'set of intersections' resulting from those similarities (the slopes are 'similar enough in position (to allow the possibility of an intersection)', and 'different enough to be sides of a polygon (which are 'maximally different' slopes)')
                - this 'set of limits' is a core interface structure (if you know the 'requirements/limits (of what something certainly/definitively is or is not)', you can often solve the problem) that fulfills one of these core function intents like 'filter'
                - 'similarities' (adjacency of position, difference of slope) -> 'intersections' (different type of similarity) -> 'limits' (as 'requirements' of some similarity, or 'limits in the sense of differences from other possible structures')
                - the 'intersections' are adjacently convertible into another useful structure 'set of limits' (if they connect enough to form a bounded shape, where those limits can act like boundaries)
            - what other structures fulfill a different intent, and do those structures capture the same information and do they use different similarities/structures and how can those be connected to these similarities/structures, through identifying variables of those 'equivalent alternate' structures
            - a 'symmetry through an equilateral triangle', the 'length of the symmetry' and the 'radius of change that constructs the remaining side of the triangle' can also construct a triangle if its equilateral (has a symmetry dividing it in equal sections), which reflects the utility value of a vertex between two vectors in creating a triangle (once you have one vertex with two vectors as a corner of the triangle intersecting with the symmetry, which is retrievable with the symmetry and the radius of change which determines the length of the remaining side, the triangle is determined), where the 'length of the symmetry' and the 'presence of the symmetry' determine the other angles which are possible, similar to how the 'hypotenuse of a right angle triangle' encodes info about the other sides (once its known that its a right angle triangle)
                - the 'set of similarities' in this example is the 'set of similar radius lengths from the symmetry, which creates a similarity in the angles possible in the equivalent-angled corners, in their equivalent interactions with the point identified as the remaining corner', and this 'similarity in angles' creates a 'filter' of the 'remaining possible angle value'
                    - 'similarity (in symmetry)' -> 'similarity (in radius length)' -> 'similarity (in two equivalent angles)' -> 'filter (of remaining angle, through the "requirement" of angle sums that acts like a filter)'
                    - these structures are adjacently connectible (such as how a 'similarity across a high ratio of inputs, as in the equivalent angles' leads to a filtered set of 'possible other inputs that could explain a difference, as in the remaining angle')
            - the 'similarity in angle sums of a triangle' contains related information which can be used to filter possible angles of different corners, once one corner is known
                - the 'set of similarities' in the 'similar limits (upper bound) on angles of a triangle' which leads to a 'similarity in related structures (the limit/upper bound of "combined angles of a triangle")' which leads to another similarity in related structures (the similarity between each angle and the other angles, and each angle and its relation to the upper bound of their sum and each angle and its relation to requirements like 'at least one right angle' or 'two equal angles', as the progressing sum of each newly identified angle has to be some degree of similarity/difference from each other, the upper bound, and requirements (given the scarcity of allowed number of angles and the requirement created by the first known angle and the 'requirements about triangle type which are known', with rules such as those regarding 'differences between variables of types of angle connections (like two equal angles or all different angles or one right angle) in different types of triangles (like isosceles triangles or triangles with all different angles or one right angle)') to the other known angle but different enough from the sum to allow a third angle before the upper bound of the sum is reached, depending on their position)
                -  'set of similarities' in the 'similar limits (upper bound) on angles of a triangle (where each angle has to be less than 180)' -> 'similarity in related structures (the limit/upper bound of "combined angles of a triangle")' as their sum has to be less than 180 as well -> another similarity in related structures (the similarity between each angle and the other angles, and each angle and its relation to the upper bound of their sum, as these angles of the triangle corners encode info about the intersections/limits found in another structure, as a 'vertex of two vectors' encoding the angles is another format than the 'set of slopes/starting points' that intersect or the 'set of intersections' defining the endpoints of the triangle
                - this encodes a similarity between upper bounds across structures that are operated on with a symmetry (the upper bound of each angle's maximum possible value 'in certain cases of the other angles', applies to the sum of the angles as well, bc summing the angles preserves this similarity) which creates another similarity in 'first-next/next-last angle' values in the sequence, as each angle is filtered and applied as a filter of the next, as the 'sum' sequence has an 'alignment' with this 'angle' sequence and can be used to filter the 'angle' sequence
                - the 'unit/sum' adjacency is the basis for the similarity (the units are adjacent to and useful in creating the sum), and the angles are adjacent to each other (in required similarities or limited differences) as well as the sum and the requirements of the triangle sub-type
            - similarly, triangles can often be trivially changed such as reflected/rotated to create other triangles
            - what is the common structure across these variables ('set of intersections/limits', 'set of symmetries/radii/lengths', 'set of vertexes (of vectors)', 'set of equivalences in sums/related angles/requirements', 'set of similar shapes like other triangles') which create the same shape type?
                - all of them encode different structures of 'similarity' through applying different sets of core structures, similarities which connect different 'differences' (like different 'input formats') which are variations of the same information (different 'sides/endpoints of a triangle'), where these differences are similar enough to the input and target structure to be 'equivalent alternates' in capturing the same info, which can connect 'maximal differences' (such as different slopes required for an equilateral triangle) within some range determined by a similarity (like an angle sum requirement)
                - all of them differentiate the structure from other shapes ('given these intersection points, it cant be a square'), and some of them differentiate within triangle sub-types
                - all of them fulfill a "sequence connecting interaction levels of the primary different sub-structures of a triangle that couldnt build a different shape", given their possible connection functions (components that can be used to 'build' the triangle or 'limit other possible shapes so that a triangle is required'), starting with input structures that have the right differences to create a triangle with a few changes and applying changes on various interaction levels of similarities/differences so that the final structure is created as a result of this sequence
                - these involve 'conversions across "types of similarities"' and eventually create a useful 'difference' (from other shapes) or 'similarity' (to the target shape), or a 'similarity in a difference' (a 'common connection between multiple maximal differences like different slopes/angles' or a 'similarity between a vertex/intersection structure and a triangle' or a 'different way of connecting/representing/generating/determining/deriving the same info') that can be used to connect the various input sets to the target structure
                - given that a triangle is adjacent to 'core' structures (like 'lines/points/angles'), it should be similar based on these/other adjacencies, such as similar to these core structures to some degree (similar to components like lines/points/angles and structures of them like vertexes), similar across sub-types (similar to other triangles of different types), similar to the sub-type (similar to requirements of a triangle sub-type), similar to itself (similar to requirements of the triangle definition), different from non-triangles, similar to the supertype (other polygons), and possibly also similar within some symmetry defined in a requirement of a sub-type (such as how 'two angles must be equivalent' in some sub-type), similar to adjacent structures of core structures ('intersections' as a structure of a core structure like a 'line', or a 'combination' applied to a core structure like an 'angle', or a 'combination' applied to a core structure like a 'triangle side' to create 'vertexes' as 'combinations of sides'), similar to problem-solving core interaction functions/structures like 'limit/filter/connect/combine' as core structures (or structures of core structures) can interact in ways that are different enough to be similarly variable and therefore interactive with those core difference structures ('limit/filter/connect/combine') before achieving the target difference of a 'triangle' from the 'inputs'
                - similarly, the similarities present in the connectivities/interactivities/adjacencies (like how the corner/intersection is adjacent to the side) of the maximal differences (three sides) determine the 'equivalent alternate' structures which can store the same info, where the 'maximal differences' act like symmetries to base changes around, and the adjacent changes found with those changes determine 'equivalent alternates' around the symmetry of the 'side', where the 'corners' also act like 'connection' structures of the 'sides' (similar to how input/output sequences and interactive structures like 'coordinating output/input sets' can be used as alternates bc of the 'connectivity' of one meaning the 'interactive structures' for the other meaning the 'input/output sequences')
            - from this, it can be derived that a structure common across solution-finding methods is that a solution-finding method results in a clear and stable similarity/difference to the target solution structure or adjacent solution structures or problem structures
            - similar to 'adjacencies', identifying 'asymmetries in probabilities' (as in, identifying structures that make another structure 'likelier', as a more specific form of 'adjacent') is similarly useful in determining structures that co-occur in a set or occur in a causal sequence, as an alternate useful structure to apply as 'connections', 'sequences', 'causes', etc
            - identifying 'functions (such as find/build/change/derive)' that act like interfaces (as a 'find information' function can be used to solve all problems) is useful for identifying other 'sets of equivalent alternates' that can be used to apply variables/similarities to problems to find 'equivalent alternates' in other problem spaces, where these core functions connect interfaces (such as the 'filter' structure with the 'find' intent and 'reduce' functions) and act like a set of structures that connects all primary interfaces and therefore act like alternates to the primary interfaces (like a set of horizontal lines of these core functions like 'find', as opposed to the vertical lines of primary interface structures like 'potential', which cross the same information and are highly interactive and can act like equivalent alternates for some intents, which is a useful metaphor for all information sets that can act like an interface)

    - identify useful structures like 'problem/solution connection structures' such as how 'adjacent info' is connected to the problem input and 'agent incentives is likelier to be closer to the solution as they indicate probabilities which are useful for predicting functions/solutions' so connecting 'adjacent info' and 'agent incentives' is likely to be useful to solve problems as a 'connection structure', which is related to the workflow 'connect structures adjacent to problems/solutions rather than the actual problem/solution'

    - identify useful structures like 'scaled interactions' like 'group interactions' such as how 'the implication of one example is not sufficient to solve most problems' and similarly, other info is required such as the vertex between the 'group-group interaction' (whether one group is suboptimal for other groups) and the 'group-system interaction' (whether one group is suboptimal for the system), as 'group interactions' are high-variation and can be applied to solve problems involving group/scaled interactions

    - identify useful structures like structures that are useful for a particular problem-solving core interaction function like a filter or other core interaction functions, such as how 'maximally different' structures are useful for 'filters/generative functions' and similarly 'randomness' and 'sensory deprivation' and 'extremely independent/disconnected variables' is useful as a 'optimizing/learning input' for 'deriving' functions
        - similarly, these similarities fulfill other solution metrics like 'alignment of incentives between opposing forces' as 're-using a structure across functions' is a useful optimization of function usage

    - identify useful structures such as attributes of useful structures like 'connectivity', as a useful structure is likely to be connectible to other useful structures and is likely to interact with other useful structures, as in likelier to be used by useful structures and other encodings of similarity like 'commonly found as an input or used in a useful function', since theyre simmilar by being useful and are therefore likely to be similar in other ways

    - identify useful structures like 'requirements' such as 'optimism' which is required as an input to continue pursuit of a reward and therefore required for success, but which may be contraindicated by negative inputs, so identifying false statements is likely to connect that negative state with a 'successful/solution' state as it buys time to find a solution

    - identify useful structures like 'concepts' (like 'fairness') and 'structures' (like 'scale') which are useful to apply to find limits/emergent effects, such as how 'scaled fairness' creates a very aligned and organized system (in that 'potential/ability' and 'problems requiring that potential' are aligned) but also a limiting system where there isnt much freedom to vary and therefore finding alternate optimizations to the system is less likely, although such a system isnt likely, which is useful for finding 'new directions of inventions'

    - identify useful structures like structures like 'metrics' to 'determine if structures are equivalent', such as if two structures are more interactive, theyre likelier to be equivalent which is true bc theyre likelier to exist on the same interaction level and are therefore likelier to coordinate than not and bc theyre likely to contain differences (like 'input structures like receptors/spike proteins') that can be changed to interact with each other

    - identify useful structures like useful 'mappings' such as between the 'navigation' problem space and 'networks' to apply navigation solutions to networks to find network format optimizations, such as how when investing work in multiple different solutions (where each agent with a car creates possible 'solution/error' structures in the 'driving/navigation' problem space where the problem is 'generally find a useful solution-finding method or specifically find a route and usage of this route to get to a destination point without errors', where those solutions intersect or pass, the slower solution can temporarily stop and use various structures of terms from the faster solution to test if it is an optimization structure, where each solution-finding method has a network of solutions that it finds or is likely to find or capable of finding with adjacent changes given the network's defaults (like similarity metrics, position change functions, etc), and the overlap of these networks is useful to identify so that these intersections between networks can be identified and the optimal route between these networks (switching between solution-finding methods) is findable on this set of networks, where the network might only represent the adjacent solutions a solution-finding method might find

    - identify useful structures like 'optimization' structures such as a suggested/optional 'slow/fast lane' which optimizes for grouping drivers with different relevant sets of intents, relevant to core functions/metrics like 'speed' to minimize a specific error type like 'lane changes' frequently cause
        - why is identifying a new solution/error in a problem space almost equivalent to a solution automation workflow? bc solution automation workflows are 'unique abstract/other interface connections between problems/solution structures', and each error is a new step added to or changed in an existing connection, creating a new connection between problem/solution structures, which is easily abstracted or otherwise converted to a primary interface, at which point its likely to be a new solution automation workflow
        - identifying a new solution/error is similarly useful for identifying new variables/differences/similarities/other useful interface structures (like how identifying a new error type is also identifying a new solution type bc you can just apply differences to the error type to adjacently derive the solution type, as errors are different from solutions), which is a useful problem-solving intent (identifying a new useful structure or new variable/difference type is often the same as identifying a new solution-finding method)

    - identify useful structures like 'generosity/strictness' (non-adjacent vs. specifically accurate) or 'optimistic/pessimistic' ('implying info about a future change, as in a future change of a data set') or 'experienced/amateur' ('rule-compliant vs. rule-unaware') which can be applied as a solution metric of predictions, similar to how metrics like 'specific/general' can be used to find optimal solutions

    - identify useful structures like 'different information accessible from a different position' and structures to find that info like 'changes/functions applied at that different position by other agents'

    - identify useful structures like insights such as how 'adjacent changes' (like in gradient descent) cant be used as the solution in all cases, bc some changes can be errors but seem like solutions (like the 'low-cost decision') and can occur at scale, so that any agent in that system doesnt see the errors or the net effect but there is still a huge change set required to fix it ('making agents intelligent' is not a solution likely to be found by gradient descent, neither is its implementation)

    - identify useful structures like optimizations such as where a function can be applied with trivial changes like 'in reverse' and still be useful, such as the 'lane change signal', which can be applied 'in front of a car to signal lane changes' or 'behind a car to pass in case they will see it and move' and still be useful, or similarly, speeding up, which can be useful for 'covering more distance faster' or 'to communicate that other cars should switch lanes to allow car to pass'

    - identify useful optimization structures like 'moving horizontally to see info farther away' which should be regularly applied in cases where 'the lane change function might be useful' and 'info about far-away errors like traffic jams or crashes is signalled as a possibility'

    - identify useful structures like 'specific examples of how a statement/solution could be true/false (as in correct/incorrect)' such as 'specific data sets that could make a solution true/false' so that 'determining difference from these true/false example data sets' is useful as a 'test/filter of a specific solution'

    - identify useful structures like similarities between error structures, such as in the 'driving' problem space where a 'sequence of agents' can produce multiple errors such as when 'driving next to the sequence where other errors or error inputs occur, such as the sequence is slow, which makes it easy to speed by it (a dangerous speed if any of the sequence agents uses a common function to change their position), and one of the agents changes lanes and its difficult to see their change lane signal (input block error) bc the sequence is slow as in a traffic jam so theyre close together' and the 'driving sequence will be difficult to pass if the incoming traffic is distributed rather than occasional or grouped together'

    - identify useful structures like 'independent subsets' ('independent' as in 'equivalent alternates' which have similar/overlapping info coverage/storage/functionality), such as 'convergences/divergences', which are useful to identify useful sets of structures which can be used to derive a high ratio of other info (like 'intersections' and 'change directions'), and which therefore are useful to identify 'similarities/differences', just like 'filters/averages/densities/limits' and 'parallels/orthogonals' are useful independent sets of structures to identify that cover a high ratio of info
        - the core insight is that 'equivalent alternates' can act like each other and replace each other, and are therefore independently capable of deriving info, so identifying 'independence' in this sense is a way of identifying 'equivalent alternates' and vice versa

    - identify useful structures like how applying 'proving' function often requires 'holding some information constant that is normally a variable', and therefore requires a 'distortion like a stretch/angle' of reality to achieve, as systems might only falsely (as in 'rarely') actually 'be in that state, or function that way, or even just seem/be represented that way', so much so that its arguable false to distort it in that way by holding those variables constant, as in 'proven true, in a generally false (as in "requiring any distortion at all" or "requiring unusual distortions that are false, in that they are not generally useful for other problems") but specifically true (as in "having some trivial number of conditions/changes to be true" and "possibly true, as in not guaranteed to be impossible") distortion of reality', which is less like 'proving a statement to be true' and more like 'forcing a statement to be true' (where there are usually some contexts where any given statement can be interpreted as true but some statements require more work than other statements to be verified or proven and are therefore less true)
        - similarly, identify useful structures like 'specific applications of insights' (such as that 'most statements are true to some degree and false to some degree', such as by applying 'specific variants of insight components like truth such as useful') where the 'specific application of that insight' using 'useful' instead of 'true' is 'most structures are useful in some way', such as how an almost completely useless structure might be only useful as 'another example of some type which is unnecessary as there are plenty of other examples', or only useful in that it 'creates errors', so it provides info about 'errors to avoid', so identifying extremely useful structures is a matter of identifying common variables of these useless/error/suboptimal structures and differentiating from those structures, as 'useful' structures are often more identifiable than 'true' structures (as 'true' is less testable/verifiable and more variable/general than 'useful'), so its useful to apply that specific variant of the definition

    - identify useful structures like 'connections between useful structures' such as connections between concepts like 'random' and 'nothing', as 'randomness' is associated with a 'lack of info' and also 'equally distributed info', so that 'nothing' might have an alternate definition like 'extremely/maximally distributed info', which could be a default state of the universe that is regularly returned to when info optimizations are found and applied to distribute info again, where 'organization structures' (like patterns that occur in random value sequences) also occur by default given this randomness, and can lead to temporary organizations that often produce these optimizations which then distribute info to return the state to nothing again, and these patterns in random value sequences can identify other possible 'initial conditions' of organization leading to other universes

    - identify useful structures like 'empathy' which are connected to other useful structures, such as how 'empathy' can produce an error like 'bias' if applied with other 'biases' (like 'locality', where 'empathy applied only locally' might produce 'biases toward a local group that is the majority locally') where it can also produce useful structures like 'complete/deep understanding' if applied 'absolutely locally' (as in 'to one system' rather than to 'local systems in general', which could be useful for understanding systems in general, where this 'deep understanding' provides an opposing structure to more efficient structures like 'type' or 'local interactions' as a representation structure and can be created by combinations of these more efficient structures)

    - identify useful structures like 'optimizations' such as 'incentives' applied to specific useful structures like 'attention' (meaning 'attention incentives' like 'distractions') which can optimize for intents like 'routing resources in a direction to centralize/distribute resources', such as 'clearing a lane' in the 'driving' problem space by 'creating distractions that require being in the other lane'

    - identify useful structures like 'requirements' as 'reflections of real structures like real differences' such as how a 'structural pattern' (like a mnemonic device or a structurally similar word) can only be used so many times before it creates errors like 'over-use' where the set of structures that use it is large enough the the structural pattern isnt unique anymore, and another structure must be identified to use as a 'structural pattern' to apply to create useful changes, a set of 'required different structures' that reflects real similarities/differences in real info in real systems

    - identify useful structures like useful solution metrics such as 'sensitivity', which is a structure with high/extreme output from low-cost input, which is useful for 'magnifying/extreme/differentiation' intents such as 'identifying certain/extreme/obvious error structures from an initial/trivial error signal, errors that are possible to identify by magnifying changes like scaling the signal'

    - identify connections between useful structures like 'risk/cost', such as how 'minimizing risk' doesnt necessarily correlate with 'minimizing cost', which is useful to identify as a possible error if 'cost minimization' is a solution metric

    - identify useful structures like 'optimizations made possible by applying variables like "scale" to error structures', such as an 'over-reduction' error can produce optimization structures, like identifying irreducibilities more adjacently than other structures

    - identify useful connections like connections between useful structures like 'neural networks' and other useful structures like 'clocks' and 'input interfaces like senses that store/process overlapping information but process that information in different ways' which can be used to derive different filters which are 'complementary/overlapping/compoundingly useful' as opposed to being 'equivalent alternates', such as how sight/touch/hearing can all detect overlapping/equivalent information about 'position changes' but each of those interfaces stores and uses different info/info formats (sound/light frequency/connectivity/overlaps, heat, etc), and different processing functions ('identify areas of light reflection frequency differences such as color areas', 'identify overlapping audio frequencies like echoes') to derive that same info about 'position changes', with varying info available/adjacent on different interfaces like how 'language is more variable on the sight/sound interface than the touch interface, but each interface adds information compoundingly even if there are overlaps, such as how interpretation of language is more successful, the more interfaces are available', which identifies useful structures like 'frequency' adjacently which can be applied in a useful way in neural networks
        - similarly other adjacent structures on these interfaces like 'focus' in the 'sight' interface can adjacently derive structures like 'filters' and 'functions' (like 'identify type/local interactions') which are commonly used in 'sight' info processing as a specific implementation of a 'focus/filter' function, which is useful in its specificity and approximation potential of the general 'focus' function (when you 'focus on/think about' a structure, you often identify its type/adjacent changes/local interactions as a quick reference to the structure that encapsulates a high degree of variation and functions like a definition, where its type or adjacent changes or local interactions store enough info to act like an alternate to a definition, as they are representations of the structure just like a definition is)
        - 'neural networks' are similar to 'clocks' in that they both implement standards, capture high variation, reflect reality, connect info into sequence structures, have variable difference/distance metrics, are useful for comparison of changes, etc, where 'clocks' act like an alternate info format similar to 'networks', 'sequences', 'interfaces', and 'concepts', as for example in the 'driving' problem space, 'structures moving at similar speeds' are more relevant than other structures (such as by identifying errors like 'sequences of cars that are difficult to pass' and 'inability to pass a car with a simmilar speed'), where both slower/faster cars are easier to interact with ('pass if theyre slower or let them pass if theyre faster') than similar/equal-speed cars, bc of the 'extreme similarity' in cars with similar speed, which makes it more difficult to differentiate and therefore more difficult to interact with, as everything but that car will be easier to differentiate and differentiating the observer and the other car will be more difficult as a result of the comparative ease of differentiating everything else, which will get more attention allocated by default and will be easier to determine by default

    - identify useful structures like connections between 'error' structures like 'identifiable certain differences from a solution' and 'commmon/core functions' like 'approximation', such as how 'repeated approximations can generate errors'

    - identify useful structures like connection between common/core interface structures like 'general and specific structures', like how 'general structures can be derived from specific structures as specific structures contain more info, but the reverse is not as useful bc it involves an error of a 'loss of information' and how 'specific structures can offset the over-generality of otherwise useful abstractions like how variability can explain everything but its useful to store specific interactions of variability like how it tends to be preserved across interfaces as they act like equivalent alternates in their information storage/coverage', connections which can be applied to create a 'directed probabilistic graph' where the direction favors change in the direction of specificity

    - identify useful structures like connections between core interface structures (like 'time', 'position' and 'distance' variables) and other useful structures like 'critical points/useful points/ranges/thresholds/starting points' such as how the 'time/distance to a critical point' is a useful structure in the 'driving' problem space, as there are frequent requirements to identify 'approximations of distance and time to move to a structure/point, where some threshold is relevant such as a range of criticality/importance/relevance/variability', such as how its useful to identify the 'optimal distance from a sign to the relevant structure in the rule (like "ice forms on bridge") of the sign (like a bridge)' by identifying the connection between 'distance/time' (as in 'time it takes to slow down, from distance x' and 'distance from relevant rule structure like "bridge" at time of "successful slow down" at a given "slowdown starting point"' and 'optimal distance from relevant rule structure like "bridge" at time of "successful slow down" at a given "slowdown starting point"') and the critical range where the info in the sign is useful (as in "before the critical point, after which its impossible to use the info in an optimal way as in 'slow down to prevent sliding on ice'")

    - identify useful structures like 'connections between formats and useful functions', such as how changing the 'sequence' param (as in 'switching order of a set of items' to differentiate it from 'changing position of one item' by connecting the change to its impact on the 'rest of the set of items') of a structure formatted as a 'sequence' is a common solution structure in that format (as its different from core functions like 'change position of one item' but is still trivial to identify and has higher variation than the core function and also involves higher connectivity than the core function), similar to how 'changing similarity/connectivity metric of a network' when a structure is formatted as a network is a common useful solution structure in that problem space of optimizing networks (a function that is not a core function in that problem space but is still trivial to identify and is high variation and related to connectivity)

    - identify useful structures like 'connections' between 'optimizations' and 'error structures' (or 'errors and other errors') using useful structures like 'scale', such as how 'lying may be optimal in some cases' but its useful to identify errors that can occur if that optimization is applied at an extreme/scale, such as how 'gaming speed traps' can be easier than always obeying the speed limit (which requires functions like 'forcing attention to speed' and 'forcing a limit on a default like speeding'), but if applied at scale or at every possible opportunity, 'gaming speed traps' has errors like 'future costs' such as 'more iterated/recursive speed traps' or 'lost time/resources from traffic violation tickets' and 'cost of ineffective monitoring/enforcement' and 'impact on other regulations and the system of regulations, given the lack of enforcement of a particular regulation', which is likely to create errors like 'suboptimal system states' like 'lack of rule of law'
        - similarly, other errors are likelier when applying the suboptimal strategy, such as 'over-use of brakes' (which tends to occur in common cases like 'at any non-constant speed') which is derivable from the other error structures bc the 'variation in speed' of a '"gaming speed trap" driving pattern' is unlikely to match 'required/optimal variation in speed, given other metrics like normal/optimal brake usage patterns, like only braking at stops/red lights and to slow down enough that a constant speed is maintainable'

    - identify useful structures like 'possible errors' such as 'input blocks' which can be 'connected' to other useful structures like 'optimal/suboptimal cases to apply a function (like the "pass-car" function)' and 'possible similarities' like how an 'input block' (blind spot) could be similar to the 'size of a common structure in the driving problem space like a car, depending on distance from observer' and could be in a 'position that blocks the input of the car', which is a 'suboptimal case to apply the "pass-car" function'

    - identify possible alignments that represent useful structures like 'overlaps between multiple metrics' to fulfill intents like 'check map' and 'dont crash', where specific intents like 'keep eyes on the road' are useful as approximations of those general intents like 'dont crash' to create opportunities for useful 'overlaps' like 'align "phone position" and "input field" (by creating a similarity between phone position and input field and identifying that its not required to take eyes off the road in order to look at the phone bc of large background visual clues which are easy to identify, bc of peripheral and unfocused vision, and the size/color of the objects in the background)'
        - from this example it can be determined that in general 'overlaps' are common useful structures in problem-solving, partly bc of the 'structural similarity' and the 'adjacent difference' between the overlapping structures, which reflects a workflow such as 'change a base solution (using adjacent changes, given the semi-correctness of the base solution)' and which encodes other useful structures like a 'rotation'
        - this connection between the structure 'overlap', the workflow 'change a base solution', and the common useful function 'rotation' is useful to identify, as it reflects that common useful structures are adjacently connectible to solution automation workflows and other useful structures like 'core interaction functions' on other interfaces like the 'math' interface

    - identify useful structures like 'types of vision' and their connection to other problem spaces like 'neural networks', such as how the 'light' problem space has problems that are solved by 'vision structures (like eyes and types of vision and light receptors)' so that 'solving a problem of vision (interpreting information formatted as light)' is related to 'solving a problem of cognition', with related functions like 'focus/group' which map to other useful structures like 'filter/combine'
        - for example, 'night vision' is useful for identifying 'heat' which is a proxy for information such as 'location of useful structures', which is a different info format to detect other than standard visual inputs like 'moving light structures', as the 'temperature' interface reflects similar variation as the 'light' interface, and different types of vision can detect different information (that can have an overlap with information detected on other interfaces), where 'vision types' act like interfaces in this problem space
        - similarly, the 'peripheral vision' interface identifies information like 'background/unfocused information' as a 'bet-hedging investment strategy, to account for the uncertainty of possibility of the case where current focused objects dont provide sufficient information for intents like survival'

    - identify useful problem spaces to apply as problem formats which are useful in standardizing a problem and other interface structures, such as the 'light' problem format, which is useful for finding interface structure interactions such as new variables by applying simple functions and other useful interface structures in that problem space
        - for example, applying 'mirrors' as a 'symmetry' structure to 'find new variables' is possible by applying workflows like 'trial and error' to different 'mirror' structures (like 'angle', 'count', 'position', etc)
        - similarly, connecting structures like 'symmetries' to other useful structures like 'filters' is possible by applying differences to the 'mirror' structures that create a 'filter' effect by allowing some light to pass through
        - similarly, identifying 'extreme differences possible in extreme similarities' is a useful structure that is adjacently derived in this problem space, such as 'creating opposites (like shadows/colors) from light similarities like reflections'

    - a default implementation method is to use existing workflows in this repo as configuration and writing a function to apply a workflow to an 'input problem space'
        - remaining questions are variables like 'which combinations of which variables of reality can be optimally applied as equivalent alternate defaults forming a core interaction level (used to adjacently derive the other interaction levels)', which is semi-defined but still unresolved (as not completely specified) by other workflows/implementation methods

    - identify useful structures like 'accuracy/truth/realism/other solution metrics' that identify other useful structures like an 'error in other metrics'
        - for example, if some statement is 'technically' true, and 'only technically' true, that indicates that other truth metrics (like 'priorities', 'optimality for general intents', etc and other metrics of a statement) are extremely false, so much so that the net effect of these truth metrics is false, as the 'network of truth metrics' is more useful to apply with queries of the network (a structure that answers the question 'which probabilistic sequences/combinations of truth metric network nodes are useful to apply, in what cases')

    - apply structures like 'variables' which can be applied to other useful structures like 'probable filters' (as in 'activate/deactivate' probable alternate filters to resolve the uncertainty of which filter is correct) to fulfill intents such as 'apply an uncertainty as a variable rather than a constant' and 'try every probable variable value to find the correct value and find overlapping/otherwise similar values (once probable variable values are filtered into a solution subset, at which point "trial and error" becomes useful again)', to apply insights like 'some workflows are only applicable/useful in some cases (like "low input count")' and 'optimizations of workflows like "trial and error"  include applying additional functions to connect different solutions identified as non-errors by some similarity metric, to identify solution filters that produce similar/equal solutions'

    - identify structures that its useful to identify, like how its useful to identify 'limits on areas/ranges of usefulness' (like how a 'cloud' can be useful for determining 'direction' up to a point as in during a very temporary time interval, where the cloud moves) just like its useful to identify 'limits on error ranges', as these are useful to filter inputs to specific ranges but also to identify common ranges of usefulness/errors of inputs and other interface structures of these limits, as 'limits' are a problem-solving format just like 'filters' are, though they are often more useful when combined, as interface structures are compoundingly useful

    - identify optimizations such as using 'local info' that is more useful for being more specific or more real/accurate when used vs. expert/technically correct but irrelevant/useless info (such as 'names used in actual local signs' vs. expert/technically correct info like 'the numerical number of a road that is only known to a map database but is not used in a local road sign', or directions involving expert navigation knowledge like 'move in the direction of a neighborhood' rather than knowledge that is accessible to most users like 'move south'), through removing requirements to acquiring info (acquiring info about 'which direction is south' requires less steps than acquiring info about 'which direction is a neighborhood in')

    - identify useful structures like 'false equivalences/constants' between useful structures to identify errors in, like 'errors in reasons for a variable change, which are not reflected in approximating metrics' such as where a reason for an approximating metric change (change in 'estimated route time') is assumed to reflect only and exactly the corresponding reason in the approximated metric ('distance to destination') even though the approximated/approximating metrics are not equivalent as there could be other reasons for a change in the approximating metric, as a variable like a 'estimated route time' can vary for many different reasons (like a 'traffic jam' as opposed to a 'missed turn'), and assuming that the actual relevant metric being approximated (the 'distance to destination') is equivalent to the change in the approximating metric ('estimated route time') is false as these metrics are not equivalent alternates but are useful approximations of each other in some cases which are likely to occur (like where a 'general estimate' is more useful than an 'exactly correct estimate')

    - identify useful structures like 'directions to move in which have no immediate errors', which are useful to apply as default variables (like default changes to a base solution) until errors are identified in those directions, at which point its useful to connect existing error structures to that error point once found, to find other error structures like 'error thresholds' and 'error areas'

    - identify useful structures like variables such as 'scalability' that cause problems like 'inefficiency' such as 'lack of specific efficient identification of a useful item in a set of many items', like how 'ill-defined structures (like overloaded terms)' and 'disorganized structures (like multiple words for the same concept)' and 'non-adjacent definition (conceptually distant) structures (like applying concepts like variability and interactivity and similarity without storing specific useful implementations/variants of these that are known optimal in general or for specific purposes)' are another cause of problems like 'lack of understanding', to fulfill intents like 'find useful problem-solving intents such as "find opposing structures of these problem-causing structures to solve problems"', and similarly other concepts like 'complexity' are frequently causative of problems with known optimizations to resolve those conceptual problem causes

    - identify error structures like 'input blocks' which can be used to identify other error structures (such as other errors that are likely to occur, as they are causally adjacent/relevant to those error structures) or solution structures (like 'structures to avoid errors like input blocks, such as alternate structures that appear to be other structures so when inputs are blocked/removed, these alternate structures can be identified/derived, such as alternate information-producing structures like probabilities (like "there is probably a car within crashing distance") or requirements/certainties (like "there will probably be a car in the opposing lane at some point, with such certainty that its like a requirement"), or such as false similarities that can identify alternate structures, like how a flat/linear-seeming surface can actually be a set of hills/waves which is only possible to confirm nearby or from a different direction'), where combining these error structures with these requirements can identify other error structures (like how a 'false similarity' such as a 'set of hills that looks like a flat surface can hide a required/probable structure like a incoming vehicle in the opposing lane while scaling a hill producing an input block like a blind spot which could produce an error like a crash if another useful function like "pass car" is applied in a case like "two-lane highway"')

    - identify useful problem formats like 'navigation' which can be applied across structures once standardized to a format like a 'directed network (such as a probabilistic network where more connected nodes are more probable to occur from each other, or where ambiguity-resolution functions are organized by probability of usefulness, as ambiguity-resolution structures are alternate structures of filters that can be used to fulfill the decision/selection function)' which have known 'optimization' structures on the structures of that problem format like 'path decisions' (like 'path decisions that enable other path decisions, rather than leading to possible error structures dead-ends, which involve work to correct, where staying in decision-enabling loops is usually more optimal through the interactivity/connectivity enabled by those structures') which can be applied across problems formatted that way (like a network to avoid over-specifying solutions, where over-specific solutions are less interactive/connective with other routes on the network, which is useful to identify in cases like where some problem shouldnt be solved with a specific solution but where solutions should be changed/tested regularly to avoid over-investing in one solution bc the uncertainty of the problem isnt resolvable with available information), a format that is useful for finding structures like 'alternate routes', 'points in a route where errors occur or where errors become inevitable/required', 'common nodes/patterns/structures of error/optimal routes', 'reasons for route suboptimality/success', 'structures like directions that are useful to vary or move in', 'alignments between structures like directions and intents', etc
    - relatedly, its useful to identify specific useful ambiguity-resolutions, like differentiating errors from solutions, error directions from errors, errors from variables/differences, etc

    - identify useful structures like function sets that can be used to solve or improve errors in many problems, such as 'identify when constants are optimal such as when constants provide stability and reliability which are useful for basing changes on' and 'identify structures that can create constants (like how slowing down can create a "constant" in the safety variable in a case like rain)' and 'identify when constants arent optimal or arent required or when its required to change the constant', such as in the 'driving' problem space where a 'constant speed' may avoid errors like 'over-use of brakes' and the 'rarity of a payoff of a speed-up except in cases like where next car is far-away', but which create errors in specific cases (like where the constant speed is zero or where constant speed of the first car in a sequence slows down other cars behind it or cases where speed-ups and slowdowns are more optimal, such as how slowing down is more optimal in cases like 'rain' to replace/re-create the usefulness of the higher constant speed in the 'rain' case)
        - this type of difference required to solve a problem ('change a constant into a variable') occurs in cases like 'where existing constants/functions dont fulfill requirements' (where there is a misalignment between existing and required solution structures), as 'injecting a variable' is useful in workflows like 'change a base solution' (as a base solution is identified and identifiable as suboptimal, as in different from a solution structure), as opposed to other workflows like 'reduce problem complexity (where complexity is the cause of the problem)'

    - identify useful connections like how workflows like 'trial and error' can identify other useful structures like 'convolutions' as in 'try every permutation of combinations/other interactions of attributes across two items' and 'neural network interaction functions' as in 'try every adjacent change combination, until a threshold of error minimization is reached')

    - identify useful inputs to intents like identifying 'interesting (as in high variation, or interactive between high variation structures) questions' that can be used as an input to queries like 'if a question/query is useful' (interesting questions that involve some interaction between a high-variation concept and a useful structure like known/available structures like 'neural network node sets', like 'if neural network structures like synchronicity is required for mass-coordination of neurons and whether this provides a communication function between neurons')
    
    - identify structures that can make other structures useful like optimization structures such as structures that can make 'risk' (uncertainty, ambiguity) useful, such as in cases where there will be immediate feedback indicating a possible payoff, where the change is reversible, etc

    - identify useful structures like variables that, when changed, can change other useful variables (like how 'slowing down' can help find information farther away, through avoiding error structures like 'input blocks' which can block far-away information)

    - identify useful structures like when useful structures (like 'high-variation variables') are not useful/relevant which can occur in cases like 'complex systems where many sub-systems can operate semi-independently of each other and therefore irrelevantly'

    - identify useful functions like 'invest' which have related adjacent useful concepts like 'payoff expectation by some threshold' which are useful for finding useful applications/specifications of analysis functions like 'cost/benefit' analysis
        - similarly, a useful 'investing function' is to 'invest in cases with a high ratio of equity', which is like 'investing in cases with a high information content like a useful prediction model or rule database that is already acquired which can be used as an input' (investing in higher-certainty structures), which is an example of how the rules of investing can be used to solve navigation/filtering problems, as investing involves similar information problems

    - identify useful structures like cases where error structures are not errors
        - such as when irreversibility is not an error but a useful intent to some agent, like an irreversibility (in maintaining some useful structure or another useful state change that applies some absolute truth to avoid applying that truth as an uncertainty by making sure its sustained)

    - identify the 'degree of difference/variation captured' and 'meaning of the difference' and other difference structures necessary to determine/identify/generate useful structures like 'cross-interface structures' and 'equivalent alternate sets of concepts like "spectrums of opposing attributes" (like chaos/organization, truth/falsehood, real/unreal, impossible/required, certainty/uncertainty, stability/instability, power/powerlessness, cost/benefit, constant/variation, embedding/containing, similarity/difference, adjacent/distant) and "similarities between attributes" (like potential/chaos and potential/energy and potential/power)' which are sufficient to solve most problems, since interface structures of a difference can fulfill intents like 'determine other useful variables and their interactions'
        - for example, different sets of concepts can solve most problems, just like cross-interface structures can, which is useful in that they have similar differences captured in the interactions of these high-variation structures
        - this means that most problems for agents involve differences between 'mental functions/attributes (like scalability)' and the 'complexity of systems they interact with' (the problem is that the human brain cant process a lot of information at once, not that the problem is particularly complicated)
        - however more complex methods are necessary bc the adjacent solutions found by existing algorithms are usually suboptimal in ways that are not identified by the algorithm and are not obvious/trivial/default interpretations of the solution (a 'solution found by an algorithm to solve a systemic condition involves a compound that switches off an immune function, which causes another condition not solved/identified by the compound, bc the algorithm doesnt connect all variables adjacently, just the inputs/outputs of the data set')
        - identifying the 'degree of variation required to solve most problems' can filter the set of functions/variables that should be included in a solution-finding method or solution
        - existing simple interface structures solve 'medium-complexity problems' which neural networks are frequently trained to solve, which are mostly a problem caused by 'scalability limits of the brain'
        - more complex interface structures on the outer layers of a generative function set diagram solve more cerebral problems like 'logic puzzles' which require more complex structures kept in working memory to make trivial to solve, like 'organization/optimization functions', which can allow other required mental functions for higher complexity problems like 'identify new concepts/similarities/variables' and 'identify unifying systems/structures which interact in a new way to explain/solve a complex problem'

    - identify useful structures that occur in other systems (like the brain/bio-system) which could be related to optimizations of useful structures (like neural networks)
        - for example, the 'blood flow' in the brain changes how the brain can be and is used, and similarly other neurons can evaluate how other neurons are doing, which implies that there should be extra neurons kept in a neural network for evaluating the progress and connectivity/interactivity and similarities of other neurons just like extra neurons are capable of creating scalable mental functions (associated with intelligence) like 'evaluating a culture' which requires a lot of extra variables so there would need to be free neurons to handle evaluating complex interactions like that
        - connecting the 'neural structures' is useful to identify existing biological mechanisms for brain structures like functionality and errors which can be used for optimization or avoided once mapped directly to neural structures)
            - 'blood flow' (as "a force acting to group neurons' interactions and enable other functions like reward/pain signal delivery and waste removal")
            - 'electrical activity/conductivity/other electricity structures' (as 'modifiers of relative position through modifying speed of connections')
            - 'neuron connectivity/presence/activation' (as 'usage' and 'available resource' structures)
            - 'waste removal and immune functions' (as a 'de-noising' structure, to get rid of 'noisy/irrelevant' information structures like dead-ends, unused neurons, outliers, over-specific parameters, and other error/suboptimal structures)
            - identifying other error structures like in order to 'trigger a useful process that creates rewards', there needs to be a 'lack of rewards, if this process checks for that'
            - other structures with high-level general states like 'emotions' (encoding them as defined in neural structures like defining 'depression/mania' as a 'wide-spread, mass-scale de-activation/over-activation of neurons, especially neurons which connect to other subsets like high-traffic subsets' and mental disorders like 'schizophrenia' (like 'forced separation of neural groups which are activated one at a time, forcing each group to develop all useful mental functions as equivalent alternate brains'), as well as 'solution structures of those mental disorder states' such as 'connection/variable' structures for 'autism/schizophrenia', 'positive/variable/opposite' structures for 'depression', 'boredom/constant' structures for 'mania/bipolar', etc, and similarly connecting 'personalities' (as 'brain usage structures like creative/intelligent personalities that optimize for some metric like novelty/boredom') to these other brain structures like 'brain disorders', 'default brain perspectives', 'brain connection units (as repeated structures in the brain, like common connection functions between neurons like "check for related specific variant nearby")', 'brain optimizations' (like commonly useful solution structures to brain disorders and functions to build multiple different brains to use to evaluate each other to implement default power structures like solution structures like checks/balances which offer 'equivalent alternate powerful structures that are connected by some cyclical dependency'), and 'emotion states' (as 'outputs (as outputs of a reward/cost assignment function) of structures common to various personalities and the resulting/associated perspectives and disorders')
            - identifying possible errors with connections/interactions between these mechanisms is valuable to identify mental/logical/other brain usage errors and opportunities/solutions for optimization of brain structures, such as how 'empathizing with people with depression/other brain disorders can produce similarly negative signals, if repeated enough, and these structures could become permanent as they might create a cascade, if repeated above a threshold value', and 'brain rewards are required as inputs for the bio-system to continue to exist, rather than always being a significant signal of some successful mental process'
            - identifying possible optimizations (like 'repeating a particular function consciously to adjust how structures are connected/stored in memory', such as 'only focus on new difference types and how theyre different from known difference types (or new solutions), and ignore other variables/structures', which could create a brain that only sends success signals for new variable types identified (or 'sources of high variation' which are likely to produce these new variable types), which is highly optimized for a problem-solving intent like 'identify new variables') to suboptimal memory storage structures (such as 'always connecting memories back to the same problem or same system', which might not be a sufficiently high-variation system to contain all useful variables or might contain so much unnecessary information that its not useful as a mechanism to organize information)
            - other structures like 'optimal neural states' such as 'mental flow' which is characterized by high connectivity, frequent successful usage of the system so its completely imported into working memory, organization such as applying probabilistic connections to base more trivial changes around core probable connections, and lack of barriers to connectivity such as random noise, while still allowing for variation in inputs, which are optimally routed in this sub-network of working memory enabling states like 'flow'
            - other errors such as 'surprise even when the input is normal, bc its necessary to believe falsehoods (trick the brain) to survive shockingly negative frequent truths, so normal inputs continue to trigger surprise'
            - other errors that could be formatted as 'neural states' like 'loss of executive function' (where the 'outer neural layers and available neurons enabling consciousness and executive control are disrupted or used for other tasks as a result of required functions or neuron loss in other regions, where alternate executive function structures take the form of interim networks that connect and control connections between sub-networks like between available neurons and task-specific grouped neurons like memory storage neurons and working memory neurons')
            - 'memory' (like a 'directed network of neurons with similarities in interface structures like patterns and concepts that efficiently store most useful information', as well as 'working memory' as a 'set of available neurons distributed among neurons used for specific frequently used information' and 'recent memory' as a 'set of neurons storing recently encountered info as a proxy for relevant info', and 'abstract memory' as a 'set of neurons to store commonly used general rules which are easy to remember and frequently used and capture high variation and are abstract enough to be generally applicable' and 'physical memory' as 'neurons spread through the bio system to store repeated sensations and connections between them', and 'neurons which remember feelings in the brain like feelings about blood flow as a signaling system to trigger other useful processes, like immune processes where blood flow is inadequate' and other memory types)
               - similarly, 'memory-finding functions' like 'think about (apply functions in, apply differences in) context when other related info to the memory is known, like where a memory was originally stored, in order to find the memory'
            - 'brain waves' as a 'synchronization process where neurons are required to temporarily coordinate in some different interaction pattern like a different neuron/memory access pattern, with each other and with other neuron groups coordinated by some other wave or interacting with themselves as theyre pulled in a different direction by the opposing waves, for functionality like "locally distributed scaled interaction functions fulfilling some general intent, like regulating/contradicting/limiting some other waved/mass-coordinated/temporary process"'
            - similarly, connecting 'biases' with neural structures like 'dedicated subsets of neurons for a particular intent (like a memory network storing recent information) leading to biases (like recency)' and 'locality/self biases (through sensory inputs about local information being highly prioritized in the brain)' and 'similarity biases (like familiarity (as in self-similarity or recent/experience/memory-similarity) to encode minimal new information)'
            - similarly, memory formats can be useful to design a neural network using a mix of memory formats
                - like a 'concept and a structure indicating a variant of that concept when applied to the concept', such as deriving the concept of 'austerity' through concept of 'restraint' having applied a related structure like 'antidote' which is structurally similar but not semantically related, where the concept encodes the semantic relevance and the structure encodes the structural similarity needed to connect the input query with the output word 'austerity' which is a specific variant of 'restraint' that is useful to store in memory for various intents, as opposed to only remembering the concept of 'restraint' or 'limit'
            - similarly, attention-maximization mechanisms such as 'emphasis' are useful through applying known attention insights (such as 'emphasized words are paid more attention to' by applying the variable of 'cadence') to solve attention errors like attention deficits such as might occur in cases like 'lack of risk' (where normally 'risk' is the trigger of attention, but its still useful to pay attention in 'cases with low-risk' as in 'boredom' structures, cases in which attention is difficult to trigger), and similarly other structures can be applied to solve the problem of 'lack of risk' (like how 'games' are by definition 'low-risk' but still incentivize paying attention through various mechanisms like 'accessible rewards' such as 'imagined/otherwise false risk'), and similarly other solutions like 'saying thoughts out loud or writing them on paper' as a way of 'forcing paying attention'
            - similarly, brain states like 'boredom' can be applied as a solution metric for the 'neural network architecture/algorithmm search' problem, as both an optimization structure (a 'bored brain' is a brain that has solved some problems or reached a case where there are no problems to solve, so that case should be applied as a possible optimal input case) and an error structure ('boredom' can lead to other errors like 'lack of creativity' and 'increased capacity to try sub-optimal/error cases/functions to find structures like variation/problems/other complexity structures which may be suboptimal')
            - 'bias' structures are useful to identify, as they optimize for some metric, like how 'bias toward interactive structures which can be interacted with' optimizes for intents like 'adjacently identifying useful variables for local intents' and the reasons why these biases are optimal in some way, such as that the 'bias toward interactive structures is related to the local bias' and the 'interactive bias is useful for local intents' and 'its also more useful to optimize for local intents in some cases like where error interactions are probable such as when opposing-direction vehicles are passing each other, in which case its justified to do more computations regarding local structures like local agent intents/inputs/errors'
                - similarly, bias structures are useful in that they can often be adjacently combined in some network where they offset each other's over-prioritization errors (similar to how combinations of high-variation/abstract/otherwise useful concepts like 'power/balance' can be adjacently combined in a network to offset each other's over-prioritization errors)
                - other structures should be applied to offset different error structures than 'over-prioritization' errors, as this is just an example of a 'solution structure to that error' and the other structures that solve other errors should be applied in combination with this one solution structure
                - relatedly, its useful identify useful structures like 'errors of brain mechanisms like biases' such as how 'errors' could seem like a 'requirement/certainty' if a function is applied to 'identify errors' repeatedly, so that the existing brain mechanisms like 'memory' misinterpret it as a 'certainty' structure through its commonness/familiarity/recency/repetition

        - insights about brain usages/interactions/functions through connecting structures like 'feedback' are similarly useful to apply in an optimized neural network
            - the insight that 'existing reward mechanisms are too easy to game/rig/guarantee to incentivize complex problem-solving' can be applied with a solution to that problem (such as 'connect reward mechanisms to problem-solving processes' and 'inject variables like problems in an organized structure like "sorted by increasing complexity" to require problem-solving processes')
            - similarly, 'existing reward mechanisms' dont account for error structures like 'randomness' (rewards can occur randomly, which can be useful for learning complex variable interactions leading to false randomness, but this is not incentivized, as the reward already occurred prior to a decision to identify the falsely random procesess that resulted in the reward, and the reward may seem too random to examine for possible stable variable interactions to identify) and other 'negative' structures like 'corruption' ('rigging/falsely guaranteeing rewards') resulting from 'existing reward mechanisms'
        
        - in reverse, connecting interface structures to possible neural structures that could fulfill them
            - the 'insight' structure in the brain is related to a structure like 'liquid waves' (which could be related to blood flow or electricity or brain waves), which are evocative of a crossing of a threshold, above which momentum is such that it causes an 'organizational cascade', similar to how once the threshold between function layers is crossed (by identifying/generating new structures on the outer layers), the outer layers provide more complex structures that capture higher variation, which can be used to trivially organize structures on the inner layers (an 'organizational cascade' occurs when the new complex variation-capturing structure is identified, which easily organizes other structures), where this momentum is required to identify some new structure
            - 'derivation' structures like a 'mental simulator as a set of neurons that is regularly emptied, so different intents can be fulfilled by importing different structures like "different available core structures" regularly to connect known structures in a different interaction function to find new structures' (interim thinking)

        - also other variables can be injected in neural networks (like 'constant once identified as useful for success' as opposed to inactive/active, as a neural network node type that doesnt get updated once its identified as a contributor to some successful change during training, which is likelier when variable-sized changes are allowed by the update algorithm as opposed to prioritizing adjacent changes)

    - identifying useful structures like 'solution filters' (as in 'checks') that can be applied to solve specific problems
        - for example, identifying that the earth is not flat (a 'default assumption') is a matter of identifying useful filters like:
            - identifying required connections (if the earth is a different structure like round, there would be people who had traveled in the same direction around the world, or from connective subsets of the path)
            - testing a core point on a unit structure ('if a round ball can support a structure on it once the structure is scaled down to be sufficiently small, the earth theoretically could as well') that disproves a core counterpoint (the 'flatness' would be the 'foundational structure' required for structures on it not to fall over, would be the default argument contradicting a round earth)
        - relatedly, alignments tend to resolve ambiguities, such as how alignments between function intent and usage resolve the ambiguity of how the function is optimally used

    - identifying useful structures (like existing solutions and variables/causes of them) to connect with other useful structures (like new solutions)
        - for example, other useful inventions solved a problem that reached a threshold where it was common and there were tools available to fix it if enough manual labor was done, addressing some problem of reality like 'scaling the functional hours in the day' (light bulb), 'reaching enemies farther away' (gun), 'increasing the speed of information transmission and of vehicles through more explosive but controllable materials' (telephone, engine), which were problems of scalability, which is a core structure of most problems, and which created other problems of scalability like 'removing pollution from lungs' and 'verifying transmitted information faster than false information can be distributed'
        - predicting new inventions is a matter of identifying problems that would be solved with more scalable structures of existing solutions
        - similarly, predicting new inventions (and their suboptimalities/errors/optimizations) can be done with other relevant high-variation variables like 'agent personality', such as how identifying error structures in a personality like a 'false assumption/constant' or an 'over-prioritization' error or a 'self-contradiction' error or an 'over-used core/interaction function' error can be identified from a personality, such as how a 'hypocrite' personality reflects an error of 'local bias' (specified through a structure relevant to neural networks of 'avoidance of applying analysis/optimization functions to the neural network itself that produced this error', so an 'optimization structure' of a 'neural network' involves including a function to handle this error structure), these personalities involving variation around core types (based on core personality interaction functions like 'judge/analyze/criticize', 'test', etc), just like intents vary around core intents (like 'safety', 'risk-minimization', 'scaling', etc), as personalities are 'optimization' structures of 'brain usage' structures (as in a 'style of brain usage' since its a variable like 'driving style' as not every type of interaction with the brain is required/constant) that solve some problem (like personalities related to brain disorders like attention deficits/hyperactivity that account for these brain errors, such as 'saying thoughts out loud to force paying attention to them', where the personality results from some usage of this solution to the brain error, such as being particularly talkative/social through speaking more, or more thoughtful as a result of paying more attention to thoughts with corrective measures like 'speaking out loud or writing thoughts down'

    - identifying useful structures like 'repetitions' and 'scaling' and 'core changes to core structures' that can adjacently identify other useful structures
        - similarly, useful structures are useful in connecting other useful structures, like a function (like a spiral, as a 'repeated change that creates other change structures like change rate changes') that intersects with and therefore connects every useful function set layer in a function layer diagram, a connection from which other functions on that layer can be derived (similar to how a simple straight line that abstracts away some attribute/functionality with each step could intersect it if the layers are known/understood or defined/required to be organized by being increasingly abstract), and similar to how 'core cross-interface structures' have a 'vertex structure (of two vectors with an endpoint in common)' on the system layer diagram of interfaces and may also have a 'similarity in vertex angle', if the graph is organized by a similarity of the core interaction in these core cross-interface structures, which would be an example of a useful organization method for identifying other useful structures using that similarity
            - relatedly, structures that occur across problems like 'spirals' can be a different type of useful structure in that they can form vortexes, which is similar to how interfaces attract change
        - similarly, identifying 'core changes to core structures' (like changing position to top of a stack) can identify other useful differences like 'scaled' structures (like far-away structures, like to 'identify water from far-away by scaling a mountain, or identifying clouds that produce rain and cloud movement direction, or identifying lower elevation areas as likelier to contain water and identifying errors of input like false equivalences to identify lower elevation areas even when blocked by an input block like a physical barrier, or identifying concepts like "diversity" which map to useful structures like "variability" which can direct attention, such as through "searching for differences from the current error state, in variables like color" such as identifying green areas which are likely to have water, or concepts like "connectivity" or structures like "networks" to identify "underground networks of water" as a structure to search for or stay within some range of, or identifying plants likely to have water such as through "functional requirements (like water storage)" which adjacently identifies "thickness" as a useful variable to identify, and similarly identifying the "opposite structures of water" (such as a mirage) which can be identified through moving in a way that is not "moving closer" to filter out default errors such as that 'water evaporates by moving closer to it' where 'motion other than moving closer (like moving position of inputs but not position of agent)' could also disrupt the mirage, but would filter out the error structure)
        -  identifying core differences that can be repeated and can interact like scaled life forms to identify other scaled structures like causally distant (past/future) structures (such as identifying that 'dinosaur fossils' could exist by scaling the core structure of 'dust deposited from air' and 'scaling size of existing animals, which might exist at a different time bc of coexistence impossibilities', which is useful for identifying other useful structures like 'asteroids' and 'evolution')
        - finding the full set of example structures that can create surprising real differences when combined trivially is a useful problem-solving intent

    - find similarities in workflows with similar solutions found so different workflows can be found and checked for different probability of finding diifferent minima of the error function
        - similarly, finding overlapping optimals can identify positions without overlapping optimals
        - relatedly, identifying 'variable sets which can be combined in any way to still produce a solution' is a useful set to find, as is a 'variable set that can still produce a solution when an item from some set of terms is added/removed', just like finding common similarities between solution functions (like similar high-power terms) is a useful intent to fulfill
    
    - identifying useful structures like slowdowns/constants as proxies for minima in the error function, and identifying reasons for these slowdowns/constants so they can be connected and differentiatied as needed for various intents
        - various limits can produce a slowdown in other systems which are useful to identify (variants of a limit, as in a 'limit on change potential')
        - identify 'areas of slowdowns' is more possible with some functions than finding 'minima' and these structures fulfill similar intents
    
    - identifying useful structures like connections like that cross-interface structures can be equivalent alternates or symmetries used to build maps and similarities or build complementary info usable to build full sets of differences, to connect very similar or different info usable to solve a problem, which is why cross-interface structures occur in solution workflows, as these cross-interface connections are highly similarizing/differentiating as they connect differentiating structures
        - for example 'function-intent' and 'position-speed' are useful differences to connect and apply in a set, such as how its useful to regularly apply a function to 'check for a more optimal interaction with a core variable (like position)' to connect a function/variable with a useful structure like an optimal usage intent (such as a function to 'find an opportunity to change relative car position' to fulfill intents like 'pass a slowpoke' which can be a required intent to add to an algorithm that accounts for speed/position/far-away change predictions, as 'changing relative car position' is so useful it may as well be a required function to include with any 'driving' method), similar to how knowing the intent of a question can evaluate whether an answer should be found (whether the function to find it is useful to use)
        - similarly, identifying useful intents is useful, such as 'connecting local error minumums with generally useful functions' and 'connecting useful differences in definitions', like how 'function network' is an inadequate but simple description of ai that is more useful when grouoed with intents to solve for, like 'connect many changes to one change' and 'connect complete changes to composing changes' to fulfill intents like 'identify which changes (of the many used to create a particular solution function) are most useful in a particular successful change of a solution function'
        - similarly, 'problem-solution sets' are high variation in that they by definition 'connect differences in a useful way' and can be applied as a structure to solve other problems as a result of this variation-resolution
        
    - identifying useful structures (like differences from interfaces and causes/components of interfaces) which can be used to identify other useful structures (like interfaces)
        - identifying that symmetries are a form of irrelevance (where one variable cannot change the other), so that 'areas of irrelevance' can be identified, such as 'info that can replace other info, like equivalent alternates, so that the other info is not necessary and is irrelevant' which are possible interfaces, just like 'equivalent alternates' are also useful structures to find other interfaces, to find structures like 'positional irrelevance' (the 'position' doesnt matter, what matters is 'any membership in a set/type at all') and other forms of irrelevance which can identify alternate xtructures (like 'similarity/solution metrics') to be used, similar to how identifying alternates to 'sequences' in the 'predict the next word' problem space is useful in that it can identify structures like 'common sets like common phrases/patterns' which can take priority over or replace sequences
        - similarly, 'causal irrelevance' occurs where the 'causal structures' dont matter, what matters is 'whether some event occurs at all'
        - this form of irrelevance is like the opposite of an interface, where the structure's irrelevance indicates that the problem is solved or more solvable on a different interface

    - identifying useful structures (like 'mixes of workflows') that are other interface structures (like 'optimals' or 'requirements') in some cases
        - for example, its useful to identify position and speed but also mix that with another method like 'check farther ahead than is necessary' where that method is mixed by applying it regularly, similar to how a method to 'remain some distance from other structures' can act like an equivalent alternate of that mix, where these methods are better when mixed together to avoid their known errors like 'avoiding structures by not moving at all' where a network of comtexts could allow identifying the contexts' similarities to quickly check if a direction leads to an error
        - identifying insights like that "a 'mix of algorithms' is optimal in some cases, such as where one algorithm will inevitably be wrong at some point, so mixing it with another algorithm at regular intervals can handle this error" is useful as a set of core interface structures which are inherently useful to connect
        - for example, when guessing a route with no instructions available except a general direction to move in, 'following one road' is likely to be incorrect in some way like inefficiency or at some point like where it diverges from the optimal path or the direction that represents the problem to solve (the problem of 'find a path that fulfills this direction'), so injecting a variable like 'changing to other roads' is likely to improve on the "first guessed road" if extrapolated to the entire path
        - similarly, other insights apply to point to this insight, such as where 'structures rarely self-sustain, so assuming one structure will always be reliable is unlikely to be correct, as structures interact without being instructed to and these interactions create degradations of some variables'
        - similarly, other workflows can identify other variables adjacently through identifying variables of a problem space and the interface structures connecting these variables which could be new workflows (variables of a problem space like 'driving' such as 'interactive structure differences like surface differences and usage differences' to identify other variables like 'road texture' as important for safe driving where the solution is swerving to avoid differences in road texture if swerving is sustainable over time, and other usage variables like 'fatigue' and 'lack of requirement to use both hands when steering at all times' which when combined allows for useful optimization functions to be identified, such as 'switching hand used to steer' in cases like 'where next car is far ahead so swerving to avoid a crash is less likely to be required'), workflows such as identifying useful sets of structures like 'error-causing inputs (fatigue)' paired with 'structures of cases (where "next car is far away, so errors like crashes are less likely and more variables can be used") that include a "lack of requirement" (as in a variable, "steering hand currently used") that allows for an adjacent change (switching steering hand), where this adjacent change solves (reduces) the error-causing input (fatigue)', and similarly, identifying generative/identification functions of these variables (apply useful concepts like 'interactivity' to identify specific variants of it like 'surfaces' and errors like 'crashes' and 'usage structures (like driving style which is a high-variation variable of how the agent interacts with the driving tools)' and other useful structures like 'variability causes' and 'error causes' to find these useful concepts like 'interactivity' as a useful specification of 'variability' which is more useful to store than to derive from 'variability' every time), which is useful for problem-solving intents like 'identify new variables (for intents such as to explain a variable interaction not already explained by known variables)'

    - identify useful structures (like 'scaled interactions' and 'time interactions') which can be used to fulfill problem-solving intents like 'identify more useful workflows'
        - for example, the 'trial and error' workflow is associated with a 'multiverse' universe (as the ultimate scaled version of the 'trial and error' algorithm is like building a time machine which can test/simulate all possibilities as good as or better than reality can, so that any moment (and any sequence or other structure of moments) can be examined using the 'trial and error'-implementing program) and some structures are particularly useful to identify for this algorithm, such as 'equivalent alternates having some pattern that allows them to be derived from each other' and other structures that could reduce the computation requirements of 'trial and error' (to equate some possibilities, so that the set of possibilities considered to be equivalent to 'all possibilities' is reduced)
        - any problem-solving workflow is similar to a time machine in its reflection of the variables of reality (as these workflows connect variables in a way that all or most of reality can be determined/simulated/tested by it), but some are more optimal than others, and identifying all of these workflows which can simulate reality builds a more complete time machine (in the sense of a simulation machine) than other methods, and similarly any position where an observer has access to such an optimal machine that reflects reality accurately and efficiently is likelier to be a higher-variation position that attracts information and variation, which could bend spacetime in unexpected/unknown ways, thereby impacting other positions' ability to simulate time/reality through requiring them to simulate it similarly efficiently as the original observer or be subject to the spacetime changes created by the original observer, where the original observer has a tool to process high variation interacts that they are likely to attract with this efficient handling of variation and other observers do not, which is likely to create inconsistencies in how spacetime stabilizes/bends that are resolvable if the simulator is distributed, inconsistencies which can store other universes' information inadequately/suboptimally (other universes being likelier to be attracted to and stored in this universe, the better the variation handling becomes), so once this optimal simulator is found, it should be as distributed as possible and the distributed versions of the simulator should be differently optimized to handle more variation in information attracted to this universe, so there is a structure in place to capture it once its pulled to this more efficient storage mechanism by forces like gravity, to avoid these 'suboptimal universe storages' from occurring, and other strategies to minimize these errors should be applied (like 'inefficiency signals' should be distributed as well to avoid attracting too much information to this universe before the implications of doing so are fully understood)
        - identifying that this algorithm in its ultimate form is like building a multiverse is non-trivial, unless a program identifies the 'meaning' of a structure (such as its set of high-variation interactions in various important/relevant systems), at which point identifying this interaction is a matter of applying 'scale' and the 'physical information' sub-interface of the 'information' interface and identifying 'alternate usage intents' of the resulting scaled structure which could be similar to some structure in the 'physical information' interface (like some variant implementation of itself such as a 'multiverse')
        - similarly, identifying whether some algorithm more efficiently captures reality than reality is a matter of identifying whether there is some simulation of reality that requires fewer representations (such as fewer repetitions) while still correctly predicting every important variable interaction (by identifying priorities which are important and fulfilled by that algorithm's simulation of reality) and other metrics (like if there was such an efficient simulation of reality, it would likely vacuum information in as it would store/interact with information better than reality, and other realities that existed prior to this computer would likely mimic this efficient reality and thereby standardize and synchronize realities to that more efficient reality)
        - therefore identifying how algorithms interact with universe structures as well as other useful inventions (like blockchains and time machines and neural networks) is useful to identify, just like identifying how algorithms interact with good/evil and truth/falsehood and problems/solutions and power/balance are also useful to connect each algorithm to, as important metadata of the algorithm to compute
        - 'structures (like "sequences") of problems to solve' are similarly useful to identify, to direct how these workflows are optimally used, such as a 'sequence and distribution of databases (of structures like "task lists" and "known variable interactions" and "useful problem-solving workflows") to create, which will push civilization in the direction of some priority like fairness, organization, efficient, etc'
        - the 'database sequence (or database network sequence)' is identifiable as an available structure reflecting a 'state sequence' which is the 'core structure' of the 'physical information' interface (reality), which is usable as a component to build a path to a particular future and connect other spacetimes as well
        - what does it mean that workflows (like trial and error) relate to possible implementations of reality (like the multiverse) through similarities (like the 'computation type required' in both 'trial and error' and the 'multiverse')? it means that some universe configurations can be filtered out as impossible or less possible using these workflows and structures of them like combinations of them
        - given that multiple workflows exist and are optimal for some purpose, and there is no 'general purpose, always optimal, in all cases' algorithm (except interface analysis which produces all of the optimal algorithms), the universe is likely to reflect the structure of 'a set (and an interactive mix) of these optimal algorithms' (for example, a multiverse, but an optimized one, with several 'equivalent alternate' universes which overlap or otherwise intersect on their symmetries/common standards), where some interfaces reflect each other as they capture the same information and can be used to derive each other
        - this is different from saying that an 'algorithm operates best in a particular context' (like a 'worst-case scenario' context as indicated in the 'algorithmica' or 'heuristica' world of the 'five worlds' of complexity), as 'system context' is an input to algorithms, but there are 'absolute contexts' where some algorithms would be optimal in all cases and those absolute contexts can be extrapolated to differentiate those from physical reality
 
    - identify structures which are useful for identifying other useful structures like 'maximal differences' and 'equivalent alternates' which are useful to identify variables and thereby fulfill other problem-solving intents
        - for example, different senses provide complementary or equivalent information, usually related to some 'feedback about position change' (such as a 'reward/pain feedback' sense and a 'gut sense of nearby life forms involving microbe-microbe interactions' and a 'health sense of functionality that is working' and a 'thought sense of triggers of useful thoughts like considering alternatives' and a 'sense of synchronicity/alignment between different internal systems' and a 'logic sense of information such as time passed, which indicates survival, which indicates some useful interaction with the external world', regardless of whether the primary senses are available), where different structures can be adjacently derived from different senses and some can combine to form equivalent alternate information (useful for finding different interfaces for information) or complementary information (useful for building a complete set of cross-interface information)
        - similarly, 'connections between maximally different graphs' are useful to identify 'equivalent alternates', 'interaction levels', 'differently useful information', and other useful structures
        - similarly, the question of "identifying what is not an interface (such as 'other interfaces', 'limits', 'interface components', etc)" is useful to identify useful variables, which are useful for many problem-solving intents
        - similarly, the question of 'what errors produce useful functions in neural networks (such as concussions in an autistic brain, which might add connectivity if a coincidental angular interaction occurs with the concussion, a useful input to consciousness and other functions)' can identify 'structural errors connected to functions' in neural networks

    - identify useful intents to fulfill such as "identifying interactions between core interaction functions or problem-solving functions/intents and cross-interface structures" (like 'identify structures that change functionality of structures' and 'identify causes of different functionality in similar structures and similar functionality in different structures') which are specific variants of problem-solving intents (like 'connect different structures') and therefore are more useful as approximations or implementations of those intents (fulfilling those two identification functions could solve a high ratio of problems)
        - related intents include identifying useful interaction levels of useful structures like common concepts/structures such as 'complexity' ('high count of function usage to fulfill required intents') and 'embedding' ('structure enabled by a base structure') and 'heterogeneity' ('difference embedded in a structure, as in non-uniformity or non-unitary interactions'), which have some structures in common like interaction functions (apply/base) and cross-interface structures (function-intent, usage-intent, usage-function, etc) which indicates that other cross-interface structures interacting with core interaction functions could identify other useful concepts
        - this useful interaction level implies that identifying more advanced/complex interactions (such as iterated embeddings) is useful as a mechanism of evaluating simpler structures (identify 'embedding an embedding in an embedding' to evaluate 'embeddings in an embedding' successfully, as an 'outer layer of consciousness, where the inner layers fulfill requirements and the outer layers are free to evaluate the other layers as they can store the inner layers or queries of them in their own layer, given the ratio of free neurons available to import structures into for evaluation processes, in the outer layers')

    - identify useful structures like high variation variable interactions like specific difference structures or specific variable interactions that co-occur to identify other useful structures like 'probable variable sets'
        - identifying interactions between variables like 'difference structures' (such as 'homogeneity', 'heterogeneity/differentiation' and 'specialization', as 'highly different structures' (like different senses or species) tend to occur in real systems rather than 'highly similar structures (like high ratios of repetitions)') which tend to occur across structures, and similarly variable interactions like 'interactivity' and 'variability' which tend to co-occur (like how liquids tend to be higher variation than solids bc of the interactivity within the structure), to create other useful structures (like 'directed attribute graphs of various difference types like 'heterogeneity' where direction indicates sequence and preceding terms in the sequence indicate probability' which can be used to identify 'probable variable sets')

    - identify useful interface structures (like 'type interaction functions' and 'complementary information') which can be connected to other useful structures (like 'change inputs') which are known to be connectible to other useful structures (like 'differences in solution functions')
        - for example, attributes/functionality interact in a way that changes attributes as theyre not static constants (which is why multiple solution functions are better to identify than one in complex systems), such as how a type may mimic other types to hide its identity to opposing agents and therefore seem like other types that it has encountered or which are less antagonistic to the opposing agent, or the 'type' functionality expectations may be valid for only some subsets of inputs (like 'during specific time periods when the type is checked or when the type is first initialized which is when its identifying attributes are most extreme, after which its likelier to vary from these identifying attributes than to maintain them perfectly'), where functionality (like 'type-mimicking' functionality) can act like complementary information to the attributes (like the 'type') as it can change the attributes so its better to have both attribute/functionality information
        - this cross-interface structure of 'attributes/functionality' can be used to identify other structures like 'change inputs' (like 'causes of a variable interaction change to have a different possible state'), similar to how other relevant interface structure interaction functions like 'type merging' or 'default/backup types' are known structures, and can be applied as constants to check for type interaction changes over time or which are evident in the original data set, as these interface structure interactions are connectible to differences in solution functions for a particular variable interaction
      - this is related to how a 'density' can be mapped to various interface structures (like an efficiency, a type, a certainty, a requirement, etc)
      - similarly, identifying useful connections like 'complementary info of adjacent info' is useful for 'creating other connections' and 'identifying structures that can be applied as a certainty' (as in a 'base for change')

    - identify useful structures like common variables of useful structures like mappings between 'common useful structures', 'functions', and 'solution automation workflows'
        - for example, the 'symmetry' structure has a related workflow like 'change a base solution', similar to how the 'limit' structure has a related workflow ('fulfill limits such as requirements/solution metrics' or 'limit the problem structures/inputs/outputs') and the 'connection' structure has a related workflow ('connect useful structures like interactive structures or problem/solution structures') and the 'shortcut' structure has a related workflow (like 'find efficiencies and maximal differences and other components of optimization structures'), and all of these structures have related functions ('differentiate around a similarity', 'limit/filter/reduce', 'connect/standardize/compare/similarize/differentiate', 'map independent systems/connect maximal differences')
        - these are related bc they are alternate variants of each other (the structures are variants of the functions, and the workflows based on these structures/functions are 'solution automation workflows' bc these structures/functions describe core differences of reality)
        - 'finding variants of a useful structure' makes it likelier to be possible to trivially connect that structure to problem/solution structures like solution automation workflows

    - identify useful function sets that can connect sufficiently different, disconnected/uncorrelated structures (like independent variables) that they would likely contain most important variable interactions in their connections
        - for example, the important variables of positive/negative, true/false, certain/uncertain, finite/infinite, similar/different, useful/useless, and stable/unstable are extremely uncorrelated/independent and different and are therefore useful to connect, so much so that connecting all of these variable values in all their possible useful/powerful connections could act as an alternative to deriving other rule sets to identify the rules/limits of real systems or just the complete rule set of the interface network, as some of those attributes encode a primary interface concept like 'information' (certain/uncertain, true/false) and some are concepts that can describe reality like stability/usefulness or structures which are commonly useful like limits (finite/infinite), and different sets of these structures/variables/concepts can be similarly useful in capturing the variation of reality
        - this is related to how finding a map between very different systems (like biology/physics) is useful as a way to derive a sufficient ratio of real rule sets that it can be used as a proxy for a complete rule set in many cases

    - identify solution structures (like solution metrics, solution set size, interactions with other solutions, and other solution info) by applying useful structures like similarities/differences in the problem system (and across problem systems)
        - for example, in biology there are usually multiple different pathways (the solution functions) for the same mechanism (output), so there are multiple solution functions to find, rather than one, and these solution functions may be trivial to generate from each other, after identifying other variables (other required inputs and the connections between them), as the solution functions often seem falsely very different until one connection between variables like some subset of inputs like 'elements and their adjacent functions' or some pattern in the variable interactions like a 'common threshold all of the solutions cross' is identified
        - similarly, there is often a 'backup' and a 'default' solution in biological systems, indicating that there are usually multiple solution functions to identify in many real systems (like a way for different structures like ratios of elements or different sets of elements to produce the same functionality when one is at a suboptimal ratio, and if you had the 'variable interaction' info between the 'requirements' and 'functions' of the system and these 'element sets' which have some useful similarity, the 'other element sets that act like another element set' would likely be trivial to identify, which is a useful set of cross-interface variables to identify the interactivity between which can explain high variability in suboptimal conditions that vary by input structures like ratios of elements, which is a high ratio of conditions)
            - for example, if you had the 'variable interaction of elements and functionality', youd have info like 'sulfur is a required input for central nervous system functionality' and deriving other info like 'desulvofibrio bacteria (which reduce sulfur) could be related to central nervous system functionality disruption (like parkinsons)' is trivial once you know that element-functionality interaction and the related pathogen-element interaction, as the 'element inputs' are a surprisingly high-variation capturing sub-interface of the biological system (bc of its surprisingly extreme limits in its optimal input ranges) and can explain a high degree of variability in pathogen/condition response, so connecting 'elements' with each other and with 'functionality' is useful for deriving these 'input-condition connections' and 'equivalent alternate element sets'
            - similarly, other common variables of pesticides associated with parkinson's can be found with variables like 'interactivity', 'variability', 'persistence', 'negativity of side effects (known toxicities)', 'spectrum (across species and across systems) of negative side effects', 'connection to required components of health specific to nervous system', 'similarity to other known substances with negative nervous system effects', 'extremity of effects', 'multiplicity of uses/effects', 'uniqueness of functions/effects', etc (while recognizing that they dont have to have much in common to have similar side effects in a specific system, as 'structural similarities' arent the only useful structure to identify)
                - dicofol - has other negative systemic side effects (carcinogenicity, organ damage) but also specifically nervous system damage (related to parkinsons), is known to be toxic to aquatic life
                - trifluralin - is known to be toxic to aquatic life as well as extremely variable interactions (meaning its likely to be harmful in some way) and unusual attributes (meaning its likely to be different from known effects such as negative, neutral/harmless and/or positive/coordinating effects in some way, 'difference from positive' meaning 'negative'), has 'broad spectrum activity' and 'core interactivity (interfering at a core stage of development)', has similarity to other toxic substances (like 'explosives'), deactivated by water (a core bio-system component, indicating an extreme difference from the host bio-system)
                - copper sulfate - has broad spectrum activity (fungicide, molluscicide, algicide), toxic to aquatic life, highly interactive (reacts with many different substances), extreme attributes (colorant, variability in color, multiplicity of uses, many different related forms), identified as a toxin by human immune system, related to 'sulfur' (key nervous system component)
                - folpet - chemically similar to other toxic agents (trichloroethylene), powerful extreme compounding and multiple mechanisms of functionality ('fungicidal')
                - endosulfan - persistence/robustness, known to have negative systemic side effects (endocrine disruption), broad spectrum of activity, unusual mechanism, contains at least one component of other toxic substances (trichloroethylene), produces highly interactive/reactive substances ('reactive oxygen species'), related to 'sulfur' (key nervous system component)
                - naled - broad spectrum of activity (kills birds, pesticide), known to be toxic to aquatic life, chemically similar to other toxic agents (trichloroethylene), deactivated by water, toxic with trivial transforms (acid), highly interactive, has known negative nervous system effects, has known negative systemic effects (inhibits key nervous system enzyme)
                - propargite - broad spectrum of activity (kills fish/amphibians, pesticide), known to be toxic to aquatic life, has other negative systemic side effects (carcinogenicity) 
                - endothall - broad spectrum of activity (herbicide, pesticide), known to be toxic to aquatic life, disrupts intestine health, destroys useful inputs (dessicant to plants), inhibits a useful structure (tumor suppressor enzyme)
                - diquat - broad spectrum of activity (herbicide, pesticide), produces highly interactive/reactive substances ('reactive oxygen species') with trivial changes, persistent/robust, known to be toxic with other mechanisms, destroys useful inputs (dessicant to plants)
            - similarly, identifying other useful structures is trivial like the 'connection between high-variation variables (like different sub-systems like gut/brain in the biological system, to infer connections like "check gut microbiome pathogens by default for brain dysfunctionality", and connections between similarly high variation, high coverage, systemic variables like "input/food processing" and "mutations/conditions" which are similarly high variation and could therefore be adjacently explanatory with a trivial transform)'
            - the 'high variation variables' act like examples of solution metrics (as in 'common structures of solutions, like high-variation variables frequently are') which can be used to derive the others (as all the high variation variables interact so they can be formatted as different nodes on a causal sequence and derived from each other as they capture similar info variation), answering the question 'what is likely to cause this variable change' ('apply a high-variation variable filter (check the high-variation variables first)', as 'high variation variables' are a way to adjacently connect different solutions involving high-variation variables, and thereby derive the others from the first one identified, using a 'high variation variable-connection function')
            - relatedly, other sub-interfaces in the system could also explain the output mechanism, as errors (conditions like mutations) can also create functionality (since other sub-interfaces in biological systems can use inputs like element sets/ratios differently, and these sub-interfaces like pathogens are interactive with other biological systems) as well as intentional inputs (with intent to create that functionality)
            - 'how to connect the functionality of important inputs (elements) and other useful structures to connect (requirements, functionality, response to conditions)' is a useful structure to identify as being capable of capturing enough info and variation in the system that it can act like a proxy for other useful structures like a 'full set of all important mechanism inputs (like element sets or state triggers)'
        - another difference is that some different variable subsets can explain the same mechanism (which occurs when variable subsets act like equivalent alternates), and sometimes different functions in the same variable subset can explain the same mechanism (like when an over/under-supply of some nutrient produces the same output condition bc the area of its optimal value is so limited and fragile, like blood ph), which is useful to identify in the input problem system
        - this info about solutions is useful for identifying other useful info like patterns that occur across solutions (such as equidistance/intersectivity between solutions, similar rank/structure, solution set size, similarities in variation of variable subsets that can act like equivalent alternates) which can be used to identify other solutions once one is found, where finding one solution is trivial in the case of having many different solutions and identifying the connection between solutions is more useful in understanding the problem system than identifying one solution
        - identifying 'different input sets that produce the same solution' is useful to identify other useful structures like 'other different input sets that produce the same solution', the 'causes of similarity in outputs, across differences in input sets', and 'similarities/differences in inputs that would produce different solutions'
        - identifying certainties is useful to apply as info 'filters', similar to how applying a 'known similarity as a symmetry (around which differences can be applied)' is useful for building structures in a graph that reflect other certainties (structures like similarities/differences) of reality and is therefore useful for finding those reflected certainties
            - this is bc 'certainties' by definition/interaction map more adjacently to 'structures' (such as useful 'standards/filters', 'combinations/sequences', 'sets/densities', or 'graph organization structures like partitions/connections/nodes/layer/boundaries/intersections or graph usages like graph queries') than 'uncertainties'
            - finding the 'full set of graphs that apply a certainty in a way that reflects its real structure' (the certainty applied as a filter, as a base, as a density, as a network connection function, etc) could be a useful workflow for finding 'other adjacently related certainties, as reflected in these very different graphs that have the same certainty embedded'
            - an example of a useful filter combination is 'ratio/probability' and 'positive/negative' (or 'probability' and 'cost/benefit'), such as how identifying common variables of positive responses to negative inputs can be done by applying definitions of positive/negative (health structures vs. conditions/mutations/requirements/attributes like fragility) and identifying other 'similar' structures (like how identifying 'rare health structures' can be useful for identifying causes of 'rare positive response to negative inputs', and identifying specific examples of 'rare health structures' can be found by applying variation to the definition of health structures, like 'extreme athletes', 'extreme diets having healthy components like raw vegetables', 'extreme adaptations like high performance', to check those positive/healthy subsets first for the positive/healthy response, the extremity of which can be useful in identifying obvious differences, extremity being a useful truth structure through its connection to commonness/probability/stability), and implicitly applying a 'similarity' in positive/negative ratios/probabilities across variable subsets ('rare good health is associated with other rare good health structures like specific healthy responses' as a default solution structure to check first)

    - identify useful functions which are useful for some useful problem-solving intent like 'identifying other useful structures like specific functions that fulfill some different intent/metric optimally'
        - for example, 'control' and 'replicate' are two general/specific variants of other functions ('reduce/limit' and 'copy') which can be used to change the problem structures in a useful way, where 'control' is useful as a more general alternate function which has many variants that could be useful specifications to derive (like 'limit' or 'reduce' or 'isolate' or 'use (as in trigger)' or 'change' the problem or 'convert the problem structures into a constant or a simpler form, so its more controllable' which are adjacent variants of 'control') and where 'replicate' is a useful variant of 'copy', where 'replicating the problem' can solve the problem (as once the problem is replicated, some of its causes are trivially identified from the variables used to replicate it), where 'copy' is a generally useful function for workflows involving 'testing' such as 'trial and error'

    - identify useful specifications of problem-solving workflows that can still be applied across different problem systems
        - 'change the interactive surface (change the input/output of a function by applying injections of surrounding interim functions, rather than the function itself)' or 'change the host system where the suboptimal variable interaction occurs' is another interface query that can act as approximately equivalent to other function sets like 'change a base solution' or 'find adjacent change combinations of existing structures' or 'find interactive structures and build other useful structures out of interactive structures' or 'change the intent to solve for to a more trivially solvable intent, which is sufficiently similar to the original intent', as 'changing the interactive surface of a function' (such as changing the surface of pathogens or gene structures by applying coatings/encapslations/receptor alternatives, etc) is a useful variant of changing the problematic structure of the function itself (rather than optimizing the internal logic, change the interactive components of the logic until the interactive components change it enough to be more optimal/useful), which is a merging of 'find interactive structures to apply as components' and 'change a base solution until its more optimal' that is more trivial to solve for (and therefore more useful) than other more general intents like 'change a base solution', as it adds usefulness in solving the problem of 'which structures to change' ('interactive' components like inputs/outputs or surfaces) by specifying that variable into a constant
        - this is related to how some math structures are useful through their 'specificity' (like specific values of constants/sequences/limits/functions that are particular powerful/useful in some way) but seeing the generally useful structures like 'abstractions' is still useful as well, to add variation when known specific structures arent sufficient to solve a problem adjacently, like to find new generally useful structures like general solution-finding methods, as there is no known set of constants that adjacently explains everything (if there is, its interface analysis), so keeping some variation structures like general abstractions is still useful to offer an opposite/counterpoint when structures are too specific to be useful for some intent

    - identify useful connecting structures like 'high-variation variables' and 'problem space similarities which are high-variation (but not random)' and 'connections between useful approximately equal intents, where some subset of the intents are more trivial than others' which can be applied as useful target structures to apply as inputs to other workflows
        - for example, the 'transformer' applied 'sequences' in the position of 'word sequences', identifying that the variation of the last/next word in the sentence is a high-variation variable, but not so high-variation that it looked like randomness, and not so simple that trivial/few simple rules were usable to predict it, rather being in between these low/high variation variables, so that there was enough structure to predict the next/last word in a sequence of an isolated unit like a sentence/clause, but not so much structure that it was trivial to predict (so that it wasnt a problem at all) or impossible to predict (so that there is no selectable solution that is clearly better in some way), this structure including sequential patterns like grammar, definitions, etc, and embedded structures in those structures like 'possibilities allowed within definitions' and 'sensibility/reasonableness'
        - however just knowing the 'sequence' structure is common or even relevant to the problem of 'sentence completion/prediction' isnt enough on its own to derive 'transformers (which are a structure of predicting sequence completions)', but knowing that there is a 'similarity' to apply as a base for problem-solving in the sentence/paragraph prediction/completion problem space can make this problem possible/adjacent to solve (the similarity between adjacent sections of the sentence/paragraph/document is sufficient to allow prediction in most cases, as its a similarity based on relevance such as adjacency and connectivity and even cause like causal degree, except extreme nonsense/randomness or extreme creativity which deviates from known rules in a realistic way, so applying this similarity as a base for problem-solving is useful and works, meaning its possible to use this similarity to fulfill the 'completion' intent)
        - relatedly, high-variation variables are useful to identify and apply as inputs in other workflows (like how pathogens are high-variation causing inputs, so theyre a useful cause to examine first when trying to explain other high-variation variables like conditions, before other causes, as the similarity in variability implies a possible adjacent connection between pathogens and conditions)
        - the common structure is 'similarity between (high-variation but not random) structures' which can identify more achievable/trivial but still useful sub-intents to fulfill (like 'predict the next word in a sentence' rather than 'predict all sentences given a general high-variation goal like "write an essay"') is in a useful range of variation to make prediction algorithms useful, and which can be useful as similar structures are more connectible than other structures, so 'finding similarities in the problem space' is useful as a problem-solving intent to fulfill other intents like 'connect similar structures'
        - similarly, knowing 'what structures are useful to build high-variation variables' is useful to apply as default inputs to test (generality, few requirements, types, etc as components/inputs that build high-variation complex systems)
        - relatedly, knowing other high-variation structures like 'embeddings' can identify probable structures like 'errors' in a system (such as 'prompt injections' which are usually embedding structures, like a 'character embedded in a story, an embedding which makes the character seem false, and not a person speaking sincerely' or other structures which can produce 'vacillation' structures between relevant dichotomies like certain/uncertain or true/false, such as 'pretending to (pretending to (pretend))', so that 'emergent' structures can be used to identify the net effect of the embedded structures), as most complex structures will be an adjacent structure (like a 'combination') of these high-variation structures (like embeddings, waves, rotations, similarities, etc)
        - similarly, useful structures like 'completion functions from a partial subset' and interim sub-intent connection structures (like 'clarity functions which connect a partial subset to a probable higher coverage-ratio subset with a clear implication of the full set') are useful to apply
        - the 'set of sequences of set completion functions (like similarity indexes, between each adjacent pair of completion states in the sequence)' is a useful structure to identify as an equivalent alternative to other 'useful interface queries' for 'connect' intents in solution automation workflows (similar to how the 'set of sequences of differences' is useful for 'filter' intents)
        - relatedly, identifying 'similarities/differences, paired with the intents they adjacently fulfill' (similarity-intent structures) is useful to identify ('similarities that make some structure combinable/stackable' or 'differences that make other differences obvious'), and the same goes for other useful structures ('function/cause/opportunity/perspective-intent')
        - similar to how adjacent combinations of high-variation structures make problems trivial to solve, common interaction/connection functions make problems trivial to solve, as they offer different versions of the same information (what is connected/what is similar), just like other info offers an alternative (what are the most common/powerful inputs, which offers a different version of the same info) just like 'maximal differences which are equivalent alternates in some way like having similar degrees of variability within some area of probable solutions' make problems trivial to solve ('similarities across differences' and 'differences based on similarities' being highly explanatory structures), so identifying connecting structures of these 'different versions of the same info' is useful to identify the other versions of the same info (as well as identifying 'differences that change/remove the info being connected' which should be avoided)
        - relatedly, identifying useful structures that are not adjacently connectible/combinable is useful to identify other functions that a neural network should support (structural non-combination functions like 'embed/merge/filter' which indicate connections between different states/intents of information other than just 'building a structure out of combinations of components' as well as more complex information functions like 'switch perspective', 'conceptualize', 'explain', 'organize', etc)

    - identify useful structures like solution metrics such as 'realism metrics' (structures that can be used to determine if some structure is real) which can be added to a solution metric set (realism metrics such as 'iteration' and errors like 'irrationality/illogic/unreasonableness/unintendedness' which occur in real systems, where an artificial system might over-reduce/optimize for some metric like uniqueness or minimum memory storage), where realism is useful for simulating real systems (such as representing systems of variables like in the 'regression' problem space, so adding 'realism' to a function makes it likelier to be a good representation of reality, which representation functions in general are supposed to be as well as being good representations of a data set, although for some intents that might be suboptimal like for 'compression, to optimize minimum memory storage')
        - these can apply a similar impact to a solution function for very different or similar reasons, such as how 'iteration' might produce 'non-linearity through iteration of embedded changes' or 'trivial differences around the copied structure (adding randomness, divergence into different functions if the differences are scaled by further iteration, and cascading/self-sustaining errors in the host system of the variables)', and similarly errors like 'irrationality (or deviation from reasonable intent)' can add either/both randomness and/or non-linearity to a function through the 'variation in differences' this error can adjacently create
        - similarly, 'diversity' is a metric of reality so it should be applied as a solution metric of the solution-finding method (does the solution-finding method handle diverse inputs and diverse input cases)
        - how did I arrive at this conclusion (what is the interface query to generate this)? I was thinking of important differences, like how reality is different from most graphs (related to the problem of creating a 'reality-generating graph') and one 'reason' is that 'repetition' is allowed in 'reality', and relatedly attributes like 'count' and processes like 'iteration' (function application count), which are not abstract concepts but which are useful to determine real vs. artificial structures, and then identified that real structures would show up in a 'representation of reality' or a 'realistic solution-finding method' (similar to how more complex solution functions tend to be more real (occur in real systems) than simple linear functions), 'representation' being related to the 'regression' problem

    - identify useful structures like 'opportunities to use workflows optimally', like the 'opportunity to identify new math structures' by identifying math structure variables and applying a simple iteration
        - this example uses a 'trial and error' workflow in an optimal way to identify useful structures by recombining variables like components of known math structure definitions, which is a huge information gain from a small work investment, which can be enhanced with output filters to gain more useful information like the most different resulting structures from known structures and from each other
        - 'maximal differences from equivalent alternates' and 'maximal differences from known structures' are useful filters to apply in a 'combination' structure in many cases, as they filter a set of different structures in many cases, except in a case like where all items in a set of equivalent alternates are known except one, and then they identify the same structure
        - the structures which can be used to fulfill this intent of 'identifying opportunities' include structures like 'complex problem spaces with few known useful (as in re-usable) structures' which indicates that there are likely other structures which are similarly useful which havent been identified yet, and identifying other structures which are different from other structures without having a reason for that difference (unlike other complex systems which have many useful known structures, the math system didnt have as many, which indicates an opportunity to correct that difference, as it seems like a false difference) and similarly identifying similarities which can be applied in a useful way like 'similarities across systems which make it possible to apply variables from one system in the other to find false differences and other errors to correct, where these false differences are adjacently connectible to useful structures like useful re-usable math structures'

    - identify useful structures like 'similarity sources in real systems' such as 'memorization/memory storage structures/functions' (like 'mnemonics') which are by definition a way to remember/store/retrieve/use structures by some similarity (like a pattern), similar to how 'compression functions' and 'abstractions' and 'usage contexts' (in which a function is used) are a useful way to store information about connected/relevant structures that could be used to derive the structures themselves based on just these connections
        - for example, usage contexts can help differentiate the meaning of a structure, such as 'two-dimensional', which can describe an infinite series or a structure with two dimensions or four sides like a plane or square, which are very different structures that can be described by that attribute, so including the usage context in which this description was selected can help filter the set of structures it could refer to
        - these 'similarity sources' are useful to identify new similarity metrics to use when 'finding similar/different structures'
        - the 'usage context-function' index is useful similar to how the 'insight-function' and 'intent-function' indexes are useful, by providing similar info through surrounding/adjacent info like 'info triggers/inputs' such as the reason to use a function and the context it is optimally used in
        - similarly, more specificity can add value (just like a mix of specificity/generality can add value as a problem-solving, difference-connecting structure), as specific examples of usages/intents/insights are useful, and possibly more useful than a network of insights, depending on the insight, such as how a given 'attribute/variable network' can have limited usefulness compared to one specific (pre-filtered) attribute/variable (such as 'balance of power') which could explain most variable interactions and which can be applied in a way so that it is more useful than the whole 'attribute network' to identify specifically in some cases, where the one useful specific attribute can connect the comparatively static attribute graph to other graphs, as its more abstract/conceptual and connected to other graphs than the whole 'attribute network' might be (the 'balance of power' attribute can reverse the limiting, static impact of identifying the attribute network, which could be just useful enough to make other useful structures falsely seem obsolete and reduce thinking into a set of simple structures/functions of those attributes, and expand the static network into a set of connections to other graphs (which is an example of 'reversing the trap' of specificity, by creating generality (its opposite) from it) - connected by balance, power, balance of power, concepts, opposites, alternates, and other related interface structures to this particularly useful structure of concepts, which can be used to derive the interface network and other useful structures with comparatively few changes), similar to how having 'information including the specific primary interface variables' is more useful than having 'information about some specific system' in some cases

    - identifying useful structures like 'overlaps between very different structures like perspectives/interfaces' which enable intents like 'identify structures that fulfill multiple priorities/solution metrics/intents which connect different systems'
        - for example, a structure like a 'word combination' could be a simple structure like a 'sequence' of 'objects (like words)' but it is also the unit of 'conceptual math (combining a concept with another concept)' if the words are abstract concepts (objects with many different definitions), however to call it 'conceptual math' is misleading if its not applied in a useful/relevant way like to identify insights, and if no other structures are ever applied than simple combinations/sequences of objects (like words)
        - the overlap of those systems in the 'word sequence' is useful to connect those systems (simple structures and conceptual operations), as it can identify other simple structures (like convolutions) that can be applied to the other system (like 'every possible interaction between a set of structures'), as well as being useful to differentiate those systems (some function that only fulfills simple structures like sequences is unlikely to ever be able to complete conceptual math operations except the simplest unit, which is not useful in every problem), which is useful to identify structures that should be applied in neural networks other than adjacent change combinations
        - identifying the structures with these overlaps that can occur in some form in every maximally different system/perspective/interface is likely to identify a useful problem-solving structure (like a structure that can be used as a filter/gauge/standard/ring/interface/system/perspective/concept)

    - identifying the important structures determining a solution in a particular problem space (like 'similarity' in the 'regression' problem space) which can be applied across problems
        - what useful structures (like function similarities/types, higher powers in functions, densities in data sets) have in common is that they store some information about some similarity type, which is useful for finding the solution function (the most similar structure to the data set that is also similar to some solution metrics like generality which have absolute abstract insights stored in them)
        - finding structures that are similar to these solution metrics (like 'finding functions using a solution-finding method, where abstract useful concepts like balance/power are applied in relevant structures like solution function metrics in that problem space') is a useful solution structure (the 'balanced powerful structure, balanced in specificity and powerful in storing information about similarity to the data set with fewer more powerful variables') to aim to connect to the problem input (data set) or variations of the input (like representation functions of the data set, subsets/densities of the data set, etc), as the solution also has to be similar to the problem input enough to retain the important info, just like it should be similar to the solution metrics (whether absolute/abstract/general solution metrics or specific solution metrics)
        - this applies the workflow 'find structures adjacent to the problem/solution (adjacent representation functions/subsets, absolute solution metrics) that can be connected instead of the source problem input/target solution output'

    - apply useful structures like 'function sets' to other structures (like 'interface structures including other function sets') to identify useful structures like 'usefulness thresholds/limits/directions'
        - structures applied to define useful high variation functions like 'describe' (map some 'input' structure to a structure like 'combinations' of structures from a 'set of known structures') and 'determine' (make some structure obviously implied and unique and therefore required, such as filling in the ratio of a circle required to imply the circle, and no other structure as obviously) are maximally different enough to be useful when applied to other interface structures to create useful differences like new variables (which is applying the outer layer of generated functions which includes the highest variation functions, to components of inner layers to create a new outer layer)
        - this is useful to identify bc its possible to make it obvious when more iterations are suboptimal by applying similar changes to existing changes, which is when its more optimal to use existing changes than the new changes created, which is useful for identifying when to stop specifying a function by adding more variables, as there are useful structures like 'thresholds' where re-using known simpler components is more useful than identifying and applying new more complex components, 'thresholds' which can be identified to find other 'thresholds' where iterations start to become less useful and related structures like limits more adjacently

    - identifying useful adjacent structures (like 'polynomial factors' as a combination of different structures like 'differently powered terms which can be multiplied to create solution structures like the complete polynomial') that have a similarity to some interface structure (like a 'component') so they can be applied to relevant workflows using that interface structure
        - identifying useful structures like 'factors of polynomials' as useful components for 'identifying similar functions' and 'changing one function into a similar function', a structure which can be found by applying a 'combination' to a 'variable and a constant' to identify a useful difference in the exponent of a variable, rather than only handling terms in isolation or the whole function at once, to identify useful function formats like 'overlapping areas created by offsetting terms having the same exponent', so that a function network that was a network of 'variable + constant terms (like x - 1)' could be another way to format the network to find useful combination sets/paths as well as finding an optimized network where all combinations/paths are adjacent and finding similar functions by similarities like patterns in their components using a function network that can be applied as a function similarity index

    - identifying useful structures like 'inputs to useful structures' which can be used to find other useful structures
        - for example, an 'error' can be a useful structure to identify variables, as 'errors' are 'differences from optimal states like solution states (as in opposites of/differences from positive states)' and are therefore useful in identifying variables, such as how errors (created from lack of control) in 'filling in a circle with a color' can result in useful structures like 'incompleteness', 'overlaps', 'opacity' (through connecting unfilled/filled sections with a gradient) and other useful variables of colors which are possible once the 'color' interface is applied to a 'structure' interface (like a shape)
        - the error results from a poorly controlled application of a function (like 'filling in a shape with a color'), which leads to a difference that is measurable and therefore makes some variable clearer, so 'removing functions that regulate applications of functions' is an input that can be directly applied to identify variables
        - this can be applied to derive another workflow, such as 'apply adjacent changes of every type/combination/structure to the problem to check if there are clear improvements/errors in some direction or other structure of change' which is a useful specification of the workflow 'apply changes to the problem space, which will be likelier than not to improve it, given that its a problem and changes to a negative structure are likely to produce a positive structure'
        - similarly, other workflows can be derived from this such as 'structures which are uncertain like "unrequired/unguaranteed structures" are likelier to be useful to solve some problem, as certain structures like "requirement structures" are likelier to have already been tried and found to not be solutions' (the solution is likely not extremely similar to requirements, or they would be used as a solution, its likelier than more different structures are required than extremely similar structures to requirements)

    - identifying the reason for useful structures and generating other useful structures based on that reason
        - for example, regarding useful structures like 'change a constant (like only one) into a variable (like some or every)' applied to structures like 'core interaction functions' (like reduce/connect) connecting problems/solutions, the query 'does every core interaction function also connect problems/solutions' results, with answers like 'worsen' the problem (so it gets more attention and other problem-solving resources, as attention is an input to problem-solving processes) and 'contain' the problem (find a structure that can wrap the problematic structure and prevent its problematic inputs/outputs from having an impact, as the inputs/outputs of problematic structures are likely to be related to why its problematic so handling those instead by isolating them is a problem-solving automation workflow)
        - the reason that other core verbs like 'worsen' and 'contain' are useful in problem-solving as core interaction functions of problems/solutions is that they change the problem structure in some way, which is necessarily useful for problem-solving, though some functions are likelier than others, and identifying the differences between these is useful ('worsen' only helps if there are available resources like attention and other inputs, whereas other functions are likelier to be less harmful such as 'connect/reduce')
        - similarly, 'copy' the problem (to another system) is likely to be useful, in cases like 'if the other system is different enough to change the problem in a useful way' or 'if the other system is a simplification of the original problem system so it makes the solution or the source of the problem clearer'
        - similarly, 'expand/amplify' the problem is likely to be useful in that it will make the limits of the problem clearer once its expanded or amplified until these limits are reached or implied
        - these are opposing functions of typical solution automation workflow functions, which usually seek to reduce the problem in some way or directly connect it to solution structures

    - identifying useful structures by whether they adjacently fulfill multiple useful intents (such as abstract structures which are adjacently varied to generate very different structures, or functions that can fulfill the opposite intent if their direction is changed)
        - the structure of 'opposite direction (reversible, which can expand the solution space, rather than just reduce it) filters' is useful for identifying different directions of maps that can be useful, such as a 'function to identify the original data set from a solution function', which can be used as a 'filter of solution functions' bc identifying 'data sets that are possible for different possible solution functions' identifies solution functions that are too general/specific in the set of extremely different (so as to be irrelevant) data sets that they map to, and similarly is useful for identifying offsetting variables and fulfilling different intents like 'reverse-engineer a data set from a representation function, to make sure the function retains enough info about the data set'
            - these filters are useful for filtering the set of possible data sets, which is useful to identify, just like identifying filters to filter the set of possible functions for a data set bc both intents allow identification of useful attributes of filters, like info retention (reversibility, specificity, etc), as well as useful applications of filters, like useful sequences of filters that optimize for some filter attribute like specificity
            - checking if the solution function applies to a 'extremely different system of variables which change/interact with the function in different ways is useful' to identify different reasons, change types, probabilities, and other structures associated with the functions, as well as identifying the probable correctness metrics of the function (like the 'longevity' of the solution function, in being robust to predictable changes in the host system)
            - similarly, once the function and filters and data set and host systems are connected with these maps, other structures are connectible, like 'host systems producing similar variable interaction functions that change in similar ways'
        - the structure of the 'average' function is less optimal bc it is more difficult to derive the original data set from this function, even if it optimizes an error metric like 'distance from the solution function', bc it loses the non-linearity/complexity of the original data set, rather than being curved, which is likelier to retain the original complexity of the data set, which can be used to 'filter out filters that over-reduce or over-expand information'

    - identifying useful structures to be connected in problem-solving, such as 'spectrum variables' that can be applied as useful structures like 'solution filters'
        - identifying useful structures like solution metrics such as 'spectrums like good/evil' which are useful to connect problem/solution structures to, for intents like 'find solutions that are likelier to be useful for good intents than otherwise', so arguably every workflow should be connected not just to problem/solution structures but also other useful structures like the most powerful reality spectrums such as good/evil, uncertainty/certainty, constant/variable, etc
        - solutions that are likelier to be useful for good intents fulfill many alternate solution metrics (to adjacently fulfill intents like 'identifying many ways to be good, to cover the highest ratio of users'), use many different variables/functions which are also interface structures so the solution can be used to teach useful structures and fulfill intents like 'making users smarter', etc

    - identifying powerful differences in workflows like 'abstract an insight and connect it to problem/solution structures' like 'core cross-interface functions'
        - identifying that 'any combination of primary abstract concepts (power, balance, similarity) can be combined to produce useful functions that can connect most structures' is not a result of the workflow 'abstract an insight and connect it to problem/solution structures', but rather other workflows like 'is a constant (like only specific abstract concepts that are useful for some intent) a requirement, or can it be changed to a variable or removed without changing functionality/usefulness of some structures (to change it to some or all)' (answering the question 'is it "only" these concepts, or "all" concepts, or "some structure of concepts" like differently-sized subsets of all abstract concepts, which can be adjacently transformed into a useful structure that can fulfill some problem-solving intents')
        - 'abstract an insight' (concept-information interface) vs. 'change a constant into a variable' (change-function interface) vs. 'connect structures to problem/solution structures' (structure-information interface) are useful core cross-interface functions, which can be applied as components/inputs/alternate examples to find other workflows
        - the reason these are useful is that they 'connect maximally different systems', which is likely to be a useful map for solving other problems as it will connect many different variables
        - identifying useful structures to combine these components with is similarly useful, such as identifying that 'equivalent alternates' are a useful input to the 'change a constant (only one) into a variable (every)', which is more useful when applied to one item in a set of equivalent alternates

    - identifying different useful structures like different input/output formats that can be mapped to different problem/solution structures
        - other function/data set formats can be useful, such as a 'probability of an adjacent point being in a specific adjacent dense area, given some input point' (a format of 'points and related dense areas to those points', applied as a predictor of adjacent points) being arguably more useful than other formats in some cases
        - this is similar to how 'knowing that a particular word is in a sentence at all' or 'knowing whether "words that frequently change meaning of surrounding words/phrases" are in a sentence' may be a more powerful predictor than other structures like 'sequences' ('sets' and 'probabilities of words in sets' are alternate useful structures of 'sequences', similar to how 'word patterns like iambic pentameter' are useful to identify, and 'similarities between word interaction functions (like core verbs such as "like" as a proxy for words like "similarity")' are more useful to identify than sequences), and 'differences within a sentence' and 'differences to other/standard sentences' and 'differences from core similarizing (the most standardized, common) words' and 'inputs to differentiating words/sentences' and 'common components of the most different/useful sentences' and 'common sentence interaction structures like embeddings (like a specification such as an example of a point in a previous sentence)' and representations (the 'summarizing sentence of other prior/subsequent sentences') and 'very different sentences with the same meanings (similarities in differences)' and 'sentences around which other sentences gravitate or vary' and 'over-used sentences that would be better phrased differently but which are convenient like excuses or slogans/idioms' or 'sentences that standardize sentences that follow (uniting differences to equate other sentences that follow)' as well as the opposing 'controversial sentences which polarize sentences that follow' and 'sentences which frequently surround facts'

    - identify structures that are common/compelling with uncertainty in the reason for that useful attribute and connect them to useful structures to resolve that uncertainty
        - why do we find symmetries compelling? bc of the 'variation of their contradiction' ('similarity in a difference') and their 'alignment' (the 'similarity in the changes', present in the similarity of their common base/origin/center and the similarity in the structure of changes in relation to that base), as 'opposing' structures and 'embedded' structures (an 'opposing/different structure in the same structure') and different structures ('similarity in a difference') and paradoxes ('similarity in a difference') are useful structures to identify as they are commonly explanatory of other changes like more complex changes once they are iterated/combined/otherwise structured and useful for intents like 'differentiation/filtering' and 'identifying similarities across different systems to connect systems/differences' and 'identify interfaces (similarities that connect all systems)'
            - what makes symmetries more compelling? structures like 'balance', 'intersectivity', 'orthogonality' make symmetries even more compelling
        - similarly, we find metaphors like 'yin and yang' compelling, as it indicates other useful structures like a pair of monodromies ('structures such as spaces having a point representing one impossibility') fulfilling some useful structure (like 'spirals/fractals') which together adjacently create other useful structures ('circles') and which are similar/complementary/coordinating in some ways (structure, alignment in relation to axes) and different/contradictory/opposing in other ways (color, rotation degree in relation to each other) and contain a set of impossibilities which contradict each other in the same structure (a paradox), and a similarity to other useful structures like symbols such as the 'symbol for infinity' (achievable by a rotation to create an overlap, to connect it to other useful structures like the symbol of infinity represents such as a 'set of loops/angles that would necessarily develop and/or would create momentum in real systems to sustain itself and refer to itself by intersecting with itself, indicating self-sustaining/infinite change around a common base which is the intersection point')
        - similarly a wave ('connection between extreme differences') has interface structures embedded in it (common structures like 'circles' as well as change limits, embedded changes like change rate/type changes, etc)
        - similarly a 'peace' symbol has useful structures like 'connections between similar but different directions into one unified direction' and in reverse, 'isolation of a variable into component variables' and a 'variable connected to a constant' and 'lines from a common origin applied as connectible to a circle through being applied as radii of a circle' among other structures
        - similarly a 'flywheel' symbol is a useful variant of a wheel that can be used in its place
        - these are useful structures that have other useful structures embedded/intersecting/otherwise connected to them, which can be applied as useful base structures that are useful for 'identifying other useful structures', "identifying ways that very different structures can interact without violating definitions (which is useful for intents like 'coexistence')", 'identifying maximally different stable structures', 'identifying structures common to useful structures', etc

    - identify insights by applying other insights/interface structures and identify variables of insights to find new insights to apply as inputs to other workflows involving insights
        - identifying errors in perspectives like 'long-term thinking' (such as the 'lack of short-term/immediate feedback') and 'short-term thinking' (such as the 'lack of a solution for scaled/long-term/emergent effects') is possible to achieve by applying structures like 'generally useful structures' (such as known optimal structures like 'feedback' which would ideally be integrated in every solution) as well as 'advantages of opposite structures' ('what is the benefit of short-term thinking and is that missing from long-term thinking') by applying insights (like 'any one solution is rarely the only solution' and 'its useful to identify/apply balance points in spectrum variables like long/short-term thinking') and known error structures (like 'avoid over-prioritization errors caused by applying few structures such as only one structure')
        - the variables of these insights include interface structures ('insights frequently use interface structures, which are known to capture high variation, since interfaces by definition support other variation and allow variation to develop as they are a base structure, and insights are likelier to evolve from structures of high variation-capturing variables, whose interactions are less likely to already be known but are likely to be powerful interactions and therefore important/useful') such as 'multiple' (math), 'alternate' (change), 'spectrum' (structure), 'balance' (concept), which occur across interfaces (from which you can also derive that 'insights often use/connect interface structures from multiple interfaces, as those are high-variation structures which are powerful/useful to connect')

    - applying insights like 'any one solution is rarely the only solution (apply multiple to useful structures like solutions and useful structures)' to structures where useful (where errors corrected by the insight are possible and relevant and where the insight structures dont contradict solution metrics like accuracy)
        - for example, identifying structures of errors that can be inputs to a loss function in neural networks is useful for identifying error structures like 'overlapping error areas (of multiple error types)', such as how 'adjacent input errors' can be used to identify a 'directional error' (an error of an incorrect direction, either the first error that indicates it or is measurable as indicating it or the error that initiates the error direction by being different from the correct direction), to apply an ensemble loss function of various loss functions or structures of loss functions (like combinations of loss functions), as knowing 'similar minimums across loss functions' is useful for finding 'absolute/compromise errors across loss functions'
        - 'a type of difference that is not an error' is another useful structure to identify, such as an error that probably wont push a function beyond a range that would classify it as a slight generalization/specification of a data set average/regression/representation line
        - it is useful to keep multiple loss functions separate bc adding them removes the information of their extremes/averages and its useful to know which types of errors are optimized for at a certain point
        - another example is how 'over-prioritization errors' are likelier the fewer the priorities are (even generally good metrics like 'independence' can be applied to negative effect, such as how 'apathy' is a result of 'extreme independence (leading to lack of caring)'), where multiple priorities are likelier to offset the possible over-prioritization errors that are likelier with fewer priorities, as more priorities are likelier to be able to check each other's power and errors

    - identify useful structures like useful function/structure sequences to apply for some problem-solving intent like 'identify useful similarities/differences to apply to create useful structures'
        - for example, 'if a condition has an output and another condition has the same output, one condition could cause the other' (or 'alternate inputs (having the same output) could cause each other') is a rule that is possible to infer and apply as a rule to find/describe possible variable interactions, given the useful identification of the uncertainty of the causal connection between the alternate inputs and their relative position (either could be causal of the other or there could be no causal connection), which is useful to identify as this uncertainty allows for a possible connection between possibly previously unknown connections (alternate inputs of the same output), which might have a useful difference in differentiating the position of the alternate inputs (as in positioning one before the other, causally), and given that there is no requirement that either condition not cause the other, thereby allowing that uncertainty to exist and the possibility of a causal connection between conditions to be identified
            - these are variants allowed by the known structures, which is not adjacent, as in its not obvious to position one condition before the other in a straight line before the output, indicating a guaranteed causal relationship, between alternate inputs
            - some graphs will position alternate inputs at the same/different distances from the output, but without differentiating information, they would be at the same distance from the output by default in a correct graph that doesnt add info (such as by differentiating the distances) that isnt available/known
            - the uncertainty develops bc the position of the alternate inputs can change while still being an input to the output (still being 'before' the output) and the alignment of their angles to/distances from the output implies different connections to the output and to the other input, meaning there is a possibly valuable difference to apply by 'aligning the angle of alternate inputs to the output, but keeping position different (rather than overlapping at the same point)', so that in the resulting structure, one alternate input is causative of the other (at which point it could still be an 'alternate' input, as the whole causal trajectory isnt necessarily required to traverse)
            - this is applying a (difference in a) difference/similarity (the changing of the alignment/equivalence of the angle/distance between the alternate inputs and the output) and a (similarity in a) difference (the maintenance of a difference in position, to differentiate the alternate inputs as not equivalent) in a combination structure that identifies a possible equivalent alternate structure of a structure (like 'alternate inputs of the same output, whose causal connection is possible but unknown'), where info that is different and should remain different is held constant (the difference in position of alternate inputs is preserved) and a unrequired constant (the similarity in angle/distance from the output) is varied to be different in distance or similar in angle, in a way that doesnt violate the requirements/definitions of the structures
            - this is applying trivial functions (combinations/applications/embeddings) of similarities/differences applied to connections (equivalences, alignments, similarities, positions) within the limits of requirements/definitions, which is a useful default function to apply

    - identify variables of useful graphs to combine/mix/otherwise apply as components of an integrated graph that makes other useful structures trivial to identify
        - given the different graphs available to graph useful structures like interface queries and workflows, such as the default 'functions/lines connecting structures/points' diagram, identify the usefulness of different graphs
        - different graphs are useful to identify different structures adjacently
        - applying some known structure as a similarity in a graph can help derive missing alternate structures
        - an 'iteration' graph can differentiate 'iterated structures (like embedding an embedding in an embedding or pretending to pretending to pretend)' and possibly also the similarities that these iterated structures have in common (line sequences vacillating between opposing/contradictory structures like container/component or truth/falsehood)
            - this graph is interesting because highly scaled iteration can produce complex structures (similar to how mixes of these structures can), such as a 'nested embedding that contains/stores/compresses the host system in which it is embedded', or an 'application of an application of an application that solves the problem of which structures to apply' (self-referential structures, self-containing structures and inverses of traps, where information is derived from approximately zero information, such as the 'minimal information of a unit structure', which is iterated to create the final structure)
            - this graph could be formatted as a set of line sequences connecting opposites like truth/falsehood, or a set of nodes which are also system layer diagrams to identify patterns in the embedded iterations of each structure
        - for example, identify a trivial graph where interface queries/solution automation workflows or other useful structures have a structure (similarity) in common, which is applied as a constant (the similarity is guaranteed to be a similar structure in the graph), like the following, in order to identify missing workflows or other useful structures by applying a trivial transform (such as 'rotate the structures around some center of the similarity, like a common starting point')
            - a 'line of function steps' or a 'wave/cycle between (sequences/lines representing functions)', if formatted as a set of lines connecting nodes, so that different workflows would be lines in different directions from an origin, so that the other workflows can be calculated by determining lines in the areas in between known workflows
            - a 'query across several different layers (representing different function sets)' in a system layer diagram (a similarity which would reveal that objects from different function sets frequently interact in useful structures like workflows)
            - finding a graph where similar structures are guaranteed to have some similarity (possibly by trial and error or another dumb algorithm) is useful for deriving other structures of that type
            - as another example, identifying an error graph where 'overlapping error points/areas (having multiple errors)' are visible is useful for identifying points/areas with one error (compounding/additive errors and their points/areas of intersection/overlap being useful to identify as another error structure that is complementary in usefulness to a 'minimum of a loss function')
        - these graphs make these similarities trivial to identify, by applying some known/constant original similarity structure (like a layer in a diagram referring to functions from the same function set), around which the input structures graphed in the graph (the workflows which can be represented as line sequences connecting functions/structures across the levels) are allowed to vary (meaning 'a workflow isnt restricted to one function set/layer'), then finding the similarities in those variances from the known/constant original similarity structures

    - identifying useful structures (like 'concepts that can store/connect very different structures') is useful for intents like 'connecting maximally different structures' and 'identify function types', which are useful for problem-solving intents like 'filter a solution space'
       - identifying 'uniqueness' which is connected to 'units' which are useful as 'components' for 'combine' intents as well as being connected to 'useful equivalences' (like the 'equivalence' of a single function that is the best representation of a data set by being equal to it in some way, which may be unique in its equivalence in that way to the data set) and which is powerful in that it identifies structures which may be new stable structures (unique structures may be new, as in 'different from previous existing structures'), which is useful since 'uniqueness' captures very different structures like 'unit components' and 'filters' in one concept
       - similarly, identifying 'similarity' as the important structure connecting a 'regression function' and a 'data set', rather than finding the 'average' (identifying that 'similarity' is important in its connectivity to the concept of 'average') is useful to identify as it allows other formats like similarities/differences to be applied rather than default formats like 'numerical values/addition/multiplication' and connect those other default formats with other useful formats like conceptual formats which are useful to identify which equivalences/similarities/differences should exist in which positions in a problem space
       - similarly, 'randomness' can represent 'equivalence' (in its probabilities) and also 'extreme/maximal difference' (in the set of possibilities allowed), uniting very different structures in the same concept

    - identify useful structures (like 'information-intent connections' or 'intent-output connections') that can be applied as an alternate to solution metrics (like concepts such as 'balance') and the full set of structures that fulfill these to apply as components/base solutions
        - for example, for intents like 'acquire a product', its useful to know information like 'impact of product/purchase on economy', which creates a more optimal 'balanced' structure (of an intent and a counterintent, or 'reasons not to fulfill the intent') that can be applied in place of generally useful conceptual solution metrics like 'balance' bc its an implementation of the concept in another system
        - this can be abstracted to include other solution metrics in every solution, meaning a workflow to first 'find all the structures (like fairness, or a point-counterpoint structure) in specific structures (like a particular position in a system, like 'on either side of a purchase (before/after or purchase/instead of the purchase) in an economic system', answering the question of 'where should the optimal structure (balance between information/intent) be in the economic system (around the purchase)' and 'why' (bc given the insight 'think globally, act locally' and useful structures like 'common' structures, its one of the most common 'acts', and therefore also an important scaling structure that is very powerful as a result of this scalability and commonness, and 'powerful structures should be balanced as they are meaningful and meaning is positively connected to balance (they change in the same direction in most ways)')) that implement useful conceptual solution metrics (like balance and its connection to meaning, as in "balance in the position of purchases between global/local intents can allow meaning in a system")' and then apply them as base solutions to apply changes to or solution components to combine to find solutions for a specific problem
        - here Im once again applying the workflow 'find an insight, abstract it, and find a way to connect it to problem/solution structures, then differentiate it from other workflows if it is equivalent, to find a new solution automation workflow' (youll notice Im applying this with other workflows to first find the insight, in which I was thinking about how to prevent errors, such as 'what information stops people from fulfilling an intent', like a 'negative purchase that harms the world', and how would you frame that structure (as a concept like 'balance' in structures such as a 'fair or good decision' which is related to other important concepts like 'meaning'), and similarly was thinking about other insights like 'think globally, act locally' that Ive heard my whole life which are embedded in my subconscious but which I also frequently think about consciously, as its useful to be able to derive insights in many ways, so thinking about the same powerful structures and how to connect/change them in different ways is useful, as those connections/changes will likely be similarly powerful and might also be reusable in explaining other useful connections)
        - connecting queries (and components such as concepts like 'balance') to 'meaning' (the 'ultimate impact in relation to other meaningful structures') is so important that its why I frame that as the 'host interface' on which other interfaces can exist and where interface queries can be run and where they should start/end, bc interfaces/queries should all be connected to meaning and that should be the focus/intent of the queries (every interface query should end with a connection back to meaning, and every workflow would ideally connect to meaning in some way, if not directly then indirectly by connecting to problem/solution structures)

    - applying different problem formats (like 'differentiate/oppose a negative structure') is useful to find useful structures like inputs to other problem format workflows (like 'filter a solution set')
        - for example, the 'type' filter is used to filter 'problem/solution types' ('pathogen types', 'drug types', 'drug component types') which means the errors of other examples of that type can be predicted more accurately once the type is known and an example is tested
            - similarly, other filters are useful to filter possible drugs, like finding 'matching offsetting differences' that should be corrected/differentiated (such as 'differences from pathogens', 'differences from other solutions/drugs', 'differences from existing system components', 'differences from pathogen or condition side effects') which are 'negative' so that finding 'opposing (positive) differences to that negative' is useful (like 'matching opposing structures of a side-effect receptor or pathogenic component')
        - other problems arent by default formatted as a 'find opposing/offsetting differences to correct a negative difference' format, but it is a problem format that problem types can be standardized to, since 'differentiate' is a default interaction function like 'connect' or 'reduce'
        - for example, in the 'regression' problem space, the 'negative' structures to oppose/differentiate are structures like 'randomness in the solution space (a solution space area that is uncertain)', which its useful to find differences from (like 'different solution bases or types to apply as base solutions to apply changes to')
        - finding the connections between problem formats ('differentiate/oppose' and 'filter') is useful to identify so they can be converted to each other ('differences are useful as an input to filters since they segment the solution space', and 'applying differences to a negative structure' is related to filters which 'apply differences to find reduced/unique structures'), which is also useful to identify, as it adjacently identifies structures that are useful in one problem format ('negative structures to differentiate from') which can be applied in other problem formats ('randomness in the solution space (which should be differentiated from, with opposing structures like maximal differences to filter the solution set)')

    - identifying useful structures like variants and variables of workflows as well as interim workflows in between core workflows which are still functional solution automation workflows
        - for example, 'change a base solution' has a variant of 'combine subsets of existing solutions', which is not a trivial subtype variant of the original workflow, but still has structures in common like 'existing/base solutions', and is still a solution automation workflow in that its likelier than randomness to have value in some cases/problem formats, and can be generated by applying a workflow using the 'build' function to the original workflow ('build a solution out of solution components')
            - similarly, the 'find' function can be applied to the original workflow, to generate workflows like 'change a base solution (for the "find a filter" problem, to find more optimal filters to be used in solving the original problem)'
            - the 'change/apply' function can be applied to the original workflow, to generate workflows like 'change a base solution (to find base solutions or changes to base solutions that are more optimal)' or applied to other workflows using the 'change' function like 'change problem/solution structures until they are adjacently connectible' ('change a base solution until its more similar to the problem inputs, and apply it as an input to other workflows')
        - these mixed interim workflows are different from core workflows which are likelier to be adjacent to existing formats and likelier to be useful in some cases
        - given the generality/coverage of these functions, there are many ways they can interact, so they can be combined in many ways to adjacently connect new structures
        - they can be used to generate other workflows with simple structures like embeddings/combinations and matching those embeddings/combinations to problem-solving structures like intents

    - identifying useful structures like specific types of similarities (like 'patterns' or 'internal function input/output or sequential similarities') that are useful for specific useful intents like 'filter the remaining solution set, from a subset'
        - for example, 'internal function input/output similarities' (similarity of a subset with another subset of a function, such as 'rotate the subset to get the other subset') are useful for intents like 'filter the solution set', bc if these similarities exist, trivial changes based on those similarities can be applied to generate the rest of the function rather than checking the full set of solutions, just like 'function-function similarities' are useful for 'finding similar functions that can be applied as alternative solutions to check' and 'finding maximally different functions (within some degree of difference governed by the similarity)'
        - some types of similarities are useful in contexts that are counterintuitive, such as how a 'pattern' (such as a 'sequence of similarities') can be useful in the 'regression' problem space, which is counterintuitive bc the rest of the function cant always be filtered by any given local subset of the function, although if there is a pattern like a wave, that is useful to identify by checking for that similarity type (sequential pattern) bc that type is possible in the definition of a function and does fulfill the intent of 'filter the rest of the function (the remaining solution set), from a subset' if there is a sequential pattern or other detectable pattern in change types/rates/other change structures
        - patterns (like 'repeating sequence') are useful to identify in that the repetition allows them to fill out the rest of the function without checking a large remaining solution set
        - the 'repetition', the 'trivial changes' (like 'rotations'), and other similarity types (like 'similar functions having some intersections/subsets in common') structures are useful for intents like 'filter', in a counterintuitive way (once you identify the intent of 'find the rest of the function from a subset', it is more intuitive/obvious, and once you identify the possibility of functions having other formats than a standard input for a regression problem, like functions with formats like 'sequential patterns', these other structures like repetitions become more obviously useful), whereas structures that are obviously useful for 'filter' intents are structures like 'maximal differences in functions' or 'randomly sampled subsets' that can be applied as filters or function approximators, similar to how 'approximation' is a more useful intent related to 'filtering solution sets' than other intents
        - basically any interface structure in the regression problem space (interface structures of 'data sets/functions') that creates or connects adjacently to other information (making derivation of that information trivial) can be applied in a similar way, to 'filter the solution sets'

    - identifying useful structures like insights such as 'if any abstract concepts can be used to solve a problem, its likely that other subsets of abstract concepts can be used to solve the same problem, given that abstract concepts cover reality in a structure like a field (or a quadrant of euclidean space or wedge of a circle, which can be rotated to cover the other sections)'
        - for example, different subsets of concepts like the following can be used to solve 'regression' problems, which are connectible on the concept interface but are different enough for this similarity in functionality to be an insight rather than obvious
            - 'power' (through more important or differentiating power of variables)
            - 'balance' (through its adjacency to core intents like 'comparison' useful for intents like 'filter solution sets' and relation to the abstraction/specificity dichotomy which is particularly relevant as well as its relation to structures like 'interim' which is useful for finding 'maximally different' structures like 'different base solutions to apply changes to')
            - 'specificity' (as a primary interface variable of 'abstraction')
            - 'certainty' (as a foundation for applying changes to known variable interactions to find the unknowns) 
        - similarly, applying this to other interfaces (if any physics structures can be used to solve a problem, other physics structures can be used to solve that problem, given these structures power/absoluteness/differentiability and connectivity on that interface) identifies physics structures such as 'vortexes' on other interfaces like 'information' such as 'info vortexes' (like a variable that, when added, creates a 'info vortex' that makes all certainties seem uncertain or less certain), which are related to 'interface queries' by an 'opposite' structure, as they add information using trivial changes
        - more powerful/interactive/absolute/core/interface/complex structures on an interface are likelier to be adjacently combinable in this way (as structures that contain/connect to enough differences that they can be used to solve any problem, so they act like alternates)
        - why is it true that equivalent alternates develop? bc powerful/useful structures are likely to be required for many intents which they cant all fulfill in real systems simultaneously, so backups are useful, and similarly most systems have enough variables that differences are stable and can compound in different directions, and exact copies are less likely just like extremely different structures are less likely, so in between there are interim similar but different structures that occur if enough variation is possible, and powerful structures are likely to have alternate states that they can form at rest or in different contexts, so 'variations of an original structure' are likely to create equivalent alternates at some point in their development, and 'trivial differences applied to a base structure' is a useful structure as it can frequently fulfill very different intents with these trivial differences, which are another way that equivalent alternates can develop
        - primary concepts can be described with interface interactions (like 'power' describes important input/input interactions like an 'input that can replace other inputs' or input/output interactions like 'low input/high output', and balance describes a 'specific state of "equivalence" in a connection between important structures like compared structures') that can be used to identify other primary concepts that are useful in some way ('equivalence' is useful for a 'compare' intent, and 'power' is a definition that can be adjacently used to describe 'usefulness' or 'importance' or 'relevance' or 'meaning' or 'efficiency')

    - identifying useful structures like 'false similarities' applied to structures like 'definitions' is useful for intents like 'identify/filter', given the usefulness of 'definitions' for those intents
        - for example, finding all the ways a structure can be false (an embedding that makes some identifying attribute false (such as in a 'false copy' or a 'false context') but still be included as a component, so it will still seem true if only that component is checked, a falsely similar substitution of some identifying attribute, a false subset of identifying attributes that is used in an identifying index) is useful for identifying the real/false structures
        - this is useful bc real systems often have false structures, as false structures (imperfectly complying with some definition) are cheaper than true structures (perfectly complying with some definition), to the point where they can be assumed to be more common or the default

    - identifying useful structures like insights such as 'similarities can be used as filters' by applying useful structures like differences and checking for similarities in those differences, to find useful intents like 'find function similarities (and similarity types)' and specific structures to implement these intents
        - for example, a function can be mapped to a set of its identifying structures (x^2 has an identifying structure of a parabola (so the 'probability of a function being x^2 (or other exponents) given that there is a parabola at all, is higher than not'), bc these structures and the functions have a similarity in common (the set of possible functions can be filtered by knowing a set of structures that are associated with a function)
        - this similarity can be applied as a map (a set of structures to a set of functions having those structures)
        - this similarity between function structures and functions can also be applied as a network of term types (or 'significant example (such as defining example or differentiating example)' terms or term 'units') and their neutralizing/magnifying terms (the set of functions you would need to apply to x^2 to erode its identifying structures, even in functions with an x^2 term, such as offsetting 'fraction coefficients', 'inverses', and 'higher powers')
        - other similarities exist between function sets, such as the mapping that adds useful attributes like non-linearity to a function, which can be used to 'connect function sets' (or convert a linear function into a non-linear function with some similarities like general structure, intersection points, average, etc), which is a useful if indirect problem-solving intent in the 'regression' problem space
        - this is in contrast to other attribute sets like 'generality' and 'linearity', which dont have a 'default similarity (like a higher probability of some similarity) to use as an identifying filter' (there are 'general' functions which are either 'non-linear' or 'linear', with no clear differentiation like a 'more probable attribute value', although 'simplicity' of a data set can connect 'generality' and 'linearity')
        - this means the info is retained across this similarity transform/mapping (it is symmetric in some defining structure)

    - identifying useful attributes like 'intersectivity' can be used to adjacently identify other useful structures
        - identifying that 'systems of linear equations' can help identify 'intersectivity' is useful to identify 'systems of linear equations' as a useful input to finding similarities (such as intersections) between functions
        - identifying structures that fulfill the intent of 'connecting subsets of a function' is a way to identify 'intersectivity' as a useful structure
            - for example, identifying that simple transformations like rotations/shifts/scales of one subset can identify another subset (in some cases), and similarly simple direct connections can be used to connect two subsets, and similarly components of one subset can be used to identify another subset, and similarly intersections of two different subsets can be used to connect the subsets
        - identifying that 'intersectivity' is particularly important in differentiating functions that are locally similar or have similar averages
        - identifying the 'intersectivity' of functions is a similarity metric that can be used to differentiate them
        - identifying 'intersectivity' is useful for finding similar functions (such as functions that are 'similar to averages', 'similar to data sets', 'similar to maximally different base functions', 'similar to local function subsets', etc)
        - identifying the 'intersectivity' of tangents as a way of finding 'function limits'
        - identifying functions that are highly 'intersective' is useful for finding a reduced set of functions to check when some of the function is already identified
        - identifying maximal differences based on 'intersectivity' (like the most different structures that can have a particular line segment in common, like a parabola/circle) is useful to find functions to filter once an intersective subset is known
        - relatedly, identifying the 'reason for similarities/differences' is useful, as in identifying 'coincidental equivalences' (function sets may have a subset in common but are still so different they may as well not have anything in common, such as a 'required equivalence (such as all terms with a coefficient of 1 which are a power of x must cross y = 1) that doesnt summarize the rest of either function or connect adjacently to the reason for the intersectivity or help differentiate the functions having that equivalence') which are useful to differentiate from other equivalences that are more significant (functions that have a subset in common and its bc theyre an adjacent transform of each other)
        - identifying 'interface structures like reasons' of errors like 'insignificant and/or false intersections' (like the common intersections of a straight line with a wave, in which a 'higher power is missing' and the 'intervals of the wave align with the subset tested for equivalence with the straight line') is similarly useful

    - deriving inference rules like 'what is definitely not contained in a function (a set of changes applied to inputs to generate outputs)' (its inputs/outputs are almost definitely not bc of the existence of the function which implies a 'lack of those structures, indicating the existence of a structure to create those structures', unless its the defined exception that requires it to contain its inputs like an identity function) is possible by applying changes to certainty structures (like 'definitions') and checking if the limits of those certainty structures are violated
        - the definition of a function indicates a difference between a 'set of changes' and 'inputs', otherwise they would be connected with equivalence structures such as 'like/is' or the definition would likely use the same term rather than different terms
            - given this identified difference in the definition, a query like 'what is different from a function' is trivial to answer (the inputs/outputs)
            - deriving questions that are also trivial to answer is similarly possible by applying trivial changes 
                - questions like 'what different functions are equal/similar to this functions' with answers like 'functions with similar inputs/outputs'
                - questions like 'what causes function inputs' with answers like 'functions with the inputs as outputs'
                - questions like 'what requirements does a function have' with answers like 'function inputs' and 'a system where function inputs can exist'
            - deriving questions that are not trivial to answer is similarly possible by applying maximally different changes like interface structures (which already capture high variation, as they are defined as being the more useful structures) such as 'changes allowed within a definition but which are maximally different within that definition (like allowed contradictions of sub-types within a type)'
                - 'whats a previously undefined statistics structure related to a function' with answers like 'interaction levels where this function is adjacent/default' or 'variable embedding interaction functions which can predict most variable interactions' or 'different formats of a function like filter sets or indexes/mappings' or 'densities paired with associated tensors whose trivial rotations (within a limit) or intersections cover most of the data set or most of its ensemble averages across algorithms'

    - identify useful sets of implication/confirmation or implication/contradiction structures (like a point that when paired with a corner implies a shape, a tensor, or an angle sequence, and when that set is paired with another corner, a shape is more certain if the corner has an attribute like 'concavity' and implies another corner that can be derived and checked without traversing the entirety of the third line, just like the full set of remaining corners of a square can be derived/checked once two sides are known, paired structures that confirm/determine/contradict the implications of the initial structure)
        - this is related to applying 'maximally different variables within a solution set/possibility space, once a filter is applied'
            - this structure of 'increasingly incrementally more specific filters (diminishing returns of each subsequent filter)' is only optimal in cases where the initial filter is the primary differentiator (like a general type variable) and every filter that follows is more specific (like a sub-type variable of that initial type variable), which is unlikely to occur by default without applying organization structures to force it to have that pattern, as variables would need to be sorted to make that filter pattern relevant
        - similarly, identifying non-adjacent structures (like an 'allowed sub-type definition contradiction (like a straight line segment in an otherwise curved function, which is allowed but violates a function type definition of a non-linear function, which is allowed in the definition of a continuous function but not in the definition of a curved function, which are associated but not required to be equal, and is also improbable)') is trivial just like deriving adjacent structures is trivial (like a 'continuation/repetition/iteration of a local pattern'), and both are useful to identify and apply as structures to check/filter
        - structures of volatility are similarly useful to identify in a problem space (such as 'non-repeating structure inputs' and a 'high number of variables' and a 'degree of difference from randomness')
        - structures (like 'allowed sub-type definition contradiction within a type definition' which are similar to a 'paradox' structure, which is a maximally different interface structure) that differ from simpler structures ('iterations/extensions/rotations/other trivial changes applied to a local pattern') are possible interface structures to derive/apply as defaults for intents involving maximally different structures (which are common in problem-solving)

    - identifying useful structures (like 'similarizing differences') useful structures (like 'network hubs') by applying known useful structures (like the 'commonness' value of the 'frequency' variable to derive important interface structures like 'probability')
        - a 'language network' used to find the 'most common words/nodes' would trivially identify structures like 'network hubs' as useful, the reason for this usefulness being the fact that the difference between 'network hubs' and other structures is that 'number of connections' (which act like a similarity between many different structures)
        - identifying similarly useful structures is a matter of identifying/deriving variables, applying these differences, and identifying new emergent differences in core structures like networks, as the primary differentiating variables of core structures like networks are likely to be useful for other intents (applying a workflow of 'finding useful structures' and 'finding intents to use these structures by some similarity in intent once useful structures are known, given their high probability of usefulness and similarity to some useful set of intents, like how a "network hub" can be useful as an input to an average/density/similarity-finding function')
        - similarly, identifying useful formats of info structures (such as formats of 'filters' like 'twisted light' as 'different paths to connect info on different sides of useful positions/structures to connect') is a way of formatting problems in a way that can make use of other system rules to identify 'maximally differentiating variables' of useful structures like filters (the problem becomes 'find a useful set (like the minimal set) of connections/angles that can connect the two structures on either side (optionally using interaction functions of light, like how its preserved across various filters)')

    - identifying useful structures like 'filters' of problem structures like 'solution spaces'
        - for example, when solving the 'find a drug (input) that corrects/changes a condition' problem, the set of possible solutions can be filtered by removing the 'known common inputs', as those are likely to be similarly distributed in people with the condition as the general population (known common supplements like turmeric can be removed as a possible solution, as its unlikely to be the solution, given that its a common input in general and is also likely to be a common input in the subset of the population with the condition)
        - 'filter out known common inputs' can be applied as a filter to find 'uncommon (uncommon in the sense of being new/rare/unnatural) substances'
        - this is a way of deriving info (which substances to ignore/filter out) without having info (info about the specific mechanism of all substances or the mechanism of the condition), using other more available relevant info ('what inputs are common' and 'whether the substance is likely to be similar/different from the common inputs (different)')
        - the info of 'what inputs are common' can be made relevant to this problem by applying changes ('similar/different' applied to 'common inputs') and checking if those changes are useful (is a 'similar substance to a common input' or a 'difference substance to a common input' likelier to be useful for the condition), and given that workflows/rules such as 'change existing suboptimal base solutions to be more optimal for a problem' and 'if a problem exists, it is unlikely to be solved by adjacent/existing/available inputs' are known, and given that rules of probability/reality are known like 'small input changes are unlikely to produce large output changes (volatility isnt a default structure without other attributes like independence, as very independent non-interactive structures are likelier to be extremely different and therefore likelier to produce attributes of randomness and related attributes like volatility)', and similarly other rules like 'if an adjacent input was the solution, it would likely already be known' are known or easily derived, and given that specifically 'common inputs' have already been tried as a solution indirectly and can be determined to not definitely be solutions, these rules can be connected and applied to determine that the 'common inputs' should be differentiated to become a solution ('differences from common inputs are useful')
        - additional filters can be applied like the 'commonness of the condition' (if its a common condition, then the uselessness of common inputs is even likelier than if its a rare condition)
        - therefore by connecting the available relevant info ('what inputs are common') with problem/solution structures ('what solutions already exist and have already been tried' and workflows like 'change existing solutions to be more suboptimal') by applying changes ('structures that are similar/different than common inputs') and applying interface structures (like 'probabilities', such as 'info that is likely, if other info is true') to check if those changes are useful, the solution space can be filtered to exclude the useless structures ('similar substances as the common inputs')
        - filters like this can be applied as 'alternate filter sets' to identify solutions that 'fulfill multiple filter sets' (to find the 'most probable solutions') or to identify solutions that 'fulfill different filter sets' (to find the 'maximally different solutions')
        - this is related to the insights that 'a solution might only work in one case' (with the associated implication that 'it might be the case that was checked') and 'a solution might only work bc it was changed a lot' (with the associated implications that 'these extreme changes might have occurred', and 'other solutions exist which are more optimal/adjacent', given that 'its possible to change any structure into any other structure')

    - identifying useful positions (like specific interaction levels) where useful structures are already default/specified which makes useful intents adjacent

        - applying/implementing 'embeddings, maps, and other powerful interface structures' to neural networks
            - if a node/subset of nodes/layer can learn/apply a 'space/context-switch' operation (like a function to 'identify possible relevant/useful similarities/differences' and a function to 'generate a change that will probably make some relevant similarity/difference obvious'), these can be used to fulfill more complex intents than 'apply standard algorithms to combine adjacent changes' or 'learn an index mapping one structure set to another', intents like 'find emergent effects that are obvious in another context'
            - these structures can be applied as sub-functions that emerge from a sub-network of functions within the network, or as additional structures that can be applied wherever an opportunity/reason/requirements for additional variation or optimization is identified in a subset of the network

        - 'finding/deriving the system (find the filter applied), from a position in the system (from a filtered position)' (such as by 'applying symmetries to "remove filters/limits used" or "abstract variables specified" to create the filtered position') is a useful problem-solving intent to apply as a 'core structure of understanding' ('reverse the trap')
            - for example, a definition of the 'universe' as a 'description of maximal differences, which can compress reality adjacently (like with a few folds or a repetition of some core unit)' or an equivalent alternate like 'an emergent structure which can make sure it always evolves (can use any input to generate itself)'

        - identifying structures of time/change as 'embedded time/change, relative time/change, time/change base, time symmetries/position, and limits of time/change' being additional relevant variables, rather than just its 'direction/asymmetry'
            - similar to how 'membership in a set' can be more important than 'which member of the set an item is (what its position is relative to other items in the set)'
            - for example, having a 'gravitational boundary' on possible time/change allows faster time/change to bounce backwards against boundaries and interact with slower time/change
            - as another example, a structure of 'fluid time' that changes until it fulfills some structure like the 'interface network' and then becomes a constant, or a structure of time that is constant in that it requires a field of uncertainty surrounding any structure to allow some structure (like a 'ratio') of interactivity/potential or some interaction with other structures
            - from this structure, its adjacent to identify that 'symmetries' act like 'positions' and identify useful queries like 'are there absolute connections between symmetries and asymmetries, such as that every symmetry has some connected asymmetry, leading to change in a direction away from the symmetry position and therefore its invalidation, or are there usually some combination of coordinating complementary asymmetries that create/maintain the symmetry by their net effect'

        - 'time structures', 'filters and filter derivations', and 'maximal differences from standard/adjacent change combinations (embeddings, maps, etc)' are useful starting positions for identifying default/obvious and non-default/counter-intuitive structures that are useful, and identifying the intents/structures that are adjacent once those defaults/non-defaults are found, for fulfilling other intents like 'identify extreme/scaled limits of a structure' (what happens when time is iterated at scale, according to these possible structures of time), which are known relevant/useful intents for problem-solving intents like 'identify requirements' (thereby connecting these interim structures like "useful high variation interaction levels" to useful problem-solving structures, allowing variables of this connection (variables of 'core change structures') to be identified as well to fulfill intents like 'generate other interaction levels that are useful to apply as interface structures')
            - these structures are also interfaces or sub-interfaces (time is a 'useful standard that allows structures to be compared in a relevant way', which is an interface)

        - similarly, in a different interaction level of 'number types (and representative constants/units of those types)', imaginary numbers are identifiable as useful in that they are related to rotations and a 'rotation' is an input structure of another useful structure, 'orthogonality' (and can be used to create/identify other differences like independence, maximal differences, symmetries, etc), and that they connect other relevant numbers of rotations (like pi) with numbers like i which offer a different difference type, so i is useful as a default structure as it adjacently identifies other useful structures and fulfills useful intents adjacently
            - similarly, quaternions offer 'embedded differences in their interactions', like how they are equivalent alternates at generating i (using a square operation) and also have different interactions that can generate i (multiplying them by each other)
            - similarly, imaginary numbers offer independence in the form of non-embeddability, in their connection to a similar but different space (where i is a unit) whose interval-determining output points (the powers of its units, where the corner point unifying some i-sided square intersects with euclidean space, acting like an endpoint of two vectors, like every negative number is a base point of right-angled vectors of alternate i-unit operations, and positive numbers have vectors of 1-unit operations and a power of the power of the i-unit operations as well, as positive numbers cant be used as inputs to a power to generate the negative numbers) are embedded in euclidean space by reference of the power operation
            - relatedly, questions like the following are adjacent from this position
                - 'is the directedness of the negative/negative multiplication operation that can create positive values (in contrast to the positive/positive multiplication operation which cant create negative values) significant in determining the existence of a number type that is relevant in creating differences like opposites and which has embedded reflections of this connection (the imaginary/real roots act similarly, as they apply the same operation and are inputs to the original negative/positive values referenced), and are there limits to this reflection of this connection that dont align with the limits of the definitions of the numbers/structures themselves but rather require different structures to identify those limits'
                - 'does the directedness of that connection align with other structures having direction, like useful causal input/output sequences (should imaginary numbers be used as inputs to real numbers for some purpose like expanding dimensionality, just like neural networks expand dimensionality of inputs to apply multiple alternate change sequences that could connect inputs/outputs) and time (are there continuous/regular/adjacent sequences of synchronized changes in "number definitions" which can describe time)'
            - relatedly, 'intersectivity' is another possible input of 'orthogonality' and intersections which create a difference (in 'direction') in a similarity (same right angle applied every time), when applied in some structure (like an iteration of the right angle), or intersections which create an equivalence (in the area/distance/angle structures identified by the separation created by the intersection) add value in creating orthogonality, and relatedly intersectivity adds value in that it standardizes the area created by unit distances in either axis of the intersection to match (and minimize the work of) standard multiplication
            - these numbers (and structures of numbers like number types and opposites of a number type within the type definition) exist by being useful to connect other number sets, some numbers seeming to exist only to complete a set (like a set of type examples) or fulfill a useful definition like continuity or isolatability (for other intents like traversibility or repeatability or adjacency of a specific number type to another type, or to provide a connecting/isolating number in between some pair of numbers, to avoid making those numbers adjacent or equivalent or otherwise non-isolated)
            - other numbers seem to exist to implement interface structures like 'limits' (infinities) or 'interaction levels' (relative infinities), as if every number is guaranteed to have some related number that acts as each interface structure (like a limit) on it when it occurs in some set or is acted on by some operation, as if numbers are the default/unit system where interface structures can exist or be identified
            - the fact that there are absolutes (like a highest infinity or most unitary unit) indicates that there are different starting points to view number sets from, which make these absolutes obvious and which make these sets obvious (infinities acting like waves which repeat, self-sustaining changes without contradictions, or non-zero continuities which reference an infinite set)
                - building a definition of an object like 'infinity' makes it clear how some structures are more optimal to define it with, as some definitions are non-unique in that they can reference other structures and are therefore less optimal, but which can be applied optimally as a surrounding structure with other surrounding structures which act like a set of limits creating the definition
                - these interim definitions (which describe very different structures non-uniquely/generally) can be useful as a default set of definitions to apply to adjacently connect other structures
            - given that these intents (like traversibility/specificity arising from continuity) are fulfilled, they are likely to be useful in some way for other intents (such as 'make sure that all numbers are connectible' and 'make sure there are numbers that can be inferred with other descriptions/definitions, but which cant describe real structures inside the universe, to provide a path to other structures outside the universe')
            - given that a rotation is a core structure of a symmetry/center, a circle, a change, a base (like a vector set with a common endpoint as a base), and other useful structures, its useful to identify a rotation as a default structure and find equivalent alternate formats of it and routes to it

    - identify structures that are relevant/useful to useful structures (such as relevant in that they 'cause' useful structures) like 'similarities across different equations (different as in "not reducible to the same equation")'
        - for example, functions having 'similar defined input/output ranges or similar high-variation ranges where their variation occurs (as opposed to where their constance occurs which is less likely to produce intersections)' are likelier to have intersections than other functions
            - within that filtered set of functions with similar defined input/output ranges or similar high-variation ranges, functions with 'similar shapes/scales/averages/limits but which are trivially different in that they are shifted in one variable to preserve a similarity in another range, or so their difference types dont align in some variable (like when peaks/inflections are not aligned vertically)' are similarly likelier to have intersections
        - this is related to the workflow 'find interactive structures and then apply those as possible connection structures to connect a problem/solution'

    - identify structures that differentiate useful different but related info formats like 'truths/falsehoods' and 'certainty/uncertainty' and 'similar/different' and 'constant/variable', which are the foundation for different primary interfaces but are adjacently connectible as variants of each other
        - for example, a connection between 'truth/falsehood' and 'constant/variable' is constant structures are likelier to be truth structures and variable structures are likelier to be false structures (variables are likelier to be in a state of change, either changing toward a value change or decay or toward further differentiation or toward resolution into one constant value, where constant structures are likely to change toward a value change or toward decay or differentiation but have fewer states which could be likely to change than variables as constants dont have the potential to change by resolution into a constant value as theyre already a constant, and if theyre already a constant, it is likelier that they will remain one than that a variable will become constant, as structures which are likely to exist are likely to already exist in some way), so 'finding system initial conditions, scaled convergences, decay points and change thresholds/limits (as the system will occupy multiple states between these structures so finding these structures determining the boundaries of those states is more useful than finding an individual state)' is likelier to be a more useful intent than 'finding current variable interactions in a current system state'
        - identifying these connections is useful for switching between these formats, identifying useful formats, and applying multiple formats to fill in complementary information

    - identify structures that are useful for some useful problem-solving intent like 'find the remainder/some of the function, given a subset'
        - for example, structures that derive some abstract variant of that intent like 'derive a part from the complete set' or 'find a common similarity (solution value) that connects a system of equations', structures which can be used as a default structure to apply when searching for specific variants (or combinations or other structures) of the structure which are useful
        - identifying these alternate specific intents which are useful for some problem-solving intent is useful where inputs of those specific intents are already available (like 'partial differential equations'), which are also useful for other intents like 'simplify a function (by identifying which variables determine its highest variation and removing them until its linear)', which is an attribute of useful structures

    - identify structures that resolve ambiguities like 'structures that specify an example of some general description', where the 'example' or the 'example with the description' is more or independently or compoundingly useful in combination with the description, than the 'description on its own'
        - for example, the phrase 'similarly similar' is difficult to capture/define by a structure, until you apply a structure like a 'space where some example similarities are comparable and clearly similar but trivially different', where this structure is more and independently useful for its clarity through its structure than just the term 'similarly similar' on its own, and bc it occupies a more 'interim' position on the space of reality variables (independence, abstraction, certainty, etc), and makes other structures (like 'clarifying structures that make some similarity/difference obvious') more trivial to find as a result of these other attributes (similar to how 'similarly similar' makes other useful structures like 'similarly different' more trivial to find), just like the concept of 'abstraction' is useful on its own but specific useful abstract concepts like 'power' and 'balance' are more useful to apply as a default position
        - bc of the high variation possible in defining an abstract structure like 'similarly similar', specifying an exact example associated with this structure is useful to filter that set of possibilities, 'abstraction-resolution filters' being useful structures to identify
        - finding a space where 'abstract concepts are iterated and allowed to interact and vary enough to fully capture their definitions to a useful degree of specificity' (applying components of reality like 'iteration/repetition' and other interface variables like 'change' and 'interactivity' to fulfill intents like 'expand possibilities' that enable fulfilling intents possible with these structures like 'specify' and 'structure') is another useful intent

    - identify structures which are highly explanatory through interactions with simple/default/core/otherwise useful structures
        - for example, functions like 'pretend' and 'lie' can be iterated as a repetition of a core structure of 'falsehood', as 'pretending to (pretending to (lie))' is an iteration of a common structure of falsehood ('lie') which can explain more complex system dynamics when functions (like 'lie') are not invalidated ('still allowed') but simpler variants of them become less useful over time, as other agents change at similar rates
        - the 'lie' structure is useful in systems as a 'cheap/efficient structure' where 'false signals of value' are more efficient than 'true signals', but become less useful as other agents change, and iteration of these structures becomes more optimal while still being cheap/efficient, as the 'lie' structure is still allowed in a given system
        - identifying the points where 'efficient structures' (falsehood) intersect with 'inefficient structures' (truth) is useful for predicting future behaviors (it can become more efficient to tell the truth at some point, so much so that it is useful as a default strategy, as signals of true value are useful to other agents in a way that signals of false value are not, as truth structures pay dividends when applied as defaults)
        - the 'iteration' structure is useful to identify as a powerful explanatory variable when applied with core variables to generate/describe complex system interactions in a simple way (as an iteration of the same core structures)
        - 'pretending to (lie)' is a cheap way to falsely signal true value ('obviously pretending to self-deprecate, when the deprecation is actually true' to signal 'true value' in a false/cheap way) when 'lies' are applied as a default structure that can be identified by other agents
        - 'iteration' structures are a useful way to identify 'embedded/symmetry' variables
        - similarly, 'iteration and other symmetry-hiding variables' are a way to identify 'false complexity', where 'true complexity' is an interim structure between 'random' and 'symmetrical or otherwise simple' functions
        - these structures are useful to identify for adjacently predicting other future structures, like how an increasingly optimal 'truth identification machine' would capture & alter reality more quickly (given the relative simplicity of default 'simple or otherwise adjacent/default, but false' lies, as well as default lies based on truths), given its usefulness at supporting a high degree of variation, and how supporting more variation attracts more variation
        - similarly, identifying variable structures like 'degrees of truth' and useful cases of differences within those variables (such as 'statements that are minimally true, given the structure of the relevant definitions' and 'statements that are obviously true/false' as a way of finding 'connections between definition-invalidating differences (some structure like a limit making some statement impossible by trivially changing it to cross the limit) to connect structures like the minimally true with the minimally false and maximally false (impossible), which are useful to apply to make other structures like their differences obvious')

    - identify different useful interactions between data set subsets, complete data sets, solution functions, solution function ranges

        - identifying networks of 'data set subsets that could be a subset associated with many highly different complete data sets or solution functions' which can be used to navigate between data set subsets, starting with each of the subsets that could be the most alternate complete data sets or solution functions (having the most in common with the most other subsets) and testing for difference types to navigate to more specific ranges of possible complete data sets/solution functions, up to a point of accuracy on some metric

        - identifying maximal differences in a data set that can be mapped to one function in a set of maximally different functions, as a range of different shapes of a data set that could be described by some solution function that is a function in a set of maximally different functions, as a way of finding the maximally different data sets associated with a solution function that is maximally different, to 'identify possible solution functions to use as a base' and 'identify whether the data set matches any of the maximally different data sets associated with each of the possible maximally different solution functions as being within an error range of some metric'

        - identifying 'implications of symmetries in a possible solution function' as useful for intents like 'determining the remainder/some of the function from a subset of the function', as 'symmetries' can be formatted as a 'core prediction structure' that can be applied to 'predict some subsets of the function, once an "implication of a symmetry (like a similar slope sequence)" is found', and similarly identifying structures of symmetries like 'symmetrical peaks' and 'symmetry breaks (which can indicate a position of a new symmetry)' and 'symmetry embeddings (of layered/nested symmetries)' is useful as a set of structures to identify as an input to a function to 'identify the remainder/some of the function from a subset'
            - relatedly, identifying 'differences from symmetrical functions/data sets (like symmetrical waves)' is a useful intent when fulfilling intents like 'building a function similarity index' and 'predicting the remainder/some of the function from a subset', as 'differences from symmetries' are similarly useful as 'symmetries' to know about as an input to a 'find the remainder/some of the function from a subset' intent
                - relatedly, identifying a space of similarities that are useful through co-occurrence like 'intersection ratio' and 'symmetry type (like recursion or rotation symmetry)' function similarities, which are useful to identify when they co-occur, as this provides alternate functions to determine similarly similar functions, is a related useful intent to 'finding useful similarities to build function similarity indexes on'
            - relatedly, identifying symmetry-breaking changes (like 'sort') that alter a function beyond its symmetries is useful to identify 'functions to avoid, in a space of functions to apply when identifying functions (like "blur" or "compress" or "filter") that can change an input like a data set or subset or function into a more useful format'
            - identifying a 'general symmetry score' across 'maximally different spaces where various structures like combinations of symmetries are used to defined distance/position' (which functions retain their symmetries when indexed differently by some set of determining function similarity metrics) is a useful intent for 'function similarity indexing'
            - identifying 'maximal differences' in a 'function similarity index' is useful for finding useful alternate solution bases to apply as inputs to a workflow like 'apply changes to a base solution function to find a more optimal solution function', as 'navigating the function similarity index to find patterns of maximal differences and therefore finding the maximal differences' is useful for finding patterns of contradictions (determining differences between similar functions) to filter the solution space (like 'identifying when a function only overlaps with another on one point or at inflection points or in general structures or at its roots or at a high ratio of subsets' and 'identifying the useful structures of these similarities/differences to use when navigating the function similarity index to find patterns of maximal differences'), as identifying 'functions with the highest similarity score and the highest difference score' is useful for other intents like 'find common/probable functions that also are maximally different from other functions (unique) in some way'
            - relatedly, identifying the useful 'interim function similarity indexes' which are in between a 'similarity index of all possible functions' (most expanded index) and a 'similarity index of abstract function-determining/describing variables' (most compressed index) is another useful intent related to 'function similarity indexes', similar to how finding a 'similarity index of similarity indexes' (organized by absolute similarity or abstract similarity or otherwise useful similarity such as 'realistic similarity' that mimics their indexes' true connections in reality, like how 'similarities are embedded on foundational similarities, up to a limit') is useful as a way of organizing these indexes in a network that can be queried to find relevant functions/similarities/differences, interim indexes which are useful structures to identify for their useful mix of 'reality dichotomy' variables like 'certainty/uncertainty', 'simplicity/complexity', 'similarity/difference', 'independence/dependence', and 'generality/specificity'
                - relatedly to these 'reality variables', identifying structures that would be useful ('spectrum variables determining reality') which can be used to fulfill other intents like 'identify a continuous space representing reality to traverse, to fulfill other intents like "identify areas/points/structures of optimality"'

    - identifying useful interface structure interactions in a problem space and applying those based on their interactions (like their interactions with requirements)
        - for example, a general and specific function are often different but there are cases where they are the same, so framing their connection as 'more probably different but not definitely different' is useful for determining differences probable/required in different solution functions
        - a 'general' change type is less likely to occur with a high degree of variation, and similarly a 'specific' change type is likelier to occur with a high degree of variation, but these are also not required
        - connecting these conceptual functions with other interface structures like probabilities and requirements is useful for identifying a solution function based on conceptual requirements like 'an interim function in between extremes like general and specific'
        - identifying and filtering out cases that dont apply for being less likely or less useful (like cases where general/specific functions are the same) is useful when identifying structures like 'data set-function indexes'
        - relatedly, identifying when a different function format is optimal, like identifying cases where its optimal to retain a 'mix of different (as in unrelated) specific/general functions' in the same solution function, is useful to avoid suboptimal structures for a problem or input
 
    - identifying interface structures resulting from common queries connecting interface query components like 'what would be useful if it was true' (in known/measurable and/or complex or otherwise useful problem spaces) that can be applied as alternatives to those query components
        - in the case where the solution intent is a 'way to change greenhouse gases to avoid heating' and a 'specific intent' implementing that general intent is a 'way to configure molecules to avoid heating' and specifically a 'way to gather molecules to avoid heating', applying a common query like 'what would be useful' (what structures would make this possible/adjacent/efficient/optimal in some way) identifies useful sub-intents like a 'way to scale existing molecule-interaction tech (like lasers) to cover more area than usual (like in different shapes like cones to have higher output from the same or similar input of a laser)' which filters the solution space and identifies structures like 'scalability' or 'efficiency' that can be applied as alternatives to the query 'what would be useful to connect intents or specify some intent'
        - this is actually a new structure bc its an 'interim intent' (between problem structures and solution structures) which would be useful if it was true (but isnt definitely known, as its not an existing/known useful structure, so it has to be generated/derived/found as a specific structure that is useful for a specific problem space, then checked for usefulness), which provides a useful filter to focus work on to identify if inputs to that useful structure are possible, at which point the useful structure can be created and applied to fulfill the target solution structure
        - in the example of the 'find a drug for a condition' problem, the related useful intents that can be derived include intents like 'find a drug that has general system optimization attributes (like improving inflammation or lipid metabolism)' or 'find drugs that fulfill multiple coordinating intents' (and apply them as a default reduced solution set) or 'find drugs that fulfill multiple contradictory intents' (and filter them out as less probable solutions)
            - similarly, 'find a drug that changes system-level variables' (with associated implication regarding inputs like 'bc the input condition is a system-level condition') applies an 'interaction level' filter to find a useful filtered intent that is more relevant bc of the filter
            - similarly, 'find drugs that change most variables' (with the associated intent of 'to find drugs to exclude, as these produces too many changes, more than what is likely required to correct this condition, and is likelier to produce more negative side effects than positive side effects even if the positive side effects are to correct the condition')
            - similarly, 'find drugs with some structural similarity (like similarity in functionality structures, that reduces some attribute/function) in the intent/function/interface structures (like functionality) to fulfill specific bio-system intents like "regulation of errors from over-functionality" (which align with reduction of functionality)' (find 'reducing functions' to fulfill 'regulation intents' bc of the structural similarity producing the alignment between 'reduce/regulate' intents)
        - these intents are useful to derive bc they provide focus in reducing/filtering the space of structures to search for (now the problem is 'find a drug with this attribute' instead of 'find a drug that corrects this condition', which offers value in its specificity and may also be more trivial than the general original problem-solving intent)

    - identifying useful computation intents and the limits of computation which would make those more useful when incompletely implemented in a specific subset of ways
        - for example, in the 'regression' problem space, identifying a '"data set-function" uniqueness mapping causal/filtering diagram' which maps 'sequences of differentiating/determining points to check for in a data set' to select a 'representative function'
        - this type of algorithm network (a network of directions representing probable flows as causes or filters of the sequence toward an output function, and nodes representing data set points (or missing data set points as a useful signal, or similarly data set subset areas, densities, similarities/differences, local averages, or other related data set structures), and leaf nodes representing output functions) would be useful only up to a point, with finite computation available (unless every available computer is forced to constantly compute more complex mappings to infinitely compute it as much as possible)
        - determining that point of limits on its usefulness (only compute the mapping up to a level of difference in the data set represented by some metric like a ratio of randomness applied to common variable interaction functions, and only up to x number of data points in the data sets) can be determined by filters like 'reduced rate of increase (diminishing returns) from reward from computing more fine-grained or complex data set-to-function mappings'
        - this is the 'trial and error' algorithm applied to a 'pre-computed index of problems (data sets)/solutions (regression functions)', which is more useful when applied incompletely (selective trial and error) if implemented a specific subset of ways (applied to maximally different data set/function pairs covering many types of variable interactions in real complex systems, etc)
        - finding all the 'ways a function can be incomplete but still useful/similar (like retaining slopes at determining points which can be used to derive the function to some degree of accuracy)' is an intent that can identify the differences (subsets taken out of the function which allow the function to retain its general structure and determining attributes like inflection points) that should be applied as filters of functions to solve the regression problem for when building a 'unique data set-function index causal/filtering diagram' ('direction reversals (like found at peaks)' are a useful difference to apply when identifying functions to solve the regression problem for) or identifying function types/base functions to check for when filtering the full solution space
        - checking for 'function volatility sets' (extremely different functions that can be generated with trivial changes, which can be used to check the rest of the data set for signs of the other functions in the volatility set which indicates a higher probability that the function is in that set and switches between them given the allowed input variable interactions) and 'sub-function volatility sets' (extremely different subsets of a data set or function with extremely different implications for the rest of the function which are almost contradictory but are part of the same function) are other useful filters to apply to avoid checking the whole data set while still identifying its general probable structure
        - identifying 'subsets of a data set or function that can be used to derive maximally different functions' is similarly useful bc these should be discarded or specified with additional filters, rather than applied as specific filters that have a one-to-one mapping to a solution function
        - similarly, 'identifying data set input ranges to divide it into subsets to select from, then a subset of a data set across many different input ranges using some filter, then identifying the implied solution function (or structures of it like a probable range), then applying checks for differences that would confirm/invalidate it with the most certainty (like combined difference types when the solution function is tested on some subset, such as constant/non-linear slope and high value difference)' is a similarly useful set of sequences to identify, to select a subset of probable solution functions
        - similarly, identifying specific error filters of maximally different functions that could all describe a data set (like how a highly volatile wave could look like randomness) is useful for identifying possible errors like false similarities and tests to filter those similarities (the differences in the similarities, like a 'lack of direct connection function between peaks')
        - similarly, identifying 'areas of the solution space to skip, given the extremity of differences to the probable solution function parameters and variable interaction functions' is useful as a way of filtering the solution space to only include similar functions (by their parameters and variable interaction functions) to the probable solution function like a general average function, to discard extremely complex functions for example, even if they use the same parameters, as extremely complex variable interactions are less likely
        - similarly, identifying similarities/differences between structures of values/functions 
        - identifying similarities between the 'differences produced by unit function interactions (normalized to within a certain range, such as exponent/multiply or add)' and the 'data set structure' can be made obvious without retaining the original values in the unit function interactions (if unit functions produce a peak, that similarity to the data set structure is obvious (by changing angle or surrounding structures so as to make comparisons to determine equivalence of a shape trivial) without scaling it to the exact data set scale) and finding similarities in this 'unit function interaction structure (different functions produced by unit function interactions)' as well as differences to 'non-unit function interaction structures (different functions produced by non-unit function interactions at different scales or otherwise different from unit structures)' is a useful intent, these differences resulting from scale of these unit/non-unit functions being useful to identify and connect as equivalences to values of inputs
            - 'identifying a set of functions of inputs that create the same differences as a particular value of an input (making that input an alternate)', and 'identifying a set of functions that changes according to the scale of the function inputs' are useful intents to determine variable interactions
        - identifying differences (like 'intersection points of upper/lower bound lines, these intersection points indicating a difference of "parallelism between upper/lower bounds, which is useful for indicating a clear average line"') between sets of related interface structures (like 'pairs of upper/lower bound lines indicating useful differences in data sets') which offer an alternative to some computation (identifying an intersection invalidates the computation of determining similarity of slope) is a useful problem-solving intent

    - identifying useful structure sets and connections between them, such as how alternative structures can often be combined with an additive usefulness
        - identifying connections of interface structures like 'causes/inputs of a problem' with structures like 'adjacent/existing resources as invalidating/opposing inputs of those problematic causes/inputs' is useful as a common interface structure set that appears in problem-solving processes in a useful way
        - example: moving co2/methane so they cant create the greenhouse effect is a more adjacent target than pushing them farther away so they disperse in outer space or compressing them and storing them in the ocean, so tech like lasers can be used to move/separate the gases so they dont have that side effect, as the 'compression/adjacency' of these gases/molecules is the problem creating the side effect of 'trapping heat'
        - specifying this set of interface structures to make it more useful can be done by applying filters (like 'are functions available, or likely to be available given other interface structures like common interaction functions, that can move/separate molecules/clouds of these elements')
        - connecting 'adjacent resources' with 'one of the causes of the problem' is useful as a structure to start from, to specify with filters
        - alternate structures are useful to identify such as 'find functions that are more possible (moving horizontally into non-heat trapping structures), if other functions are known to be less possible (moving outward toward outer space)'
        - similarly, other alternate structures like 'core structures with opposite functions of the problematic functions' ('different configurations of molecules that dont trap heat') are useful to identify as alternatives and optimizations of this workflow
        - 'more adjacent/possible functions (moving in a different direction)', 'opposite-function core structures (molecular/relevant/core differences as useful differences), 'connections between problem causes and adjacent resources', 'connections between adjacent structures like available resources and non-adjacent functions', and 'specifying filters' are useful to identify and apply in the same workflow (since the 'connection between problem causes and adjacent resources' is so general, its useful to 'apply specification structures like filters', as workflows should 'connect opposing ends of spectrums (a default foundational structure that allows these opposites to be connected) that act like primary interfaces' to be useful, just like they become more useful when they connect generality/specificity, uncertainties with certainties, constants/variables and similarities/differences)
            - therefore connections between these 'opposites within spectrums' (of primary interface variables like abstraction, information, change, system) can form a default 'workflow component set' to apply changes to (like connecting them to problem/solution structures) in order to generate other workflows
        - multi-function structures, as in structures that are useful alternatives which are also useful optimizations to apply in a combination as well, are by definition more useful, so its useful to identify alternatives for the problem-solving intent 'identify useful optimizations resulting from combinations of alternates'

    - apply variation to useful structures (like problem-solving intents) to find specific structures fulfilling those structures

        - for example, for the 'find highly similar (in a known way, which can be applied as a constant, reduced and ignored) but relevantly different (in an unknown way, which should be expanded and focused on) functions' intent, applying differences to that intent to connect it with problem-solving structures identifies useful structures like useful intents to fulfill

        - the following intents ('identify useful changes to a base function', 'identify variables of a function type', 'identify similar/representative structures as the data set (like subsets)', 'identify useful function similarity types and variables generating maximal differences within that type') are relevant to this 'find similar but different functions' intent and can be generated as specific examples/variants/implementations of it which are relevant to other problem-solving intents/structures

            - identifying errors (like worst case scenarios like 'randomness') and directions of errors and check for change in that direction to avoid changing a function in the direction of errors
                - identifying/deriving/generating the possible errors like 'randomness' in a problem space like 'regression' is useful as a general problem-solving intent
                - another example of a 'worst case scenario' is where very different functions (different in the important ranges, to a significant ratio and degree) appear to be the same in some subset that is coincidentally selected as the subset to check as a representative subset
                - identifying 'similar functions (like by similar sub-sections, integrals, etc)' is useful for applying differences to a suboptimal solution function (to check for errors to revert to a previous solution function or otherwise correct a function), specifically 'functions having locally similar sub-sections/slopes that are maximally different in non-local subsets' are useful to identify as a 'worst case scenario' to misidentify (a false illusion of local accuracy/similarity that is highly inaccurate in other subsets), so identifying these 'worst case scenarios' and 'function similarity spaces' allows these similar but different functions to be checked for, to avoid that 'worst case scenario' for that function similarity type
                - changes applied (to the 'find similar but different functions' intent or other problem-solving intents/structures of the regression problem space): rather than identifying local minima of an error function or a generally representative function by applying some average metric (to minimize differences from that average), identify directions that are useful/suboptimal to apply changes in, by identifying 'worst case scenarios' and identifying directions of 'worst case scenarios'

            - identifying variables (like 'start/end position') to create a standard function space (such as where function positions are overlapping as functions have been normalized to occupy similar start/end positions, to highlight other differences than 'start/end positions') where function structures are standardized to more easily identify the maximal differences supported by a function type definition (like 'polynomials') as well as function structures like specific variables/variable interactions that break/reduce those definitions/differences, is useful to fulfill intents like 'find similar but different functions'
                - changes applied: rather than starting from a data set and identifying connections between points, start from solution function definitions, then identify & apply standards that are useful for intents like 'identify maximal differences within a range (like a function type or within a function area or having some points in common)'

            - identifying useful structures (like 'data set subsets' and 'limits') that represent the data set in some way (meaning, equivalently, 'are similar to the data set in some way'), and can therefore be used as a constant base to apply differences to, such as finding a local subset set and a point in each subset to use as a constant subset to base maximal differences on, like the 'maximal differences in functions that intersect with those points', given that these 'constant base' structures represent the data set to some degree (above a set of randomly selected points in the entire space), so applying changes to them (a solution automation workflow) is likelier than randomness to find useful functions relevant to the data set and solution function
                - changes applied: rather than finding just a 'solution function' structure, the definition of a solution structure is expanded to include other structures like data set subsets and limits (like 'function ranges') as similarly useful in their capacity as a constant base to apply changes to, as a 'base solution function' like a 'cheap average to find'

            - identifying function similarity indexes (like 'integral value similarity' and 'primary/powerful term similarity' and 'intersection point similarity' and 'slope similarity' and 'inflection point similarity') (and structures like useful combinations of these similarity indexes) which would fulfill useful intents like 'reduce differences between functions' to fulfill other useful intents like 'find similar but different functions', which is useful for problem-solving intents like 'find maximally different functions, within some similarity range', and identify the variables that connect these similar functions of that type of similarity (such as 'specification/generalization' variables which create similar function integrals, but inject a high ratio of differences in the function variables) to fulfill intents like 'find similar but different functions'
                - these function similarity indexes are useful to traverse as a way of filtering possible solution functions (after first generating the similarity index as a set of probable solution functions, given some known subset of the data set applied as a constant input)
                - changes applied: 
                    - 'probable function areas/ranges/limits' as a 'solution structure' of the 'regression' problem space has been changed to have a different 'difference/distance definition' (the similarity index)
                    - rather than traversing the whole (or a high ratio of the) data set, a subset of the data set is identified, a similarity index is selected based on that subset, a set of probable similar functions is generated by that index, and that set of similar functions is traversed, checking for known differences within these similarities outside of that subset which is used to identify similar functions

            - identifying function-adjacent structures, given its definition (like 'continuous'), such as 'areas of the data set which are more adjacent to a continuous function (like a dense subset) than other areas of the data set (like a sparse subset)', which can be prioritized when connecting/selecting subsets
                - this is similar to identifying subsets that are easily identified as belonging to the same function (such as 'points on a constant line'), but applies a different similarity type ('continuity' as a 'similarity' through 'connectivity' between adjacent points)
                - changes applied: connects different types of similarity such as 'continuity' as a 'similarity of adjacent points', which is more useful to identify than connections between distant sparse points bc there is more uncertainty in those connections and uncertain connections are a less stable base, allowing for more variation in function variables

    - identifying useful mappings like the mapping between the 'system-math' interface, which is where most of the useful algorithms have yet to be found
        - the 'system' interface can depict concepts like 'power' in a useful way, depicting a compressed variant of reality in the system network diagram (power dynamics are easily visualized in a diagram like this, such as 'some nodes would have more connections and more resources than other nodes, and would use that to cause changes in other nodes')
        - multiple methods to map this into a continuous differentiable function exist, such as:
            - mapping concepts to function attributes
                - mapping 'powerful variables' to 'more impactful variables', meaning 'higher variation-generating variables, like the primary exponent or high volatility-causing variables, or highly limiting variables, like variables that make a function never reach infinity or otherwise restrict a function (a wave-generating limit variable)'
                - mapping 'number networks (like important connections between 1 and other numbers like itself, its square, its opposite, its inverse, its nearest positive even number, etc)' so that 'powerful variables/functions' can be found by 'connecting these numerical networks (which have emergent conceptual/systemic attributes like "similarities/equivalences between variants of a number") by conceptual queries' for intents like 'find the most powerful/simple/adjacent (or other common/required attribute) connections between input/output values'
                    - this directly maps nodes on the math interface to system/conceptual attributes using 'function networks of numerical nodes', which could be done with 'representative (maximally different) numbers'
                - similarly, mapping 'abstract variables' to 'general variables', meaning 'higher variation-generating variables, like the primary exponent'
                    - relatedly, finding 'structures in common' across the various examples of power structures in various systems will identify default inputs (used like an 'abstract base' of the definition) like a 'maximal difference (high-low ratio)' that these structures frequently adjacently depict in common, so that finding 'limiting structures on what is not power despite having a high-low ratio or other related structure to a definition of power' is similarly trivial and useful
                    - why do this at all? 
                        - bc knowing certainties like that a 'specific set of concepts exists in every system' allows a program to iterate through all (or a filtered set) of the possible structures depicting these concepts and testing those
            - other mappings like 'finding/generating/deriving system sub-structures like 'system dynamics' and then applying vector-based changes to these system structures, then applying those vectors as input variables to find a function'
        - this is one of the more important mappings to link other non-math diagrams to existing methods like statistical regression
        - the workflow would be 'find a possible system diagram of a data set, then find a regression function for some subset of variables, then apply the system diagram regularly in a cycle to tune that function, since the system diagram will make useful structures clear, like when errors could occur that would disrupt the function, allowing for better prediction of the function'
        - network diagrams (simple connections between nodes) are useful in that some attributes emerge from the network in specific cases, like when a 'hub node' is identifiable by its high number of connections, and other structures can be implied/derived from that like concepts such as 'power'
        - the system diagram is useful in that it has default objects like 'cost/benefit' (which functions like a 'gauge' in that it is a 'revealing sub-structure that, similar to a filter, reflects other structures/makes other structures clear when applied as a certainty', structures like 'efficiencies') as it allows differences in related values (like positive/negative values) to emerge from various agent angles/trajectories in their pursuit of goals while traversing the system, where these objects are defaults
            - this is useful when the system can be depicted in such a way that the negative values seem negative in that structure (the costs are below/beyond some line/plane, and they visibly aggregate according to their actual variable interactions, from some angle, the opposite for benefits)
        - some of these graphs only differ by attributes like 'whether they allow repetition (which is a structure where symmetries might fail through over-reducing parameters)' or 'whether they isolate or group some structure' or 'whether they show all structures like "node-connection functions" or over-simplify/over-abstract to just show that a connection exists', these attributes making them useful for different intents
        - causal graphs are limited in what structures they visualize, but make some useful structures clear, like 'independence', 'input/output sequences between a start/end variable' and 'requirements (as in the "only connection leading to a particular node")', as opposed to 'order-independent' operations like 'set membership' and other graphs where the operations can be assumed/applied by default, rather than checking for sequential position (what matters is not whether one is independent/dependent, but that one variable/function always occurs in some developed/complete system/set)
        - similarly, a 'graph of similar functions by inputs or outputs' makes it clear 'what other functions could adjacently become or falsely seem similar to another function', which is useful for deriving structures like errors in the possible systems containing a function
        - similarly, finding the set of graphs where some variable interaction would be preserved (whether by some subset of variables, like relative position or distance) is useful for finding alternate graphs that dont violate some constraint as a 'truth' structure
        - similarly, identifying function structures that can be adjacently combined to fulfill problem-solving intents like 'summarize', 'predict', and 'describe' a high ratio of variable interactions
            - for example, finding a set of maximally different base functions, and identifying functions to fulfill interface structures of those functions (like a function to create an overlap/intersection between functions by applying some adjacent changes), identifying similarity scores (how many and what type of changes were necessary to create intersections), and identifying connections between these interface structures and useful solution metrics of functions like generality/similarity (a ratio of intersections at intervals indicating one could be a generalization of the other), so that these base functions can be re-applied across problems as defaults, and problems can be standardized to the 'regression' problem space to make this function set more useful
            - this specific function set applies some interface structures as defaults: a set of base solution functions (to use in a specific workflow 'change a base solution until its more optimal by some solution metric'), a set of concepts like generality, a set of interface structures like 'regular overlaps' that can be used to connect base/other functions with a solution function, given these conceptual solution metrics and those concepts' connections to those interface structures, a set of similarity indexes that can be used to identify similar/different functions for various intents, and a way to make these interface structures useful when applied in coordination (an 'input/output sequence' of the workflow that makes them useful)
            - this is an alternative to other workflows that connect math interface structures with problem-solving and other interface structures
            - this is a useful set of structures bc it offers a filtered set of specific structures to apply (rather than a bigger or more complete set of structures), through applying insights/understanding to filter specific structures that are more useful, rather than treating each variable as an uncertainty (the insights add some certainty and limit/filter the set of useful structures)
            - the reason that applying interface structures like 'overlaps' to the problem space of 'regression' (in a specific structure like 'regular overlaps') is that the interface structure of 'overlap' encodes a specific structure of the concept of 'similarity' and therefore is useful in connecting/differentiating various structures in the problem space, like possible similar solution functions, possible similar base & solution functions, possible similar general/specific functions, etc
            - other structures of similarity like 'intersectivity at inflections' and 'intersectivity at roots' can be used as complementary additive structures of similarity to determine similar functions
            - identifying similar functions is useful for intents like 'identify the maximally different functions out of the set of the most similar functions (most similar to other functions)' for intents like 'find the subset of functions that cover the highest ratio of functions by adjacent changes', which is useful for other intents like 'find variables that can be used to adjacently generate these function subsets', 'function similarity' being a specific implementation of the 'interactivity' structure that can be applied in workflows involving connecting 'input/output sequences' (where first, interactive structures are found, and then these are used as component inputs to other structures like 'connections that can connect problem/solution structures')
            - this type of solution that covers variable interactions that can be solved with regression can be enhanced by other math interface structures like 'space embeddings' to create spaces where other useful function structures can be found, to connect a problem to a type of space where the solution structure is more obvious, which is another useful problem-solving intent

    - identifying useful structures like 'minimal interface structures required to describe most variable interactions'
        - for example, some concepts are useful in isolation of other structures, and some concepts are more useful when structure is applied to them (when combined with the structural interface), so that a minimal structure that is still useful requires both the concept and the structure, and a set of the minimum interface structures required to describe most variable interactions would contain all of the concept, the structure, and the interaction that is useful, where the structure and the interaction would overlap with the usefulness of the concept, and replace some of its usefulness, so rather than describing the concept fully and applying that, including the structure and the interaction as part of the useful structure is more useful than applying the concept in its full in isolation
        - this means there is a set of structures on each interface that should be included in a minimal set that provides complementary value rather than redundant value (which is useful for intents like 'find the minimal set of maximally different structures to build the maximally different structure')
        - similar to how identifying the concept of 'power' is useful in isolation of many other structures, identifying how 'power' interacts with other variables and systems (such as the 'set of functions of how power is frequently misused when over-centralized') is similarly useful in isolation of many other structures, where identifying the 'interaction of the concept & function set, as well as identifying just the concept and the function set' is more useful than identifying either or both, in isolation of their interaction, so the 'minimal useful set' is the concept, the function set, and the interaction function, rather than any subset of these
        - this insight is useful bc it adjacently identifies useful graph structures, such as 'finding the ways that all concepts vary and aligning those ways in a set of aligned network graphs'
            - for example, all concepts vary by structures such as 'multiple' and 'unit', so creating a stack of networks (where each network identifies the variations of the definitions of a concept), then aligning these networks so that 'multiple' occurs in the same position (relative to the vertical stack of concept networks, or relative to other nodes on each concept network), is one way to implement the interface network structure, where queries would be able to use 'vertical position alignments' to find cross-interface interactions such as similarities or mappings between interfaces (where the concepts would include interface-specific concepts like 'functions' rather than only abstract concepts, although just applying abstract concepts in this way would be useful to identify how concepts interact with structure)
            - relatedly, other interface structures can be applied, to identify other similarities, like 'common variable interaction structures' as a structure to align concepts on
        - generating other useful graphs is trivial with other workflows, at which point insights can be reverse-engineered from these graphs bc of their usefulness for some intent, using this graph-insight connection
            - by identifying the usefulness of 'aligning similar structures across graphs', the insight of 'a minimally useful structure set involving overlaps/interacts between structures' can be adjacently reverse-engineered as the 'connections between these graphs' (the 'overlap' structure of the 'multiple' structure which is obvious when viewed after applying some core variables to find new insights like a change in perspective such as 'vertically' is similarly/compoundingly useful as both of the individual graphs, so that these should be grouped as a 'useful minimum' of a structure set)
        - in this way the 'structure' interface (with structures like 'multiple' and 'unit' as some of its default core structures) is embedded as the 'host interface' that connects the others, which can be switched to other interfaces (the 'meaning' interface is the default 'host interface' of interface analysis, which organizes structures by relevance/usefulness), or alternately some interaction function of the interfaces (like the concept-structure interface) can be applied instead, such as applying a 'minimum useful set of interface structures' as the host interface
        - other useful structures to apply can be found by applying variables to definitions of useful concepts and identifying the most useful structures resulting from those variables
            - for example, applying variables (like 'variables') to find interactions with useful concepts like 'simplicity' can identify other useful structures to align networks on (like 'low numbers of variables' which is the structure of 'simplicity' applied to the variable of the concept of 'variables'), so a 'variable count' becomes a possible useful structure to align networks on, which would identify other useful structures for their specificity (like 'linearity' on the 'math' interface)
            - similarly, applying the 'low count' attribute found with this application to other interface structures like 'functions' identifies other structures of 'simplicity', such as 'low count of sub-functions in a function', which is useful to store as a default useful structure in isolation of other structures but also in the same set as other structures, as including the 'low count' in a set of defaults including 'count' is compoundingly rather than redundantly useful, rather than generating 'low count' every time, as if it's not a known useful structure
            - identifying other 'compoundingly' (independently, complementarily) useful structures can be done by applying attributes of other compoundingly useful structures (such as 'structures that can aggregate to create differences, rather than structures which can be simply similarized and therefore made redundant, using simple combinations of structures like concepts such as 'simplicity'), where 'similarity' doesnt completely equate to 'connectibility/dependence' as some similar structures are 'falsely similar' or 'independent and/or indirectly connected' (more reliably or adjacently connectible using some other origin/base/symmetry), but it is related to that concept and can be used in some contexts
            - aligning the networks so that 'low count' of variables on the 'change' interface is aligned with the 'low count' of 'sub-functions' on the 'function' interface embeds the concept of 'simplicity' as a 'positional' attribute of these graphs, so that the position of simple structures on the change/function interfaces are adjacent/overlappping in this structure and therefore trivial to compute
            - finding different structures of an abstract concept like 'adjacency' (which can describe almost anything, depending on the graph's positional attribute, if the positional attribute determining distance between points can describe extreme differences, which is possible) by applying this method (of applying core variables of interfaces and finding interaction functions connecting them to the original concept of 'adjacency') would identify structures like 'similarity', 'simplicity', 'efficiency', 'probability', structures which trivialize/reduce differences (like 'commonness', 'types', 'expansion structures (that capture a high degree of variation to trivialize differences)') and other useful structures, where this network of concepts related to 'adjacency' is useful in isolation of other structures, bc of the specificity it adds to the definition of this abstract concept
            - applying some specific concepts is more useful than other concepts, such as applying 'independence' as an organizing attribute to separate truths/falsehoods on different sides of a network (as 'maximally independent' structures are less likely to be connectible, as in causally connected, as in related/relevant, which is another way of deriving the 'minimum structures to create the maximally different structure')
            - relatedly, some structures should be paired/grouped, such as how the 'system-function' interface is more useful in some contexts than either interface on its own, as the 'system in which a function is used' and the 'function structure' are not possible to completely isolate as they depend on each other for important metrics like 'emergent functionality', as the 'system' can be taken as an 'input of the function' and the 'function' can be taken as an 'input of the system' with variable definitions of 'input' ('context' in one definition variant, 'component' in another, and 'interactive structure' in both variants)
            - similarly, 'empathy' is a useful structure in that 'maps between different systems' is adjacently derivable with a trivial change 'isolating the structure of the process (the system-mapping function) and the emergent result of the process (the map between systems) and related interface structures (the "alignment" required for the map to be useful)', which identifies other structures that make insights trivial to identify (high variation-capturing, high variation-reducing, highly structural functions) as an alternative to useful graphs like 'aligned network stacks which embed a useful concept in the alignment, making other intents trivial such as "mapping across systems"'
            - optimally, the interface network implementation would have 'maps between systems' as a default structure on the interaction level of the host network (a network of 'maps between systems (such as the concept-structure map, the system-function map, the requirement-potential map)' as a default implementation structure, which would add 'high variation-capturing' functionality and 'alignment of high variation' and therefore 'maximal differences' by default)
            - relatedly, similar concepts united by a common definition are useful for other intents like most other useful structures are, such as 'find causal variables (since similar structures are likely to be inputs as previous states)'

    - identifying useful structures like 'error structures of common structure formats (like incentives)'
        - 'incentives' are useful as a structural format as they capture a high ratio of information such as 'adjacent changes' and 'defaults' and 'benefit/cost ratios', similar to how similarities/differences capture a high ratio of information
        - finding error structures of incentives (like 'carefully avoiding switching to another strategy when it becomes optimal, just bc its easier to continue making adjacent/incremental changes in a wrong direction') is useful as a way to solve problems once formatted as incentives and opposing structures of incentives (like limits on benefits and cost-maximizing structures that create disincentives)
        - an opposing (solution) structure to this error structure would be 'generating alternate solution functions and calculating switching points when one becomes suboptimal compared to another one given the switching cost, to navigate between alternate solution functions'
        - a 'solution-switching' function (fulfills a 'mix/change' intent) is similarly valuable as a 'solution-filtering' function (fulfills a 'find' intent), just like an 'alternate solution generation' function is valuable (fulfills a 'build' intent)
        - similarly, useful structures in the 'math' interface have usefulness emerging from the variation of their interactive components (the 'set & its associated functions, and the limit/symmetry the set/functions interact with', or the 'definition of distance/similarity and the emergent similarities (the attributes that are continuous (topological) or preserved (symmetrical)) using that definition', or the 'defining/standardizing functions like the gauge/kernel/norm/basis of a space and the similarities (like continuities/convexities/supersets) related to structures in that space and its defining/standardizing functions'), which offers a default problem-solving structure that can be changed to create other useful problem-solving structures as they represent a variant of the un/certainty or difference/similarity set that describes all problem-solving structures

    - similarly, identifying alternate graph structures and the connections between them and other useful structures like the interfaces they represent and the structures that make them more useful
        - for example, a 'causal network' is useful bc of other useful structures (derivatives) and structures like 'injected nodes' which allow for other variable interactions to be considered, and is useful for finding useful structures like 'alternate causes', 'high-cause (powerful) variables', 'requirements' or 'dead ends'
            - specifically, an example of a 'causal network' (implemented as a neural network) is a graph that allows for multiple alternate paths between inputs/outputs to be considered, additionally having multiple variables per path (where various layers provide nodes on the path), which is formattable as an 'angle sequence (query bundle) set that allows overlaps, where the angle format is relevant bc its directed'
            - the core structure of causal graphs can be enhanced by other structures of cause, like other relevant measures of similarity (such as 'root/inflection intersectivity with and adjacency to other functions'), which is not directly determined by derivatives, which describe differences in change across variable sets
        - similarly, other related graphs are useful such as 'boundary graph (indicating boundaries between sets)', 'threshold graphs (which graph areas between thresholds)', 'query bundle graphs (a graph that connects different queries of another network that are equivalent/similar in some attribute like meaning/structure but differ in structure/endpoints)', 'variable networks (a standard variable connecting graph without causal direction)', 'gravity/hub graphs (which prioritize graphing variables that are powerful, common, or highly connected/interactive as opposed to graphing direct causal connections, as these are the important variables to graph, as other variables gravitate around them or are directly caused by them so they can be left out)', 'heat/energy maps (which graph specific definitions of variation/potential)' and have associated useful structures making them more useful
        - these graphs visualize a primary variable of reality that has a value in all structures, that corresponds to an abstract variable and also appears on other interfaces, where other graphs that are useful can be generated by applying variables of these graphs and applying those requirements (this reverse-engineers the primary interfaces, by identifying which graphs capture the highest ratio of variation and variables of those graphs)
        - for example, 'a graph of tensors of cross-interface/graph change types' are a useful structure, as a 'set of interactive/powerful change types' that occupy different graphs, as an alternate format of query sequences across a network like a language network
        - another useful graph might be a slightly more complex function network having boundaries between function sets which are sub-structures of the function network, like a graph of 'areas of similar functions (like different areas for generative functions and filter functions)' where any line on the graph has to select one function from each area/set, as these are useful functions when grouped together
            - injecting similarities in each area (like organizing each area by complexity, commonness, interactivity or another useful dimension) would make this graph more obviously useful, in identifying the function attributes that create the most effective combinations for some intent
            - graphing 'boundaries of function sets' in function networks may make some other structures obvious, depending on the connection/position function used to create the graph, such as where there are probably missing nodes to fill in that are not already known, to capture variation indicated/implied/defined by adjacent sub-structures of the network

    - identifying alternate interaction functions between core problem formats, workflows, and solution-finding functions
        - for example, the 'mix' function can be applied to other solution structures like 'input/output sequences' which normally you would apply as inputs to a 'connect' or 'combine' function (in a 'connect problem/solution' or 'build solution from solution components after breaking down problem into sub-problems' workflow), but when a difference is injected at the 'connections between useful structures', value is still retained in that these functions are interactive and act like equivalent alternates in many cases, so they can be switched without losing too much information
        - similarly 'combine filters' and 'reduce solutions (to common/simple solution variables)' and 'break solutions into solution components' are similarly useful as the default core workflows but require adjacent differences be applied to generate them and also require some useful structures to make them more useful
        - this function can be optimized by applying other useful structures like 'interactive' input/output sequences, which reduces the uncertainty of the inputs to the 'mix' function
        - this applies a 'difference' to 'core useful structures' like 'core interaction functions of solution automation workflows' which is applying another workflow 'apply changes to useful structures to find adjacent useful structures which can act like approximations/inputs/otherwise useful structures by their adjacency to useful structures'
        - similarly, identifying the full set of functions to adjacently connect useful interface structures like 'core interaction functions' and 'problem/input formats' (such as 'how to change the mix function or the input format so it can be used with the mix function, like applying useful filters')
        - similarly, identifying the 'functions to connect known un/certainty structure sets (as well as adjacent variants of them)' (connect incorrect variants which are likely to occur in reality, so that the input or function needs to be changed before it is useful)

    - identifying useful structures like 'incorrect definitions where the correct definition is definitively in-between them or similarly calculatable from the error definitions'
        - for example, a 'symmetry angle' is where both points to be connected are equidistant from the origin of the angle (the point overlapped by or connecting the two lines), and is in-between the angles which are not symmetric, which is useful to identify when finding symmetric angles, by applying a line and a copy of it until theyre connected and changing those lines until theyre connectible
        - these sets of structures like 'known incorrect angles that do not create symmetries' which allow some useful structure like a symmetry to be calculated trivially from them (like applying some filter like 'in-between' to the 'position symmetry')
        - similarly, some unit structure of symmetric angles like a 'right angle' (which is shorter than other angle types and has equidistance possible at its midpoint) is useful with some change set like 'moving the right angle until one point connects to some structure to be connected with the symmetry and rotating and scaling it until the other point is overlapping with an endpoint'
        - similarly, an angle that is just barely not a straight line is similarly trivially possible to position as a symmetry (identifying the midpoint) that overlaps at its endpoints with the two points to be connected with the symmetry
        - these 'error structures connected with these change sets' add 'specific (and therefore useful) structure' to the solution by their definitions which differ from it
        - these 'in-between solution structures' can be calculated when some error structure is similar to another error structure by being its opposite in some way, at which point, its possible that a solution might exist between them (the errors might occur on either side of the solution in this case)
            - this 'function to determine cases where error structures offer information about a solution' is related to this function set as another function to include in the set

    - identifying useful functions to identify like 'identifying absolute (or relatively absolute) truths and applying them locally'
        - for example, identifying an 'absolute truth' of 'sun position' and 'angle to an object' creating a 'symmetry in shadow angle reflecting sun position' with limits like 'daytime motion' and 'positions where sun isnt blocked or otherwise missing like inside/near a building' and 'time interval like "hour of the day"' as a 'key variable' to determine 'direction of motion' and 'routes or components of routes like sub-routes that could maintain some direction from start to finish or generally' as well as 'differences in route or sub-routes that could change direction', for a problem like 'move in the same direction' which requires solving problems like 'find out the direction of some motion'
        - this requires finding 'absolute truths' by identifying 'what is true in different positions' (such as the similarity of sun angle in different positions at the same time) and 'what structures can identify different positions (a field structure where the same rules would likely apply)'
        - this requires connecting that 'absolute truth' with the 'current position/problem', so a 'function to connect absolute and local truths' is a related useful function to 'find absolute truths by identifying what is true elsewhere'
        - related functions include functions to 'identify what truths will still be true or will become false when some position is local (when change of type "motion" has occurred)'
        - this is useful as a solution automation workflow bc the dichotomy of absolute/local contexts is reflective of reality, so applying it with an 'un/certainty' structure and 'connecting the two extremes of the spectrum' is useful at solving all problems
        - relatedly, connecting some reality-describing dichotomy variable like 'abstract/specific' using variants of the definition of those structures on different interfaces (apply the 'abstract' interface to other interfaces and connect the structures across interface) is another set of functions that can be used to solve all problems (such as how variants of the definition of abstract/specific (such as 'local' and 'example/exception' and 'subset' and 'limited/contextual' which are variants of 'specific', and 'global' and 'type' and 'general' and 'superset' and 'acontextual' which are variants of 'abstract', which are 'structures of specificity/abstraction (on other interfaces than the abstract interface)') can be used to connect the concept's structures across interfaces)

    - identifying useful definitions of useful structures that add value in terms of metrics like the descriptive/generative/connective capacity of the definition
        - for example, a function set that 'identifies variables which are also "maximal differences" to the point of being "independent" (as in providing "complementary information") and adjacently connects those maximal independent differences (since theyre still related despite being independent or there wouldnt be a requirement to connect them)' is a useful variation of the 'interface' definition bc it provides a useful target for filtering the set of all functions and offers a base for other 'solution automation workflows', as a function set like this would be able to solve most or all problems, as most problems can be formatted as a problem of missing information about how to connect very different variables (which cant be connected in some obvious way without knowing interface analysis, or it wouldnt be a problem and could be easily solved by an error)
        - similarly finding the full set of functions that adjacently connect independent maximally different variables is useful as a set of alternative solution automation workflows
        - similarly identifying structures like 'maximal differences' and 'complementary information (as opposed to redundant information that reflects the same truths)' that create other useful structures like the concept of 'independence' is similarly useful
        - this structure applies the same core useful structure of a 'highly differentiating filter' or 'primary interface' or a 'un/certainty pair' that is a default structure of 'problems', as the 'connection between the variables' is certain and the uncertainty is 'how theyre connectible adjacently or otherwise optimally', so applying variations to the connection can be a proxy intent for solving all problems, if the variables are sufficiently independent that their alternate connections could reflect a high ratio of information about reality
            - relatedly, a 'similarizing function' that can make any structure similar to some 'core set of structures' is useful for finding 'equivalences'
            - relatedly, a 'differentiating function' that can make any structure similar to some 'core set of differences (problems)' is useful for finding 'differences'
        - this workflow is also useful for finding the 'level of specificity/generality' at which some definition is maximally useful for problem-solving (similar enough to some useful core definition structure used to create other useful structures that it can change into that core structure, and similar enough to known useful structures that it can easily become those, and different enough from other definitions that it can be independently useful for different tasks but similar enough that it can act like an equivalent alternate to some set of other useful definitions to replace it if necessary)
        - similarly, identifying useful structures of definitions such as 'structures that appear falsely similar/different but are/not really the same structure' is similarly useful (for example, identifying that 'reasonable sustained (real/legitimate) changes' may falsely seem like an error such as 'hypocrisy', if some information is left out)

    - identifying functions that can connect useful structures adjacently across interfaces as a useful approximation of interface analysis
        - for example, finding a function set that can identify structures that fulfill priorities (like 'prevent harm to innocent agents' and 'incentivize harmless/beneficial functions' and 'incentivize intelligence development'), requirements (like 'fulfill common intents like store data for quick retrieval'), finding error structures (like error structures of a rule like 'too good to be true', such as 'the exception to a rule where it would become false' and the 'threshold that this error crosses'), finding variables (like 'finding variables of errors'), which when found & connected across the primary interfaces would approximate interface analysis, as an alternate minimum function set that acts similar enough to interface analysis to approximate it
        - similarly, function sets that can identify useful structures like 'similar inputs, similar outputs, and input/output sequences/sets' are useful structures for 'prediction' intents, just like 'highly differentiating switches' are useful for 'sort' intents, and 'highly differentiating filters' are useful for 'find' intents in that they are 'components of solution structures'
            - relatedly, other structures are useful for other reasons than being 'components of solutions', such as being causative, determinative, differentiating, required (rather than just inputs/components of solution structures which are adjacent to those structures when connected by their default interaction function 'combine' applied to these components)

    - identifying different function networks/graphs that are useful for different intents (as a proxy for implementing interface analysis) is a useful intent to solve for
        - a function network (of function types & other similarities like input/output, attributes like volatility, sub-functions, potential, best/worst cases) organized by similarity of relevant metrics like 'output attributes/structures' is useful for switching to other similar functions when one function seems to be false, starting from a core of common different functions
            - areas on this network where functions have useful similarities like 'similar sub-functions and similar input/output connections (like function shapes)' are useful to identify
            - this would be useful for tasks like 'find the most incorrect that a function can be while seeming correct at some subset of points' and relatedly 'find the way to connect these "incorrect-implication point subsets" with "correcting point subsets"'
            - finding the intents that can be adjacently fulfilled with these function networks is useful, to identify how similar these fulfilled intents are to problem-solving workflows & their interface queries, or whether these fulfilled intents can be composed to fulfill problem-solving workflow interface queries or fulfill the workflows directly
            - the network can be organized to make these areas predictable/calculatable so that different areas (areas of functions that differ in these attributes) are also predictable/identifiable
        - finding function sets that can generate other function sets (covering the highest ratio, the most adjacently, etc), such as the 'function set that can be composed/changed to generate the find/change/derive/build/connect function set'
            - for example, a 'structure function network' might have common function structures (like a peak/wave, line, rotation, layer, overlap, etc) and it would generate functions by composing these with adjacent transforms
                - this structure function network might solve the problem of 'quantum attribute observability' by finding common structures and assembling them in a way that seems to match observed attributes of the quantum functionality, such as a 'fluid or other non-constant state of some component of spin/position that both have in common, which resolves to either spin or position depending on where most of the fluid is concentrated at the time of observation on its rotations' or 'intersecting wave functions that intersect with zero at various points so that one has no value when the other has value' or 'a spectrum with multiple thresholds in different directions, where above either threshold, an attribute like position/spin is measurable' or other common structural dynamics that seem to match some attribute of observed quantum functions
            - similarly, a network of common meanings/intents (given the relative lack of variation in these in common structures) so that any new problem can be solved with adjacent combinations of these common component meanings/intents is similarly useful as a function network built the same way (as in 'common component functions')
                - a useful insight is that primary interface structures capture a high or total ratio of information but also reduce computation requirements drastically (meaning there arent that many abstract concepts/intents/functions that are equivalently useful, despite the ability of these sets of structures to describe a high ratio of information), a metric which can be used to find other primary interfaces or proxies of them (these primary interfaces are highly similarizing structures, despite supporting a high ratio of differences)
                - common meanings include structures such as 'common interaction functions' as well as 'common concepts like balance (which are abstract enough to describe most systems in some way, such as balance structures like justice/symmetry/equivalence/opposites)'
            - just like 'sequences/sets' are different core structure formats which are useful for different intents ('sequences' being useful for intents like 'derive/connect' and 'sets' being useful for intents like 'combine/build'), other structures are useful for different intents ('commutative sequences' can be implemented as 'sets', thereby connecting the two formats with a directly measurable/calculatable attribute)
                - these interaction levels ('isolatable sets of interactive & adjacent structures') of equivalent alternates (sets/sequences) that are connectible using core interaction functions ('change order of objects/functions') are useful to identify as 'units of reality' that can be applied and connected to other similar interaction levels to create a useful function network of these interaction levels, which would similarly act as a proxy of interface analysis (interface analysis being the derivation/generation of interface queries likely to be useful/relevant for a particular problem, using interface definitions and known useful structures like 'core function sets')
        - a function network of 'similarizing differences' and 'differentiating similarities' would be similarly useful, as a way of implementing a 'filter network' that fulfills the 'find function' requirements (find any information from any other information, using differences like relevance/interfaces/variations)
            - these structures are useful for various specific intents, such as how 'similarizing differences' is useful for 'identifying types that different structures have in common' and 'differentiating similarities' is useful for 'identifying variables that could indicate sub-types or new types, which differ across structures having the same type'
            - similarly, 'differentiating differences' is useful for making differences more obvious (differentiating from an average), and 'similarizing similarities' is useful for identifying an abstract type or merging different examples into a representative average (the most similar structure)
            - 'identifying groups' and 'identifying the most different abstract types that similar structures can be reduced to' are related useful intents of 'similarizing similarities', which is useful for the efficiency of the description by using types instead of the full set of attributes/values
        - these function networks have structures in common like organizing the network to make non-trivial calculations adjacent to high variation-capturing structures like 'common known functions/components'
        - these function networks (a function network optimized to 'find areas of similarity across multiple attributes', a function network optimized for 'finding the most adjacently generative function set to use as components of all other functions', and a function network optimized for 'finding similarizing differences and differentiating similarities to filter all information adjacently') are similarly useful at capturing high ratios of information but use different 'core concepts that optimally standardize information' and 'variable/constant sets' (applying some structure as a default and allowing related structures to vary to handle uncaptured information) to implement that & other intents
        - similarly, a 'potential' function network organized by related concepts of potential (like 'adjacency of what a structure can be with few changes' and 'extremes of what a structure can be if scaled enough' and abstract concepts like interactivity/power/variability) would capture and organize information in a similarly useful way, as would applying any core concept as a default constant to organize a function network
        - using queries between these function networks is a useful proxy for interface analysis, since it allows queries to be run like 'find an obvious error or symmetry using the similarity/difference network (which identifies these structures quickly bc of the definnitions of their core default structure), otherwise proceed with finding similar functions on the function similarity network organized by input/output similarity and sub-function difference, to check maximally different functions, given some subset of known inputs/outputs, to find the rest of the function'
        - similarly, finding differences like the 'differences between optimality and acceptability (the bare minimum over a threshold, connecting optimality with relevant threshold structures)' which are likely to describe a high ratio of variable interactions based on other useful structures like 'incentives' are intents which are adjacently fulfilled with a network of function networks organized this way
        - similarly, a network of error structures, where the network acts like a 'selection/filter function to select between errors' is similarly useful to identify, as a way to prioritize some abstract conceptual metric like 'balance' by navigating between specific error structures, as every perspective has 'obvious errors/suboptimalities', but some errors are acceptable/optimal in some contexts (inputs/starting points, query paths/functions) and others are not
           - for example the 'balanced perspective' specifically would have more optimal errors like 'being different from most specific useful structures (but different in a similar way, as in different in an equivalent and adjacent way)'

    - symmetry structures applied as a default variable structure, rather than 'incremental change combinations', so that by default, symmetry combinations are sought, rather than incremental change combinations, to always frame change in terms of symmetries
    - combinations of useful intents (such as those that are relevant/realistic) as a default interface to base changes on, finding the functions to fulfill them at query time, as implementation variables
    - structures of relevant cross-interface structures as a default structure to apply changes to
        - for example, 'patterns of causal structures of structures of structures' (like the patterns of cause of 'a sequence of structures that develops from another structure')

    - identify the useful structures (like 'metrics optimized for') in useless structures (like error structures), as everything has both positive/negative qualities, and if there is an error, its bc there was something initially or somewhat useful about the error or it wouldnt have occurred (the error state is a lower-energy/more maintainable state, for example) as a way to find other useful structures like 'similarities in differences' that will have compounding value across problems
        - for example, the 'useful structure' of an error structure like 'hypocrisy' is that a system can contain contradictions/paradoxes/differences without destroying itself, which is useful for generating differences, a core intent of problem-solving
        - this is a 'similarity in the difference/error' structure that is useful across problems, and relatedly 'finding the oppositely-charged structure of a structure (finding the positive structure of a negative structure)' is another useful intent of problem-solving
        - to find 'similarities in differences' in the 'usefulness of a useless structure' (or to find similarly useful structures like 'structures that can support opposite ends of a spectrum' or 'spectrums' as useful structures), applying interface analysis again makes this trivial, but also identifying 'common' structures (like similarities/differences) or 'common error structures' like 'self-invalidation/neutralization/contradiction' structures is another way or applying useful perspectives like 'optimism' (with related intents of the perspective like 'find a way to make everything useful' or 'create differences from errors to find solutions')
        - as another example, the 'useless error structure' of 'blaming the incorrect source of an error' has a useful structure that adjacently derives the insight that 'some functions can handle blame as they are highly variable' and therefore the usefulness of these functions is their variability which makes them 'independent' of the blamers, 'variability' and 'independence' being another useful structure derived adjacently from this useless error structure
        - deriving the attributes of these 'adjacently useful useless error structures' such as their 'measurability' (which is useful in that it makes them trivial to avoid), is similarly useful as a core intent of problem-solving, as some functions develop new functionality to handle the responsibility of blame, and responsibility is an important useful structure (related to fulfillable intents as 'functionality' and requirements)

    - identifying error structures like 'gaps in existing resources/inputs like neural networks' and matching those to 'possible specific functions to resolve those specific error structures'
        - for example, ai has errors such as:
            - 'not identifying/fixing its own emergent outputs' (emergent outputs like 'inability to improve itself on optimal human metrics like meaning, efficiency optimization using interaction level switching, and intent-selection')
                - ai cant switch to using structures like 'civilizations' on other 'interaction levels' by default bc it uses combinatorial approaches in many cases, applying some core unit as an input to a combination function that will never adjacently derive those structures, so computing a function like 'a competition civilization collided with a meta civilization' isnt something it can do bc those arent clearly defined, have high computation requirements (without interface analysis to adjacently derive understanding), and they are not adjacent combinations of anything, so an 'interim thinking' algorithm that uses interface structures will get the answer before an existing neural network
                - this is bc it applies a 'bottom-up' approach by default (combining some core unit) and cant switch that to something else (applying complexities like 'civilizations/universes/rule sets as defaults to overlap/merge/connect/format', as opposed to 'core units to combine')
                - it also cant identify this error structure in its emergent output and cant design another ai to fix that error structure
            - 'not identifying its most efficient paths to derive all variable connections adjacently'
                - an example solution structure could be 'applying an algorithm to find the variation-maximizing subsets and substructures of a neural network is a useful algorithm to include in the network, so those substructures can be applied by default for high-variation intents'
            - 'not filtering/generating its own usage intents to optimize its own usage, as it has no concept of its own usage intents that isnt directly input by a human'
                - identifying a graph where pro-social intents are computible/determinable is a useful intent to apply with neural networks
            - 'not applying/identifying/deriving meaning structures like "the emergent impact of a structure in some or all computible contexts or from every angle" or "the adjacent alternate definition routes of a structures that determine/describe/generate/compress it efficiently" or the "most useful structures that fulfill all meaningful intents adjacently"'
                - 'an algorithm to identify useful structures and requiring neural networks to identify those first' are useful intents to fulfill with other related intents rather than requiring human users to apply changes to neural networks to fulfill those intents manually
            - 'not identifying other specific useful structures like similarity metrics and regression algorithms and spaces where some computation is reducible to an inner product calculation which are comparatively useful compared to neural networks and making sure the neural network can adjacently derive these and other new useful structures and apply them (injecting nodes to allow itself to work on that during training or new incoming prediction calls, as new types/generative paths of "differences from optimals" are identified)'
                - ideallly, the neural network would inject new nodes and then not propagate those until it finds a way to reduce the new nodes injected to some interaction level where its functionality is adjacently derived/applied/found
        - 'identifying error structures describing this difference between existing functions and understanding functions as well as the solutions opposing those error structures' is a useful intent to fulfill
        - identifying structures to map useful concepts (like 'wideness' and 'deepness') with structures of the neural network (like 'adding weight path sequences and more possible changes in between input/output layers') and the conceptual impact of that ('increasing ability to handle variation') is another useful intent to fulfill

    - applying concepts related to 'truth' like 'absolute/infinite', 'reality', 'consensus', or 'sanity' (as a structure of 'being correct' or 'connected to the truth' as opposed to 'being wrong' or 'disconnected from the truth') to derive 'sanity interface queries' that can be used to fulfill intents like 'filter true statements'
    	- for example, an implementation of a sanity algorithm would implement functions that relate to sanity, some component of it, or some input of it, or fulfill some intent/output of sanity better than or similarly as sanity does, within some degree of change applied to the set of known facts that matches a sane ratio
    	- relevant sttuctures of 'sanity' include 'regularly checking if the self is creating errors' and 'regularly testing solutions for correctness' and 'identifying different types of correctness' and 'using true statements as inputs/defaults/constants' and 'avoiding improbable structures' and 'avoiding immeasurable/untestable structures'
    	- as a counterpoint (to implement the insight that 'every truth statement has a related false statement that is likely to be true is some other possible context'), some insanity (as 'being wrong') is useful for some intents (like 'creativity (to add differences to reality, as in improve the truth)', 'identifying patterns and variables of wrong structures as useful through being different from/opposite to right structures')
    	- so a 'sanity algorithm' could start with 'true statements', and apply 'insanity' structures to 'create differences from these true statements' for intents like 'finding new connections between true statements or new networks where those true statements are false' or 'identify errors that could be adjacently derived by true statements'
    	- finding similarities in common between these various conceptual algorithms implementing the definitions of those concepts is useful to find a general algorithm of truth

    - identifying useful structures like 'connections between useful structures' such as connections between requirements/symmetries and useful tests/changes to identify requirements which can be used in place of other structures bc of these connections like 'using the test to identify a requirement in order to identify a symmetry'
        - bc requirements of a system are often sufficiently equal to be its symmetries, a change that identifies a requirement (such as 'removing a variable to see if the system re-generates it bc its required for the system to continue to exist') can also identify a symmetry or similarly a stability
        - the connection between 'requirements and symmetries' as well as the connection between 'changes to identify a structure and the definition of that structure' are similarly useful in fulfilling problem-solving intents like 'connect useful structures' and 'find variables of useful structures'

    - formatting 'neural networks' (or 'regression') as a 'matching' problem, as in 'finding a subset of the network to start applying changes to inputs at, as some sub-structure of the network seems to fit inputs/outputs of the original data set adjacently, then injecting/removing nodes within or around that sub-structure as needed, or adding connectivity to other sub-sections/sequences of the network that seem adjacently useful'
        - relatedly, deriving the 'probable change types (like some exponential change or directional change) or other interface structures' required to connect inputs/outputs of the original data set, finding a sub-structure of the network with those change types, and applying that sub-section as default initial changes, adding more or connecting more as needed to connect inputs/outputs approximately/completely
        - similarly, finding 'optimizations' (such as 'approximations') of the 'regression' problem space (such as 'only finding the subsets with negative slope' or 'only finding subset filters and approximate local averages of those subsets and connections between those local averages') that can be applied to reduce the computations required and change the target of the solution (an approximate solution rather than the original solution)

    - identifying patterns of 'reasonabilizing functions' that connect some structures in a way that makes sense (a 'common sense' function implementation)
        - for example, to connect the very different words 'sky' and 'encryption', you can use an interface query like:
            - 'what is encryption used in' (communication)
                - 'what supports/enables communication (what is an interface on which communication can develop)' (network)
                    - 'what is a variant of that interface which enables communication that is relevant to the target (sky)' (satellite network)
        - the interface query of this sequence ('what is it used in', 'what is an interface of that', 'what is a relevant variant of that') is relatively simple and captures a high ratio of information about the variable connection
        - this isnt just a useful sequence for solving one problem, but it also connects structures to other structures in way that makes them reasonably connectible ('reasonable' connections involve 'intents', 'usages', 'foundations like interfaces', and 'relevant structures such as adjacent change structures', which are reasonable to use as defaults in determining reasonable/probable and therefore realistic connections)
        - finding a 'reasonable' solution connection/function that has multiple reasons why it could be real/true (as it connects to multiple reasonable structures like relevant/useful/usage structures) is useful as a general problem-solving intent
        - finding interface structures like 'patterns' of interface queries that are reasonable (or otherwise 'make sense', 'sense' being 'similarity/equivalence to the truth/validity/other structures of truth') is similarly useful
        - this is particularly useful when information about possible/true structures is missing
        - relatedly, a structure that adjacently connects very different structures that are not related (like highly variable structures which offer different sub-interfaces such as 'sights/sounds' which are only connectible by core abstract structures like 'frequency') would allow more similar structures (like problems/solutions) which are related to be even more adjacently connected
        - similarly, a way to generate variables that have reasons for the variation (like the variable is similar to an equivalent alternate of the original variable in some way, like how 'acidity' can be derived in 'water' by applying attributes of equivalent alternate 'core elements' like 'heat' which has a 'burn' function) as a problem-solving intent to fulfill
        - similarly, a graph where the direction of errors is known/guaranteed bc of structures like 'reasons why an error occurs (bc of its initial advantages before becoming a measurable error)' is also useful for implementing a 'reasonableness' algorithm

    - identifying a function to connect 'inputs/outputs' with 'solution functions' using 'connection structures (like generalization)' to predict an 'error graph where errors are obviously knowable/identifiable/differentiable' is a useful problem-solving intent
        - the 'error function of (a solution function of) input/output connections' is not immediately adjacent for a complex data set, which is the problem solved by neural networks/regression
        - as an example, this 'conversion function (from a data set to a solution function, or from solution function to an error function)' could apply a mix of differentiating structures ('de-volatilizing/specifying or generalizing a subset of the graph', 'applying local averages to a subset', 'applying maximal differences like change in cardinal directions to another subset') in ways that are similar enough to the data set that they are reasonable to apply (within a 'reasonable range') if there is a more optimal summary function or a summary function that is adjacent with these structures
        - the point of solving this common problem is to find the error (the missing info) without having it clearly/adjacently graphed in the input/output connection function
        - the 'solution function of connections between inputs/outputs' acts like an interface between the data set and the error function, where the target of regression is the error function determining how accurate the solution function is for a given data set, so mapping the data set to the error function on the solution function interface, where predictable (as in similar) solution structures can connect the data set and some subset range of error functions, is a useful intent to solve for
        - this would answer the question 'which error functions are probable/possible for a solution function, given specific data sets' and 'which of those data sets align with (are similar to) the actual data set' or in the other direction, 'which solution functions align with the data set and which error functions are associated with those solution functions'
        - another format of this intent is 'what error functions are possible, given some solution function and some adjacent changes to it using some similarity metric', which is a useful problem-solving intent to solve for
        - given some similarity of 'solution function' error (like those producible by solving for 'maximally different subsets' of the data set or 'applying a function to summarize points at regular intervals of subset selection/sampling' or solving for a 'generalization of some more specific function or a specification of some more general constant function'), where 'predictable similarities' in the error function can be generated by applying these structures as variables to vary within reasonable limits (the solution functions produced by similar interval sampling should be similar), these 'predictable similarities' in the error function can be used to identify similar ranges of the error function like 'lower ranges with probable minima' (the changes that lead to 'generalization of a function' or 'volatilization of a function' can be mapped to a set of ranges in the error function)
        - the point of this is that applying 'generalizing changes' (as opposed to 'adjacent changes') to a solution function has a predictable impact on the error function of a solution function set (the solution function will either be clearly right or clearly wrong almost immediately by applying generalizing changes (which involve a high ratio of assumptions and resulting variation from those assumptions), so the error function producible by generalizing changes will be predictably/reliably volatile)
        - this is related to the insight that 'powers/opposites/additional terms are likelier to produce volatility than other structures', as it involves the connection between highly differentiating functions (like de-randomizing, generalization, or mixes of interface structures within reasonable limits) that could produce a summary function, and the preserved highly differentiating attributes like 'volatility' of the error function set producible by that differentiating function applied to create different solution functions
        - other functions that 'skip directly to other minima' or 'clearly differentiate errors across similar solution functions' are useful to identify as default structures to use/mix in problem-solving workflows
        - finding a 'range of reasonable limits' in which possible changes should be limited is also useful for solving the 'find a regression function' problem
        - similarly, answering the question 'what solution-generating methods produce some useful structure in error functions (like constant vs. curved vs. volatile error functions) given some more measurable subset of data set attributes' is another way to format this intent
        - relatedly, determining measurable errors (like 'generalizing a shifted version of the solution function, where the error is that it was generalized on a different level bc of a mis-subsampling, producing an error that could be corrected by a shift down/up, as the error is constant') of these solution-generating methods like 'generalization'
        - relatedly, finding 'similar/different variables' that are adjacently usable to generate local minima identified in the error functions of a subsample of different 'input/output points' is a useful intent using existing algorithms like gradient descent, as the variables in common are likelier to explain other 'input/output point' sets and the variables which differ are likelier to explain different 'sources of error (interfering functions, emergent phases, switches between alternate functions) at different points/ranges/phases' or other useful relevant structures
        - this means given some subset of adjacent sets of solution/error function connections, the rest of the error function is determinable, by applying these 'differentiating' structures
        - this is relevant for cases like evaluating the 'loss function of a specific solution function, across all input/output sets', like where volatility in the data set should appear in the solution function or the loss function will be volatile (these are 'conceptual math connections' between these functions in the regression problem space)
        - similarly, some 'similar solution-generating functions (or solution-generating functions)' such as 'inversions/rotations of a solution function around some symmetry, which will preserve info in the form of some error value for some input/output set' will have some similarities in their error functions, these solution-generating functions being relevant for 'generating different possible functions to connect an input with an output (or to minimize some input/output-connecting summary metric like the difference in the integrals of all input/output sets)' (the 'error function' as referenced here)
        - another related point is that individual input/output connection functions or output/error difference should not be considered in isolation and should always be calculated with some other input/output set
            - a useful application of this point is to evaluate a generated possible solution function for multiple maximally different input/output pairs at a time, evaluating whether some improving change to the function exceeds some metric like a specific ratio of input/output pairs (x% of the pairs' error function values are improved by the change, as in the error function values are decreased) and only keeping changes above that ratio
        - relatedly, another useful application of these structures is to evaluate input/output sets until the 'determining attributes that reduce the set of possible solutions sufficiently' (such as volatility, randomness, etc) are identified (meaning they stop changing to some degree), as a way to select a subset of points to evaluate, as the 'set of functions having the same volatility, proximity to randomness, etc are determinable from those attributes' and once you know which 'function set having similar attribute values' a function belongs to, you can trivially determine other equivalent alternate functions in this set
        - 'similarities across different error functions' can be predicted/tested with a function to generate solution-generation functions that preserve similarities
        - formatting the problem as 'differences between relevant function structures' such as 'differences between error values of one input/output set compared to average input/output sets' highlights other relevant differences to these loss/error/solution functions/solution-generation functions
        - formatting solution functions as isolated subset lines (at various selected intervals) to indicate a 'high ratio of probable outputs' is another possibly useful solution function format that offers a representation of the data set as definitely crossing or represented by some set of subset lines but not being constantly-defined elsewhere
        - relatedly, finding errors resulting from specific/similar methods, such as a 'specify' function that optimizes for 'number of intersections with original data set', which can have errors like:
            - 'in the worst-case scenario, selecting a suboptimal subset to apply as specific test cases of a solution function, such as a subset that is simpler than the rest of the data set'
            - 'a case where the correct solution function is a wave, but the "specify" function implementation optimizes for "number of intersections" so it finds a straight horizontal line for a solution function, bc there are many points in the middle of the wave and that is the subsample selected to optimize for as test cases'
        - this means finding 'change types' that create other useful structures like 'generalizations', and applying those to predict the structure of the error function given the solution-generating function that generates similarities/differences in adjacent error values for a particular input/output set
            - for example, changing the inputs in some way (such as increasing the subsample ratio or improving the accuracy of the subsample filter like requiring it to apply maximal differences) might have the same effect on accuracy as generalization, without being generalization
            - differently general variants of a function might have similarly low error function values (might be mimima of the error function)
            - finding these alternate structures to generate generalization effects might allow skipping to other minima from one minimum
            - these alternate structures offer similar attributes in terms of preserving/representing relevant info about the data set and removing irrelevant differences

    - identifying connection functions/variables between known useful structures like functions/structures to fulfill intents like 'de-noise or otherwise vary the original data set to change its shape to more probably or definitively correct structures' and 'de-volatilizing functions of a data set, to identify adjacent more probable/predictable/general/otherwise useful functions' and 'probable function ranges' and 'optimal solution-generating functions which frequently produce absolute minima in error values with adjacent changes' and 'differences from randomness' and 'invariance to best/worst cases' and 'function attributes preserved across changes generated by a solution-generating function' and 'functions with equivalent determining attributes like volatility/intersectivity/adjacency/representativity/connectivity' and 'standard statistical structures like the probability density function' and 'local point set merging functions' and 'alternate representation/solution/accuracy metrics' and 'alternate subset function-connecting functions' and 'info-preserving functions' and 'alternate useful data set formats' and 'local subset filters' which are known useful structures in the 'regression' problem space which are also maximally different and have likely other different variants to apply that are similarly useful, and which connection problem/solution structures like 'adjacent structures to problems/solutions' in different ways
    
    - identifying useful structures that can identify useful attributes like 'intersectivity' as a 'useful similarity metric' and the intents these structures are useful for like 'finding maximally different functions with similar output functions'
        - for example, the method of 'solving systems of linear equations' can be extended and applied to find 'intersectivity' to maximize that to fulfill the intent of 'finding maximally different functions that produce similar/equal functions' as a way of generating different solution functions to filter
        - similarly, the attribute of 'commutativity' can identify 'equivalent alternate' routes to a particular value

    - functions that can adjacently derive the concept of an 'interface' (given its usefulness) or other useful structures are useful for their proximity to useful structures
        - for example, formatting the 'division' problem as a problem of finding a function that 'finds similarities (like a unit base) in the differences between the divided number and its divisor (like a similarity of the divided number or the divisor to some base like 10) that make the problem trivial to solve' applies the concepts of similarities/differences in a way that is proximal to the concept of other useful structures (like an interface)
        - identifying functions that generate adjacent useful structures of useful structures is similarly useful as finding the direct functions of those useful structures, even though the variant isnt the exact definition of an interface or another useful structure, as they will approximate the usefulness of the useful structure if applied in that position

	- functions that act like useful/powerful filters of relevant information, such as:
	    - a function that can determine an attribute like 'input change type' that reveals similarity/difference of a relevant structure, like 'output change type (integer)'
	    - more generally, a function that can determine attributes that reveal 'information about a lot of other information'
	    	- such as how an 'average data point' reveals 'a lot of information about other data points (in that most of them will be near the average)' and an 'extreme/limit of a range of data points' reveals 'a lot of information about other data points (in that most of them will be within that range)'
	    - as another example of highly useful information like types & extremely differentiating filters, absolutely/generally/frequently/conditionally/probably true statements about 'truths/falsehoods' also reveal a high degree of information compared to the input information required to determine their applicability
	        - for example, given the truth statement 'truths usually have a counterpoint that is true to some degree in some context', a 'proof of falsehood by contradiction' which rules out a possibility bc one contradictory example is found can be considered a contextual proof, as there is usually a context where every truth is false (there may be some space that corresponds to the space in the proof where the interaction rule holds true rather than being contradicted)
	        - what is the common factor between useful extremely differentiating filters, types, and truth statements
	            - there is a high degree of information embedded in these structures (when you determine a type of an object, you may know many other things about the object such as the type attributes, similarly when you know whether a filter rules out some object, you know a lot about that object such as its attributes differentiated by the filter, and similarly when you know whether a statement fulfills some truth statement, you know a lot of information about its truthhood)
	            - these structures are highly organized and well-defined/rigid, meaning they are useful by their 'certainty' for comparison/differentiation tasks to determine uncertainty structures
	            - these structures are extremely useful/relevant to many useful intents like 'determine truthhood of a statement (test if a solution is correct)', 'filter out sub-optimal/error statements/solutions', 'classify a statement as true/false or another information type'
	            - these structures occur on different interfaces and have the 'certainty' attribute in common, making them corresponding 'certainty' structures on different interfaces
	        - similarly, other structures can be framed as 'high information embedding structures' such as 'function subsets that act as implementation methods of interface analysis', 'the set of efficient compressions of reality', 'the maximally different variable structure where any problem difference is easily connectible with a query on the structure', the 'optimal set of interface queries that solve most problems adjacently or otherwise optimally'
            - similarly, concepts & other primary interface variables can be framed as 'high information-storing/embedding' structures which are powerful through these functions and similarly offer high ratios of information (knowing the concepts related to a structure gives a high ratio of information compared to non-primary interface metrics)
            - similarly, some structures are 'sufficiently different to some specific structures (as in different from simple structures enough to be capable of handling stress/complexity/embedded variables)' and 'sufficiently similar to other structures (like input information, as they preserve info)' that they are more probable, and likelier to be legitimate/real/truth structures than other structures

    - identifying useful structures that are cross-interface (such as 'structure-concept' interface) highly useful through some metric like 'commonness' to fulfill intents like 'prediction' and 'explanations'
        - for example, 'right angles' are useful in providing 'orthogonality/independence', 'angles' are useful in providing a 'central base to form an angle for comparison', 'multiple right angles' are useful in providing more extreme differences, and 'unifying structures (like a tensor)' are useful for providing relevance, which provides a structure that fulfills concepts like 'orthogonality, simplification of comparison intents, and relevance of different structures' (a cross-interface structure on the 'structure-concept' interface)
        - therefore finding structures like 'tensors' (multiple angles having the same central base) across interfaces are more powerful than other structures
        - other cross-interface structures that fulfill these useful intents and concepts can be derived and used as defaults

    - identifying the map of 'structures that are optimally useful in some metric' and the 'specific structures which can implement them adjacently or with high probability of success or some other solution metric'
        - for example, the 'multi-task' structure is useful, where some function fulfills multiple very different intents optimally
        - specific structures which can implement this functional attribute adjacently include 'undifferentiated structures' (similar to how stem cells are useful through being 'sufficiently similar/adjacent to multiple specialized structures that they can easily become those structures' (occupying some position in between other structures which are optimized for some specific intent, as opposed to being generally useful but suboptimal)
        - the 'map structure' connecting a 'multi-task function' to an 'undifferentiated structure adjacent to specialized structures' is useful in that it can connect other useful intents with specific implementation structures
        - finding a 'specialized structure that is optimal for some specific task, which is also useful for tasks in general or multiple other tasks' is rare bc few tasks require enough functions that the inputs to the functions also cover many other function inputs and usually involves interface structures and/or high degrees/amounts of computation/data
           - finding structures which 'adjacently generate the highest ratio of function inputs' is similarly a useful specific intent to fulfill, as a specific implementation of the 'multi-task function' structure
           - relatedly, as mentioned previously, finding structures which are 'sufficiently similar to every useful structure that they can adjacently generate it' is another useful specific intent to fulfill
           - relatedly, finding 'overlapping functions having some functionality in common (as the metric of function similarity)' is another specific intent to fulfill which can derive the 'multi-task function' on its own
           - relatedly, finding the 'maximally different structure' is likely to be able to be useful for multiple intents, which is another way to find multi-task structures
           - relatedly, finding the structures which are useful across problems, like 'formats which standardize other structures, so that one function can be re-used and useful across different inputs once formatted' are similarly likely to be useful in finding 'multi-task' structures, as 'standardizing formats' compound the usefulness of functions that are useful in the output format
        - identifying the concept of a 'adjacent structure to other useful structures which can be parameterized and these parameters changed to produce other useful structures' is adjacent to the structures of 're-using an existing solution' and 'approximate solutions which are adjacent to optimal solutions'
        - the 'map structure' connecting a 'find useful structures' intent with a 'multi-task function' is similarly useful
        - this is a specific example of a 'function-intent map' structure which is highly useful across problems
        - similarly, identifying the map of 'error structures' and 'specific structures implementing that error' is similarly useful
            - for example, the 'invisible information' structure is useful as an example of a 'missing information' error, in comparison to other 'error structure implementation examples' like 'unmeasurable information', 'corrupted information', etc, which are related but different concepts and are useful to differentiate

	- a function set to determine the meaning of an interface query is useful as a 'reverse-engineering' implementation method
	    - for example, the requirements would be a function to generate all possible/legitimate interface queries and a function to determine the meaning of each query
	    - where the meaning of a query is described as the impact of the query on other relevant structures like problems and the connections like similarities/differences to other queries like on metrics like cross-problem solvabiility

    - given the primary core interaction functions of problem-solving (find/filter, build/combine, apply/change, derive/connect, organize/sort, simplify/reduce, define/structure, try/test, match/map, abstract/specify, standardize, etc which resolve some problematic information structure like a seemingly random large set to filter, or a difference between resources and target structure that can be built with those resources, or a difference between a disorganized structure and a structure that would make finding tasks trivial like a sorted structure, or extra variables complicating some structure, or undefined structures that need to be identified), apply these functions to each other on increasing interaction levels to find other functions of problem-solving
        - for example, given core interaction functions of one unit (change, filter), generate other functions on other interaction levels like multiple units ("change filters"), and connect these to problem-solving workflows ('"change filters" used to solve a problem until the filters are different enough or fulfill some other metric that they could optimize some problem')
        - as another alternative, find core structures of problem-solving like useful structures and find new functions to connect them, as an alternate way of generating useful problem-solving functions
        - as another alternative, find the variables of these functions (spectrums like certainty, simplicity, etc) and generate the set of possible interaction functions that way
        - in all of these, some structure is held constant ('units of core functions', 'positions of useful structures') and another structure that interacts with the certain structure is variable ('how those units are combined', 'how those positions are connected') which allows new structures to be found in the variation allowed within the limits framed by those constants
        - so finding the information which reflects the other information (finding that 'core functions as units' reflects information such as 'how those units can be (adjacently/usefully) connected/combined', reflecting the info through the limits imposed by those core functions being applied as 'certain constants', which interacts with that information in some way, like with a core interaction function) around some symmetry (the 'combination/connection' interaction function symmetry, allowing the units to be combined/connected in a useful way without being destroyed) is useful for finding which sets of information allow this 'certainty/uncertainty' structure to be applied to find new information structures like new connection functions

    - identifying alternate formats that problems can be converted to, such as specific problems like 'finding ways information can be preserved/stored (problems formatted as a info storage problem)', 'finding all connections between useful structures (problems formatted as a problem of identifying useful structures and deriving them from each other)', 'finding all the ways information can be hidden (problems formatted as missing information)', is useful as an input to a change workflow to derive other useful structures related to these problem formats
        - these alternate formats highlight different useful structures
            - the problem of 'finding ways information can be preserved' highlights other useful structures like:
                - the fact that info 'requires structure and also specific structures like connections to other information and limits on info storage'
                - function attributes like consistency/predictability (since some functions vary on how predictable/consistent their outputs are)
        - applying adjacent changes to useful structures such as 'applying them to each other' or 'applying them to known problems' adjacently identifies these other useful structures

	- applying specific interface structures as a default constant set
		- applying 'information' interface interaction rules of the 'physical reality' interface such as:
		    - 'structures that break definitions of some concept can be structures of falsehood, unless the structure is more stable/relevant/otherwise true than the definition, in which case it can be a structure of truth, as change patterns/structures of truth arent false by default but rather can be more true than their inputs'
			- 'truths can become false when over-depended on, beyond their appropriate context or meaning, or beyond their potential to illuminate or support other truths, or in an incorrect structure like a foundation for other truths'
			- 'truths can be so irrelevant to an intent as to be equivalent to false (example: citing the heat death as a reason not to try to do anything)'
			    - 'relatedly, truths are meaningful statements, where meaninglessness makes some statement so different from relevant/useful/integrated/meaningful structures as to be false'
			- 'truths can be so rarely/improbably true as to be equivalent to false (example: an error state that is so rare you basically dont have to plan for it, like where neutrinos would coincidentally flip all the bits on a server at once)'
            - 'truths that are generally true (like something with a high benefit to cost ratio is too good to be true) have an associated truth that makes them false in some way (like their error structure such as "when the better ratio is actually true, this truth will miss that exception"), where the "error structure that could find the structure with the better ratio" could be more true than the generally true statement, depending on the ratio and the usefulness/importance of the structure with the better ratio'
			- 'truths can be so unstable (difficult to maintain) as to be equivalent to false (example: a rare atomic state that degrades into another more stable state more frequently), as truths generally take less energy to maintain or make true, as lower energy-requiring structures are likelier to exist, and similarly scalable truths (which can be repeated or otherwise scaled without or with fewer contradictions) are more true than other truths'
				- 'relatedly, truths, if prevented or destroyed, will re-occur if true, as its unlikely for a truth to occur in absolute isolation, only once, and simlarly can support more embedded variables if true than a falsehood can'
				- 'relatedly, truths must have some structures or they are unlikely to be true, such as how requirements (costs, responsibilities) are required or the structure is unlikely to be true'
			- 'truths are less likely to be extremely surprising/different, based on a comparison to an input set of sufficiently variable mixed facts'
			- 'truths are less likely to be on either extreme of various spectrums like specificity (an extremely specific fact like a "specific behavior of how to be good" is less likely to be or remain true than a general fact like a generally useful priority like "being good")'
			- 'truths are likelier to have truth-associated structures like costs, whereas a lie is likelier to have fewer of these structures (a lie about zero cost)'
			- 'the ratio of truths to falsehoods is likely to be stable to some degree, because when a truth is measured and sustained enough to be measured, some other true structure might decay, although entropy is a powerful process that might act as a counterpoint to this, decreasing the number of truths (stable structures) over time'
			- 'truths can be so difficult to measure/verify/calculate/derive as to be equivalent to false (example: number of atoms in the universe, or some phenomenon that occurs below the synchronized directed information frequency that constitutes 'time' that it cant be measured in isolation and can only be inferred by its emergent effects)'
			- 'truths can be so non-adjacent to other probable/known truths as to be equivalent to false (example: future truth of a reality that is non-adjacent to current reality and is unlikely to occur)'
			- 'truths can be so lacking in reasons as to be equivalent to false (example: there is no reason for a rare anomaly except random coincidence so it may as well be ignorable)'
			- 'for every fact (which acts like a representation of reality) there is another fact that represents reality equally/similarly accurately or represents a similar proportion of reality or otherwise represents reality in a similar way by some metric of representation, so that these other facts in the same position on this representation metric index can act like equivalent alternates'
			    - 'statements at different positions on this representation metric index can act like a cross-section of reality that acts like a determinant of reality (the cross-section is usable to determine other facts, which is itself a representation metric), though statements that tend to be closer to either extreme of absolute truth/falsehood or the center are more valuable for their obviousness of similarity/difference to other facts/falsehoods'
			- 'truths are generally independent in that they have relatively few dependencies in order for them to be true (less conditional and contextual, more absolute and inevitable)'
			    - 'at the same time, truths are likelier to have more reasons why they are true/can exist (existential risks to the truth were prevented by many guarantees), making it more stable, compared to a falsehood (an agent decided to make it seem true, using cheaper structures than truths)'
			    - 'this is related to the fact that truths usually have fewer opposing statements that are true in some way/degree/context (bc paradoxes/symmetries/rotations may be a core structural unit of reality, just like randomness/ambiguities/balance points where very different structures seem equally true are alternate structures of reality related to symmetries, as most structures can be connected to symmetries given that theyre a structure of stability/robustness where some structure can be sustained under change conditions and therefore a structure of existence/reality, just like useful structures can be a structure of reality as theyre more stable), compared to falsehoods, which usually have more opposing statements that contradict them, and related to the fact that truths are more similar to other truths as they are more adjacently connected to other truths (using truths)'
			    - the question of 'what are the limits of symmetries, or why isnt everything a symmetry (in every permutation/interaction and on every level), given their commonness' is adjacent to that, with related insights like:
			        - 'variables are a type of symmetry', 'variables are arguably descriptive of everything (the change interface can capture all variation)', 'variables are embedded on other variables', 'some variables break/create symmetries and that is more important to categorize them than to call them just another symmetry', 'symmetries dont always resemble symmetries in various states of development', 'some symmetries neutralize each other, leaving randomness (which is different in that it is the interactions of unlimited changes, rather than constrained changes of a symmetry)', 'symmetry types are a meta-symmetry', 'meta-symmetries (symmetries of symmetries) are more powerful than standard symmetries', 'not every symmetry is equally interactive with all other symmetries', 'other structures like levels/systems where symmetries act and symmetry interaction functions are equally important if not more'
			    - similar to the question of 'why isnt everything formatted or adjacent to an input/output sequence by default' which is bc information can be adjacently and usefully stored in equivalent alternate formats, not everything is a symmetry bc there are variables of symmetries (interface structures like 'state' and 'self' can be applied to symmetries) as theyre not absolutely required, and there are many variants of the definition of a symmetry as theyre related to 'limited change' (similar differences) which is a fundamental structure reflecting reality (true/false spectrum pair), so just like input/output sequences arent enforced in all conditions/contexts, neither are symmetries
			    - similarly, the 'limits of definitions' are equivalently useful to the definitions themselves
			- structures of meaning like 'the comprehensive impact of its interactions/functions at various scales, in isolation and in various system contexts, and with other common variables applied' and 'useful structures' are useful approximations of the definition to use in place of the full definition, similarly compressions of reality are more useful than reality itself, these compressions acting like interfaces between the user and the system being compressed
			- 'truths can be so simple that they will essentially be false in that they will be more useful when changed in some way and will be an input used by other more powerful/complex/high-variation functions which could independently generate/alter these simple truths as needed due to their simplicity, and therefore are less real/true, as reality allows complexity and therefore requires some complexity for survival, survival/stability being related to truth, and similarly can be so complex that they are essentially not usable by other powerful/complex/high-variation functions and therefore are less real/true'
			    - 'relatedly, there is always a simpler and more complex variant of a statement within the bounds of simplicity/complexity required for reality, and the same goes for other dichotomies of reality like truth/falsehood, similarity/difference, balance/imbalance, variable/constant, meaning there is a network of statements related to one statement that act like different variations of the same statement, so that the statement acting like the symmetry of this network of statements is more true than any of the variants and will reflect the interface network to some degree/type/structure'
			    - 'truths will be more adjacently connectible than falsehoods to interface structures (such as the primary abstract concepts like balance/power/work which correspond to the dichotomies of reality)'
			- 'almost every fact has another fact that is more relatively true and a falsehood that is more relatively false, except the most absolutely false/true statements, which are relatively rare'
			- 'relative truth/falsehood is determined by proportion of reality represented, accuracy of the representation, power/usefulness/relevance for enabling/determining other representations of reality, logical validity, adjacency to absolute reality, im/possibility, inevitability/requirement, consistency of the fact/falsehood across contexts, consistency of truths/falsehoods across different representation formats, potential of all other metrics to remain the same or change, & other metrics of truth'
			- 'truths often come with an opposing counterpoint as most truths are not absolutely true but are wrong in some way like in a particular context/usage, so that a truth without a counterpoint as how it might not be true is unlikely to be true'
			    - 'relatedly, truths are generally somewhat variable, acting like a symmetry/manifold in that they can vary in variables like position/shape/relationship to other structures without being destroyed, in their robustness to change, whereas lies by comparison are relatively fragile'
			    - 'similarly, truths are often seen to have a "balancing offsetting truth", similar to a "rise and fall" structure pair (which may differ in side lengths, as a statement may be more true than its counterpoint(s)), which is a fundamental structure related to parabolas, angles, waves, etc'
			    - 'however, given that other fundamental structures exist, the "rise and fall" of a point and its counterpoint is not the entire description of reality, despite being extremely useful and common in the form of the concept of balance'
			    - 'similarly, other concepts than balance are known to be relevant, as not everything is balanced, such as the ratio between matter/antimatter, and therefore other concepts are required to describe reality'
			    - 'similarly, other structures than one interface are important to describe reality, such as how a cross-interface structure (such as the structures that a concept takes in various systems) is more useful than either structure in isolation on one interface'
			    - 'for example, given that the interim and combined concept, respectively, of a circle, parabola, curve, a fractal, efficient embedded change (using the previous value as an input as an alternate to a constant multiplier), a sequence describing the variable interactions rather than a set of interacting independent variables, a stability in the change rate, an origin/symmetry, and a concentric circle set is a spiral, how does this interact with the "rise and fall" structure of a point and its offsetting counterpoint?'
			        - 'during the standard process (which may be the "rise and fall" of a new change as it first generates change and then is offset by opposing forces to settle in to its stable form), other changes are allowed to occur, such as embedded changes, curved changes, etc, which can disrupt the stabilization of the new change or even invalidate it or isolate the initial change from its counteracting change forces to prevent their interaction'
			        - 'given that the spiral with a stable change rate (that is maximally different from other structures like a full circle and a cornered shape) has stable structures, it can inject stability in other changes, such as by making a rise or fall more stable than its opposing structure'
			        - 'given the changes enabled by the spiral and its stability and therefore its disruptive power, spirals are a better structure of randomness than some other structures, as an offsetting uncertainty structure to more default certainty structures like the "rise and fall" which are more certain bc they are more balanced'
			- 'truths that are in between simple priorities & rule sets (like "opposites attract") and complex priorities & rule sets (like those generating randomness) are likelier to be true, and truths that connect simple and complex rules are likelier to be true, as there are both simple and complex rules in reality, and neither has absolute priority, as the interim truths between extremes are likelier to be stable and therefore true, and similarly the interim balance point between all dichotomies at which theyre intersecting is likelier to be the most stable perspective, from which the others are adjacently generated'
			- 'truths are rarely the only truth explaining a variable interaction, as there is rarely a variable requirement requiring that specific combination and preventing any other combination from succeeding as there are more often many routes between two points than one route, and there are many alternate equivalent preceding/succeeding variable interactions that are similarly explanatory, bc errors are default in most systems and therefore differences applied to sequences/routes/components are likely and are less likely to be ruled out by some requirement if they still generate movement in the original direction, and similarly bc components are frequently unitary and can be combined in different ways to generate the same structures, and similarly bc real systems are often complicated and subject to errors from their high degree of interactivity with other systems in contrast to a theoretical high degree of isolation'
			- 'truths usually are not obvious (such as the wishes of agents as their default, and therefore also obvious/simple to them, so these wishes can be assumed to be not true) but if they are, a beginner without expert knowledge of rules is likelier to see them than an expert bound by their knowledge of rules'
			- 'truths that are not measured or otherwise useful are likelier to change than truths being measured bc the act of measurement requires finding a way to keep some fact true enough to show up on a measurement scale, while other variables are allowed to remain variable to cause other variables to change, which are not being forced to remain true bc attention is directed at other truths to keep them constant and randomness indicates that anything not forced to remain constant is subject to variation injections (a function to generate change, whose change-generation speed is faster than measurement speed, is likelier to be a more useful intent, to make all measurements obvious or unnecessary, as the changes that are true will sustain themselves and the changes which arent true wont, so no measurement/testing is required, measurement being useful for "changing direction of focus/motion/work/etc")'
			    - 'relatedly, truths in general are likely to change at some point due to allowed variable interactivity/dependence, so any set of facts that was true is unlikely to be completely or equivalently true at some point in the future, as variation isnt equivalent to falsehood but a fact of truth-generation & adaptation & interactivity & dependence'
			    - 'relatedly, truths that are independent from another set of truths (such as processes on another planet) are less likely to remain true, unless another truth intervenes (something needs it to remain true as its useful for some intent)'
			    - 'similarly, truths that dont require ignoring other information (having no contradictions) are likelier to be absolute truths'
			- 'truths above a ratio of already identified truths may be so useless as to be false, as they may be irrelevant, and also preserving the ratio of truths/falsehoods is useful for agents with intents to incentivize additional change/uncertainty (some intent like a wish/dream must be false for them to be able to justify doing work to change any variables)'
			    - 'similarly, future truths can be predicted using the intents/incentives of agents (agents who are capable of fulfilling all of their intents, who dont always select the incentivized option, as "always selecting the incentivized option" contradicts "capacity to fulfill all of their intents"), once their intent & fulfillment capacity (like 'to increase uncertainty', 'to seek freedom', 'to control time', 'to understand physical reality') and incentive selection (ratio of selecting incentivized option) variables are known'
			    - 'similarly, some possible truth can be so different from incentivized truths or intended truths as to be false'
			- 'truths are generally those structures which fulfill multiple solution/optimization metrics as opposed to just one (its rare for something to be true which is only useful in one way, such as ability to change or interactivity with other useful structures, generally structures which survive fulfill multiple optimization metrics as structures generally interact on multiple interaction levels)'
			    - for example, a structure is not usually just 'efficient' in some metric (efficiency meaning low-cost or adjacent in some other way, which means its near to something else, as not every point is adjacently useable as a starting point to find adjacent structures to), it also needs to be 'similar enough to existing starting points/interaction levels' to be useable (in order for that efficiency to be efficient)
			- 'truths can be less true than a falsehood, if the falsehood is more probable, relevant, useful, is about to be true and if the truth is about to be false, is important to be made true, is approximately/generally true, reflects more interface structures like potential, etc'
			- 'truths are generally more useful at generating other truths (used as inputs to other truths), as opposed to generating falsehoods or lies being more useful at generating truths'
			    - 'similarly, truths are useful at specifically generating power, in the sense that truths enable other functions to be more powerful, so statements (about a variable relationship) that seem to control/determine/cause other variables are likelier to be true'
			    	- 'for example, specific structures act like truths (such as certainties/guarantees) on specific interfaces through their determination of other structures, such as how gravity/speed acts like certainty in the physical information interface, and structure acts like certainty in the math interface, which can be used to find different certainty formats in other interfaces (what type of similarity/change acts like gravity/light in the math interface)'
			    - 'similarly, truths are useful at connecting true/real structures, additionally connecting cross-interface structures, such as by "specifying a concept" by connecting the concept to a structure'
			    - 'similarly, truths such as interfaces are useful at supporting variation, so high variation sources indicate a truth supporting that variation, as if variation is false in some way (such as by contradicting truths) it will likely be stopped by agents who benefit from truths, so if variation continues, it is likely to be based on a truth that has created enough stability to support additional changes'
			- 'truths generally coexist with other truths, rather than more frequently/generally contradicting them'
			- 'truths generally are more similar to other truths, rather than being extremely different from them, partly bc of the fact that once a structure is found that is stable/otherwise useful, it tends to be repeated rather than changed'
			- 'truths that seem "counterintuitive/random/complex/asymmetric" are adjacent to some interaction level, despite seeming false (through dissimilarity/disorganization/complexity/asymmetry) at first, as truths are usually an adjacent combination of some components (though neural networks dont usually switch perspectives to start from a different angle/starting point and identify these interaction levels from which everything is adjacent) and by comparison lies will fulfill some usually-checked metric of truth (similarity/organization/simplicity/symmetry) while containing a contradiction (false similarity, etc)'
			- 'truths are often trivially different from lies, in that in order to be believable, a false statement has to be sufficiently similar to truths (plausible, reasonable, logical, sensical, meaningful, relevant) and cant be obviously false (silly/nonsensical/inappropriate for the context/meaningless) at which point it becomes an obvious wrong (a joke), therefore a lie (a difference from the truth, or an error) can be used to generate a truth by applying it to another lie (a difference from the truth) or by basing it on or connecting it to the truth (making the lie obviously false so its not really a lie), so lies should be connected to an offsetting lie/truth rather than given in isolation, as they wont be robust to testing/changes on their own, unlike the truth'
			    - the types of difference that a truth is robust to are fragile bc of this triviality in their difference from lies (applying an 'opposite' transform could make any true statement false, so thats not a useful change type to determine robustness of a statement to change and therefore its truthhood), however given that the truth often has multiple alternate forms, applying extreme differences to a statement could also produce another truth
			- 'lies take meaning from the truth, to the point that above a ratio of lies, truths begin to degrade/decay, and fuzzy approximations/representations of the truth & tools to automate that become more true (in the sense of being more relevant/useful/usable than most absolute facts), to the point that computers/algorithms become "relative/approximate truth" factories, based on some ratio/structure of falsehoods they can compute filters for (and how well they can integrate new info with the few details that are important to hold constant rather than allowing them to decay)'
			- 'truths that invalidate other truths are generally more foundational (identifying a previously unknown interface that invalidates other truths by reframing/explaining/decomposing them to such a degree that they become irrelevant, such as a statement that "everything is a function of cause", so that everything seems invalid/irrelevant except cause)'
			- 'structures that have more functions such as "organize information more effectively" are likelier to exist/be true than other structures as they increase the stability/inputs of other structures and allow other structures to exist (are more useful and coexisting), and also increase their own stability (organizing information effectively allows the structure to handle stress better and solve more problems better), leading to a win-win situation that is valuable for life to occur and stabilize through coexistence'
			- 'structures of truth (like derivation methods) that describe/generate/derive/find/apply/compress/store/retrieve info better than other structures are likelier to be true as theyre more effective/optimal in some way than existing structures (the universe is likely to move toward a state that more efficiently compresses reality than other sets of rules, if a state becomes adjacently possible relative to the existing state), meaning truths that use other truths the best/most effectively are likelier to remain/become true'
        - finding the 'information traps (at which point no further information is derivable, as in a dead end) and the sequences to get out of information traps' such as:
            - trajectories across different graphs (like usage graphs, as opposed to attribute or variable or function graphs, to find paths back to a high-variation/high-potential position)
            - applications of complementary (as in independent) insights, such as applying how 'every fact has an opposing statement that is true in some way/degree/context' to find the errors in the generator of a trap and fix those errors which are inputs to the trap (errors like 'inability to derive information' and 'inability to change' and 'inability to be independent'), or similarly applying variables to generate independent directions of change like the primary interfaces, or similarly applying variables to the generator/trap to make it improved to capture more information more efficiently, so as to become an input to the trap/generator, or similarly applying variables to the trap/generator to make it independent, or similarly applying variables to the generator/trap to make it trap itself, or similarly applying limits to the generator/trap so it is the dead-end
                - a good metaphor for the primary interfaces is the 'cardinal directions' (as dimensions of reality) each being capable of reaching the center and the edge of the universe, in different ways (using different combinations generating different structures), where the other directions are derivable from each direction if the center or edge is known and each direction is enough to understand the universe on its own, having a sufficient cross-section as an input, each providing equivalent information in independent ways
                - the cross-interface connection/change sequences between these 'primary directions of change (on one interface)' (which themselves support connection/change sequences) are important as a foundation of reality (the ways that information can change into other information being a source of freedom/potential where change can occur)
            - applications of limits to trap the generator of a trap or the trap itself and applying variables to the information that is trapped to reverse positions, or applying variables to the trap/generator after applying a variable to change its direction/intent to set it free in another direction
		- applying interim connecting structures of useful structures representing the truth like 'rules databases', 'attribute networks', etc, such as 'common structures to both systems' such as 'constant attribute rules' (variable interaction rules)
		- every true statement should connect to other sources of truth (high-variation variables like interfaces, powerful variables like cause/energy, important sources of potential like 'black holes' as potential energy sources)
		    - 'humans are working on climate change (because we havent tapped the extreme potential energy of black holes) and (because we havent identified all the interfaces) and (because we havent optimized energy efficiency)'
		    - true statements should take important/relevant true statements into account
		    - structures like black holes are a source of truth bc they store information/energy/reality efficiently, just like the interface network does, and just like some formulas do (euler/gravity equations, etc)
		
		- definitions/limits/requirements create a metaphorical corrollary of 'gravity' in that they make some structures more similar/probable/coordinating than others
		    - where does variation go which cant be stored/have structure in the universe, or is this not possible and all impossible variation is approximated/echoed in brains to conceptualize impossible structures (like how you can know some things about an infinite set, such as that one side has no limit, and the connection function of adjacent items, and the impossibility of it stopping, and its starting point, which is like glimpsing a subset of infinity, as that subset is all that can occur in or occupy reality), which implies that this infinite level of variation would contradict some other real structure which has more energy or which has other rules protecting its stability, meaning its more powerful than that infinity
		    - limits like 'are there functions which are possible to graph in 3-d euclidean space but which are impossible to form with real structure variables' are useful to find, to determine the differences between math and physical reality and the limits of this connection
		        - questions like this would also determine structures like 'whether math stores more possibilities than reality' or 'whether reality stores more possibilities than math' (math definitions allow for fewer variables than reality), or does reality offer a useful alternate format of math (like a 'cohesive usage network of repeatable math functions/structures'), and which system controls the other (does math determine reality, or do agents in reality control math in the sense that they can use it for their intents), and 'what is the limit on the variation producible with known math definitions, and can that variation explain reality'
		        - if math offers more possibilities than reality could ever support, reality is like a compression of (a subset of) math, and its possible that reality could only ever represent a subset of math structures, rather than all possible math structures, and 'all possible states of reality' is determined by the set of 'all possible subsets of math, below a certain threshold or in some structure limiting the subset', and 'identifying the structures to connect reality rules with these limits on what math structures can take physical form at any time/state (rules like "nothing too chaotic or simple")' is a useful related intent
		        - a compression function could also act like a generative/descriptive/explanatory/causative function
		        - are some structures only impossible bc of other structures that could change reversibly (undoing the change to revert to a lower-variation or more stable state while retaining the new information) 
		        - 'what information/structure is likely to be irreversible, unstoreable/unretainable, incompressible, or create chaos cascades' is a useful related question

		- identifying useful problems and useful problem formats to solve, such as 'identifying how to format problems/variable/interface structures so that they can be formatted as a problem of "solving a system of equations" to find the alternate function sets that can coordinate to determine reality', like 'which set of functions preserves/stores/organizes information the best without contradictions (using the same components and without removing any components from the original function sets which are known as true, or removing any other functions)'

		- identifying connection functions to apply the opposite or other differences to known problem-solving workflows, which are similarly useful in reverse or with other differences applied
		    - for example, given that a default solution automation workflow is to 'generate possibilities and filter them to find a reduced solution set/area/range', an 'opposite-direction' variable applied to this workflow's direction would be to 'connect filtered/reduced sets with the filters applied to possibilities, to reverse-engineer the sets of possibilities that were filtered by reverse-engineering these filters'
		    - an example of these filters would be the 'type' filter, that when a type is identified, a high degree of other information about the structure is known (the 'type' filter reveals a high degree of information about the other structures that are members of the 'type' group, as well as the other 'attributes' of the 'type')
		    - once you know these connection functions, you can apply them by default as 'default filters' to filter generated sets to identify 'subsets with predictable attributes resulting from those filters, given their connection function', or to connect a 'few specific examples' with 'general patterns of possibilities' or 'full sets of possibilities'

		- identifying function sets to map 'generality (in intent) to specificity (in implementation)' is a way to identify the core functions that need to exist (for intents like adjacently generating all other functions), using examples of general intents fulfilled by specific functions, as a way of writing the fewest functions (writing an abstract general function and then varying it when specific valid intents require changing it)
		- identifying ways to integrate alternate core general useful structures like applying 'causal networks' to decompose some subset of data sets, where some points like 'density centers' indicate connections between points such as 'probability of adjacent points' (as a cause of expecting to find other points nearby) and some points like outliers or non-pattern-compliant points representing structures like limits/dead ends in causal networks (as a reason to find those patterns, or as a reason to discard a point as being generated as only an output by one rare process so as to reduce its possible interactions and usefulness), or alternate causal networks of input feature subsets where some causative relationship is probably in a set of alternative relationships, since adjacent/equal points arent necessarily absolutely independent, in the sense that one point may lead to generation of other similar points if successful/useful in some way, or a particular point may be so useful such as simple that it is frequently generated independently, and these related points can be reduced to one representative/general point that can be used to generate all of them, and instead the 'reduced set of representative points' can be altered by applying these 'reasons for point similarity/repetition in data sets' to generate the probable alternate variations of them, while applying regression to just the reduced set, or the 'reduced set + the generated set', to isolate actually or more independent points

		- identify methods of describing useful methods like interface analysis using functions/variables not directly referenced in their definitions as good approximations of it, like describing interface analysis as 'interim thinking' (as 'thinking without definitions/rules to find new definitions/rules to connect known definitions/rules') and 'meta-representation and similarity reductions and integrations' and 'potential maximization', these descriptions being useful for identifying approximations of the logic (similar to how concepts can more effectively describe a structure than the full set of details about it) which can be used as inputs to generate the logic & identify variables of these useful methods to generate the others
		    - similarly, other good descriptions include: 
		        - 'a semi-populated network of interface structures with empty nodes to be determined in between and external to these certain interface nodes, which when viewed from various perspectives/angles outside the network, offers new obvious connections that offer a different perspective that is still true in some way as some interface structures are still included in the subset made obvious and the connections made obvious in that subset, where outer layers of the network grow as new interaction levels of interface structures and new interaction functions applied to them (where the semi-populated network is different/new every time, to avoid repetition)'
		        - 'starting from no information (a position in between known structures), what minimum of information (what trivial set of structures) can you apply that will derive a connection to information (derive a new "function to derive information", like a solution automation workflow, which can be used to connect any information with any other information)'
		        - 'finding new interactions of useful structures like cross-interface structures as default units of computation, given their usefulness when composed adjacently'
		        - 'finding/deriving connections (and derivation functions of 'connections' or of "systems of connections") between previously unconnected interface structures'
		        - "finding new extreme differences from 'adjacent change combinations', as new useful structures are the least likely to be produced by adjacent change combinations, which will never approximate 'thinking' unless the adjacent change combinations are applied to extremely useful structures like solution automation workflows & other interface structures, which when slightly changed, are usually still useful"
		    - this is in opposition to the default 'rule-set' based thinking, which applies trivial changes to known structures to create 'trivial innovations' (which could be replaced by a 'poor copying' function)
		    - 'interim thinking' starts with a 'lack of rules/information (known structures)', like finding a 'path to a safe planet from an abyss in the universe', not by applying rule sets (like 'use known physics rules and calculations using those rules and a calculator'), but by asking high variation-capturing questions which direct changes, and applying trivial structures (like the 'functions of light' and a 'increasing light-interaction function' like a magnifying glass has and interface structures connecting and differentiating these like 'abstractions of these structures'), until a new connection is found that can be used to reduce/remove/change/otherwise solve the problem, questions which are new every time which can produce new structures, bc if a problem exists, its likely that a new solution is required, which is a very limited rule set, so much that it may as well be nothing, or an empty rule set having a template/boundary that is fillable with new useful structures, which is like 'carefully avoiding rule sets while being similar enough to existing rules to be useful and probably realistic'
		
		- identify new intents that when fulfilled, have increasing value over time, as they are highly interactive with other useful interface structures
		    - such as 'find new extreme differences' or 'find new patterns of info storage structures or other structures of truth' or 'find new statements that are generally/absolutely/usually/required to be true' or 'find new equivalent alternates to useful structures' or 'find new variables such as patterns of useful structures' or 'find new interface structures (types, connections, reductions, incentives, simplifications, interaction levels, variables, etc) of interface structures' and other intents that can be generated as equivalently useful to these, which are generally useful intents for problem-solving

		- identify structures that always describe some variable interaction when in different formats, which can be applied as default variable interaction structures to check for
		    - for example, there is always a 'self-similar unit effect (like a domino effect) on some interaction level where that structure is a unit, that when repeated up to a threshold, explains some other variable', so checking for 'domino effects' is a structure to generate from inputs & look for in outputs, just like there is a 'halting effect' placing limits on repetitions, and there is always an 'interface interaction structure' that can be checked for, even if these patterns/outputs arent always fully-developed/detectable at every point
		    - relatedly, generating the structures between these known required/probable structure is useful to identify interim states

		- identify/generate structures that can act as default/core/component structures of useful structures like 'filters' to apply as default useful structures
		    - for example, a 'filter' might take the form of a 'barrier with openings that allow some subset of possible inputs to pass all the way through', or a 'set of cage-like structures that can trap excluded structures rather than being specifically designed to only allow inputs of some shape' or a 'sequence of tests that remove functionality/attributes of excluded structures so they cant move/change further or so they dissolve' or 'some structure that moves different structures to different positions', which are default structures to look for when fulfilling intents like 'identify filters', as they fulfill sub-intents of filters in various ways

		- applying useful reality representation structures such as 'metaphors (similar/relevant but different/inaccurate structures to format something differently in a useful way to achieve understanding)'
		
		- applying physics rules such as the 'fuzziness of physical reality' as a way to find other true structures ('symmetries', 'alternative definitions', 'patterns')
		
		- applying structures like the functions that can generate the highest variation (such as the 'Conway game of life') as a default structure to generate other high-variation structures like reality
		
		- applying specific mappings across interface structures (like concepts such as 'balance', physics structures like 'symmetry', and physical reality structures such as information agent structures such as 'justice' and 'economic equilibrium', or concepts like 'interface', structures like 'bases', information structures like 'metaphors' being variable implementations of the 'interface' concept) as default structures (such as default inputs of a neural network used to predict other mappings)
		
		- apply useful perspective structures (like combinations of perspectives such as the 'optimization perspective' and the 'religious perspective' to efficiently describe other useful structures like 'what agents want and what could be true using adjacent transforms of reality') as a default constant set
		
		- apply useful structure sets that should go together (like how 'dependence' is useful in the 'causal' interface but negative in the 'physical reality' interface)
		- apply useful structures (like 'direction or line connecting starting/goal points' and 'implementation structures to get to that point') to model useful structures that are useful when applied together, like 'intents/implementations of intents', as a useful input to neural networks to fulfill the task of 'adjacently combining them in a way that connects them'
		    - similarly apply patterns of implementation structures such as 'causal loops referencing nested sub-problems resolved with some structure and output to the host structure once solved' as default components of implementations of intents
		- fulfill a primary function of a particular interface that can solve most problems, such as 'evaluate meaning', and the useful functions to fulfill that function efficiently
		- apply structures iteratively to calculate the global/universal meaning to check if its obviously false at scale (in its extreme form which is more easily determined), such as applying a rule like 'its ok to violate someones rights if theyre a genius and youre fascinated by them' (after iterating to an extreme scale, its obvious that no, that couldnt be right, then geniuses wouldnt want to live if that rule is applied at scale, to guarantee that geniuses' rights will always be violated, and it couldnt be right for many other reasons, such as it is probably 'better/more sustainable/and therefore lower cost' to prioritize turning people into geniuses than to persecute geniuses with rights violations which will reduce our supply of geniuses to zero)
			- structures that make other structures obvious are obviously useful
		    - apply interactions of known structures at scale as default filters of other scaled structures, determining what else can exist at that scale, deriving core structures of truth from these scaled structures that could also exist at that scale
		- apply structures that can be used for multiple intents as default useful structures bc they can handle more variation than other structures and are therefore likelier to be compoundingly useful when applied in structures like combinations
		- find useful spaces including definition spaces where some attribute makes it useful for common problem-solving intents
	    	- maximum that a definition can change without breaking, depending on the relationship between changes supported/required by that definition
	        - differences that create different 'definition routes' of a definition (comparisons to similar/opposite definitions, usages, formats, different subsets of attributes that differentiate it in some space)
	        - space where embeddings (apply), combinations (build), connections (derive), and divisions/filters (find) are adjacently connectible
	        - finding a space between other spaces that is required to exist by definition where some attribute is also required (some attribute like an equivalence required to exist bc its between a greater/less than relationship)
	        - a space that enables finding the right layer to evaluate meaning/impact at, such as the 'layer where any pattern is identifiable' or the 'layer where variables are identifiable' or the 'layer where unique variables are identifiable'
	        - a space that makes equivalent alternates adjacent, equivalent alternates such as the 'maximally different structure' and the 'minimal structure required to describe primary interface concepts like potential/cause' and the 'dichotomies of reality', which are probably equivalent but are not trivial to derive from each other
		- apply structures that can offset neural network inadequacies such as 'limits of a series' being useful for solving the problem of modeling 'longer sequences of input/output connections' as a tool to determine when a sequence might converge, which is obviously useful for prediction intents, as well as other structures that allow the user to 'see far ahead', such as 'highly complex/different realistic/stable/efficient' structures, useful determining functions of extreme attribute similarities/differences (like absolute change type interactions, such as preserved change types when some function is applied) as 'extreme' structures and 'equivalent alternate structures (that can keep each other in check and identify invalid structures)', and other structures that allow you to efficiently calculate a lot from a little information
		    - you can see how useful functions (like calculating an adjacent attribute such as 'constance/addition' that reveals non-adjacent information like information about 'extreme change type preservation') act like a powerful information filter that reveals information that is difficult to calculate (non-adjacent)
		    - finding useful structures to determine attributes of other useful structures (useful structures like 'extremes') can be done by reverse-engineering 'input problem cases' where these revelatory 'filter' structures would be useful:
		        - finding highly different structures like extremes or complex manifolds (high-variation change based on an interface like a connected structure such as a manifold, which is a useful math structure adjacently mapped/corresponding to an interface)
		        - determining what is easily determined about that highly different structure once you know it
		        - determining what cases this information could be useful in
		        - determining whether those cases are useful
		    - similarly apply functions to 'calculate convexity' and 'filter possible functions given maximal differences (structurally similar change types that can look similar at first like exponential change, waves, hyperbolic functions, etc but which have adjacent filters to filter them out) detected by some point' as a way of improving 'gradient descent errors'
		    	- you can see how these 'interface structural similarities' (like an 'exponentially increasing subset') provide a useful constant base to apply changes to, to determine other possible structures and identify filters of those possible structures
		    	- this is an embedded application of the 'interface' concept, which is clear in the usefulness of this 'similarity to base changes on'
		    - similarly, using 'lifetime data with associated errors across a lifetime' to put a statement into perspective, or having 'civilization lifetime/trajectory' data as opposed to phrase/sentence/paragraph/story data, is likelier to connect information to more relevant structures (to answer questions like 'what problems were solved by a mind that regularly produced this thought')
		    - similarly, including 'historical/prediction data of past/future truths' with answers such as 'additionally, next year, this will probably change x% and previously this was y%' to give context and only provide info that is immediately outdated but is likely to still be true the next time they run the query
		- apply interaction levels ('adjacently connectible structures that interact/connect') as 'input information' or an 'information format' to neural networks (which fulfill the task of 'finding adjacent combinations to connect variables')
	- applying a function to find interaction levels where the problem is solvable with adjacent combinations and the function to find those adjacent combinations once transformed to that interaction level
	- iterating through filtered interface structure combinations and checking test cases of input/output pairs
	- create a network of solutions to use as bases to navigate
	- iterate constant/variable pairs to model the highest degree of certainty/uncertainty pairs
	- applying more adjacently determinable structures as an input
	    - attribute networks as an input and function networks as an output implementing those attribute networks
	- finding an 'input-output sequence' that would be useful and would probably be adjacently constructible in that sequence
	    - like a sequence of first building an 'attribute network' to describe all variables and then building a 'function network' describing that network and then a 'function generator network' to compress/generate that function network
	- applying default interface structures as the core functions of a machine learning network so that errors like 'not understanding abstract concepts' can be avoided by injecting abstract concepts as a default input or function or related structure, so that 'combinations of abstract concepts' required to 'solve a problem of understanding some conceptual combination' are adjacent
	    - 'inventing new machine-learning models' doesnt just require 'sequences of change combinations on some input features'
	    - getting machine learning models to actually invent a new optimal machine learning model requires a function to 'identify optimal models which are maximally different & useful across all problems'
	    - this is 'accidentally stumbling on useful functions' by adjacently changing a weight sequence representing a function (as functions have variables such as components)
	    - this may look like intelligence but is only "accidental coincidental effects of making 'variable changes' resulting from the function chosen seemingly at random"
	    - this "discovery" is a re-iteration of the known fact that "ai models change variables that can act like functions, and then those changes are sometimes useful at reducing the loss, so the model keeps those useful changes rather than discarding them"
	    - this "discovery" is an advertisement to the lack of understanding that "some functions can be re-used across problems"
	    - the model didnt magically know this particular function would be useful across problems, it just applied adjacent change combination that are basic/core changes, and which are easy to find, which coincidentally happened to be useful across problems, bc of the power of some core changes
	    - that is a fact of reality that should be an input to an actual intelligence algorithm (core functions are the only type of function youll get with 'adjacent change combinations' unless your 'inputs are adjacent to interface analysis structures' or your 'connective function is maximally differentiating'
	    - https://arxiv.org/pdf/2211.15661.pdf
	- find adjacent structures to useful structures and how to connect the adjacent structures so that useful structures are adjacent using these connections
	    - for example, finding that 'poverty costs more to maintain than to fix' is a result of finding 'similarities' to 'costs', such as 'poverty', and then connecting them (why is it similar to a 'cost'? its related to a lack of money/resources)
	    - as another example, finding that 'mirrors are useful structures to find hidden information visible from another angle (as they offer useful differences to input angles around the symmetry of a right angle)' is a result of finding 'similarities to light' such as 'light symmetries' which are the reason theyre useful for that intent
	    - these have a structure in common like:
	        - 'cross-interface structures (cost-lack and light-symmetry)'
	        - 'similarities to important variables (these important variables being variants like cost/light of primary interface or otherwise useful variables like information, which are a component of inputs to information such as probability, as costly information is less probable to exist or be measured and probability is an input to information, or are variants of information on different interfaces like the physical interface, light and cost being important variables which are related to information)'
	    - the question of 'what is the limit of the usefulness of finding different variants of a definition and replacing the original definition with the variant in its interactions with other structures' and 'what is similar across similarities to important variables' are other related useful queries

	- finding functions to find/derive/generate & avoid errors in a complex system
	    - for example, in the problem space of 'creating a virus to restore original dna without epigenetic mutations', some worst-case scenarios could be:
	        - some reversal of a mutation causes the problem youre solving in a different way so that 'solving it' in that way just shifts the problem to another position/state (constitutes an illness trigger in a particular host state, or causes an illness given other mutations/genes)
	        	- the host has adapted to the mutations, meaning if you reverse them, the host dies bc they now need that mutation, which need youd have to fix first to avoid killing the patient by reversing the mutation
	        	    - epigenetic mutations store some memory or functionality that some process has come to rely on
	        	    - reversing the mutation in a healthy patient would work, but the patient has complications like organ damage that make the gene therapy deadly
	        	- there are several 'state changes' that need to happen in a sequence to reverse direction away from some bad health state, where if you skip a step or get the order wrong, the host dies
	        	    - some mutation is ok to reverse in one state such as when otherwise healthy, but not given the mutations required by this sequence
	        	    - once this sequence of steps to reverse all mutations is calculated and applied, it turns out a state in the sequence was optimal to stop at, but the completed sequence was deadly, as some mutations could be kept without killing the host for some reason (they made the host better at something, which theyre now lacking)
	        	- the mutations have become so different from the original state that making these reversing changes is too stressful as its too many changes at once, and they need to be broken into multiple subsets of changes to avoid over-stressing the system
	        	- some negative environmental or default response to the gene therapy triggers reversal of mutation reversals (stress-handling methods create mutations, such as exercise, stimulant substances, or cell cycle/immune triggers, which might reverse some reversals before the therapy can be completed)
                - some other system variable needs to be synchronized with the therapy, such as to 'deliver it at a certain time to sync with circadian clocks or right before sleep cycles, distributed over several days' or where some other alignment needs to happen to successfully apply the therapy in a non-fatal way ('deliver it in a way that doesnt trigger heat stress proteins or disrupt cell communication or other intrinsic required processes that must be stable for the host to live or for the host to respond successfully to treatment', for example)
                    - the gene therapy has to be used with some other treatments (like immune suppressing treatments) to deactivate existing processes to prevent mutations, or some process will neutralize it (the immune system will attack it)
                    - it turns out that some unforeseen condition is true (like 'if one cell has a mutation, it will re-occur in all/most other cells', or 'if you change dna in a way that the system is not accustomed/adapted to, the mutations will re-occur out of convenience for the system that uses them'), conditions which only occur in some patients/health states/environments or with some mutations
                    - or some other condition like 'if you apply a gene therapy that is sub-optimal in some way like involving a lot of mutations, you also have to speed up the cell cycle temporarily or induce cell communication or apoptosis in some cell type, or it wont take hold' or 'gene therapy in some sub-optimal way like "many simultaneous mutations" is only effective if some gene to control gene editing/immune function is switched on' applies that we dont know about
                - the gene therapy changes how other complex systems interact (how the host microbiome interacts with their brain, given the impact of dna on both systems) in a way that causes illness in other complex systems which are difficult to predict
                - the gene therapy is successful in the cells it reaches, but it doesnt reach many cells bc default dna repair processes are too effective at preventing this many mutations or those specific mutation types and they effectively deactivate or kill the virus
                - the virus is too useful for other pathogens, which eat/use/attack it before it can complete its processing
                - the virus quickly becomes pathogenic and causes sub-optimal mutations/illnesses, as it gets mutations in its own dna, either from the host, from the host's illness state, or from other pathogens/chemicals in the host
                    - similarly, the virus allows existing pathogens to become more pathogenic
                - the set of mutations required to fix it is so volatile & different for each individual and you would have to know every pathogen, every input (environmental, habitual), every distorted variable in their system, and every illness state they have before making an accurate prediction of how to fix it bc of the volatility (meaning the solution can be drastically different even for similar people)
                - the gene therapy is successful in a patient but the therapy spreads to everyone around them by environmental exposure and it can kill other people so they have to be quarantined or quarantined with people having similar genetics for some period, or the mutations would have to be paired with some other mutation/state/treatment so that the virus would die/deactivate after completing its processing
                - the side effects of changing a lot of genes at once is frequently deadly, no matter which genes youre changing, bc of default bio-system responses (similar to the toxic responses like sepsis to mass cell death triggered by the immune system)
                - the therapy is successful, but the patient's original dna was vulnerable to some other pathogen which they will probably encounter
                - the therapy is successful, but it erases information gained over the patient's lifetime (like immune memory cells or neurons tied to some microbiome change/state)
                - the therapy is successful, but the patient had habits that made their original dna sub-optimal as they didnt use their genes optimally, and their pre-therapy genes were actually more usable in an optimal way
                - the therapy is successful, but habitual/environmental changes could have replaced it with less effort or the pre-therapy genes had some unknown benefit or there was a way to trigger original dna copying without artificial therapy but we dont find it bc we focused on the artificial therapy
                - the therapy is successful, but bc of environmental conditions like medicine/pollution that cant be avoided, the patient's original dna is suboptimal compared to some other set of mutations or their pre-therapy illness state
                - the therapy would be successful, if not for other currently poorly understood components like dna fragments and metabolism and their interactions with dna edits
                - each change to the bio-system would have to either not impact required processes (like energy/enzymatic pathways) or change these processes to an alternative, which means an alternative is required, and there are some processes with no alternative in the bio-system, and these processes could easily be impacted negatively by some gene changes (or the patient would need some complementary treatment to handle these impacted pathways like blood cleaning processes to remove toxins from immune/cell cycles)
                - some changes to dna dont impact patients negatively, but these are probably relatively rare as they require some attributes like 'lack of interactivity with other components' and/or 'similarity to components which are already handled to some degree in some way', and most dna changes are unlikely to have these harmless attributes
                - dna processes have some requirement we dont know about, such as requiring 'some junk/repeated data or neurons or pathogens to help handle mutations' or some other dna edit process that we're unaware of, which is only a problem in some patients
                - mutations are inevitable without some extreme behavioral condition like extreme workouts/supplementation with some substance/rest/temperature, so some mutations will probably/inevitably re-occur, requiring re-treatment
                - mutations present in the host make it unclear/ambiguous what the original dna was, so the target optimal state is not determinable by seeking the original dna state, and a 'generally probable healthy state, which is healthy for most/other people or given some known optimal gene configurations' is a better goal to aim for
                - dna changes have impacts at different interaction levels (dna groups, chromosomes, dna fragments, dna-editing dna, jumping genes, etc) which are currently not predictable/known

        - alternate intents to aim for
            - a 'more general solution that helps most patients to some degree' is likely a better short-term target (which undoes a few common harmful mutations without harming most people) than aiming immediately for a general highly customizable solution that works well for each individual (which it may not currently be feasible to retrieve all the variable information required to implement such a solution)
                - similarly, a 'dna damage-reversal solution' that reverses disruptions to important/required processes, like solutions to 'soften arteries', 'lengthen telomeres', 'de-calcify calcium deposits', 'grow new cells to replace cells damaged irreversibly by acid' and other treatments for negative stress responses from dna damage, microbiome damage & other related problems, might be more effective short-term targets, to fix some negative side effects of dna damage rather than fixing the dna damage itself
                - similarly, a technology to 'prevent dna edits of some type or of any type' could be useful to maintain some health state or to maintain health once mutations are reversed or useful mutations are applied
            - finding mutations to protect against the more powerful/invincible viruses with more evolved self-sustaining dna might be a better goal as they have attributes in common like 'hiding mechanisms from the immune system' which can be targeted

        - implementation methods
            - finding the 'healthy gene sets required for a normal healthy bio-system in most people' and making sure dna edits dont change those required gene sets for health
                - similarly, finding mutations that help optimize existing systems, like 'gene sets that make antibodies more trivial to create and more optimal/effective to help the immune system'
            - finding mutations that mimic or match patterns of or are otherwise similar to 'natural healthy evolution dna edits (like those that improve stress-handling response or immune function)' as opposed to 'illness-causing dna edits' and restricting changes within the found range of dna edits on some graph where these regions of evolutionary or otherwise healthy mutations are adjacent
                - similarly, formatting genetic information on a useful high variation-capturing interface like the stress interface, so that some illness-causing mutation is adjacent to others bc of the bio-system stress they cause or the stress which is required for them to become a problem
            - some genetic changes can be 'tested' in the sense that side effects are probable/known to be 'increasing from zero' and 'initially non-fatal' to a degree that there is enough time to reverse them with an opposing or de-scaling mutation virus after if side effects indicate a negative health state change, or altered to an interim state that is less strong in the metric that caused the state change, to allow for a tuning process of changes during testing
            - finding stressors that reverse dna mutations (on the assumption that there is a stressor that can reverse any mutation) and find a sequence of stressors that is survivable in most patients for the illness-reversing mutations these stressor produce

       - as an alternative complex problem system example, similarly, the reason why 'racism' is an inaccurate descriptor is that there are more correct structures like 'self-similarity/approximation/historical/group/power dynamics' as capturing more variation than the specific structure of 'racism'

        - as an alternative complex problem system example, some interface structures (which are already known to be useful across problems and are defined in interface analysis) can be useful for finding errors
            - to find the error of a function like 'fulfill default intents', use an interface query such as:
                - 'at scale, what is the emergent output of a function (a function such as 'fulfill default intents') given some pattern (like 'contradiction of agent-agent intents (meaning death)') in some interaction level (like 'agent-agent interactions') when repeated, and what is the limit of this interaction (societal collapse from decreased population)'
                - when agents fulfill their default intents (some of which will inevitably contradict each other), and apply no other strategies, anarchy occurs and society collapses from the population decrease
            - this interface query would find an 'intent-contradiction error' in 'emergent output in agent-agent interactions' by applying the 'context' concept of the 'system' interface which checks for impact of a function in the system it is applied in ('context' here means measurable structures such as 'emergent output at scale when repeated to its limit')

        - as an alternative complex problem system example, 'competition' is sometimes an 'error' structure, such as in complex systems like those with rules designed to prevent the errors of competition such as the free market with regulations applied to protect privileged/predator agent incentives, which is the type of regulation that the free market produces
            - in cases where 'competition' leads to errors like 'false or otherwise trivial displays of virtues instead of actual contributions (like in an information economy such as the internet, where information is abundant and cheap and easily faked)', 'competition' can result in abundant false information where true information is drowned out by comparison
            - this can be derived by applying interface structures like 'incentives' (such as the 'incentive to falsify information'), 'incentives' being something that agents are likely to apply as 'default decision rules' to fulfill common/default intents like 'cut costs', which is a default intent in a 'competitive' context
                - the 'incentivized action' in this context is to 'falsify information', and in the specific context of a 'market that rewards virtues but doesnt test for them or require them', the specific incentive is to 'falsify information about virtue'
            - finding incentives as a default structure is as simple as applying the default interface 'cost-benefit' variable to determine 'common cost-benefit interaction variables (like the common "high benefit/lost cost ratio")'
            - what alternatives exist to correct this error structure?
                - a default 'opposite' structure of this error is: "apply 'any cost by default' to create problems to require thinking by default ('thinking' as 'applying more rounds of analysis') which is the opposite of 'acting on obvious default incentives'"

	- finding a function set that 'converts/connects interface structures to/with other interface structures' is a function set that can generate all other useful interface structures bc the primary interfaces already identify the primary directions of change (similar to cardinal directions) and functions that can generate one from the other can identify the full set as well as identifying the differentiating variables embedded on those interfaces
	    - interfaces can be defined alternately as 'structures which can cover/support/capture the information of reality like a continuous field/fabric'
	- for the intent of 'finding useful spaces'
	    - finding 'structures of related optimals' in a space of sub/optimals (a meaningful space, where every point is useful for some ratio of intents, similar to how 'useful structures' are meaningful in the sense of 'adjacently or directly implementing some intent') where some structure of optimals avoids errors above some threshold is useful to connect useful structures like variations of optimals
		- like how some structures which may seem required or otherwise optimal like the 'reward' in a 'reward function' may seem optimal, but the assignment of the reward may interfere with additional inference processes (a learning function with an understanding of meaning wouldnt need a test/reward function set to guide it)
		- related to how the 'reward isnt the point' but the point is 'developing a learning function or learning function-finding method that can learn anything', as nearly everything is both correct and incorrect in some way (the reward helps the learning function when its constructed a certain way, but it also takes resources and isnt the point of a learning function, and learning without a reward would be better if it aligns more with the point or otherwise is more adjacent to an accurate understanding of meaning, as a reward is an additional requirement that not every learning function would need), as everything can be connected using insights, so finding structures that connect structures of optimals (like a set of differences such as 'variables of different filters applied to sort before or halt a trial and error algorithm' to connect suboptimals or pairs of sub/optimals) is more useful than a function to find any one optimal, and given that anything is connectible, an optimal learning algorithm should be able to derive learning rules from anything (such as learning a generate/change, test, filter/reward function set or equivalently an insight set that is useful for solving problems, etc) to achieve meaningful understanding
		- other useful spaces include the 'space of complex structures' where equivalently complex structures (like complex inventions which are similarly useful or equivalent alternates) are mapped to or otherwise connected to each other, where solving for some complexity space means solving all the spaces more simple than it by default, as the complex components can be re-used to solve simpler problems but not all simpler problem components can be adjacently combined to solve the complex problems, like how time is asymmetric in that it favors a direction toward some structure like a ratio or interim point of organization/entropy or interactivity/variation, prompting questions like 'what asymmetries can offset a foundational symmetry like cpt symmetry' and 'is time a foundational symmetry allowing other symmetries to form, so there is no more efficient foundation to navigate to and no way to countradict/change it, or are there other foundational symmetries like "equivalent interchangeable alternates" which are stronger as a foundation than time'
		- a space of 'maximally different structures' (like independence, interactivity, organization) which can invalidate questions about variables of each maximally different structure is useful for adjacently generating new perspectives
		- similarly a space of 'most-used interface structures' (like a 'function generating primary abstract conceptual interactions (such as power interactions) in a system') is extremely useful
		- a space that organizes optimals/solutions in such a way that each optimal can be calculated with some simple function from another optimal (like a lattice where you can move a set amount to get to another point) is a useful organizing structure to search for (using some navigation function, like 'change each of the maximally different variables that describe/generate/determine the function while still maintaining a cross-interface structure')
		- a space of 'equivalent interchangeable alternates' (such as 'generative/determining/descriptive/representation' functions) where these equivalents are adjacent is useful for finding more adjacent structures from a given starting point
		- a method such as 'standard least squares linear regression' leaves out almost everything relevant about the universe to identify whether some variable set 'changes together' (are there more adjacent variables that can be generated from these variables as interim variables connecting them to y? is there randomness interfering with some variables? are some variables just common structures like obeying incentives, stress, or lies? do all variables 'change together' in some way once connected or formatted differently? what level of directness/independence of connectivity is being tested for by this method?)
		    - some subset in the space of all generated functions (under some number of steps, including 'identifying/generating new spaces allowing differences and making other differences obvious') is actually good at sorting and organizing this information in a useful way (identifying possible alternative relationships, identifying possible ambiguities/unknowns, identifying interference from other systems/randomness, identifying variable attributes like directness/independence, identifying useful insights to connect variables, identifying position in a system state sequence, identifying possible systems where these variables could exist and interact in this way, identifying maximally different variables in the input set)
		- identifying connective spaces to connect non-math structures (like language/system diagrams) with math structures (like sequences/networks) is useful for solving problems with existing resources (such as indicating language using a set of spaces where different sequence angles indicate some difference in a subset of language variables in sentence/paragraph/manifesto sequences, where the set of spaces determines independent complementary information about language)
		- finding a space of 'interaction levels where a function would be adjacently implementable' so that similar points on the space can find alternate interaction levels (of functions/variables to use as components to adjacently combine to find a function implementation)
		    - this considers the investment or 'amount of work/energy' invested in the work of constructing/finding a function, the work done in the final function logic itself, and the work of applying a function, as an alternate function attribute, similar to intent, and related to complexity but different from both, more related to the 'differences in the set of possible functions fulfilling an intent' and the 'differences created in functions by different amounts of work (low-effort vs. high effort functions)'
		- similarly, the 'space of all symmetries' is obviously useful for finding 'symmetries/similarities/differences/patterns/interactions of symmetries'
		    - when these spaces are graphed, queries like 'if you switched the values of all equivalent alternates (or other definitions of symmetries) in some specific case, would you occupy a different configuration of spacetime that is stable and are there reversible trajectories connecting these configurations' are more adjacently/obviously computable
	- finding all high-variation variables/functions and applying that as a base to describe other high-variation variables/functions
	- apply optimization interaction structures (like a 'set of game strategies that can result in a tie') as being useful structures to model other useful structures like 'interchangeable equivalent alternates'
	- apply the 'biggest differences in between known problems/solutions' as a set of useful differences to apply to model other differences which are likely to be more connectible than those differences
	- apply combinations of workflows to find other workflows, to find solution-finding methods
	- apply structures guaranteed to be relevant (like 'changes within x causal degrees') as a default set of structures likely to be useful in solving a problem, then apply other useful structures like 'combinations/input-output sequences' to those structures to find the useful set of structures to solve the problem
	- apply a sequence of these other implementation methods as a way of designing a path from mvp to final product
    - apply useful structures that are more adjacent to solutions than problems are, such as a structure including 'interactive structures' and 'useful descriptions of the problem'
        - the better explained a problem is with the best representation of it, the easier it is to solve it
    - finding structures that represent the most interface structures (like a 'high variation structure that represents various core interface structures like interactions and requirements and generative functions')
    - finding structures that describe most problems/solutions in terms of interface structures such as 'a breaking of an interface' or 'misrouting of randomness' or a 'structure of resolution between ambiguous alternates' and other useful interface descriptions of useful structures, so these can be re-applied or applied as defaults or core structures
    - similar to how solving a maze can involve applying these structures, as 'equivalent alternate' structures that can provide the same or similarly useful information in solving the maze, deriving other useful possible solutions is possible by applying useful concepts like 'incentives' (to derive basic structures likely to occur)
       - applying regular 'direction of motion' checks to make sure the agent is still traveling toward the goal and not repeating routes
       - applying standard maze configurations as possible alternatives to select from
       - solving for standard tricks to check for in a maze that are incentivized and therefore likely to be encountered, and solving for the solutions to those tricks
       - solving for interactive sequences of paths that coordinate/cancel each other and can/cannot exist in the same maze
       - solving for 'maximal filters' that can filter out possible mazes the most efficiently
       - apply reverse-engineering to find indicators of various possible sequences of end paths nearest the exit
       - these structures involve solving other problems than 'apply any route using trial & error as a base solution, and change it as you go' which is a default solution to solving a maze
       - solving problems like mazes is a proxy for solving other problems bc of the high variation captured in a maze, if the maze reflects realistic randomness and other variable interaction patterns enough, similar to solving other games if they reflect realistic structures enough
    - a set of useful descriptions of reality that have reasons why they could be true (like a 'calculator of efficient methods of preserving energy cycles' and a 'structure to support maximal variation/uncertainty (time)' and a 'stable alternate coexisting sequence (time) finder') are useful to apply as defaults and efficient structures and truth structures, as well as the variables creating uncertainties (like 'unknown beneficiaries of calculations' and the 'maximally different structure' and the 'most stable system that supports the most variation') in these descriptions and the variables between them (such as interface structures like different priorities), as well as the differences between the description and related structures like 'problems solved with that description' and 'priorities fulfilled by that description' which make the meaning and reality of each description more calculatable
    - apply useful structures like 'clarifying structures' which make something obvious similar to how certain filters make some solutions obvious such as 'standardizing' and 'difference-maximizing' filters make differences more obvious
        - for example, finding standardizing structures like 'matrixes' which are a useful format that is also useful for other intents like 'mapping sets of sequential operations' and 'reducing some function to a set of adjacent combinations once in that format' such as 'solving linear systems of equations' 
    - applying useful methods such as methods of deriving information about other useful structures such as 'types' (all members of a type have this attribute that defines the type) and connectivity (constant lines are formed by a type of addition so it makes sense that adding them which is applying more addition doesnt change their shape, as their shape is the product of addition, as opposed to an operation that adds a dimension or restricts range, which would be required to create different shapes)
        - finding the 'type' of an object gives almost free (low-cost) information about that object, bc of the information stored in the type, where the type acts like an interface that can support some variation within the definition
        - other low-cost, high information-producing structures (like abstract concepts such as balance, alternate definitions, interaction levels, similarity to some other known structure, useful filters like standards to derive maximal information, useful networks to know a position in which is a high-information structure in useful networks) can be derived in a similar way and prioritized as default structures to find
    - useful formats of structures similar to a standard network but different in a useful way like 'gap networks of connected empty shapes' to represent related structures like intents to fulfill with implementations filling the shapes, or 'state networks to model useful sequences/queries' or 'maps to model useful connections' or 'map networks to model different analysis perspectives' or 'interface networks to model bases that capture high variation'
    - a set of certainty/uncertainty pairs to apply as default problem/solution structures capturing high variation
    - build sets of 'loosely related possible' associations (which arent guaranteed by definitions but which are allowed) using definition-adjacent connections, like how the connection between variables 'constant' and 'constant squared' involves a definition involving 'multiplication', but also has related attributes that are outputs like curvature which are required by the definition and other connections which are not as relevant like 'constant preservation of data type between input/output as a scalar' which is a loosely associated connections rather than tightly bound by the definition, using these loose associations to discover new possible connections not explicitly defined but also not definitely restricted
    - find component functions of data set subsets and iterate applying these until a non-matching point is found outside of the acceptable error range, adding terms or alternate functions to process in parallel to handle new points where found outside the range
    - find subsets of the data set that shouldnt be reduced to a function bc of complexity and randomness and other factors likely to predict insufficient information or variable injection points or other uncertainties that cant be resolved, where other subsets of the function are clearly mappable to functions
    - find more useful structures to describe variable interactions such as 'variables (such as squares) creating requirements (such as required growth, as in positive or nonzero growth)' and 'variables creating other useful structures like embedded change (change on change, like exponential growth leading to the accretion of matter)' and inputs to these structures like 'equivalences in factors creating self-similarity (in multiplication/area inputs) leading to multiple differences in outputs (of multiplication, compared to adjacent inputs)' which are useful in their capturing of high variation (similarities/equivalences creating differences like multiple differences between change rates of adjacent inputs)
      - the core unit of maximized potential of a variable in its interactions in isolation of other variables (self-interactions) is the area that can be created by the unit of core maximally different interaction (multiplication) with itself, representing its interaction space in the 'adjacent sides of a rectangle' operation (multiplication), leading to its potential (in its possible range of impact, as squaring it is maximizing its differentiability) to influence probabilities
      - finding the important alternate sets of functions that lead to these important structures like 'required growth (a relevant structure of reality)' can generate a 'limit scaffold' ('the model must not contradict required growth of some structure that has growth as a requirement or other form of certainty, as in some variable in the model must not grow in a way that contradicts known required growth of some other variable') that represents 'points of impossibility' that should be used as filters to avoid when modeling reality
      - the intersection of 'generative scaffolds' and 'limit scaffolds' is a useful place to start modeling the dichotomy between certainty/uncertainty to explore, format, & filter the space between them that is allowed by reality
    - finding other useful representations of a function (such as a 'stack of squares of increasing side length' as a useful alternate representation of x-squared to represent the value of y in a clearer way that reflects the equivalence in multiplication inputs and the multiple differences created by the equivalence in multiple dimensions) which increase the relevance and meaning of a function representation, as its probable interactions with other functions is more clear given the core relevant differentiating attribute of equivalent factors
      - similarly, the equivalence in factors leading to a line with slope 1 (1,1, 2,2, 3,3 as inputs) is significant and indicates the relevance of x-squared as different from other functions and more relevant as a unit function of change and indicates the relevance to the output by its squared area created by the difference between these input pair points and the origin
    - applying rules to find highly useful structures like interfaces such as by asking questions in a sequence like:
    	- 'what are the maximal difference-capturing variables like change' (such as 'functions vs. constants') or 'what contradictions exist in useful structures like change' such as 'what changes dont change' and 'what are variables of changes that dont change (limited change around a symmetry)' and 'what changes unchanging variables (constants, symmetries)'
    - rules like "'what are not inputs' are also a cause of structures in addition to inputs" (bc resources not invested determine states of alternate functions) which is another reason to derive missing information (not just to determine what a function will likely do bc of some input but also what other functions could stop that function given missing inputs devoted to alternate functions)
    - an alternate implementation of a 'blur' algorithm (using the compression/filters/other interface structures involved in vision) to quickly determine trends in a data set is to sample the data set and evaluate each subset quickly to benefit from the overall impression of the emergent pattern visible across a sequence of alternate subsets, where the impression is formed by easily differentiated structures like 'border angles', 'densities', 'ranges', etc which are common across multiple subsets in the sequence, or to align subset data sets as a sequence or network of subsets (organized by some similarity), to make trends more easily identified
        - similarly iterating through data set subsets quickly (to efficiently store memory by identifying the most obvious repeated patterns) is another way to implement an algorithm similar to 'blur', if the neural network has the ability to derive 'overall impressions of a shape' to compress the data set rather than just simply storing the whole data set
        - similarly, identifying a regression function is possible by finding which functions seem to be in the center/average of a data set (as a proxy for their summarizing capacity for longer when rotations/angle changes are applied (to view it from a lower or higher position), a transform that doesnt change the summarizing capacity of a function immediately if its a good summarizing function, 'summarizing capacity' of a function being easily determined from 'impressions' achieved by low memory (obvious feature retention) or fast processing times ('blur' effect)
        - similarly, identifying a data set or regression function that is 'most similar to the origin data set' is trivial by applying some transforms to some maximally different or standard or randomly selected data sets and positioning them in a way that aligns with the original data set (like by aligning its ranges in a separate graph that is close enough to easily identify differences but obviously separable) so it's obvious whether the added data set/line is similar to the original data set (and therefore the regression line of the added data set can be used as an approximation), which applies the 'comparison' functionality of a symmetry to the concept of 'data sets/regression lines'
        - where the 'summarizing capacity' of a function describes how much info the summary preserves (such as info about limits/averages/densities/vectors describing variation from base functions/patterns/possible interactivity/adjacencies/errors/ratios/probabilities, etc) which are metadata about the data set that can be variables summarized by the data set as well as subsets of data set points can be summarized as components of the data set, these variables of the function being possible input components or output solution metrics or interim sub-intents of a regression algorithm, which can be used to determine all other algorithms
        - as a metaphor, the "'definition routes' that when combined say the same thing (in a different way)" is a good set of nodes on a language network to use as an approximation of the variables of regression algorithms (and problem-solving in general), as a stable interface structure that allows maximal variation (it will be similar to my set of verbs like find/build/derive and structures I have identified as useful and the interfaces Ive identified and so on), as different regression algorithms indicate a general summary of the input, which varies somewhat around the interface of the input data set, while "saying the same thing" in the sense of preserving info about the input and "saying it in a different way" such as by preserving different variables and applying different functions
            - a query to find the 'summaries that are the most similar, where the inputs are highly different' is a good way to find these nodes
        - relatedly, finding out if a function is a straight line or a wave function with very small magnitude (or another type of polynomial with difficult-to-measure incremental changes) is a matter of testing a subset of the data set for variation and extrapolating/expanding that pattern to the rest of the data set (as opposed to checking the whole data set), but this isnt a good way to find out if those wave patterns are errors or if errors or error-similar structures like waves vacillating around an average are a default component of reality, which is related to the problem of determining the level of specificity and discreteness to assess incremental changes of integrals and polynomials (waves being a default difference structure), but this problem can be addressed to some degree by applying scalars to magnify the subset to make differences from constants/waves more obvious, checking it for robustness by applying randomness/errors, and other methods of identifying a stable function
            - just like the 'required component of a curve' is a 'subset with count greater than 2', 'subset with count greater than 2' can be used as an 'adjacent input format' to speed up an algorithm to assess non-linearity of a data set regression function, just like other algorithms have input formats that are more optimal for the algorithm than other formats, and 'subset with count 2' dont have as much info embedded that could relate to non-linearity as a 'subset with count greater than 2'
            - related questions to this problem are 'at what interaction level do you zoom out/in to in order to describe the variable interactions adjacently describing the most variation, or otherwise in a useful way (until there is a clear summary line, until the points are obviously differentiable between point types, until the point connections are obviously differentiable between connection types, etc)', as changes in scale and distance from data set can either erase or magnify differences like non-linearity, just like changes in scale/distance from the data set can make similarities such as patterns like lines more obvious
        - finding out 'non-adjacent similarities that are relevant', such as how finding 'non-adjacent inputs with equivalent outputs' is useful bc it implies a 'horizontal line' structure might be relevant, relevant such as 'being a useful base solution function to apply changes to in order to find variation based on that line, or to use as a simplistic summary function', which can be framed as a 'high density output' with importance bc of the repetition of that output in some pattern like an interval indicating cyclical patterns, or like a ratio indicating commonness, either way indicating usefulness of that 'high density output' as a base to apply changes to or use as a simplistic summary function
            - relatedly, vertical lines (indicating an input that can become any value) are so unlikely in a data set that isnt random that they can be ruled out as improbable in most functions except where required or made probable by another route
        - calculating the function from a set of symmetry structures (local subset averages (most similar point or change from most similar change rates to other change rates), a set of inflection points (a change in change structure like charge of change rates), a set of peaks (change in direction of change rate)) or symmetry-limiting structures (extremes of a probable range representing limits) is trivial once a ratio of these are known given some complexity (& other metadata like input ranges) of the function
            - relatedly, another useful problem-solving intent in regression is 'finding asymmetries in peaks which are useful to know about using adjacent info, bc peak symmetry is a useful assumption when true to determine more/all of the function from knowing a subset and relatedly useful to know about when false to identify when implications/similarities are contradicted and not absolute'
            - formatting the average as being more similar to points or being more similar to point connections (or being different from randomness or extreme points) is a useful set of alternatives when finding inputs that are common across useful structures or finding useful formats for algorithms that use averages
    - given that the complexity in regression is caused by non-linear shapes as opposed to constant lines, finding which variable interactions exist (which input variables are exponents) and which cause these complexity structures (exponents of an input variable, constant coefficients of exponents, addition/multiplication with other exponent terms, etc) would enable removing those complexity structures to reveal a simpler form of the data set that is more easily condensed into a regression line or a base solution function to apply changes to (like how removing parabolas that are symmetrical around a clear or probable average is a trivial task that makes the average more obvious), such as applying inputs of complexity structures to the base solution function to specify the more general simpler function
        - this is related to workflows like 'remove variables until a simpler component function emerges'
        - this is also related to workflows like 'find a simpler combination of inputs preceding the original function/data set' which can be applied in another direction such as 'find the function interaction level where maximal differences begin to emerge' such as how combining functions by type (like step functions, wave or other polynomial functions, sequence functions, discrete functions, closed shape functions) can produce functions that represent a 'combination of these maximally different function types' which may be more useful at filtering a function solution set than other interaction levels, so this interaction level of function types may be a more useful place to start when finding a regression function to summarize points
            - relatedly, finding 'sets of useful interface queries that decompose most variation' can be as simple as finding sequences of interface structures like 'find common/powerful/probable/high-variation variable interactions that match common system interactions like common system errors or common differences from incentives/defaults' and 'apply the function type interaction level to decompose the remaining unknown variable interactions, once these common/powerful/probable/high-variation variables are known', sequences which could be generally useful in 'decomposing most variation across problems', which is a useful problem-solving intent
        - relatedly, finding algorithms to connect simplicity and complexity (and independence/dependence and unstable variable/stable constant), generating one from the other (like simple core functions that when repeated generate surprisingly sustainable/cascading and complex differences) are useful to decompose the likely starting point and simple/complex components of a system
            - like the simplicity in the central limit theorem generated from a complex combination of random variables, and the tendency of complex systems to neutralize each other's differences in some way when combined/repeated, for interactivity (as a complex system is less likely to be sustainable on its own when repeated), and the requirement of a variable like a symmetry in a simple function for it to easily generate complexity
    - given the possible set of interaction functions (direct connection, constant connection, side of an area representing a limit/border, error connection, random connection), use these interaction types as a variable to identify algorithms to classify connections between points based on their probability of being one of these interaction types and the line patterns that emerge given some set of interactions of a particular type pattern in some subset of the data set
        - for example, any two points in a data set are very unlikely to have a direct connection, so that should be used rarely
        - if a connection is confirmed or probable to be a direct connection, adjacent connections are less likely to be direct connections and other connections similar to the direct connection in other subsets are likelier to also be direct connections
        - if connections indicate low volatility, finding a connection with an extreme slope implying 'volatility' is less likely after some ratio of connections are tested/determined/assigned a probability of some interaction type
    - a function to find the simplest (or otherwise effective) polynomial to describe averages of local data set subsets (where angle of lower/upper borders and densities influence the average) is an example of a standard method that can be found with adjacent structures
    - changing the definition of useful structures like 'averages' to find alternate methods
    	- defining an average as a 'line that when changed the most compared to other functions, still fits within the boundaries described by upper/lower limits' points directly to a method to determine the average function fulfilling that definition 'find a subset of possible different functions to change, and changes that can be applied to these functions, and apply boundaries as limits to these changes'
    	- defining an average as a 'difference from extremes' or the 'usefulness of right triangles in finding average functions (and vectors applied to them to generate extremes) of a data set' points directly to methods like 'find angles applied to a possible average line that capture the highest variation in a data set, once possible extremes are known'
    - evaluating a function's 'differences from randomness' is another starting point to base changes on rather than basing them on an average bc its the 'opposite of the intended information' and is therefore similarly useful in that adjacency
        - other function bases include functions that 'connect non-adjacent subset averages', that 'connect adjacent subset averages', 'connect function upper/lower ranges', & other representative/summarizing functions
        - a random data set is not useful and is therefore useful to determine early on in calculations, just like function limits and patterns are useful to determine, and randomness may as well be an indicator of falsehood (as in 'something that needs to be changed in order to determine truth, like requirements/impossibilities') bc of this lack of usefulness
        - other known structures that are not useful are equally likely to use as bases for change, like how known useful structures like core components are, bc of their dichotomy in the certainty of their usefulness
        - applying 'common types/variables of functions that can form randomness' (such as 'contradicting/neutralizing change types that cancel each other out' or 'complementary opposite change types (like triangles which form a square)' or 'randomness-amplifying which doesnt change the randomness of the inputs' or 'symmetries like the equivalent weight of dice sides' or 'a high number of variables' or other functions that are likelier than average to create the requirement of randomness as 'even distribution of probable outputs') is useful as a way to 'determine probable randomness' or similarly to 'determine differences from randomness', 'remove/add randomness' and other intents related to randomness
    - building an interface structure of interacting rules to base changes on, like a 'set of requirements' (like how 'connecting components of a structure' is 'required' to 'form component connections to create that structure' by definition) or similarly a 'set of rules that are definitely impossible or not true' as a foundation for other changes (where possibilities exist between contradicting limits imposed by requirements) is a useful structure to start discovering new rules from
        - identifying rules that identify non-adjacent information required by a definition is similarly useful like definitions that identify adjacent/obvious information required by a definition are, like how numbers in a sequence like the set of integers are required to be one unit away from other integers and required to increase if its the set of positive integers, so knowing that a set is sorted in an order like this gives you information about all the numbers between two items in the set bc of the definition determining the set allowing functions like 'estimate where an item will be found to reduce the search space', or how the type of a number determines some of its known functions
    - a standard method to solve problems is framing them in terms of core structures like 'similarities and differences' (like 'similarities to constant representative/average lines', 'similarities to averages', or differences like 'difference-maximizing functions', 'highest angles connecting adjacent/similar subsets as a function that is almost guaranteed to be incorrect to base changes on'), then applying a function to 'determine which differences to resolve' (like 'differences between base functions like averages and alternate similarities like densities' or 'differences from highest angles connecting adjacent subsets and a base average/density-determining function')
        - this method finds the variables likeliest to be known/similar (or easily derived/predicted as adjacent to known variables) constants like 'averages' and 'local subsets (locality as an indicator of similarity)' and 'densities', and variables likeliest to be unknowns like 'alternate more complex functions with more variables' and applies similarities/differences to model those and find which differences are relevant to resolve (the differences that 'connect related similarities/differences', related by providing complementary information for intents like 'represent a data set', complementary information like 'base functions' and 'specifying differences customizing that base')
        - finding a function that models the 'maximally different local subsets of the data set' is a solution-finding method easily produced by this method
        - this is an implementation of the workflow involving 'finding matching structures based on common attributes' to model 'uncertainties within that structure' and connect these structures to problem/solution definitions
        - other differences to resolve could be the 'difference between the set of possible/probable functions and the set of best representative functions' or the 'difference between a set of probable determining variables and a probable representative function' or the 'difference between probable linear representative adjacent local function sets and the non-linear variants that are better representatives'
        - this is related to a structure like 'applying variables to a structure like a "set of maximally different angles" applied as a symmetry to find the angle set that hits (or alternatively/equivalently approaches) the most data points when the angle set is changed the least (like rotated, shifted, scaled, etc)', since a 'line that approaches the direction of or is adjacent to the data point densities' is similarly useful as a 'line that intersects some ratio of points', as a 'representative function' isnt required to intersect with any data points, so that intersection is a variable that can change in the solution-finding method, as well as other properties not required by the definition
        - this formats structures in a way that makes it adjacent to identify variables and sources of variation (like rules like 'sudden constants/similarities that enable other changes to begin by providing a foundation like a barrier/limit for differences are a good way to identify interface variables in systems'), which is why the primary interfaces are useful in the first place (they allow highlighting uncertain differences by applying certain similarities/differences through standardizing/similarizing to fundamental/core unchanging variables like 'cause' as in embedded variables of that variable like 'causal degree' that support/describe/limit other variation and otherwise fulfill intents related to variation the most completely)
            - relatedly, identifying the most powerful variables as the biggest sources of error in a particular format, such as 'causal position' being a source of error in the causal network format (such as how an input might be an output of the output but it could seem like an input in some false similarity errors) and the 'connection function' in the network format (such as how some changes can seem adjacent/probable in one network format but its a coincidence, where some other network format is more reliable at making those connections adjacent)
            - relatedly, the primary interfaces are also useful for being based on the 'reasons' why a structure may be relevant to another structure (it is caused/changed/allowed/required/intended by other structures, it is useful to or interactive with other structures, it is a variation (as in a definition route) of other structures like concepts, etc) which are united on the 'meaning' interface (determining relevance/usefulness of structures to each other)
            - calculating the structures that are not in a data set but which could be relevant based on other structures that filter structure combinations like probability/similarity/commonness is useful as an intent to predict possible real structures that will be found in future data sets (calculating uncertain differences before theyre a problem/before theyre real)
        - this is related to other workflows like 'find maximum differences (what something is not/find the opposite of something to find limits of what it is)' by asking questions like 'what is not cause' (with answers such as 'an event that follows another event is not necessarily a causal sequence bc they may be so indirectly connectible that they are effectively independent') to find useful structures like the limits of the causal interface and how it interacts with other interfaces like meaning (such as how 'events that follow each other in time may be causally separable and arent required to occur in the same system or detectably influence each other')
            - similarly, finding why structures would not interact (such as how 'some structures cant detect/measure other structures, and therefore cant use them as inputs') is useful as a filter of 'meaning' interface logic
    - a unifying function of the various representations of a function where the representations are variants supported by the unifying function is likelier to represent the function the best
        - similarly, a unifying structure that supports various representations (like a 'set of maximally different directions', a 'set of reflective mirrors as polygon sides capable of producing different variants of the same information', a 'network of foundation structures around which maximal changes are supported which can coordinate', a 'set of filters capable of filtering the highest ratio of solution sets with the highest similar degrees of accuracy', a 'set of overlapping shapes with a common center (of common components) that model reality with similar accuracy', a 'set of connections between common high variance-capturing structures like maps/filters/networks') of the interface network (in various perspectives that filter it) is likelier to best represent the interface network
        - finding a useful 'sequence of filters' is useful as a good way to avoid problems of assuming too much & other basic errors of bias, like by applying "possible, known, required, probable, computable, measurable/testable, usable, & realistic" structures early on in the filter sequence
        - similarly, a unifying function of solution metrics (efficiency, accuracy, generalizability, flexibility) is a useful base to apply changes to in order to determine variables of solution-finding methods
    - finding useful structures to combine as defaults is useful, such as how 'symmetries', 'fractals', 'randomness as a limiting counter-structure', and 'right angles' are useful as core structures to describe a high degree of changes bc of their definitions ('applying fractals to changes in the direction of a right angle based on a symmetry up to the limiting point where additional changes appear random in their accuracy at describing change' can describe probable changes around that symmetry) bc of the relations between their definitions ('symmetries' and 'fractals' both having a 'common base (of a "self") for change, and a limit on changes to that base' in common, so applying these in the same structure benefits from their common symmetry in their definition and applies changes to this symmetry in their definitions, and 'fractals' further fulfills other attributes of symmetries like a 'limited change, as fractals converge')
        - similarly other examples like 'why i is a relevant number to rotations' given its adjacent concepts which make this functionality probable or inevitable (not only its allowance by definitions, but also possibly its difference in ability to produce a difference in sign/direction from the origin, its core operation of 'multiplication' being a definition of the components of a type of n-dimensional change, 'multiplication' as relevant to angles through creation of closed cornered shapes, a unit of change that is relevant to rotation, the relevance of sign changes to wave functions which are relevant to circles, etc, which are useful changes for connecting more directly relevant structures to rotations like pi), in its function connecting two other structures of rotation such as e, a structure related to spirals (a structure with a useful equivalence in its change rate ratios of adjacent compounding) and pi, a structure that acts like an origin/symmetry of external/internal spirals (trending toward polynomials in the external direction and trending toward individual points in the other direction) and would be a spiral but is missing the 'change rate increase' to create a different equivalence than a spiral has (a change rate equivalence, that creates the property of 'closedness' in the circle as opposed to the spiral but is sufficiently different from a 'cornered shape' in either the internal/external direction so as to justify its own definition separate from spirals or cornered shapes, despite being equidistant from these and other and unknown structures in their definitions)
    - mapping problems to more defined fields like highly structural creative industries such as 'music' to find concept mappings that are more easily determined like how finding the rule 'intelligent goodness is more difficult than obvious/complicated wrongness or obvious goodness' by applying the clear definitions in music of "obvious rights/wrongs like compliance with major/minor chords/notes or compliance with patterns" and how finding the intelligent goodness requires knowing the obvious errors it avoids like incentives like 'cheap rewards from any difference, even wrong differences' as its easier to create a new minor song than to create a new complicated but good song, as the range of possible solutions is narrower but can still host complexity/variation that intelligence could survive in, and complicated goodness is more complicated than complicated wrongness bc of the additional problem of the limited range that requires creativity to sustain intelligence in, a rule that would help avoid errors of over-simplification, prioritizing any difference, and avoid obvious errors as well as errors that create changes that violate a solution structure like a range of good solutions, and would incentivize finding high-variation variables sooner (to stay within the limited range) than prioritizing wrongness would
    	- finding a corresponding physics version of a useful structure is a good filter to apply when determining useful structures or other structures to apply as defaults bc these are likelier to be functionally useful if not more plausible, realistic, or possible
    	- example metaphor: gravity between 'equivalent alternates' (such as alternative theories) is weak but enough to keep them in the same definition (allows 'aggregation' to occur)
    	- other example metaphors which are useful in the sense of being evocative or otherwise useful for calculating something: 'supersymmetry' and 'interface network', 'string theory' and 'variables as waves, as reality units (possibly related to twistors/spinors)', 'isomorphisms retaining histories/inputs' as some definition of a 'wormhole', the 'jacobi identity' (and other structures of inequality like asymmetries) as an example of a 'way of determining what structures will accrete in some definition of unidirectional time (like to determine what will become matter)', 'commutativity' and 'equivalences in quantum superposition probabilities', 'invariance attributes like associativity/commutativity being a useful structure as a base to apply other changes to, to base other more speculative variables on', 'non-orientability and CFT violation', 'whether representations of reality form reality, to the extent that it can collapse/expand into or be based on other representations as those become more energy efficient, like in a cycle of different representations', etc
    		- related questions: 
    			- are all measurable invariance attributes components of reality or are some of them the basis of some reality and others the basis of another like points on a lattice that can be a foundation of reality, where points in between are not, so travel between them could only involve motion in the transform producing either from the other
    			- which symmetries are foundational and absolute in the math interface? which symmetries (like abstraction as filter or map, connecting similar concepts like energy/variation) connect these absolute math symmetries to physics symmetries? which symmetries allow maximal differences to develop? what symmetries exist in reality (such as symmetries across space-time states preserving potential energy or entropy, so that we only have access to functions within a probable range area governed by that metric)?
    		- the usefulness of these metaphors depends on the variability between 'definitely possible' and 'definitely not possible' (plausible, logical, not definitely impossible, evocative, suggestive/implicative, conditional, etc), so that a tool to 'traverse similarities and apply maximal differences to find structures like requirements, symmetries, concepts, and limits' can benefit from embedded but not articulated useful variables/structures such as connection in language (superset of definitions containing a subset of math definitions)
    	- similarly, other metaphors include how the number-based spaces can be extended to apply to number types (like a 'prime-based space'), how a 'complex number-based space as a possible structure of reality' has a corrollary in the insight that "almost every fact has a counter-fact (similar to paradoxes with local contradictions across statements) where it is related to its equally legitimate opposite (where it's not true), where opposites are allowed by definitions (similar to manifolds that can seem like extremely different objects that contradict each other absolutely but are actually related consistently by some common structure)", where alternate possible spaces may represent/embed other possible insights as their generative/limiting/determining functions, and which may intersect/overlap in a space of these insights (which has a corrollary with quantum field theory), etc
    	- 'time travel' could be possible in the sense that realizing things faster and being able to store/determine/compress more information (which increases the potential time available for the realizer, by slowing down time for them, as they have more functions/variation than other people) can put everyone else in the past relatively by decreasing their relative rate of change, but only while the realizer continues to do so and if they can reverse that change
    	- 'increasing similarity in the sense of synchronicity' by giving everyone the same perspective (like by giving them a maximally different structure to handle problems with) is another function of putting agents in the same space-time or on the same timeline (converging timelines and integrating/connecting spacetimes)
    - finding solution functions for possible known errors to existing methods, like how 'standard neural networks' can have an error of 'finding a different function of different components for each input/output pair' or 'finding overly simple functions that re-use the same (or otherwise simple) components the most but dont handle extreme/new contradictory cases' or 'finding all possible incremental components of some size that could differentiate some outputs and removing some of the less useful/adjacent/common incremental components'
    - identifying interface query intents fulfilled by known useful structures (like how 'cellular automata' and similarly 'standard neural networks' are useful for interface queries such as 'finding maximal differences generatable by one logical/change unit on its own, to identify where a seemingly complex phenomenon can be identified by one variable (the logical unit), thereby reducing the complexity of high variation data sets or the complexity of finding variable interactions between variables of high variation data sets')
    - neural networks may have an error created by the 'sequence of training data', in which earlier training data influences the final solution function disproportionately to its utility value
        - finding the worst case scenario where each training algorithm could miss the most obvious (or otherwise useful) alternate/absolute optimal (such as with gradient descent) bc of the order of training data and how to correct these problems (such as 'start training multiple models at multiple different points and attempt to integrate/converge to an absolute optimal given their change types, like pursuing only those descents where lower values are clearly identified or otherwise where lower values continue to be possible')
        - abstracting this workflow to 'find common variables like "robustness to order changes" that are highly differentiating in math fields/functions, then apply these common variables to neural networks/regression algorithm to check for variation resulting from these variables'
        - allowing variable interactions identified in later training data (like more foundational base symmetries) to replace/change variable interaction structures (like an emergent lesser symmetry already identified in earlier training data) may be more complicated than just applying PDEs to assign changes to specific weights (like by integrating the 'weights of the lesser symmetry' with the 'structures like limits/invalidations of the lesser symmetry allowed/required by the base symmetry' on which it depends, such as by consolidating weights indicating the lesser symmetry variables into one more powerful variable on the base symmetry)
        - having an 'update function' that handles updating the weights in each of these worst-case variable interaction cases (or the most erroneous or most common cases), such as where 'the final training input indicates the base symmetry that is extremely different from the lesser symmetry already identified'
        - the reason interface analysis is so powerful is that it acts like the 'base symmetry (or hyper/metagraph) of all base symmetries (or hyper/metagraphs)', so having a neural network that tests interface analysis variables as a prioritized structure and integrates them into update functions is more powerful than other neural networks
    - 'minimize extreme errors of a solution function' and 'maximize data set coverage of a solution function' are both 'alternative contradictory' intents to solve the 'find a regression function' problem, which offer some degree of 'complementary info' rather than 'definitely overlapping or equivalent info', as the 'extreme errors like missing an entire variable or missing an outlier' and the 'maximum data set coverage' have no guaranteed overlap, as a 'maximum data set coverage' could easily exclude extreme values or other sources of extreme error like variables which are easily missed if some subset is selected
    - an example of specific simplifications to solving prediction problems can involve avoiding known suboptimals/errors/violations of requirements by identifying & applying differences to those and allowing all other variation to develop, such as predicting a subset of just the worst case scenarios by applying extremes to increasingly high variation variables (technological development in some direction like electricity/automation/chemical printing/speed/compression/computation) and identifying opposing variables (particle accelerators/generators which can modify components of or interactors with electricity, software likely to be used with electricity, & quantum technological development which can modify components of electricity can oppose a possible error possible with electricity technology in an extreme such as in a concentration of one position, 'if energy technology or energy technology variation (innovation) was concentrated in one position by some entity, what could oppose/change it'), and otherwise modifying high variation variables ('what are all the worst case combinations of high variation variables with other high variation variables like quantum computers and neural networks'), as if variables dont cause high variation, they can sometimes be ignored in some cases until they indicate change in the direction of causing high variation, as knowing the 'structures like patterns of variable development such as patterns of large-scale errors' is more useful than solving for every possible variation interaction in cases like with computation limits, similar to how 'limit change patterns' are also useful in decomposing all variation
        - this can generate possible innovation intents like 'encrypt physical molecules so they cant be read', 'check if enough particle changes occur in a sufficiently continuous area, whether different types/configurations of gravity can emerge', 'inject variables in reality in a way that creates gravity, if uncertainties attract gravity to require interactivity to develop to handle the increase in variation without destroying the foundational structure (create a structure like a black hole, an isolated uncertainty cascade)', 'check if there are different units of spacetime like an opposing fact/falsehood pair or a pair of space-time states connected in a sequence or the interim structure of other suggested units like a wave/particle', 'speed up information travel/acceleration technology so farther information can be read and used in predictions more quickly (such as by directing radiation/randomness wherever its not visibly/directly reflected back at us by hitting a structure, so that it hits something we cannot measure and therefore is likelier to encounter maximal differences if they exist, which are likely to respond back as intelligence sources and improve our rate of technological advancement, or similarly direct radiation at inputs to these sources which is likelier to be findable and is similarly likely to get a response as well as likelier to incentivize organization/connection and lead to an increase in the power of radiation to cover distances as those will be reduced by this connection and therefore increase its speed), or similarly direct radiation in known stable ways that create maximal differences here to attract other maximal differences as intelligence sources who require differences to solve their problems' and therefore the direction that changes (innovation) may be applied to fulfill those intents, as measurable intents are likelier to be focused on & fulfilled, and relatedly 'horizontal innovation' can also be incentivized to connect these innovations in different directions on a new interaction layer
        - conceptual math is particularly useful here (not just 'add a concept to another in a simple combination/set structure') but entailing all of the possible useful interactions between concepts
            - if you can run queries like 'build a new (not already known) possible interaction type between these concepts which could be true' and 'what are the probable error and limits of the interactions between these concepts' and 'which concepts are more interactive with these concepts' and 'which concepts reduce the interactions of these concepts', you are applying conceptual math, as opposed to just stringing words together in a simple combination/set
            - 'what are the possible interactions of interface variables, such as some areas of powerful developing technologies (like batteries, quantum computers, encryption, and 3-d printers)'
    
    - identifying the few 'specific problems to solve, which can be determining of all other structures' is useful for reducing the required computations to apply problem-solving methods to, such as how identifying some variable interactions is more useful (like eigenvectors/eigenvalues, or energy/entropy/potential) than other variable interactions, which can be optimized in their usefulness by identifying how problems are related so once these variable interactions are found, the other problems are adjacently solved (identifying the 'problem network' that will fall to these variable interaction functions)
    
    - identifying useful structures (like 'math structures' such as 'independence') which act as proxies of other useful structures (like the 'maximally different structure')
        - for example, finding the 'most independent variables' is useful for intents like 'creating randomness', 'identifying organization', 'identifying interactive variables', 'identifying non/correlating variables', as 'independent variables' are useful as possible interactive components of systems (as opposed to the self-interactions of dependent variables which are by definition connected), as this intent is more mathematical and measurable given the more well-defined definition of 'independence' compared to other structures (like the 'most complex structure' or the 'most different structure' which may not specify how to determine differences, but can be more usefully framed as being the most consistent structure creatable with independent variables), as 'independent variables' by definition come from 'different systems' where they can be isolated, and finding the most independent structures which involve the most variables from the most different systems is likely to be useful in generating related structure like the maximally different structure
        - this is another example of why 'cross-interface structures' are more useful than other structures, such as when a specific math structure is more useful for calculations so knowing concept-math maps is useful for that intent
    
    - identifying interim structures between defined structures is useful where existing definitions dont fulfill intents optimally
        - identifying 'complementary filter sets' is useful as they are an information format that can be adjacently connected to the interface definition, as a set of filters that captures complementary information can offer a more complete ratio of information capture than other structures, and filters are adjacently connectible to interfaces in that they highlight specific differences within a similarity limit
        - similarly the 'system layer diagram' is useful in the same way, as being adjacently connectible to the interface definition (the set of component interaction layers describe a system with sufficient isolation and interactivity to capture high variation and fulfill problem-solving intents like 'build/break' such as when applied to problem structures like 'break a problem into sub-problems' or 'build a solution')
        - both of these structures are useful for very different intents
        	- for example, the system layer diagram is also useful for indicating overlapping set memberships and can adjacently generated other useful diagrams like venn diagrams, and occurs in math contexts like 'lie group & quaternion projections'
        	- 'complementary filter sets' can also identify 'complementary variables that frequently co-occur in systems'

    - identifying functions in between error functions and solution functions has some known structures to help determine the interim space, which can be applied as default bases to change to determine interim functions which are more useful than either extreme
        - for example, the functions that produce errors every time except in some specific known/derivable condition such as 'when the solution is given to these functions directly as an input' (identity functions, simple functions) are known to be sub-optimal for most intents, and the functions that are optimal in some way similarly have structures which differentiate them from these known sub-optimal functions (though they are high-cost in some way as well, such as 'check every possibility'), and in between are functions similar/different to both which can be derived
        - specific problem format (benefit/cost problem format) difference-resolution rules like 'differences between cost-handling demand and cost-handler supply' like 'dont centralize costs/dependencies on one cost-handler/independent function, which is known to destroy the value/structure of that independent structure' are similarly known sub-optimal functions which can be altered to produce more probably optimal functions ('instead apply a different structure such as, distribute costs with a non-centralized method such as distributing them evenly')
            
    - identifying structures that are useful in the absence of some information that is probable to be missing across problems
        - when 'maps/fields of similarly sub/optimal solutions' arent available, which simplifies the problem of finding new/optimal solutions, bc simple position changes in some known optimal (or known different-from-suboptimal) direction can produce a better solution, what other variables can be applied instead of variables in that structure
            - apply difference/diversification structures (try multiple maximally different solutions and apply adjacent changes to those until some subsets are filtered out)
        - when a data set isnt graphable bc of its complexity/size, which variables can be applied to determine/visualize it
            - apply standardization, to identify highly differentiating variables on highly similarizing variables to determine/visualize the differences from how these 'differences in similarities' differ from other known structures of 'differences in similarities')
            - identify truth structures, as in some subset/variable of it that probably retains/stores a high degree of variation and determining/visualizing that instead
        - relatedly, what does it mean to have a condition like 'not having a map of similarly sub/optimal solutions' (meaning 'how does it relate to problem/solution structures like available resources/inputs or errors & the solutions to those errors')
            - it means either one of the following
                - the input variables are incomplete/not the correct variables/otherwise missing (the solution involves finding/deriving/generating new info, formatting variables, etc)
                - that the solution field is volatile, so traversing it is not optimally useful (the solution involves de-volatilizing a function, so that adjacent input changes create adjacent output changes)

    - identifying specific maps to connect known useful interface structures
        - for example, knowing which variables (like an extreme difference in outputs of adjacent input) produce which attributes (descriptive attributes like volatility) is useful for mapping these structures/intents
        - similarly, knowing functions which oppose/offset this attribute (like a function to make a graph un-volatile) can be checked for in the data set to predict that attribute in the output

    - identifying common variables in the most useful structures, as a useful intent that can fulfill other problem-solving intents adjacently
        - for example, the most valuable structures are 'cross-interface structures', 'simple connections between complex structures', 'maximally similarity-differentiating or difference-similarizing structures', and other structures which are unique in that they connect highly different/complex structures using adjacent functions
        - another common factor between these is that they reduce dichotomies to an interim structure (the 'most complex simplicity', the 'most uncertain certainty', the 'most variable constant or most constant variable', the 'most different similarity', and other extremes testing the limits of the core definition at the root of these dichotomies) that is more powerful through involving these extreme differences in the same structure, and offering an interim point at which to base changes on to generate the other points
        - identifying the 'points between other useful structures' that can act like averages/symmetries to generate the other useful structures is another useful intent
            - this is related to finding an interim/average/origin that is useful in generating other points trivially, such as with applying a rotation or applying a vector of the same length
        - identifying the connections between useful structures (like 'different intents fulfilled by the same function' or 'all the different ways to use a function' and 'different functions that can fulfill the same intent' or 'all the different ways to connect some structures of the intent') that make some useful intent trivial, connections such as 'similar functions' and 'similar intents' and 'similar functions/intents', where these similarities (yet to be identified, such as similarity in 'information-preservation' intent fulfillment of a function, or other useful metrics like 'specificity of the intent resulting in a highly filtered possible solution function space') offer alternate connection components to form routes to fulfill some intent, as opposed to finding a route in some default/standard space like connecting input/output points
    - identifying the core variables of physics/math can be used to identify other primary interfaces
        - for example, the 'lagrangian' (to measure potential, such as 'adjacent possibilities', or 'all possibilities'), jacobian or relatedly commutativity (to identify sequences/order), lie algebra (to identify symmetries), and other default math/physics structures can be used to identify other primary interfaces and useful interface structures
        - similarly, the concept of 'gauge-fixing' (as a 'disambiguation' structure through 'difference from some embedded symmetry, like a zero/non-zero sloped line connecting rotational states') is related to useful structures in the 'regression' problem space, such as insights/rules like that 'any line crossing a general data set area can be useful in determining the actual average line (reducing the set of functions that could be equally valid, acting like a filter of possible functions)'
        - 'non-local causality' and 'quantum entanglement' (connected non-local nodes, connected by some function like equivalence, to account for interface structures like 'false structural similarities' that occur in real systems but arent semantically relevant (the reason for the similarity is different and independent)) are examples of physics structures that can be injected into neural network algorithms
        - similarly, the concept of 'diffusion/dispersion' comes with the concept of a 'source point' which leads to the other points, which is useful as an alternative to an 'average/interim/hub' point to apply the concept of 'state' to a data set
            - relatedly, the concept of 'heat' corresponds to the concepts of 'potential energy', 'entropy', and 'interactivity'
        - similarly, the math structure of an 'affinity' which 'preserves some similarity metric (like parallelism but not necessarily distances/angles)', or an 'isometry' which 'preserves some similarity metric (like distance between structures)' is useful as a 'variation' of the 'structure of an interface', an interface being that which reduces (rather than preserves) the distance between some similar structures to connect, to make some intents trivial (such as 'connect differences' and 'differentiate similarities'), and relatedly differentiates some other structures to differ (by identifying structures like symmetries and applying those as default interaction objects)
        - similarly, the math structure of a 'contraction mapping' that reduces distance between all points is related to interface analysis, in that its default structures reduce the work a query has to do to connect problem inputs and solution outputs, as it makes all useful structures adjacent to connect, so that trivial queries on the interface network solve problems formatted as differences on the interface
        - similarly, the concept of a 'topology that can be changed to make any of its structures adjacent without altering its properties (like storing maximal differences)' is a useful math concept that can provide structure to filter the searched set of possible solution structures when searching for the 'maximally different structure'
        - the 'stress-energy tensor' of physics is analogous to the intent/function cross-interface structure (the problem/intent/usage of a function and its functionality, as in its ability to respond to the problem represented by the intent)
            - relatedly, the derivation of the 'stress-energy tensor' indicates a sequence of multiple expansions, substitutions with equivalences, groupings, change rates, and orderings, as well as standardization by a similarity in common (coefficient) to reveal an irreducibility/isolatability/component (energy density of the system) or a target defined structure (the 'divergence'), which is related to the core functions which are re-used across problems, changing core variables like order/set/terms/complexity with a set of common operations to produce most results adjacently, and which follows a general 'expand/generate and filter' solution automation workflow
            - relatedly, the mathematical 'group' is analogous to the 'structure-function cross-interface structure' as useful for forming other function/structure pairs or the 'function (as a unit of change, analogous to a variable)-constant dichotomy' that is so useful across problems to produce other un/certainty pairs
                - relatedly, the idea of finding the 'interim' (or other equivalence like an 'average') point between two structures that makes both of them trivially generatable with the same operation applied to the interim point as a useful compression structure of both points
                - relatedly, the 'algebraic structure' that is a 'set/operation/axioms', as a variant of the highly useful default 'generate/filter' problem-solving workflow
                - relatedly, 'lattices' as a structure to identify paths between points using the same vector as a unit of change
        - the 'frame of reference' as an 'observer and a coordinate system' in physics is related to the interface analysis concept of a 'perspective' (as a filter with priorities that highlight/clarify different structures)
        - similarly, the 'momentum' variable shows up in physics frequently and indicates the 'incentive' variable that determines many interactions
        - similarly, the 'frequency' variable shows up in physics frequently and indicates the 'commonness' attribute that can lead to 'probable structures'
            - relatedly, the reason for using the 'wave function' as a unit of polynomial functions and using it as a unit of spacetime, as a high variation structure with a symmetry and a high variation interim interactive structure, respectively
        - similarly, the 'p-adic numbers' offer a mathematical variant of the useful structure of a 'similarizing difference' by applying a similarity to the difference between numbers, changing which numbers are adjacent (highly different numbers rather than the standard of highly similar numbers, by applying a different similarity 'extremity of the high value of the power')
        - similarly, the 'imaginary numbers' of math reflect truths of reality like 'every true statement has an opposing statement that is true to some degree or in some way or in some context' and physics structures like matter/anti-matter as well as the importance & commonness of dichotomies and spectrums as important/powerful variables, which is an adjacent transform, once that insight is known & applied and those structures like dichotomies are known, just like a 'ghost of a definition' which retains the bare minimum 'skeleton' required for the definition to still be true, but is still so different bc of the variation allowed within the definition that it seems to contradict the definition, these overlapping/integrated definition 'ghosts' determining the potential and 'probabilities of resolution (into a structure/state)' of a definition
            - relevantly, applying 'alignments' to preserve connections (like an 'opposite definitive' space of 2-d euclidean space to graph imaginary values, or a 'system' space where both values are definable/graphable but the 'opposite' structure of their connection is maintained, such as that they move in opposite directions, such as an 'opposite of space-time' where the definitions still hold so that 2-d imaginary structures can be applied to physical structures described with the opposite of imaginary numbers, as opposed to using dotted lines to indicate imaginary variables, like an 'inverse space with a negative component indicating the imaginary component, in the position of an exponent coefficient, or a standard vector space indicating change direction components, or a space preserving the positions/connections between roots of real-valued variant transforms like even exponents, with the odd exponents positioned by definition in between, while mapping the powered spaces into the unit space')
                - an 'inverse' originally framed as a possibly useful opposite structure (an opposite as in a orthogonal root or an inverse) is particularly interesting given the 'inverse of x' whose integral area equal to 1 identifies e, which together with x, acts similar to e^i x pi in euler's equation as a factor of -1, as if an 'inverse/e taken to a power of (root-type/i x rotation-type/pi)', given that when multiplied by itself i x pi times it generates -1 (i squared), which isnt quite the case (these arent directly mappable to the concepts of inverse/root/rotation)
                    - but this inverse structure is still related to i (however its only when e is in the position of the base multiplied by itself i x pi times, that i and pi act similarly as i squared, and i is the cofactor of the power rather than the base, and the function is not a square of equivalent factors, but the factors are alternate factors of -1 than i)
                - why look for the 'area equal to 1' (in relation to 1, as in above 1) in a function like the inverse function at all?
                    - finding equivalences, especially in units like 'unit areas (the unit square having side length 1)', is useful as a comparison metric between functions, to allow other intents like 'converting/mapping between functions'
                    - the 'unit area of side length 1' mapped to other functions is by definition related to the unit exponent (the square of 1 and -1, produced by i and 1 respectively)
                    - so you might look for the 'unit area equal to 1' in a function (like the inverse function) bc you know that the unit exponent (one squared, negative one squared) is related to i, and you know units are important, and the 'unit exponent' is a core/important unit
                - why look at the 'inverse function 1/x' at all when examining i
                    - x and y represent different sets of 'factors of 1' in that function, which are related to i's opposite unit (1)
                - what other differences should be examined
                    - the mapping to unit areas of a four-sided shape with all curved edges could be relevant as well, as an interim structure between a square, the area under a function like the inverse, and a circle, and how these structures are related to other interim structures like spirals (which could be useful for related intents like 'identifying change on the outer loop according to a base change of the inner loop')
                    - finding the non-obvious sums that generate i * pi which could be equal and interesting (meaning adjacent/relevant to other intents) as alternate square roots of i when positioned as powers of e
                        - what sets of values of e^x create a unit area of 1 when multiplied and other related constants, and how do these values and the powers of e that created them interact with other relevant structures to euler's equation
                    - finding alternate operation sets that generate useful constants (what else other than roots/opposites/rotations/inverses can be combined with simple operations like exponents/coefficients to generate useful constants like i/pi/e, such as waves vacillating between i/1 or the unit circle)
                - the question is 'what combinations of e and pi can act like i' which can be translated to 'what combinations of alternate unit areas in the inverse function (e) and the rotation constant (pi) can produce an opposite type such as the opposite of the unit root 1 (i)' (why do these 'inverse/rotation' difference types from e and pi produce the 'opposite difference type' that i adds) and 'in what structure/position do these constants act like i (deriving eulers equation)'
                - this is another example of how an 'evocative' structure such as an 'inverse' that is evocative of i (that can not be defined as directly equivalent to some relevant term) can still be useful (can be related another way, like an alternate function/format that identifies the same connection function/variable), similar to how 'circles' are evocative of 'primes' given their false appearance of randomness
                - what does it mean to say that e has i dimensions of itself (e is multiplied by itself i times) and e has pi dimensions of itself (e is multiplied by itself pi times)
                    - related question: is there an interim value between i and pi definitively that makes the equation an alternate root of -1? (e multiplied by itself x times where x is definitely in between pi and i)
            - relatedly, questions like 'what are the types of time in between imaginary and real-valued time' and 'what are the limits of imaginary time in supporting a higher proportion of structures within the time definition than is implied by current understanding of definitions' and 'what connections are required/possible between different types of time and what interaction levels use these types of time as defaults' and 'what variables/functions determine/maintain/require the interface between reality (stable/constant structures emerging from quantum physics interactions) and potential (quantum physics components), is reality similar to seeing a cross-section of a wave that is at a interval where perception can occur and the interval points can be connected and change can be synchronized across those point connections, where other realities are not on the detectable spectrum at that cross-section without making inferences by calculating gradients to adjacent detectable points' are useful as evocative thought experiments and interface queries to find useful structures
        - similarly, the use of 'light' as a metaphor for 'varition' is useful for determining core variables of change (like angles of possible motion in a sequence) to determine valid and realistic methods of inference (like 'scan an area created by some angle of change from this sequential pattern, according to how light reflects information at angles')
            - relatedly, the concept of 'color' is created by 'differences in configurations of points/components in a set' as opposed to just 'simple structures like counts of points/components in a set' which allow different wavelengths to pass through the set without hitting a point and different wavelengths to hit the points of a set, which can be applied to statistical regression in structures such as 'finding which simpler wave functions pass through a subset and hit a subset, indicating information about the wave functions (or variants of them) that reflect info about the whole set', which can be applied to 'preceding sets of former versions of inputs' rather than just 'subsets of inputs', like a reverse fourier analysis
            - similarly, finding other variables like inputs/outputs of light (that reflect information such as 'input/component patterns/connections', such as shadows/angles/reflectivity) is useful for finding information about a data set in statistical regression
            - similarly, finding info-reflecting variables (as in 'info outputs/signals' like 'shadows') to look for when some other useful structure has been identified (like a 'compounding sequence' or a 'contradictory/negating/neutralizing sequence') is useful for identifying the probable identity of the final solution function using shadows of preceding/input functions in the sequence (as in, 'given some output/shadow earlier in the input function sequence, what info is derivable about the shadow of the final solution function, and what does the shadow indicate about the final solution function')
            - relatedly, finding 'similarities/commonalities' in possible solutions/outputs such as the 'shadow shapes that can be generated by extremely different inputs' as well as finding the 'outputs/signals of differences in inputs' like 'changes in the shadow shape when some trivial function is applied' and finding those functions which highlight these important differences to identify, given the extremity of the difference in their inputs, are related useful structures
            	- for example, as mentioned elsewhere, its important to differentiate a parabola and a wave as they can look locally similar, and similarly its important to find other volatile similarities as well as functions to differentiate them (produce the changes that make the difference obvious/detectable)
                - as another example, a 'superconductor' is a metaphor for useful structures like interfaces, which add high value output (electricity) at slight/cheap inputs (adjacent changes), as the structure of the superconductor itself reflects a high variation model of reality that is already useful without inputs at connecting variables
        - similarly, the rules of brain structure interactions, which reflect interface structures such as truths (like 'differences in similarities' as 'random electrical activity in hyper-connected component brain regions')
        	- https://science.slashdot.org/story/23/03/21/2219254/psychedelic-brew-ayahuascas-profound-impact-revealed-in-brain-scans
        - similarly, the related concepts of 'independence/orthogonality/non-intersectiveness' in math is analogous to useful interface structures like 'maximal differences' (such as differences in 'maximal difference-uniting symmetries' like cross-interface structures like 'cross-system similarities')
        - similarly, the structure of 'quaternions' is a useful analogy to a 'unit interface structure' in that it encapsulates a rotation and an axis of rotation, and also applies a 'maximal difference possible within a definition (of a real value) without breaking the definition' as a useful structure to fulfill intents like 'create opposites/differences'
        - similarly, the manifold is a useful corrollary to a 'type' or 'definition', in the sense that items belonging to the type or qualifying as the definition can vary within a set of limits maintaining the manifold structure
        - similarly, the concept of an 'exponent' (self-similarity) is related to 'randomness' (in that a square allows more randomness if its the shape of a data set as any connection between low/high x-values is equally possible) and both are also related to 'symmetries' (in that the square has four symmetries where a straight line has one, and that randomness aligns with symmetries in that symmetries act like equivalences under some change), which you could predict from the 'self-similarity' attribute of the exponent
            - relatedly infinities could act like a symmetry in that if there was an infinity in physical reality, it would be invariant to change (taking one item away from the infinite set wouldnt change its 'infinity' attribute)
        - similarly, the 'density' of a black hole is a corrollary to a 'density' of a data set in that both are 'powerful' (the black hole is powerful through being a source of energy, and the density storing information is powerful to the extent that it influences the prediction function)
            - this is an adjacent sequence once both structures are standardized to the 'information interface' (the 'density' is a significant attribute of a black hole to focus on, bc of how the black hole interacts with 'information', similar to how the density of a data set interacts with information by storing/representing information through its center/average)
            - this indicates if you had an equation like the following, you could derive x using interface analysis to derive 'density of information' and 'power of information' as a useful connecting structure allowing the equivalence/similarity indicated by '=' and identify 'some energy type (like dark energy)' as the x-value
                black hole/x = density of a data set/power (in relevant problems like prediction or determining the rest of the data set)
            - this is a task that transformers should be very good at, given their ability to map systems to other systems, if applied to interfaces as the systems to map
        - similarly, variables such as 'heat' connect different interaction levels (as a 'primary change-determining and change-generating variable' of chemicals, connected to a 'primary exchange unit and primary input' of cells) so these variables connecting different interaction levels are valuable to identify as connections to new interaction levels
        - a universe that requires components to exist also requires a combination function and combinable structures, and one that doesnt require 'uniqueness' allows for 'repetition' and therefore 'quantity' to be measured and 'a number set to differentiate quantities'
        - the idea of a 'particle/frequency of cause' could be the wrong structure to look for, as 'cause' emerges from constants that allow the following, meaning that a coordination between many particles and input/system conditions is required for the concept of 'cause' to be allowed and emerge
        	- change to occur in a 'input/output sequence' that can co-exist with other sequences (occupying and validating the same timeline, and also creating it as a component of the timeline)
        	- interactions between structures, as opposed to requiring isolation of structures
        	- interactions/changes that dont invalidate a high ratio of other structures by default
        	- inefficiency of structures in handling variation injections, needing to move or disintegrate in response to variation like collisions, rather than having stronger forces maintaining their structure, which is an output of energy transfer
        	- however cause could be said to occur at a 'frequency' in that it requires two states to be connectible by some distance across which information/structure can be preserved, and similarly in the sense that a structure is a cause of other structures if its maintained long enough to impact other structures, a structure could be said to be 'causative'
        	- 'causal erasure' relatedly occurs under conditions where many equivalent alternate common structures lead to the same effect, as opposed to a direct identifiable unique causal connection, and similarly erasure of other attributes of cause like 'inevitability' can remove the concept of cause
        	- relatedly, is there a structure such as a 'default network of entanglements (default synchronized structures)' that ensures primary interface concepts like 'information', 'cause', 'change', and 'potential' can continue existing, which creates new entanglement connections if some are disrupted, and is reality the network or set of overlaps connecting these default networks of entanglements, and can these entanglement networks be used to distribute variation/energy evenly (using a randomness entanglement network) so that spacetime curvature doesnt occur and spacetime is experienced similarly for all observers, or would the universe move too fast in some direction once observers are moving at constant speeds for that to be useful or would it distribute change across the universe so that there is no motion of the universe as its neutralized by the evenness of the distribution
        	- more likely than 'absolute retrocausality' is the idea of a 'mass of energy (acting like a large body of fluid) that may overlap/flow into some future state in some condition temporarily (like an occasional errant wave flowing farther out than the others), but will reverse course to be centered in its prior primary stable state, so that this specific future flow seems like an orphan leaf rather than a new timeline, as the return to the primary body of fluid is the stable state, and the future state wasnt connectible/interactive with other states, so time didnt continue to flow in that direction, even though that future state may regularly happen, but not often enough or probably enough to be really real in the sense of stable/connectible time flows, and this future state may or may not impact the primary body of fluid (it may cause itself to re-occur by temporarily flowing back into the primary body of fluid rather than farther away from it, or it may not have a measurable impact on the primary body of fluid)'
        - similarly, 'potential' emerges from constants that 'allow some changes/interactions to occur within a limit, allowing organization to occur' rather than 'preventing any change/interaction' and also 'prevent all changes/interactions from occurring in a way that leads to chaos' and similarly 'allow change types or change inputs to be stored rather than used'
        - why is it possible to make predictions using something other than 'input/output sequences'? bc there are alternative equivalent structures (requirements, networks, probability/potential fields) which are non-sequential and which are useful structures (stable structures of reality)
            - "is there always a 'mirror/symmetry' to reflect across every symmetry (is there always a 'mirror' to reflect past/future states of a system, given some distance to reflect information across)" is another useful question
    - identifying function sets to identify similar structures that can generate useful structures in an alternative way, such as how some sequences can generate a 'change rate that changes every time' like the fibonacci sequence, which can look like and function similar to other types of change like simple exponential change, using a different input structure, as a way to fulfill useful problem-solving intents like identify different possible inputs
        - fulfilling a limited set of problem-solving intents is possible with specific functions which are alternately called 'interim functions'
    - identifying the possible usefulness of alternate function formats (like a 'unifying parameter' of 'maximally different sequences (different for each set of x-values such as 'adjacent x-value pairs') that when summed, converge to the y-value' which would be useful to connect systems of variables that could make every set of x-values justified in having its own function, and in applying a different function format of a sum of a sequence as the sequence of change combinations that are adjacently computed which can be used to calculate the output sum that is the y-value), formatting the problem of regression as a 'solving a system of equations' problem, where the parameter to solve for generates the sequences, and where maximally different sequences indicate different possible systems that the variables could interact with, which could all be equivalent alternates in the sense of generating the same prediction function, which mostly only makes sense when using a set of x-values with y-values to connect in a particular function so that the sequences are relevant by default
        - neural networks should apply changes as 'variables/functions that are actually encountered in the real world', as a variable is applied within a system, as opposed to the approximated variant or the over-simplified or over-deconstructed variant
    - identifying 'points of relevance' where functions can be injected in a useful way to optimize a structure, such as 'during any given training iteration, applying a function like "checking whether changes are moving in the direction of the target output to filter less probably successful change sequences earlier in the training iterations" is useful to avoid computations that are probably less useful to invest in' using rules like 'where are processes repeated or otherwise inefficient which can be reduced in some way to fulfill some relevant solution metric like "reducing required steps" (as in not maximally different structures)'
    - identifying the 'path from math to physical reality' will probably depend on identifying all of the 'maximally useful structure' (on the 'meaning' interface), as physical reality indicates the 'structures which are more computable/adjacent/efficient/stable/sustainable/measurable/independent', and useful structures (useful across problems and interfaces) are the most stable/efficient/adjacent structures
        - I imagine the 'maximally different structure connecting problem/difference types' is going to become necessary to connect math with physical reality, whether as a unit of reality/time or in adjacently connecting other structures or some other fundamental structure
        - a useful depiction of 'time' occurring on this 'maximally different problem/difference type connection structure' is changes to the queries run on that structure, queries which connect increasingly more variables of it, give it additional structures describing it like symmetries/rotations, or create new differences to integrate with the structure (if possible once the maximally different structure is identified)
    - 'solution automation workflows' can follow rules governing selection of structures like 'spaces' to position them in structures like 'sequences', so for instance by the time a query starting on the 'interface network' and moving to the 'math interface network' to a 'causal/neural network' gets to 'finding polynomials in euclidean space', it should be heavily filtered so that the extremely high ratio of 'possible functions allowed' relative to 'probable solution function variation' in that final space are mostly filtered by the point the workflow arrives there in the sequence (as in, a 'probable function range area' or 'probable primary function components' are identified by that point), or otherwise progresses from a space with more possibilities to fewer possibilities if no such filters are applied, so that the space itself can act like a filter on the possible solution functions
    - a function to 'find all different known variable interactions' and a function to 'generate & check new variable interactions (generating new possible interactions and finding a system that seems to be modeled by those new interactions) to fill in the gaps left by known interactions' can be a useful function set on its own as most variable interactions follow common patterns like 'interval interactions', 'cyclical interactions', etc
    	- as another example, functions to derive variable interactions based on insights (like insights interfaces/symmetries, which could be applied as a function to identify the rules of 'embedding variables' as that is a required variable interaction which can be used to frame all variable interactions (how many variables can be embedded in another interface variable, of what type and how can they interact without violating an interface variable theyre based on and depend on to exist and change, in what real systems))
    	- applying symmetries as a 'magnet for change' can help model a system's handling of events like 'interface overload', 'variation injections', 'definition violations', etc (when one symmetry cant handle a change type, meaning it violates the symmetry, what happens in the system having that symmetry, does that variable always obey another symmetry or form its own or decay)
    	- a set of 'known physics functions' is another useful function set, which can be used as a core function set to connect all variable interactions to (a function to connect all variable interactions to physics laws would be useful and independent of other function sets in solving problems, for example 'genetic variables' would be related to 'collision physics and cell pressure/charge rules')
    - identifying useful structures like 'mirrors' that act like metaphors to evoke other useful structures as a way of representing info structures like 'filters', such as how mirrors offer information without having access to all information (like if positioned at an angle), similar to how some info filters reveal info about an entire number type, allowing computations to be skipped, or allowing a program to 'look ahead'
        - these cross-interface structures can be maximized in differences like angles (similar to a staircase or helix shape across stacked interfaces), creating maximally different structures to use as a base for other algorithms requiring changes
        - relatedly, maximally different structures are useful for identifying extremely different ways to frame/format the same variables, such as how 'differences from randomness', 'interim points in between dichotomies like complexity/simplicity', 'adjacencies', 'simplicities', 'embeddings', 'densities', 'curvature', 'probable function area ranges', 'filters of equivalently accurate/possible functions', and 'linear/exponential change type filters' are very different structures which are equivalent alternates (or possibly complementary in providing different info when applied with other different formats) in their usefulness for representing a data set as a function, these structures being non-obvious/trivial to generate the full set of, and optimally useful once generated
    - identifying useful connections and other structures between useful structures
        - for example, the connection between 'filters' and 'interfaces' is useful bc filters are extremely useful when they 'preserve some variation of the input' and when they 'magnify (similarize/differentiate) some variable' to make some similarity/difference more obvious, as this is an implementation of the certainty/uncertainty interface that is so powerful in problem-solving
        - similarly, from this it is adjacently derivable that the concept of an 'interface' has an opposite in that the 'interface' applies a standard format to similarize some variable, to make differences obvious (to identify/differentiate structures), where an opposite of an interface would differentiate some variable to make similarities obvious (to equate/connect structures), which is a new connection between these structures that is useful and can be included in the definition of each interface so it can handle these alternate intents (like to 'make some structures similar to structures on some other interface' to fulfill intents like 'connect interfaces')
    - finding resolution functions for commonly useful connections/transforms, like connecting the 'densities to sparsities' or 'density patterns across densities' or 'densities to extremes' or the 'edge points to an edge line' or 'upper/lower/average edge lines' or the 'densities to regression lines' or the 'shapes (like graphs) formed by densities to regression lines' or the 'local subsets represented/connected and the subsets skipped/unconnected' and other useful connections in the data set regression problem space
        - other useful structure examples to apply as defaults given their higher probability for various reasons (adjacency to requirement, commonness, etc)
            - identifying non-useful structures to filter the set of useful structures
                - for example, identifying what slopes are unlikely to describe a function (making extremely distant points falsely adjacent and vice versa, to determine the slopes that are unlikely to describe the function change rates)
                - 'connections between a sparse subset of maximally different points (in their original positions)' is also a useful structure for intents like 'determining general maxima/minima of a function'
            - identifying points in between extreme errors/suboptimalities as the interim point that is more useful for more general intents while being generally sub-optimal as it doesnt specialize in optimization metrics
                - for example, identifying algorithms with variables that allow 'complementary' best-case input scenarios that are optimized by various algorithms to maximize coverage of input cases in the combined algorithm using these algorithms as components/alternates given some input range
                - for example, gradient descent is optimal in cases where local minima are good approximation of absolute minima and where the whole function is infeasible or inefficient to check
                - in cases where they are not good approximations of absolute minima, or where there are no minima, other algorithms would be better to find other sub-optimals that are less sub-optimal in those cases (like 'maximally different input-finding functions')
            - identifying compressed input info (like the 'output variables') which are more useful to identify than 'adjacent variables to input variables' for determining the outputs
                - the output info contains patterns and other interface structures which are useful independently of inputs in some cases, which can allow skipping connecting them to inputs if the outputs are compliant enough with patterns to be approximated by other functions like probability distributions (such as where the value is usually within some trivial difference from an average output, so this average can be used as a probable output, or where the outputs stay within a range and are relatively equally distributed in a range, which indicates a wave among other possible function shapes)
            - identifying useful structures like 'reasons why some interface structure is useful across multiple useful solution metrics, making it likelier to be robust to changes' such as how 'curvature' relates to 'self-interactivity' which is a core efficiency structure (that generates different change types with one input and one operation) as well as offering 'one cohesive unifying function of disparate sets of linear functions (such as local subset linear functions)'
                - similarly identifying other useful structures (like the 'e ratio' which resembles a core interface structure when formatted as a maximally different angle with a common base, as in a 'right angle', having a smaller change based on a larger change) as a highly explanatory indicator of symmetries which also relate to stability of systems (as systems that vary change types using minimal inputs are useful, small changes within some limit are useful, and are likely to reoccur as a result) and can act like a 'base solution to test first' in the absence of other base values to test as parameters (meaning 'apply e as a base parameter to check if the system has reached some local extreme of optimization/stability'), as the 'e ratio' is likely to sustain itself when applied repeatedly (a spiral), where other ratios are likely to intersect with themselves (circular ratios) which is useful for different intents and other ratios are likely to never intersect with or relate to themselves (leading to infinite change, which is not as commonly useful)
                - similarly other structures may be useful when applied as certain/constant/default inputs to a system, such as the 'exploitative ratio'
                - given that these specific math structures are highly connected to important/useful variables like stability/symmetry/balance, they are useful as specific math structures to start from/use as a base in some problems (like in stable/simple systems) where more complex systems that are not stable are likelier to benefit from differences from this parameter
                - finding the reason why a structure occurs is important bc different reasons create different types of change
                    - for example, if the reason why there is a negative correlation is bc some agent had an incentive to falsify the positive relationship, that would change in the next more accurate batch of data as being opposite to the correlation found, so knowing that reason is useful to predict how the data set might change
                    - similarly, if one agent is involved and if there is an incentive to falsify information, the information is likelier to be simple (follow a simple pattern) than it is to be complex, as simple-minded agents are likelier to need to fake some information
                - finding other constants (or other specific structures) of stable/optimal systems is similarly useful as these constants (or other specific structures)
        	
        	- identifying useful structures like 'changes which preserve info about the data set (such as symmetries in the data set) given the intent to find a representation, rather than an intent to remove info' to identify possible solution functions (composed of those changes) which might be useful for representing the data set (like finding average lines)
                - any function which changes the data set in some way (like removing outliers or removing sets of points that dont change the average) in a reductive manner (reducing computations/inputs/redundancies/noise), while also preserving a relevant solution metric (such as a type of 'average' like a 'local average' which is nearer to the final solution function, or 'probability density' or other 'moment' of a function) could be useful in finding the final prediction function, which is the ultimate average or other representation that is useful for minimizing prediction errors
        		- relatedly, 'mirrors' (or 'look-ahead tools to skip computation') depend on finding symmetries around which changes vacillate within a known range defined by the symmetry, so that once a symmetry is found, all changes around it are determined and dont have to be checked (just like a 'type' acts like a symmetry around which change develops, staying within the range defined by the type definition), which means symmetry structures like 'averages/extremes/inflection points/densities' are useful for determining info with minimal computation
        		- relatedly, finding the 'sets/sequences of symmetries which capture most relevant info about the data set' is useful for finding which sets of symmetries should be applied for which intents, symmetries being useful for predicting change as they indicate change bases & limits
        		- other symmetries (which are highly explanatory of changes in the data set) include higher powers, as the higher powers of a function are good at forming a base function to apply specific changes to in order to find the specific customization of that base which applies to a greater ratio of the data set
        		- finding the opposite of these symmetries (lower powers) is possible to do efficiently if done locally, to find change rates that could be adjacently produced with lower powers, but finding the higher powers is still required and these cant be used to skip a high degree of computations
        		- checking 'exponents of identified local change types' is useful to find possible simpler/alternate adjacent functions (once x^3 is found to be true locally, x^4 and x^5 should be checked as well in adjacent or maximally different subsets)
        		- as another example, the following equivalent alternate sets of insights make interface analysis trivial to identify:
        			- insight about variables of problem-solving
        				- 'there are solution automation workflows (like trial and error)'
        				- 'there are multiple workflows (there are others, like break a problem into sub-problems)'
        				- 'this means there are variables of solution automation workflows'
        				- 'these workflows interact with some objects like problems and have variables in common, like information requirements/interaction types/errors'
        		    - insight about multiple interfaces and automatability
	        		    - 'concepts have structure'
	        		    - 'information has structure'
	        		    - 'problems have structure'
	        		    - 'rules/functions have structure, and are therefore automatable'
	        		    - 'objects with structure are automatable'
	        		- insight about the interface concept itself
	        		    - 'problems are a matter of identifying similarities/differences'
	        		    - 'most problems are resolvable with standards that make comparison tasks trivial'
	        		- insight about the usefulness of each primary interface
	        		    - 'concepts can independently be used to solve a problem, without logic, information, or other primary interfaces, and without functions like "test" which would normally be involved in problem-solving'
	        		    - 'primary interfaces like concepts/logic/information are equivalent alternates in that they can be used to solve a problem independently of each other in best cases'
	        		- insight about the conceptual relevance of default/core structures
	        		    - 'direction corresponds to the intent interface'
	        		    - 'distance corresponds to the change interface and the similarity/difference interface'
	        		    - 'surrounding structures correspond to the system context interface'
	        		    - 'alternate possible structures correspond to the potential interface'
	        		    - 'angles correspond to the perspective interface'
	        		    - 'chainable functions correspond to the logic and function interface'
	        		- insight about how cross-interface structures are more useful
	        		    - 'concepts are only clear when you have an example (such as the variant of the concept in physical reality or in a particular problem) if you dont know the whole definition'
	        		    - 'concept-structure structures are more powerful than either on their own'
	        		- insight about how some variables are more powerful at more useful tasks (like explaining/describing/generating/determining) than others
	        		    - 'there are variables that are more powerful than others, like the general variable of cause, which is highly explanatory'
	        		    - 'there are equivalent alternates that are useful to know (explaining/describing/generating/determining intents, or find/build/derive intents)'
	        		    - 'cause is also good at these equivalent alternate intents'
	        		    - 'cause has an equivalent alternate in that logic can replace its value in these intents'
	        		- insight about how some structures are more generally useful across problems
	        		    - 'structures like rotations, similarities, and limits keep re-occurring across problem-solving methods'
	        		    - 'these structures are different types of objects like math objects, standard objects, and system objects'
	        		- insight about finding the 'most reduced set of useful/important structures'
	        		    - 'if you try to reduce language to the most useful structures, youll find structures like inconsistencies, perspectives, requirements, implications, overlaps, etc'
	        		    - 'the most variation-capturing variables of these useful structures are the bases where the others can exist, like differences/errors, structures, or functions'
	        		- insight about multiple perspectives and simlarities/differences being related to formats (which are like interfaces)
	        		    - 'different formats make different intents trivial'
	        		    - 'everything can be differentiated by changing perspective'
	        		    - 'a perspective is like a filter/standard/format'
	        		    - 'everything is similar and different to everything else in some way to some degree'
	        		    - 'variables/functions are related to interfaces as they change within a defined limit'
	        		    - 'differences in similarities and similarities in differences make problems trivial (similar to the comparison insight above)'
	        		    - 'standardizing to the same format makes some problems trivial to solve, as it highlights meaningful differences'
	        		    - 'some standards/formats (logic/concepts) are useful structures (interfaces)'
	        		- insight about useful graph structures
	        		    - 'some graph structures are more powerful than others, such as networks, maps, trees, sequences, etc'
	        		    - 'the differences between these useful structures involve objects (variables/functions) in the graphs, connection functions (like similarity or interaction function), and the structure variables like direction (in causal networks, for example)'
	        		    - 'a graph of graphs (like the interface interface, or the meaning interface) is a useful structure to organize these variables'
	        		- insight about different problem formats
	        		    - 'different problem formats exist, such as filtering problems and sorting problems and building problems and simplification problems'
	        		    - 'different optimal solutions and solution-finding methods are trivially derivable given the format of the problem'
	        		- insight about equivalent alternates
	        		    - 'there are some structures which are equivalently useful, such as common high-variation functions, common formats, common errors, common causal variable structures, common problem-solving sequences/workflows, etc'
	        		    - 'there are some problem-solving structures which are equivalently useful, like alternate function sets or alternate useful structures like interaction levels or problem metadata like problem formats'
	        		- insight about common problem-solving differences/functions
	        		    - 'different problem-solving functions exist which are common across problems, such as "find a solution" and "derive a solution" and "build a solution" and "change an existing solution"'
	        		    - 'these functions have structure, and there are other common functions to problem-solving processes, such as common problem-solving intents'
	        		    - 'other function sets common across problem-solving processes exist, such as core interaction functions of problem-solving processes and cross-interface functions and connection functions of problems/solutions and formatting/standardizing functions'
                    - insight about how primary concepts are powerful 
                        - 'simplicity/complexity' corresponds to a primary 'difference-resolution or connection' function, so it corresponds to a primary 'problem-solving' function, as problems' default format is a 'difference to resolve', bc a problem can be solved by making it simpler or finding functions that in general simplify other variable interaction functions
                        - other attributes which can describe any structure also correspond to primary difference-resolution and problem-solving functions (like 'work', 'intent', 'potential', 'change', etc)
                        - for example, 'changing an existing solution to a similar problem' is a default problem-solving and difference-resolution function
                        - what other primary abstract concepts can be used to resolve differences (solve problems)?
                            - 'balance' can be used as a primary problem-solving function in the form of 'balancing extremes, as extremes often lead to errors like over-prioritization errors or scaled errors', which translates to a problem-solving function like 'balance the maximum differences in a problem to find a more general/stable format of the inputs which is likelier to be correct (a solution)' or 'find functions that are adjacent/interim to a high ratio of information'
                            - 'power' can be used as a primary problem-solving function in the form of 'finding functions that can reduce the work of all other functions, making these functions more powerful' or 'find functions that store high ratios of information'
                            - 'potential' can be used as a primary problem-solving function in the form of 'finding functions that can interact with the highest ratio of other functions, making these functions higher potential in the higher variability of their interactions' or 'find functions that have a high ratio of usage functions (can find/build/derive a high ratio of information)'
                            - 'intent' can be used as a primary problem-solving function in the form of 'finding functions that determine what other functions are used for (intents) and what theyre useful for (adjacently/optimally fulfilled intents)' or 'find functions that are more useful for a high ratio of information-related intents (like information storage)'
                            - 'cause' can be used as a primary problem-solving function in the form of 'finding functions that identify input/output (causal) sequences of connected variables (such as why a function is useful, as in the reasons the optimally fulfilled function intents develop to be common, such as the efficiency of using the functions and the commonness of the requirement of their outputs)' or 'find functions that cause a high ratio of information'
                            - 'certainty' can be used as a primary problem-solving function in the form of 'finding functions that change certainties slightly, as these slight changes are likely to solve new problems unsolved by existing/known certainties' or 'find functions that require/determine/generate/describe a high ratio of certain information'
                            - 'abstraction' can be used as a primary problem-solving function in the form of 'finding functions that can store/embed most variables adjacently' or 'find functions that can support a high ratio of information'
                            - 'perspective' (as a filter formatted as a set of priorities, or structurally as an angle that makes a subset of information obvious/certain) can be used as a primary problem-solving function in the form of 'finding functions that can change angles/filters to make connecting/differentiating any variables trivial' or 'find functions that can identify perspectives that are useful for a task like embedding information or identify perspectives that can make all information trivially stored'
                        - given how these concepts interact with information (can store a high ratio of information or otherwise are useful for information-related intents), the structure of an interface emerges
                        - the primary abstract concepts (power, balance, complexity, stability) offer good candidates for interfaces bc of the fact that they are high information/variation-storing and every structure has these attributes in some way, so they are a way to access different fields of connections (differences/similarities) to other structures

                - most of these have common variables, such as including 'known common useful structures', applying common useful intents like 'reduce/connect/compare', identifying structures that are relevant to 'problems', applying interface structures as a useful compression/explanation/other intent fulfillment structure, etc
                    - they differ in their method structure variables, such as how 'identifying the type of object that a useful structure is' differs from other insight sets in that it applies a bottom-up direction (generalize from specific examples) from a 'specific' starting point

            - identifying interface structures like 'efficiencies' that align with regression problem space structures such as 'symmetries' or 'densities' by relevant structures like 'causes' ('this point is a hub' meaning 'a lot of inputs created this output bc it was particularly efficient for something') and identifying functions to connect those interface structures with causes that can be mapped to other variable sets (other efficiencies can be generated and checked for, once the reason of 'efficiency' is known as a cause of some 'variable interaction structure'), meaning other relevant useful 'low-cost, high-reward uses of inputs' for known intents can be hypothesized and checked for in the data set to determine the prediction function, as well as 'other causes of efficiency such as energy limits, which could cause other structures related to energy like power imbalances, power concentrations/compoundings, power takeovers, power dispersions, power vacillations, etc'
                - this is useful bc structures like 'efficiencies' are related to 'symmetries' and other specific useful structures in the regression problem space like 'densities' in their usefulness for predicting/explaining/limiting change
            - identifying useful structures like 'questions to answer (like "where does change change") to solve different sub-problems of the problem (like "how to divide the data set into subsets which are likely to contain different change types/rates/degrees/etc")' which make the rest of the problem trivial (makes solving the other sub-problems like "find the most different subsets (which could possibly contradict a function for another subset)" trivial) and identify the sub-problems they make trivial, and filter these by which subsets of sub-problems can replace other subsets or the whole set for some metric like 'finding an approximation of the solution'
            - identifying useful structures like 'adjacencies' as a way to determine if alternate variants of the data set are more obviously compliant with a pattern, as 'adjacent transforms' of a data set are likelier to be valid than other transforms, and some of the adjacent transforms may have more obvious patterns/averages/densities/other useful structures than other types of transforms
                - similarly, identifying useful structures like 'repetitions' to apply to relevant structures like 'subsets' (specific sets) to generate structures (like 'repeated change types across different local subsets') which are more relevant to a prediction function (the complete general set), or 'differences' to relevant error structures like 'non-local subset change types' (which are likelier to be less relevant to the prediction function, as in inaccurate, than local subset change types and therefore applying changes to these errors structures is useful to generate the actual prediction function)
                - relatedly, a 'mix of simple/complex transforms' applied to a base function to balance the extremes of the dichotomies that are useful/determining of the problem space and allow resolution of them by testing small differences favoring either priority may be useful as a testing/diversification structure to hedge bets in the prediction function, as well as embedding other variables reflecting other dichotomies (certainty/uncertainty, specific/general, discrete/continuous, adjacent/extreme), which is similar to retaining multiple different equivalent alternate functions which are ambiguously correct as they fulfill solution metrics similarly
                    - organizing these differences by 'which are likely to be adjacent' & related metrics is a useful way to filter out functions that are exactly obviously incorrect (by placing a constant in exactly the maximally wrong subset, for example)
                - similarly, if there is a subset of input variables that can be used to create the exact opposite (or similarly different variants) of the found prediction function indicating that the variables can be easily made to contradict each other, the symmetry uniting them is likelier to be more true than either the found or the opposite function
            - identifying useful structures like thresholds to filter possible alternate interface variables of a function, like determining the function up to a local subset size that its possible to determine its change types, like whether its exponential or linear, as the meaning of a subset emerges and is more obvious, the greater the number of points in the subset (subset ratio), where the meaning of a point is difficult to determine in isolation of other points
            - identifying the primary interaction functions between subset structures, such as how a subset might represent a fraction such as a quarter of a data set shape, so it should be reflected across two symmetries to create the rest of the data set, or a subset might be an 'orbit' or 'boundary' of the rest of the data set, so checking if these interaction functions apply to the rest of the data set is useful and identifying these interaction functions is useful
        	- identifying useful structures to filter out possible errors like 'randomness in the data set' by checking for 'associated structures of randomness (when defined in that problem space)' like plus/x shapes of lines with equivalent/similar numbers of intersecting points with the data set, or ambiguity shapes like squares (which could represent any line with positive/negative or constant/zero slope with equal probability) or similar areas reflecting randomness when found to describe the whole/most of the data set, or look for indicators of usefully biased shapes like rectangles/circles which dont represent completely equal/ambiguous change
        	    - similarly, identifying the set of 'lines with equivalent/similar intersecting points, optionally using lines as connections between adjacent points in the set (to add simplicity as a proxy of generality)' as indicators of equally probable solution functions or 'lines which skip the fewest points to form a straight or otherwise simple line' are useful structures to use as generators of possible solution functions, similar to other generative strategies like applying variable changes like 'embeddings on other variables' to generate possible maximally different functions, just like how 'any line crossing the data set' is similarly useful as a 'probable line-of-best-fit' to check errors and determine shape/slope of the correct function by assembling 'adjacent errors (to form shapes like lines/curves/boundaries)' and 'error changes like sign changes or phase shifts' into the solution function shape
        	    - randomness is particularly important to identify bc structures that exist are unlikely to be random (equivalent alternates that seem ambiguously similar usually resolve into favoring one or merging or differentiate further and become obviously different in equivalently useful ways, if the host system continues to exist bc multiple equivalent alternates are redundant, high-cost to maintain, and therefore less likely to occur & continue to exist), so determining inputs/components of non-random structures (such as biases, or simple rules) produces a set of 'probable structures to exist'
            - identifying useful dichotomies like 'parabola or line' is particularly useful as the core question to answer in the regression problem space, to generate algorithms such as 'connect the outputs at the lowest/highest x-values, and check midpoints in between the lowest/highest x-values to check for an error indicating a parabola', as its more important to identify when there is a parabola/wave vs. a straight line than to identify any other structure except more core unit structures like averages/extremes/inflection points and errors like 'gaps in data' and other structures resulting from core structures like extremes (such as limits/infinities)
               - the inputs to this algorithm are the x-range extremes (lowest/highest x-values) as well as the midpoints in between which would be useful to check for parabola-type errors at (the selection function of subsets)
               - identifying trends in error structures (like if the error is always 1, the function should probably be shifted up/down by 1, or the program is only checking values at a wave peak/valley where the magnitude is 1 while identifying the horizontal line crossing the wave at its midpont) is useful for identifying corrections to try early on, and identifying how to correct them such as by 'evaluating values at different intervals rather than at the same interval' and 'identifying the simplest line that intersects with the data set the most' to avoid this error, are similarly useful to reduce the probability of repeating that error
               - as another example, if you have a horizontal s-curve (one up and one down peak), identifying that is as simple as identifying that three other pieces of information are required, and identifying those three of the y-values at regular x-intervals (or the two/three determining points of the peaks/inflections), at which point the s-curve will be obvious if curvature is used to connect these points and your interval function identified the extremes of the peaks, if the program specifically checks for that type of curve as a common curve type (the skew/concavity or the squareness vs. linearity of the curves is another useful variable to determine as an important specifiying variable, the squareness indicating less likely change types and the curvature/linearity indicating more probable change types), so an algorithm to identify the variable set of 'curve peaks, x-ranges, and squareness/skewness/volatility at some subset of the function' is likely to produce a generally accurate function, on its own regardless of other variable sets, which can be enhanced by other known variables capable of producing extreme errors such as 'threshold phase change' variables which can make a function act like a totally different function in some continuous subset
            - identifying useful sequences of interface structures like a 'maximal difference connecting line indicating the connection between the most different points' and a 'inflection-point intersecting line indicating a smaller direction of change' which can provide a simple set of linear functions to base changes on to find the regression line quicker, which in a curve with exponent 3 (which starts lower, has an upward parabola, then a downward parabola, then increase indefinitely) would indicate the primary deviation from the primary summary of the change patterns (a line with positive slope connecting the low initial points and the high later points as the primary summarizing line, specified by an interim line with negative slope crossing the inflection point to indicate the primary deviation from that primary summarizing line, which can be further specified by tangents indicating extremes or alternately limits of vertical change)
            - identifying useful sequences of filters to reduce regression solution sets by useful trivially calculatable structures like change type variables, such as a sequence like 'check for a parabola, which implies an exponent, then given that an exponent implies other powers could exist like even/odd powers which determine maximal differences in the resulting function, check if powers are even/odd, then proceed to other variables to determine the rest of the function' which starts with an easily found structure and proceeds to other structures made possible/probable by that structure
                - this is related to other workflows like 'check for a change type, then infer other change types based on that, given change type interactions' but involves finding alternate sequences which are maximally differentiating/filtering or otherwise useful
                - these sequences' usefulness is maximized in cases like 'when the first item in the sequence is an interface variable that can support embeddings of other variables (like a unit exponent variable or other variable structure to differentiate function types) and the following variables are embedded in that interface, so the filters begin with the maximal differentiating filter and then decompose the remaining variation on that interface'
        	- identifying optimal algorithms involves finding useful (such as 'measurable') variables with 'obvious errors/optimals', such as how an algorithm in between simplicity/complexity is likelier to be optimal than an algorithm that is at either extreme (this is useful for filtering all possible algorithms to reduce the search space)
        	    - similarly, algorithms that involve some common useful interface structures (such as symmetries, which are like information wormholes, as well as limits and maximal differences) are likelier to be useful (and true and relevant) than other algorithms
        	- identifying variable interaction functions that could not be true given some system of reality where those functions could exist that is not true, like identifying that a 'wishing reality system' is not an accurate model of reality by identifying that 'agents wish for problems to be easy to solve (they wish for types of freedom)' and 'problems agents have are not usually easy to solve' and 'some common wishes of agents contradict other wishes of their own and of other agents, and would also contradict a wishing reality system for other agents' and therefore a 'wishing' variable interaction to connect problems/solutions is not a valid problem-solving function applied absolutely, which is useful to rule out variable interaction functions to find variable interaction functions that are possible/legitimate as a way of finding a reality system model, though finding states that move in the direction of that system is possible using realistic rules (increasing agent intelligence makes problems easier to solve and reduces agents' contradictory wishes against other agents, which is a solution of removing the intents that make the 'wishing reality system' impossible to logically sustain)
            - similarly, identifying high-variation explanatory variables like 'incentives' and 'interaction levels' from a typical data set where structures like 'defaults' and 'input/output similarities, differences, and requirements enabling interactions' (as more probable than other variable values) and 'adjacencies' (in casual degree) and 'efficiencies' (in benefit/cost ratio and stability) and 'interactivities' (as default interactions between variables) and 'types' (as efficient captors of information) appear more common than others, 'incentives' being a common factor in these common variables, as these structures are incentivized compared to other structures that may be more complex/difficult in some way, so simple queries like 'find common high-variation variables in common high-variation variables to identify other common high-variation variables' are useful in typical problems like regression, as if a common variable seems high cost its likely that we just havent identified the incentive yet, as the incentive is the determining variable more often than not, just like randomness is not usually real equivalence of probability in outcomes, but is likelier to just be lack of information about variable interactions that makes a variable interaction falsely seem random
                - for example, the dynamic between 'filters (as measurement/input-selection tools), as a core structure of differentiating functions' and 'incentives to distort the definition/structure of the filter to exploitatively avoid/subvert it or otherwise exploit it' is a highly explanatory interaction, where once a filter is applied, the incentive is to apply differences to game the filter (either to become a false/illegitimate input, and/or to expand its input range, or to use the functionality guarded by it without having the filter applied to it), and once the filter function has changed enough to handle these predictable adjacent incentivized differences, the opportunity to exploit differences to the filter is closed and differences are applied elsewhere, where these 'differences applied to the filter to handle adjacent differences/errors used to exploit it' is a 'common useful change sequence' to be able to re-use across filters, as the filter structure usually is applied too simplistically initially and must usually be changed to handle obvious exploits, 'filters' being a variant of 'standards' and are therefore useful in core intents like 'finding bases/limits of adjacent similarities/maximal differences'
                    - usually the filter follows patterns of errors, such as being too simple, too specific, too rigid, too structural, etc - so that it benefits from common useful optimizations like 'generalization' (as well as fulfilling useful 'core optimization intents' like 'increasing interactivity with other structures that dont adjacently cause errors', etc), an 'optimization change sequence' that can be matched to an initial filter by its probable error metadata (simplicity, specificity, rigidity, structurality, etc)
                - this can be applied to algorithms in general, such as for example, 'abstracting an input filter' in the 'find a regression function' problem space, to "find points belonging to the same type and connecting those points in a 'type function' to differentiate them from other points" or 'find maximally different points that should be connected in the same function, as they are legitimate and/or representative points'
                - these common distortions of a filter definition can be anticipated in advance so the solutions to these exploits are built-in to the filter definition, rather than applying the 'optimization change sequence' after errors are exploited
                    - a useful question to identify these change sequences: 'what is normally found implemented as a filter in typical systems?' for example, a 'domain/topic/type sub-type filter' such as a 'specific sub-type of object within a type with different rules that should be applied to it', so that a 'filter to find items of this type' is applied, often with a hard-coded function/dict to identify those items, and given the value of differences created by those different rules, other items will try to seem like those items, in predictable adjacent incentivized ways
                    - what other useful functions are commonly implemented? 'functions to correct distortions/errors beyond some threshold used as a filter of acceptable differences', 'functions to batch or aggregate items having some attribute', 'functions to connect some different objects using maps/functions/filters', etc - all of which can be implemented using some filter function, which is why this structure maps to a core interaction function 'find'
                    - a useful application of this would be to 'find new differences to apply filters of in a system, where these filters would be more useful to differentiate some structures and arent already used in a system, to optimize the system'
                - as some algorithms/queries are applied, other interface structure become obviously useful, such as 'similarities which are not adjacent' and 'changes that are neither obviously adjacent similarities or maximal differences' or 'interim differences between maximal differences' (useful as variants of maximal differences to test more different possibilities that are more similar to some interim base) or 'similarities between maximal differences', as 'adjacent similarities' and 'maximal differences' are obviously useful, so connecting these structures using related structures (related by the same base structure of 'similarities/differences') and finding useful variants of these structures and useful intents these structures are likely to adjacently fulfill (such as 'more complex changes which are less obvious and likelier to describe complex systems as well as the differences between various standards/similarities/symmetries that describe a high ratio of variable interactions') and standards/symmetries within these structures ('similarities between maximal differences' to identify input/generator/descriptor/limit variables of these structures) is useful while applying differences to create new structures (which should be abstracted/fit into other interfaces to check for new types of useful structures)
            - identifying the structures like 'shapes that when overlapped and rotated and viewed from a distance look like more common shapes like circles' as useful for describing reality in that they cover and explain more variation than other structures (this structure has a structure embedded in it that indicates 'equivalent alternates (having an equivalence in their center & a similarity in their rotation)', the 'incompleteness of any one alternate in this set when viewed in isolation', the 'usefulness of general trends once a process is repeated/scaled', the 'balance inherent to multiple equivalent alternate perspectives', the 'ever-changing nature of foundational structures leading to changes in interaction levels', and other fundamental structures that are descriptive if not generative of reality)
                - relatedly, viewing reality as a 'set of repeating processes applied to partial overlapping/embedded structures at varying intervals/magnitudes/scopes' may be more useful than some 'networks with unique nodes or just functions as nodes/queries or just individual usages as nodes/queries' bc the repeatability models useful interactions like 'aggregations at scale', 'net/emergent effects', 'in/stabilities', and other important structures created by the interactions of real systems, so this model of reality may simulate a more useful interaction level than abstractions like function networks tend to capture
                    - similarly, 'similarities (like patterns) in "differences from incentives"' and 'similarities in entropy/uncertainty/potential reduction structures' and 'structures of differences (such as attributes like unfulfilled/impossible/required intents) between differences/variables and difference-connection/solution structures' are related useful structures that captures high variation and formats differences in a minimally complex structure
                    - a network/field of these sets of useful structures that describe high variation in reality is useful as an alternative to a set of useful interface structures which can generate these adjacently
                - relatedly, viewing reality as a set of 'differences from required limits (impossibilities)' or alternately 'commonalities (similarities to probabilities)' (produced by adjacent/input/causative structures of commonalities like scale/aggregation/repetition/isolation/efficiency/incentives/investments/adjacencies/examples/usages as ways to produce commonalities) and differences from commonalities (like those that occur when "previously isolated commonalities interact after repeating enough to reach each other's position") can be more useful in its specificity as different from other useful formats like just any high ratio of all combinations of 'similarities/differences', this specificity being useful in the certainty it provides to base changes on
            - identifying the structures like combinations/ratios/networks/interaction levels of optimals, stabilities, requirements, errors, and other interface structures that occur in reality is important for solving the 'find a regression function' problem, as there will inevitably be something that a real system 'optimizes for incompletely', something that a real system 'should do but doesnt given this priority/requirement/input/opportunity', something that 'stabilized after a particular error type', and other combinations of interface structures like 'conditions' and 'causes' and 'optimals' (such as rules like 'apply high variation or high potential-variation variables first to decompose high variation'), which can explain most variable interactions but are not default/core structures already identified/required (these structures are more complex variants resulting from more interactions), which are obviously useful despite their complexity, as more useful to retain and use as defaults that re-generating them from core components every time, similar to how solution automation workflows can be more useful to retain than re-generate
                - these are useful for determining when a data set (or the system it reflects or the parameters of the analysis process like definitions) is incomplete/incorrect or otherwise suboptimal/erroneous in some way
                - relatedly, structures like 'monodromies' can be useful to apply existing math structures to indicate which points/lines/other structures that could represent 'limits which should not or need not be crossed' (which differences should not be resolved, such as where an ai program might 'resolve a difference' of a medical problem which is a 'state of difference from health' by allowing patients to die to create a 'difference from the requirement for health/health resolution to invalidate the problem' which is the difference to avoid except in extreme circumstances like where a 'health generator is adjacent, invaliding the requirement for health' and otherwise resolving unnecessary differences like 'resolving a difference that is already a known constant' or 'resolving a simple difference that is obvious' and 'resolving a difference that leads to an extreme such as hyperbolic change which is unhandled')
                - relatedly, retaining structures that are more useful to make constant (such as where its more useful to retain a map of inputs/outputs than to find a connecting function, like where extremely high variation is observed/possible and not reducible or where reduction is contradictory to other intents) and use as input defaults than to re-generate is a way of identifying the stable structures of reality that should not usually be changed/tested but rather applied as default inputs, combined in increasingly complex ways to describe more variation, rather than replaced with more descriptive variables, as these defaults are already the best at description
        	- identifying patterns/limits/useful representations (like areas)/other interface structures of useful 'false similarities' of functions (at some point, subset, in relation to some threshold, or range), such as when '(x^2) + 1' depicted as an area will look like a rectangle (when x is near 1) and when it will look like a square (as x approaches infinity), and identifying the point where these 'false similarities of areas' change (when x is sufficiently large that 1 looks trivial by comparison), and whether other changes are possible/defined/likely (whether those 'trivializing' changes will apply again at some point in that direction) or whether the pattern will continue, to identify 'different change types possible with a function', which is useful for determining the remaining sets of a function given a determined set of input/output relationships and in filtering functions that are equivalent or seem equivalent in some subsets but are not
        	    - similarly, finding 'standardizing' functions like to remove 'obviously non-impactful variables' such as the 'shift to move intercept value at y-axis to y = 0' that make the change type patterns of x^2 more obvious (the inevitability of the 'increase in area created by x' being exponential being more obvious once the 1 constant additive is removed as the impact of adding 1 at higher x values is more trivial which obscures the otherwise obviously exponential area increase, and the addition of 1 being increasingly trivial in changing this exponential increase in an invalidating direction)
        	    - similarly, other 'variable combinations with obvious/predictable impacts' exists like how a 'sum of constantly increasing and constantly decreasing variables' could easily be a horizontal line depending on their ratio
        	    - depicting these 'possible variable interactions' in a way that their commonalities are depictable as common points/overlaps or other obviously similar structures (such as combinations of inputs having an attribute in common crossing the same line/point or having the same shape or area) is useful for filtering probable interactions that are relevant to some other interaction (like an input/output interaction)
        	- similarly, finding a subset of points that represents an average/midpoint of some other set of points can replace the calculations required to connect the set of endpoints and instead just using the midpoint is acceptable as an approximation algorithm of the others, if enough different points are preserved to retain the general shape of the data set, just like 'removing extremely similar points' as 'redundancies' (but not redundant in all cases, as in the case of points around a density center, which preserve the weight of the density given its surrounding adjacent points) and removing 'removing non-adjacent (but non-maximally different) points connectible with local constant functions' as 'improbabilities' is another way to reduce the set of points required to connect in the same function
        	- finding useful formats like the 'set of angles/areas creating the components/features/products created by an input' which are summed to generate the output and which can be easily checked as un/applicable to other inputs by changing the angle of perspective in viewing these angles/areas so the impact of component areas/coefficients is obvious (viewing the interim products of operations in between x and y as a stacked set of areas, angles referring to the differences between area upper/lower limits compared to input x), rather than repeating the multiplication/sum operations on the other input, which is related to the format of a 'network of areas (representing products of variable pairs/sets) that is separated and aligned to make the output (sum) more obvious to avoid re-computing it', these formats being useful for approximating/prediction/avoiding computations by identifying similarities in component metadata like ratios of areas, and identifying obviously wrong 'product component sums', similar to filters like types that determine 'information about what else is also true' from a 'set of input facts'
        	    - this format is maximally useful when the interim components are few & large or otherwise obviously different, 'either very large or small' and 'very few' (meaning more adjacent to the final output)
        	    - once probable 'large components' (or otherwise simplified components) are identified as probable, filtering the sets of possible smaller input components of these components to resolve the ambiguity of 'which smaller inputs created these larger outputs' is a reduced problem compared to 'filter all possible solution functions' and possibly also 'apply incremental changes in a direction that reduces error' in some cases, which can be repeated up to the point where additional component-resolution is trivial (differences added by further component identification are trivial, or equivalent to other differences created by some other component set or simple layer like some function of randomness)
            - similarly, the 'equivalences in differences' from a particular 'possible solution function' are useful to identify 'probably useful summary functions', as functions that produce 'equivalent differences' are likelier to be more adjacent to the actual solution function (transformable to the solution function by some trivial transform to produce these equivalent differences, like a shift or scale change)
                - relatedly, finding these useful standardization functions that produce an 'equivalence in differences' from one function to another is useful to find these functions which have equivalent differences
                - relatedly, the '(patterns and other interface structures of) differences in functions that have equivalences at various subsets' are useful to identify, as having a common interface defined by their equivalent subsets which vary adjacently by some variables that generate the different functions united by those subsets, which make the problem of 'selecting between equivalent alternates such as ambiguous solution functions' more trivial
        	- identifying the 'maximally different subsets' that will stay under an error threshold for the same given general solution function is useful to solve for and apply once the 'maximally different subsets' of a data set are identified to filter the possible solution functions by taking a subset
        		- similarly finding the probability of a particular solution function by comparing the ratios of coverage of these 'different subsets, which are equivalent in their error range of a general solution function' (which function corresponds to a higher proportion of maximally different subsets under a minimum error range, this proportion representing an approximation of the probability of that function being the solution function)
        	- checking for set of 'points and directions' within a horizontal slice of a probable range (where the densities are, indicating where the solution function probably is within that range) is more trivial than checking for inputs corresponding to any outputs, allowing a method to skip points corresponding to outputs outside of that slice, as a subset of 'points with adjacent directions of change' is an alternate format of the solution function that can be used to determine the rest of the function, as finding the actual points in a horizontal slice of the data set and adjacent directions of change moving away from those points captures the value of a subset of determining points (in a high-variation slice of a probable range area), as an alternative to the usual determining points like extremes/inflections/averages, which are further determining of the rest of the function when paired with adjacent change directions, if the slice is in a high-variation section so that most variation of the function is captured in that slice (the same can be said for any horizontal slice of the data set but moving it to within the probable area range of the solution function and specifically to a high-variation subset makes it more useful, and pairing points with adjacent change directions increases their determinability of the rest of the solution function), so 'finding the most useful position of the slice' is the problem to solve, in addition to 'finding the point/adjacent direction pairs that determine a function, instead of the usual determining points/metadata'
            - identifying useful structures like different formats to handle cases where it's not possible to select between alternate solution functions, such as where a range of probable area is too large and functions within that range are equally or similarly possible, in which case every possible function in that range is more useful to format as a node on a function network, as every connection can be distorted to be every other connection type by applying some variable or error, these variables or errors being possible to apply generally across systems, so any data set could be the product of an error applied to some correct function and could therefore require some distortion to correct, so a set of weights indicating which function is likelier in a function network is more useful than a single function, or alternately formatted as a function generating a set of 'certainties' like angles (indicating a probably correct point, after which there is a divergence creating a probable area), and 'randomnesses' acting like 'ambiguities' where many possible functions exist
                - this is related to other methods involving handling alternate functions but applies the insight that 'every possible connection function between points could be valid and could be altered by excluded/hidden variables its likely to encounter until it becomes every other function, and having a function network where identifying the node where a function is gives useful information about adjacent/probable alternate functions, which is useful for identifying the probability of the function becoming other functions and the probability its an incorrect function, given some known correct input variables, this function network organized by probability of becoming adjacent functions'
            - identifying patterns of error feedback indicating obvious solutions to correct the errors is useful, such as where the errors of a function that is incorrect bc its shifted to another position follow obvious error patterns that can be easily identified and corrected
            - identifying points that, when some simple connection function is applied, can generate the most other points is useful to identify as simpler connections are likelier to be true (if the whole data set can be reduced to a set of center points and circular border points around these centers, these patterns are useful to identify as being significant in their repetition, structure, simplicity, and difference from simpler/core functions like lines, even if the effect of these overlapping circles seems like randomness or a linear function in some subset)
        	- identifying useful components to describe data set subsets like adjacent point sets such as 'filters/bottlenecks/convergences (connect only different surrounding directions) and random/circles (connect points in all surrounding directions) and extremes/limits/peaks (connect similar but opposite surrounding structures) as default structures of adjacent point connections to use as components'
        	- 'filtering structures similar to outputs to find the outputs' is a useful function such as how its useful to know a set of the most unique (maximally different) functions to filter maximal differences, and its useful to know the 'most similar function of the set of similar functions' to filter similar functions bc the representative/average of these functions is more similar to each item in the set and is likely to be relevant to other items in the set
        	- identifying new densities from a known density is often trivial bc identifying the midpoint (or other representation like density) of a one-dimensional set of points in some direction to discover densities in (from the perspective of the known density looking in that direction) is trivial compared to the task of graphing all points and finding all densities, where finding 'trivial to identify' densities is more efficient and can benefit from some transforms like angle of evaluation changing the problem space of 'filtering all points past this x-value' into a one-dimensional set (comprised of a subset of points in that direction) to identify a 'direction to move in', finding local averages in many subsets, densities being likelier to be less common and therefore more trivial to identify than the local subsets used to calculate local averages to select/create points to connect, and structures like 'equally distributed points in some direction which would make the task of identifying a density or average trivial' is less likely to occur in a realistic data set and calculations can be skipped in those cases
        	- identifying what point interactions (like high volatility, extremes in ranges, high slopes) can indicate possible sources of high variation, which are the most important points to identify, to identify whether an average line (or base solution regression line) needs to be adjusted, so that identifying these 'high variation-causing points' can be an approximation or alternate of a regression line-finding method
        	    - similarly, identifying which regression lines correspond to 'what types/sets/ratio/other metadata of points indicate the points that must be ignored in order to make the regression lines seem accurate', like how 'change rate changes' and 'difference from base function slope sign changes (positive/negative difference from base function slope)' have to be ignored to make a constant line seem accurate, etc, so that checking for these types of points once a base solution function is found which might contradict it is trivial
            - identify what variable interactions look like with various error type structures like combinations and filters of these possibilities to apply tests to input variables or gather more input info (expanding the workflow loop to include changes to test/data gathering variables)
            - identify at what point inferring that a 'more complex function with more peaks exists after how many negative indications indicating the opposite' is the wrong inference and can be contradicted 
            - identify filters to select/switch between function formats like 'probable function range areas' (useful when an area seems more random) or 'average functions with surrounding error vectors' (useful when an error or legitimate function range seems too ambiguous to resolve with current info) based on different complexity structures and cases in data sets
            - applying variables to create randomness from a non-random data set (or similarly linearity from a non-linear data set) and identifying whether some randomness/linearity-resolution functions also use those connections required to create it (if some system dynamic like 'system collisions or overlaps' is a randomness-resolution function and a collision/overlap variable created randomness, do the systems explain the original data set before the collision/overlap)
            - identifying error/legitimacy resolution structures (at what point does an outlier seem like a trend-change predicter rather than an error)
            - identifying gaps in uncertainty/certainty, randomness/linearity, difference/similarity, complexity/simplicity, and error/legitimacy spectrum variables that cant decompose some change type adjacently and the spectrums which can resolve them using some resolution structure (or the other structures using these spectrums which havent been identified yet)
            - identifying the most useful tests to apply (like 'a method that can identify new inventions as well as non-adjacent high-variation variables or sources of randomness in systems like "high-distance high-randomness variables like neutrinos" and "powerful/change-triggering variables like incentives as explaining most variable interactions"' as well as reversing the function to 'find system structures created with incentives/randomness' or 'a method that adjacently identifies useful interface structures from highly different as in non-interface structures') to check if a solution-finding method is successful at which point different solution-finding methods can be generated combinatorially and tested iteratively
            - identifying similarity structures between subsets of the data set (similarities such as 'similarity in intersection with some line type having some attribute', "similarity in a point's difference types from neighbors") which explain the data set the best, as points which are similarizable by these attributes are likelier to be related in that way in the system producing the data set, these similarities serving as inputs to probability of relatedness and therefore can be usable as a filter of points to incorporate in some algorithm to determine a representative line, compared to some difference set to generate a composite ratio of relevant similarity types compared to difference types
            - identifying inevitabilities of structures and the related structures like points/ratios/distances required to identify them (such as two slopes on either side of a peak making the peak inevitable, given the input variable interactions implied by the slopes and given the distance between them and the existence of other peaks already identified)
                - identifying evocative structures (which are similar enough to other structures to be useful in deriving them)
            - identifying relevant truths like how the 'extremes/borders/upper/lower limits of the probable area range' might be a better representative format of the function than a line bc different behavior at a higher vs. a lower value is a realistic possibility that occurs in real systems, or the 'extremes with another structure' (like an average or a probability distribution of a point being in a particular sub-area, as in a different probability distribution for each 'x-value' or 'local x-value subset')
            - identifying useful structures to add/remove such as 'areas of randomness' such as a cube of evenly distributed points, which its possible to filter into possible solution functions to connect it with more certain lines around it but is also possible to remove as a structure of randomness that indicates neither priority so can be removed as possible noise and added back in when new information might help filter the possible solution functions, in cases such as where the more certain lines around the random structure are equivalent and indicate none of the solutions in the random area as more probable
            - identifying useful alternatives (like whether to 'apply multiple average methods and merge the average outputs (like an ensemble of networks)', or 'whether there is room for improvement in the average methods and finding new variants of them given their variables is worth pursuing (like a function of method errors where a more optimal point is implied on the error curve)') in existing methods by applying interface structures like 'input-output sequences' applied to 'average methods' as an important component of the regression problem space
        	- identifying unit structures to apply as components of a data set range (such as a data set density or probable function range) include units like 'overlapping circles which are likelier to describe a probable function range of a typical data set which is usually an area rather than a clear line', 'semi-circles shifted around a linear average line', 'shapes that indicate exponential change and also some non-trivial area like curved rectangles or circles as tiles of the probable function range' to indicate variable interaction structures that could represent the legitimate variable interactions mixed with some error likely in a system of some probable complexity (local change tiles which are likely to be simple shapes distorted by errors in some degree/way), where a 'probable function area as opposed to a probable function line' indicates either a complex system, a system of maximally different or locally representative alternates indicated by edges/corners or local averages of these shapes where the interim points are errors, a lack of complete input variables, an error like randomness injected in the input data set, where a better (simpler, clearer & more accurate) representation of the data set would be a previous variable set on a prior causal node where the differences creating the area are graphed as vectors and the variable interaction can be graphed with a line
        	- just like some structures are useful in their simplifying and explanatory effect (like 'core components' leading to outer 'interaction levels' supporting 'maximal differences (like a function network but in every direction)' where 'interactions are adjacent between nodes'), other structures that combine important useful interface structures like 'core components' and 'interaction levels' to achieve useful intents like 'adjacent/linear interactions' are similarly useful, and can enhance the usefulness of these structures, such as 'cross interaction level errors' like what errors can happen when an output layer interacts with the core input layer, or a way to organize the structure so that frequently interactive functions are adjacent even across interaction levels
        	- identifying 'sequences of change sets' that are commonly seen across systems, to identify common useful functions found in sequences like a 'creativity/generation' step, a 'maximally different/uniqueness' step, a 'standardization/grouping' step, an 'abstraction' step, a 'contradiction/neutralization' step, a 'incentives/efficiency' step, a 'stress/competition' step (like to see which variables stay constant and which can maximize their variation), an 'explanatory/decomposition' step, a 'understanding/organization' step, a 'ambiguity or other error generation/identification' step, a 'randomization' step, in a way that reflects real change patterns, and applied to connect known certainties (probably certain trends in the data set) and uncertainties (the remaining variation), these 'sequences of change sets' being more useful than other function formats to identify commonly repeated useful functions, as well as other patterns in change sets like cycles and equivalent alternate change sets, some of which are simpler/more useful than the original connection between original inputs/outputs, and to identify different useful function formats like 'sequences of vector structures & maps' applied to inputs (which move in the direction of function networks and allow queries like 'which are the most commonly useful function networks' and 'which function network compresses all other function networks adjacently'), as structures of 'useful incompleteness' created by mixing cross-interface structures that leave irrelevant variation unhandled and are adjacently transformed into useful variation-capturing functions
        	- condensing variables as 'some or any change in this variable set of the same type (rather than specific changes in each variant of the type)' is also useful to quickly identify sources of variation on a different interaction level
        	- identifying structures like areas/slopes of maximum volatility (adjacent input-extreme output connections) and connecting them to areas of constance as a way of identifying the maximum volatility allowed by a filtered set of functions in a more constant subset of the data set, as connecting the filtered set of a function in a more determinable subset and the volatility allowed by that filtered set can determine other local subsets of the data set
        	- identifying useful similar structures like overlaps/convergences as indicators of similar but different types of change (overlaps being an indicator of robustness or probability, and convergence being an indicator of some limit structure or an average structure)
        	- given that embedded variables (meaning 'embedded on interface variables') are likelier to create 'maximal differences', testing those as a default filter to describe complex data sets that can easily look random in some subset is useful in this problem space
        	- given that there may be a transform of the data set (like a subset of input variables) may be a more efficient way to generate the probable data sets found in real life (as in, there is a subset of input variables around which most of the variation is clear and easily explained by more common functions, indicating the original input variables include random noise), its useful to identify subsets of variables with simpler variation as complex systems are less likely to be stable and are likelier to contain random noise from variance injections, similar to how identifying simple structures like 'evenly spaced repeated data set subsets' and 'densities' can identify common patterns that are simpler to model as a pattern or a component of the solution function if they represent a sufficient ratio of the data set
        	    - relatedly to the workflow involving removing simple structures, removing some subset of the 'points' that are connectible in 'straight lines' in a data set is useful to remove structures too simple to be useful (in the sense of relevance for the intent of capturing information beyond a general simple summary base function to use in finding the actual solution) in describing a more complex data set as is more commonly found than a simple linear data set
        	- some structures are more probable than others 'a range of data points around a pattern (as in an area of probable range of a function) which describes likelier structures like errors' is likelier than a 'highly variable specific function (like a function with many peaks)', so these likelier structures can be applied as more default than other structures
        	- function sets which are adjacently transformed into each other which have some base of similarity in common (like accuracy, variation, input/output patterns, or input variable subset ratio) are likelier to be equivalent alternates and probable solution function sets than other function sets
        	- 'maximally different data sets with known error types applied, mapped to solution functions' are another useful starting point for reducing the regression problem
        	- variables like 'dependence/connections between inputs' (such as a 'sequence using the previous adjacent input as an input', as opposed to a 'function of independent variables') that create dependent/independent variables are also useful to identify, as these variable differences are important to filter out, where independence of variables is not obvious but a sequence is easily detected
        	- identifying structures (like isolatable structures, including obvious components, such as 'anomalies') which are likely to explain some variation interaction patterns (like an 'occasional extra peak disrupting a more probable pattern') as common error structures to remove to find simpler functions and add to find error-handling functions or erroneous versions of data sets, similar to how modeling a 'system cascading to destruction' is useful to identify signals of these error structures in data sets, just like identifying 'probable new more stable states' of a data set is also useful
        	- increasing the 'directness' or 'adjacency' of variable interactions as being more explanatory, more likely to be linear/simple, and less likely to be subject to undetectable noise/errors is a useful intent rather than trying to identify distant connections between indirectly causally linked variables, as indirect connections are likelier to change (but also likelier to re-occur when removed so extremely distant effects are still useful to model)
        	- similarly, given how the problem of 'finding a function to summarize a data set' is a problem of 'finding missing variables (coefficients, powers, etc) that are not in the input', other formats of a problem are similarly useful to apply as defaults (what are the patterns of missing variables, are they more difficult to detect, are they less visible using data gathering techniques, do they have similar complexity like 'host dna' & 'pathogen dna' which allows inferring the 'existence of dna and pathogens in a host', and 'default function errors (like immune errors)'), similar formats like 'stable variables' to identify variables that are more stable than other variables so they will become 'high-variation'-causing variables, a useful structure to identify with 'high-variation variables'
        	    - for a more complex specific example, identifying people from their dna involves identifying 'missing variables' (like phenotype-determining genes, as well as 'genetic change variables' such as epigenetic changes, errors in dna-phenotype mappings, mutations, disorders in dna editing, etc and causes of these 'genetic change' variables) and filters of these missing variables (tests of these variables to help filter them or legitimate stable differences to filter/separate these), which are variables in between the inputs/outputs in different interaction levels than the inputs/outputs like the 'gene function' interaction level
        	    - similarly, the machine-learning problem involves finding 'missing in-between variables' (like a 'function to create differences (to find variations of structures like combinations of inputs)' and a 'function to attribute/connect useful differences to filter out (like connecting an error difference to a node or node structure)' and a 'function to calculate differences from correct values (calculate the error/loss from a particular input variation combination)')
        	    - these are highly complex sets of variables (input/output subset filters, high-variation change inputs, 'instruction' functions generating similarities, variable interaction functions, generative functions, difference-filters) which fits with highly complex systems, decomposing them to slightly less complex variables on different interaction levels, which can help with identifying missing variables (such as inferring 'alcohol' as a cause of 'genetic changes' when other complex sub-systems arent sufficient to decompose all variation in a data set, as a source of variation from another interactive complex system, given how complex systems usually are created by interactions between multiple complex sub-systems)
        	    - variable interaction functions like 'add/subtract/neutralize', 'limits/extremes', 'filter/differentiate', 'repeat (like as a default)', and 'interfaces/averages' are common (or even required) across systems so are likelier than other variables/functions to re-occur in unknown systems, which makes them useful to apply as default variable interaction structures
        	- similarly, applying known 'contradictory cases' of 'when known rules are wrong' such as when a more general function is wrong compared to a less general but more accurate function despite a common solution metric like generality
    	- similarly, identifying the 'core shape of a local subset representing a change unit of the more general function' and 'its interaction function (such as overlapping with other core units)' and its change functions (such as how it can 'rotate to some degree or vacillate in some way') as a way to identify the general function from a local subset that is sufficiently representative of the differences in the data set that it can be used to find the 'core change unit shape' that can be repeated/shifted/scaled/rotated/otherwise changed to find the rest of the function using some interaction function
    	    - relatedly, finding the network that filters the possible core change type combinations (rotate, scale, shift, vacillate/cycle, embed variables, repeat, abstract, connect, format, etc) in a maximally efficient way for most functions is a useful intent to fulfill as a default implementation strategy to start with
    	- similarly, identifying useful structures like 'maximally different directions of change such as the cardinal directions (or high/low left/right directions of change)' as useful structures to use as a filter to identify different change types that are common and highly different, as a 'maximally different unit of change' which is useful to find adjacent changes
        - similarly, identifying connections between variable interaction structures (like how a variable structure such as a 'variable upper bound and a constant lower bound' have a useful structure of "implying but not guaranteeing (making them probable and useful to test)" other variable structures like 'more change (either expansive/reductive) happening in the upper range' or 'fewer limits on change in the upper range' or 'more change incoming to the lower bound' or 'interface variables and/or constants relevant to the lower bound') which can be connected to possible filters/limits of those structures, such as whether other variable interaction structures (or specific known problem space system rules) filter/limit/prevent a possible implication
        - similarly, identifying local subset representation structures (like densities or average lines explaining the majority of differences in the subset/densities) for one local subset and then removing those probable components of various possible representations in another subset to find additional possible variables in maximally different subsets to filter the set of possible alternate representation structures (and their components) from the original subset, after identifying the maximally different subsets likeliest to have differences in probable representation structures & their components
        - similarly, identifying connections between 'uncertainties' (like subsets of the data set that are more uncertain/variable where other subsets are more easily determined) and 'uncertainty resolution functions'
            - resolution functions like 'intersecting with the most points in the subset with the simplest line' or 'the simplest line that remains some minimal distance away from the most points in the subset' or some balance of 'averageness' and 'intersection' which are useful variables of filtering/generating these 'alternate regression lines' in 'highly uncertain subsets'
        - similarly, identifying useful structures like the 'variation range' necessary to create a useful structure like an identified 'acceptable error range area' (with a trivially identified area of coverage that is likely to be useful in finding a maximum of points falling within that area when moved across the data set 'probable regression range') that can be moved across a 'probable regression range' and tested in various subsets to cover a 'ratio of the data set in that subset', this 'acceptable error range area' being useful to avoid calculating the error for a possible line at every input value and just checking if it falls within the range area centered in some 'probable regression range', either after calculating some 'probable regression line' or using the 'probable regression range' to fit it inside that range, or finding the trajectory of the 'maximum coverage direction' as the area is moved across subsets
        - similarly, identifying the relative usefulness of structures like 'intersecting/overlapping summarizing lines of data set subsets' as opposed to 'adjacent local subset summarizing lines with strict range limits' to allow for possible overlaps in local subset selections bc the borders of these ranges might not be guaranteed/required by data and to allow for known variable interaction patterns like 'multiple possible interaction types/states involving the same variables, variable interactions which can explain variation in outputs with similar/adjacent inputs
        - similarly, identifying alternates of useful structures like 'randomness' (such as how 'randomly dropping a ratio of data points can reveal robust variables') alternatives such as 'combinations/sequences of common/powerful functions across systems' can identify more probable structures to replace these less likely structures, using probability structures like 'commonness' to replace the less accurate/likely structure like 'just any randomness at all (a random selection of randomness)', which applies 'more probable & less random' randomness created by a higher degree of certainty through alternate structures like 'commonness'
            - similarly applying more relevant/useful structures of randomness like 'obvious structures like simple shapes like densities removed from the data set' which are more relevant to workaround (as simple-minded agents are likelier to intervene with obvious human error and likelier to be required to stop the interference of), as true randomness is unlikely to be easily verified like obvious randomness and is less relevant/useful to account for (above some ratio of expected noise), as more complex randomness could easily be hidden legitimate variables undeterminable from the original data set requiring more data/variables to identify such as 'incoming changes' to the data set
        - similarly, identifying a function to convert non-linear to various probable linear functions (more adjacently computable, or using fewer variables involving equivalent alternate variable subsets of the data set) is a useful intent to fulfill with structures like logarithms, topologies, & mappings to model 'interaction levels where interactions are adjacently computable with objects defined on that level'
    	- similarly, an 'index of pre-computed regression lines to compare with original data set subsets' to fulfill useful intents like 'avoiding computation' is useful for connecting original data set subsets and pre-computed regression lines for alternate subsets
    	- similarly, how clustering relevant structures (like inputs) by differences/similarities (such as organizing by the 'similarity/equivalence of output value') can identify useful structures like input patterns of similar outputs given assumptions like 'non-volatility' and 'non-randomness' and 'non-uniqueness of inputs for each output' (like input subsets that have clear patterns, like how a wave function organizing the inputs by similarity of output would have clear patterns of magnitudes/amplitudes in a few subsets that prevent requiring checking the whole input space for the pattern, as the output y-value would have a few data points associated with it such as 'input value points graphed vertically with the y-value on the x-axis' representing inputs having that output, where this vertical input pattern associated with a y-value would have clear patterns such as obvious differences in distance between points that are clear after a few points rather than many), which reduces the problem to 'find n y-values having the same value m times to check for obvious patterns in x-values for each y-value, if there are multiple x-values'
    	    - preemptively testing for validity of 'assumptions of algorithms' is an example of 'alternate equivalent structures' (like limits/requirements/intents) and an application of the 'input/output sequence' of optimal algorithm filters
    	    - this applies a useful function like 'sort' to the outputs rather than the inputs to get useful information about similar outputs once sorted that way such as how variable/cyclical/maximally different the function might be by comparing inputs of equivalent outputs
    	    - once outputs are sorted, it is possible to find highly different outputs easily, which is useful for intents like 'check for maximally different outputs' or 'find output range or output extremes'
    	- similarly, 'finding average magnitude/amplitude/count of peaks in a data set for some subset' is useful just like 'finding the general average value' and 'finding local subset averages' and 'finding reoccurring subsets' and 'finding highly different local subsets and their connecting functions' are useful
        - similarly, a 'adjacent points merging function (using some representation like an average midpoint, or using a similarity metric like distance from/angle to local densities, and using some ratio selection like merging n points at a time within some distance m of each other)' and a 'adjacent point non-merging function (leaving some points unmerged bc of their representativeness of legitimate differences)'
            - similarly, an algorithm to identify variables that can capture high variation when applied together (representation metrics, similarity metrics, ratio of input metadata like count/difference score) as useful complementary capturing variables of information, acknowledging differences in data sets like that averages are sometimes more useful than similarity metrics and sometimes the opposite is true and there is a useful input type like 'input count/ratio of the total count' that is optimal for some structures like high-variation data sets with some noise level
        - similarly, applying similarities between structures that have a reason why theyre useful for summarization/representation intents (like how 'big/simple' shapes like 'equilateral' shapes are particularly useful to identify to simplify the task of identifying a regression function bc finding their centers/averages/densities/patterns is more trivial and likelier to reflect reality as big/simple shapes are less likely to occur by accident in a data set and therefore likelier to summarize the data set, up to a certain point, like how identifying that a data set as a whole has a generally square shape makes it likely to be equivalent to random)
        - an example resolution function between 'densities and regression lines' is 'divide into subsets, then find one representative density for each local subset, then expand densities until an overlap/equivalence is reached with another expanded density' by applying the 'reason' for why it would be useful to connect 'representations (like density averages) of adjacent local subsets' (bc adjacent local subsets are connected in the original input data set, so the reason to connect them (or a variant of them like a representation of them) later is that connecting them later 'aligns with the original input' in a relevant way, relevant by 'preserving the information of the original data set' which is useful for the 'find a regression function' intent) and applying the structure of 'how' to connect them through 'expanding' them (and the reason why to use that, which is an adjacent transform applied to a density average and is therefore useful, where equivalents are also trivial to determine, and these operations in total can beat other regression algorithms in some solution metrics)
        - similarly, a function that connects the 'points that vary' and the 'points in common' across multiple probable regression lines is a useful function to solve for 
            - finding the sections of the regression line that would be variable in variations of the bias vs. variance tradeoff, to focus on finding functions to connect the 'points in common across probable regression lines' that should be optimized for in the final function, where the 'points that can vary across regression lines' can be averaged or otherwise represented by known probable points at discrete intervals rather than a continuous line, where a point not on those points can be approximated by adjacent points, where the 'points in common' can be approximated by a range that is narrower than the range for the 'points that vary' and the range representing a range of acceptable solutions, so finding a function to resolve the reduced solution set of the 'points in common' connecting functions by connecting these subsets with the functions describing the 'points that vary' is a useful function to solve for
	     - finding the connecting function between different sets of summarizing functions like the 'average' and a 'slope-standardized function (to find the useful standard to compare changes to, to find the core differentiating vectors from a straight/average line) and its scalar to scale it to the original' and the 'lines that describe local subsets to the points of extremes (similar to eigenvectors)' and the 'lines that connect averages of densities' is a useful function that connects these alternates which offer the same representation attribute but also capture different information in the data set, as connecting 'efficient representations' is more trivial than connecting 'every data point'
	         - finding the 'useful core function representing the most standardized (such as de-scaled) function' is useful to find a 'component function' to check against multiple subsets of the data set (do any known variable interactions create change types other than this component function or component function range or do they follow the structure of the component function/range) and look for variables that adjacently create/scale the core component function to check it for realistic probability of representation of variable interactions
	     - finding the most important structures to check for when filtering possible solution functions (such as how its important to check if an amplitude of a polynomial is different across different peaks to determine if a peak pattern can be applied/found, how its important to check multiple local subsets of the function input range, etc) can act like maximally differentiating filters of the solution set
    	- finding the useful ratios & other structures of inputs to an algorithm like 'find the common slopes of connection lines between points in local subsets of the data set', where the algorithm to find the 'useful ratio/count of slopes in common (a ratio compared to some standard, like the number of possible connections)' is the target to solve for, as the other structures that are useful are already known or easily determined and the uncertainty is in finding the threshold values or other values to optimize implementations of those structures
       - connecting alternate formats of the data set/regression functions like 'maximally different connectible shapes (like interfaces) that can be formed by a data set subset of some ratio' which can be used to indicate 'embedded variables' (like variations on that interface) is useful for determining one function format from another which may be more trivial than another method
        - other structures than standard regression structures (averages, connection lines, subsets) like 'maps' can be applied as a useful structure in the regression problem space bc of how mapping one subset to another through substitution can be an efficient way to decompose a more complex set of points into a more standardized or otherwise useful set that is likely to represent the original set and requires less memory to store, which are useful as components of solution-finding methods
        - finding useful metrics like 'degree of erroneous difference to ignore' between obvious average functions of local subsets is useful to find out what information to ignore when an average line of one subset differs to some degree from an average line of an adjacent subset, especially if the next subset confirms the original subset average line, applying the concept of 'data corruption' to describe some degree of error deviating from some implied metric, resolving these 'implication' structures (like the implication of a 'common subset average line') into 'conclusion' structures (like a degree of commonness of that line across subsets above some ratio), and finding useful tests of these differences, to find out when a difference may reflect a common or otherwise probable/implied structure (implied by adjacent inputs, common patterns, similarity to known implications, etc) rather than an erroneous anomaly to ignore
	- in the 'network (fuzzy space) of structures' fulfilling intents, interface structures like 'overlaps' exist between structures adjacent to or otherwise useful for multiple alternate intents, these interface structures indicating their usefulness for other intents like 'deriving alternate intents' and 'building a maximum ratio of structures'
        - in the space of useful structures, concepts like 'balance', 'alignment', 'simplicity', 'probability', 'composability', and 'uniqueness' will be obvious, which can be used as 'conceptual filters' of useful structures that are likelier to be useful than other structures
        - an example of this fuzzy space includes structures (like 'angles, partial closed shapes (like sides and corners), connection functions of partial closed shapes, higher-dimensional closed shapes, shapes that when combined can produce a closed shape in between them') as the set comprising the fuzzy space of a 'closed shape', this space being composed of components adjacent to or otherwise useful for fulfilling intents (forming/describing/differentiating a 'closed shape') related to a 'closed shape'
        - a network of similar/equivalent alternate spaces include a 'non-repeatable/unique component space (which is optimal for storage minimization)', a 'repeatable component space (which is more optimal for displaying usages/queries of components)', a 'usage adjacency component space where frequently co-used components are adjacent (that is useful for finding probably useful structures using a component)', a 'difference as adjacency space (where maximal differences are possible with adjacent queries)', a 'layered space with both intent/structures (where the fuzzy space of an intent contains maximally different structures fulfilling/adjacent to fulfilling that intent)' bc of the adjacency of these structures for these intents related to components
            - relatedly, a 'usage network (apply)' has adjacent corrollaries like a 'filter network (find)', a 'component network (build)', a 'difference-resolution network (derive)' due to the core functions it is an equivalent alternate to
        - graphing an intent by its 'surrounding related structures' such as by 'structures that use it' or 'structures that fulfill/build/create/cause it' or 'its input/output structures' or 'structures that filter out everything in some relevant subset but that intent' (or similarly 'structures that determine that intent') is another way to visualize structures like intents that are more useful when defined as a set of alternate definitions which can represent examples of them in some other system
    - framing common structures with relevant metadata like useful intents in standard terms to maximize the optimal positioning of these structures in queries
        - a network (which depicts uniqueness and similarity) is useful for 'finding new/different unique similarities by the gaps & other structures in the network' as well as 'identifying difference/similarity of two known structures'
        - a map is useful for 'finding unique connections & other metadata about connections' (like the commonness of connections having equivalent/similar connection/input/output) as well as 'translating a structure in one format (of a set that can be described by a network indicating uniqueness/similarity) to a corresponding position in another format (of a set that can be a network)'
        - a filter is useful for 'finding a similar subset of points in a network/set of points' (as in similarity to some attribute, like a solution structure/metric/requirement)
    - finding a good starting point to start applying interface structures is crucial for deriving adjacent solutions, like how a limited subset of 'logical rules such as definitions' or 'physics rules' or 'truth limiting rules (what is definitely not true)' might be useful as a starting constant input to start applying interface structures (like changes) to, to derive other rules that follow logically, are required to be true, are implied, are not contradicted, or have other structures of truth associated with them
        - similarly, finding a 'useful structure to describe common patterns in changes' is an example of a useful isolateable structure that can be a good approximation of a full implementation on its own, answering questions such as 'are most variables an adjacent combination (or other core structure) of some subset of interface structures', which is findable with iteration
        - similarly, finding a 'reason for similarities/differences' (reasons like 'its an efficient/useful combination of few inputs commonly available, so is often repeated across irrelevant systems') is another isolateable rule set that is a good approximation of a full implementation of all logic rules of interfaces
        - the differences between these 'equivalent alternate' isolateable rule sets that are good approximations of interface analysis make them useful to combine in an adjacent combination as offsetting 'ensemble' structures to weight the impact of their outputs against average outputs & other representations of outputs
        - this is like how everything can be framed as a component that can be added/multiplied to other components, but thats not always useful in terms of reducing computation requirements, such as how knowing 'addition' and 'multiplication' are capable of describing all other structures, but that doesnt capture a useful degree of complexity of the potential interactions of those two operations, where knowing concepts like 'self' and 'embedding' is more adjacent to the complex operations/functions possible with addition/multiplication (self-multiplication like 'powers' and embedding as in 'embedding of operations'), these two concepts being adjacently derivable with interface analysis through core structures like 'unit/identity (self)' and 'application/usage (embedding)', similar to how matrixes (aligned multiplication of ordered sets) and inner product spaces (spaces where some product is trivial to compute) and convolutions (complete multiplication product of sets) are not adjacent to just the functions add/multiply
             - this is a useful structure for tasks like 'encryption' as the 'set of concepts that are adjacent to a useful structure' is more difficult to guess (from the set of all possible concept sets) but is easy to verify
    - applying interface structures to optimize with ml, such as by applying 'input/output sequences' to position 'generative feature layers before filter feature layers' and other patterns that make sense to server as a useful contradiction to offset the irrelevant variation introduced by the preceding layer, or applying 'self' to apply neural networks to select preprocessing & algorithm functions/parameters
    - to handle common error structures like 'dead ends', apply structures like patterns to find the solution to (the 'way out of') traps using these errors, such as by applying insights like 'nothing is unconnected to everything' which means 'there are no real dead ends' as 'everything is both true and not true in some way' and find the distortion of the perspective that created the error of the 'dead end' and connect it to the balanced perspective (where interface structures exist or are adjacent), creating a difference that allows other differences to be embedded/connected/supported
        - if there are filters allowing for only one possibility (a 'dead end' error that leads away from a network), find the filters that represent the errors in those filters, reversing the perspective back to the balanced perspective and exiting the perspective creating the error of the lack of variation leading to that error, creating opposition to the incorrect difference
        - inject more variables to connect the 'dead end' position back to the balanced perspective, applying the interface metadata to create new differences (such as random differences through interactivity) where required to offset the incorrect differences of the over-reductive perspective, and use those differences to build differences on them and create change in another direction
        - where one structure seems to capture everything (like an attribute network), apply differences to identify other networks that capture alternate complementary information (usage networks, limit networks, difference networks, function networks, etc) to limit the limits of that over-reductive perspective
            - like how an attribute (such as 'criminal') can obscure info or over-simplify info, despite being an efficient way to store some relevant info (like the 'crime of jay-walking' being equated to all other crimes despite the fact that it is mostly only criminalized to collect fees to fund police stations and other far more harmful behaviors are not punished at all, such as stalking/copying inventors), whereas a network with contextual or functional info can more accurately depict that info to reflect its true variation
            - this is similar to how one infinity can be used to create other infinities (like the Banach-Tarski paradox) bc these graph structures are like topologies or fields in that they are capable of describing reality at every point but offer useful alternate advantages when starting from different points
        - this is like the structure of a 'mobius strip' or an 'isolated system describing the system containing it by identifying variables of observations of subsets of its own structures (regarding the incompleteness theorem)' where one structure seems ambiguously equivalent to different structures that seem to contradict each other but actually can co-exist in the same structure using the perspective interface, or like the structure of a 'rule set' occupying a point on a torus where what determines one error doesnt determine another error bc different 'rule sets' are supported and the 'rotation' and 'injection of new interaction levels (as different concentric circles allowed to be the base/core/stable level)' allowed in the torus shape enables endless (stable) balanced variation
        - a 'balance' of variation is a core structure driving interfaces, as there needs to be some variation-supporting structure like a ratio of change (like 'potential/kinetic energy' and 'momentum') and counter-change (like 'gravity', 'energy preservation/transfer limits'), or a structure like 'caring' ('connection to the most stable foundation, where this connections acts like an equivalence') that is supported and stable, otherwise the interface cant exist or similarly cant support any change, which can be used to find interfaces
        - on the other hand, filters may seem restrictive/limiting/reductive (like a trap that is a 'required error' such as a trap hiding a 'dead end' error) but may enable endless complexity/variation, like how a 'reality' filter may seem boring/reductive/simple until you identify how complex reality is, and that the 'reality' filter is powerful in that it empowers other structures to exist like 'clear descriptions/representations & measurements/experiments' as well as constants/variables and consistencies/contradictions to occur, enabling the pursuit and identification of truth and falsehood and the application of differences to both
        - "filters to identify errors such as common structures of paths leading to 'dead end' errors" are a way out of errors of 'constance' (like 'over-prioritizations' such as 'over-reductions') just like finding 'abstractions' or 'interaction levels' or 'equivalent alternates' or 'embedded perspectives' or 'connecting function of perspectives' are a way out of traps
        - finding a common perspective that hosts both a trap/error and also a solution/way out of it (or generally a 'perspective that can trap any trap' as in 'capture the variation of incorrect differences driving traps') is an example of a useful interface to apply to other problems
        - similarly, 'traps/errors may be more adjacent to a solution than another trap/error', so creating a 'path of differences leading to errors' is not just a way to find errors but also find out what is not a solution and therefore what is a solution, and value can be created from a trap if there is a way to convert it into a solution like by applying it to itself ('trap the trap')
        - the 'maximally different structure' that can support the most difference types is also the structure that can support adjacent structures of differences, like 'ambiguities' (like the ambiguity in an equivalent distance of one error from the center of this structure to the distance to another error, and like 'contradictions/paradoxes' and 'counterintuitions/complexities' and 'alternate representations' as different variations of 'maximally different structures producible with the same inputs, supportable in the same system')
    - finding a solution base that is optimal for different algorithms allows finding different possible solution bases, at which point finding the more optimal adjacent solution to those bases is possible with known/adjacent algorithms (as opposed to finding 'maximally different solution bases' to start from with one particular algorithm)
        - finding the maximally different functions that make an adjacent optimum findable with some algorithm helps 'filter out these more adjacent solutions if theyre incorrect' and 'find counterexamples of alternate possible solutions' or 'divide/filter the solution space' faster, such as by finding 'common overlaps in solution spaces generated by different algorithms' where these overlaps are more trivially calculated in some way than by applying either/both algorithms
        - deriving the 'solutions findable/verifiable with an algorithm' is a useful way to filter the solution space, find common solutions across algorithms, and find variables of algorithms to find other algorithms or match algorithms with metadata like intents/metrics optimized for
        - finding a 'network of algorithms with rules for switching between algorithms in certain cases like with certain input patterns or certain solution metrics' is also adjacent using these structures of connections between 'algorithms' and 'solutions/ranges/sets adjacently found/filtered with those algorithms'
    - finding the set of concepts that builds a solution (such as how 'adjacency' and 'generality' help build a 'regression' solution) which help to offset each other ('adjacency' in the form of 'adjacent/local subsets' or 'adjacent/local optima or adjacent/local density averages' offset by 'generality' in the form of 'representative subsets' or 'representative summaries like averages') can help form a common base set of solution structures to build on top of
        - these are useful specifications of more general spectrums like 'specific local variables, as opposed to general abstract patterns/summaries', 'adjacency' being a non-definite partial format of 'specificity' in the 'regression' problem space, having the additional concept of 'triviality' included in the subset of concepts driving 'specificity' to relevant variables like 'position (point, density, extreme)' in the 'regression' problem space
        - finding regression lines whose differences from data points frequently follow a common component pattern (like 'having the same distance from data points' or alternately 'use the fewest components (one component as in the same value)' which fulfills a 'simplicity' or 'linearity of combination' metric) is another useful intent to fulfill in the 'regression' problem space
        - similarly, finding a set of components that commonly connects extremely different points (like points from different non-adjacent subsets at different limits (upper vs. lower) connected to some representative subset like an average) is a useful intent to fulfill in a regression algorithm, the specificity of this intent making the algorithm trivial to find
        - similarly, applying rules of relevance such as 'removing variables with equivalent information coverage is acceptable if one variable remains to cover that information in an intent related to an information-preservation intent but its better to leave the variables/rules generating those variants in the data set rather than leaving in one variant or all known variants in some cases like when minimizing memory storage is prioritized'
    - apply common structures like filter/reduce/match/add/change/map/sort to format functions to identify the maximally different functions, then find the reason why those functions are useful as a way to identify 'rule sets that can act as limits of usefulness to identify the areas and boundaries of usefulness'
    	- example: 
	    	- bc its useful to have a 'map of a keyword to different variants of it', its possible to identify that "a unique signal has variations because of alternate definitions/formats it can take while still remaining unique (bc of 'definition routes')" (allowing this 'variable interaction' & 'variable interaction-structure rule' regarding usefulness to be derived: 'bc this variable interaction exists, keyword maps/definition routes are useful')
	    	- bc its useful to have filter functions in general, its possible to identify that 'not all differences are adjacently useful for every intent' and 'different variations of structures are not organized by default' and 'different structures can coexist'
	    	- bc maps (connections between 'user-assigned (relatively arbitrary)' rather than 'absolutely-defined' values) are useful, 'arbitrary connections' are also useful for intents related to randomness/uniqueness, like when connections dont matter absolutely but are still useful to assign (like organizing a filesystem to optimize common queries such as using symlinks or naming conventions, even though that organization doesnt reflect absolute truths of the universe, or such as how mapping several terms to one identifying term allows quicker identification of unique term usage while minimizing memory storage, or how an explicit map to identify a category of an attribute value set is useful when identifying the causal variables is non-trivial, to skip that causal analysis, or how substitution maps can create the appearance of randomness bc they are relatively arbitrary)
	    	- bc maximally different functions (filter, map, change) often co-occur indicating their usefulness, its possible to identify the 'common complexity of problems solved more frequently recently (bc of existing solutions to simpler problems)'
	    	- bc similar functions often co-occur (filter, sort) indicating their usefulness, its possible to identify that those functions are cooperative for various intents ('sort' speeds up some filters in some cases)
	- applying rules to identify when a regression function can be incorrect mapped directly to problem space structures
		- example: when a high ratio of incidental/random variables (like variables about a context that any species can exist in which is not relevant to identifying a species) are included in the function, or when an important variable is ignored to fulfill an incorrect priority like simplicity, these errors are mappable to differences between the incorrect function and data set subsets
		- when a more simple function is found but it contradicts the more complex function that would cover more cases bc of the data set subset chosen/available that makes the simpler function seem correct, its violating other known solution priorities like generality which can offset over-simplification errors
		- more importantly, structures of this error can be mapped to a set of example possibly relevant regression structures (data set subsets and regression lines)
		- for example, when there is a 'small pattern in a local subset indicating a more complex function' which is 'within the boundaries of a potential field of variable interactions' and 'not overlapping with probable error areas' but is ignored in favor of a simpler function, thats a structure that could be an error of 'ignoring a general trend bc of a data set subset selection (applied either before receiving available data or after)'
	- applying interface structures to known useful structure (like gradient descent) given their definition can identify relevant structures useful to those useful structures like optimizations to apply to inputs before applying the useful structure
		- the idea of 'gradient descent' is most useful when applied to a 'function that is adjacently connectible to the optimal solution' (meaning the inputs are already adjacent or equal to the actual optimal inputs) bc of the definition of 'gradient descent' which applies 'adjacent change combinations', assuming that 'adjacent change combinations' are capable of finding an optimal (which is best used when 'everything is adjacent' or 'maximally different examples are adjacent', etc)
        - this makes 'maximally different interaction levels' a useful structure to identify, to make most variable combinations or 'the most variable' variable combinations adjacent (which is what interface analysis does by default)
        - for example, to identify the structure of 'imaginary numbers (like i)' as a possible useful structure, some intents (like 'create a circle' or 'create common structures (like a circle)') make it obvious to try as a component of formulas and its not adjacent change combinations as theyre typically implemented ('addition/multiplication') but rather as an adjacent combination of interface structures (like identifying how addition/multiplication arent easily describing all structures and solving all problems, so try applying the 'opposite' of 'components of those core operations' to try to generate differences to use to describe different structures), for example to solve the problem of finding 'equivalent alternate maximally different descriptions/generators of a circle as a useful structure to describe/generate/apply', otherwise the idea of 'square root of negative one' is not adjacent to most intents but is obviously useful to identify, so identifying 'useful intents that would make such structures obviously useful' are useful to identify by applying interface analysis to fulfill probably useful intents like 'create common core components in different ways'
        - as another example, the way I discovered that circles are related to primes is by picturing multiplication in my head, in which I could easily generate squares/rectangles but I noticed there were gaps between these shapes' corners which were easily generated by integer factors, and these gap points were likely to be more describable as on a curve rather than as a product of integer factors
        	- https://twitter.com/remixerator/status/997394393471516672
        	- it also reminded me of the fibonacci sequence bc of its embedded growth between numbers in the sequence rather than clearly exponential or clearly linear growth, as a different type of default growth that was neither of the two simpler types
        	- the idea of 'gaps between integer non-1 factors (when graphed as pairs of factors)' is not actually definitive as indicating non-linearity but is still evocative and therefore still useful in adjacently deriving that non-linearity is relevant, these 'evocative' rather than 'implicative' ideas are still useful in deriving new probable structures to test
        	- this insight path would be easily identified with interface analysis 'which asks questions like "what something is not" and "what is not easily generated by these variables" and "what is not adjacent" and "what is different" and "where would similarities be expected (like with other integers) but differences are found instead (like with primes) and what interface unites these (like the dichotomy of curves vs. rectangles)" whereas machine-learning dumbly applies a few insights at a time such as 'adjacent change combinations eventually can find some useful functions if you have enough compute',
        	    - while applying other relevant structures, like filters to avoid irrelevant exceptions/rules like pieces of the definition that most closely overlap with other more relevant functions like the 'identity function (using multiplication)' being more relevant than 'addition/multiplication' as the identity function is less meaningful than non-1 factors)
        	    - identifying the alignment between 'numbers between non-1 integer products in 2-d space' and 'numbers between non-1 integer products on the number line (primes)'
        	    - other useful structures to apply are:
        	    	- 'equivalent alternate formats' of primes and their factors (the relevant default variables) such as 'sets of alternate factors and lines connecting sets'
        	    	- the basic 'interface' structure which connects the 'equivalent alternate formats (number sequence of primes and factor sets of primes)' using a common standard they both adhere to in their similarity of patterns/gaps
            - relatedly, deriving other useful structures like 'e' can be formed by maximizing output change types (differences between adjacent values) while minimizing input variables (the 'previous value' and the 'previous previous value' and the 'addition' operation) without using exponents and using a simple function (minimized differences necessary to create the extreme differences)
        - similarly, 'adjacent change combinations' like addition/multiplication are not frequently useful for useful intents like 'differentiate a wave from a circle' which a machine-learning algorithm typically would not adjacently achieve but interface analysis would by applying structures like circles by default as a common core structure on the structure interface
    - applying interface structures like 'causal sequences' to identify useful structures like 'alternate points where variation is useful to inject'
    	- for example, injecting variables at the point of data-creation/gathering rather than data-processing (pre-pre-processing) could influence the value of data (its reflectiveness of reality) such as how 'smiling' before taking pictures can make some variables more obvious/detectable, which is a 'reality change rule' that is useful for making variables less ambiguous/more obvious so a classification algorithm can be simpler, so an algorithm to identify these points/rules is useful to improve data quality, or to identify more useful problems to solve like 'if the subject was smiling, what would these hidden variable values probably be, based on similar expressions as smiling that are generatable with existing data' (similarly, 'identifying a dye to inject (input side solution) or a treatment to try (output side solution) that would differentiate cells more easily' is a useful structure for a classification algorithm to be able to identify to improve data quality and delay decision-making of the model structure until a data quality ratio/info minimum is reached), which applies the insight 'bc there are some differences between items in different classes, there will likely be other differences that are detectable'
