- examples of other implementation methods than documented default methods (like the 'apply/find/build/derive' function set)

- how to implement the interface analysis framework as a set of simple functions for an initial version, involving alternate default constant simple structures to combine, such as:
    - symmetry structures applied as a default variable structure, rather than 'incremental change combinations', so that by default, symmetry combinations are sought, rather than incremental change combinations, to always frame change in terms of symmetries
    - combinations of useful intents (such as those that are relevant/realistic) as a default interface to base changes on, finding the functions to fulfill them at query time, as implementation variables
    - structures of relevant cross-interface structures as a default structure to apply changes to
        - for example, 'patterns of causal structures of structures of structures' (like the patterns of cause of 'a sequence of structures that develops from another structure')
	
	- functions that act like useful/powerful filters of relevant information, such as:
	    - a function that can determine an attribute like 'input change type' that reveals similarity/difference of a relevant structure, like 'output change type (integer)'
	    - more generally, a function that can determine attributes that reveal 'information about a lot of other information'
	    	- such as how an 'average data point' reveals 'a lot of information about other data points (in that most of them will be near the average)' and an 'extreme/limit of a range of data points' reveals 'a lot of information about other data points (in that most of them will be within that range)'
	    - as another example of highly useful information like types & extremely differentiating filters, absolutely/generally/frequently/conditionally/probably true statements about 'truths/falsehoods' also reveal a high degree of information compared to the input information required to determine their applicability
	        - for example, given the truth statement 'truths usually have a counterpoint that is true to some degree in some context', a 'proof of falsehood by contradiction' which rules out a possibility bc one contradictory example is found can be considered a contextual proof, as there is usually a context where every truth is false (there may be some space that corresponds to the space in the proof where the interaction rule holds true rather than being contradicted)
	        - what is the common factor between useful extremely differentiating filters, types, and truth statements
	            - there is a high degree of information embedded in these structures (when you determine a type of an object, you may know many other things about the object such as the type attributes, similarly when you know whether a filter rules out some object, you know a lot about that object such as its attributes differentiated by the filter, and similarly when you know whether a statement fulfills some truth statement, you know a lot of information about its truthhood)
	            - these structures are highly organized and well-defined/rigid, meaning they are useful by their 'certainty' for comparison/differentiation tasks to determine uncertainty structures
	            - these structures are extremely useful/relevant to many useful intents like 'determine truthhood of a statement (test if a solution is correct)', 'filter out sub-optimal/error statements/solutions', 'classify a statement as true/false or another information type'
	            - these structures occur on different interfaces and have the 'certainty' attribute in common, making them corresponding 'certainty' structures on different interfaces
	        - similarly, other structures can be framed as 'high information embedding structures' such as 'function subsets that act as implementation methods of interface analysis', 'the set of efficient compressions of reality', 'the maximally different variable structure where any problem difference is easily connectible with a query on the structure', the 'optimal set of interface queries that solve most problems adjacently or otherwise optimally'
            - similarly, concepts & other primary interface variables can be framed as 'high information-storing/embedding' structures which are powerful through these functions and similarly offer high ratios of information (knowing the concepts related to a structure gives a high ratio of information compared to non-primary interface metrics)

	- a function set to determine the meaning of an interface query is useful as a 'reverse-engineering' implementation method
	    - for example, the requirements would be a function to generate all possible/legitimate interface queries and a function to determine the meaning of each query
	    - where the meaning of a query is described as the impact of the query on other relevant structures like problems and the connections like similarities/differences to other queries like on metrics like cross-problem solvabiility

	- applying specific interface structures as a default constant set
		- applying 'information' interface interaction rules of the 'physical reality' interface such as:
			- 'truths can become false when over-depended on, beyond their appropriate context or meaning, or beyond their potential to illuminate or support other truths, or in an incorrect structure like a foundation for other truths'
			- 'truths can be so irrelevant to an intent as to be equivalent to false (example: citing the heat death as a reason not to try to do anything)'
			- 'truths can be so rarely/improbably true as to be equivalent to false (example: an error state that is so rare you basically dont have to plan for it, like where neutrinos would coincidentally flip all the bits on a server at once)'
			- 'truths can be so unstable (difficult to maintain) as to be equivalent to false (example: a rare atomic state that degrades into another more stable state more frequently), as truths generally take less energy to maintain or make true, as lower energy-requiring structures are likelier to exist, and similarly scalable truths (which can be repeated or otherwise scaled without or with fewer contradictions) are more true than other truths'
				- 'relatedly, truths, if prevented or destroyed, will re-occur if true, as its unlikely for a truth to occur in absolute isolation, only once, and simlarly can support more embedded variables if true than a falsehood can'
			- 'truths are less likely to be extremely surprising/different, based on a comparison to an input set of sufficiently variable mixed facts'
			- 'truths are likelier to have truth-associated structures like costs, whereas a lie is likelier to have fewer of these structures (a lie about zero cost)'
			- 'the ratio of truths to falsehoods is likely to be stable to some degree, because when a truth is measured and sustained enough to be measured, some other true structure might decay, although entropy is a powerful process that might act as a counterpoint to this, decreasing the number of truths (stable structures) over time'
			- 'truths can be so difficult to measure/verify/calculate/derive as to be equivalent to false (example: number of atoms in the universe, or some phenomenon that occurs below the synchronized directed information frequency that constitutes 'time' that it cant be measured in isolation and can only be inferred by its emergent effects)'
			- 'truths can be so non-adjacent to other probable/known truths as to be equivalent to false (example: future truth of a reality that is non-adjacent to current reality and is unlikely to occur)'
			- 'truths can be so lacking in reasons as to be equivalent to false (example: there is no reason for a rare anomaly except random coincidence so it may as well be ignorable)'
			- 'for every fact (which acts like a representation of reality) there is another fact that represents reality equally/similarly accurately or represents a similar proportion of reality or otherwise represents reality in a similar way by some metric of representation, so that these other facts in the same position on this representation metric index can act like equivalent alternates'
			    - 'statements at different positions on this representation metric index can act like a cross-section of reality that acts like a determinant of reality (the cross-section is usable to determine other facts, which is itself a representation metric), though statements that tend to be closer to either extreme of absolute truth/falsehood or the center are more valuable for their obviousness of similarity/difference to other facts/falsehoods'
			- 'truths are generally independent in that they have relatively few dependencies in order for them to be true (less conditional and contextual, more absolute and inevitable)'
			    - 'at the same time, truths are likelier to have more reasons why they are true/can exist (existential risks to the truth were prevented by many guarantees), making it more stable, compared to a falsehood (an agent decided to make it seem true, using cheaper structures than truths)'
			    - 'this is related to the fact that truths usually have fewer opposing statements that are true in some way/degree/context (bc paradoxes/symmetries/rotations may be a core structural unit of reality, just like randomness/ambiguities/balance points where very different structures seem equally true are alternate structures of reality related to symmetries, as most structures can be connected to symmetries given that theyre a structure of stability/robustness where some structure can be sustained under change conditions and therefore a structure of existence/reality, just like useful structures can be a structure of reality as theyre more stable), compared to falsehoods, which usually have more opposing statements that contradict them, and related to the fact that truths are more similar to other truths as they are more adjacently connected to other truths (using truths)'
			    - the question of 'what are the limits of symmetries, or why isnt everything a symmetry (in every permutation/interaction and on every level), given their commonness' is adjacent to that, with related insights like:
			        - 'variables are a type of symmetry', 'variables are arguably descriptive of everything (the change interface can capture all variation)', 'variables are embedded on other variables', 'some variables break/create symmetries and that is more important to categorize them than to call them just another symmetry', 'symmetries dont always resemble symmetries in various states of development', 'some symmetries neutralize each other, leaving randomness (which is different in that it is the interactions of unlimited changes, rather than constrained changes of a symmetry)', 'symmetry types are a meta-symmetry', 'meta-symmetries (symmetries of symmetries) are more powerful than standard symmetries', 'not every symmetry is equally interactive with all other symmetries', 'other structures like levels/systems where symmetries act and symmetry interaction functions are equally important if not more'
			    - similar to the question of 'why isnt everything formatted or adjacent to an input/output sequence by default' which is bc information can be adjacently and usefully stored in equivalent alternate formats, not everything is a symmetry bc there are variables of symmetries (interface structures like 'state' and 'self' can be applied to symmetries) as theyre not absolutely required, and there are many variants of the definition of a symmetry as theyre related to 'limited change' (similar differences) which is a fundamental structure reflecting reality (true/false spectrum pair), so just like input/output sequences arent enforced in all conditions/contexts, neither are symmetries
			    - similarly, the 'limits of definitions' are equivalently useful to the definitions themselves
			- structures of meaning like 'the comprehensive impact of its interactions/functions at various scales, in isolation and in various system contexts, and with other common variables applied' and 'useful structures' are useful approximations of the definition to use in place of the full definition, similarly compressions of reality are more useful than reality itself, these compressions acting like interfaces between the user and the system being compressed
			- 'truths can be so simple that they will essentially be false in that they will be more useful when changed in some way and will be an input used by other more powerful/complex/high-variation functions which could independently generate/alter these simple truths as needed due to their simplicity, and therefore are less real/true, as reality allows complexity and therefore requires some complexity for survival, survival/stability being related to truth, and similarly can be so complex that they are essentially not usable by other powerful/complex/high-variation functions and therefore are less real/true'
			    - 'relatedly, there is always a simpler and more complex variant of a statement within the bounds of simplicity/complexity required for reality, and the same goes for other dichotomies of reality like truth/falsehood, similarity/difference, balance/imbalance, variable/constant, meaning there is a network of statements related to one statement that act like different variations of the same statement, so that the statement acting like the symmetry of this network of statements is more true than any of the variants and will reflect the interface network to some degree/type/structure'
			    - 'truths will be more adjacently connectible than falsehoods to interface structures (such as the primary abstract concepts like balance/power/work which correspond to the dichotomies of reality)'
			- 'almost every fact has another fact that is more relatively true and a falsehood that is more relatively false, except the most absolutely false/true statements, which are relatively rare'
			- 'relative truth/falsehood is determined by proportion of reality represented, accuracy of the representation, power/usefulness/relevance for enabling/determining other representations of reality, logical validity, adjacency to absolute reality, im/possibility, inevitability/requirement, consistency of the fact/falsehood across contexts, consistency of truths/falsehoods across different representation formats, potential of all other metrics to remain the same or change, & other metrics of truth'
			- 'truths often come with an opposing counterpoint as most truths are not absolutely true but are wrong in some way like in a particular context/usage, so that a truth without a counterpoint as how it might not be true is unlikely to be true'
			    - 'relatedly, truths are generally somewhat variable, acting like a symmetry/manifold in that they can vary in variables like position/shape/relationship to other structures without being destroyed, in their robustness to change, whereas lies by comparison are relatively fragile'
			    - 'similarly, truths are often seen to have a "balancing offsetting truth", similar to a "rise and fall" structure pair (which may differ in side lengths, as a statement may be more true than its counterpoint(s)), which is a fundamental structure related to parabolas, angles, waves, etc'
			    - 'however, given that other fundamental structures exist, the "rise and fall" of a point and its counterpoint is not the entire description of reality, despite being extremely useful and common in the form of the concept of balance'
			    - 'similarly, other concepts than balance are known to be relevant, as not everything is balanced, such as the ratio between matter/antimatter, and therefore other concepts are required to describe reality'
			    - 'similarly, other structures than one interface are important to describe reality, such as how a cross-interface structure (such as the structures that a concept takes in various systems) is more useful than either structure in isolation on one interface'
			    - 'for example, given that the interim and combined concept, respectively, of a circle, parabola, curve, a fractal, efficient embedded change (using the previous value as an input as an alternate to a constant multiplier), a sequence describing the variable interactions rather than a set of interacting independent variables, a stability in the change rate, an origin/symmetry, and a concentric circle set is a spiral, how does this interact with the "rise and fall" structure of a point and its offsetting counterpoint?'
			        - 'during the standard process (which may be the "rise and fall" of a new change as it first generates change and then is offset by opposing forces to settle in to its stable form), other changes are allowed to occur, such as embedded changes, curved changes, etc, which can disrupt the stabilization of the new change or even invalidate it or isolate the initial change from its counteracting change forces to prevent their interaction'
			        - 'given that the spiral with a stable change rate (that is maximally different from other structures like a full circle and a cornered shape) has stable structures, it can inject stability in other changes, such as by making a rise or fall more stable than its opposing structure'
			        - 'given the changes enabled by the spiral and its stability and therefore its disruptive power, spirals are a better structure of randomness than some other structures, as an offsetting uncertainty structure to more default certainty structures like the "rise and fall" which are more certain bc they are more balanced'
			- 'truths that are in between simple priorities & rule sets (like "opposites attract") and complex priorities & rule sets (like those generating randomness) are likelier to be true, and truths that connect simple and complex rules are likelier to be true, as there are both simple and complex rules in reality, and neither has absolute priority, as the interim truths between extremes are likelier to be stable and therefore true, and similarly the interim balance point between all dichotomies at which theyre intersecting is likelier to be the most stable perspective, from which the others are adjacently generated'
			- 'truths are rarely the only truth explaining a variable interaction, as there is rarely a variable requirement requiring that specific combination and preventing any other combination from succeeding as there are more often many routes between two points than one route, and there are many alternate equivalent preceding/succeeding variable interactions that are similarly explanatory, bc errors are default in most systems and therefore differences applied to sequences/routes/components are likely and are less likely to be ruled out by some requirement if they still generate movement in the original direction, and similarly bc components are frequently unitary and can be combined in different ways to generate the same structures, and similarly bc real systems are often complicated and subject to errors from their high degree of interactivity with other systems in contrast to a theoretical high degree of isolation'
			- 'truths usually are not obvious (such as the wishes of agents as their default, and therefore also obvious/simple to them, so these wishes can be assumed to be not true) but if they are, a beginner without expert knowledge of rules is likelier to see them than an expert bound by their knowledge of rules'
			- 'truths that are not measured or otherwise useful are likelier to change than truths being measured bc the act of measurement requires finding a way to keep some fact true enough to show up on a measurement scale, while other variables are allowed to remain variable to cause other variables to change, which are not being forced to remain true bc attention is directed at other truths to keep them constant and randomness indicates that anything not forced to remain constant is subject to variation injections (a function to generate change, whose change-generation speed is faster than measurement speed, is likelier to be a more useful intent, to make all measurements obvious or unnecessary, as the changes that are true will sustain themselves and the changes which arent true wont, so no measurement/testing is required, measurement being useful for "changing direction of focus/motion/work/etc")'
			    - 'relatedly, truths in general are likely to change at some point due to allowed variable interactivity/dependence, so any set of facts that was true is unlikely to be completely or equivalently true at some point in the future, as variation isnt equivalent to falsehood but a fact of truth-generation & adaptation & interactivity & dependence'
			    - 'relatedly, truths that are independent from another set of truths (such as processes on another planet) are less likely to remain true, unless another truth intervenes (something needs it to remain true as its useful for some intent)'
			    - 'similarly, truths that dont require ignoring other information (having no contradictions) are likelier to be absolute truths'
			- 'truths above a ratio of already identified truths may be so useless as to be false, as they may be irrelevant, and also preserving the ratio of truths/falsehoods is useful for agents with intents to incentivize additional change/uncertainty (some intent like a wish/dream must be false for them to be able to justify doing work to change any variables)'
			    - 'similarly, future truths can be predicted using the intents/incentives of agents (agents who are capable of fulfilling all of their intents, who dont always select the incentivized option, as "always selecting the incentivized option" contradicts "capacity to fulfill all of their intents"), once their intent & fulfillment capacity (like 'to increase uncertainty', 'to seek freedom', 'to control time', 'to understand physical reality') and incentive selection (ratio of selecting incentivized option) variables are known'
			    - 'similarly, some possible truth can be so different from incentivized truths or intended truths as to be false'
			- 'truths are generally those structures which fulfill multiple solution/optimization metrics as opposed to just one (its rare for something to be true which is only useful in one way, such as ability to change or interactivity with other useful structures, generally structures which survive fulfill multiple optimization metrics as structures generally interact on multiple interaction levels)'
			    - for example, a structure is not usually just 'efficient' in some metric (efficiency meaning low-cost or adjacent in some other way, which means its near to something else, as not every point is adjacently useable as a starting point to find adjacent structures to), it also needs to be 'similar enough to existing starting points/interaction levels' to be useable (in order for that efficiency to be efficient)
			- 'truths can be less true than a falsehood, if the falsehood is more probable, relevant, useful, is about to be true and if the truth is about to be false, is important to be made true, is approximately/generally true, reflects more interface structures like potential, etc'
			- 'truths are generally more useful at generating other truths (used as inputs to other truths), as opposed to generating falsehoods or lies being more useful at generating truths'
			    - 'similarly, truths are useful at specifically generating power, in the sense that truths enable other functions to be more powerful, so statements (about a variable relationship) that seem to control/determine/cause other variables are likelier to be true'
			    	- 'for example, specific structures act like truths (such as certainties/guarantees) on specific interfaces through their determination of other structures, such as how gravity/speed acts like certainty in the physical information interface, and structure acts like certainty in the math interface, which can be used to find different certainty formats in other interfaces (what type of similarity/change acts like gravity/light in the math interface)'
			    - 'similarly, truths are useful at connecting true/real structures, additionally connecting cross-interface structures, such as by "specifying a concept" by connecting the concept to a structure'
			    - 'similarly, truths such as interfaces are useful at supporting variation, so high variation sources indicate a truth supporting that variation, as if variation is false in some way (such as by contradicting truths) it will likely be stopped by agents who benefit from truths, so if variation continues, it is likely to be based on a truth that has created enough stability to support additional changes'
			- 'truths generally coexist with other truths, rather than more frequently/generally contradicting them'
			- 'truths generally are more similar to other truths, rather than being extremely different from them, partly bc of the fact that once a structure is found that is stable/otherwise useful, it tends to be repeated rather than changed'
			- 'truths that seem "counterintuitive/random/complex/asymmetric" are adjacent to some interaction level, despite seeming false (through dissimilarity/disorganization/complexity/asymmetry) at first, as truths are usually an adjacent combination of some components (though neural networks dont usually switch perspectives to start from a different angle/starting point and identify these interaction levels from which everything is adjacent) and by comparison lies will fulfill some usually-checked metric of truth (similarity/organization/simplicity/symmetry) while containing a contradiction (false similarity, etc)'
			- 'truths are often trivially different from lies, in that in order to be believable, a false statement has to be sufficiently similar to truths (plausible, reasonable, logical, sensical, meaningful, relevant) and cant be obviously false (silly/nonsensical/inappropriate for the context/meaningless) at which point it becomes an obvious wrong (a joke), therefore a lie (a difference from the truth, or an error) can be used to generate a truth by applying it to another lie (a difference from the truth) or by basing it on or connecting it to the truth (making the lie obviously false so its not really a lie), so lies should be connected to an offsetting lie/truth rather than given in isolation, as they wont be robust to testing/changes on their own, unlike the truth'
			    - the types of difference that a truth is robust to are fragile bc of this triviality in their difference from lies (applying an 'opposite' transform could make any true statement false, so thats not a useful change type to determine robustness of a statement to change and therefore its truthhood), however given that the truth often has multiple alternate forms, applying extreme differences to a statement could also produce another truth
			- 'lies take meaning from the truth, to the point that above a ratio of lies, truths begin to degrade/decay, and fuzzy approximations/representations of the truth & tools to automate that become more true (in the sense of being more relevant/useful/usable than most absolute facts), to the point that computers/algorithms become "relative/approximate truth" factories, based on some ratio/structure of falsehoods they can compute filters for (and how well they can integrate new info with the few details that are important to hold constant rather than allowing them to decay)'
			- 'truths that invalidate other truths are generally more foundational (identifying a previously unknown interface that invalidates other truths by reframing/explaining/decomposing them to such a degree that they become irrelevant, such as a statement that "everything is a function of cause", so that everything seems invalid/irrelevant except cause)'
			- 'structures that have more functions such as "organize information more effectively" are likelier to exist/be true than other structures as they increase the stability/inputs of other structures and allow other structures to exist (are more useful and coexisting), and also increase their own stability (organizing information effectively allows the structure to handle stress better and solve more problems better), leading to a win-win situation that is valuable for life to occur and stabilize through coexistence'
			- 'structures of truth (like derivation methods) that describe/generate/derive/find/apply/compress/store/retrieve info better than other structures are likelier to be true as theyre more effective/optimal in some way than existing structures (the universe is likely to move toward a state that more efficiently compresses reality than other sets of rules, if a state becomes adjacently possible relative to the existing state), meaning truths that use other truths the best/most effectively are likelier to remain/become true'
        - finding the 'information traps (at which point no further information is derivable, as in a dead end) and the sequences to get out of information traps' such as:
            - trajectories across different graphs (like usage graphs, as opposed to attribute or variable or function graphs, to find paths back to a high-variation/high-potential position)
            - applications of complementary (as in independent) insights, such as applying how 'every fact has an opposing statement that is true in some way/degree/context' to find the errors in the generator of a trap and fix those errors which are inputs to the trap (errors like 'inability to derive information' and 'inability to change' and 'inability to be independent'), or similarly applying variables to generate independent directions of change like the primary interfaces, or similarly applying variables to the generator/trap to make it improved to capture more information more efficiently, so as to become an input to the trap/generator, or similarly applying variables to the trap/generator to make it independent, or similarly applying variables to the generator/trap to make it trap itself, or similarly applying limits to the generator/trap so it is the dead-end
                - a good metaphor for the primary interfaces is the 'cardinal directions' (as dimensions of reality) each being capable of reaching the center and the edge of the universe, in different ways (using different combinations generating different structures), where the other directions are derivable from each direction if the center or edge is known and each direction is enough to understand the universe on its own, having a sufficient cross-section as an input, each providing equivalent information in independent ways
                - the cross-interface connection/change sequences between these 'primary directions of change (on one interface)' (which themselves support connection/change sequences) are important as a foundation of reality (the ways that information can change into other information being a source of freedom/potential where change can occur)
            - applications of limits to trap the generator of a trap or the trap itself and applying variables to the information that is trapped to reverse positions, or applying variables to the trap/generator after applying a variable to change its direction/intent to set it free in another direction
		- applying interim connecting structures of useful structures representing the truth like 'rules databases', 'attribute networks', etc, such as 'common structures to both systems' such as 'constant attribute rules' (variable interaction rules)
		- every true statement should connect to other sources of truth (high-variation variables like interfaces, powerful variables like cause/energy, important sources of potential like 'black holes' as potential energy sources)
		    - 'humans are working on climate change (because we havent tapped the extreme potential energy of black holes) and (because we havent identified all the interfaces) and (because we havent optimized energy efficiency)'
		    - true statements should take important/relevant true statements into account
		    - structures like black holes are a source of truth bc they store information/energy/reality efficiently, just like the interface network does, and just like some formulas do (euler/gravity equations, etc)
		- definitions/limits/requirements create a metaphorical corrollary of 'gravity' in that they make some structures more similar/probable/coordinating than others
		    - where does variation go which cant be stored/have structure in the universe, or is this not possible and all impossible variation is approximated/echoed in brains to conceptualize impossible structures (like how you can know some things about an infinite set, such as that one side has no limit, and the connection function of adjacent items, and the impossibility of it stopping, and its starting point, which is like glimpsing a subset of infinity, as that subset is all that can occur in or occupy reality), which implies that this infinite level of variation would contradict some other real structure which has more energy or which has other rules protecting its stability, meaning its more powerful than that infinity
		    - limits like 'are there functions which are possible to graph in 3-d euclidean space but which are impossible to form with real structure variables' are useful to find, to determine the differences between math and physical reality and the limits of this connection
		        - questions like this would also determine structures like 'whether math stores more possibilities than reality' or 'whether reality stores more possibilities than math' (math definitions allow for fewer variables than reality), or does reality offer a useful alternate format of math (like a 'cohesive usage network of repeatable math functions/structures'), and which system controls the other (does math determine reality, or do agents in reality control math in the sense that they can use it for their intents), and 'what is the limit on the variation producible with known math definitions, and can that variation explain reality'
		        - if math offers more possibilities than reality could ever support, reality is like a compression of (a subset of) math, and its possible that reality could only ever represent a subset of math structures, rather than all possible math structures, and 'all possible states of reality' is determined by the set of 'all possible subsets of math, below a certain threshold or in some structure limiting the subset', and 'identifying the structures to connect reality rules with these limits on what math structures can take physical form at any time/state (rules like "nothing too chaotic or simple")' is a useful related intent
		        - a compression function could also act like a generative/descriptive/explanatory/causative function
		        - are some structures only impossible bc of other structures that could change reversibly (undoing the change to revert to a lower-variation or more stable state while retaining the new information) 
		        - 'what information/structure is likely to be irreversible, unstoreable/unretainable, incompressible, or create chaos cascades' is a useful related question
		- identifying useful problems and useful problem formats to solve, such as 'identifying how to format problems/variable/interface structures so that they can be formatted as a problem of "solving a system of equations" to find the alternate function sets that can coordinate to determine reality', like 'which set of functions preserves/stores/organizes information the best without contradictions (using the same components and without removing any components from the original function sets which are known as true, or removing any other functions)'
		- identifying function sets to map 'generality (in intent) to specificity (in implementation)' is a way to identify the core functions that need to exist (for intents like adjacently generating all other functions), using examples of general intents fulfilled by specific functions, as a way of writing the fewest functions (writing an abstract general function and then varying it when specific valid intents require changing it)
		- identifying ways to integrate alternate core general useful structures like applying 'causal networks' to decompose some subset of data sets, where some points like 'density centers' indicate connections between points such as 'probability of adjacent points' (as a cause of expecting to find other points nearby) and some points like outliers or non-pattern-compliant points representing structures like limits/dead ends in causal networks (as a reason to find those patterns, or as a reason to discard a point as being generated as only an output by one rare process so as to reduce its possible interactions and usefulness), or alternate causal networks of input feature subsets where some causative relationship is probably in a set of alternative relationships, since adjacent/equal points arent necessarily absolutely independent, in the sense that one point may lead to generation of other similar points if successful/useful in some way, or a particular point may be so useful such as simple that it is frequently generated independently, and these related points can be reduced to one representative/general point that can be used to generate all of them, and instead the 'reduced set of representative points' can be altered by applying these 'reasons for point similarity/repetition in data sets' to generate the probable alternate variations of them, while applying regression to just the reduced set, or the 'reduced set + the generated set', to isolate actually or more independent points
		- identify methods of describing useful methods like interface analysis using functions/variables not directly referenced in their definitions as good approximations of it, like describing interface analysis as 'interim thinking' and 'meta-representation and similarity reductions and integrations' and 'potential maximization', these descriptions being useful for identifying approximations of the logic (similar to how concepts can more effectively describe a structure than the full set of details about it) which can be used as inputs to generate the logic & identify variables of these useful methods to generate the others
		- applying useful reality representation structures such as 'metaphors (similar/relevant but different/inaccurate structures to format something differently in a useful way to achieve understanding)'
		- applying physics rules such as the 'fuzziness of physical reality' as a way to find other true structures ('symmetries', 'alternative definitions', 'patterns')
		- applying structures like the functions that can generate the highest variation (such as the 'Conway game of life') as a default structure to generate other high-variation structures like reality
		- applying specific mappings across interface structures (like concepts such as 'balance', physics structures like 'symmetry', and physical reality structures such as information agent structures such as 'justice' and 'economic equilibrium', or concepts like 'interface', structures like 'bases', information structures like 'metaphors' being variable implementations of the 'interface' concept) as default structures (such as default inputs of a neural network used to predict other mappings)
		- apply useful perspective structures (like combinations of perspectives such as the 'optimization perspective' and the 'religious perspective' to efficiently describe other useful structures like 'what agents want and what could be true using adjacent transforms of reality') as a default constant set
		- apply useful structure sets that should go together (like how 'dependence' is useful in the 'causal' interface but negative in the 'physical reality' interface)
		- apply useful structures (like 'direction or line connecting starting/goal points' and 'implementation structures to get to that point') to model useful structures that are useful when applied together, like 'intents/implementations of intents', as a useful input to neural networks to fulfill the task of 'adjacently combining them in a way that connects them'
		    - similarly apply patterns of implementation structures such as 'causal loops referencing nested sub-problems resolved with some structure and output to the host structure once solved' as default components of implementations of intents
		- fulfill a primary function of a particular interface that can solve most problems, such as 'evaluate meaning', and the useful functions to fulfill that function efficiently
		- apply structures iteratively to calculate the global/universal meaning to check if its obviously false at scale (in its extreme form which is more easily determined), such as applying a rule like 'its ok to violate someones rights if theyre a genius and youre fascinated by them' (after iterating to an extreme scale, its obvious that no, that couldnt be right, then geniuses wouldnt want to live if that rule is applied at scale, to guarantee that geniuses' rights will always be violated, and it couldnt be right for many other reasons, such as it is probably 'better/more sustainable/and therefore lower cost' to prioritize turning people into geniuses than to persecute geniuses with rights violations which will reduce our supply of geniuses to zero)
			- structures that make other structures obvious are obviously useful
		    - apply interactions of known structures at scale as default filters of other scaled structures, determining what else can exist at that scale, deriving core structures of truth from these scaled structures that could also exist at that scale
		- apply structures that can be used for multiple intents as default useful structures bc they can handle more variation than other structures and are therefore likelier to be compoundingly useful when applied in structures like combinations
		- find useful spaces including definition spaces where some attribute makes it useful for common problem-solving intents
	    	- maximum that a definition can change without breaking, depending on the relationship between changes supported/required by that definition
	        - differences that create different 'definition routes' of a definition (comparisons to similar/opposite definitions, usages, formats, different subsets of attributes that differentiate it in some space)
	        - space where embeddings (apply), combinations (build), connections (derive), and divisions/filters (find) are adjacently connectible
	        - finding a space between other spaces that is required to exist by definition where some attribute is also required (some attribute like an equivalence required to exist bc its between a greater/less than relationship)
	        - a space that enables finding the right layer to evaluate meaning/impact at, such as the 'layer where any pattern is identifiable' or the 'layer where variables are identifiable' or the 'layer where unique variables are identifiable'
	        - a space that makes equivalent alternates adjacent, equivalent alternates such as the 'maximally different structure' and the 'minimal structure required to describe primary interface concepts like potential/cause' and the 'dichotomies of reality', which are probably equivalent but are not trivial to derive from each other
		- apply structures that can offset neural network inadequacies such as 'limits of a series' being useful for solving the problem of modeling 'longer sequences of input/output connections' as a tool to determine when a sequence might converge, which is obviously useful for prediction intents, as well as other structures that allow the user to 'see far ahead', such as 'highly complex/different realistic/stable/efficient' structures, useful determining functions of extreme attribute similarities/differences (like absolute change type interactions, such as preserved change types when some function is applied) as 'extreme' structures and 'equivalent alternate structures (that can keep each other in check and identify invalid structures)', and other structures that allow you to efficiently calculate a lot from a little information
		    - you can see how useful functions (like calculating an adjacent attribute such as 'constance/addition' that reveals non-adjacent information like information about 'extreme change type preservation') act like a powerful information filter that reveals information that is difficult to calculate (non-adjacent)
		    - finding useful structures to determine attributes of other useful structures (useful structures like 'extremes') can be done by reverse-engineering 'input problem cases' where these revelatory 'filter' structures would be useful:
		        - finding highly different structures like extremes or complex manifolds (high-variation change based on an interface like a connected structure such as a manifold, which is a useful math structure adjacently mapped/corresponding to an interface)
		        - determining what is easily determined about that highly different structure once you know it
		        - determining what cases this information could be useful in
		        - determining whether those cases are useful
		    - similarly apply functions to 'calculate convexity' and 'filter possible functions given maximal differences (structurally similar change types that can look similar at first like exponential change, waves, hyperbolic functions, etc but which have adjacent filters to filter them out) detected by some point' as a way of improving 'gradient descent errors'
		    	- you can see how these 'interface structural similarities' (like an 'exponentially increasing subset') provide a useful constant base to apply changes to, to determine other possible structures and identify filters of those possible structures
		    	- this is an embedded application of the 'interface' concept, which is clear in the usefulness of this 'similarity to base changes on'
		    - similarly, using 'lifetime data with associated errors across a lifetime' to put a statement into perspective, or having 'civilization lifetime/trajectory' data as opposed to phrase/sentence/paragraph/story data, is likelier to connect information to more relevant structures (to answer questions like 'what problems were solved by a mind that regularly produced this thought')
		- apply interaction levels ('adjacently connectible structures that interact/connect') as 'input information' or an 'information format' to neural networks (which fulfill the task of 'finding adjacent combinations to connect variables')
	- applying a function to find interaction levels where the problem is solvable with adjacent combinations and the function to find those adjacent combinations once transformed to that interaction level
	- iterating through filtered interface structure combinations and checking test cases of input/output pairs
	- create a network of solutions to use as bases to navigate
	- iterate constant/variable pairs to model the highest degree of certainty/uncertainty pairs
	- applying more adjacently determinable structures as an input
	    - attribute networks as an input and function networks as an output implementing those attribute networks
	- finding an 'input-output sequence' that would be useful and would probably be adjacently constructible in that sequence
	    - like a sequence of first building an 'attribute network' to describe all variables and then building a 'function network' describing that network and then a 'function generator network' to compress/generate that function network
	- applying default interface structures as the core functions of a machine learning network so that errors like 'not understanding abstract concepts' can be avoided by injecting abstract concepts as a default input or function or related structure, so that 'combinations of abstract concepts' required to 'solve a problem of understanding some conceptual combination' are adjacent
	    - 'inventing new machine-learning models' doesnt just require 'sequences of change combinations on some input features'
	    - getting machine learning models to actually invent a new optimal machine learning model requires a function to 'identify optimal models which are maximally different & useful across all problems'
	    - this is 'accidentally stumbling on useful functions' by adjacently changing a weight sequence representing a function (as functions have variables such as components)
	    - this may look like intelligence but is only "accidental coincidental effects of making 'variable changes' resulting from the function chosen seemingly at random"
	    - this "discovery" is a re-iteration of the known fact that "ai models change variables that can act like functions, and then those changes are sometimes useful at reducing the loss, so the model keeps those useful changes rather than discarding them"
	    - this "discovery" is an advertisement to the lack of understanding that "some functions can be re-used across problems"
	    - the model didnt magically know this particular function would be useful across problems, it just applied adjacent change combination that are basic/core changes, and which are easy to find, which coincidentally happened to be useful across problems, bc of the power of some core changes
	    - that is a fact of reality that should be an input to an actual intelligence algorithm (core functions are the only type of function youll get with 'adjacent change combinations' unless your 'inputs are adjacent to interface analysis structures' or your 'connective function is maximally differentiating'
	    - https://arxiv.org/pdf/2211.15661.pdf
	- find adjacent structures to useful structures and how to connect the adjacent structures so that useful structures are adjacent using these connections
	    - for example, finding that 'poverty costs more to maintain than to fix' is a result of finding 'similarities' to 'costs', such as 'poverty', and then connecting them (why is it similar to a 'cost'? its related to a lack of money/resources)
	    - as another example, finding that 'mirrors are useful structures to find hidden information visible from another angle (as they offer useful differences to input angles around the symmetry of a right angle)' is a result of finding 'similarities to light' such as 'light symmetries' which are the reason theyre useful for that intent
	    - these have a structure in common like:
	        - 'cross-interface structures (cost-lack and light-symmetry)'
	        - 'similarities to important variables (these important variables being variants like cost/light of primary interface or otherwise useful variables like information, which are a component of inputs to information such as probability, as costly information is less probable to exist or be measured and probability is an input to information, or are variants of information on different interfaces like the physical interface, light and cost being important variables which are related to information)'
	    - the question of 'what is the limit of the usefulness of finding different variants of a definition and replacing the original definition with the variant in its interactions with other structures' and 'what is similar across similarities to important variables' are other related useful queries

	- finding functions to find/derive/generate & avoid errors in a complex system
	    - for example, in the problem space of 'creating a virus to restore original dna without epigenetic mutations', some worst-case scenarios could be:
	        - some reversal of a mutation causes the problem youre solving in a different way so that 'solving it' in that way just shifts the problem to another position/state (constitutes an illness trigger in a particular host state, or causes an illness given other mutations/genes)
	        	- the host has adapted to the mutations, meaning if you reverse them, the host dies bc they now need that mutation, which need youd have to fix first to avoid killing the patient by reversing the mutation
	        	    - epigenetic mutations store some memory or functionality that some process has come to rely on
	        	    - reversing the mutation in a healthy patient would work, but the patient has complications like organ damage that make the gene therapy deadly
	        	- there are several 'state changes' that need to happen in a sequence to reverse direction away from some bad health state, where if you skip a step or get the order wrong, the host dies
	        	    - some mutation is ok to reverse in one state such as when otherwise healthy, but not given the mutations required by this sequence
	        	    - once this sequence of steps to reverse all mutations is calculated and applied, it turns out a state in the sequence was optimal to stop at, but the completed sequence was deadly, as some mutations could be kept without killing the host for some reason (they made the host better at something, which theyre now lacking)
	        	- the mutations have become so different from the original state that making these reversing changes is too stressful as its too many changes at once, and they need to be broken into multiple subsets of changes to avoid over-stressing the system
	        	- some negative environmental or default response to the gene therapy triggers reversal of mutation reversals (stress-handling methods create mutations, such as exercise, stimulant substances, or cell cycle/immune triggers, which might reverse some reversals before the therapy can be completed)
                - some other system variable needs to be synchronized with the therapy, such as to 'deliver it at a certain time to sync with circadian clocks or right before sleep cycles, distributed over several days' or where some other alignment needs to happen to successfully apply the therapy in a non-fatal way ('deliver it in a way that doesnt trigger heat stress proteins or disrupt cell communication or other intrinsic required processes that must be stable for the host to live or for the host to respond successfully to treatment', for example)
                    - the gene therapy has to be used with some other treatments (like immune suppressing treatments) to deactivate existing processes to prevent mutations, or some process will neutralize it (the immune system will attack it)
                    - it turns out that some unforeseen condition is true (like 'if one cell has a mutation, it will re-occur in all/most other cells', or 'if you change dna in a way that the system is not accustomed/adapted to, the mutations will re-occur out of convenience for the system that uses them'), conditions which only occur in some patients/health states/environments or with some mutations
                    - or some other condition like 'if you apply a gene therapy that is sub-optimal in some way like involving a lot of mutations, you also have to speed up the cell cycle temporarily or induce cell communication or apoptosis in some cell type, or it wont take hold' or 'gene therapy in some sub-optimal way like "many simultaneous mutations" is only effective if some gene to control gene editing/immune function is switched on' applies that we dont know about
                - the gene therapy changes how other complex systems interact (how the host microbiome interacts with their brain, given the impact of dna on both systems) in a way that causes illness in other complex systems which are difficult to predict
                - the gene therapy is successful in the cells it reaches, but it doesnt reach many cells bc default dna repair processes are too effective at preventing this many mutations or those specific mutation types and they effectively deactivate or kill the virus
                - the virus is too useful for other pathogens, which eat/use/attack it before it can complete its processing
                - the virus quickly becomes pathogenic and causes sub-optimal mutations/illnesses, as it gets mutations in its own dna, either from the host, from the host's illness state, or from other pathogens/chemicals in the host
                    - similarly, the virus allows existing pathogens to become more pathogenic
                - the set of mutations required to fix it is so volatile & different for each individual and you would have to know every pathogen, every input (environmental, habitual), every distorted variable in their system, and every illness state they have before making an accurate prediction of how to fix it bc of the volatility (meaning the solution can be drastically different even for similar people)
                - the gene therapy is successful in a patient but the therapy spreads to everyone around them by environmental exposure and it can kill other people so they have to be quarantined or quarantined with people having similar genetics for some period, or the mutations would have to be paired with some other mutation/state/treatment so that the virus would die/deactivate after completing its processing
                - the side effects of changing a lot of genes at once is frequently deadly, no matter which genes youre changing, bc of default bio-system responses (similar to the toxic responses like sepsis to mass cell death triggered by the immune system)
                - the therapy is successful, but the patient's original dna was vulnerable to some other pathogen which they will probably encounter
                - the therapy is successful, but it erases information gained over the patient's lifetime (like immune memory cells or neurons tied to some microbiome change/state)
                - the therapy is successful, but the patient had habits that made their original dna sub-optimal as they didnt use their genes optimally, and their pre-therapy genes were actually more usable in an optimal way
                - the therapy is successful, but habitual/environmental changes could have replaced it with less effort or the pre-therapy genes had some unknown benefit or there was a way to trigger original dna copying without artificial therapy but we dont find it bc we focused on the artificial therapy
                - the therapy is successful, but bc of environmental conditions like medicine/pollution that cant be avoided, the patient's original dna is suboptimal compared to some other set of mutations or their pre-therapy illness state
                - the therapy would be successful, if not for other currently poorly understood components like dna fragments and metabolism and their interactions with dna edits
                - each change to the bio-system would have to either not impact required processes (like energy/enzymatic pathways) or change these processes to an alternative, which means an alternative is required, and there are some processes with no alternative in the bio-system, and these processes could easily be impacted negatively by some gene changes (or the patient would need some complementary treatment to handle these impacted pathways like blood cleaning processes to remove toxins from immune/cell cycles)
                - some changes to dna dont impact patients negatively, but these are probably relatively rare as they require some attributes like 'lack of interactivity with other components' and/or 'similarity to components which are already handled to some degree in some way', and most dna changes are unlikely to have these harmless attributes
                - dna processes have some requirement we dont know about, such as requiring 'some junk/repeated data or neurons or pathogens to help handle mutations' or some other dna edit process that we're unaware of, which is only a problem in some patients
                - mutations are inevitable without some extreme behavioral condition like extreme workouts/supplementation with some substance/rest/temperature, so some mutations will probably/inevitably re-occur, requiring re-treatment
                - mutations present in the host make it unclear/ambiguous what the original dna was, so the target optimal state is not determinable by seeking the original dna state, and a 'generally probable healthy state, which is healthy for most/other people or given some known optimal gene configurations' is a better goal to aim for
                - dna changes have impacts at different interaction levels (dna groups, chromosomes, dna fragments, dna-editing dna, jumping genes, etc) which are currently not predictable/known

        - alternate intents to aim for
            - a 'more general solution that helps most patients to some degree' is likely a better short-term target (which undoes a few common harmful mutations without harming most people) than aiming immediately for a general highly customizable solution that works well for each individual (which it may not currently be feasible to retrieve all the variable information required to implement such a solution)
                - similarly, a 'dna damage-reversal solution' that reverses disruptions to important/required processes, like solutions to 'soften arteries', 'lengthen telomeres', 'de-calcify calcium deposits', 'grow new cells to replace cells damaged irreversibly by acid' and other treatments for negative stress responses from dna damage, microbiome damage & other related problems, might be more effective short-term targets, to fix some negative side effects of dna damage rather than fixing the dna damage itself
                - similarly, a technology to 'prevent dna edits of some type or of any type' could be useful to maintain some health state or to maintain health once mutations are reversed or useful mutations are applied
            - finding mutations to protect against the more powerful/invincible viruses with more evolved self-sustaining dna might be a better goal as they have attributes in common like 'hiding mechanisms from the immune system' which can be targeted

        - implementation methods
            - finding the 'healthy gene sets required for a normal healthy bio-system in most people' and making sure dna edits dont change those required gene sets for health
            - finding mutations that mimic or match patterns of or are otherwise similar to 'natural healthy evolution dna edits (like those that improve stress-handling response or immune function)' as opposed to 'illness-causing dna edits' and restricting changes within the found range of dna edits on some graph where these regions of evolutionary or otherwise healthy mutations are adjacent
            - some genetic changes can be 'tested' in the sense that side effects are probable/known to be 'increasing from zero' and 'initially non-fatal' to a degree that there is enough time to reverse them with an opposing or de-scaling mutation virus after if side effects indicate a negative health state change, or altered to an interim state that is less strong in the metric that caused the state change, to allow for a tuning process of changes during testing
            - 
                
	- finding a function set that 'converts/connects interface structures to/with other interface structures' is a function set that can generate all other useful interface structures bc the primary interfaces already identify the primary directions of change (similar to cardinal directions) and functions that can generate one from the other can identify the full set as well as identifying the differentiating variables embedded on those interfaces
	    - interfaces can be defined alternately as 'structures which can cover/support/capture the information of reality like a continuous field/fabric'
	- for the intent of 'finding useful spaces'
	    - finding 'structures of related optimals' in a space of sub/optimals (a meaningful space, where every point is useful for some ratio of intents, similar to how 'useful structures' are meaningful in the sense of 'adjacently or directly implementing some intent') where some structure of optimals avoids errors above some threshold is useful to connect useful structures like variations of optimals
		- like how some structures which may seem required or otherwise optimal like the 'reward' in a 'reward function' may seem optimal, but the assignment of the reward may interfere with additional inference processes (a learning function with an understanding of meaning wouldnt need a test/reward function set to guide it)
		- related to how the 'reward isnt the point' but the point is 'developing a learning function or learning function-finding method that can learn anything', as nearly everything is both correct and incorrect in some way (the reward helps the learning function when its constructed a certain way, but it also takes resources and isnt the point of a learning function, and learning without a reward would be better if it aligns more with the point or otherwise is more adjacent to an accurate understanding of meaning, as a reward is an additional requirement that not every learning function would need), as everything can be connected using insights, so finding structures that connect structures of optimals (like a set of differences such as 'variables of different filters applied to sort before or halt a trial and error algorithm' to connect suboptimals or pairs of sub/optimals) is more useful than a function to find any one optimal, and given that anything is connectible, an optimal learning algorithm should be able to derive learning rules from anything (such as learning a generate/change, test, filter/reward function set or equivalently an insight set that is useful for solving problems, etc) to achieve meaningful understanding
		- other useful spaces include the 'space of complex structures' where equivalently complex structures (like complex inventions which are similarly useful or equivalent alternates) are mapped to or otherwise connected to each other, where solving for some complexity space means solving all the spaces more simple than it by default, as the complex components can be re-used to solve simpler problems but not all simpler problem components can be adjacently combined to solve the complex problems, like how time is asymmetric in that it favors a direction toward some structure like a ratio or interim point of organization/entropy or interactivity/variation, prompting questions like 'what asymmetries can offset a foundational symmetry like cpt symmetry' and 'is time a foundational symmetry allowing other symmetries to form, so there is no more efficient foundation to navigate to and no way to countradict/change it, or are there other foundational symmetries like "equivalent interchangeable alternates" which are stronger as a foundation than time'
		- a space of 'maximally different structures' (like independence, interactivity, organization) which can invalidate questions about variables of each maximally different structure is useful for adjacently generating new perspectives
		- similarly a space of 'most-used interface structures' (like a 'function generating primary abstract conceptual interactions (such as power interactions) in a system') is extremely useful
		- a space that organizes optimals/solutions in such a way that each optimal can be calculated with some simple function from another optimal (like a lattice where you can move a set amount to get to another point) is a useful organizing structure to search for (using some navigation function, like 'change each of the maximally different variables that describe/generate/determine the function while still maintaining a cross-interface structure')
		- a space of 'equivalent interchangeable alternates' (such as 'generative/determining/descriptive/representation' functions) where these equivalents are adjacent is useful for finding more adjacent structures from a given starting point
		- a method such as 'standard least squares linear regression' leaves out almost everything relevant about the universe to identify whether some variable set 'changes together' (are there more adjacent variables that can be generated from these variables as interim variables connecting them to y? is there randomness interfering with some variables? are some variables just common structures like obeying incentives, stress, or lies? do all variables 'change together' in some way once connected or formatted differently? what level of directness/independence of connectivity is being tested for by this method?)
		    - some subset in the space of all generated functions (under some number of steps, including 'identifying/generating new spaces allowing differences and making other differences obvious') is actually good at sorting and organizing this information in a useful way (identifying possible alternative relationships, identifying possible ambiguities/unknowns, identifying interference from other systems/randomness, identifying variable attributes like directness/independence, identifying useful insights to connect variables, identifying position in a system state sequence, identifying possible systems where these variables could exist and interact in this way, identifying maximally different variables in the input set)
		- identifying connective spaces to connect non-math structures (like language/system diagrams) with math structures (like sequences/networks) is useful for solving problems with existing resources (such as indicating language using a set of spaces where different sequence angles indicate some difference in a subset of language variables in sentence/paragraph/manifesto sequences, where the set of spaces determines independent complementary information about language)
		- finding a space of 'interaction levels where a function would be adjacently implementable' so that similar points on the space can find alternate interaction levels (of functions/variables to use as components to adjacently combine to find a function implementation)
		    - this considers the investment or 'amount of work/energy' invested in the work of constructing/finding a function, the work done in the final function logic itself, and the work of applying a function, as an alternate function attribute, similar to intent, and related to complexity but different from both, more related to the 'differences in the set of possible functions fulfilling an intent' and the 'differences created in functions by different amounts of work (low-effort vs. high effort functions)'
		- similarly, the 'space of all symmetries' is obviously useful for finding 'symmetries/similarities/differences/patterns/interactions of symmetries'
		    - when these spaces are graphed, queries like 'if you switched the values of all equivalent alternates (or other definitions of symmetries) in some specific case, would you occupy a different configuration of spacetime that is stable and are there reversible trajectories connecting these configurations' are more adjacently/obviously computable
	- finding all high-variation variables/functions and applying that as a base to describe other high-variation variables/functions
	- apply optimization interaction structures (like a 'set of game strategies that can result in a tie') as being useful structures to model other useful structures like 'interchangeable equivalent alternates'
	- apply the 'biggest differences in between known problems/solutions' as a set of useful differences to apply to model other differences which are likely to be more connectible than those differences
	- apply combinations of workflows to find other workflows, to find solution-finding methods
	- apply structures guaranteed to be relevant (like 'changes within x causal degrees') as a default set of structures likely to be useful in solving a problem, then apply other useful structures like 'combinations/input-output sequences' to those structures to find the useful set of structures to solve the problem
	- apply a sequence of these other implementation methods as a way of designing a path from mvp to final product
    - apply useful structures that are more adjacent to solutions than problems are, such as a structure including 'interactive structures' and 'useful descriptions of the problem'
        - the better explained a problem is with the best representation of it, the easier it is to solve it
    - finding structures that represent the most interface structures (like a 'high variation structure that represents various core interface structures like interactions and requirements and generative functions')
    - finding structures that describe most problems/solutions in terms of interface structures such as 'a breaking of an interface' or 'misrouting of randomness' or a 'structure of resolution between ambiguous alternates' and other useful interface descriptions of useful structures, so these can be re-applied or applied as defaults or core structures
    - similar to how solving a maze can involve applying these structures, as 'equivalent alternate' structures that can provide the same or similarly useful information in solving the maze, deriving other useful possible solutions is possible by applying useful concepts like 'incentives' (to derive basic structures likely to occur)
       - applying regular 'direction of motion' checks to make sure the agent is still traveling toward the goal and not repeating routes
       - applying standard maze configurations as possible alternatives to select from
       - solving for standard tricks to check for in a maze that are incentivized and therefore likely to be encountered, and solving for the solutions to those tricks
       - solving for interactive sequences of paths that coordinate/cancel each other and can/cannot exist in the same maze
       - solving for 'maximal filters' that can filter out possible mazes the most efficiently
       - apply reverse-engineering to find indicators of various possible sequences of end paths nearest the exit
       - these structures involve solving other problems than 'apply any route using trial & error as a base solution, and change it as you go' which is a default solution to solving a maze
       - solving problems like mazes is a proxy for solving other problems bc of the high variation captured in a maze, if the maze reflects realistic randomness and other variable interaction patterns enough, similar to solving other games if they reflect realistic structures enough
    - a set of useful descriptions of reality that have reasons why they could be true (like a 'calculator of efficient methods of preserving energy cycles' and a 'structure to support maximal variation/uncertainty (time)' and a 'stable alternate coexisting sequence (time) finder') are useful to apply as defaults and efficient structures and truth structures, as well as the variables creating uncertainties (like 'unknown beneficiaries of calculations' and the 'maximally different structure' and the 'most stable system that supports the most variation') in these descriptions and the variables between them (such as interface structures like different priorities), as well as the differences between the description and related structures like 'problems solved with that description' and 'priorities fulfilled by that description' which make the meaning and reality of each description more calculatable
    - apply useful structures like 'clarifying structures' which make something obvious similar to how certain filters make some solutions obvious such as 'standardizing' and 'difference-maximizing' filters make differences more obvious
        - for example, finding standardizing structures like 'matrixes' which are a useful format that is also useful for other intents like 'mapping sets of sequential operations' and 'reducing some function to a set of adjacent combinations once in that format' such as 'solving linear systems of equations' 
    - applying useful methods such as methods of deriving information about other useful structures such as 'types' (all members of a type have this attribute that defines the type) and connectivity (constant lines are formed by a type of addition so it makes sense that adding them which is applying more addition doesnt change their shape, as their shape is the product of addition, as opposed to an operation that adds a dimension or restricts range, which would be required to create different shapes)
        - finding the 'type' of an object gives almost free (low-cost) information about that object, bc of the information stored in the type, where the type acts like an interface that can support some variation within the definition
        - other low-cost, high information-producing structures (like abstract concepts such as balance, alternate definitions, interaction levels, similarity to some other known structure, useful filters like standards to derive maximal information, useful networks to know a position in which is a high-information structure in useful networks) can be derived in a similar way and prioritized as default structures to find
    - useful formats of structures similar to a standard network but different in a useful way like 'gap networks of connected empty shapes' to represent related structures like intents to fulfill with implementations filling the shapes, or 'state networks to model useful sequences/queries' or 'maps to model useful connections' or 'map networks to model different analysis perspectives' or 'interface networks to model bases that capture high variation'
    - a set of certainty/uncertainty pairs to apply as default problem/solution structures capturing high variation
    - build sets of 'loosely related possible' associations (which arent guaranteed by definitions but which are allowed) using definition-adjacent connections, like how the connection between variables 'constant' and 'constant squared' involves a definition involving 'multiplication', but also has related attributes that are outputs like curvature which are required by the definition and other connections which are not as relevant like 'constant preservation of data type between input/output as a scalar' which is a loosely associated connections rather than tightly bound by the definition, using these loose associations to discover new possible connections not explicitly defined but also not definitely restricted
    - find component functions of data set subsets and iterate applying these until a non-matching point is found outside of the acceptable error range, adding terms or alternate functions to process in parallel to handle new points where found outside the range
    - find subsets of the data set that shouldnt be reduced to a function bc of complexity and randomness and other factors likely to predict insufficient information or variable injection points or other uncertainties that cant be resolved, where other subsets of the function are clearly mappable to functions
    - find more useful structures to describe variable interactions such as 'variables (such as squares) creating requirements (such as required growth, as in positive or nonzero growth)' and 'variables creating other useful structures like embedded change (change on change, like exponential growth leading to the accretion of matter)' and inputs to these structures like 'equivalences in factors creating self-similarity (in multiplication/area inputs) leading to multiple differences in outputs (of multiplication, compared to adjacent inputs)' which are useful in their capturing of high variation (similarities/equivalences creating differences like multiple differences between change rates of adjacent inputs)
      - the core unit of maximized potential of a variable in its interactions in isolation of other variables (self-interactions) is the area that can be created by the unit of core maximally different interaction (multiplication) with itself, representing its interaction space in the 'adjacent sides of a rectangle' operation (multiplication), leading to its potential (in its possible range of impact, as squaring it is maximizing its differentiability) to influence probabilities
      - finding the important alternate sets of functions that lead to these important structures like 'required growth (a relevant structure of reality)' can generate a 'limit scaffold' ('the model must not contradict required growth of some structure that has growth as a requirement or other form of certainty, as in some variable in the model must not grow in a way that contradicts known required growth of some other variable') that represents 'points of impossibility' that should be used as filters to avoid when modeling reality
      - the intersection of 'generative scaffolds' and 'limit scaffolds' is a useful place to start modeling the dichotomy between certainty/uncertainty to explore, format, & filter the space between them that is allowed by reality
    - finding other useful representations of a function (such as a 'stack of squares of increasing side length' as a useful alternate representation of x-squared to represent the value of y in a clearer way that reflects the equivalence in multiplication inputs and the multiple differences created by the equivalence in multiple dimensions) which increase the relevance and meaning of a function representation, as its probable interactions with other functions is more clear given the core relevant differentiating attribute of equivalent factors
      - similarly, the equivalence in factors leading to a line with slope 1 (1,1, 2,2, 3,3 as inputs) is significant and indicates the relevance of x-squared as different from other functions and more relevant as a unit function of change and indicates the relevance to the output by its squared area created by the difference between these input pair points and the origin
    - applying rules to find highly useful structures like interfaces such as by asking questions in a sequence like:
    	- 'what are the maximal difference-capturing variables like change' (such as 'functions vs. constants') or 'what contradictions exist in useful structures like change' such as 'what changes dont change' and 'what are variables of changes that dont change (limited change around a symmetry)' and 'what changes unchanging variables (constants, symmetries)'
    - rules like "'what are not inputs' are also a cause of structures in addition to inputs" (bc resources not invested determine states of alternate functions) which is another reason to derive missing information (not just to determine what a function will likely do bc of some input but also what other functions could stop that function given missing inputs devoted to alternate functions)
    - an alternate implementation of a 'blur' algorithm (using the compression/filters/other interface structures involved in vision) to quickly determine trends in a data set is to sample the data set and evaluate each subset quickly to benefit from the overall impression of the emergent pattern visible across a sequence of alternate subsets, where the impression is formed by easily differentiated structures like 'border angles', 'densities', 'ranges', etc which are common across multiple subsets in the sequence, or to align subset data sets as a sequence or network of subsets (organized by some similarity), to make trends more easily identified
        - similarly iterating through data set subsets quickly (to efficiently store memory by identifying the most obvious repeated patterns) is another way to implement an algorithm similar to 'blur', if the neural network has the ability to derive 'overall impressions of a shape' to compress the data set rather than just simply storing the whole data set
        - similarly, identifying a regression function is possible by finding which functions seem to be in the center/average of a data set (as a proxy for their summarizing capacity for longer when rotations/angle changes are applied (to view it from a lower or higher position), a transform that doesnt change the summarizing capacity of a function immediately if its a good summarizing function, 'summarizing capacity' of a function being easily determined from 'impressions' achieved by low memory (obvious feature retention) or fast processing times ('blur' effect)
        - similarly, identifying a data set or regression function that is 'most similar to the origin data set' is trivial by applying some transforms to some maximally different or standard or randomly selected data sets and positioning them in a way that aligns with the original data set (like by aligning its ranges in a separate graph that is close enough to easily identify differences but obviously separable) so it's obvious whether the added data set/line is similar to the original data set (and therefore the regression line of the added data set can be used as an approximation), which applies the 'comparison' functionality of a symmetry to the concept of 'data sets/regression lines'
        - where the 'summarizing capacity' of a function describes how much info the summary preserves (such as info about limits/averages/densities/vectors describing variation from base functions/patterns/possible interactivity/adjacencies/errors/ratios/probabilities, etc) which are metadata about the data set that can be variables summarized by the data set as well as subsets of data set points can be summarized as components of the data set, these variables of the function being possible input components or output solution metrics or interim sub-intents of a regression algorithm, which can be used to determine all other algorithms
        - as a metaphor, the "'definition routes' that when combined say the same thing (in a different way)" is a good set of nodes on a language network to use as an approximation of the variables of regression algorithms (and problem-solving in general), as a stable interface structure that allows maximal variation (it will be similar to my set of verbs like find/build/derive and structures I have identified as useful and the interfaces Ive identified and so on), as different regression algorithms indicate a general summary of the input, which varies somewhat around the interface of the input data set, while "saying the same thing" in the sense of preserving info about the input and "saying it in a different way" such as by preserving different variables and applying different functions
            - a query to find the 'summaries that are the most similar, where the inputs are highly different' is a good way to find these nodes
        - relatedly, finding out if a function is a straight line or a wave function with very small magnitude (or another type of polynomial with difficult-to-measure incremental changes) is a matter of testing a subset of the data set for variation and extrapolating/expanding that pattern to the rest of the data set (as opposed to checking the whole data set), but this isnt a good way to find out if those wave patterns are errors or if errors or error-similar structures like waves vacillating around an average are a default component of reality, which is related to the problem of determining the level of specificity and discreteness to assess incremental changes of integrals and polynomials (waves being a default difference structure), but this problem can be addressed to some degree by applying scalars to magnify the subset to make differences from constants/waves more obvious, checking it for robustness by applying randomness/errors, and other methods of identifying a stable function
            - just like the 'required component of a curve' is a 'subset with count greater than 2', 'subset with count greater than 2' can be used as an 'adjacent input format' to speed up an algorithm to assess non-linearity of a data set regression function, just like other algorithms have input formats that are more optimal for the algorithm than other formats, and 'subset with count 2' dont have as much info embedded that could relate to non-linearity as a 'subset with count greater than 2'
            - related questions to this problem are 'at what interaction level do you zoom out/in to in order to describe the variable interactions adjacently describing the most variation, or otherwise in a useful way (until there is a clear summary line, until the points are obviously differentiable between point types, until the point connections are obviously differentiable between connection types, etc)', as changes in scale and distance from data set can either erase or magnify differences like non-linearity, just like changes in scale/distance from the data set can make similarities such as patterns like lines more obvious
        - finding out 'non-adjacent similarities that are relevant', such as how finding 'non-adjacent inputs with equivalent outputs' is useful bc it implies a 'horizontal line' structure might be relevant, relevant such as 'being a useful base solution function to apply changes to in order to find variation based on that line, or to use as a simplistic summary function', which can be framed as a 'high density output' with importance bc of the repetition of that output in some pattern like an interval indicating cyclical patterns, or like a ratio indicating commonness, either way indicating usefulness of that 'high density output' as a base to apply changes to or use as a simplistic summary function
            - relatedly, vertical lines (indicating an input that can become any value) are so unlikely in a data set that isnt random that they can be ruled out as improbable in most functions except where required or made probable by another route
        - calculating the function from a set of symmetry structures (local subset averages (most similar point or change from most similar change rates to other change rates), a set of inflection points (a change in change structure like charge of change rates), a set of peaks (change in direction of change rate)) or symmetry-limiting structures (extremes of a probable range representing limits) is trivial once a ratio of these are known given some complexity (& other metadata like input ranges) of the function
            - relatedly, another useful problem-solving intent in regression is 'finding asymmetries in peaks which are useful to know about using adjacent info, bc peak symmetry is a useful assumption when true to determine more/all of the function from knowing a subset and relatedly useful to know about when false to identify when implications/similarities are contradicted and not absolute'
            - formatting the average as being more similar to points or being more similar to point connections (or being different from randomness or extreme points) is a useful set of alternatives when finding inputs that are common across useful structures or finding useful formats for algorithms that use averages
    - given that the complexity in regression is caused by non-linear shapes as opposed to constant lines, finding which variable interactions exist (which input variables are exponents) and which cause these complexity structures (exponents of an input variable, constant coefficients of exponents, addition/multiplication with other exponent terms, etc) would enable removing those complexity structures to reveal a simpler form of the data set that is more easily condensed into a regression line or a base solution function to apply changes to (like how removing parabolas that are symmetrical around a clear or probable average is a trivial task that makes the average more obvious), such as applying inputs of complexity structures to the base solution function to specify the more general simpler function
        - this is related to workflows like 'remove variables until a simpler component function emerges'
        - this is also related to workflows like 'find a simpler combination of inputs preceding the original function/data set' which can be applied in another direction such as 'find the function interaction level where maximal differences begin to emerge' such as how combining functions by type (like step functions, wave or other polynomial functions, sequence functions, discrete functions, closed shape functions) can produce functions that represent a 'combination of these maximally different function types' which may be more useful at filtering a function solution set than other interaction levels, so this interaction level of function types may be a more useful place to start when finding a regression function to summarize points
            - relatedly, finding 'sets of useful interface queries that decompose most variation' can be as simple as finding sequences of interface structures like 'find common/powerful/probable/high-variation variable interactions that match common system interactions like common system errors or common differences from incentives/defaults' and 'apply the function type interaction level to decompose the remaining unknown variable interactions, once these common/powerful/probable/high-variation variables are known', sequences which could be generally useful in 'decomposing most variation across problems', which is a useful problem-solving intent
        - relatedly, finding algorithms to connect simplicity and complexity (and independence/dependence and unstable variable/stable constant), generating one from the other (like simple core functions that when repeated generate surprisingly sustainable/cascading and complex differences) are useful to decompose the likely starting point and simple/complex components of a system
            - like the simplicity in the central limit theorem generated from a complex combination of random variables, and the tendency of complex systems to neutralize each other's differences in some way when combined/repeated, for interactivity (as a complex system is less likely to be sustainable on its own when repeated), and the requirement of a variable like a symmetry in a simple function for it to easily generate complexity
    - given the possible set of interaction functions (direct connection, constant connection, side of an area representing a limit/border, error connection, random connection), use these interaction types as a variable to identify algorithms to classify connections between points based on their probability of being one of these interaction types and the line patterns that emerge given some set of interactions of a particular type pattern in some subset of the data set
        - for example, any two points in a data set are very unlikely to have a direct connection, so that should be used rarely
        - if a connection is confirmed or probable to be a direct connection, adjacent connections are less likely to be direct connections and other connections similar to the direct connection in other subsets are likelier to also be direct connections
        - if connections indicate low volatility, finding a connection with an extreme slope implying 'volatility' is less likely after some ratio of connections are tested/determined/assigned a probability of some interaction type
    - a function to find the simplest (or otherwise effective) polynomial to describe averages of local data set subsets (where angle of lower/upper borders and densities influence the average) is an example of a standard method that can be found with adjacent structures
    - changing the definition of useful structures like 'averages' to find alternate methods
    	- defining an average as a 'line that when changed the most compared to other functions, still fits within the boundaries described by upper/lower limits' points directly to a method to determine the average function fulfilling that definition 'find a subset of possible different functions to change, and changes that can be applied to these functions, and apply boundaries as limits to these changes'
    	- defining an average as a 'difference from extremes' or the 'usefulness of right triangles in finding average functions (and vectors applied to them to generate extremes) of a data set' points directly to methods like 'find angles applied to a possible average line that capture the highest variation in a data set, once possible extremes are known'
    - evaluating a function's 'differences from randomness' is another starting point to base changes on rather than basing them on an average bc its the 'opposite of the intended information' and is therefore similarly useful in that adjacency
        - other function bases include functions that 'connect non-adjacent subset averages', that 'connect adjacent subset averages', 'connect function upper/lower ranges', & other representative/summarizing functions
        - a random data set is not useful and is therefore useful to determine early on in calculations, just like function limits and patterns are useful to determine, and randomness may as well be an indicator of falsehood (as in 'something that needs to be changed in order to determine truth, like requirements/impossibilities') bc of this lack of usefulness
        - other known structures that are not useful are equally likely to use as bases for change, like how known useful structures like core components are, bc of their dichotomy in the certainty of their usefulness
        - applying 'common types/variables of functions that can form randomness' (such as 'contradicting/neutralizing change types that cancel each other out' or 'complementary opposite change types (like triangles which form a square)' or 'randomness-amplifying which doesnt change the randomness of the inputs' or 'symmetries like the equivalent weight of dice sides' or 'a high number of variables' or other functions that are likelier than average to create the requirement of randomness as 'even distribution of probable outputs') is useful as a way to 'determine probable randomness' or similarly to 'determine differences from randomness', 'remove/add randomness' and other intents related to randomness
    - building an interface structure of interacting rules to base changes on, like a 'set of requirements' (like how 'connecting components of a structure' is 'required' to 'form component connections to create that structure' by definition) or similarly a 'set of rules that are definitely impossible or not true' as a foundation for other changes (where possibilities exist between contradicting limits imposed by requirements) is a useful structure to start discovering new rules from
        - identifying rules that identify non-adjacent information required by a definition is similarly useful like definitions that identify adjacent/obvious information required by a definition are, like how numbers in a sequence like the set of integers are required to be one unit away from other integers and required to increase if its the set of positive integers, so knowing that a set is sorted in an order like this gives you information about all the numbers between two items in the set bc of the definition determining the set allowing functions like 'estimate where an item will be found to reduce the search space', or how the type of a number determines some of its known functions
    - a standard method to solve problems is framing them in terms of core structures like 'similarities and differences' (like 'similarities to constant representative/average lines', 'similarities to averages', or differences like 'difference-maximizing functions', 'highest angles connecting adjacent/similar subsets as a function that is almost guaranteed to be incorrect to base changes on'), then applying a function to 'determine which differences to resolve' (like 'differences between base functions like averages and alternate similarities like densities' or 'differences from highest angles connecting adjacent subsets and a base average/density-determining function')
        - this method finds the variables likeliest to be known/similar (or easily derived/predicted as adjacent to known variables) constants like 'averages' and 'local subsets (locality as an indicator of similarity)' and 'densities', and variables likeliest to be unknowns like 'alternate more complex functions with more variables' and applies similarities/differences to model those and find which differences are relevant to resolve (the differences that 'connect related similarities/differences', related by providing complementary information for intents like 'represent a data set', complementary information like 'base functions' and 'specifying differences customizing that base')
        - finding a function that models the 'maximally different local subsets of the data set' is a solution-finding method easily produced by this method
        - this is an implementation of the workflow involving 'finding matching structures based on common attributes' to model 'uncertainties within that structure' and connect these structures to problem/solution definitions
        - other differences to resolve could be the 'difference between the set of possible/probable functions and the set of best representative functions' or the 'difference between a set of probable determining variables and a probable representative function' or the 'difference between probable linear representative adjacent local function sets and the non-linear variants that are better representatives'
        - this is related to a structure like 'applying variables to a structure like a "set of maximally different angles" applied as a symmetry to find the angle set that hits (or alternatively/equivalently approaches) the most data points when the angle set is changed the least (like rotated, shifted, scaled, etc)', since a 'line that approaches the direction of or is adjacent to the data point densities' is similarly useful as a 'line that intersects some ratio of points', as a 'representative function' isnt required to intersect with any data points, so that intersection is a variable that can change in the solution-finding method, as well as other properties not required by the definition
        - this formats structures in a way that makes it adjacent to identify variables and sources of variation (like rules like 'sudden constants/similarities that enable other changes to begin by providing a foundation like a barrier/limit for differences are a good way to identify interface variables in systems'), which is why the primary interfaces are useful in the first place (they allow highlighting uncertain differences by applying certain similarities/differences through standardizing/similarizing to fundamental/core unchanging variables like 'cause' as in embedded variables of that variable like 'causal degree' that support/describe/limit other variation and otherwise fulfill intents related to variation the most completely)
            - relatedly, identifying the most powerful variables as the biggest sources of error in a particular format, such as 'causal position' being a source of error in the causal network format (such as how an input might be an output of the output but it could seem like an input in some false similarity errors) and the 'connection function' in the network format (such as how some changes can seem adjacent/probable in one network format but its a coincidence, where some other network format is more reliable at making those connections adjacent)
            - relatedly, the primary interfaces are also useful for being based on the 'reasons' why a structure may be relevant to another structure (it is caused/changed/allowed/required/intended by other structures, it is useful to or interactive with other structures, it is a variation (as in a definition route) of other structures like concepts, etc) which are united on the 'meaning' interface (determining relevance/usefulness of structures to each other)
            - calculating the structures that are not in a data set but which could be relevant based on other structures that filter structure combinations like probability/similarity/commonness is useful as an intent to predict possible real structures that will be found in future data sets (calculating uncertain differences before theyre a problem/before theyre real)
        - this is related to other workflows like 'find maximum differences (what something is not/find the opposite of something to find limits of what it is)' by asking questions like 'what is not cause' (with answers such as 'an event that follows another event is not necessarily a causal sequence bc they may be so indirectly connectible that they are effectively independent') to find useful structures like the limits of the causal interface and how it interacts with other interfaces like meaning (such as how 'events that follow each other in time may be causally separable and arent required to occur in the same system or detectably influence each other')
            - similarly, finding why structures would not interact (such as how 'some structures cant detect/measure other structures, and therefore cant use them as inputs') is useful as a filter of 'meaning' interface logic
    - a unifying function of the various representations of a function where the representations are variants supported by the unifying function is likelier to represent the function the best
        - similarly, a unifying structure that supports various representations (like a 'set of maximally different directions', a 'set of reflective mirrors as polygon sides capable of producing different variants of the same information', a 'network of foundation structures around which maximal changes are supported which can coordinate', a 'set of filters capable of filtering the highest ratio of solution sets with the highest similar degrees of accuracy', a 'set of overlapping shapes with a common center (of common components) that model reality with similar accuracy', a 'set of connections between common high variance-capturing structures like maps/filters/networks') of the interface network (in various perspectives that filter it) is likelier to best represent the interface network
        - finding a useful 'sequence of filters' is useful as a good way to avoid problems of assuming too much & other basic errors of bias, like by applying "possible, known, required, probable, computable, measurable/testable, usable, & realistic" structures early on in the filter sequence
        - similarly, a unifying function of solution metrics (efficiency, accuracy, generalizability, flexibility) is a useful base to apply changes to in order to determine variables of solution-finding methods
    - finding useful structures to combine as defaults is useful, such as how 'symmetries', 'fractals', 'randomness as a limiting counter-structure', and 'right angles' are useful as core structures to describe a high degree of changes bc of their definitions ('applying fractals to changes in the direction of a right angle based on a symmetry up to the limiting point where additional changes appear random in their accuracy at describing change' can describe probable changes around that symmetry) bc of the relations between their definitions ('symmetries' and 'fractals' both having a 'common base (of a "self") for change, and a limit on changes to that base' in common, so applying these in the same structure benefits from their common symmetry in their definition and applies changes to this symmetry in their definitions, and 'fractals' further fulfills other attributes of symmetries like a 'limited change, as fractals converge')
        - similarly other examples like 'why i is a relevant number to rotations' given its adjacent concepts which make this functionality probable or inevitable (not only its allowance by definitions, but also possibly its difference in ability to produce a difference in sign/direction from the origin, its core operation of 'multiplication' being a definition of the components of a type of n-dimensional change, 'multiplication' as relevant to angles through creation of closed cornered shapes, a unit of change that is relevant to rotation, the relevance of sign changes to wave functions which are relevant to circles, etc, which are useful changes for connecting more directly relevant structures to rotations like pi), in its function connecting two other structures of rotation such as e, a structure related to spirals (a structure with a useful equivalence in its change rate ratios of adjacent compounding) and pi, a structure that acts like an origin/symmetry of external/internal spirals (trending toward polynomials in the external direction and trending toward individual points in the other direction) and would be a spiral but is missing the 'change rate increase' to create a different equivalence than a spiral has (a change rate equivalence, that creates the property of 'closedness' in the circle as opposed to the spiral but is sufficiently different from a 'cornered shape' in either the internal/external direction so as to justify its own definition separate from spirals or cornered shapes, despite being equidistant from these and other and unknown structures in their definitions)
    - mapping problems to more defined fields like highly structural creative industries such as 'music' to find concept mappings that are more easily determined like how finding the rule 'intelligent goodness is more difficult than obvious/complicated wrongness or obvious goodness' by applying the clear definitions in music of "obvious rights/wrongs like compliance with major/minor chords/notes or compliance with patterns" and how finding the intelligent goodness requires knowing the obvious errors it avoids like incentives like 'cheap rewards from any difference, even wrong differences' as its easier to create a new minor song than to create a new complicated but good song, as the range of possible solutions is narrower but can still host complexity/variation that intelligence could survive in, and complicated goodness is more complicated than complicated wrongness bc of the additional problem of the limited range that requires creativity to sustain intelligence in, a rule that would help avoid errors of over-simplification, prioritizing any difference, and avoid obvious errors as well as errors that create changes that violate a solution structure like a range of good solutions, and would incentivize finding high-variation variables sooner (to stay within the limited range) than prioritizing wrongness would
    	- finding a corresponding physics version of a useful structure is a good filter to apply when determining useful structures or other structures to apply as defaults bc these are likelier to be functionally useful if not more plausible, realistic, or possible
    	- example metaphor: gravity between 'equivalent alternates' (such as alternative theories) is weak but enough to keep them in the same definition (allows 'aggregation' to occur)
    	- other example metaphors which are useful in the sense of being evocative or otherwise useful for calculating something: 'supersymmetry' and 'interface network', 'string theory' and 'variables as waves, as reality units (possibly related to twistors/spinors)', 'isomorphisms retaining histories/inputs' as some definition of a 'wormhole', the 'jacobi identity' (and other structures of inequality like asymmetries) as an example of a 'way of determining what structures will accrete in some definition of unidirectional time (like to determine what will become matter)', 'commutativity' and 'equivalences in quantum superposition probabilities', 'invariance attributes like associativity/commutativity being a useful structure as a base to apply other changes to, to base other more speculative variables on', 'non-orientability and CFT violation', 'whether representations of reality form reality, to the extent that it can collapse/expand into or be based on other representations as those become more energy efficient, like in a cycle of different representations', etc
    		- related questions: 
    			- are all measurable invariance attributes components of reality or are some of them the basis of some reality and others the basis of another like points on a lattice that can be a foundation of reality, where points in between are not, so travel between them could only involve motion in the transform producing either from the other
    			- which symmetries are foundational and absolute in the math interface? which symmetries (like abstraction as filter or map, connecting similar concepts like energy/variation) connect these absolute math symmetries to physics symmetries? which symmetries allow maximal differences to develop? what symmetries exist in reality (such as symmetries across space-time states preserving potential energy or entropy, so that we only have access to functions within a probable range area governed by that metric)?
    		- the usefulness of these metaphors depends on the variability between 'definitely possible' and 'definitely not possible' (plausible, logical, not definitely impossible, evocative, suggestive/implicative, conditional, etc), so that a tool to 'traverse similarities and apply maximal differences to find structures like requirements, symmetries, concepts, and limits' can benefit from embedded but not articulated useful variables/structures such as connection in language (superset of definitions containing a subset of math definitions)
    	- similarly, other metaphors include how the number-based spaces can be extended to apply to number types (like a 'prime-based space'), how a 'complex number-based space as a possible structure of reality' has a corrollary in the insight that "almost every fact has a counter-fact (similar to paradoxes with local contradictions across statements) where it is related to its equally legitimate opposite (where it's not true), where opposites are allowed by definitions (similar to manifolds that can seem like extremely different objects that contradict each other absolutely but are actually related consistently by some common structure)", where alternate possible spaces may represent/embed other possible insights as their generative/limiting/determining functions, and which may intersect/overlap in a space of these insights (which has a corrollary with quantum field theory), etc
    	- 'time travel' could be possible in the sense that realizing things faster and being able to store/determine/compress more information (which increases the potential time available for the realizer, by slowing down time for them, as they have more functions/variation than other people) can put everyone else in the past relatively by decreasing their relative rate of change, but only while the realizer continues to do so and if they can reverse that change
    	- 'increasing similarity in the sense of synchronicity' by giving everyone the same perspective (like by giving them a maximally different structure to handle problems with) is another function of putting agents in the same space-time or on the same timeline (converging timelines and integrating/connecting spacetimes)
    - finding solution functions for possible known errors to existing methods, like how 'standard neural networks' can have an error of 'finding a different function of different components for each input/output pair' or 'finding overly simple functions that re-use the same (or otherwise simple) components the most but dont handle extreme/new contradictory cases' or 'finding all possible incremental components of some size that could differentiate some outputs and removing some of the less useful/adjacent/common incremental components'
    - identifying interface query intents fulfilled by known useful structures (like how 'cellular automata' and similarly 'standard neural networks' are useful for interface queries such as 'finding maximal differences generatable by one logical/change unit on its own, to identify where a seemingly complex phenomenon can be identified by one variable (the logical unit), thereby reducing the complexity of high variation data sets or the complexity of finding variable interactions between variables of high variation data sets')
    - neural networks may have an error created by the 'sequence of training data', in which earlier training data influences the final solution function disproportionately to its utility value
        - finding the worst case scenario where each training algorithm could miss the most obvious (or otherwise useful) alternate/absolute optimal (such as with gradient descent) bc of the order of training data and how to correct these problems (such as 'start training multiple models at multiple different points and attempt to integrate/converge to an absolute optimal given their change types, like pursuing only those descents where lower values are clearly identified or otherwise where lower values continue to be possible')
        - abstracting this workflow to 'find common variables like "robustness to order changes" that are highly differentiating in math fields/functions, then apply these common variables to neural networks/regression algorithm to check for variation resulting from these variables'
        - allowing variable interactions identified in later training data (like more foundational base symmetries) to replace/change variable interaction structures (like an emergent lesser symmetry already identified in earlier training data) may be more complicated than just applying PDEs to assign changes to specific weights (like by integrating the 'weights of the lesser symmetry' with the 'structures like limits/invalidations of the lesser symmetry allowed/required by the base symmetry' on which it depends, such as by consolidating weights indicating the lesser symmetry variables into one more powerful variable on the base symmetry)
        - having an 'update function' that handles updating the weights in each of these worst-case variable interaction cases (or the most erroneous or most common cases), such as where 'the final training input indicates the base symmetry that is extremely different from the lesser symmetry already identified'
        - the reason interface analysis is so powerful is that it acts like the 'base symmetry (or hyper/metagraph) of all base symmetries (or hyper/metagraphs)', so having a neural network that tests interface analysis variables as a prioritized structure and integrates them into update functions is more powerful than other neural networks
    - 'minimize extreme errors of a solution function' and 'maximize data set coverage of a solution function' are both 'alternative contradictory' intents to solve the 'find a regression function' problem, which offer some degree of 'complementary info' rather than 'definitely overlapping or equivalent info', as the 'extreme errors like missing an entire variable or missing an outlier' and the 'maximum data set coverage' have no guaranteed overlap, as a 'maximum data set coverage' could easily exclude extreme values or other sources of extreme error like variables which are easily missed if some subset is selected
    - an example of specific simplifications to solving prediction problems can involve avoiding known suboptimals/errors/violations of requirements by identifying & applying differences to those and allowing all other variation to develop, such as predicting a subset of just the worst case scenarios by applying extremes to increasingly high variation variables (technological development in some direction like electricity/automation/chemical printing/speed/compression/computation) and identifying opposing variables (particle accelerators/generators which can modify components of or interactors with electricity, software likely to be used with electricity, & quantum technological development which can modify components of electricity can oppose a possible error possible with electricity technology in an extreme such as in a concentration of one position, 'if energy technology or energy technology variation (innovation) was concentrated in one position by some entity, what could oppose/change it'), and otherwise modifying high variation variables ('what are all the worst case combinations of high variation variables with other high variation variables like quantum computers and neural networks'), as if variables dont cause high variation, they can sometimes be ignored in some cases until they indicate change in the direction of causing high variation, as knowing the 'structures like patterns of variable development such as patterns of large-scale errors' is more useful than solving for every possible variation interaction in cases like with computation limits, similar to how 'limit change patterns' are also useful in decomposing all variation
        - this can generate possible innovation intents like 'encrypt physical molecules so they cant be read', 'check if enough particle changes occur in a sufficiently continuous area, whether different types/configurations of gravity can emerge', 'inject variables in reality in a way that creates gravity, if uncertainties attract gravity to require interactivity to develop to handle the increase in variation without destroying the foundational structure (create a structure like a black hole, an isolated uncertainty cascade)', 'check if there are different units of spacetime like an opposing fact/falsehood pair or a pair of space-time states connected in a sequence or the interim structure of other suggested units like a wave/particle', 'speed up information travel/acceleration technology so farther information can be read and used in predictions more quickly (such as by directing radiation/randomness wherever its not visibly/directly reflected back at us by hitting a structure, so that it hits something we cannot measure and therefore is likelier to encounter maximal differences if they exist, which are likely to respond back as intelligence sources and improve our rate of technological advancement, or similarly direct radiation at inputs to these sources which is likelier to be findable and is similarly likely to get a response as well as likelier to incentivize organization/connection and lead to an increase in the power of radiation to cover distances as those will be reduced by this connection and therefore increase its speed), or similarly direct radiation in known stable ways that create maximal differences here to attract other maximal differences as intelligence sources who require differences to solve their problems' and therefore the direction that changes (innovation) may be applied to fulfill those intents, as measurable intents are likelier to be focused on & fulfilled, and relatedly 'horizontal innovation' can also be incentivized to connect these innovations in different directions on a new interaction layer
        - conceptual math is particularly useful here (not just 'add a concept to another in a simple combination/set structure') but entailing all of the possible useful interactions between concepts
            - if you can run queries like 'build a new (not already known) possible interaction type between these concepts which could be true' and 'what are the probable error and limits of the interactions between these concepts' and 'which concepts are more interactive with these concepts' and 'which concepts reduce the interactions of these concepts', you are applying conceptual math, as opposed to just stringing words together in a simple combination/set
            - 'what are the possible interactions of interface variables, such as some areas of powerful developing technologies (like batteries, quantum computers, encryption, and 3-d printers)'
    - identifying the few 'specific problems to solve, which can be determining of all other structures' is useful for reducing the required computations to apply problem-solving methods to, such as how identifying some variable interactions is more useful (like eigenvectors/eigenvalues, or energy/entropy/potential) than other variable interactions, which can be optimized in their usefulness by identifying how problems are related so once these variable interactions are found, the other problems are adjacently solved (identifying the 'problem network' that will fall to these variable interaction functions)
    - identifying the core variables of physics/math can be used to identify other primary interfaces
        - for example, the lagrangian (to measure potential), jacobian or relatedly commutativity (to identify sequences/order), lie algebra (to identify symmetries), and other default math/physics structures can be used to identify other primary interfaces and useful interface structures
        - similarly, the 'momentum' variable shows up in physics frequently and indicates the 'incentive' variable that determines many interactions
        - similarly, the 'frequency' variable shows up in physics frequently and indicates the 'commonness' attribute that can lead to 'probable structures'
        - similarly, the 'imaginary numbers' of math reflect truths of reality like 'every true statement has an opposing statement that is true to some degree or in some way or in some context' and physics structures like matter/anti-matter as well as the importance & commonness of dichotomies and spectrums as important/powerful variables, which is an adjacent transform, once that insight is known & applied and those structures like dichotomies are known, just like a 'ghost of a definition' which retains the bare minimum 'skeleton' required for the definition to still be true, but is still so different bc of the variation allowed within the definition that it seems to contradict the definition, these overlapping/integrated definition 'ghosts' determining the potential and 'probabilities of resolution (into a structure/state)' of a definition
            - relevantly, applying 'alignments' to preserve connections (like an 'opposite definitive' space of 2-d euclidean space to graph imaginary values, or a 'system' space where both values are definable/graphable but the 'opposite' structure of their connection is maintained, such as that they move in opposite directions, such as an 'opposite of space-time' where the definitions still hold so that 2-d imaginary structures can be applied to physical structures described with the opposite of imaginary numbers, as opposed to using dotted lines to indicate imaginary variables, like an 'inverse space with a negative component indicating the imaginary component, in the position of an exponent coefficient, or a standard vector space indicating change direction components, or a space preserving the positions/connections between roots of real-valued variant transforms like even exponents, with the odd exponents positioned by definition in between, while mapping the powered spaces into the unit space')
            - relatedly, questions like 'what are the types of time in between imaginary and real-valued time' and 'what are the limits of imaginary time in supporting a higher proportion of structures within the time definition than is implied by current understanding of definitions' and 'what connections are required/possible between different types of time and what interaction levels use these types of time as defaults' and 'what variables/functions determine/maintain/require the interface between reality (stable/constant structures emerging from quantum physics interactions) and potential (quantum physics components), is reality similar to seeing a cross-section of a wave that is at a interval where perception can occur and the interval points can be connected and change can be synchronized across those point connections, where other realities are not on the detectable spectrum at that cross-section without making inferences by calculating gradients to adjacent detectable points' are useful as evocative thought experiments and interface queries to find useful structures
        - similarly, the use of 'light' as a metaphor for 'varition' is useful for determining core variables of change (like angles of possible motion in a sequence) to determine valid and realistic methods of inference (like 'scan an area created by some angle of change from this sequential pattern, according to how light reflects information at angles')
        - similarly, the related concepts of 'independence/orthogonality/non-intersectiveness' in math is analogous to useful interface structures like 'maximal differences' (such as differences in 'maximal difference-uniting symmetries' like cross-interface structures like 'cross-system similarities')
        - similarly, the structure of 'quaternions' is a useful analogy to a 'unit interface structure' in that it encapsulates a rotation and an axis of rotation, and also applies a 'maximal difference possible within a definition (of a real value) without breaking the definition' as a useful structure to fulfill intents like 'create opposites/differences'
        - similarly, the manifold is a useful corrollary to a 'type' or 'definition', in the sense that items belonging to the type or qualifying as the definition can vary within a set of limits maintaining the manifold structure
        - similarly, the concept of an 'exponent' (self-similarity) is related to 'randomness' (in that a square allows more randomness if its the shape of a data set as any connection between low/high x-values is equally possible) and both are also related to 'symmetries' (in that the square has four symmetries where a straight line has one, and that randomness aligns with symmetries in that symmetries act like equivalences under some change), which you could predict from the 'self-similarity' attribute of the exponent
            - relatedly infinities could act like a symmetry in that if there was an infinity in physical reality, it would be invariant to change (taking one item away from the infinite set wouldnt change its 'infinity' attribute)
        - similarly, the 'density' of a black hole is a corrollary to a 'density' of a data set in that both are 'powerful' (the black hole is powerful through being a source of energy, and the density storing information is powerful to the extent that it influences the prediction function)
            - this is an adjacent sequence once both structures are standardized to the 'information interface' (the 'density' is a significant attribute of a black hole to focus on, bc of how the black hole interacts with 'information', similar to how the density of a data set interacts with information by storing/representing information through its center/average)
            - this indicates if you had an equation like the following, you could derive x using interface analysis to derive 'density of information' and 'power of information' as a useful connecting structure allowing the equivalence/similarity indicated by '=' and identify 'some energy type (like dark energy)' as the x-value
                black hole/x = density of a data set/power (in relevant problems like prediction or determining the rest of the data set)
            - this is a task that transformers should be very good at, given their ability to map systems to other systems, if applied to interfaces as the systems to map
        - similarly, variables such as 'heat' connect different interaction levels (as a 'primary change-determining and change-generating variable' of chemicals, connected to a 'primary exchange unit and primary input' of cells) so these variables connecting different interaction levels are valuable to identify as connections to new interaction levels
        - a universe that requires components to exist also requires a combination function and combinable structures, and one that doesnt require 'uniqueness' allows for 'repetition' and therefore 'quantity' to be measured and 'a number set to differentiate quantities'
        - the idea of a 'particle/frequency of cause' could be the wrong structure to look for, as 'cause' emerges from constants that allow the following, meaning that a coordination between many particles and input/system conditions is required for the concept of 'cause' to be allowed and emerge
        	- change to occur in a 'input/output sequence' that can co-exist with other sequences (occupying and validating the same timeline, and also creating it as a component of the timeline)
        	- interactions between structures, as opposed to requiring isolation of structures
        	- interactions/changes that dont invalidate a high ratio of other structures by default
        	- inefficiency of structures in handling variation injections, needing to move or disintegrate in response to variation like collisions, rather than having stronger forces maintaining their structure, which is an output of energy transfer
        	- however cause could be said to occur at a 'frequency' in that it requires two states to be connectible by some distance across which information/structure can be preserved, and similarly in the sense that a structure is a cause of other structures if its maintained long enough to impact other structures, a structure could be said to be 'causative'
        	- 'causal erasure' relatedly occurs under conditions where many equivalent alternate common structures lead to the same effect, as opposed to a direct identifiable unique causal connection, and similarly erasure of other attributes of cause like 'inevitability' can remove the concept of cause
        	- relatedly, is there a structure such as a 'default network of entanglements (default synchronized structures)' that ensures primary interface concepts like 'information', 'cause', 'change', and 'potential' can continue existing, which creates new entanglement connections if some are disrupted, and is reality the network or set of overlaps connecting these default networks of entanglements, and can these entanglement networks be used to distribute variation/energy evenly (using a randomness entanglement network) so that spacetime curvature doesnt occur and spacetime is experienced similarly for all observers, or would the universe move too fast in some direction once observers are moving at constant speeds for that to be useful or would it distribute change across the universe so that there is no motion of the universe as its neutralized by the evenness of the distribution
        - similarly, 'potential' emerges from constants that 'allow some changes/interactions to occur within a limit, allowing organization to occur' rather than 'preventing any change/interaction' and also 'prevent all changes/interactions from occurring in a way that leads to chaos' and similarly 'allow change types or change inputs to be stored rather than used'
        - why is it possible to make predictions using something other than 'input/output sequences'? bc there are alternative equivalent structures (requirements, networks, probability/potential fields) which are non-sequential and which are useful structures (stable structures of reality)
            - "is there always a 'mirror/symmetry' to reflect across every symmetry (is there always a 'mirror' to reflect past/future states of a system, given some distance to reflect information across)" is another useful question
    - identifying function sets to identify similar structures that can generate useful structures in an alternative way, such as how some sequences can generate a 'change rate that changes every time' like the fibonacci sequence, which can look like and function similar to other types of change like simple exponential change, using a different input structure, as a way to fulfill useful problem-solving intents like identify different possible inputs
        - fulfilling a limited set of problem-solving intents is possible with specific functions which are alternately called 'interim functions'
    - identifying the possible usefulness of alternate function formats (like a 'unifying parameter' of 'maximally different sequences (different for each set of x-values such as 'adjacent x-value pairs') that when summed, converge to the y-value' which would be useful to connect systems of variables that could make every set of x-values justified in having its own function, and in applying a different function format of a sum of a sequence as the sequence of change combinations that are adjacently computed which can be used to calculate the output sum that is the y-value), formatting the problem of regression as a 'solving a system of equations' problem, where the parameter to solve for generates the sequences, and where maximally different sequences indicate different possible systems that the variables could interact with, which could all be equivalent alternates in the sense of generating the same prediction function, which mostly only makes sense when using a set of x-values with y-values to connect in a particular function so that the sequences are relevant by default
        - neural networks should apply changes as 'variables/functions that are actually encountered in the real world', as a variable is applied within a system, as opposed to the approximated variant or the over-simplified or over-deconstructed variant
    - identifying 'points of relevance' where functions can be injected in a useful way to optimize a structure, such as 'during any given training iteration, applying a function like "checking whether changes are moving in the direction of the target output to filter less probably successful change sequences earlier in the training iterations" is useful to avoid computations that are probably less useful to invest in' using rules like 'where are processes repeated or otherwise inefficient which can be reduced in some way to fulfill some relevant solution metric like "reducing required steps" (as in not maximally different structures)'
    - identifying the 'path from math to physical reality' will probably depend on identifying all of the 'maximally useful structure' (on the 'meaning' interface), as physical reality indicates the 'structures which are more computable/adjacent/efficient/stable/sustainable/measurable/independent', and useful structures (useful across problems and interfaces) are the most stable/efficient/adjacent structures
        - I imagine the 'maximally different structure connecting problem/difference types' is going to become necessary to connect math with physical reality, whether as a unit of reality/time or in adjacently connecting other structures or some other fundamental structure
        - a useful depiction of 'time' occurring on this 'maximally different problem/difference type connection structure' is changes to the queries run on that structure, queries which connect increasingly more variables of it, give it additional structures describing it like symmetries/rotations, or create new differences to integrate with the structure (if possible once the maximally different structure is identified)
    - 'solution automation workflows' can follow rules governing selection of structures like 'spaces' to position them in structures like 'sequences', so for instance by the time a query starting on the 'interface network' and moving to the 'math interface network' to a 'causal/neural network' gets to 'finding polynomials in euclidean space', it should be heavily filtered so that the extremely high ratio of 'possible functions allowed' relative to 'probable solution function variation' in that final space are mostly filtered by the point the workflow arrives there in the sequence (as in, a 'probable function range area' or 'probable primary function components' are identified by that point), or otherwise progresses from a space with more possibilities to fewer possibilities if no such filters are applied, so that the space itself can act like a filter on the possible solution functions
    - a function to 'find all different known variable interactions' and a function to 'generate & check new variable interactions (generating new possible interactions and finding a system that seems to be modeled by those new interactions) to fill in the gaps left by known interactions' can be a useful function set on its own as most variable interactions follow common patterns like 'interval interactions', 'cyclical interactions', etc
    	- as another example, functions to derive variable interactions based on insights (like insights interfaces/symmetries, which could be applied as a function to identify the rules of 'embedding variables' as that is a required variable interaction which can be used to frame all variable interactions (how many variables can be embedded in another interface variable, of what type and how can they interact without violating an interface variable theyre based on and depend on to exist and change, in what real systems))
    	- applying symmetries as a 'magnet for change' can help model a system's handling of events like 'interface overload', 'variation injections', 'definition violations', etc (when one symmetry cant handle a change type, meaning it violates the symmetry, what happens in the system having that symmetry, does that variable always obey another symmetry or form its own or decay)
    	- a set of 'known physics functions' is another useful function set, which can be used as a core function set to connect all variable interactions to (a function to connect all variable interactions to physics laws would be useful and independent of other function sets in solving problems, for example 'genetic variables' would be related to 'collision physics and cell pressure/charge rules')
    - identifying useful structures like 'mirrors' that act like metaphors to evoke other useful structures as a way of representing info structures like 'filters', such as how mirrors offer information without having access to all information (like if positioned at an angle), similar to how some info filters reveal info about an entire number type, allowing computations to be skipped, or allowing a program to 'look ahead'
        - these cross-interface structures can be maximized in differences like angles (similar to a staircase or helix shape across stacked interfaces), creating maximally different structures to use as a base for other algorithms requiring changes
        - relatedly, maximally different structures are useful for identifying extremely different ways to frame/format the same variables, such as how 'differences from randomness', 'interim points in between dichotomies like complexity/simplicity', 'adjacencies', 'simplicities', 'embeddings', 'densities', 'curvature', 'probable function area ranges', 'filters of equivalently accurate/possible functions', and 'linear/exponential change type filters' are very different structures which are equivalent alternates (or possibly complementary in providing different info when applied with other different formats) in their usefulness for representing a data set as a function, these structures being non-obvious/trivial to generate the full set of, and optimally useful once generated
    - finding resolution functions for commonly useful connections/transforms, like connecting the 'densities to sparsities' or 'density patterns across densities' or 'densities to extremes' or the 'edge points to an edge line' or 'upper/lower/average edge lines' or the 'densities to regression lines' or the 'shapes (like graphs) formed by densities to regression lines' or the 'local subsets represented/connected and the subsets skipped/unconnected' and other useful connections in the data set regression problem space
        - other useful structure examples to apply as defaults given their higher probability for various reasons (adjacency to requirement, commonness, etc)
            - identifying points in between extreme errors/suboptimalities as the interim point that is more useful for more general intents while being generally sub-optimal as it doesnt specialize in optimization metrics
                - for example, identifying algorithms with variables that allow 'complementary' best-case input scenarios that are optimized by various algorithms to maximize coverage of input cases in the combined algorithm using these algorithms as components/alternates given some input range
                - for example, gradient descent is optimal in cases where local minima are good approximation of absolute minima and where the whole function is infeasible or inefficient to check
                - in cases where they are not good approximations of absolute minima, or where there are no minima, other algorithms would be better to find other sub-optimals that are less sub-optimal in those cases (like 'maximally different input-finding functions')
            - identifying compressed input info (like the 'output variables') which are more useful to identify than 'adjacent variables to input variables' for determining the outputs
                - the output info contains patterns and other interface structures which are useful independently of inputs in some cases, which can allow skipping connecting them to inputs if the outputs are compliant enough with patterns to be approximated by other functions like probability distributions (such as where the value is usually within some trivial difference from an average output, so this average can be used as a probable output, or where the outputs stay within a range and are relatively equally distributed in a range, which indicates a wave among other possible function shapes)
            - identifying useful structures like 'reasons why some interface structure is useful across multiple useful solution metrics, making it likelier to be robust to changes' such as how 'curvature' relates to 'self-interactivity' which is a core efficiency structure (that generates different change types with one input and one operation) as well as offering 'one cohesive unifying function of disparate sets of linear functions (such as local subset linear functions)'
                - similarly identifying other useful structures (like the 'e ratio' which resembles a core interface structure when formatted as a maximally different angle with a common base, as in a 'right angle', having a smaller change based on a larger change) as a highly explanatory indicator of symmetries which also relate to stability of systems (as systems that vary change types using minimal inputs are useful, small changes within some limit are useful, and are likely to reoccur as a result) and can act like a 'base solution to test first' in the absence of other base values to test as parameters (meaning 'apply e as a base parameter to check if the system has reached some local extreme of optimization/stability'), as the 'e ratio' is likely to sustain itself when applied repeatedly (a spiral), where other ratios are likely to intersect with themselves (circular ratios) which is useful for different intents and other ratios are likely to never intersect with or relate to themselves (leading to infinite change, which is not as commonly useful)
                - similarly other structures may be useful when applied as certain/constant/default inputs to a system, such as the 'exploitative ratio'
                - given that these specific math structures are highly connected to important/useful variables like stability/symmetry/balance, they are useful as specific math structures to start from/use as a base in some problems (like in stable/simple systems) where more complex systems that are not stable are likelier to benefit from differences from this parameter
                - finding the reason why a structure occurs is important bc different reasons create different types of change
                    - for example, if the reason why there is a negative correlation is bc some agent had an incentive to falsify the positive relationship, that would change in the next more accurate batch of data as being opposite to the correlation found, so knowing that reason is useful to predict how the data set might change
                    - similarly, if one agent is involved and if there is an incentive to falsify information, the information is likelier to be simple (follow a simple pattern) than it is to be complex, as simple-minded agents are likelier to need to fake some information
                - finding other constants (or other specific structures) of stable/optimal systems is similarly useful as these constants (or other specific structures)
        	- identifying useful structures like 'changes which preserve info about the data set (such as symmetries in the data set) given the intent to find a representation, rather than an intent to remove info' to identify possible solution functions (composed of those changes) which might be useful for representing the data set (like finding average lines)
                - any function which changes the data set in some way (like removing outliers or removing sets of points that dont change the average) in a reductive manner (reducing computations/inputs/redundancies/noise), while also preserving a relevant solution metric (such as a type of 'average' like a 'local average' which is nearer to the final solution function, or 'probability density' or other 'moment' of a function) could be useful in finding the final prediction function, which is the ultimate average or other representation that is useful for minimizing prediction errors
        		- relatedly, 'mirrors' (or 'look-ahead tools to skip computation') depend on finding symmetries around which changes vacillate within a known range defined by the symmetry, so that once a symmetry is found, all changes around it are determined and dont have to be checked (just like a 'type' acts like a symmetry around which change develops, staying within the range defined by the type definition), which means symmetry structures like 'averages/extremes/inflection points/densities' are useful for determining info with minimal computation
        		- relatedly, finding the 'sets/sequences of symmetries which capture most relevant info about the data set' is useful for finding which sets of symmetries should be applied for which intents, symmetries being useful for predicting change as they indicate change bases & limits
        		- other symmetries (which are highly explanatory of changes in the data set) include higher powers, as the higher powers of a function are good at forming a base function to apply specific changes to in order to find the specific customization of that base which applies to a greater ratio of the data set
        		- finding the opposite of these symmetries (lower powers) is possible to do efficiently if done locally, to find change rates that could be adjacently produced with lower powers, but finding the higher powers is still required and these cant be used to skip a high degree of computations
        		- checking 'exponents of identified local change types' is useful to find possible simpler/alternate adjacent functions (once x^3 is found to be true locally, x^4 and x^5 should be checked as well in adjacent or maximally different subsets)
        		- as another example, the following equivalent alternate sets of insights make interface analysis trivial to identify:
        		    - insight about multiple interfaces and automatability
	        		    - 'concepts have structure'
	        		    - 'information has structure'
	        		    - 'problems have structure'
	        		    - 'rules/functions have structure, and are therefore automatable'
	        		    - 'objects with structure are automatable'
	        		- insight about the interface concept itself
	        		    - 'problems are a matter of identifying similarities/differences'
	        		    - 'most problems are resolvable with standards that make comparison tasks trivial'
	        		- insight about the usefulness of each primary interface
	        		    - 'concepts can independently be used to solve a problem, without logic, information, or other primary interfaces, and without functions like "test" which would normally be involved in problem-solving'
	        		    - 'primary interfaces like concepts/logic/information are equivalent alternates in that they can be used to solve a problem independently of each other in best cases'
	        		- insight about the conceptual relevance of default/core structures
	        		    - 'direction corresponds to the intent interface'
	        		    - 'distance corresponds to the change interface and the similarity/difference interface'
	        		    - 'surrounding structures correspond to the system context interface'
	        		    - 'alternate possible structures correspond to the potential interface'
	        		    - 'angles correspond to the perspective interface'
	        		    - 'chainable functions correspond to the logic and function interface'
	        		- insight about how cross-interface structures are more useful
	        		    - 'concepts are only clear when you have an example (such as the variant of the concept in physical reality or in a particular problem) if you dont know the whole definition'
	        		    - 'concept-structure structures are more powerful than either on their own'
	        		- insight about how some variables are more powerful at more useful tasks (like explaining/describing/generating/determining) than others
	        		    - 'there are variables that are more powerful than others, like the general variable of cause, which is highly explanatory'
	        		    - 'there are equivalent alternates that are useful to know (explaining/describing/generating/determining intents, or find/build/derive intents)'
	        		    - 'cause is also good at these equivalent alternate intents'
	        		    - 'cause has an equivalent alternate in that logic can replace its value in these intents'
	        		- insight about how some structures are more generally useful across problems
	        		    - 'structures like rotations, similarities, and limits keep re-occurring across problem-solving methods'
	        		    - 'these structures are different types of objects like math objects, standard objects, and system objects'
	        		- insight about finding the 'most reduced set of useful/important structures'
	        		    - 'if you try to reduce language to the most useful structures, youll find structures like inconsistencies, perspectives, requirements, implications, overlaps, etc'
	        		    - 'the most variation-capturing variables of these useful structures are the bases where the others can exist, like differences/errors, structures, or functions'
	        		- insight about multiple perspectives and simlarities/differences being related to formats (which are like interfaces)
	        		    - 'different formats make different intents trivial'
	        		    - 'everything can be differentiated by changing perspective'
	        		    - 'a perspective is like a filter/standard/format'
	        		    - 'everything is similar to everything else'
	        		    - 'variables/functions are related to interfaces as they change within a defined limit'
	        		    - 'differences in similarities and similarities in differences make problems trivial (similar to the comparison insight above)'
	        		    - 'standardizing to the same format makes some problems trivial to solve, as it highlights meaningful differences'
	        		    - 'some standards/formats (logic/concepts) are useful structures (interfaces)'
	        		- insight about useful graph structures
	        		    - 'some graph structures are more powerful than others, such as networks, maps, trees, sequences, etc'
	        		    - 'the differences between these useful structures involve objects (variables/functions) in the graphs, connection functions (like similarity or interaction function), and the structure variables like direction (in causal networks, for example)'
	        		    - 'a graph of graphs (like the interface interface, or the meaning interface) is a useful structure to organize these variables'
                    - insight about how primary concepts are powerful 
                        - 'simplicity/complexity' corresponds to a primary 'difference-resolution or connection' function, so it corresponds to a primary 'problem-solving' function, as problems' default format is a 'difference to resolve', bc a problem can be solved by making it simpler or finding functions that in general simplify other variable interaction functions
                        - other attributes which can describe any structure also correspond to primary difference-resolution and problem-solving functions (like 'work', 'intent', 'potential', 'change', etc)
                        - for example, 'changing an existing solution to a similar problem' is a default problem-solving and difference-resolution function
                        - what other primary abstract concepts can be used to resolve differences (solve problems)?
                            - 'balance' can be used as a primary problem-solving function in the form of 'balancing extremes, as extremes often lead to errors like over-prioritization errors or scaled errors', which translates to a problem-solving function like 'balance the maximum differences in a problem to find a more general/stable format of the inputs which is likelier to be correct (a solution)' or 'find functions that are adjacent/interim to a high ratio of information'
                            - 'power' can be used as a primary problem-solving function in the form of 'finding functions that can reduce the work of all other functions, making these functions more powerful' or 'find functions that store high ratios of information'
                            - 'potential' can be used as a primary problem-solving function in the form of 'finding functions that can interact with the highest ratio of other functions, making these functions higher potential in the higher variability of their interactions' or 'find functions that have a high ratio of usage functions (can find/build/derive a high ratio of information)'
                            - 'intent' can be used as a primary problem-solving function in the form of 'finding functions that determine what other functions are used for (intents) and what theyre useful for (adjacently/optimally fulfilled intents)' or 'find functions that are more useful for a high ratio of information-related intents (like information storage)'
                            - 'cause' can be used as a primary problem-solving function in the form of 'finding functions that identify input/output (causal) sequences of connected variables (such as why a function is useful, as in the reasons the optimally fulfilled function intents develop to be common, such as the efficiency of using the functions and the commonness of the requirement of their outputs)' or 'find functions that cause a high ratio of information'
                            - 'abstraction' can be used as a primary problem-solving function in the form of 'finding functions that can store/embed most variables adjacently' or 'find functions that can support a high ratio of information'
                        - given how these concepts interact with information (can store a high ratio of information or otherwise are useful for information-related intents), the structure of an interface emerges
                        - the primary abstract concepts (power, balance, complexity, stability) offer good candidates for interfaces bc of the fact that they are high information/variation-storing and every structure has these attributes in some way, so they are a way to access different fields of connections (differences/similarities) to other structures

                - most of these have common variables, such as including 'known common useful structures', applying common useful intents like 'reduce/connect/compare', identifying structures that are relevant to 'problems', applying interface structures as a useful compression/explanation/other intent fulfillment structure, etc
                    - they differ in their method structure variables, such as how 'identifying the type of object that a useful structure is' differs from other insight sets in that it applies a bottom-up direction (generalize from specific examples) from a 'specific' starting point

            - identifying interface structures like 'efficiencies' that align with regression problem space structures such as 'symmetries' or 'densities' by relevant structures like 'causes' ('this point is a hub' meaning 'a lot of inputs created this output bc it was particularly efficient for something') and identifying functions to connect those interface structures with causes that can be mapped to other variable sets (other efficiencies can be generated and checked for, once the reason of 'efficiency' is known as a cause of some 'variable interaction structure'), meaning other relevant useful 'low-cost, high-reward uses of inputs' for known intents can be hypothesized and checked for in the data set to determine the prediction function, as well as 'other causes of efficiency such as energy limits, which could cause other structures related to energy like power imbalances, power concentrations/compoundings, power takeovers, power dispersions, power vacillations, etc'
                - this is useful bc structures like 'efficiencies' are related to 'symmetries' and other specific useful structures in the regression problem space like 'densities' in their usefulness for predicting/explaining/limiting change
            - identifying useful structures like 'questions to answer (like "where does change change") to solve different sub-problems of the problem (like "how to divide the data set into subsets which are likely to contain different change types/rates/degrees/etc")' which make the rest of the problem trivial (makes solving the other sub-problems like "find the most different subsets (which could possibly contradict a function for another subset)" trivial) and identify the sub-problems they make trivial, and filter these by which subsets of sub-problems can replace other subsets or the whole set for some metric like 'finding an approximation of the solution'
            - identifying useful structures like 'adjacencies' as a way to determine if alternate variants of the data set are more obviously compliant with a pattern, as 'adjacent transforms' of a data set are likelier to be valid than other transforms, and some of the adjacent transforms may have more obvious patterns/averages/densities/other useful structures than other types of transforms
                - similarly, identifying useful structures like 'repetitions' to apply to relevant structures like 'subsets' (specific sets) to generate structures (like 'repeated change types across different local subsets') which are more relevant to a prediction function (the complete general set), or 'differences' to relevant error structures like 'non-local subset change types' (which are likelier to be less relevant to the prediction function, as in inaccurate, than local subset change types and therefore applying changes to these errors structures is useful to generate the actual prediction function)
                - relatedly, a 'mix of simple/complex transforms' applied to a base function to balance the extremes of the dichotomies that are useful/determining of the problem space and allow resolution of them by testing small differences favoring either priority may be useful as a testing/diversification structure to hedge bets in the prediction function, as well as embedding other variables reflecting other dichotomies (certainty/uncertainty, specific/general, discrete/continuous, adjacent/extreme), which is similar to retaining multiple different equivalent alternate functions which are ambiguously correct as they fulfill solution metrics similarly
                    - organizing these differences by 'which are likely to be adjacent' & related metrics is a useful way to filter out functions that are exactly obviously incorrect (by placing a constant in exactly the maximally wrong subset, for example)
                - similarly, if there is a subset of input variables that can be used to create the exact opposite (or similarly different variants) of the found prediction function indicating that the variables can be easily made to contradict each other, the symmetry uniting them is likelier to be more true than either the found or the opposite function
            - identifying useful structures like thresholds to filter possible alternate interface variables of a function, like determining the function up to a local subset size that its possible to determine its change types, like whether its exponential or linear, as the meaning of a subset emerges and is more obvious, the greater the number of points in the subset (subset ratio), where the meaning of a point is difficult to determine in isolation of other points
            - identifying the primary interaction functions between subset structures, such as how a subset might represent a fraction such as a quarter of a data set shape, so it should be reflected across two symmetries to create the rest of the data set, or a subset might be an 'orbit' or 'boundary' of the rest of the data set, so checking if these interaction functions apply to the rest of the data set is useful and identifying these interaction functions is useful
        	- identifying useful structures to filter out possible errors like 'randomness in the data set' by checking for 'associated structures of randomness (when defined in that problem space)' like plus/x shapes of lines with equivalent/similar numbers of intersecting points with the data set, or ambiguity shapes like squares (which could represent any line with positive/negative or constant/zero slope with equal probability) or similar areas reflecting randomness when found to describe the whole/most of the data set, or look for indicators of usefully biased shapes like rectangles/circles which dont represent completely equal/ambiguous change
        	    - similarly, identifying the set of 'lines with equivalent/similar intersecting points, optionally using lines as connections between adjacent points in the set (to add simplicity as a proxy of generality)' as indicators of equally probable solution functions or 'lines which skip the fewest points to form a straight or otherwise simple line' are useful structures to use as generators of possible solution functions, similar to other generative strategies like applying variable changes like 'embeddings on other variables' to generate possible maximally different functions, just like how 'any line crossing the data set' is similarly useful as a 'probable line-of-best-fit' to check errors and determine shape/slope of the correct function by assembling 'adjacent errors (to form shapes like lines/curves/boundaries)' and 'error changes like sign changes or phase shifts' into the solution function shape
        	    - randomness is particularly important to identify bc structures that exist are unlikely to be random (equivalent alternates that seem ambiguously similar usually resolve into favoring one or merging or differentiate further and become obviously different in equivalently useful ways, if the host system continues to exist bc multiple equivalent alternates are redundant, high-cost to maintain, and therefore less likely to occur & continue to exist), so determining inputs/components of non-random structures (such as biases, or simple rules) produces a set of 'probable structures to exist'
            - identifying useful dichotomies like 'parabola or line' is particularly useful as the core question to answer in the regression problem space, to generate algorithms such as 'connect the outputs at the lowest/highest x-values, and check midpoints in between the lowest/highest x-values to check for an error indicating a parabola', as its more important to identify when there is a parabola/wave vs. a straight line than to identify any other structure except more core unit structures like averages/extremes/inflection points and errors like 'gaps in data' and other structures resulting from core structures like extremes (such as limits/infinities)
               - the inputs to this algorithm are the x-range extremes (lowest/highest x-values) as well as the midpoints in between which would be useful to check for parabola-type errors at (the selection function of subsets)
               - identifying trends in error structures (like if the error is always 1, the function should probably be shifted up/down by 1, or the program is only checking values at a wave peak/valley where the magnitude is 1 while identifying the horizontal line crossing the wave at its midpont) is useful for identifying corrections to try early on, and identifying how to correct them such as by 'evaluating values at different intervals rather than at the same interval' and 'identifying the simplest line that intersects with the data set the most' to avoid this error, are similarly useful to reduce the probability of repeating that error
               - as another example, if you have a horizontal s-curve (one up and one down peak), identifying that is as simple as identifying that three other pieces of information are required, and identifying those three of the y-values at regular x-intervals (or the two/three determining points of the peaks/inflections), at which point the s-curve will be obvious if curvature is used to connect these points and your interval function identified the extremes of the peaks, if the program specifically checks for that type of curve as a common curve type (the skew/concavity or the squareness vs. linearity of the curves is another useful variable to determine as an important specifiying variable, the squareness indicating less likely change types and the curvature/linearity indicating more probable change types), so an algorithm to identify the variable set of 'curve peaks, x-ranges, and squareness/skewness/volatility at some subset of the function' is likely to produce a generally accurate function, on its own regardless of other variable sets, which can be enhanced by other known variables capable of producing extreme errors such as 'threshold phase change' variables which can make a function act like a totally different function in some continuous subset
            - identifying useful sequences of interface structures like a 'maximal difference connecting line indicating the connection between the most different points' and a 'inflection-point intersecting line indicating a smaller direction of change' which can provide a simple set of linear functions to base changes on to find the regression line quicker, which in a curve with exponent 3 (which starts lower, has an upward parabola, then a downward parabola, then increase indefinitely) would indicate the primary deviation from the primary summary of the change patterns (a line with positive slope connecting the low initial points and the high later points as the primary summarizing line, specified by an interim line with negative slope crossing the inflection point to indicate the primary deviation from that primary summarizing line, which can be further specified by tangents indicating extremes or alternately limits of vertical change)
            - identifying useful sequences of filters to reduce regression solution sets by useful trivially calculatable structures like change type variables, such as a sequence like 'check for a parabola, which implies an exponent, then given that an exponent implies other powers could exist like even/odd powers which determine maximal differences in the resulting function, check if powers are even/odd, then proceed to other variables to determine the rest of the function' which starts with an easily found structure and proceeds to other structures made possible/probable by that structure
                - this is related to other workflows like 'check for a change type, then infer other change types based on that, given change type interactions' but involves finding alternate sequences which are maximally differentiating/filtering or otherwise useful
                - these sequences' usefulness is maximized in cases like 'when the first item in the sequence is an interface variable that can support embeddings of other variables (like a unit exponent variable or other variable structure to differentiate function types) and the following variables are embedded in that interface, so the filters begin with the maximal differentiating filter and then decompose the remaining variation on that interface'
        	- identifying optimal algorithms involves finding useful (such as 'measurable') variables with 'obvious errors/optimals', such as how an algorithm in between simplicity/complexity is likelier to be optimal than an algorithm that is at either extreme (this is useful for filtering all possible algorithms to reduce the search space)
        	    - similarly, algorithms that involve some common useful interface structures (such as symmetries, which are like information wormholes, as well as limits and maximal differences) are likelier to be useful (and true and relevant) than other algorithms
        	- identifying variable interaction functions that could not be true given some system of reality where those functions could exist that is not true, like identifying that a 'wishing reality system' is not an accurate model of reality by identifying that 'agents wish for problems to be easy to solve (they wish for types of freedom)' and 'problems agents have are not usually easy to solve' and 'some common wishes of agents contradict other wishes of their own and of other agents, and would also contradict a wishing reality system for other agents' and therefore a 'wishing' variable interaction to connect problems/solutions is not a valid problem-solving function applied absolutely, which is useful to rule out variable interaction functions to find variable interaction functions that are possible/legitimate as a way of finding a reality system model, though finding states that move in the direction of that system is possible using realistic rules (increasing agent intelligence makes problems easier to solve and reduces agents' contradictory wishes against other agents, which is a solution of removing the intents that make the 'wishing reality system' impossible to logically sustain)
            - similarly, identifying high-variation explanatory variables like 'incentives' and 'interaction levels' from a typical data set where structures like 'defaults' and 'input/output similarities, differences, and requirements enabling interactions' (as more probable than other variable values) and 'adjacencies' (in casual degree) and 'efficiencies' (in benefit/cost ratio and stability) and 'interactivities' (as default interactions between variables) and 'types' (as efficient captors of information) appear more common than others, 'incentives' being a common factor in these common variables, as these structures are incentivized compared to other structures that may be more complex/difficult in some way, so simple queries like 'find common high-variation variables in common high-variation variables to identify other common high-variation variables' are useful in typical problems like regression, as if a common variable seems high cost its likely that we just havent identified the incentive yet, as the incentive is the determining variable more often than not, just like randomness is not usually real equivalence of probability in outcomes, but is likelier to just be lack of information about variable interactions that makes a variable interaction falsely seem random
                - for example, the dynamic between 'filters (as measurement/input-selection tools), as a core structure of differentiating functions' and 'incentives to distort the definition/structure of the filter to exploitatively avoid/subvert it or otherwise exploit it' is a highly explanatory interaction, where once a filter is applied, the incentive is to apply differences to game the filter (either to become a false/illegitimate input, and/or to expand its input range, or to use the functionality guarded by it without having the filter applied to it), and once the filter function has changed enough to handle these predictable adjacent incentivized differences, the opportunity to exploit differences to the filter is closed and differences are applied elsewhere, where these 'differences applied to the filter to handle adjacent differences/errors used to exploit it' is a 'common useful change sequence' to be able to re-use across filters, as the filter structure usually is applied too simplistically initially and must usually be changed to handle obvious exploits, 'filters' being a variant of 'standards' and are therefore useful in core intents like 'finding bases/limits of adjacent similarities/maximal differences'
                    - usually the filter follows patterns of errors, such as being too simple, too specific, too rigid, too structural, etc - so that it benefits from common useful optimizations like 'generalization' (as well as fulfilling useful 'core optimization intents' like 'increasing interactivity with other structures that dont adjacently cause errors', etc), an 'optimization change sequence' that can be matched to an initial filter by its probable error metadata (simplicity, specificity, rigidity, structurality, etc)
                - this can be applied to algorithms in general, such as for example, 'abstracting an input filter' in the 'find a regression function' problem space, to "find points belonging to the same type and connecting those points in a 'type function' to differentiate them from other points" or 'find maximally different points that should be connected in the same function, as they are legitimate and/or representative points'
                - these common distortions of a filter definition can be anticipated in advance so the solutions to these exploits are built-in to the filter definition, rather than applying the 'optimization change sequence' after errors are exploited
                    - a useful question to identify these change sequences: 'what is normally found implemented as a filter in typical systems?' for example, a 'domain/topic/type sub-type filter' such as a 'specific sub-type of object within a type with different rules that should be applied to it', so that a 'filter to find items of this type' is applied, often with a hard-coded function/dict to identify those items, and given the value of differences created by those different rules, other items will try to seem like those items, in predictable adjacent incentivized ways
                    - what other useful functions are commonly implemented? 'functions to correct distortions/errors beyond some threshold used as a filter of acceptable differences', 'functions to batch or aggregate items having some attribute', 'functions to connect some different objects using maps/functions/filters', etc - all of which can be implemented using some filter function, which is why this structure maps to a core interaction function 'find'
                    - a useful application of this would be to 'find new differences to apply filters of in a system, where these filters would be more useful to differentiate some structures and arent already used in a system, to optimize the system'
                - as some algorithms/queries are applied, other interface structure become obviously useful, such as 'similarities which are not adjacent' and 'changes that are neither obviously adjacent similarities or maximal differences' or 'interim differences between maximal differences' (useful as variants of maximal differences to test more different possibilities that are more similar to some interim base) or 'similarities between maximal differences', as 'adjacent similarities' and 'maximal differences' are obviously useful, so connecting these structures using related structures (related by the same base structure of 'similarities/differences') and finding useful variants of these structures and useful intents these structures are likely to adjacently fulfill (such as 'more complex changes which are less obvious and likelier to describe complex systems as well as the differences between various standards/similarities/symmetries that describe a high ratio of variable interactions') and standards/symmetries within these structures ('similarities between maximal differences' to identify input/generator/descriptor/limit variables of these structures) is useful while applying differences to create new structures (which should be abstracted/fit into other interfaces to check for new types of useful structures)
            - identifying the structures like 'shapes that when overlapped and rotated and viewed from a distance look like more common shapes like circles' as useful for describing reality in that they cover and explain more variation than other structures (this structure has a structure embedded in it that indicates 'equivalent alternates (having an equivalence in their center & a similarity in their rotation)', the 'incompleteness of any one alternate in this set when viewed in isolation', the 'usefulness of general trends once a process is repeated/scaled', the 'balance inherent to multiple equivalent alternate perspectives', the 'ever-changing nature of foundational structures leading to changes in interaction levels', and other fundamental structures that are descriptive if not generative of reality)
                - relatedly, viewing reality as a 'set of repeating processes applied to partial overlapping/embedded structures at varying intervals/magnitudes/scopes' may be more useful than some 'networks with unique nodes or just functions as nodes/queries or just individual usages as nodes/queries' bc the repeatability models useful interactions like 'aggregations at scale', 'net/emergent effects', 'in/stabilities', and other important structures created by the interactions of real systems, so this model of reality may simulate a more useful interaction level than abstractions like function networks tend to capture
                    - similarly, 'similarities (like patterns) in "differences from incentives"' and 'similarities in entropy/uncertainty/potential reduction structures' and 'structures of differences (such as attributes like unfulfilled/impossible/required intents) between differences/variables and difference-connection/solution structures' are related useful structures that captures high variation and formats differences in a minimally complex structure
                    - a network/field of these sets of useful structures that describe high variation in reality is useful as an alternative to a set of useful interface structures which can generate these adjacently
                - relatedly, viewing reality as a set of 'differences from required limits (impossibilities)' or alternately 'commonalities (similarities to probabilities)' (produced by adjacent/input/causative structures of commonalities like scale/aggregation/repetition/isolation/efficiency/incentives/investments/adjacencies/examples/usages as ways to produce commonalities) and differences from commonalities (like those that occur when "previously isolated commonalities interact after repeating enough to reach each other's position") can be more useful in its specificity as different from other useful formats like just any high ratio of all combinations of 'similarities/differences', this specificity being useful in the certainty it provides to base changes on
            - identifying the structures like combinations/ratios/networks/interaction levels of optimals, stabilities, requirements, errors, and other interface structures that occur in reality is important for solving the 'find a regression function' problem, as there will inevitably be something that a real system 'optimizes for incompletely', something that a real system 'should do but doesnt given this priority/requirement/input/opportunity', something that 'stabilized after a particular error type', and other combinations of interface structures like 'conditions' and 'causes' and 'optimals' (such as rules like 'apply high variation or high potential-variation variables first to decompose high variation'), which can explain most variable interactions but are not default/core structures already identified/required (these structures are more complex variants resulting from more interactions), which are obviously useful despite their complexity, as more useful to retain and use as defaults that re-generating them from core components every time, similar to how solution automation workflows can be more useful to retain than re-generate
                - these are useful for determining when a data set (or the system it reflects or the parameters of the analysis process like definitions) is incomplete/incorrect or otherwise suboptimal/erroneous in some way
                - relatedly, structures like 'monodromies' can be useful to apply existing math structures to indicate which points/lines/other structures that could represent 'limits which should not or need not be crossed' (which differences should not be resolved, such as where an ai program might 'resolve a difference' of a medical problem which is a 'state of difference from health' by allowing patients to die to create a 'difference from the requirement for health/health resolution to invalidate the problem' which is the difference to avoid except in extreme circumstances like where a 'health generator is adjacent, invaliding the requirement for health' and otherwise resolving unnecessary differences like 'resolving a difference that is already a known constant' or 'resolving a simple difference that is obvious' and 'resolving a difference that leads to an extreme such as hyperbolic change which is unhandled')
                - relatedly, retaining structures that are more useful to make constant (such as where its more useful to retain a map of inputs/outputs than to find a connecting function, like where extremely high variation is observed/possible and not reducible or where reduction is contradictory to other intents) and use as input defaults than to re-generate is a way of identifying the stable structures of reality that should not usually be changed/tested but rather applied as default inputs, combined in increasingly complex ways to describe more variation, rather than replaced with more descriptive variables, as these defaults are already the best at description
        	- identifying patterns/limits/useful representations (like areas)/other interface structures of useful 'false similarities' of functions (at some point, subset, in relation to some threshold, or range), such as when '(x^2) + 1' depicted as an area will look like a rectangle (when x is near 1) and when it will look like a square (as x approaches infinity), and identifying the point where these 'false similarities of areas' change (when x is sufficiently large that 1 looks trivial by comparison), and whether other changes are possible/defined/likely (whether those 'trivializing' changes will apply again at some point in that direction) or whether the pattern will continue, to identify 'different change types possible with a function', which is useful for determining the remaining sets of a function given a determined set of input/output relationships and in filtering functions that are equivalent or seem equivalent in some subsets but are not
        	    - similarly, finding 'standardizing' functions like to remove 'obviously non-impactful variables' such as the 'shift to move intercept value at y-axis to y = 0' that make the change type patterns of x^2 more obvious (the inevitability of the 'increase in area created by x' being exponential being more obvious once the 1 constant additive is removed as the impact of adding 1 at higher x values is more trivial which obscures the otherwise obviously exponential area increase, and the addition of 1 being increasingly trivial in changing this exponential increase in an invalidating direction)
        	    - similarly, other 'variable combinations with obvious/predictable impacts' exists like how a 'sum of constantly increasing and constantly decreasing variables' could easily be a horizontal line depending on their ratio
        	    - depicting these 'possible variable interactions' in a way that their commonalities are depictable as common points/overlaps or other obviously similar structures (such as combinations of inputs having an attribute in common crossing the same line/point or having the same shape or area) is useful for filtering probable interactions that are relevant to some other interaction (like an input/output interaction)
        	- similarly, finding a subset of points that represents an average/midpoint of some other set of points can replace the calculations required to connect the set of endpoints and instead just using the midpoint is acceptable as an approximation algorithm of the others, if enough different points are preserved to retain the general shape of the data set, just like 'removing extremely similar points' as 'redundancies' (but not redundant in all cases, as in the case of points around a density center, which preserve the weight of the density given its surrounding adjacent points) and removing 'removing non-adjacent (but non-maximally different) points connectible with local constant functions' as 'improbabilities' is another way to reduce the set of points required to connect in the same function
        	- finding useful formats like the 'set of angles/areas creating the components/features/products created by an input' which are summed to generate the output and which can be easily checked as un/applicable to other inputs by changing the angle of perspective in viewing these angles/areas so the impact of component areas/coefficients is obvious (viewing the interim products of operations in between x and y as a stacked set of areas, angles referring to the differences between area upper/lower limits compared to input x), rather than repeating the multiplication/sum operations on the other input, which is related to the format of a 'network of areas (representing products of variable pairs/sets) that is separated and aligned to make the output (sum) more obvious to avoid re-computing it', these formats being useful for approximating/prediction/avoiding computations by identifying similarities in component metadata like ratios of areas, and identifying obviously wrong 'product component sums', similar to filters like types that determine 'information about what else is also true' from a 'set of input facts'
        	    - this format is maximally useful when the interim components are few & large or otherwise obviously different, 'either very large or small' and 'very few' (meaning more adjacent to the final output)
        	    - once probable 'large components' (or otherwise simplified components) are identified as probable, filtering the sets of possible smaller input components of these components to resolve the ambiguity of 'which smaller inputs created these larger outputs' is a reduced problem compared to 'filter all possible solution functions' and possibly also 'apply incremental changes in a direction that reduces error' in some cases, which can be repeated up to the point where additional component-resolution is trivial (differences added by further component identification are trivial, or equivalent to other differences created by some other component set or simple layer like some function of randomness)
            - similarly, the 'equivalences in differences' from a particular 'possible solution function' are useful to identify 'probably useful summary functions', as functions that produce 'equivalent differences' are likelier to be more adjacent to the actual solution function (transformable to the solution function by some trivial transform to produce these equivalent differences, like a shift or scale change)
                - relatedly, finding these useful standardization functions that produce an 'equivalence in differences' from one function to another is useful to find these functions which have equivalent differences
                - relatedly, the '(patterns and other interface structures of) differences in functions that have equivalences at various subsets' are useful to identify, as having a common interface defined by their equivalent subsets which vary adjacently by some variables that generate the different functions united by those subsets, which make the problem of 'selecting between equivalent alternates such as ambiguous solution functions' more trivial
        	- identifying the 'maximally different subsets' that will stay under an error threshold for the same given general solution function is useful to solve for and apply once the 'maximally different subsets' of a data set are identified to filter the possible solution functions by taking a subset
        		- similarly finding the probability of a particular solution function by comparing the ratios of coverage of these 'different subsets, which are equivalent in their error range of a general solution function' (which function corresponds to a higher proportion of maximally different subsets under a minimum error range, this proportion representing an approximation of the probability of that function being the solution function)
        	- checking for set of 'points and directions' within a horizontal slice of a probable range (where the densities are, indicating where the solution function probably is within that range) is more trivial than checking for inputs corresponding to any outputs, allowing a method to skip points corresponding to outputs outside of that slice, as a subset of 'points with adjacent directions of change' is an alternate format of the solution function that can be used to determine the rest of the function, as finding the actual points in a horizontal slice of the data set and adjacent directions of change moving away from those points captures the value of a subset of determining points (in a high-variation slice of a probable range area), as an alternative to the usual determining points like extremes/inflections/averages, which are further determining of the rest of the function when paired with adjacent change directions, if the slice is in a high-variation section so that most variation of the function is captured in that slice (the same can be said for any horizontal slice of the data set but moving it to within the probable area range of the solution function and specifically to a high-variation subset makes it more useful, and pairing points with adjacent change directions increases their determinability of the rest of the solution function), so 'finding the most useful position of the slice' is the problem to solve, in addition to 'finding the point/adjacent direction pairs that determine a function, instead of the usual determining points/metadata'
            - identifying useful structures like different formats to handle cases where it's not possible to select between alternate solution functions, such as where a range of probable area is too large and functions within that range are equally or similarly possible, in which case every possible function in that range is more useful to format as a node on a function network, as every connection can be distorted to be every other connection type by applying some variable or error, these variables or errors being possible to apply generally across systems, so any data set could be the product of an error applied to some correct function and could therefore require some distortion to correct, so a set of weights indicating which function is likelier in a function network is more useful than a single function, or alternately formatted as a function generating a set of 'certainties' like angles (indicating a probably correct point, after which there is a divergence creating a probable area), and 'randomnesses' acting like 'ambiguities' where many possible functions exist
                - this is related to other methods involving handling alternate functions but applies the insight that 'every possible connection function between points could be valid and could be altered by excluded/hidden variables its likely to encounter until it becomes every other function, and having a function network where identifying the node where a function is gives useful information about adjacent/probable alternate functions, which is useful for identifying the probability of the function becoming other functions and the probability its an incorrect function, given some known correct input variables, this function network organized by probability of becoming adjacent functions'
            - identifying patterns of error feedback indicating obvious solutions to correct the errors is useful, such as where the errors of a function that is incorrect bc its shifted to another position follow obvious error patterns that can be easily identified and corrected
            - identifying points that, when some simple connection function is applied, can generate the most other points is useful to identify as simpler connections are likelier to be true (if the whole data set can be reduced to a set of center points and circular border points around these centers, these patterns are useful to identify as being significant in their repetition, structure, simplicity, and difference from simpler/core functions like lines, even if the effect of these overlapping circles seems like randomness or a linear function in some subset)
        	- identifying useful components to describe data set subsets like adjacent point sets such as 'filters/bottlenecks/convergences (connect only different surrounding directions) and random/circles (connect points in all surrounding directions) and extremes/limits/peaks (connect similar but opposite surrounding structures) as default structures of adjacent point connections to use as components'
        	- 'filtering structures similar to outputs to find the outputs' is a useful function such as how its useful to know a set of the most unique (maximally different) functions to filter maximal differences, and its useful to know the 'most similar function of the set of similar functions' to filter similar functions bc the representative/average of these functions is more similar to each item in the set and is likely to be relevant to other items in the set
        	- identifying new densities from a known density is often trivial bc identifying the midpoint (or other representation like density) of a one-dimensional set of points in some direction to discover densities in (from the perspective of the known density looking in that direction) is trivial compared to the task of graphing all points and finding all densities, where finding 'trivial to identify' densities is more efficient and can benefit from some transforms like angle of evaluation changing the problem space of 'filtering all points past this x-value' into a one-dimensional set (comprised of a subset of points in that direction) to identify a 'direction to move in', finding local averages in many subsets, densities being likelier to be less common and therefore more trivial to identify than the local subsets used to calculate local averages to select/create points to connect, and structures like 'equally distributed points in some direction which would make the task of identifying a density or average trivial' is less likely to occur in a realistic data set and calculations can be skipped in those cases
        	- identifying what point interactions (like high volatility, extremes in ranges, high slopes) can indicate possible sources of high variation, which are the most important points to identify, to identify whether an average line (or base solution regression line) needs to be adjusted, so that identifying these 'high variation-causing points' can be an approximation or alternate of a regression line-finding method
        	    - similarly, identifying which regression lines correspond to 'what types/sets/ratio/other metadata of points indicate the points that must be ignored in order to make the regression lines seem accurate', like how 'change rate changes' and 'difference from base function slope sign changes (positive/negative difference from base function slope)' have to be ignored to make a constant line seem accurate, etc, so that checking for these types of points once a base solution function is found which might contradict it is trivial
            - identify what variable interactions look like with various error type structures like combinations and filters of these possibilities to apply tests to input variables or gather more input info (expanding the workflow loop to include changes to test/data gathering variables)
            - identify at what point inferring that a 'more complex function with more peaks exists after how many negative indications indicating the opposite' is the wrong inference and can be contradicted 
            - identify filters to select/switch between function formats like 'probable function range areas' (useful when an area seems more random) or 'average functions with surrounding error vectors' (useful when an error or legitimate function range seems too ambiguous to resolve with current info) based on different complexity structures and cases in data sets
            - applying variables to create randomness from a non-random data set (or similarly linearity from a non-linear data set) and identifying whether some randomness/linearity-resolution functions also use those connections required to create it (if some system dynamic like 'system collisions or overlaps' is a randomness-resolution function and a collision/overlap variable created randomness, do the systems explain the original data set before the collision/overlap)
            - identifying error/legitimacy resolution structures (at what point does an outlier seem like a trend-change predicter rather than an error)
            - identifying gaps in uncertainty/certainty, randomness/linearity, difference/similarity, complexity/simplicity, and error/legitimacy spectrum variables that cant decompose some change type adjacently and the spectrums which can resolve them using some resolution structure (or the other structures using these spectrums which havent been identified yet)
            - identifying the most useful tests to apply (like 'a method that can identify new inventions as well as non-adjacent high-variation variables or sources of randomness in systems like "high-distance high-randomness variables like neutrinos" and "powerful/change-triggering variables like incentives as explaining most variable interactions"' as well as reversing the function to 'find system structures created with incentives/randomness' or 'a method that adjacently identifies useful interface structures from highly different as in non-interface structures') to check if a solution-finding method is successful at which point different solution-finding methods can be generated combinatorially and tested iteratively
            - identifying similarity structures between subsets of the data set (similarities such as 'similarity in intersection with some line type having some attribute', "similarity in a point's difference types from neighbors") which explain the data set the best, as points which are similarizable by these attributes are likelier to be related in that way in the system producing the data set, these similarities serving as inputs to probability of relatedness and therefore can be usable as a filter of points to incorporate in some algorithm to determine a representative line, compared to some difference set to generate a composite ratio of relevant similarity types compared to difference types
            - identifying inevitabilities of structures and the related structures like points/ratios/distances required to identify them (such as two slopes on either side of a peak making the peak inevitable, given the input variable interactions implied by the slopes and given the distance between them and the existence of other peaks already identified)
                - identifying evocative structures (which are similar enough to other structures to be useful in deriving them)
            - identifying relevant truths like how the 'extremes/borders/upper/lower limits of the probable area range' might be a better representative format of the function than a line bc different behavior at a higher vs. a lower value is a realistic possibility that occurs in real systems, or the 'extremes with another structure' (like an average or a probability distribution of a point being in a particular sub-area, as in a different probability distribution for each 'x-value' or 'local x-value subset')
            - identifying useful structures to add/remove such as 'areas of randomness' such as a cube of evenly distributed points, which its possible to filter into possible solution functions to connect it with more certain lines around it but is also possible to remove as a structure of randomness that indicates neither priority so can be removed as possible noise and added back in when new information might help filter the possible solution functions, in cases such as where the more certain lines around the random structure are equivalent and indicate none of the solutions in the random area as more probable
            - identifying useful alternatives (like whether to 'apply multiple average methods and merge the average outputs (like an ensemble of networks)', or 'whether there is room for improvement in the average methods and finding new variants of them given their variables is worth pursuing (like a function of method errors where a more optimal point is implied on the error curve)') in existing methods by applying interface structures like 'input-output sequences' applied to 'average methods' as an important component of the regression problem space
        	- identifying unit structures to apply as components of a data set range (such as a data set density or probable function range) include units like 'overlapping circles which are likelier to describe a probable function range of a typical data set which is usually an area rather than a clear line', 'semi-circles shifted around a linear average line', 'shapes that indicate exponential change and also some non-trivial area like curved rectangles or circles as tiles of the probable function range' to indicate variable interaction structures that could represent the legitimate variable interactions mixed with some error likely in a system of some probable complexity (local change tiles which are likely to be simple shapes distorted by errors in some degree/way), where a 'probable function area as opposed to a probable function line' indicates either a complex system, a system of maximally different or locally representative alternates indicated by edges/corners or local averages of these shapes where the interim points are errors, a lack of complete input variables, an error like randomness injected in the input data set, where a better (simpler, clearer & more accurate) representation of the data set would be a previous variable set on a prior causal node where the differences creating the area are graphed as vectors and the variable interaction can be graphed with a line
        	- just like some structures are useful in their simplifying and explanatory effect (like 'core components' leading to outer 'interaction levels' supporting 'maximal differences (like a function network but in every direction)' where 'interactions are adjacent between nodes'), other structures that combine important useful interface structures like 'core components' and 'interaction levels' to achieve useful intents like 'adjacent/linear interactions' are similarly useful, and can enhance the usefulness of these structures, such as 'cross interaction level errors' like what errors can happen when an output layer interacts with the core input layer, or a way to organize the structure so that frequently interactive functions are adjacent even across interaction levels
        	- identifying 'sequences of change sets' that are commonly seen across systems, to identify common useful functions found in sequences like a 'creativity/generation' step, a 'maximally different/uniqueness' step, a 'standardization/grouping' step, an 'abstraction' step, a 'contradiction/neutralization' step, a 'incentives/efficiency' step, a 'stress/competition' step (like to see which variables stay constant and which can maximize their variation), an 'explanatory/decomposition' step, a 'understanding/organization' step, a 'ambiguity or other error generation/identification' step, a 'randomization' step, in a way that reflects real change patterns, and applied to connect known certainties (probably certain trends in the data set) and uncertainties (the remaining variation), these 'sequences of change sets' being more useful than other function formats to identify commonly repeated useful functions, as well as other patterns in change sets like cycles and equivalent alternate change sets, some of which are simpler/more useful than the original connection between original inputs/outputs, and to identify different useful function formats like 'sequences of vector structures & maps' applied to inputs (which move in the direction of function networks and allow queries like 'which are the most commonly useful function networks' and 'which function network compresses all other function networks adjacently'), as structures of 'useful incompleteness' created by mixing cross-interface structures that leave irrelevant variation unhandled and are adjacently transformed into useful variation-capturing functions
        	- condensing variables as 'some or any change in this variable set of the same type (rather than specific changes in each variant of the type)' is also useful to quickly identify sources of variation on a different interaction level
        	- identifying structures like areas/slopes of maximum volatility (adjacent input-extreme output connections) and connecting them to areas of constance as a way of identifying the maximum volatility allowed by a filtered set of functions in a more constant subset of the data set, as connecting the filtered set of a function in a more determinable subset and the volatility allowed by that filtered set can determine other local subsets of the data set
        	- identifying useful similar structures like overlaps/convergences as indicators of similar but different types of change (overlaps being an indicator of robustness or probability, and convergence being an indicator of some limit structure or an average structure)
        	- given that embedded variables (meaning 'embedded on interface variables') are likelier to create 'maximal differences', testing those as a default filter to describe complex data sets that can easily look random in some subset is useful in this problem space
        	- given that there may be a transform of the data set (like a subset of input variables) may be a more efficient way to generate the probable data sets found in real life (as in, there is a subset of input variables around which most of the variation is clear and easily explained by more common functions, indicating the original input variables include random noise), its useful to identify subsets of variables with simpler variation as complex systems are less likely to be stable and are likelier to contain random noise from variance injections, similar to how identifying simple structures like 'evenly spaced repeated data set subsets' and 'densities' can identify common patterns that are simpler to model as a pattern or a component of the solution function if they represent a sufficient ratio of the data set
        	    - relatedly to the workflow involving removing simple structures, removing some subset of the 'points' that are connectible in 'straight lines' in a data set is useful to remove structures too simple to be useful (in the sense of relevance for the intent of capturing information beyond a general simple summary base function to use in finding the actual solution) in describing a more complex data set as is more commonly found than a simple linear data set
        	- some structures are more probable than others 'a range of data points around a pattern (as in an area of probable range of a function) which describes likelier structures like errors' is likelier than a 'highly variable specific function (like a function with many peaks)', so these likelier structures can be applied as more default than other structures
        	- function sets which are adjacently transformed into each other which have some base of similarity in common (like accuracy, variation, input/output patterns, or input variable subset ratio) are likelier to be equivalent alternates and probable solution function sets than other function sets
        	- 'maximally different data sets with known error types applied, mapped to solution functions' are another useful starting point for reducing the regression problem
        	- variables like 'dependence/connections between inputs' (such as a 'sequence using the previous adjacent input as an input', as opposed to a 'function of independent variables') that create dependent/independent variables are also useful to identify, as these variable differences are important to filter out, where independence of variables is not obvious but a sequence is easily detected
        	- identifying structures (like isolatable structures, including obvious components, such as 'anomalies') which are likely to explain some variation interaction patterns (like an 'occasional extra peak disrupting a more probable pattern') as common error structures to remove to find simpler functions and add to find error-handling functions or erroneous versions of data sets, similar to how modeling a 'system cascading to destruction' is useful to identify signals of these error structures in data sets, just like identifying 'probable new more stable states' of a data set is also useful
        	- increasing the 'directness' or 'adjacency' of variable interactions as being more explanatory, more likely to be linear/simple, and less likely to be subject to undetectable noise/errors is a useful intent rather than trying to identify distant connections between indirectly causally linked variables, as indirect connections are likelier to change (but also likelier to re-occur when removed so extremely distant effects are still useful to model)
        	- similarly, given how the problem of 'finding a function to summarize a data set' is a problem of 'finding missing variables (coefficients, powers, etc) that are not in the input', other formats of a problem are similarly useful to apply as defaults (what are the patterns of missing variables, are they more difficult to detect, are they less visible using data gathering techniques, do they have similar complexity like 'host dna' & 'pathogen dna' which allows inferring the 'existence of dna and pathogens in a host', and 'default function errors (like immune errors)'), similar formats like 'stable variables' to identify variables that are more stable than other variables so they will become 'high-variation'-causing variables, a useful structure to identify with 'high-variation variables'
        	    - for a more complex specific example, identifying people from their dna involves identifying 'missing variables' (like phenotype-determining genes, as well as 'genetic change variables' such as epigenetic changes, errors in dna-phenotype mappings, mutations, disorders in dna editing, etc and causes of these 'genetic change' variables) and filters of these missing variables (tests of these variables to help filter them or legitimate stable differences to filter/separate these), which are variables in between the inputs/outputs in different interaction levels than the inputs/outputs like the 'gene function' interaction level
        	    - similarly, the machine-learning problem involves finding 'missing in-between variables' (like a 'function to create differences (to find variations of structures like combinations of inputs)' and a 'function to attribute/connect useful differences to filter out (like connecting an error difference to a node or node structure)' and a 'function to calculate differences from correct values (calculate the error/loss from a particular input variation combination)')
        	    - these are highly complex sets of variables (input/output subset filters, high-variation change inputs, 'instruction' functions generating similarities, variable interaction functions, generative functions, difference-filters) which fits with highly complex systems, decomposing them to slightly less complex variables on different interaction levels, which can help with identifying missing variables (such as inferring 'alcohol' as a cause of 'genetic changes' when other complex sub-systems arent sufficient to decompose all variation in a data set, as a source of variation from another interactive complex system, given how complex systems usually are created by interactions between multiple complex sub-systems)
        	    - variable interaction functions like 'add/subtract/neutralize', 'limits/extremes', 'filter/differentiate', 'repeat (like as a default)', and 'interfaces/averages' are common (or even required) across systems so are likelier than other variables/functions to re-occur in unknown systems, which makes them useful to apply as default variable interaction structures
        	- similarly, applying known 'contradictory cases' of 'when known rules are wrong' such as when a more general function is wrong compared to a less general but more accurate function despite a common solution metric like generality
    	- similarly, identifying the 'core shape of a local subset representing a change unit of the more general function' and 'its interaction function (such as overlapping with other core units)' and its change functions (such as how it can 'rotate to some degree or vacillate in some way') as a way to identify the general function from a local subset that is sufficiently representative of the differences in the data set that it can be used to find the 'core change unit shape' that can be repeated/shifted/scaled/rotated/otherwise changed to find the rest of the function using some interaction function
    	    - relatedly, finding the network that filters the possible core change type combinations (rotate, scale, shift, vacillate/cycle, embed variables, repeat, abstract, connect, format, etc) in a maximally efficient way for most functions is a useful intent to fulfill as a default implementation strategy to start with
    	- similarly, identifying useful structures like 'maximally different directions of change such as the cardinal directions (or high/low left/right directions of change)' as useful structures to use as a filter to identify different change types that are common and highly different, as a 'maximally different unit of change' which is useful to find adjacent changes
        - similarly, identifying connections between variable interaction structures (like how a variable structure such as a 'variable upper bound and a constant lower bound' have a useful structure of "implying but not guaranteeing (making them probable and useful to test)" other variable structures like 'more change (either expansive/reductive) happening in the upper range' or 'fewer limits on change in the upper range' or 'more change incoming to the lower bound' or 'interface variables and/or constants relevant to the lower bound') which can be connected to possible filters/limits of those structures, such as whether other variable interaction structures (or specific known problem space system rules) filter/limit/prevent a possible implication
        - similarly, identifying local subset representation structures (like densities or average lines explaining the majority of differences in the subset/densities) for one local subset and then removing those probable components of various possible representations in another subset to find additional possible variables in maximally different subsets to filter the set of possible alternate representation structures (and their components) from the original subset, after identifying the maximally different subsets likeliest to have differences in probable representation structures & their components
        - similarly, identifying connections between 'uncertainties' (like subsets of the data set that are more uncertain/variable where other subsets are more easily determined) and 'uncertainty resolution functions'
            - resolution functions like 'intersecting with the most points in the subset with the simplest line' or 'the simplest line that remains some minimal distance away from the most points in the subset' or some balance of 'averageness' and 'intersection' which are useful variables of filtering/generating these 'alternate regression lines' in 'highly uncertain subsets'
        - similarly, identifying useful structures like the 'variation range' necessary to create a useful structure like an identified 'acceptable error range area' (with a trivially identified area of coverage that is likely to be useful in finding a maximum of points falling within that area when moved across the data set 'probable regression range') that can be moved across a 'probable regression range' and tested in various subsets to cover a 'ratio of the data set in that subset', this 'acceptable error range area' being useful to avoid calculating the error for a possible line at every input value and just checking if it falls within the range area centered in some 'probable regression range', either after calculating some 'probable regression line' or using the 'probable regression range' to fit it inside that range, or finding the trajectory of the 'maximum coverage direction' as the area is moved across subsets
        - similarly, identifying the relative usefulness of structures like 'intersecting/overlapping summarizing lines of data set subsets' as opposed to 'adjacent local subset summarizing lines with strict range limits' to allow for possible overlaps in local subset selections bc the borders of these ranges might not be guaranteed/required by data and to allow for known variable interaction patterns like 'multiple possible interaction types/states involving the same variables, variable interactions which can explain variation in outputs with similar/adjacent inputs
        - similarly, identifying alternates of useful structures like 'randomness' (such as how 'randomly dropping a ratio of data points can reveal robust variables') alternatives such as 'combinations/sequences of common/powerful functions across systems' can identify more probable structures to replace these less likely structures, using probability structures like 'commonness' to replace the less accurate/likely structure like 'just any randomness at all (a random selection of randomness)', which applies 'more probable & less random' randomness created by a higher degree of certainty through alternate structures like 'commonness'
            - similarly applying more relevant/useful structures of randomness like 'obvious structures like simple shapes like densities removed from the data set' which are more relevant to workaround (as simple-minded agents are likelier to intervene with obvious human error and likelier to be required to stop the interference of), as true randomness is unlikely to be easily verified like obvious randomness and is less relevant/useful to account for (above some ratio of expected noise), as more complex randomness could easily be hidden legitimate variables undeterminable from the original data set requiring more data/variables to identify such as 'incoming changes' to the data set
        - similarly, identifying a function to convert non-linear to various probable linear functions (more adjacently computable, or using fewer variables involving equivalent alternate variable subsets of the data set) is a useful intent to fulfill with structures like logarithms, topologies, & mappings to model 'interaction levels where interactions are adjacently computable with objects defined on that level'
    	- similarly, an 'index of pre-computed regression lines to compare with original data set subsets' to fulfill useful intents like 'avoiding computation' is useful for connecting original data set subsets and pre-computed regression lines for alternate subsets
    	- similarly, how clustering relevant structures (like inputs) by differences/similarities (such as organizing by the 'similarity/equivalence of output value') can identify useful structures like input patterns of similar outputs given assumptions like 'non-volatility' and 'non-randomness' and 'non-uniqueness of inputs for each output' (like input subsets that have clear patterns, like how a wave function organizing the inputs by similarity of output would have clear patterns of magnitudes/amplitudes in a few subsets that prevent requiring checking the whole input space for the pattern, as the output y-value would have a few data points associated with it such as 'input value points graphed vertically with the y-value on the x-axis' representing inputs having that output, where this vertical input pattern associated with a y-value would have clear patterns such as obvious differences in distance between points that are clear after a few points rather than many), which reduces the problem to 'find n y-values having the same value m times to check for obvious patterns in x-values for each y-value, if there are multiple x-values'
    	    - preemptively testing for validity of 'assumptions of algorithms' is an example of 'alternate equivalent structures' (like limits/requirements/intents) and an application of the 'input/output sequence' of optimal algorithm filters
    	    - this applies a useful function like 'sort' to the outputs rather than the inputs to get useful information about similar outputs once sorted that way such as how variable/cyclical/maximally different the function might be by comparing inputs of equivalent outputs
    	    - once outputs are sorted, it is possible to find highly different outputs easily, which is useful for intents like 'check for maximally different outputs' or 'find output range or output extremes'
    	- similarly, 'finding average magnitude/amplitude/count of peaks in a data set for some subset' is useful just like 'finding the general average value' and 'finding local subset averages' and 'finding reoccurring subsets' and 'finding highly different local subsets and their connecting functions' are useful
        - similarly, a 'adjacent points merging function (using some representation like an average midpoint, or using a similarity metric like distance from/angle to local densities, and using some ratio selection like merging n points at a time within some distance m of each other)' and a 'adjacent point non-merging function (leaving some points unmerged bc of their representativeness of legitimate differences)'
            - similarly, an algorithm to identify variables that can capture high variation when applied together (representation metrics, similarity metrics, ratio of input metadata like count/difference score) as useful complementary capturing variables of information, acknowledging differences in data sets like that averages are sometimes more useful than similarity metrics and sometimes the opposite is true and there is a useful input type like 'input count/ratio of the total count' that is optimal for some structures like high-variation data sets with some noise level
        - similarly, applying similarities between structures that have a reason why theyre useful for summarization/representation intents (like how 'big/simple' shapes like 'equilateral' shapes are particularly useful to identify to simplify the task of identifying a regression function bc finding their centers/averages/densities/patterns is more trivial and likelier to reflect reality as big/simple shapes are less likely to occur by accident in a data set and therefore likelier to summarize the data set, up to a certain point, like how identifying that a data set as a whole has a generally square shape makes it likely to be equivalent to random)
        - an example resolution function between 'densities and regression lines' is 'divide into subsets, then find one representative density for each local subset, then expand densities until an overlap/equivalence is reached with another expanded density' by applying the 'reason' for why it would be useful to connect 'representations (like density averages) of adjacent local subsets' (bc adjacent local subsets are connected in the original input data set, so the reason to connect them (or a variant of them like a representation of them) later is that connecting them later 'aligns with the original input' in a relevant way, relevant by 'preserving the information of the original data set' which is useful for the 'find a regression function' intent) and applying the structure of 'how' to connect them through 'expanding' them (and the reason why to use that, which is an adjacent transform applied to a density average and is therefore useful, where equivalents are also trivial to determine, and these operations in total can beat other regression algorithms in some solution metrics)
        - similarly, a function that connects the 'points that vary' and the 'points in common' across multiple probable regression lines is a useful function to solve for 
            - finding the sections of the regression line that would be variable in variations of the bias vs. variance tradeoff, to focus on finding functions to connect the 'points in common across probable regression lines' that should be optimized for in the final function, where the 'points that can vary across regression lines' can be averaged or otherwise represented by known probable points at discrete intervals rather than a continuous line, where a point not on those points can be approximated by adjacent points, where the 'points in common' can be approximated by a range that is narrower than the range for the 'points that vary' and the range representing a range of acceptable solutions, so finding a function to resolve the reduced solution set of the 'points in common' connecting functions by connecting these subsets with the functions describing the 'points that vary' is a useful function to solve for
	     - finding the connecting function between different sets of summarizing functions like the 'average' and a 'slope-standardized function (to find the useful standard to compare changes to, to find the core differentiating vectors from a straight/average line) and its scalar to scale it to the original' and the 'lines that describe local subsets to the points of extremes (similar to eigenvectors)' and the 'lines that connect averages of densities' is a useful function that connects these alternates which offer the same representation attribute but also capture different information in the data set, as connecting 'efficient representations' is more trivial than connecting 'every data point'
	         - finding the 'useful core function representing the most standardized (such as de-scaled) function' is useful to find a 'component function' to check against multiple subsets of the data set (do any known variable interactions create change types other than this component function or component function range or do they follow the structure of the component function/range) and look for variables that adjacently create/scale the core component function to check it for realistic probability of representation of variable interactions
	     - finding the most important structures to check for when filtering possible solution functions (such as how its important to check if an amplitude of a polynomial is different across different peaks to determine if a peak pattern can be applied/found, how its important to check multiple local subsets of the function input range, etc) can act like maximally differentiating filters of the solution set
    	- finding the useful ratios & other structures of inputs to an algorithm like 'find the common slopes of connection lines between points in local subsets of the data set', where the algorithm to find the 'useful ratio/count of slopes in common (a ratio compared to some standard, like the number of possible connections)' is the target to solve for, as the other structures that are useful are already known or easily determined and the uncertainty is in finding the threshold values or other values to optimize implementations of those structures
       - connecting alternate formats of the data set/regression functions like 'maximally different connectible shapes (like interfaces) that can be formed by a data set subset of some ratio' which can be used to indicate 'embedded variables' (like variations on that interface) is useful for determining one function format from another which may be more trivial than another method
        - other structures than standard regression structures (averages, connection lines, subsets) like 'maps' can be applied as a useful structure in the regression problem space bc of how mapping one subset to another through substitution can be an efficient way to decompose a more complex set of points into a more standardized or otherwise useful set that is likely to represent the original set and requires less memory to store, which are useful as components of solution-finding methods
        - finding useful metrics like 'degree of erroneous difference to ignore' between obvious average functions of local subsets is useful to find out what information to ignore when an average line of one subset differs to some degree from an average line of an adjacent subset, especially if the next subset confirms the original subset average line, applying the concept of 'data corruption' to describe some degree of error deviating from some implied metric, resolving these 'implication' structures (like the implication of a 'common subset average line') into 'conclusion' structures (like a degree of commonness of that line across subsets above some ratio), and finding useful tests of these differences, to find out when a difference may reflect a common or otherwise probable/implied structure (implied by adjacent inputs, common patterns, similarity to known implications, etc) rather than an erroneous anomaly to ignore
	- in the 'network (fuzzy space) of structures' fulfilling intents, interface structures like 'overlaps' exist between structures adjacent to or otherwise useful for multiple alternate intents, these interface structures indicating their usefulness for other intents like 'deriving alternate intents' and 'building a maximum ratio of structures'
        - in the space of useful structures, concepts like 'balance', 'alignment', 'simplicity', 'probability', 'composability', and 'uniqueness' will be obvious, which can be used as 'conceptual filters' of useful structures that are likelier to be useful than other structures
        - an example of this fuzzy space includes structures (like 'angles, partial closed shapes (like sides and corners), connection functions of partial closed shapes, higher-dimensional closed shapes, shapes that when combined can produce a closed shape in between them') as the set comprising the fuzzy space of a 'closed shape', this space being composed of components adjacent to or otherwise useful for fulfilling intents (forming/describing/differentiating a 'closed shape') related to a 'closed shape'
        - a network of similar/equivalent alternate spaces include a 'non-repeatable/unique component space (which is optimal for storage minimization)', a 'repeatable component space (which is more optimal for displaying usages/queries of components)', a 'usage adjacency component space where frequently co-used components are adjacent (that is useful for finding probably useful structures using a component)', a 'difference as adjacency space (where maximal differences are possible with adjacent queries)', a 'layered space with both intent/structures (where the fuzzy space of an intent contains maximally different structures fulfilling/adjacent to fulfilling that intent)' bc of the adjacency of these structures for these intents related to components
            - relatedly, a 'usage network (apply)' has adjacent corrollaries like a 'filter network (find)', a 'component network (build)', a 'difference-resolution network (derive)' due to the core functions it is an equivalent alternate to
        - graphing an intent by its 'surrounding related structures' such as by 'structures that use it' or 'structures that fulfill/build/create/cause it' or 'its input/output structures' or 'structures that filter out everything in some relevant subset but that intent' (or similarly 'structures that determine that intent') is another way to visualize structures like intents that are more useful when defined as a set of alternate definitions which can represent examples of them in some other system
    - framing common structures with relevant metadata like useful intents in standard terms to maximize the optimal positioning of these structures in queries
        - a network (which depicts uniqueness and similarity) is useful for 'finding new/different unique similarities by the gaps & other structures in the network' as well as 'identifying difference/similarity of two known structures'
        - a map is useful for 'finding unique connections & other metadata about connections' (like the commonness of connections having equivalent/similar connection/input/output) as well as 'translating a structure in one format (of a set that can be described by a network indicating uniqueness/similarity) to a corresponding position in another format (of a set that can be a network)'
        - a filter is useful for 'finding a similar subset of points in a network/set of points' (as in similarity to some attribute, like a solution structure/metric/requirement)
    - finding a good starting point to start applying interface structures is crucial for deriving adjacent solutions, like how a limited subset of 'logical rules such as definitions' or 'physics rules' or 'truth limiting rules (what is definitely not true)' might be useful as a starting constant input to start applying interface structures (like changes) to, to derive other rules that follow logically, are required to be true, are implied, are not contradicted, or have other structures of truth associated with them
        - similarly, finding a 'useful structure to describe common patterns in changes' is an example of a useful isolateable structure that can be a good approximation of a full implementation on its own, answering questions such as 'are most variables an adjacent combination (or other core structure) of some subset of interface structures', which is findable with iteration
        - similarly, finding a 'reason for similarities/differences' (reasons like 'its an efficient/useful combination of few inputs commonly available, so is often repeated across irrelevant systems') is another isolateable rule set that is a good approximation of a full implementation of all logic rules of interfaces
        - the differences between these 'equivalent alternate' isolateable rule sets that are good approximations of interface analysis make them useful to combine in an adjacent combination as offsetting 'ensemble' structures to weight the impact of their outputs against average outputs & other representations of outputs
        - this is like how everything can be framed as a component that can be added/multiplied to other components, but thats not always useful in terms of reducing computation requirements, such as how knowing 'addition' and 'multiplication' are capable of describing all other structures, but that doesnt capture a useful degree of complexity of the potential interactions of those two operations, where knowing concepts like 'self' and 'embedding' is more adjacent to the complex operations/functions possible with addition/multiplication (self-multiplication like 'powers' and embedding as in 'embedding of operations'), these two concepts being adjacently derivable with interface analysis through core structures like 'unit/identity (self)' and 'application/usage (embedding)', similar to how matrixes (aligned multiplication of ordered sets) and inner product spaces (spaces where some product is trivial to compute) and convolutions (complete multiplication product of sets) are not adjacent to just the functions add/multiply
             - this is a useful structure for tasks like 'encryption' as the 'set of concepts that are adjacent to a useful structure' is more difficult to guess (from the set of all possible concept sets) but is easy to verify
    - applying interface structures to optimize with ml, such as by applying 'input/output sequences' to position 'generative feature layers before filter feature layers' and other patterns that make sense to server as a useful contradiction to offset the irrelevant variation introduced by the preceding layer, or applying 'self' to apply neural networks to select preprocessing & algorithm functions/parameters
    - to handle common error structures like 'dead ends', apply structures like patterns to find the solution to (the 'way out of') traps using these errors, such as by applying insights like 'nothing is unconnected to everything' which means 'there are no real dead ends' as 'everything is both true and not true in some way' and find the distortion of the perspective that created the error of the 'dead end' and connect it to the balanced perspective (where interface structures exist or are adjacent), creating a difference that allows other differences to be embedded/connected/supported
        - if there are filters allowing for only one possibility (a 'dead end' error that leads away from a network), find the filters that represent the errors in those filters, reversing the perspective back to the balanced perspective and exiting the perspective creating the error of the lack of variation leading to that error, creating opposition to the incorrect difference
        - inject more variables to connect the 'dead end' position back to the balanced perspective, applying the interface metadata to create new differences (such as random differences through interactivity) where required to offset the incorrect differences of the over-reductive perspective, and use those differences to build differences on them and create change in another direction
        - where one structure seems to capture everything (like an attribute network), apply differences to identify other networks that capture alternate complementary information (usage networks, limit networks, difference networks, function networks, etc) to limit the limits of that over-reductive perspective
            - like how an attribute (such as 'criminal') can obscure info or over-simplify info, despite being an efficient way to store some relevant info (like the 'crime of jay-walking' being equated to all other crimes despite the fact that it is mostly only criminalized to collect fees to fund police stations and other far more harmful behaviors are not punished at all, such as stalking/copying inventors), whereas a network with contextual or functional info can more accurately depict that info to reflect its true variation
            - this is similar to how one infinity can be used to create other infinities (like the Banach-Tarski paradox) bc these graph structures are like topologies or fields in that they are capable of describing reality at every point but offer useful alternate advantages when starting from different points
        - this is like the structure of a 'mobius strip' or an 'isolated system describing the system containing it by identifying variables of observations of subsets of its own structures (regarding the incompleteness theorem)' where one structure seems ambiguously equivalent to different structures that seem to contradict each other but actually can co-exist in the same structure using the perspective interface, or like the structure of a 'rule set' occupying a point on a torus where what determines one error doesnt determine another error bc different 'rule sets' are supported and the 'rotation' and 'injection of new interaction levels (as different concentric circles allowed to be the base/core/stable level)' allowed in the torus shape enables endless (stable) balanced variation
        - a 'balance' of variation is a core structure driving interfaces, as there needs to be some variation-supporting structure like a ratio of change (like 'potential/kinetic energy' and 'momentum') and counter-change (like 'gravity', 'energy preservation/transfer limits'), or a structure like 'caring' ('connection to the most stable foundation, where this connections acts like an equivalence') that is supported and stable, otherwise the interface cant exist or similarly cant support any change, which can be used to find interfaces
        - on the other hand, filters may seem restrictive/limiting/reductive (like a trap that is a 'required error' such as a trap hiding a 'dead end' error) but may enable endless complexity/variation, like how a 'reality' filter may seem boring/reductive/simple until you identify how complex reality is, and that the 'reality' filter is powerful in that it empowers other structures to exist like 'clear descriptions/representations & measurements/experiments' as well as constants/variables and consistencies/contradictions to occur, enabling the pursuit and identification of truth and falsehood and the application of differences to both
        - "filters to identify errors such as common structures of paths leading to 'dead end' errors" are a way out of errors of 'constance' (like 'over-prioritizations' such as 'over-reductions') just like finding 'abstractions' or 'interaction levels' or 'equivalent alternates' or 'embedded perspectives' or 'connecting function of perspectives' are a way out of traps
        - finding a common perspective that hosts both a trap/error and also a solution/way out of it (or generally a 'perspective that can trap any trap' as in 'capture the variation of incorrect differences driving traps') is an example of a useful interface to apply to other problems
        - similarly, 'traps/errors may be more adjacent to a solution than another trap/error', so creating a 'path of differences leading to errors' is not just a way to find errors but also find out what is not a solution and therefore what is a solution, and value can be created from a trap if there is a way to convert it into a solution like by applying it to itself ('trap the trap')
        - the 'maximally different structure' that can support the most difference types is also the structure that can support adjacent structures of differences, like 'ambiguities' (like the ambiguity in an equivalent distance of one error from the center of this structure to the distance to another error, and like 'contradictions/paradoxes' and 'counterintuitions/complexities' and 'alternate representations' as different variations of 'maximally different structures producible with the same inputs, supportable in the same system')
    - finding a solution base that is optimal for different algorithms allows finding different possible solution bases, at which point finding the more optimal adjacent solution to those bases is possible with known/adjacent algorithms (as opposed to finding 'maximally different solution bases' to start from with one particular algorithm)
        - finding the maximally different functions that make an adjacent optimum findable with some algorithm helps 'filter out these more adjacent solutions if theyre incorrect' and 'find counterexamples of alternate possible solutions' or 'divide/filter the solution space' faster, such as by finding 'common overlaps in solution spaces generated by different algorithms' where these overlaps are more trivially calculated in some way than by applying either/both algorithms
        - deriving the 'solutions findable/verifiable with an algorithm' is a useful way to filter the solution space, find common solutions across algorithms, and find variables of algorithms to find other algorithms or match algorithms with metadata like intents/metrics optimized for
        - finding a 'network of algorithms with rules for switching between algorithms in certain cases like with certain input patterns or certain solution metrics' is also adjacent using these structures of connections between 'algorithms' and 'solutions/ranges/sets adjacently found/filtered with those algorithms'
    - finding the set of concepts that builds a solution (such as how 'adjacency' and 'generality' help build a 'regression' solution) which help to offset each other ('adjacency' in the form of 'adjacent/local subsets' or 'adjacent/local optima or adjacent/local density averages' offset by 'generality' in the form of 'representative subsets' or 'representative summaries like averages') can help form a common base set of solution structures to build on top of
        - these are useful specifications of more general spectrums like 'specific local variables, as opposed to general abstract patterns/summaries', 'adjacency' being a non-definite partial format of 'specificity' in the 'regression' problem space, having the additional concept of 'triviality' included in the subset of concepts driving 'specificity' to relevant variables like 'position (point, density, extreme)' in the 'regression' problem space
        - finding regression lines whose differences from data points frequently follow a common component pattern (like 'having the same distance from data points' or alternately 'use the fewest components (one component as in the same value)' which fulfills a 'simplicity' or 'linearity of combination' metric) is another useful intent to fulfill in the 'regression' problem space
        - similarly, finding a set of components that commonly connects extremely different points (like points from different non-adjacent subsets at different limits (upper vs. lower) connected to some representative subset like an average) is a useful intent to fulfill in a regression algorithm, the specificity of this intent making the algorithm trivial to find
        - similarly, applying rules of relevance such as 'removing variables with equivalent information coverage is acceptable if one variable remains to cover that information in an intent related to an information-preservation intent but its better to leave the variables/rules generating those variants in the data set rather than leaving in one variant or all known variants in some cases like when minimizing memory storage is prioritized'
    - apply common structures like filter/reduce/match/add/change/map/sort to format functions to identify the maximally different functions, then find the reason why those functions are useful as a way to identify 'rule sets that can act as limits of usefulness to identify the areas and boundaries of usefulness'
    	- example: 
	    	- bc its useful to have a 'map of a keyword to different variants of it', its possible to identify that "a unique signal has variations because of alternate definitions/formats it can take while still remaining unique (bc of 'definition routes')" (allowing this 'variable interaction' & 'variable interaction-structure rule' regarding usefulness to be derived: 'bc this variable interaction exists, keyword maps/definition routes are useful')
	    	- bc its useful to have filter functions in general, its possible to identify that 'not all differences are adjacently useful for every intent' and 'different variations of structures are not organized by default' and 'different structures can coexist'
	    	- bc maps (connections between 'user-assigned (relatively arbitrary)' rather than 'absolutely-defined' values) are useful, 'arbitrary connections' are also useful for intents related to randomness/uniqueness, like when connections dont matter absolutely but are still useful to assign (like organizing a filesystem to optimize common queries such as using symlinks or naming conventions, even though that organization doesnt reflect absolute truths of the universe, or such as how mapping several terms to one identifying term allows quicker identification of unique term usage while minimizing memory storage, or how an explicit map to identify a category of an attribute value set is useful when identifying the causal variables is non-trivial, to skip that causal analysis, or how substitution maps can create the appearance of randomness bc they are relatively arbitrary)
	    	- bc maximally different functions (filter, map, change) often co-occur indicating their usefulness, its possible to identify the 'common complexity of problems solved more frequently recently (bc of existing solutions to simpler problems)'
	    	- bc similar functions often co-occur (filter, sort) indicating their usefulness, its possible to identify that those functions are cooperative for various intents ('sort' speeds up some filters in some cases)
	- applying rules to identify when a regression function can be incorrect mapped directly to problem space structures
		- example: when a high ratio of incidental/random variables (like variables about a context that any species can exist in which is not relevant to identifying a species) are included in the function, or when an important variable is ignored to fulfill an incorrect priority like simplicity, these errors are mappable to differences between the incorrect function and data set subsets
		- when a more simple function is found but it contradicts the more complex function that would cover more cases bc of the data set subset chosen/available that makes the simpler function seem correct, its violating other known solution priorities like generality which can offset over-simplification errors
		- more importantly, structures of this error can be mapped to a set of example possibly relevant regression structures (data set subsets and regression lines)
		- for example, when there is a 'small pattern in a local subset indicating a more complex function' which is 'within the boundaries of a potential field of variable interactions' and 'not overlapping with probable error areas' but is ignored in favor of a simpler function, thats a structure that could be an error of 'ignoring a general trend bc of a data set subset selection (applied either before receiving available data or after)'
	- applying interface structures to known useful structure (like gradient descent) given their definition can identify relevant structures useful to those useful structures like optimizations to apply to inputs before applying the useful structure
		- the idea of 'gradient descent' is most useful when applied to a 'function that is adjacently connectible to the optimal solution' (meaning the inputs are already adjacent or equal to the actual optimal inputs) bc of the definition of 'gradient descent' which applies 'adjacent change combinations', assuming that 'adjacent change combinations' are capable of finding an optimal (which is best used when 'everything is adjacent' or 'maximally different examples are adjacent', etc)
        - this makes 'maximally different interaction levels' a useful structure to identify, to make most variable combinations or 'the most variable' variable combinations adjacent (which is what interface analysis does by default)
        - for example, to identify the structure of 'imaginary numbers (like i)' as a possible useful structure, some intents (like 'create a circle' or 'create common structures (like a circle)') make it obvious to try as a component of formulas and its not adjacent change combinations as theyre typically implemented ('addition/multiplication') but rather as an adjacent combination of interface structures (like identifying how addition/multiplication arent easily describing all structures and solving all problems, so try applying the 'opposite' of 'components of those core operations' to try to generate differences to use to describe different structures), for example to solve the problem of finding 'equivalent alternate maximally different descriptions/generators of a circle as a useful structure to describe/generate/apply', otherwise the idea of 'square root of negative one' is not adjacent to most intents but is obviously useful to identify, so identifying 'useful intents that would make such structures obviously useful' are useful to identify by applying interface analysis to fulfill probably useful intents like 'create common core components in different ways'
        - as another example, the way I discovered that circles are related to primes is by picturing multiplication in my head, in which I could easily generate squares/rectangles but I noticed there were gaps between these shapes' corners which were easily generated by integer factors, and these gap points were likely to be more describable as on a curve rather than as a product of integer factors
        	- https://twitter.com/remixerator/status/997394393471516672
        	- it also reminded me of the fibonacci sequence bc of its embedded growth between numbers in the sequence rather than clearly exponential or clearly linear growth, as a different type of default growth that was neither of the two simpler types
        	- the idea of 'gaps between integer non-1 factors (when graphed as pairs of factors)' is not actually definitive as indicating non-linearity but is still evocative and therefore still useful in adjacently deriving that non-linearity is relevant, these 'evocative' rather than 'implicative' ideas are still useful in deriving new probable structures to test
        	- this insight path would be easily identified with interface analysis 'which asks questions like "what something is not" and "what is not easily generated by these variables" and "what is not adjacent" and "what is different" and "where would similarities be expected (like with other integers) but differences are found instead (like with primes) and what interface unites these (like the dichotomy of curves vs. rectangles)" whereas machine-learning dumbly applies a few insights at a time such as 'adjacent change combinations eventually can find some useful functions if you have enough compute',
        	    - while applying other relevant structures, like filters to avoid irrelevant exceptions/rules like pieces of the definition that most closely overlap with other more relevant functions like the 'identity function (using multiplication)' being more relevant than 'addition/multiplication' as the identity function is less meaningful than non-1 factors)
        	    - identifying the alignment between 'numbers between non-1 integer products in 2-d space' and 'numbers between non-1 integer products on the number line (primes)'
        	    - other useful structures to apply are:
        	    	- 'equivalent alternate formats' of primes and their factors (the relevant default variables) such as 'sets of alternate factors and lines connecting sets'
        	    	- the basic 'interface' structure which connects the 'equivalent alternate formats (number sequence of primes and factor sets of primes)' using a common standard they both adhere to in their similarity of patterns/gaps
            - relatedly, deriving other useful structures like 'e' can be formed by maximizing output change types (differences between adjacent values) while minimizing input variables (the 'previous value' and the 'previous previous value' and the 'addition' operation) without using exponents and using a simple function (minimized differences necessary to create the extreme differences)
        - similarly, 'adjacent change combinations' like addition/multiplication are not frequently useful for useful intents like 'differentiate a wave from a circle' which a machine-learning algorithm typically would not adjacently achieve but interface analysis would by applying structures like circles by default as a common core structure on the structure interface
    - applying interface structures like 'causal sequences' to identify useful structures like 'alternate points where variation is useful to inject'
    	- for example, injecting variables at the point of data-creation/gathering rather than data-processing (pre-pre-processing) could influence the value of data (its reflectiveness of reality) such as how 'smiling' before taking pictures can make some variables more obvious/detectable, which is a 'reality change rule' that is useful for making variables less ambiguous/more obvious so a classification algorithm can be simpler, so an algorithm to identify these points/rules is useful to improve data quality, or to identify more useful problems to solve like 'if the subject was smiling, what would these hidden variable values probably be, based on similar expressions as smiling that are generatable with existing data' (similarly, 'identifying a dye to inject (input side solution) or a treatment to try (output side solution) that would differentiate cells more easily' is a useful structure for a classification algorithm to be able to identify to improve data quality and delay decision-making of the model structure until a data quality ratio/info minimum is reached), which applies the insight 'bc there are some differences between items in different classes, there will likely be other differences that are detectable'