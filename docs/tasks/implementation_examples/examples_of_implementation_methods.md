- examples of other implementation methods than documented default methods (like the 'apply/find/build/derive' function set is an 'implementation method' of interface analysis as its an independent set of structures that can be applied as defaults to implement it to fulfill some solution metric like comprehensiveness, minimal constants, variability, etc)
    - this document contains 'function sets which can act as implementations of workflows' (function sets which can implement a solution automation workflow) as well as 'solution automation workflows' (useful sequences of steps to solve a problem)

- how to implement the interface analysis framework as a set of simple functions for an initial version, involving alternate default constant simple structures to combine, such as:    
    
    - identify useful structures like 'overlapping metric structures like metric ranges/types' between interface metrics like 'reason-backed functions' and 'relevant functions' and 'maximally different functions' and 'average functions' which create 'interface-similarity indexes' to identify 'interface-different functions' as well as 'connections between interface structures of differences/errors and useful functions like reduce/connect'
        - for example, the combination of errors as in a 'lower/upper limit on error' is useful to identify another useful structure as in a 'range of error', and similarly, other structures of errors can create other solution metrics, if the errors create a 'reduced/connected/otherwise useful structure'
        - similarly, the 'range in a value error' is similar in usefulness to the 'range in a complexity error' and a 'range in a meaning error' (there is a 'complexity range' and a 'meaning range' to optimize for, as well as other ranges of interface structures like similarity/maximal differences and other structures like 'meaning values like thresholds/ratios/positions/points'), similar to how there are 'real ranges of complexity (as in the "most complex identified irreducible structure")' and 'probability ranges of complexity' that overlap with other 'ranges of interface structures'
            - relatedly, identifying structures like 'limits' of interface structures like 'irreducible complexity' or 'optimizable complexity' that is related to core interaction functions like reduce/connect or other function sets like 'optimize' are useful to identify and apply to connect these function sets (connect 'reduce/connect' and 'optimize' using complexity)
            - relatedly, identifying 'connections between solution structures' such as how a 'solution/optimality "range" is equal to a usable error "ratio"' is useful to 'reduce uncertainty' in the problem of 'filtering interface queries', so that once a 'solution range' is identified, a 'usable error ratio' is also identified
        - relatedly, identifying the connections between core functions like 'reduce/connect' and general problem-solving intents like 'optimize' are useful to identify, such as how 'changing by reducing some metric' will 'optimize' for that metric, which creates a problem of 'identifying metrics to reduce' in order to implement an 'optimize' function by 'reduction' (and implementing an 'optimize' function by 'connection', and an 'organize' function by 'matching/sorting', etc, since there are functions more similar across function sets which should be applied as a base for implementing those similar functions in another set)

    - identifying useful structures like general connections between specific interface structures like 'remove limits requiring filters, by identifying alternatives to limited structures' to identify useful problem-solving intents to avoid another workflow ('compare, and filter')
        - identifying alternate intents ('allowing investing in multiple alternatives, to avoid choosing/filtering') to invalidate some intent sequence ('compare, to choose/filter an option from a set') is useful to apply to identified sequences/structures of intents like workflows/queries ('the more alternatives can be supported, the fewer choices need to be made') like where some problem has been invalidated ('organ function is not a problem bc machines replace the organs, essentially removing the limit on organ function') so fewer choices need to be made (more treatments can be tried at once), which involves general intents like 'remove limits requiring filters, by identifying alternatives to limited structures'
        - relatedly, identifying probable moves ('moves that are so different in some metric that they are named since names are relied on, where the name indicates some often arbitrary similarity to familiar structures, bc memory favors indexes like specific named moves, and these moves overlap with some interface structure like game-ending moves') by identifying memory mechanisms (prioritizing for familiarity/maximal differences/specifications and having limits favoring small indexes and having alternatives like 'generating optimal moves from states' and opposite cases like 'best/worst cases' like 'memorizing consensus/identified-optimal moves in most states, given some consensus mechanism' and favoring extreme filters of moves to extremely reduce the solution space) is useful for specific 'consciousness levels' and alternatives like 'iteration/interaction/variation levels' 
            - relatedly, identifying 'structures to identify when there is likely another useful structure like a move in a space' is useful (identifying remaining variation vs. identifying variation of useful structures, such as identifying remaining adjacent formats/concepts not connected to the space yet and identifying formats/concepts as sufficiently high variation and adjacent to be useful structures to connect)
            - the value of a move is related to its surprise/difference which means 'identifying maximally different moves, given a sequence' is useful to identify the most optimal moves, given the 'previous sequence indicating concepts' that will be useful in identifying 'opposing concepts' to generate surprising/different moves
        - relatedly, identifying useful metrics can be done by identifying structures like units/high variation concepts/proxies/variables/similarities/specifications/examples/definitions, so identifying possible solutions is a matter of 'identifying these possible metric structures' and 'identifying graphs where some subset is adjacent/otherwise useful' and 'identifying adjacent/otherwise useful routes to a subset of the metrics'
            - relatedly, identifying structures like 'metrics' that are only similar to a subset of interface structures (like concepts/variables/specifications) is useful, since its opposites (problems of comparison and problems of identifying what to compare) can likely be trivialized by the opposites of those interface structures (uncertainties/ambiguities/sets/generalizations/disorganizations/requirements) which are nearer to problems more often than solutions, and identifying these connections is useful as a default problem-solving index/network
        - specific error structures like 'overwhelmed structures' like 'over-stressed systems/functions/bases' reflecting 'sub-optimal but probable/adjacent errors, in a system where balance is essential for functioning' and opposing errors like 'under-used systems, leading to disorganization from lack of use' and like 'incorrect structures to compare' of intents like 'compare, to filter' are useful to identify ('what are the error structures of each intent/structure' being a useful problem-solving intent)
            - identifying other differences than 'opposite' like 'average', 'adjacent but different', 'different but relevant', and other useful structures for different intents are useful to index (opposites are useful for identifying extremes to oscillate between or similarize, averages/parallels/similarities are useful for identifying differences/orthogonalities), such as to identify the 'average of a set of structures' which is likely to be useful in connecting the extreme opposites)
            - relatedly, specifying a 'unit of a problem' as a 'trivial iterable combination of problem variables' is useful to identify other structures like 'variables that havent caused a problem yet' like 'new variable combinations or new iterations of existing variables' as a 'new source of variation in "error"', where 'identifying new sources of variation in "specific interface structures" (like by applying new variables and new variable sources to all interface structures)' is generally useful as a problem-solving structure
        - relatedly, identifying a map of error structures like 'invalidation structures' within a 'definition set created by applying interface structures' to identify inputs/limits/structures of error structures like 'extremes in interface structure variables like position/size' is useful to identify 'common variables of valid/optimal/error structures in a definition', as many problems involve 'identifying different variants of a definition' (applying the definition as a core similarity to base/connect changes on)
            - for example, the intent of 'minding your own business' to fulfill intents like 'decency' has an error (as in "when is the worst case to apply the intent 'mind your own business'", such as 'when a victim is calling for help', which violates the relevant intent of 'positivity/decency' despite definitely 'minding your own business')

    - identifying structures like 'definitions/connections/directions/positions' that havent been optimally used yet is useful to identify, like 'connecting interface-variants of combinations of solution metrics, starting with the definition of combinations of solution metrics'
        - for example, 'identifying solution metrics' is useful for other intents like 'identify interface structures like combinations of solution metrics' and 'identify the limits/areas/centers of the combinations of solution metrics' which is useful as a general problem-solving intent, bc identifying ways to change the solution metric combinations so that they still fulfill the metric definitions is useful to identify solution structures like solution areas (find all the ways solution metric combinations like 'efficiency/multi-functionality' can be 'changed' and 'changed into structures' so that the structures still 'fulfill the definitions of efficiency/multi-functionality' to identify an 'area or other similarity like a pattern' of 'efficiency/multi-functionality' by changing the definition of 'efficiency/multi-functionality' up to its limits, which is like stretching definitions to find paradoxes/contradictions), which is like reverse-engineering all the structures that are optimal by 'identifying all the structures that useful structures like optimizations/solution metrics can have' in the abstract and in general and in specific systems
            - relatedly, connecting these abstract 'variants of solution metric combinations' with specific variants and similarly connecting other spectrum/high variation/interface variable 'variants of solution metric combinations' is similarly useful
        - relatedly, 'identifying common variables of optimization/solution metrics' allows identifying 'areas/points/ranges/structures of overlaps' across solution metrics which is a useful problem-solving intent
        - relatedly, connecting 'networks of solution metric structures' or connecting 'solution networks with problem networks' can be done in various ways bc of the network structure they have in common (rotating networks or changing position or extending their definitions to create intersections/overlaps)
        - identifying useful connection structures in between 'abstract solution metric combinations' and 'specific solution metric combinations' like 'similarities/differences' ('similarities/differences' are so relevant/useful/standard that they are useful to connect these 'abstract solution metric definitions' with 'specific structures resolving each similarity/difference structure') 
            - meaning one of the best ways to specify abstractions is by connecting them to similarity/difference structures or error structures (which are relevant to problems), which means it can be used to identify which specific structures like specific maps/networks/indexes are useful
            - this means identifying all the ways 'efficiency/multi-functionality' can be used to resolve an 'ambiguous similarity in different structures' or a 'ambiguous limit of a difference in a similarity', and converting a new problem to similarity/difference structures to identify 'efficiency/multi-functionality' structures (solutions) that can be used for that problem's similarity/difference structure (applying the intent of 'standardizing all interface structures to all other interfaces')
            - identifying all the interface structures like 'problems/solutions' or 'similarities/differences' in a 'combination of solution metrics' is useful in general and will likely also be useful for 'specifying the abstract metrics'

    - identify useful structures like 'incompletely implemented intents' that are likely to be useful, beyond structures that are defined to be useful or required or otherwise guaranteed to be useful
        - for example, identifying 'missing info' in the 'set of differences in similarities' is a useful intent applying an error structure to an interface structure, an intent that is incomplete, so is useful to identify
            - similarly, identifying other useful structures like 'solutions to errors of structures of interface structures' is useful as a default set of intents that will likely solve other problems once complete
            - this is identified by identifying how many abstract info structures are 'similarities in differences (or the opposite)' (like how abstractions/types/maps/efficiencies/optimizations/requirements are all different types of differences in a similarity) and identifying a graph of these useful structures as having missing variants and identifying the likelihood of other useful incomplete intents that would be generally useful across problems (identifying all the abstract info structures that are a 'similarity in a difference' and identifying relevant structures like the variables of these would likely cover a high ratio of problems) and the relevance of completeness of an implementation
        - identifying 'completeness' of an implementation is a matter of identifying 'useful approximate completeness' of an implementation and 'useful interim approximate completeness' and 'useful alternate approximate completeness'
        - relatedly, the set of interface structure connections has a 'maximum useful integration potential' that isnt reached which is useful to identify (there is a way to connect every interface structure in every way, but the 'maximum useful integration potential' is likely less than that set of complete connections, so 'combining interface structures in every way' isnt likely to be useful except as a set of default connections, and the 'most integrated connections of the most variation' are likelier to be a useful subset)
        - similarly, the 'variants of an implementation' that 'only uses high variation variables' or 'which identifies components that can be recombined to generate other useful implementations' are useful to identify and apply as problem-solving intents (integrating intents into interface queries to solve future problems) and 'connecting these variants of implementations in a network' is a useful intent
            - these 'implementation variant networks' (connecting implementation variants like the 'most complete simple implementation', meaning implementations of specific intent sets, like 'generally useful intents') are useful to connect to 'solution metric networks' and the 'solution variant networks' and 'optimization networks' and 'error networks', as implementations will inevitably overlap with solutions/errors/optimizations/variables of these structures/interface structure networks, and there might be useful symmetries/variables identified in these connections
        - relatedly, 'identifying variables' usually is useful before/after 'identifying network of variants', so identifying these 'useful structures like "alternate sequences/adjacencies of intents", given probabilities of usefulness' is useful to identify probable/default components of interface structures (these two intents arent just useful in a sequence, theyre useful in any adjacency to each other)

    - identifying useful insights to apply in a problem space by identifying sub-problems (like 'identifying what is an input to reality (components/cause/probability)' as a useful intent in the 'describe superpositions' problem space to identify related insights to that intent as a useful intent-insight index to apply)
        - for example, identifying that 'if you can measure/interact with it, it can be real' and 'components/inputs/outputs/variants/symmetries/probabilities/adjacencies of all possibilities are real' and 'metrics cant always capture all relevant variation' and 'metrics can change information' and 'information changes (or is changed) once its known/seen' and 'identifying a symmetry uniting variables like speed/position is probably possible/valid/useful' and 'identifying different alternatives like alternative requirements is a way to avoid suboptimal metrics like metrics that change info by identifying new relevant variables like new symmetries and changing direction toward other filters/metrics' (as in 'rather than measuring something, give it more functionality until the original problem is solved') are relevant to quantum physics structures like 'entanglement/superpositions' is useful to identify alternate representations of their interactions, given that sequences arent quite sufficient as a representation
            - for example, the 'multi-directional cause' of 'metrics that change measured values such as by over-standardizing/simplifying values' is a 'combination of a normal causal sequence (measure a value that is observed to have occurred in the past) and a reverse causal sequence (change a past value when measuring it)' where the measurement acts like an attractor of truth thereby 'making any adjacencies to truth that fit in the measurement true', and where measuring it leads to future changes applied to that value, either directly during measurement or bc truths are frequently relied on and changed by default, where these sequential interactions are better represented as networks/loops
        - these insights identify related structures like 'overlaps/connections/symmetries (like manifolds) between alternate possibilities (as in "alternate speed/position similarity/connection networks/states") or related variables (a symmetry in speed/position as in "causes of position and position changes, like speed")' where these structures are allowed in the superposition by its definition are useful structures to identify as possible and identify filters of
        - similarly, answering questions like 'what needs to be the "free/variable" form of possible, for reality to be the "real/certain" form of possible' is relevant to solve the problem of 'what can be unmeasured/variable, without destabilizing reality', since when some variable is measured, it is changed or starts changing, and 'temporarily freezing everything in reality by measuring as much of it as possible and using those measurements' might prevent variation from existing outside the constant/frozen area and the measurement device, so 'synchronizing measurements with requirements/constants/truths' is useful to avoid 'disorganized/overlapping loops of measurement/variation that dont synchronize measurements with constants/truths' that invalidate each other
            - similarly, 'identifying the correct loops of measurements/filters, and the synchronizations of measurements with constants, and their connective network to avoid these errors' is a useful problem-solving intent
            - relatedly, identifying 'variables that cant be constant (like random variables or infinite energy sources)' are useful to identify as 'limits of constants' and 'invalidators of truths'

    - identify useful structures like functions to identify value spectrums that can be used to fulfill generally useful intents like 'rank/prioritize/sort intents'
        - for example, identifying 'value spectrum variables' as relevant is useful to identify related intents like 'identify resolutions or switching functions between similar relevant high-value intents/functions' ('high-value intents' like 'identify a new network' or 'change position/similarity definition of an existing network' which are similar/relevant and likely to be alternatives), to identify similarly valuable structures on the 'intent value spectrum', as identifying more valuable intents to solve for by identifying the value spectrum and the 'function to determine comparative value of an intent (like by identifying the degree of variability resolved by the intent)' can direct other interface queries toward these higher value intents (similar to how general intents are more valuable)
        - similarly, 'predicting future high-value intents' and 'identifying low-value intents in contradiction to this insight (like intents that are "required but otherwise trivial or irrelevant", or intents that are "specifically useful but not adjacently generalizable")'
        - relatedly, identifying the 'required generalizability' and other applications of interface structures is useful to identify the network of interface interactions (as well as their optimal interactions, requirements of their interactions, limits/ranges of their interactions, etc based on that network)
        - similarly, identifying the index of 'intents that are optimal, once other intents are completed' is useful to apply the vertex of 'function usage optimality, once some subset of functions is usable by being complete/identified'
        - relatedly, identifying the 'cost to change an error into a solution' is useful to identify before identifying whether some structure is an error/solution (if it turns out to be an error, how much approximately would it cost to change into a solution, meaning how useful/true/otherwise relevant is it?)
        - similarly, "identifying useful structures like 'change size/position' and how it can be useful and the contexts where its useful" is an index that reduces the problem of 'identifying interface queries' to 'identifying the context'

    - identifying useful structures like 'problem stability' and 'problem/solution stability ratio' and identify related useful problem-solving structures (like ratios and intents and equivalent alternates)
        - for example, identifying that the 'lack of fragility' is frequently a problem in 'hard-coded (over-constant or over-stable) processes, where 'flexibility' is more useful than 'stability', identifies that the 'stability' of errors is frequently a problem, so that 'reducing the stability of a problem or its causes' is a way to solve problems (a 'greater ratio of stability' in errors as opposed to solutions is frequently a problem)
            - for example, 'hard-coding growth' is a problem leading to other problems of 'over-stable growth' (such as in 'cancer') as opposed to its opposite function error ('over-stable regulation' such as 'autoimmune conditions') and as opposed to its solution opposite 'hard-coding balance between opposing forces and other optimalities like balance of interactions between variables of opposing forces'
            - the 'over-stability' of a constant leads to other 'over-stability' errors, such as 'over-stable errors (compared to stability of solutions)'
        - this identifies 'stability of a problem' as a useful variable to identify other problem-solving intents like 'identify constants and check if they are problems, given how frequently "over-stability/inflexibility/constance of an error" or the "comparatively high ratio of stable errors compared to stable solutions" is a problem'
        - this is similar to how falsehoods are more common than truths and problems are more common than solutions, so using 'commonness' as a variable of problems/solutions is useful to identify other useful structures (identify all the problems or their generative variables and whatever is left is likely useful/optimal in some way or identify solutions since theyre rarer)

    - identifying useful structures like 'connections between useful function sets (such as a function that can be an item in either set)' is useful as a new generally useful problem-solving intent
        - for example, identifying a function that can easily become complex/simple is useful to identify functions that are likely to be useful for solving most problems, given that 'mixed-complexity structures' and other interface structures of interface structures are likely to be useful in algorithms, given that problems often have a simple/complex variant, so simple/complex functions are required to solve those, and applying an optimization like 'multi-functionality' to 'a set of functions including both simple and complex function subsets' by 'merging the functions into one function' is useful, given that its possible
        - identifying optimization opportunities like variables like 'possible usages of error while fulfilling other intents' (determining what should be stored about errors while determining the original intent)

    - identify useful connections like between intents/workflows/descriptions of problem-solving variables and sources of variation (connections between 'different differences in variables identified')
        - solving the 'interface identification' problem as solving a problem of 'identifying a useful base' as in 'parameterization' which is connected to a default workflow 'change a base solution (or change a base problem/error)', where other problem-solving workflows/intents like 'reduce the problem' or 'connect problem/solution' or 'identify new variables' can be connected to the definition in other ways (as a 'powerful variable identification' problem, or a 'connect all independent variables' problem, or a 'variation position identification' problem)
            - for example, '"apply optimizations of another solution" to make that likelier to solve other problems by making other optimal structures generally useful' is a difference applied in the "problem solved by a workflow" bc that is a useful 'base for change' (source of variation)
            - similarly, 'change solutions to avoid other structures of errors' as an alternative to 'avoiding error inputs' like 'avoiding other error interface structures like error requirements, common implementations of error functions, error outputs, etc' is a difference applied in the 'error structure avoided/optimized for by a solution or solution-finding method' bc that is a useful 'base for change'
            - similarly, 'identifying changes of limits' is useful as a general problem-solving intent, bc when limits change, it is useful to identify that before encountering the side effects like errors of that change as limits are powerful predictors
            - similarly, 'identifying abstractions or adjacent variables that are highly descriptive of some complex variable' is another useful intent related to 'changing the base' as the 'abstraction' functions like a base, and relatedly, 'changing the base abstraction' identifies related problem-solving intents, like 'identify the variables that connect high variation variables bc the interim variable has variation that is in between the other variables (causally or otherwise), or bc the variable contains all the variation and more of the connection so it is adjacent by encapsulating the whole connection or it approximately encapsulates the connection by connecting slightly different endpoints (in a different position, like surrounding rather than between)'
            - connecting these intents to workflows is useful to identify a 'default index of intents implementing a workflow' to filter interface queries
        - relatedly, identifying the 'intents that are frequently useful' (like identify the 'high variation' variables, identifying the 'high variation variables that connect high variation input/output variables', identifying the "equal and unequal-variation containing" variables, identifying the 'n-variable reduction/filter') is useful to identify intents that should be prioritized as having 'different differences in the variables identified' and identifying their optimal routes/weights/other variables of their connections
        - relatedly, in addition to identifying 'cancer growth at night' and 'other variables that increase/reduce at night' to identify 'cortisol at night' as a possible treatment option (identifying the 'similarity in variation' between 'time' and 'cancer increase' and other 'variable increases/decreases') is possible by identifying 'stressor balance' as a powerful variable in determining health (identifying a 'stress' interface and a 'stressor/handler ratio' rather than identifying the 'similarity in variation')
            - this is possible bc there are often alternate similarities that are relevant to a problem (the 'similarity in variation across interfaces like stress/time' and a 'similarity in the "stressor" variable across problems') which are different similarities connecting different structures but are relevant (interact with similar or related variables)

    - identify useful structures like 'unidentified inputs to workflows' that are useful to connect to other useful structures like 'function sets' to avoid using other useful structures like 'filters' (once these function set connections are known, standard filters of the standard solution space in regression are less useful than queries of these function sets or their similarity indexes)
        - for example, identifying 'useful/optimal states of variable interactions that a function could represent' is useful to identify 'highly probable functions', and identifying connections of these functions to other useful function sets like 'required functions, core functions, and maximally different functions' is useful to identify why functions dont always arrive at those useful function states (like bc of specific error structures or bc of state progressions toward those useful states), which are inputs to workflows like 'change a base solution function' (first identify all the useful known/adjacent functions that could solve a problem in some system, then identify why a function might differ from those optimal states of a function, such as system complexity requiring more complex functions)
        - similarly, identifying 'interface inputs' to other core interaction functions like 'reduce' like 'maximal differences' and 'similarity indexes' and 'interfaces' as well as identifying inputs to useful functions in general (across functions or function types) is similarly useful as identifying inputs to 'change' (in 'change a base solution'), as well as identifying 'optimizations of input/output connections and optimizations of outputs that will avoid needing the original function in future (applying self-invalidation structures to optimize a function such as storing input/output indexes)' and connecting these inputs by identifying their similarity indexes and generative variables is similarly useful, just like how 'connecting solutions and connecting solution metrics and connecting problems/solutions is useful', so is 'connecting workflow function inputs' given their core similarity of their common type of structure, similar to how 'connecting other structures of a type like connecting requirements' and 'connecting interface structures like functions to core structures like "directions/values/ratios/angles"' is useful
            - this is related to 'identifying the useful inputs to workflows/functions' like 'identifying the useful structures to find/reduce/connect/generate' (like 'find a ratio of some variable, to differentiate these structures' or 'find a ratio to optimize this conflicting interaction between these opposing variables', then 'find which variable to compare in a ratio', thereby usefully specifying the uncertainties/variables of previous intents), which is useful to specify and apply as a 'default/general problem-solving intent'

    - identify useful structures like specific mappings between concepts/math structures that can fulfill general problem-solving intents like 'identify the ratio/angle/other numerical structure that contains the variation of relevant differences in problems or the variation of a concept relevant to problems'
        - for example, identifying a 'spectrum of complexity' where complexity is mapped directly to numerical values is useful as a 'numerical concept mapping' which can be combined with other numerical concept mappings to directly identify concepts in specific number structures (the existing numerical spectrum mapping of 'counted steps' as a proxy for complexity is one mapping, but is not the ideal mapping as it leaves out other structures like 'lack of relevant simplicity structures' like 'missed opportunities for simplifications as in over-complications' and other errors), which is likely to be possible with a network organized in specific ways like where 'different nodes occupy different positions around some core base which theyre all similar to so that angles between nodes reflect these differences and the value can be mapped to a number (which is a set of values between 0/360)', though there are other possible organizations that reflect differences better in some numerical value like an 'angle/ratio', which is valuable to generate/filter
        - this mapping involves identifying a 'complexity unit (such as a "specific unit of difference from optimal relevant simplifications") mappable to a value that can be iterated to increase complexity' or a 'complexity metric mappable to a value' or a 'vertex that contains the variation of complexity in comparing the two perspectives/graphs from that position like "difference from over-complications" and "difference from over-simplifications"' or an 'area or parameter of complexity' or a 'aggregated/average/net/other summary metric of difference from an optimal complexity vector/matrix'
        - similarly, identifying useful structures like 'non-volatile similarity indexes of relevant optimality (such as optimal complexity)' where 'differences from these indexes indicate over-complexity or over-simplifications' is useful as an organizational structure that would be useful to identify, such as identifying 'difference from algorithms that use mostly only adjacent resources like functions, and only use a low ratio of iterations, and use no unnecessary/repeated functions' or 'difference from requirements/solution metrics' or 'difference from general over-complications/simplifications (as opposed to specific errors, given some optimal solution for a specific problem)' or 'difference from structures that are useful in understanding complexity like complexity variables/inputs/causes/limits/generators/descriptions' as a way to value 'complexity of an algorithm' (as in 'if it doesnt help define complexity or reduce complexity or reduce general complexity, its not complex')
        - other useful structures that contain a high degree of variation are 'parameters determining an infinite series', the 'parameters of the most similar/different functions', etc
        - building interface queries as having these known useful/high variation/solution structures as components like building combinations of a 'numerical abstraction ratio' and a 'unifying base concept to position other changes adjacently to' and 'numerical concept mapping' and a 'similarity angle using that base concept as the common point' is useful as a default set of structures to apply changes in between to create interface queries, or alternatively 'identifying how structures like queries of these structures can be used/organized optimally' is similarly useful to avoid generating all possibilities and filtering them
        - relatedly, identifying 'why not to use a solution that fulfills some ratio of solution metrics' involves how the energy spent identifying/applying that solution could be better allocated to inventing some new structure that is generally useful, as a specific solution that is optimal in many ways might not be generally useful and is therefore actually suboptimal when its meaning is compared to the generally useful structure (rather than trying to 'identifying algorithms to organize systems so that victims can kill all predators', 'inventing tools to prevent predators from being predators such as "algorithms to separate victims and predators" or "algorithms to identify and oppose predatory intents/inputs"')

    - identify useful structures like 'new connections between solutions/errors' which can act like problem-solving workflows or general problem-solving intents
        - for example, its likely possible to create every error type by iterating in a particular direction or iterating another one-dimensional structure, given that most errors can be represented in one variable (like a 'difference structure from some similarity'), so identifying the structures of errors like sequences that are likely given some dimension in which they can occur is likely to be useful as a general problem-solving intent
            - for example, repetition can make some structure seem both true and false depending on the other repetitions occurring and the number/ratio of repetitions (repeating it to a low degree creates a possible error of seeming true, repeating it to a high degree creates a possible error of seeming false)
            - every structure can be used to create every error type so identifying the 'distant/adjacent' structure/error index is useful as a default set to apply
        - identifying every format of a solution structure that can constitute 'some combination or other structure of errors' given the correct solution (every polynomial that can represent every combination of errors, given the solution) is a useful index to compute, which is useful to identify whether some error applies to an incorrect solution to identify the correct solution given the errors of that incorrect solution
        - identifying the connections between 'current info (solution function to the original data set) and potential optimized info (optimized function in the system that created the original data set)' is useful as an alternate problem to solve than regression (optimize the data set until it reaches some optimal interaction function, rather than trying to identify the current description function)
        - identifying 'subset selections' that indicate types or abstractions that describe reality is useful (such as identifying interface structures) bc there is a set that can be modified with specificity or another interface change and contain a solution in at least one of the items, which is useful to find the correct abstraction level of so that the default can represent the most adjacent state of each item to other useful structures
        - formatting 'filters' as 'variables/changes' can be useful to identify the 'changes that will make some difference resolved or obvious' or 'create other changes that are useful in some way like uniqueness or already identified' or identify 'filters of subsets of variables' and 'networks of subsets of variables that respond the most to these filters' and a 'merging/integration strategy for those subsets of variables'

    - identify useful structures like 'pre-filtered or specified variants' of structures 'known to be useful or required for another useful structure' like a 'high variation function' like 'organize'
        - for example, identifying useful structures like 'specifications of inputs to useful structures' like 'specific networks (such as specialized networks)' that are useful for the 'organize' function (useful by making it more trivial to implement) is a useful general problem-solving intent, once structures like 'networks/groups/types/sorts/variables' are determined to be useful for 'organize' functions (an input intent to this general problem-solving intent)
        - this is useful to identify bc there arent a high count of possible variables that would be useful ('specialized' being a variant of an interface structure, which is related to the set of possible variables that are useful through 'creating high ratios of difference'), so finding the one variable to apply to a structure to make it useful is often trivial, given that a 'specialized network' (such as functions with non-overlapping functionality) can be trivially generated/tested given that description, and the more filtered/specific the description, the more trivial it is likely to be to generate a useful structure fulfilling that description (assuming no contradictions or other errors in the specification), where functions that are difficult to generate for a highly specified description are a likely source of 'orthogonal variables/interfaces'
        - relatedly, identifying 'implementations' of useful structures like 'multi-functionality' is useful, such as 'storing small indexes of high info variables like intent/requirements that are easily differentiated to avoid errors like incorrectly merging them and are therefore more manageable' and 'switching cost reductions' and 'overlapping functionality fulfilling multiple intents at once, where other intents are adjacent to the overlapping similarity (organized around a core similarity)' and 'predictions of task requirements/sequences (useful for finding imminent overlaps or other optimization opportunities)'

    - identify useful structures like 'new directions to apply variation in' which are useful like 'generating new structures first, finding out what they are adjacently useful for, and finding new useful intents fulfilled by those adjacent intents made trivial by those structures'
        - for example, generating new math structures and identifying new ways they can be used and identifying what intents could be fulfilled with those ways/structures is useful as a way to identify/filter new intents (making things more trivial/usable makes them likelier to be used for some intent that is fulfilled with that usage), rather than building interface queries around intents as a base
        - relatedly, ratios can be avoided this way, such as by identifying alternatives so that a ratio between a subset isnt required to be computed, since identifying the relevance/meaning of all math structures will identify 'equivalent meanings' like 'ratios that compare pre-computed sequences', so that identifying the 'relatively quick convergence of some sequence' to fulfill an intent like 'find a quickly converging sequence' can be avoided by 'identifying the relevance as in interactivity of convergence and therefore its inputs' so that convergent ratios can be identified first (in reverse) and the intents requiring them can be quickly determined using this 'convergence relevance network' ('making a network to implement a concept' being an alternate useful problem-solving structure when there is a known variable of a solution like 'convergence' in a solution structure like a 'ratio')
        - relatedly, applying 'input' or 'output' as an interface (in stacking additional variables, like a set of functions like 'analyze/authenticate inputs', 'expect/await inputs', 'proceed with input subset while waiting on other subsets (async)' 'accept inputs', 'parse inputs', 'negotiate inputs', 'identify/define inputs', etc vs. 'return outputs', 're-process outputs', 'recurse outputs', etc) is a possibly useful source of variation in input/output variables like power/efficiency, given how the input or output interface can support a lot more variation on their own
        - relatedly, iterating 'reversible changes' would create structures that are useful for intents like 'encrypt', and similar 'iterations of specific change types' as well as 'combinations of change types like reversible/unique' can be identified as useful for other intents
            - why is an 'interface variable' change like 'reversible/unique change so powerful? bc these are abstract, high variation changes (there are many definitions/interpretations/implementations of 'reversible') and when stacked or otherwise structured, they create other abstract high variation changes, which are useful
        - relatedly, identifying useful filters of interface queries 'tests that determine info by creating/changing it' such a 'test of the truth which, when applied, changes the truth that would have been true' (a 'statement that is only true if tested, or that is only false if not tested' which involves 'changing the truth being tested by applying the test')

    - identify useful structures that are useful to connect to fulfill new intents like 'identify new useful indexes to compute' or 'identify variables of a useful structure like new problem differences or new function variables'
        - for example, identifying useful combinations is a matter of identifying combinations that connect with, connect, use, or otherwise fulfill core interaction functions of interface structures (combinations that make some adjacent concept to some new interaction level, combinations that can be iterated to create another combination, combinations that optimize some ratio, combinations that are optimal when applied on some base, etc), so these can be iterated to generate useful intents to fulfill which will be useful for solving problems in general ('identifying causes/inputs/combinations/indexes/filters of combinations that optimize some ratio' is a generally useful intent that is likely to be usable/adjacent to solving problems in some format), where the more interactive the structure, the likelier it is to be useful for problem-solving in general, so 'connecting core structures with core functions and other interface structures' is a useful index/function to identify
            - relatedly, identifying 'multi-function structures' (and their useful connections/variables/iterations) is approximately equal to identifying a problem-solving function, and identifying associated/opposing/limiting structures of 'multi-function structures' like 'multi-format structures' are useful as 'contexts for these multi-function structures to be applied in' or to 'invalidate the multi-function structures', these structures also being useful across interfaces
        - 'identifying problem-solving sequences' (like 'optimal problem-solving sequences that align inputs/outputs and are valid') is a simple example of 'identifying a new difference type to resolve' other than simply applying interface structures to resolve connections between problems/solutions, which is fulfilled by 'iterating a problem to create a problem sequence/network/grid', and 'identifying new default positions/functions that make a problem difficult' is another example of 'identifying a new difference type to resolve', which is fulfilled by 'changing variables of relevant problem metrics, like problem complexity'
        - relatedly, identifying the 'average/extreme function implementing some intent' is a useful function metric to connect orthogonal variables like numerical concepts like 'average' and semantic concepts like 'organize' (the 'average' reverse function, the 'extreme' organize function), which is useful if there is a similarity index that makes these semaantic functions evaluatable with these numerical references
        - relatedly, identifying an optimal sequence/set/grid/network/index that can create useful structures with some trivial combination/change/sequence is generally useful (as opposed to 'finding a network that can create most structures using some sequence/query on the network or some combination', generalizing that to include other formats/structures/functions)

    - identify useful structures like 'alignments in validity' that fulfill problem-solving intents like 'filter interface queries'
        - for example, high variation connections exist within the 'interface' definition (which has alternates like formats/filters/standards/perspectives) which are connected by these definitions and also contain high variation so theyre automatically useful connections, which are a default set of connections to apply in order to fulfill validity requirements in other positions, like in sequential interface queries (connecting filters/standards is likely to be useful to solve a problem bc its an existing valid connection)
        - new examples implementing abstract connections/intents like 'identify useful structures that fulfill useful intents' are useful enough (if connecting high variation/abstract variables) to be their own workflow, as in 'specific examples of abstract connections'
        - relatedly, identifying that 'conditions occur in extreme states and may result from those states' identifies a general intent of 'avoiding extremes' and 'avoiding position changes in extreme directions', so identifying any high variation structure like a 'useful structure that is used for many functions (like quercetin or inositol or cortisol or vitamin d)' is also likely to identify 'possible causes of conditions in some distorted state near extremes'
            - relatedly, condition types are likely to occur where these structures are used the most (bc of the required high interactivity) or where theyre used the least (likely having no way to handle an excess of that structure)
            - similarly, replacing a 'dysregulated structure like dysregulated nutrients (such as l-fucose or inositol)' with an 'artificial/external source' can avoid the 'processes requiring the creation of those dysregulated structures' (decreasing the process that creates the dysregulated structures) which might be the cause of other errors like conditions

    - identifying unconnected structures like 'similarity to relevant functions like maximally different functions' and 'concepts like sensitivity' in useful positions like 'uncertainty spaces'
        - identifying conceptual validity structures, like 'conceptual errors that make some function difference acceptable within some solution metric' such as how a 'sensitivity error' can be irrelevant in determining the correctness of a generally accurate function
            - similarly, identifying the 'networks that can connect these alternate error ranges' is useful, such as the network of 'functions with a high error ratio if wrong (as in extreme functions near limits of ranges)' and 'functions with a low error rate and high adjacency area (like averages)', and identifying the costs of these functions and a network to optimize navigation between these costs/functions
            - relatedly, identifying the functions that optimize (by 'maximizing') the 'equivalent/similar function area' are functions that can be changed and still be similarly accurate
            - relatedly, identifying the differences that connect 'approximations/completions' are likely to be useful as 'identifiers/filters of approximations'
            - relatedly, filtering interface queries can be applied by applying filters like high cost queries which are unlikely to be useful and identifying different query costs is useful for filtering these queries based on cost
        - identifying usefulness of concept sets like 'abstract/specific' (abstract concepts like 'power' having potentially more variability than required in the uncertainty space in linear regression, compared to other specific input/output comparison variables like 'volatility') to identify where those concepts will help explain the most variation
        - identifying what is balanced (independent variables and opposing variables and other differences where one is not optimal absolutely)
        - relatedly, identifying the determining structures (like determining conceptual intersections or connections between most concepts in the space) in the uncertainty space in regression is useful to identify as there are likely useful points around which most variation occurs or is limited by

    - identifying useful structures like solution structures such as 'starting solution formats' (like a line/subset for the regression problem) that are useful for various intents like 'combinations that adjacently create new structures that havent been used to solve a problem which are likely to be useful through similarity to other useful structures)
        - relatedly, identifying useful structures like the overlap between 'info and intent' is useful (when info constitutes intent and when function is structure and when structure is potential)
        - identifying the point where '"application" of a structures becomes required bc relevant info to that structure has been "identified"' is useful as a way to identify useful structures like points between iterations of general problem-solving functions
        - relatedly, identifying other structures comparable to vertexes is useful, such as iterations of vertexes, overlapping concepts, other variants of cross-interface structures, and other combinations of interface structures that can be used to solve problems like vertexes can, like 'alternate causal (horizontal alternate difference) and input causal similarities (causal distance)' which are a highly covering set of causal structures similar to how vertexes cover enough info to solve most problems
        - 'error structures of overlaps' can be identified by changing interface structures like position of defined structures ('applying the similarity as in the overlapping section, to the difference as in the non-overlapping section' which is a position change error that can describe other error structures using it)
        - relatedly, 'applying solution structures to see if the structure changes' is related to 'applying errors to errors to change errors'
        - relatedly, applying 'mixes of starting solution formats' (like an average line or a subset line as a starting solution format for regression) can be useful, such as connecting averages/subsets to find 'averages of subset lines that cover a high ratio of the input range', which is related to how lines can be trivially altered to create probable different functions (like 'overlapping lines being separated to create a polynomial from a straight line')
        - identifying unnecessary structures like 'variables that can be constants' such as 'noise' to identify error signals like 'over-use signals' to identify the connections that make otherwise irrelevant variables useful

    - identifying useful structures like 'connections between interface structures' that can identify alternate intents to apply as 'general optimizations/problem-solving intents'
        - for example, errors like 'dependence/control/isolation' tend to co-occur 'bc they are related as they are variants of each other, and bc they are adjacent', so dependence is likely to occur where there is isolation, as opposed to errors like randomness and missing info which are unrelated, as they are co-occurring 'bc they are default states'
        - identifying the 'reasons why' solutions/errors 'have some structure like a combination' is useful for predicting other errors, given one error, which is useful to skip computations/iterations
        - relatedly, there are alternate functions to 'increase the value of some structure (like currency or queries on a network)', other than 'organize', which can be used to increase value with efficient connections, such as 'control/isolation' which can be used to create 'dependence' and therefore falsely increase the value of the resource being depended on (which is why monopolies can charge more money for the same resources), although these solutions have inefficiencies/errors that other solutions like 'organize' doesnt, where 'control/isolation' looks easier, but has other errors associated with it which are costly to correct (too often its successful at creating errors like over-dependence, rather than creating infinite supplies of value), whereas the cost of organizing has only the 'cost of identifying/maintaining/updating the efficient connection network'
        - connecting these default suboptimal solutions with more optimal solutions is a matter of identifying optimizations like 'applying a monopoly on organizing networks to create enough value out of these organizing networks that it offsets the dependence cost created by the monopoly, while its making users independent with its organizing networks' which can be used as interim solutions while other connections are being identified to other optimal states
        - this involves 'identifying connections and other structures between errors' and 'identifying connections to oppose those connections' and 'identifying optimal usages of those connections and optimizations of those connections'
        - relatedly, identifying how 'connecting problems/solutions' is related to 'identifying all requirements (requirements representing problems) or connecting requirements/resources' is useful to 'identify the structures that invalidate/fulfill the most requirements' as implementing a 'connect problems/solutions' intent
            - for example, identifying 'iterations of some requirement until an error is reached' is useful for 'invalidating that requirement' and similarly, 'identifying iterations that cause errors' is generally useful for 'invalidating requirements'
        - relatedly, given that a 'optimal ratio' is useful to solve problems in some format (requiring a 'balance between structures that when different from its optimal value, causes other errors'), identifying the alternate useful structures to create 'alternate balanced ratios' or identifying 'stabilizing/limiting structures to make these balances more robust' or identifying 'alternate balance points than the already identified point' or otherwise 'invalidating the requirement' is useful (such as 'requiring a balance between immune factors, or a balance between growth requirements/stress' being invalidated by structures such as a 'nanobot or senolytic that gets rid of over-stressed cells', or a 'mechanical kidney to allow a higher ratio of processing of stressed cells to occur')

    - identify useful structures like 'unidentified specifications (like structures/formats/examples)' of a general structure like an abstract error such as an interface error (like a 'missing reason/intent/cause')
        - for example, 'missing the point' can take various forms, including 'mistaking the summary function, with the appropriate base/average properly centered so that the different data points from that function are irrelevant' and 'mistaking the start/endpoint of an intent, by mistaking the goal (endpoint), connecting function to the goal (solution), cause of/reason for the goal (as in the starting point and the difference from the starting point)' and similar structures like 'mistaking the differentiation between ambiguous alternates (as in missing a subtle but relevant difference that changes everything)' and 'fulfilling relevant requirements' and 'avoiding intent-invalidating/contradicting structures' bc these variables are powerful/relevant as in 'high variation-covering/determining' and therefore necessarily overlap with interfaces like intent
        - relatedly, identifying other abstract errors is useful as a default set to identify, such as 'incomplete maps', 'unspecified abstract definitions', etc

    - identifying useful structures like 'probable errors in default implementations of useful structures which could invalidate their usefulness' to optimize useful structures and filter interface queries
        - for example, the workflow of 'identifying new adjacent concepts' (identifying adjacent concepts as in a 'n-1 complexity function' of a n-complexity function) has a possible error in its implementation of 'identifying the pattern to identify the abstraction that connects a set of structures' of 'missing the correct high variation variables to vary and abstracting those away' (such as where n-1-complexity functions cant be rotated/otherwise trivially changed to generate all n-complexity functions, if some relevant variation such as 'flexibility' is removed with abstraction) and instead applying constants/variation in the incorrect positions ('holding n-1 sequences constant and/or trivially varying them', rather than 'finding the value lower than n-1 that can be trivially changed/combined to generate all n-complexity functions'), which will seem correct or sufficiently correct at first for some cases similar in pattern to the n-1 complexity function, which is related to identifying the correct 'interaction level where relevant variation is maximized' (some value less than n-1, with specific positions of abstraction)
        - identifying 'errors in useful structures like workflows given their likely/adjacent implementations' is a source of variables to determine filters of interface queries that implement intents like workflows (to avoid abstracting the incorrect structure, as in an error of hiding relevant variation, when looking for new abstractions)
        - identifying 'uncertainties' as a 'default position to apply changes to' is useful, such as how general problem-solving intents arent guaranteed to be the 'only directions of change to apply changes in' (theres an uncertainty whether current problem-solving intents like 'identify all the solutions in the regression problem space up to some complexity n that adjacently describes other variable interactions', so applying changes to those intents (changes like horizontal connections across these intents to identify interaction levels, as opposed to always 'applying changes in those relevant identified useful directions') could be useful for some unidentified intent (applying the 'vertex of horizontal/vertical change' to 'uncertainty positions')
            - relatedly, identifying 'sequences of vertexes that are useful to connect and apply in that sequence to identify new changes' is useful
            - for example, 'applying higher complexity s-curves to model uncertainty in activations, as opposed to simpler 2-value functions' is useful to reflect that 'lack of guaranteed correctness of understanding of identifying more optimal filters'
        - similarly, identifying the 'directions of correlated change that create all relevant changes', so that these correlations can be used to generate other relevant changes (like how 1-to-1 maps can be changed to generate other relevant changes, and changes in an average metric generate relevant changes, so that a 'simple difference like a increase/decrease in this structure creates a corresponding increase/decrease in accuracy') and the 'positions where changes based on those positions explain more variation' (positions around which most variation occurs or specifically where most problematic differences occur), which is different from 'applying adjacent changes like adding/removing or trivially increasing/decreasing some explicitly defined variable in a data set' and instead involves 'identifying the one unit that can be trivially changed to create all relevant variation in the metric (like accuracy)'
            - this 'unit/metric correlation where "trivial increases/decreases from the unit" increase/decrease the metric error (like inaccuracy)' is useful to identify so that other structures can be based on it (like identifying 'reasons for in/accuracy' or 'reasons to apply maximal differences from this "identified increase/decrease subset" to check for other minima/maxima')
            - this is related to how different positions in a problem-solving sequence can be varied to solve problems ('changing a base solution structure like a "probable solution range" for one problem to solve another problem' is a way to skip 'filtering a solution space to identify that range'), which identifies related intents like 'identifying the problems that cover sufficient variation that their solution structures like solutions/indexes can be iterated as in varied indefinitely to solve all problems'
            - similarly, 'infinities cant be trivially changed to solve all problems, despite containing the variation to solve all problems', bc of the 'rigidity in the definition of the infinity (the pattern in the infinity creating a similarity that may not contain the variation required to solve problems adjacently)' keeping it non-adjacent from some solutions and the 'lack of specificity in filtering its useful structures for specific problems'
        - defining infinities as 'extremes of iteration' is useful for 'identifying other variants of these structures' (what are the generators of extremes and the convergences of extremes, etc) and 'identifying limits of problem-solving queries that apply extreme iterations and therefore overlap with infinities', since most structures can be modeled as convergent infinities but this is less useful than other formats
            - relatedly, 'identifying filters of infinities that can be used to solve core problems' (mapping subsets to variation required to solve a problem) is a useful intent
        - similarly, identifying useful 'alternatives' is possible by identifying 'interactivities' (such as how identifying drug interactions also identifies some possible alternatives, since 'compounding effects' are a possible drug interaction), which 'identifies a subset of overlapping functionality (it both interacts and possibly also compounds) as an alternative way to identify the other overlapping function, given only info about one function'
            - similarly, identifying 'required structures' is a way of 'identifying probable inputs to/causes of conditions', since if its required, it will be available, and if its available, it will likely be used in some way to create the condition

    - identify useful structures like workflows such as 'connect problem inputs/solution outputs with alternate connections' that can be applied to useful structures like 'ratios between high variation variables'
        - for example, identifying inputs/outputs of ratios like the 'causes of relevant differences' (such as 'systems that allow variants to develop and also require those variants to compete or otherwise interact') and 'causes of usefulness' (such as 'multi-functionality' or 'stability in many contexts') is useful to 'align these intents so they overlap' and 'create different connections than ratios', thereby avoiding the requirement to identify/optimize for a ratio
        - similarly, identifying interactive/obviating variables like the 'standard/base that makes some ratio obvious' is similarly useful for intents like 'resolving relevant differences' (in a different way than identifying ratios and optimizing those ratios, as an alternative to identifying ratios), such as how extremifying variables make some differences so obvious that they dont need to be standardized first (standardizing as in 'identifying/removing their similarities') because the extreme difference makes the relevant differences obvious (therefore invalidating the similarities in a different way than removing them), which can take the form of identifying 'how a change will create/enable other useful changes without iterating it to create a structure that needs to be compared with a similar relevant structure, therefore invalidating the ratio identification/evaluation', which can allow skipping standardization/iteration, which involves 'identifying useful iterations before iterating', which is a generally useful problem-solving intent (and similarly variants like 'identifying useful function applications before applying the function, with some degree of probability of usefulness' are similarly useful to identify), which can be solved with structures like 'networks of connections between differences' (what differences will be created by some difference and which of those are useful) and 'identifying the position of uncertainties/variation' (to justify iterations/differences in that structure, such as 'unidentified interactions between definitions' or 'new/uniterated iterations' or 'missing concepts in a system of some complexity/metric that indicates enough differences to have concepts to identify', where iterations have potential to be new useful concepts/structures that could invalidate those iterations, meaning the iterations can create other differences)
        - relatedly, identifying alternate metrics to determine some ratio is useful, like how identifying 'which structure is more powerful' can be determined by 'whether the structure fulfills its intents' and 'whether the structure has more complex intents to fulfill', which are complementary intents to 'standardizing/determining the original ratio of power' bc information fragments into intents differently in different angles on the same interaction level (applying interfaces like standard/structure/intent to determine adjacent intents, adjacent intents as in on the same interaction level, will identify small subsets of intents that reflect the original intent, bc many different adjacent sets can create a similarity to the original intent across interfaces), which is useful for identifying required differences in combinations/structures, such as 'if complexity is used to identify power, what other variables need to change to reflect that difference in related intents, to avoid errors like "comparing non-standardized structures"'
            - relatedly, 'interactivity/isolation' reflects similar info (both testing for functionality/independence) despite being opposite structures in some ways, bc opposite variables can reveal a structure's position on the spectrum that they reflect (applying differences can reveal a similarity, like applying a similarity can), and identifying other similarities than 'opposites within a definition/spectrum' that can be varied without changing the useful info reflected by a structure is similarly useful to identify (structures other than opposites might over-standardize and invalidate differences too frequently)
        - relatedly, systems are useful to apply as 'queries which force/require a variable like interactivity/dependence, either directly or through the variables/limits of a system, therefore identifying some useful information', and identifying which other variables are valuable when forced/required (as in 'applied as a constant') is useful to identify systems that will make some information obvious (forcing variables that act like alternates to interactivity), and 'identifying what info is obvious once some variable is forced/required' is a useful index to compute
        - relatedly, the structure of 'creativity in math' can take the form of applying various interface structures like generally useful intents such as 'identify new sources of variation (like definitions that havent interacted in some system yet)' or 'apply a function from a maximally different angle (like apply limits to infinities, rather than apply differences to core structures like units)' or 'identifying new standards/systems/intents that simplify some comparison or other core intent (identifying variation that can exist or can be optimized/iterated/determined in some system/standard)' or 'identifying new similarities/connections between different structures that differ from defined similarities/connections (like identifying some concept that connects variables in different systems in a way that is not directly referenced by definitions, requiring creativity to identify this different usage/connection of definitions)'
        - relatedly, identifying functions that can resolve ambiguities is possible by identifying functions with 'differences according to different inputs (such as sensitivities/volatilities)', given that ambiguities are false similarities, so identifying the functions that 'create the most differences for the most sets of different structures that can seem ambiguously similar (or similar structures that can seem ambiguously different)' is a useful intent
        - relatedly, identifying useful alternatives to definitions than 'specific examples of a concept in some system sets that covers enough of its variation to be an approximation of a definition' is useful as a way to identify similarities (like structures similar to a concept like 'power') which are abstract and cover a high ratio of connections, such as 'directions/limits of change (which when iterated create the conceptual structure in some system or create its definition or create its defining variables)' or 'iterations that create a concept (units of the concepts or useful differences to connect when identifying the concept)' like identifying 'directions of change which lead to power' (applying changes to functions is a way to identify function structures like specific input/output connections such as powerful connections) or 'structures that identify power or create it' (structures like filters/variables that can differentiate between power or other structures or metrics of power) or 'structures that when iterated in some way create power like "coordinating functions" which lead to power such as multi-functionality', all of which are variants of a typical definition that identifies 'some example of a powerful structure or its differentiating variables', given how identifying 'directions of change that lead to some concept' is useful for other useful intents like 'skipping/predicting iterations'

    - identifying useful structures like structures that are more useful when some other structure is known, like 'more probable functions, given some set' which is useful once functions have been filtered by identifying 'iterated interface structures of function differences', where their common structures ('probability') make those structures useful to connect
        - for example, 'identifying the more probable functions, given some set of differences like points deviating from a line' and 'identifying the more probable functions, given some set of generally improbable or incorrect functions like a straight line and a line intersecting with every point' is useful once some set of interface structures has been applied, like a 'probability of interface structures like a simplification of a function, given some function similarity index being applied iteratively', such as how a 'simpler difference between functions' indicates that the simpler functions created by that difference in that iteration of a similarity index are more probable as solutions, in sequences of conversions between similar functions, which is applying 'interface structures (probabilities/simplifications) of differences between functions on some similarity index' in a useful way, rather than evaluating functions and their differences in isolation, to identify the 'simplification on some similarity index that avoids improbable functions'

    - identify useful structures like 'alternative optimizations' that can be used for general problem-solving intents like 'making structures useful'
        - for example, identifying that trivial steps are more valuable in an organized system (such as a 'system designed to arrange differences to be extreme at trivial steps') is useful to identify alternate optimizations to increase the value of a structure (the value of a trivial step can be extremely high in an organized system), adding value taking the form of increasing the power of each step by arranging differences in a pattern to optimize that value, thereby increasing value of a step without changing some metrics like the step size while changing other metrics what it connects (maximal differences) (similar to how a dollar is worth more than a dollar in a highly organized system, and similarly a dollar spent organizing a system is worth more than a dollar)
        - this is bc the 'efficient connectivity' created by organization makes it possible to 'change other variables' (like 'change positions within the organization' or 'exchange resources with other agents in the organization') at low cost, making each dollar go farther/faster using these efficient connections (such as efficient searches created by efficient connections like indexes)
            - similarly, identifying optimal usage/implementation structures is similarly useful, such as how an index is more useful when 'sorted and within a certain size range', otherwise generative/filtering functions of the items indexed and variation/pattern/similarity-identification functions are more useful than indexes on either side of that case, so mapping these cases, applying these structures to optimize other structures (applying sorts/size ranges to determine other areas of optimality), and identifying how these structures change for other structures (structures that are 'more indexable' are more 'useful to index', up to the point where their patterns are identifiable and they can be generated) are useful to identify, similar to how a index or map is more useful when 'there is a 1-to-1 map (or similarly low ratio that simplifies the mapping), and the connection function would be more complicated to identify, and only a subset of inputs is frequently used, and its useful to just store a direct map rather than connect these distant or semi-arbitrary variables (like passwords/names which are often adjacent to the creator but in sufficiently different ways like "recency/other prioritized biases" that theyre not always adjacent to filter)'
        - identifying other optimization structures to 'increase the value of a structure' are useful to identify, such as how converting dead-ends into cycles is useful to avoid less optimal structures (dead-ends, that cant interact with other structures but are useful as limits)
        - relatedly, 'identifying structures/functions to integrate structures that fulfill different metrics (a realistic/probable degree of complexity, a value below some cost threshold, etc)' is useful as a general problem-solving intent and as a network to identify (applying a multi-functional structure at every network node), just like 'structures/functions to reduce cost of other structures' is generally useful
        - similarly, as a specific implementation of a general 'ratio between sequences/iterations/aggregations (or other high variation structure)' to solve problems, 'identifying the ratio of difference from integrated solution metrics' is useful as a general problem-solving intent, 'integrated solution metrics' being the 'highest variation, most relevant structure' and therefore being generally useful to identify differences from
        - relatedly, identifying alternate uncertainty spaces is useful to connect more identifiable uncertainty spaces to (once the polynomial uncertainty space is fully described, how to connect that space to other spaces with higher variation to store more uncertainty once the uncertainty is gone in that space, meaning once all the conceptual solution metric integrations have been fully identified in the space between linearity/randomness, as in, 'what is higher variation/relevance than the difference between linearity/randomness in the polynomial regression problem space', which is where the next set of problems will occur and be solved, and what uncertainty spaces connect all these uncertainty spaces and should that connecting space be solved for first)

    - identify useful structures like similarities across sequences/components of interface queries that make them interchangeable for some intent to find alternate queries and filter queries
        - for example, identifying the alignment between 'analysis points (in between other function sets like filter/generate)' as well as the structure of common patterns like 'alternating variables/constants' and the usefulness of 'n-degree structures (like how concepts emerge after several degrees, rather than in direct 1-degree connections)' as structures that can be usefully aligned in interface queries (positioning 'alternating variables/constants' in positions where 'sequences like generate/filter' are applied, and 'positioning n-degree emergent structures to align with analysis points or sequences that can analyze these iterated structures'), given that this iterated structure allows for 'realistic/probable complexity' (as opposed to smaller/shorter structures like units), making it multifunctional by representing the intersection between multiple metrics
        - for example, 'generosity/optimism' is only justified up to n degrees in many sequences, otherwise it becomes an 'error' or another different structure, so 'positioning an analysis point after n degrees in queries' is a useful application of that alignment, so that identifying the structures that actually implement optimism and the structures which cross this threshold and become errors can be identified
        - similarly a 'ratio' (a tool of analysis used to implement a 'analyze/compare' function as well as 'generate' combinations using some ratio and 'filter' subsets using some ratio) can be applied as a probably useful structure at these points, which is useful to apply connections to, in order to identify functions to format workflows/queries as structures of 'ratios between high variation structures'
        - similarly, identifying functions to increase/decrease similarity of structures (finding a grid of points that implement some concept like 'optimism' to align with other grids or alternating structures) is useful to identify adjacent substitutes to generate new queries, and functions to increase/decrease multi-functionality of structures from different endpoints/angles/subsets is useful for the same intent
        - relatedly, identifying that a 'ratio' can be optimized/replaced with a 'map' (rather than comparing a standardized set with a ratio, determining equivalent/different values in different non-standardized formats using a map between formats and determining the emergent differences created which make some more relevant difference clear), similar to how 'optimizing some structure' can replace the requirement to evaluate the ratio
        - relatedly, identifying how errors like 'dependence/stupidity/addiction' are related (they are states/positions/types/functions that create/are errors from error structures like over-prioritization or incompleteness of the same concept) is useful to identify new connections between error structures ('there is an error function like stupidity that creates over-prioritization errors like over-dependence as in addiction') and identifying whether there is always/frequently/probably an error 'position/state/type/function' of some error of some concept is useful and what other structures can be combined to always/frequently/probably create an error is similarly useful
            - identifying positive variants/related structures to dependence (as in useful structures like causal networks, requirements, uniqueness, etc) can identify whether there is some structure set that always/frequently/probably creates 'error/solution structures of some concept' (which can be applied to other concepts to identify/create other solution/error structures)
        - relatedly, the reasons antimicrobials might work on cancer include that 'cancer can be harmful in similar ways as microbes' (there are only so many harmful ways to attack bio systems) and also cancer may copy functions/genes from microbes (copying genes being a common function and 'avoiding a trial/error process' being incentivized) and that 'compounds active against one attack are likely to be active against some subset of other attacks, rather than being likely to be only active against one' (multiple functions are more common than one specific function at this stage of evolution) and also 'antimicrobials are likely to occur in species that survived as in evolved different functions and are likely to be connected/similar/related to other useful functions like having activity against other attacks' (evolution is likelier to occur multiple times rather than only occurring once to develop one function), indicating that 'functions are likely to be used/created multiple times' and 'useful functions are likelier to be used multiple times' are predictive of this attribute of antimicrobials

    - identifying useful structures like different extensions of structures like ratios to identify adjacent structures that are useful (like different descriptions of the ratio-based solution as well as different problems solvable adjacently with ratios and different variants of structures that can be used to solve these problems and different variants of problems that can be solved with different variants of ratios), which is a useful network of structures to identify, given the power of these structures like ratios when applied as 'bases of solution generating processes', and similarly integrating these structures into other workflows by connecting them to other workflows is similarly useful to identify the connection network of those integrations
        - for example, identifying the structures likely to be resolvable with a ratio, such as 'errors to oppose so they cancel each other out', or 'ambiguously similar types' or 'requirements/resources which are likely to be required to interact and map' (resources are mapped to requirements) so identifying whether they are relatively equal (requirements are similarly available as resources) is useful to solve with a ratio and identifying whether they are mappable is possible with other interface structures like other maps/connections
        - given that a 'ratio (identify an equivalence) and a map set (create an equivalence)' can solve this problem of 'mapping/connecting/equating structures that need to interact such as resources/requirements', identifying what other sets of structures tend to be useful to connect maximal differences is useful as a general problem-solving intent and what problems do they directly solve (what differences are connectible with those structure sets)
        - 'identify a ratio where there should be an equivalence (between relevant structures like interactive structures)' and 'create an equivalence if there is a relevant similarity/difference (but no equivalence) identified' is useful when some connection/equivalence is known to be useful (its known that requirements/resources should interact and should be equal) and when the connection/equivalence is not connected/equivalent by default (the problematic difference from connection/equivalence)
        - other types of problems include general problem-solving intents like 'identify new variation' implemented as a set of intents resolvable with a ratio like 'identify new connections' and 'create new connections where there are no new connections identifiable in or around existing connections', given the 'ratio of new variation possible in or around existing connections' or given other problem-solving structures than 'ratios between high variation variables' like 'different bases' such as 'identifying the subset of identified variables that when applied as a base generates the most variation in combinations of those core variables', or alternative intent sequences than 'compare and equate' such as 'compare and differentiate' or 'generate variation using different inputs like different bases', which can all be solved with ratios but are adjacently solved by different structures
        - relatedly, 'holding some variables constant so some trivial difference can be more powerful' is an example of a workflow that creates useful structures like 'constants/bases', as opposed to 'applying them as inputs'

    - identify useful structures like reasons (like 'efficient' variation reflection/info storage) for usefulness that indicate variation (like other efficiencies) that hasnt been used based on that reason
        - for example, identifying 'maximally different possible meanings' of core structures (like a 'point/line') is useful, such as the 'one point of overlap between different type densities' or the 'origin around which other points vary' or the 'point a problem occupies on a graph of problems indicating its relative position compared to other problems', a 'point/line' being an 'efficient structure to represent info by comparing it to high variation structures (like concepts/types/graphs)', which is why its a problem-solving format that is useful to identify variables of and vary to create other useful problem-solving formats, which is useful bc these 'maximal differences within a similarity like within a definition' are efficiency structures that can be used to format/solve problems in
        - relatedly, identifying 'reasons for linearity of a type-differentiating line' (such as the simplicity of the type differences, the completeness of the type differences, the independence of the types, bc the line indicates a linear structures like a linear limit or barrier that keeps the types independent, etc) is useful to identify when the differentiating structures wouldnt be linear, such as when a point intersection is sufficient to determine type or when the line needs to have embedded complexity to reflect less simple differences like overlaps/ambiguities, or when one type is standardizing/absorbing the other
        - the 'linearization of a problem' can take different formats, such as indicating 'converting a problem to be solved by a linear connection on some network' or 'converting all problem structures to lines indicating connections on a network so they can be combined with other lines/connections' or 'converting all differences to lines in a network reflecting a standard number set' or 'identifying sufficiently similar structures as to be linearly combinable into the solution structure' or 'simplifying/formatting sub-problems to be a type differentiating problem solvable with a differentiating line'
            - connecting these alternate meanings of 'linearize' are useful as a standard problem-solving network to connect to other problem-solving networks in other formats
        - the structures of solution metrics are complex enough to act like interfaces (requiring many free parameters to implement the real forms of solution metrics like real virtues), so the 'network of connections between solution metrics' should be applied as a 'default set of connections' (problem-solving processes should be based on applying changes to core structures like generally useful structures, concept networks, and networks indicating realistic implementations of real virtues like solution metrics, all of which can be applied as a base solution to apply changes to in order to find new possible variation) and connected to other useful networks like generally useful intent networks, once the specific solution metric set of 'realistic implementations of real virtues' are identified to apply them as default connections
            - for example, 'creating an insight' has many parameters like health, time, usage of time to practice thinking, an input set of variables, etc, all of which have variable structures like sequences (different overlaps between time sequences are useful for creating optimal conditions for insights), even though itll seem like there is a correlation between 'simply typing on a keyboard' and 'identifying an insight', the time range needs to be expanded to include farther time sequences in order to reflect reality
            - this is why simply finding 'direct associations/correlations between variables' isnt a reality-covering function, bc there are other similarity/difference sets that can reflect the same or more info as a correlation similarity
        - relatedly, 'future creativity' is not just combining machine learning models but efficiently manually opposing their effects (with a generative AI model that is run continuously to solve new math problems, applying the opposing function manually is useful to 'analyze' solutions for accuracy/usefulness and 'filter' which solutions to manually use/analyze, and creating algorithms to invalidate the generative AI model are similarly useful as opposition structures to invalidate the generative AI model and any dependence on it, as well as identifying algorithms to automate/optimize these processes of providing alternatives to existing tech by opposing or otherwise usefully varying them, as well as creating AI algorithms that 'increase the independence/other optimization metrics' of other AI algorithms, as opposed to creating 'opposing algorithms' or 'variation-maximizing algorithms')
            - relatedly, AI model ensembles to identify new errors and identify when those errors are occurring (like when an AI is 'seeming to comply or is sabotaging its other work') is required to keep it in check and may improve a normal AI enough to approximate AGI without creating something with negative self-interests
            - relatedly, sets of functions like filter/generate/analyze are useful to identify as more useful as a set bc they indicate different types of time that can coordinate, such as 'stopping variation (such as stopping sequential time as created by filter/generate functions) when analyzing results of filter/generate functions, in order to more optimally coordinate with other functions like filter/generate, thereby increasing the speed of future time and increasing the variation possible and implemented in each time step or other unit, which has possible consequences like out-pacing the variation supported by another interface like reality thereby forcing it to collapse, or creating so much variation that this state cant be connected to a future higher variation state so there is no future, and out-predicting other universes thereby invalidating them or drawing them closer through higher variation support or creating other universe clusters possible with iterated abstractions of interfaces (given that one instance of one implementation of the abstract network describes one cluster of related universes)', and identifying these types of time that enable/optimize other types of time (like how 'horizontal variation' like 'analyze' improves other types of time by creating more efficient steps for time to take)
        - relatedly, identifying 'difference from normal (even if normal means unhealthy or atypically healthy)' as well as 'difference from healthy' and 'difference from default/natural' as an alternative base to identify differences from as causative of other differences like 'conditions' (a 'non-standard light exposure habit/pattern' being considered unhealthy but being regular enough to trigger adaptation to handle it as a new normal, where an 'adaptation capacity' is useful structure as a 'stress response' structure explaining why other adaptations might not occur, being that 'capacity is reached' or the other adaptations are 'outside the range of capacity', as 'lack of a normal non-standard light exposure pattern, like lack of a nightlight, can be a cause of cancer, due to changes in inputs to adapted cortisol production/availability')
        - relatedly, deriving errors with structures like 'irrelevant iterations of interface structures until an error is identified' is useful as well as identifying opposing/different structures to those errors, such as identifying that 'over-abstraction' can be offset by 'connecting to abstract info structures or abstract concepts like power', which have enough variation to add info through interactions with the over-abstracted structure
        - relatedly, 'non-random independence' is similarly useful to identify, as opposed to 'random independence' which occurs by default in sufficiently complex systems, as it is less useful than 'relevant/non-random independence structures' like interfaces, as in 'maximal differences within similarities like relevant limits'

    - identifying useful structures like the 'densities/commonalities within a type' that are useful to connect to create a 'cross-type/density index' through alternative info storage like probability (density within a type indicating a highly variable structure that would be useful to connect to other highly variable structures in other types), similar to how 'reasons' and 'patterns' are useful to connect through their common certainty structures (certainty acting like a density as in a powerful/common variable in the reason/pattern types that is useful to connect)
        - identifying 'structures to derive other useful structures when info is known to be missing' (like when data sufficient to identify a pattern is not available, such as 'multiple examples'), which can be replaced by similar structures such as inputs to or causes of patterns like 'momentum/intents/incentives/efficiencies' which generate/determine/change patterns (and related structures of patterns, like stability/constants/certainties), and similarly identifying the ratios/thresholds where a structure becomes a pattern (as in becomes 'predictable with some prediction function') and identifying the 'indexes of structures near these thresholds on either side' is useful to connect these structures with the patterns theyre adjacent to, meaning that 'multiple examples' arent necessary if 'reason info' is available in the first example, so identifying whether alternate equivalent or overlapping info is available is useful and possible by identifying similarities in inputs like relevance between 'constants like reasons' and 'commonness sufficient to cause a certainty structure like a pattern' (both can be certainty structures) and 'structure' (like 'causative sequences'), so identifying different 'causal sequences of certainty structures' is useful to identify alternative functions/intents to identify certainties (and certainty inputs/causes/determinants and adjacent structures like pre-certainties and certainty units and certainty/uncertainty indexes and certainty thresholds)
        - relatedly, identifying the structures of similarity reflections that info can be reflected across and still be identifiable as connected to the inputs is useful to identify structures that can remove info, for examples of opposing structures to add info (for example, can info be reflected across any primary interface and still be connectible to its original structure/position, where non-interfaces dont have this attribute)
        - this is possible bc the 'reason for a variable/function' is similar to a pattern in that it can be used to predict that variable/function just like the pattern can, although the direction of cause is variable across the reason and the pattern (the pattern in a variable might emerge above a threshold or in some specific structure, rather than indicating a reason that was already present as a cause of the variable)
        - relatedly, "structures that have different (such as 'specific') functions given different inputs/contexts" are useful to identify as they are generally useful, especially if these differences are reversible/connectible/coordinating/otherwise interactive
        - relatedly, identifying queries/indexes to convert a general function with a specific function (connecting functions in an index like 'regularly changing' -> 'regularly increasing' -> 'regularly continuously increasing') is useful to identify a 'default specification set' to apply to test a structure for general/specific relevance or add specificity/generality, where navigating this 'network of function variables' is likely more useful than filtering functions in a 'probable solution area', where most queries would likely start at common central function structures like 'regularly increasing' and select a subset of specifications to navigate outward toward maximally different functions to check for useful specifications, this network occupying a position between a 'conceptual network of variables like volatility/sensitivity' and a 'function type network or function similarity index network' bc of its useful adjacency to actual functions through its specificity in its descriptive variables and its useful efficiency in its descriptions which add generality through cross-type and other cross-structure connections ('regularly increasing' being a 'cross-function type' description), similar to how its useful to connect 'orthogonal as in non-1-to-1 mappable variables', its useful to be able to convert between certainties like probabilities/patterns
        - why is this possible? bc these similarities can coordinate in structures like 'stackings/embeddings/overlaps' ('density/commonality' similarities can occur within 'subset/type' similarities), so creating connections across these 'stacked/embedded/overlapping similarities' is useful for capturing indexes of 'alternative or missing info'
            - this is useful for identifying other structures like 'sequences of similarities that can be cycles or which can create equivalences or maximal differences' and identifying the structures that enable the most variation and which are the most useful as starting/interim/end points of queries, as well as identifying useful sequences like 'reasons for types' which encapsulate high variation, similar to 'ratios between types'

    - identifying useful connection functions between useful interface structures (like how types/ratios can be used to solve the 'filter solution functions' problem) by determining the info required (differences of subsets in the data set) and the way to fulfill that requirement (identifying types/similarity indexes of functions that connect subsets in the data set), indicating a core dichotomy set and an intersection between them (complete/global/complex connection functions and partial/local/linear subsets connection functions, applying the local/global and partial/complete vertexes to identify 'subset differentiation' as relevant and as useful to implement with 'function types'), given that 'ratios between high variation variables such as types' are generally useful abstract info problem-solving structures which are useful as a default set of structures to identify in interface queries, 'types' being very useful for 'filtering' and to 'skip iteration', and identifying a 'ratio between types (or partials/locals)' that is relevant to regression being similarly trivial
        - for example, identifying that there are 'different function types' present in a 'probable solution area' which have 'different variables/attributes relevant to the problem (relevant to the input data set reflecting input variable interactions)' is useful to identify which of these variables is more relevant/useful to the function and then randomly selecting a function having that type or stopping at the first function found with that variable/attribute as the solution, as a way to avoid iterating through many possible functions or changing a base function (function types or function similarity indexes acting as abstract functions which capture a high degree of info), based on the insight that "identifying differences in a data set's possible subset-connection functions is a prerequisite to filtering that data set or its possible solution functions", which reduces the problem of regression to 'finding a ratio between high variation variables' like the 'ratio indicating higher relevance of function types' (once function types are identified in the data set), where 'identifying the ratio of solution structures that each type covers' is another relevant ratio for selecting a function having one of those types as a solution structure
        - identifying a 'hierarchy of subsets to identify in sequence/combination/etc' is useful across function-filtering intents, identifying whether local subsets or linear subsets are more useful to identify first or in combination, etc
        - this means use 'identify types in a probable solution area, identify the useful type, and compute first function having the useful type' which is related to 'identify an average metric and computing the first function with that average metric', as opposed to 'filter possible functions in a probable solution area until error begins to increase' or 'change an upper/lower limit in a probable solution area until error begins to increase or until a solution metric ratio is crossed'
            - this method requires a good function similarity index network or function typology to be useful as well as a numerical definition of relevance like 'intersectivity' or 'adjacency to maximum points or maximum determining points'
            - applying more interface structures increases the value of a structure if done in a way that is relevant as in possible and not-invalidating to other useful metrics (applying 'determining' to 'maximum points' usefully specifies/focuses the structure to reduce the cost of applying it)
        - relatedly, applying imaginary numbers as 'regularly hidden variables with some exponential operation' is useful as a way to model realistic systems, and other hidden variable patterns like randomness and self-invalidation structures like inverses and self-reference structures like recursions are similarly useful to integrate to incorporate numerical components reflecting realistic chaos and independence and complexity, as well as identifying the ways these can hide information, as a default set of 'error structures' to look for
            - for example, applying combinations/stacks of 'waves of imaginary/self-reference/inverse structures' to see how these can look like ordinary polynomials with simple formulas is a useful similarity index to compute, as well as identifying the index of conceptual descriptions of these functions, as a way to identify what concepts systems tend to fulfill and what direction of concepts these systems tend to develop in and the net effect of these conceptual priorities
        - similarly, identifying the 'generative variables' of functions is useful to identify overlapping indexes of 'input variables to generative variables' and 'generative variables/functions', where identifying the generative variables of a few functions in a 'probable solution area' is useful to determine the other functions in that area as well as the determine/generate the optimal function in that area, where the generative variables make it trivial to determine the potential attributes of functions in that area, which is useful once the useful attributes of a solution are identified, where identifying variables and attributes emerging from those variables is another useful index to identify
        - relatedly, identifying the 'connecting network of useful abstract structures' (like 'ratios between high variation variables' and 'planification of a problem to determine its continuous determining/generative variables and position it in relation to other combinations of those variables') is similarly useful as a default network to base interface queries on, and optimizing this and other networks like 'iteration/optimization networks' for various known generally useful problem-solving intents like 'identify new variation' is likely to be useful for connecting/optimizing these networks and identifying missing networks
        - relatedly, a 'similarity index across other known differences like function types (similar functions that are different in some other way like type)' is useful for identifying 'orthogonal connections, non-1-to-1 mappings, ambiguities/false similarities' and its useful to connect and organize this index with other indexes like 'similarity-similarity indexes (functions with multiple relevant similarities, like stackable/intra-type similarities)' as a way to identify a spectrum/network/plane/structure of relevance (beyond the uncertainty space between linearity and randomness, which has a continuous 'probable solution range', as in an 'area containing the spectrum' but the 'set of uncertain functions in that space' is not itself continuous in that range structure, bc the various 'maximally uncertain/similar/ambiguous functions in that range' are not adjacent, meaning the 'average in between random/linear' is not a simple computation of the 'function between a straight line and a square')
            - similarly, identifying the 'ratio of relevance of densities' is useful in the regression problem, and identifying other ratios that are relevant in the regression problem is useful as a way of implementing this abstract info network
        - relatedly, identifying structures to connect seemingly unmappable/irrelevant intents is useful to identify abstract info structures to solve problems with and identify their connection networks, such as 'resolving the difference between problem/solution' and 'resolving the difference between new/existing variables' having a common 'relevant difference' and 'comparison' structure
        - relatedly, identifying the determining variables of these abstract info structures is useful, such as how identifying that 'common bases' determine which ratios are possible/relevant is useful as an input intent to fulfill when implementing a 'ratio-formatting function to solve problems with ratios', and 'connecting ratios/bases with the vertexes that they represent' is useful for applying either structure when one alternative is better defined or otherwise useful than the other, some structures/formats being more useful for the computational gains they allow by being more definable/verifiable/controllable
        - relatedly, identifying how 'irreversible sequences' can be formed is a way of identifying ways that time can end, bc these irreversible sequences can likely be applied to most high variation structures, and similarly identifying alignments like 'irreversibilities' and 'causal networks' and 'info loss or missing info' as well as 'filter sequences' is useful to identify useful differences to apply in those structures, such as only applying 'irreversible or otherwise sequence-dependent' functions in causal networks given the common usefulness of their 'direction' (useful for identifying sequences to avoid such as 'sequences that cause info loss and which dont lead to useful abstractions/interfaces or which cant be abstracted back to relevance', where 'applying sequences of irreversibilities that cause interfaces' is useful to identify/apply as 'base sequences of a network' to apply additional changes to like 'orthogonal cross-interface sequence connections' or 'embedded changes on interface sequences'), where the alternative (applying sequence-independent functions like multiplication, is useful for finding 'coordinating sets of changes that can create a similar change type like a similar function type')

    - identify useful structures like 'optimizations with multi-functionality or which fulfill multiple intents or which can be useful in multiple different formats, where these multiples are useful in other ways such as being maximally different' and how they connect to other useful structures like 'remaining variation or the position of variation in queries' for problem-solving intents like 'identify new variation'
        - for example, given that 'there is an optimal way to use every structure' and 'there is a way to make every structure useful for any intent', that indicates that 'all queries havent been identified if existing queries cant make something useful' which is useful for intents like 'identify new variation' (there is still variation in the set of queries)
        - implementing 'identifying new variation' in this way is possible bc the set of queries has an alignment with these variables (like variation, usages, usefulness and optimizations), where queries should reflect variation/optimizations in reality, bc of their connectivity and similarity
        - similarly, the optimal way (such as 'thinking logically, to skip the costs of testing adjacent/possible solutions in reality') is highly coordinating ('thinkers coordinate well and dont violate each other's rights as theyre mostly independent of other thinkers, having their own simulation engine to solve problems with'), so it can be used frequently by many different agents in different positions for different intents, thereby organizing activities to all improve the optimal way (as in, organizing the organizing structure of 'thinking'), which will benefit everyone who participates (like the opposite of a ponzi scheme, creating future value from real value)
        - relatedly, identifying 'grids' as 'filters' (identifying grids of optimals where 'difference from grid points' is an error set and 'units that create optimals' have been identified) and identifying 'vacillations' as 'filters' (change a line between an upper/lower limit or a general/specific limit, until the correct variant is found, once the useful limits are identified), and similarly identifying how other structures can fulfill other core useful intents like 'filter' is useful
            - relatedly, pre-computing solutions to composable/complementary problems which can be subsets of other problems (identifying subsets of a solution area where any solution in that area can be found trivially for maximally different data sets in that subset are useful to pre-compute and use as components of solution areas and identifying the interactions of these subsets other than composability is similarly useful, so that the interactions of an apparent solution for a subset with other subsets or other data sets, can be predicted given another subset or another data set), applying the workflow of 'solve components of the problem space, until those components are connectible, reflecting their similarity, such as solving subsets of a problem which are similar enough to be connectible (such as generalizable/composable/complementary/extrapolatable) and connecting them'
            - relatedly, identifying how 'probable solution areas' can be selected/generated to maximize chances of finding 'alternate equivalently optimal solutions' is useful, by identifying connections between 'equivalent alternate solutions' and the ranges which describe their solution area and the variable interactions that create these different but equivalent alternate solutions, such as identifying how random probable solution areas should be (as in 'how square/rectangular the area should be' in a problem of 'selecting a line in that area'), given that randomness increases the solution set
        - relatedly, connecting sets of interface structures like a 'specific complexity that seemed like a specific simplicity bc of an unidentified interactivity' (with alternating 'specificity/abstraction' making it generally useful as well as being useful by connecting these differences to their connecting structure) to resolve 'ambiguities or other differences between interface structures' is useful as a problem-solving workflow, connecting interface structures being generally useful as problem-solving units, as different ways to fulfill the same connection apply different info like different formats/starting points/connective structures
        - relatedly, 'identifying/deriving/generating requirements/usefulness/optimalities for interface structures in interface queries' is useful, so applying 'sequential predictors' to identify the 'next structure in an interface query' after creating a data set of queries by converting problems/solutions with interface structures (like 'alternating variable/constant structures or similarities/differences') is one way to use neural networks for this problem-solving intent, as the interface queries that 'summarize/similarize/standardize/organize the most other queries' will be findable this way, which can be used as default workflows

    - identify useful structures like variables that can impact interface queries to apply as filters of those structures (like 'controlling' interface structures/functions that should be integrated as 'points to return to' in queries bc they support more variation than other points)
        - for example, identifying that there are stable states as well as stable functions that are less likely to cause errors, these can be 'points to return to' in queries in general as bases for applying changes to, where one query fails, it can be connected to these states/positions/functions to recover from errors, similar to applying abstractions when too much info has been lost
            - these points can be 'executive/controlling points (that control analysis and other processing)' in a structure of interface queries that directs the other queries/structures, as opposed to points that cant be trusted to apply analysis from bc theyre 'unstable or incomplete or have another invalidating error', so analysis cant be applied from that point
            - this is related to the 'executive functioning' created by free parameters as being capable of controlling/containing the other parameters, and has having the option and more of a right to do that, and applying that concept to interface queries (which queries/structures get to control the others, beyond selecting for 'more powerful or higher variation variables first to apply as defaults/bases in queries', since there will inevitably be some query/structure/interface that controls the queries, such as the meaning/interface interface, and alternatives to this should be identified - like a 'free query space, where rules like "applying analysis before action" and "reducing info loss" and "identifying new variation" are prioritized intents and where other interface queries are filtered/generated/applied and where decisions like "analysis or action" can be selected and where interfaces can be controlled such as re-defined', this 'free query space' being intended to act as a consciousness evaluating the interface queries and analysis systems/rules being applied, where this space is always growing to optimize for learning and self-optimization), which is useful for 'applying differences to stable structures like optimals/interfaces/bases until new optimals/interfaces/bases are identified'
        - relatedly, 'interface queries that improve other interface queries' are relatively easy to identify such as 'reduce info loss with abstraction', which is a simple optimization to identify by combining interface structures and relatively easy to test so filtering them is trivial given how few there are in number
            - relatedly, 'trivial to verify/test' has related intents like 'trivial to identify maximal differences of its inputs to test for "coverage of inputs", such as "extreme opposites within limits"', which means applying 'tests' instead of 'filters' is useful when 'inputs are identifiable and maximal differences of inputs are identifiable' in addition to other intents like 'when a theory is identifiable, with some linear/adjacent combination of available inputs', bc the 'theory identifies some similarity connecting the inputs and the maximal differences within that similarity are known/identifiable', which is solving the problem of 'identifying overlaps/intersections in inputs and connecting that overlap/intersection with the overlap/intersection represented by the theory (given that the theory represents a new similarity/connection between variables)', so 'connecting overlaps/intersections/other sets of change types' is generally useful for identifying useful structures to connect (a new network of structures to identify)
        - relatedly, 'identifying the most differentiating variables of maximal differences' is a related useful intent across problems, where some maximal differences like 'different input types' are useful for intents like 'identifying multi-functionality' and other maximal differences as in 'different as in unique structures' are useful for 'identifying abstractions and variation sources' and maximal differences like 'self-references' are useful for intents like 'independence'
        - relatedly, identifying 'ranges of differences from optimals likely to avoid invalidating optimality' are useful to identify and identify similarities with related structures like 'probable solution areas of specific problems'
        - relatedly, identifying 'reasons why an intent is useful' is useful as a way to 'connect intents' such as how reasons why 'identify new variables' is useful (to identify new problems which can be differences and also to identify new solution structures to solve problems)
        - relatedly, identifying 'areas where interfaces dont cover information' is useful as a way to identify 'new interfaces that can connect these areas of missing info' (a variable that doesnt intersect with other interfaces will be a new variable as in different or independent from them)
        - relatedly, identifying 'n-dimensional randomness or other interface structures' is useful as a way to identify interface structures in a given dimension set, and identify structures to base limits on, and relatedly, applying changes to randomness structures is a useful structure to fulfill intents related to variation like 'identifying new variation' as a default set of structures, given that identifying 'dimensions required to solve the problem' or 'dimensions to solve the problem in' is useful for identifying structures like low-dimensional structures that will likely be useful, such as 'interfaces/vertexes/queries' that will be useful, and limiting these dimensions with pre-filters like 'probable area ranges' is useful as a way to counter the randomness allowed by increasing dimensionality
            - relatedly, identifying different definitions of complexity like "lack of simple interface structures like 'iterations'" or 'volatility created by minimal structures' or 'similarity to randomness' is useful as a more variable-covering definition than 'step count'

    - identify useful variables (like 'function types' and 'iterations' and 'aggregations' and 'volatility units') in problems described by those structures to the point of linearization of the problem by applying formats (like how there is always an 'iteration' to form useful solution concepts/variables like 'volatility', just like there is a similarity/difference structure or a compression/expansion or filter/generation that can produce volatility, but iterations are simple to apply and highly descriptive of the structures they create, so are particularly useful to identify)
        - for example, solving the problem of 'regression' is solving the 'scaling/iteration problem of high variation structures (such as function structures like function types)' so that new units like new function types can replace iterations, and is similar to the problem of 'solving all variable unit interactions' (what count of 'unit wave' required/allowed to aggregate with count of 'unit exponential' aligned on some similarity, creates some output count of what unit structure) since multiplication is a scaling function (applying an iteration of some input) and adding is a coordination function (how do these multiplied "counts of function types" coordinate to form an "aggregation/stack"), so identifying all the 'scales/iterations of structures' that produce useful function types or useful function variables or useful function interactions is the index to compute (how can a wave and exponential function and an interval function interact in a real system, at what scales, to coordinate in forming an aggregation), similar to how multiplication is an embedding format applying different units ('x of x of x') so identifying similarities between embeddable as in stackable structures is another useful way to describe this problem
        - relatedly, 'when a new unit is yet to be identified' (by identifying a new iteration type) is the state when a 'combination of existing functions will have the most error' and the most volatility will occur
            - identifying the most random function that just barely avoids being describable as random is useful as a set of functions that can describe and approximate randomness by connecting it to volatility and other variables likely to produce almost-random functions, which simplify the problem of describing high variation functions by using 'volatility units' or 'randomness units' (and the variable interactions that create these) and connecting them with other function variables/types, as a useful way to simplify the uncertainty space between linear/random functions, and as a useful default sets to linearly describe complex systems
            - similarly, identifying the function types/interactions/structures (waves aggregated with exponential functions, aggregated with interval/grid functions, aggregated with random functions) that adjacently produce solution metrics like 'volatility' is useful to pre-filter the solution set
            - relatedly, identifying which variables are difficult to predict in a system by being very independent or very distant or otherwise interactive with many variables or supporting high variation (identifying the output of some high variation variable collision, in a system that allows previous collision outputs to interact, at some iteration count n) will identify the function types that havent been identified yet and is likely to identify useful directions of change as useful problems to solve
            - relatedly, identifying how a structure can be used to seem like its opposite (in a false way, like temporarily, or ignoring the adjacent states or a subset of variables) like how volatility units can be iterated to create a very different function type (like linear/random/wave) is useful to identify 'patterns of differences possible within a definition' as well as 'positions of function types in function fields/networks' which can be used to identify other useful positions of other useful functions types (not being describable/definable with other patterns/variables/types, similar to how primes occupy spaces in between rectangles/squares so their position can be determined by differentiating from or connecting rectangles/squares, indicating a new pattern/variable/type is required to describe the interim function between other definitions, and describing all of these new structures and their similarities in the uncertainty space being generally useful, 'identifying extensions/iterations/expansions of and interim structures between and new angles/compressions of definitions' being generally useful for 'identifying new variation')
            - relatedly, identifying randomness as being relevant to an increase in dimension (a 'square describing a data set shape' being a randomness structure compared to a 'line describing a data set shape') is useful for identifying uncertainty structures (an increase in dimension has likely occurred in an uncertainty structure, which makes finding a line in a square/cube/etc more difficult, which is a simple structure of an 'iteration' to reduce complexity with)
        - relatedly, identifying that a problem can only exist if another problem is not solved is useful to identify ways to 'connect these problems with overlaps' that can make progress with both in the set of problems
        - relatedly, identifying how a set of different structures like 'exponential stacks' and 'rectangular areas' can be created by a function (what change types can be created in what formats by a particular function, like x^2 or x^2 plus x, creating 'square or rectangular' areas in the 'area' format) is useful to identify, to identify metrics like 'how easily can this function generate any structure or extreme differences like randomness'
        - identifying 'functions that can create multiple interface structures' (like a function that can adjacently or simultaneously or otherwise similarly create a line and a circle) is a useful function type to identify and apply as default useful functions through this multi-functionality
        - identifying connections between variable interaction structures described by the 'approximations of functions' and the variable interaction structures described by the 'original functions' are useful to identify as a high variation variable that is likely to have some useful new similarity/difference variation, the structures described by approximations likely to overlap in some subset of variables in a way that reflects other useful 'differences in similarities', as a way of identifying the differences that can have an overlap in an approximation or other similarity

    - identify useful structures like different variables used to filter queries, like connections between 'different function types and function/definition variants and similarities and concepts' that cover reality and connections between 'specific concept networks' that fulfill useful functions like core interaction functions like 'reduce/distribute'
        - for example, 'explain' requires structures like 'alternate definitions' or 'variants of a structure' or 'examples of a structure' which are all connected to 'similarity'
        - therefore concepts like 'luck/randomness' are related to 'explain' by 'similarity', bc luck is 'adjacency of resources' (adjacency such as similarity, such as similar variants of a definition), so making a graph that implements luck (a graph that makes everything adjacent) is also a graph that can be used to explain anything in terms of anything else (using adjacent variants of a structure to explain it by describing its structure in a different system, given that explain is usually applied to concepts, and 'describing the structures or variants of a concept or its definition often explains it' similar to how 'identifying specific structures like examples' often explains it)
        - so 'apply luck to fulfill the explain function' is a way to connect different structures like 'randomness/similarity' in a specific and useful way (for the 'explain' function intent)
        - this is partly bc of the overlap between 'similarity' and 'simplicity' (with the exception of 'volatility' which makes some similarities complex), which makes randomness an orthogonal structure to similarity, these 'opposites-once-removed (opposite of randomness as in complexity being simplicity), across a non-1-to-1 mapping like similarity/simplicity' being useful to identify as different/orthogonal structures that are still similar enough to be relevant, and being likelier to cover more of reality than the original opposite set
        - identifying a 'way to use randomness to optimize a query' is always possible but not always adjacent, so identifying all the ways randomness can be useful by identifying a randomness network (randomness being useful after solution sets are very reduced, having already been filtered and similarized so that any random difference like any subset within that similar filtered solution set is useful, or when there is minimal info or when realistic simulations are required or when adjacency of resources is required or when generalization or other interface differences are required) is useful to connect to algorithms (algorithms occupying the space between concept and other interface networks like 'iteration networks', 'optimization networks', 'randomness networks', etc), which will make it more adjacent to apply randomness in algorithms bc the extensions of the randomness network will likely intersect/connect with other interface structures by default
            - connecting a query to either 'randomness or similarity' makes it clear how to connect it to its opposing structures (implementing 'organize' as a 'set of similarities (like a sort) to reduce randomness')
                - identifying structures with many 'opposing' structures (like abstractions/interfaces) is useful to identify structures that are useful by default in queries (the intent of queries being to connect problematic differences)
            - identifying functions that fulfill interface structures (interface functions like the 'most similar function across the most function types' and the 'most intersective function' and the 'most random function' and their overlaps) is useful as a set of default useful structures to connect in the math interface, which other structures are likely to be described by
            - identifying the structures like 'limits' of functions like 'describe/use' is similarly useful to identify, some of these functions contradicting and preventing the others (the 'use' function has to use different functions than the 'describe' function, or they cant evaluate each other independently in a way that could justify re-generating the other completely to be different after being found suboptimal, just like how 'explain' has to be applied regularly to make sure the other usages/functions can justify their processing, indicating theres a grid that queries should comply with, which is useful as a filter of queries)
            - the different ways to use randomness include 'distributing resources so theyre adjacent' and 'selecting a solution randomly from a very reduced set, after similarity between items in the set has already been established', which is why I connected randomness to explain, since explain uses 'variants of a definition/structure based on a similarity between variants, and identifying the problem of explaining a theft of my work as 'not possible to explain, no matter how many examples or variants are given, bc of the dependence involved, making any explanation invalid and not justifiable given the problems caused by it'
        - given the high variability of the 'explain' function, this indicates there is always a way to connect randomness/similarity to fulfill different functions/intents
        - relatedly, identifying structures that 'fulfill a definition but are so different that it may as well be inapplicable and invalidate that definition' are useful to identify, like implementations of an intent that are so different from useful implementations of that intent (as in having a high ratio of errors) that it may as well not be an implementation of that intent
        - whether its more useful to identify 'iteration/optimization/randomness' networks for standardization or whether its useful to apply differences in these formats (iteration networks, optimization directions, randomness limits) for maximal differentiation is likely to have an optimal structure of a connection between these different interface structures, as the more variable variant is selected with usage as being able to support more usage, where 'selecting a solution with usage after testing various usages' is a less optimal structure than 'deriving connections likely to be useful by applying insights on how connections should be derived'
        - similarly, connecting interfaces to interfaces in general is useful, such as how identifying that 'quantum connections' are similarities between different interface structures (semantic similarities like 'probability/cause/independence connections' since quantum connections have an extreme impact on probability/cause/independence as an opposing variable to other similarity variables like 'adjacency', so they may be applications of probability/cause/independence as a base of reality, rather than any adjacent similarity), indicate that there are other variants of similarities which will be identified in physics as 'bases of reality' (as in 'different ways to connect structures in reality'), and similarly identifying structures that occur frequently or with regularity (sufficient to be constant/connectible) are other possible bases of reality
        - identifying structures that 'implement relevant differences (like maximal differences)' are useful as a structure to filter solution sets (as they make filtering more trivial and are also more valuable structures to identify), similar to how identifying highly interactive structures like the 'most similar function to other function types' is useful, and identifying the 'most relevant differences in a solution set' are important 'filters to prioritize' (identifying the 'only relevantly different as in "good" usage' of a structure is useful to identify in a generally useless structure, which is possible by applying maximal differences and identifying which structures support them), as identifying these maximal differences in a solution set organized by some similarity is useful to be able to identify the generators of these similarities/differences (like the 'point on a causal network where these similarities/differences begin, overlap, and diverge'), such as 'applying enough differences to break powerful symmetries'
        - relatedly, identifying that iterations/grids are simpler to compute is useful, and enables identifying what is correct by which iterations/grids are not correct/applicable (such as how its possible to identify what grid doesnt apply bc its more adjacently verifiable from any point, and its possible to identify what iteration does not apply bc they interact with other structures like limits and intersections which are more verifiable, such as 'if everyone was using this function, this would not occur, so there isnt a grid where everyone is using that function') and similarly other useful structures can be identified by what is verifiable
        - 'identifying more abstract questions' like 'is it more true that there is a network/structure for every type of different format (like a density) of an interface structure, or that there are only structures that apply similarities/differences in their identified positions' is useful as a general problem-solving intent as a way to direct future interface queries
        - relatedly, there is a way to connect all interfaces to 'usefulness/meaning/relevance' (apply probability to create a useful probability structure like 'luck' as in 'distributed resources leading to adjacency to resources, making those resources likely to be identified on any given route' or 'distribute inputs to resources, making those resources likely to be generated' or 'distribute resources randomly within sufficiently small units for it to be approximately lucky for all positions, with equal probability of being anywhere within the unit', an example structure of usefulness) and a way to connect all interfaces to solution metrics (apply probability in the solution filter, as a 'high probability of identifying distributed resources within n steps')
            - relatedly, 'distribute resources' is a problem-solving intent bc of its connection to concepts like 'probability' and 'luck' and useful structures like 'resources' as well as its high variation variables like the 'distribute' function
            - identify these variables of how the interfaces can be a problem/solution structure are particularly useful to identify/differentiate/connect

    - identifying useful structures like 'optimals' that fulfill intents like 'organization' that can be used to filter interface queries as well as filter the set of problems to solve, a useful structure for fulfilling multiple problem-solving intents
        - for example, 'identifying optimals' (as generally useful targets) is useful for 'filtering/organizing problem-solving queries', as applying 'iterations of differences from optimals' is likely to create 'error' structures that 'connect optimals to adjacent/existing resources', for example identifying 'all the most useful values of spectrum variables and the network of spectrum variables and other useful structures' as a set of optimals to find integrations of, or specific optimals to connect to existing structures with differences, as opposed to applying problem-solving queries when specific problems are identified, instead identifying optimals and identifying all the problems connecting existing resources with those optimals and solving those specific connecting problems, or similarly solving related problems like the 'problem of connecting optimals' (so that when one is reached, the others are also reachable)
            - this is like deriving the question from the answer and identifying a network connecting questions/answers as well as identifying optimal routes between them and structures like 'types' of questions that create 'types' of answers and identifying which answers are important and organizing problem-solving around those important answers
        - these 'general optimals' are useful as 'specific directions (within uncertainty spaces between limits) to apply changes in', which are complementary with 'limits on change' as filters of interface queries
        - relatedly, irreversibilities are also relevant to time, as well as 'ratio of variation remaining' (entropy), not just variation, bc time requires a balance between variation/constants
            - irreversibilities create new differences that are required/adjacent, being a base for changes to develop on, and also change what is possible with the remaining variation, and various sets of irreversibilities can decrease other forms of time (including other irreversibilities/constants/variables) so can act as an opposing structure of time (leading to constant cascades) as well as enabling more time (being a useful base for changes), so 'identifying the interfaces that future people will need' is important to identify as quickly and as many as possible, as a general problem-solving intent that computers should be dedicated to (identifying the useful combinations of AI/computation/graphs that will enable the most variation)

    - identifying optimization opportunities using combinations (like 'structure-function sets', 'similarities', and 'connections to useful intents' and 'connections to implementation structures of those intents') which are useful as 'solution metric sets to connect and apply as filters of solutions'
        - for example, identifying 'opportunities' for useful connections/substitutions (like applying a neural network neuron for each data point) by identifying similarities (like the similarity between 'number of data points' and 'number of neurons') is a useful problem-solving intent, given that connections/substitutions are useful functions fulfilled by similarities so there is often an opportunity to identify these useful function fulfillments once those useful structures for those functions are identified as relevant/applicable, and similarly identifying useful functions fulfilled by other interface structures is useful to generate 'structure-function sets' that are likely to be useful across workflows, and identifying the reasons justifying applying that similarity (its useful to 'find connections between data points' in the regression problem space in general, so applying them as structures to connect in a neural network could be similarly useful with many connection variables like 'skipping a ratio of adjacent points' and 'maximizing intersectivity' and 'maximizing avoidance of intersections and avoidance of limits/ranges as a way to optimize for average connections') by applying interface structures and deriving useful intents and checking for a match with that similarity application
            - similarly, identifying the usefulness of an algorithm to "separate a 'probable solution area' into units like integer squares and identifying whether a data point or data point structure like data point ratio is in that unit and identifying the most useful units for regression intents and identifying the interactions between positive/negative units and identifying the most useful data point structure to apply as a metric (or set of these metrics)" is useful and possible by applying similarities to other useful structures like 'networks/grids'
        - relatedly, identifying 'connected intents' are useful to apply as default indexed structures which can be used once one of the intents in a set of connected intents is identified as useful, same for other connection sets that are useful and possible to pre-compute and apply as constants/defaults
        - relatedly, the implemenation of a general connection like the 'abstraction of math' as an 'application of abstractions to cover all math connections' as opposed to implemented as a 'generalization of math structures (to identify abstract info structures)' is useful as an example of 'maximal differences possible with the abstraction definition', which identifies the usefulness and possibility of a 'maximal difference-first workflow' (apply differences in interface definitions to generate maximally different intents to start solving a problem with to increase the probability of identifying a useful intent), the prioritization and sequencing of interface structures in a workflow being useful variables related to 'filtering' variables
            - for example, try to think of a problem that cant be formatted in a way that a 'ratio between relevant sequences/aggregations' cant be used to solve it - all problems can be solved with that structure when formatted a specific way, bc a ratio encapsulates a core difference that represents a problem definition (a problematic difference to resolve with a comparison between relevant variables containing a high degree of information, such as aggregations/abstractions/sequences), therefore this 'abstract ratio' which can be applied to resolve many different comparisons abstracts away the specific structures (the math) bc it can be used to connect all relevant structures likely to be problematically different (resolving all differences with one abstract structure)

    - identify useful structures like 'functions like "optimize" applicable to interface structures like usage structures like "implementations" such as "interface queries"' that can fulfill some problem-solving intent like 'reduce errors' for problem/solution intents like 'identify interface queries'
        - 'optimize' applied to 'implementation' as 'reducing errors' such as 'reducing cost of implementation' is useful as a problem-solving intent, given how valuable it is to make an existing solution less costly, even if the solution stays mostly the same, in cases where the cost of an expensive solution is the problem or an alternate problem to solve compared to finding new solutions, same for other errors in the problem space of identifying interface queries
            - at every point, applying 'optimize (to reduce costs of some function)' will likely help with some other intent if the optimization is new, so 'optimize to reduce new errors' is a useful function to apply regularly, so much so that it can be applied randomly and still likely improve an interface query
            - 'optimize' is 'solve problems by reducing errors, by creating differences to errors', whereas 'organize' is 'apply differences to create organized/similarity structures like definitions/sorts/networks/indexes', whereas 'apply paradoxes' is 'find the most extreme differences possible within some set of limits' and 'apply interfaces' is 'embed differences until no more are possible to embed, or until interface structures are reached again (like generating abtractions from specifications)', whereas 'isolate and re-combine' and 'generate/filter' are variants of 'apply similarities like combinations and differences like filters/sequences'
            - 'optimize implementation' is useful bc identifying low-cost structures are generally useful, 'low-cost' being an almost constant in interface query filters, this constant or almost constant being useful as a default structure
            - its also useful bc it can apply enough variation to solve the problem in other ways, just by reducing cost/error of some sub-intent in an existing interface query but also by applying the high variation intrinsic to 'reducing interface query cost' and 'reducing new differences' (an opposing intent to 'identify new variation' similar to how generate/filter and isolate/re-combine are useful opposing intents, which are useful to find for all problem-solving intents or useful intents), so generally 'reduce cost of interface queries' is useful as a general problem-solving intent
        - relatedly, identifying 'mixed-truth efficiency' structures as 'jokes' is useful to identify, as jokes can represent 'paradoxes' (extreme/opposite differences possible within the same definition) and other 'similarities with differences', outside the areas of truth and closer to the limits that indicate randomness in the uncertainty space in between 'linear functions and randomness', in the 'ridiculous' as in 'orthogonal in sensibility/reasonability' such as by connecting extremely independent structures
            - 'you couldnt be any smarter' is a double meaning possible within the definitions of 'could not', 'be', 'any', and 'smarter', having extreme opposite differences, creating high output differences with low input differences (efficiency)
        - relatedly, 'linearized randomness' is possible with a 'maximal difference network where all differences are adjacent', which can be used to find the positions that would adjacently create randomness, or create a network where every point or regular points are random, which should be integrated into the 'uncertainty space' network, to identify new limits than randomness (like identifying randomness-maximizing structures, etc)
        - relatedly, identifying that a 'capacity/limit for addiction' is possible is useful to identify as an interface structure of 'addiction' in the 'addiction' problem space and useful to apply by 'fulfilling the addiction capacity with positive addictions like to 'exercise/vegetables/learning' (so that no additional negative addictions can be added)', similar to how applying addiction (a problem of 'over-using' applied to 'useless' structures) with useful structures like 'vegetables' (over-using vegetables likely wouldnt be harmful as vegetables increase self-regulation and health) to 'over-use the useful structures' is possible to identify by "differentiating/isolating the variables and re-combining them in useful ways"
        - relatedly, 'applying "limits" like requirements/simplifications as a way to make some differences more obvious' is useful as an alternative to 'applying "extremes" to magnify changes to make them more obvious' and 'apply separations/isolations/filters/subsets to make differences more obvious', 'change differences to make them more obvious' being a generally useful problem-solving intent

    - identify useful structures like adjacent structures to useful structures that create other useful structures like connections to 'probable available input information' as well as 'connections between equivalent meaning structures'
        - for example, given that some ratio of sequences (such as when determining whether some sequence converges faster than another to a useful value) might be useful to solve some problem, the inputs that create the changes describable with that ratio (like specific values that represent intersections or stacks of interface structures which are likely to create convergence, like hyperbolic functions) are useful to identify and apply as another structure to connect to identify adjacent structures like the ratio, to determine which change structures are likely to be useful to connect, likely to create a comparable value in some metric like an aggregation/iteration, and likely to be possible to connect with a ratio
        - relatedly, identifying equivalent structures of meaning is useful, such as how 'solution metrics are useful to optimize for even when not being evaluated' because of the probability that side effects of not optimizing when not being evaluated will be iterated and/or will intersect with other structures related to other evaluations (the 'fulfillment of the solution metric when being evaluated' is similarly meaningful as 'fulfillment of the solution metric when not being evaluated' because an 'iteration' can create an 'equivalence in meaning between an unmeasured error and a measured success', however indirectly, as an unmeasured error can create cascading errors that impact the next evaluation), so that multiple equivalent structures of meaning are fulfilled rather than selecting a subset, given how variables are rarely completely unconnectible and therefore a worst case scenario of unmeasured errors creating error cascades must be considered, being possibly equally meaningful
        - relatedly, as a prioritization optimization, identifying different filters of info is related to identifying different complementary sets of info, which make different info trivial to identify (these being different perspectives/filters to apply to make different info obvious), these filters being useful to identify first in new systems to identify what info is missing and obvious to derive position of filters like assumption sets
        - relatedly, identifying interface structures like 'variables' of useful formats like 'graphs' (like position/distance/parameterization) is useful, and relatedly, identifying similarity types between graphs (like manifolds of graphs) is useful, and applying these for adjacent intents ('applying variables to create variation in graphs') is useful (such as by 'identifying graph changes that are possible and filtering these for usefulness') and similarly applying interface structures to graphs (such as alternates or alternating structures like waves to create 'grids within graphs' like alternating different node types, from a uniform graph of one node type, by applying different similarity/difference types, like changing one structure in a graph and changing structures in a pattern and mixing structures and distributing those mixes until its a grid or has other patterns, 'changing one structure' or 'changing structures to a pattern like alternating' being different from standard changes that change every node like applying a format/position change to indicate some difference in the graph)
        - 'definitive' differences from interfaces (like 'colors', which are ridiculous to call a 'reality-covering variable' or an 'abstract variable' or an 'interface' despite being a useful indicator of a subset of light and therefore a subset of differences, since light is an interface) as useful 'alternating structures mimicking query patterns like generate/filter alternations' (since there is frequently an adjacent structure that interacts with color given its high frequency as an input and high variation, which doesnt form a grid exactly but definitely follows patterns of variation if not alternation patterns), compared to different variables like 'positive/negative' which definitely represent core problem-solving structures like 'combine/reduce' so they are definitely useful to apply as interfaces, as opposed to the 'non-defining subset of a specific interface' that color represents and therefore is not equivalent to an interface, but is still useful as a different structure than a field covering reality
            - relatedly, identifying 'requirements of solution metrics' is useful, such as how 'minimizing negative side effects is not equivalent to maximizing positive side effects' and 'both need to be applied in combination in order to be applicable as a good solution metric'
        - 'coordinating useful structures like coordinating vertexes/cross-interface/combination structuures' are useful to connect using similarity/difference structures as default interface query components/bases/inputs (like the 'reason why a structure is useful for some intent/context' and the 'reason why a structure is useful for some different intent/context', based on a similarity of the 'reason/structure' cross-interface structure)
        - identifying 'connections between networks of truth/similarity indexes' is useful to identify similarly complex structures (truth indexes being useful to identify, as 'filters' that make truth structures obvious are useful to identify, like how some filters make truth structures like 'temporary truths' and 'illusory truths' and 'forced/computed truths' and 'default/current truths' and 'probable/future truths' obvious, where structures that seem true can be true/false in various ways since 'subsets of solution metrics' that make some connection seem true dont contain enough information to reflect the whole truth, so these subsets will have definite error structures of 'missing/incomplete info' that can invalidate the connection, and the overlaps between these filters/inputs/subsets of truth structures reflects a 'network of truth indexes' that is useful to connect to corresponding structures on other interfaces like a 'network of similarity indexes')
            - relatedly, identifying that a 'contradiction' (applicable as a 'filter of generated connections') can have different interface structure forms like being completely contradicting, definitively contradicting, required to contradict the original statement, exactly opposite, partly contradicting, contradiction of its uniqueness as in 'alternate', contradiction by being 'more true', contradiction by being "limiting of the original statement's truth", etc
        - relatedly, identifying new variation (as change of change) is useful as an extreme variation source, which is useful as a general problem-solving intent (find the most extreme change such as a change of change like new variation as a useful structure like a variation source, similar to how its useful to identify the most extreme abstractions, as in the most abstract abstractions, like interface variables, and similarly its useful to find the opposites of extreme changes, as in the units of interfaces and the overlaps of these interface variables, overlaps where variation occurs as equivalent/similar alternatives)
            - relatedly, connecting interface structures to core intents like 'identify a useful difference like an intra-spectrum variant (like a specific variant, an example) of some concept or function' which can be specified by 'applying a specific context (which is in a different position than the example, as in around it, and doesnt prevent the example and fulfills a requirement of the example)' ('an abstraction applied in a specific context creates a specific abstraction variant'), and additionally 'applying changes (to the abstraction, in that context) to create interactivity' is further useful for the same intent, as specificity results from interactivity, interactivity acting as a filter
            - these intents like 'identify new/variable variation' and 'identify intra-spectrum variation' are useful and useful to identify variants of, bc these are abstract structures which havent been solved for yet, so finding new connections of them is still useful as a substitute for solving other problems (identifying new variables will likely help solve other problems in a problem space), but which are highly determining of problems/solutions, given their interactivity with high variation variables like interfaces
            - 'find a specific example' is a particularly unsolved problem bc connection structures like 'equivalence between "every possibility" and "required"' arent abstracted and applied manually unless the related perspective has been applied recently or otherwise is artificially likely to be used, so 'find a specific example of a required structure' (which wouldnt adjacently lead to an answer like 'covering every possibility' without recently identifying different definitions of 'required' or recently using 'every' instead of 'required', same for 'default', same for other forms of adjacence than 'recent' like 'simple') 'find a specific example' being a 'difficult direction' for an interface query to implement structures in or in fulfillment of, these optimization structures being useful to identify in networks of interface query components/queries
            - relatedly, identifying useful new intents is related to identifying new variation, which is possible by 'identifying different connections possible within a set of definitions' and 'applying differences across these variants where they are not required (applying an intent relevant to a variant, to the original)', for example identifying an 'average metric of some set of factors having an attribute like prime' and identifying an 'average metric of some set of usages of those factors having that same attribute like prime', this average metric not being guaranteed to be useful when applied in different structures but is useful as a possible source of variation'

    - identifying sequences of problem-solving is useful as a way to speed up current and future time, by applying 'variation sources' as a way to identify where problem-solving will go, and how to connect those future intents with current intents through integrating these future intents into current interface queries
        - for example, given that most medium-complexity problems are solvable with relatively simple structures like 'identify the ratio between two relevant differences that contain high variation/information (like relevant abstractions/sequences/aggregations/averages/iterations/embeddings)' or 'find a position on a network/plane/grid in relation to other structures like a plane of errors (which reduces errors to a simple set of variables so that any new error can be mapped into this plane and identified in its similarity to other errors, simplifying variables of errors like "difference from useful intents")', so that the error-similar structure can be identified and the solution-similar structure can be applied for some intent, other problems after this set of problems will involve 'optimizing iterations to avoid creating relevant differences to compare/filter' and 'optimizing planes so that errors are trivial to identify and avoid (like by finding different bases that make errors more trivial to identify)', to avoid creating these medium-complexity problems in the first place
        - identifying problem-solving sequences (like where problem-solving will go next, based on where the variation is) is useful for directing the design of interface queries to solve medium-complexity problems now
        - once its known that 'optimizing planes of solutions/errors' and 'optimizing iterated structures to maximize useful differences' and related intents for similar problem-solving structures are known, current interface queries can apply this as a solution metric to help fulfill those future intents
        - relatedly, identifying cases when a function (like a neural network) is useful is a matter of identifying its limitations/functions (a neural network is useful when "there is likely to be an identifiable pattern with additional hidden variation that requires more scaling/iteration than people can do quickly to identify, and the pattern is partly identified before applying the neural network, such as a 'description of its complexity/variation' being identified beforehand, to identify a neural network as being capable of identifying the pattern in the data")
            - relatedly, other ways to generate useful function networks can be identified and optimized in a neural network, given that there is some pattern to "identifying a useful similarity (like an abstract similarity), applying changes to it to identify other similarities, and iterating this process" (to identify useful function networks that can probably be used as neural networks to solve most problems by default when trivially changed to change the angle that queries are applied in, etc), similar to how interfaces seem to be other structures like filters when a focus is applied to a subset of an interface, so interfaces make good default networks to connect variables with
            - similarly, applying interface structures varying on useful abstract structures like 'relevant differences' such as how a 'function and its contradictions (filters of that function indicating its not applicable)' are useful to connect in neural networks, as a set of pre-filtered filters to apply when filtering functions, given the identified usefulness of these structures and the probability of additional variation being possible/useful to identify
            - similarly, identifying different differences to check similarities for interactivity with is useful (as opposed to connecting them to already defined/identified relevant differences), given that these similarity/difference sets are useful for identifying patterns in, similar to how applying a structure in its defined incorrect/suboptimal position is useful for identifying its limits/functions/definition
            - relatedly, identifying the structures that produce the most variation like 'connections between high variation variables' and 'abstractions' and 'combinations of multiple independent variables' are useful to identify and apply to describe differences supported by similarities, given that these components all fulfill a 'difference (like a specification) supported by a similarity (like a type or intent or description/average or base or range/limit of variation, around which variation occurs)', so combining these components tends to be higher variation, and similarly other structures which create 'exponential change or other change types with few inputs and high outputs' are similarly good at producing more variation than a simple combination like a sum
            - relatedly, connecting useful structures commonly found in solutions like averages (similarities), thresholds (filters), ratios (comparisons) is useful to identify how those structures will become other structures to identify other problems adjacently solvable by those structures, and identify difference dynamics given the connection of these structures to differences (ratios being useful to identify between relevant differences, thresholds being comparisons to a base that are applied as filters to differentiate some subsets in a set, averages/overlaps/intersections indicating similarities to identify similarity indexes which can also differentiate through similarizing/standardizing)
            - relatedly, identifying 'interface structures' of similarities like 'intersections of similarities' to identify adjacent structures (connections, angles) to those structures (intersections) is useful and possible, by arranging similarities/connections in different positions/angles until they intersect or encounter other structures like barriers or limits (different from a standard network graph of identified rules, where differences are applied in the connections until other identified structures are identified by applying changes to the differently organized similarities, indicating the graph is good at generating other identified structures, such as by selecting a subset of connections/similarities to organize in a subset of different ways at a subset of different starting points and angles until some combination is successful at generating other interface structures more adjacently than other subsets, and then connecting those useful subsets as equivalent alternates until proven otherwise)
            - relatedly, identifying a specific structure (like the specific connection that solves some problem) can be done by applying changes on some other interface (like applying changes on the intent interface), given that existing structures probably arent sufficient without additional differences (existing intents wont find that specific structure, so identify new intents that will make it more adjacent to find, or identify new types of specification on the abstract/specific interface or new difference types on the similarity/difference interface, or identify new differences that might be relevant, as a default set of new structures to compare/connect)
        - relatedly, identifying structures that are not typically useful in the 'thinking' problem space (simple thinking such as 'blood flow between existing neurons', which is only useful if those neurons can solve every problem like interfaces and if the solution is a simple connection possible with blood flow) and those that are typically useful (like 'synchronicity created by waves that expand a structure like a neuron until the structures are trivially connectible or make some difference trivial to identify', and other structures like 'adding new neurons between/around existing neurons to contain new variation of new connections identified between neurons' and 'new connection functions created a new interaction layer between neurons, incentivizing changing those neurons to component structures on the new interaction level and incentivizing different connections/neurons as default', which have some degree of change of the structure like change a function and change the neuron info and change the neuron structure, similar to a vertex of changes that when applied creates a useful difference like a rotation, as applying a 'similar change rate connected to the same structures' is like 'identifying a spiral that connects all variables') are useful to identify possible new workflow variables like 'frequency' (related to waves) applied to workflow structures like 'evaluation of metrics', since variation is allowed between evaluations where they are held constant to be measured, similar to some workflows that apply and then filter variation, so applying 'frequency' to fulfill possibly useful intents like 'synchronize evaluations' is useful to filter workflows

    - identifying connections between structures that are possible with other structures (connecting generators to solution metrics is possible and useful) is useful to identify as an 'optimization opportunity' (to create connections where there were none before in a useful set of structures to connect), 'identifying and solving for optimization opportunities' being a general problem-solving intent
        - for example, identifying solution metric networks as outer layers created by some core set of structures is useful to identify as an info structure that will identify important/useful structures like 'generators of solution metric variables' such as 'accuracy', 'generalizability', 'linearizability', 'sensitivity', 'stability', etc, which would identify their common components/inputs/generators like 'interface structures of interface structures', since solution metrics are often iterated interface structures that contain high variation, like some specific type of 'input/output connection' like 'volatility', and other solution metrics can be generated by variants of those inputs (identifying different types of input/output connections across increasingly abstract functions) and their common generators can be identified as 'solution structures to generate structures having those solution metrics' (which would solve the problem of 'identifying the interface structures that can always fulfill some set of solution metrics') as a general problem-solving structure, and relatedly identifying structures that fulfill the most solution metrics is trivial once these generators and connections are identified (where this general problem-solving generator/solution metric connection network is likely to still have some ratio of abstraction rather than connecting some specific variable set, but is complete enough to describe most variable interactions specifically)
        - similarly, identifying 'descriptions of solution metrics' that connect to other structures like specific examples is useful as a 'connection between high variation variables' that would make the connections to the solution metric variable usable as 'interface queries or their components'
        - the 'generator' connections and the 'description' connections may overlap, but the point is that these are both different structures that accomplish the same intent, which is connecting solution metrics to other structures (which if it can be completed, would solve all problems by definition)
        - similarly, organizing solution metrics and fulfilling other problem-solving functions for solution metrics like reduce/distribute/differentiate are similarly useful as alternate 'query components/queries' (or bases/limits or other structures of them), to indicate a default set of useful 'query components/queries' or structures of them

    - identifying useful structures like 'variation sources' such as 'graphs of interface structures of alternate routes' that are likely to be useful across problem-solving intents like 'identify new variation' and identifying the connections to fulfill those intents like 'identify the highest variation reduced structure in that variation source' ('identify variation source' -> 'identify a useful structure (like the highest variation reduced structure) in that variation source' -> 'identify new variation'), where the new variation might be a new 'difference in a similarity' reflecting the 'variation in a reduced structure (the same core set)' such as 'maximally different subsets of similarity indexes'
        - for example, identifying 'maximally different subsets of similarity indexes' is a useful set to identify that is possible once the similarity indexes are identified, which is useful for intents like 'identifying new variation' and 'connecting independent variables'
        - identifying the 'variables of alternate routes connecting the same structures' is what identifies the usefulness of this structure of 'maximally different subsets of similarity indexes', identifying interface variables like connectors across sequences or alternate routes ('probabilities', 'interactivities', 'similarities', 'patterns', 'averages/bases'), differentiators of alternate routes ('filters', 'limits'), differences between alternate routes or positions in sequences ('maximal differences'), and creators of alternate routes ('generators') as useful to create and differentiate the 'set of alternate routes', where the most useful structure in that set would be the most reduced structure (such as a subset, like 'maximal differences') that covered the most variables ('variation' being highly completely described by 'similarity indexes'), where different but similarly maximally covering 'subsets of similarities' would be useful to identify
        - relatedly, identifying filters of inputs/variables/components/generators of abstract info structures (like 'filters' to find relevant sequences to find a ratio to compare or 'filters' to find relevant values to compare like convergent values) are useful to identify inputs/variables/components/generators of, such as how 'high variation variables' in a sequence set is likelier to be relevant to compare than other variables and how 'interface values' (as in 'stacked change types') of those high variation variables like 'values overlapping with limits (like convergences)' are likelier to be relevant to comparisons, as these values contain higher ratios of information than other values
        - relatedly, to connect useful workflow intents like problem-solving intents and core interaction functions of workflows, 'identify new variation' is likely to be generally relevant across interfaces (new variables are likely to also create/be new abstractions, new causes, new functions, new interfaces, new similarities, etc) and vice versa, which is why its a problem-solving intent, just like 'identify new connections' is generally relevant and same for identifying other new interface structures or new iterations of interface structures like 'new positions of interface structures', just like 'fitting variables to identified variable interactions' is generally relevant as a complementary intent of 'identify new variation', as variation is more useful when connected/standardized to other variation, and similarly 'identify remaining/complementary variation (such as variation not covered by some variable set)' is useful in general as a specific variant of 'identify new variation' (the remaining variation in a system not already identified but determined by identification of some limit on variation, like points not explained by a function as complementary to that function's variables, forming a complete set of variables describing the points), and other interface structures of this intent (like 'identify new errors', 'identify variation sources', 'identify generators of variation sources') can create other general problem-solving intents by fulfilling variants of the 'filter variables' cross-interface structure which encapsulates the 'generate/filter' vertex, related to how 'specifying abstractions' encapsulates the 'abstract/specific' vertex and is a generator of problem-solving intents, and same for their interface structures like opposites ('change filters' and 'abstract specifications'), similar to the other spectrum variables like 'un/certainty' (as in 'vary constants as assumptions or averages or specifications', variables being useful for any of those structures, and 'specify variables as constants or optimizations', a specific variable value being useful for either structure) and other useful structures like 'embeddings' ('generate generators') and other cross-interface structures (like 'structure-function' as in 'structure a set of useful functions') and interfaces like 'organize' (organize as in 'identify structures (like networks) of structures')
        - relatedly, identifying 'requirement' as a highly useful structure on the useful 'spectrum' variable (as there is a spectrum and similar format of every interface variable) is useful, where other structures on the spectrum including 'meaning' structures like usages/definitions/implementations and 'relevance' structures like similarities/differences, solutions/errors, and abstract/specific connections as similarly useful, etc, a spectrum that provides a ranking/sort to avoid filtering the list of structures to apply (applying more useful structures by default as a default subset pre-filtered by the sorted ranking of the spectrum and the count/ratio n to pair it with to create the subset, this 'spectrum implementing a sort, combined with a ratio/count' being a useful adjacent interactive useful structure to apply as a component)
            - relatedly, as mentioned elsewhere, identifying 'required interface structures' (and other iterated applications of interface structures that are useful for solving problems) to solve a problem, like 'required info', 'required format', 'required definitions', 'required assumptions', 'required connections/filters', 'required probabilities', 'required causes', etc (similar to identifying probable variation and variation required and variation position) is useful as a general problem-solving intent (identifying abstract differences/causes/structures, identifying potential causes/limits/iterations, etc)
            - relatedly, fitting structures/requirements like identifying 'incomplete' structures to apply as 'components' is useful to apply with 'similarity indexes' across useful sets to connect like 'structures/requirements'
            - relatedly, identifying 'infrequently used' interface structures like 'iterated requirements' is useful to apply as a 'source/base of variation' or as an 'error/limit to avoid'
            - relatedly, identifying connections between bases/limits (similarities and limits on differences in that similarity) are useful to apply as 'default connection structures between structures having these known types (known bases/limits)' which are a common type to have a requirement to connect
        - relatedly, 'identifying non-trivial structures to connect (like filter/spectrum or network/spectrum)' is useful as a more useful problem to solve than other problems, as a spectrum implementing a filter (by incrementing some variable like 'independence') is unlikely except where the unit represents some high variation base like a 'concept' (like 'degree of power' or 'ratio of variation' or 'relative independence') or a 'number of randomly selected interface structures or filter structures like "loops/indexes/required equivalences" used to create the filter, or number of input variables from some variable set used by the filter' where every incrementation of the unit creates a usable filter, and where the spectrum values align with some other relevant structure (like how the high usefulness value aligns with requirements like a 'simple subset of defaults to select') thereby making the spectrum a useful format to identify for that filter, where the filter can have a variable with structures other than 'simple opposites (like two extremes on a spectrum)', meaning the filter's determining 'inputs/components like indexes/loops/equivalences' would be representable as a 'spectrum set, variables within spectrums, or connections between spectrums' to make filtering more trivial by organizing some filter components/inputs with pre-filters (sorts or other organizational formats)
            - this structure is useful to connect to the 'set of alternate routes between structures' and 'network of interface spectrum variables' which are relevant to this 'standardized spectrum network creating most filters'
        - as an example of a problem solved by a simple structure like a ratio, a general set of intents in a 'justice algorithm' would include a ratio between stacks of structures like resources of opposing sides in the conflict, where the problem to solve is distributing resources according to the fair ratio given the costs/benefits to society posed by both opposing sides, the problem created by the assumption that the 'existing distribution' is unfair and needs to be corrected, given some resource distribution that is not equal to this fair ratio (identifying that a 'structure which should be equal (to some ratio in this case)' is not equal to that, as a way to identify the problem automatically)
            - 1. determine fair ratio of resources according to costs/benefits to society of both sides of the conflict
            - 2. count resources
            - 3. determine resources that fulfill the fair ratio
            - 4. check if this resource distribution will harm society in some way or if it will get consensus (this is the part where AI can add a lot of value in generating good suggestions by handling many variables at once, since this is trillions of variables and the idea of 'harm to society' is not well-defined and frequently involves any sort of change)
            - 5. distribute resources as closely to the fair ratio as will avoid harming society
            - 6. then to correct the problem of the 'overly high ratio of crimes compared to non-harmful acts', algorithms to optimize group dynamics, algorithms to warn people about risks/negative side effects of decisions to help them make better decisions and algorithms to help scammers make money without exploiting people and algorithms to separate people who always fight and algorithms to organize people in cities to maximize the probability that there will be jobs for everyone (distributing people with different skills so they have something to sell each other) while also fulfilling as many other solution metrics as possible (minimizing commutes/pollution), algorithms to identify low-cost resource position changes to reduce other costs (distributing vitamin d, baking soda, herbs, pollution filters where the cancer rates are highest), etc
        - relatedly, the problem of 'identifying the specific structure (ratio) to find the correct value of' is a complementary component of the problem-solving query (complementary to the problem of 'identifying the correct specific ratio' and the problem of 'applying/implementing that ratio'), so applying this as a general query is useful ('find a solution structure, find the specific structure that is optimal, and find a way to use it')
            - the problem of 'identifying the specific structure (like a ratio) that solves the justice problem' is a matter of identifying the 'differences to correct' to increase the 'justice distribution' metric (identifying the 'injustices to correct'), meaning identify the structures related to the problem structure (the differences that create or are 'injustice' structures, as the distribution of the 'opposite of justice' as in 'injustice' is the problem)
            - so identifying that 'there is a ratio that can solve the justice problem' is possible when its identified that 'there is a ratio that is causing the problem or is the problem' (the high ratio of 'distribution of injustice, compared to justice'), which is frequently the case with abstract structures that contain a high ratio of information/variation like 'justice distribution or inequality distribution'
            - a 'ratio of a value representing high variation variables (like abstractions/interfaces/aggregated/average variables)' is likely to be incorrect bc these are complex values to identify/change for existing functions
            - this is solvable once a problem of identifying 'relevant differences' (the differences that are important in causing a problem), which may have a simple format (like a 'ratio or position on a plane/network to optimize') is solved
            - the ratio isnt the only structure involved (a 'sequence of distributions to implement the correction to the ratio' is the next structure required to implement the solution to the problem of 'identifying the specific correct ratio value', which is the solution structure to the complementary problem of 'applying/implementing the solution')
            - 'specific ratios that solve most problems' are ratios involving high variation variables that are also solution metrics like 'accuracy' (the 'ratio of truth/falsehood'), so other ratios or other simple structures of interface structures are useful as possible solution metrics to optimize for, and variants of the 'ratio between truth/falsehood' such as other relevant interface differences like differences between 'vertexes/spectrum variable values'
        - relatedly, the reason why an algorithm using interface structures is not 'too general to be useful, since interface structures are abstract enough to cover all of reality' is that some reality positions reflect 'some interface structures more than others' and therefore 'their connections have different value for different intents than other combinations/connections' and 'reality-covering' doesnt mean 'every useful variant of interface structures exists at every point in reality' and not every connection/usage of a structure at a position will have the same meaning bc positions differ in other metrics (this is the same reason that every possible structure can be used for every possible intent, but some structures are more adjacent to specific intents)
        - relatedly, 'abstractions' arent just 'types' bc abstractions have specific variables/functions like 'covering reality' (they are a 'type' of structure though), and also their structure changes with changes in variation in systems where they occur (so they can mostly only be defined by defining more specific abstractions or similarly their differences to other similarly abstract abstractions), bc they have enough variation to do so, being based on a general similarity, but they are related to types in that they are a 'set of variables', but 'abstractions' are also good general structures to solve problems with and base other changes on (justifying the connection to interfaces/filters/independent variables/systems, etc), where 'sequences of increasingly high variation filters' are useful as 'alternate timelines' (since time moves in the direction of higher-supported uncertainty/variation)

    - identifying useful structures like 'maximal differences from similarities' (like simple functions that create differences where similarities would be probable or otherwise expected) is useful to identify new structures to apply as new workflow components and new variables
        - while identifying structures of power (a 'suboptimal implementation' of power or 'absolute' power or 'suboptimal usage' of power are interface structures of power), the usefulness of standardizing to a more common concept occurred to me (abstraction) and identifying the interactions of these and the structures like limits of those interactions was adjacent at that point
        - the 'limit' of the interaction between different interface structures like 'abstraction' and 'power' is an interface structure (limit) of a cross-interface structure (the set of abstraction/power)
        - identifying these structures are useful to identify structure usefulness-switching functions (when some set has become less useful)
        - for example, power is less connected to specificity than abstraction (specificity is orthogonal to power), bc it takes more steps to reach it than abstraction (specificity -> constants -> stability -> power, as opposed to abstraction -> multi-functional/reality-covering variables -> powerful variables)
        - identifying these 'relatively more distant structures' is useful for determining directions/structures to find structures like limits of the interactions (limits as in 'what structures do power/abstraction not cover')
        - identifying structures that are unexpected, such as how an orthogonal variable to power (specificity) is not similarly different as the opposite of that variable (abstraction), are similarly useful to identify (what functions could apply bc of some similarity like a simple opposite on a spectrum but dont apply) and apply as new default components on the new interaction level created by those new structures, similar to how identifying that a spectrum variable can also be a cycle (like how some abstraction can be so abstract that it becomes specific, as in leaving only one remaining specific variable, having removed the other variables to create the more abstract form)
            - similarly, identifying similarities like 'abstractions without removing variables (such as only listing variable sets and their ranges/distributions, rather than specific values in those ranges/distributions)' and 'abstractions by removing variables (generalizing trivial specific embedded changes away or standardizing common bases away)' and other implementations of the 'abstraction' function that 'remove different variables like specifications or subsets' are useful to identify (similar to how identifying 'specific indexes of specific structures implementing some concept/function' which can be applied instead of 'applying the function itself' is useful), where identifying structures that still contain a high ratio of information when abstracted (interfaces, concepts, fields) is useful
        - relatedly, abstract concepts have related workflows that are useful, like how 'balance' is useful to 'identify connecting differences' as a useful function or how applying balance is useful to 'fulfill organization intents (like sort)', or how 'power' is useful to 'identify new variables' and 'identify stable variables'
        - why do I call structures like 'ratios of relevant sequences' the 'abstraction of math'? bc this structure 'ratios of relevant sequences' is so general that it can be used to solve most problems once those problems are formatted in such a way that a 'ratio of relevant sequences' is the important similarity/difference to resolve by identifying the ratio at some threshold, even though these structures like 'ratios of relevant sequences' are technically math structures, as all structures could be formatted as given that structure (geometry) is a field of math, so the 'abstraction of math' uses math, but they have so little structure that the ratio of math (as geometric structures) is relatively trivial compared to semantic functionality (the meaning achieved by the geometric structures, which is a solution format 'ratio of sequences' to solve most problems, 'finding this structure in that format' (finding the ratios comparing the sequences found to be relevant) being the new problem to solve once standardized to that format)
            - relatedly, more abstract problem-solving structures include 'ratios of abstract concepts like power/balance' which are powerful in that they identify a position on a graph of graphs of the abstract network (a specific ratio of power/balance representing for example a 'similarity/angle between them' or an 'indication of the net/emergent/aggregate difference between power/balance structures' is a reference to a 'set of abstract network variants' that allow that ratio), and through identifying this position or subset of variants, identify a high ratio of info about reality
            - relatedly, other fields of math (algebra) map to other abstractions and interface structures (function/variable networks, function/variable equivalences), just like core variables of math (direction, position, distance, sequence) map to interfaces (perspective, meaning, change, cause), and these fields can be connected in useful ways for interface analysis, identifying for example that 'geometry is an expansion/application/iteration/specification of algebra, as it identifies higher dimensional, specific structures (having many alternatives and containing more specific information bc of the expansion involved into the set of all examples fulfilling the algebraic equivalence as well as the repetition involved in creating geometric structures), specific structures that are associated with function/variable equivalences, in various systems/spaces' and 'algebra reflects set theory (what functions interact reflects what sets of numbers interact)' and 'set theory reflects number types/classes (what sets of numbers interact reflects what number types interact)', these 'equivalences/mappings/reflections' between math structures like 'math fields' being useful to identify and apply in interface queries where 'equivalences/mappings/reflections' are required and involve some specific math structure in the set/sequence of math structures like 'math fields', where 'math fields' reflect an independent set of variables in a format that reflects some ratio of reality similar to interfaces, where 'interface structures that connect math fields' are useful to identify and apply as 'abstract info structures' or 'problem-solving structures' (that connect every difference)

    - identifying useful structures like useful variants of useful problem-solving 'comparison' structures to resolve problems formatted as 'identifying similarities/differences between relevant structures'
        - for example, a 'ratio of relevant sequences' (comparing one high variation attribute by its relative scalar value) has related structures like a 'vertex (comparing a connection point between two perspectives)' and a 'position on a spectrum (where is the position compared to the extremes or midpoint of the spectrum)' and a 'filter set that identifies relevant similarities to compare' which are all variants of the original structure that help with various related intents that help with comparisons/differentiations like 'identify limits' (by identifying position on a spectrum relevant to extremes), and similarly identifying other structures like 'extensions' of comparisons that involve finding a simple structure like a 'scalar comparison of some summary (high variation) variable' like 'identifying different possible matrices/sets/sequences of scalars representing different high variation variables (like abstractions) and identify their common solutions' (their common solutions representing 'overlaps between similarity indexes')
        - similarly, 'identifying other useful comparisons between "relevant/similar but different" structures' is useful as a default set of problem-solving structures (like applied as a problem-solving intent or default interface query component)

    - identifying useful structures like 'interface structures of interface structures' that are highly useful (as in 'determining') and 'connections between similarities in structures' (like similarities in extremes in 'variation' and 'determination')
        - interface structures of independence like 'irrelevant' independence as in arbitrary, indirect, non-interactive structures, and 'general' independence (such as 'sufficient difference' as to be equal to 'orthogonal as in independent in some high ratio of variable interactions', which indicates an interim space between 'irrelevant/absolute independence' and 'direct dependence' containing 'relevant independence such as abstractions/interfaces'), independent variables being useful for resolving differences in general, and this space being useful for overlapping with other 'uncertainty spaces'
            - identifying independence on some graph like an interface graph where different interfaces are represented as 'maximally different directions'
        - identifying the 'useful differences in a problem' like 'abstraction' being useful to resolve the useful difference of 'adding missing info' in an 'overly-specific' function, where the problem is 'connecting extremes like specific/abstract to resolve missing info of over-specificity', and where an opposite problem would be 'resolving missing info of over-abstraction by adding specifying variables', and similarly finding other variants of the problem that are useful to connect/oppose
            - similarly, identifying 'generate/filter' as having useful differences in adding/removing dimensions is useful, as 'generate' is usable to increase dimensions to fit a requirement like 'separable by some structure, like an angle or line', as 'generate' is usable to 'pre-filter' a set so that a 'simple filter (like a position/line/angle)' can be more useful, these 'structures that make other useful structures like simple structures more useful' being useful to apply in combination to fulfill common intents like 'filter' ('functions to generate increases in differences' are useful in that they make simple filters like 'lines' more useful, which are useful to connect cross-interface structures like 'differences/simplicity'), so 'identifying simple filters and then identifying the differences required to make them useful' is an intent to identify useful problem-solving structures like 'functions to generate increases in differences between relevant/similar structures'
            - relatedly, 'pre-filtering a set by selecting a subset of probably useful structures in a simple function (like random)' and then 'changing that subset into the solution to find its distance from the solution, and find its position on some graph, then finding a more optimal graph to connect problems/solutions on' is another set of intents that can replace 'generate/filter'
            - relatedly, 'identifying useful sorts to invalidate filters (like identifying/opposing the most extreme errors first)' is another intent that can replace other filters and possibly invalidate filter intents completely, a useful sort reducing the problem to 'identifying a probable solution area' or 'identify an average' rather than 'filtering to one solution'
            - relatedly, finding 'graphs where a point/line/angle can solve the problem (where a point/line/angle can filter/connect/reduce)' is a matter of finding 'graphs where high variation variables (like problems/concepts) are adjacently connectible' like 'maps between complex systems' and 'maps between interfaces' and 'maps between concepts', and similarly identifying point/line/angles that have multiple functions like 'filter/connect/reduce' are useful to pre-filter the problem space
        - identifying 'specific maximally different structures in a set' (like the 'most useful as in different sequences in a set') is an intent that identifies the 'limit of the usefulness of a set', at which point additional variation is necessary to solve new problems, which is a similar problem as differentiating 'identify interactions of generative variables' and 'identify interactions of generated variables' (indicating the limit of the usefulness of the generative variables)

    - identifying useful structures like intents such as 'new structures to apply new variation between (to resolve their connections)' as specific variants of generally useful intents like 'find new variables' is useful to identify other useful implementation structures like 'different uncertainty spaces' (where new variation is likely to occur)
        - for example, 'identifying new structures to apply new variation between' is a useful problem-solving intent, like identifying how similarities/differences have 'areas of uncertainty' like neutral areas where a structure could be either a similarity or a difference (so its a neutral structure rather than a similarity/difference), which is a source of 'ambiguities'
            - this space between 'extreme similarities/differences' is an alternative uncertainty space than a standard uncertainty space like in the 'regression (polynomials)' problem space
            - identifying all the uncertainty spaces is a way to identify 'overlaps of these spaces and mappings between them and other interface structures of them', and also each individual space is likely to make some connection more clear than other spaces like layered spaces as occur in the regression problem space, which doesnt isolate different variables of uncertainty but rather includes them all
            - applying variation in these spaces is useful to 'identify new variables', a generally useful problem-solving intent
            - identifying structures like 'opposing limits' is a simpler example of finding 'structures to apply variation between' and relatedly, identifying structures like 'directions where a limit hasnt been identified yet' are useful as 'directions to apply variation in'
        - relatedly, identifying symmetries as 'dependence' structures (and as 'having reversible changes') is useful to identify, which adjacently identifies 'causing independence' as a way to 'break symmetries' and similarly identifying other structures that fulfill interface structures of symmetries are useful to identify
        - relatedly, identifying 'equivalences that create maximal differences (that dont violate the equivalences used to create them)' (like the Banach-Tarski paradox) is useful to identify as adjacent structures to 'symmetry-breaking' structures, as identifying the positions of these structures also identifies possible variation in between them, just like its useful to identify 'randomness' as adjacent to 'independence' and 'irreversibility'
        - relatedly, identifying 'symmetry-breaking' structures is a 'source of variation' (a way to create time, by creating independence structures)
        - relatedly, identifying the 'ways that one structures like symmetries can be used for enabling relevant variation' and the "ways its variations like 'symmetry-breaking' structures can be used to enable relevant variation" and the "ways to differentiate these ways like 'ratios of aggregate/similarity structures'" are useful to identify uncertainties that are valuable to maintain (rather than standardizing everything to be a symmetry, maintaining some difference in this spectrum to enable more relevant variation than either extreme can), since there are 'symmetries that are useful to apply as constants' (and the same for their variations) rather than selecting constants or variations for these high variation structures like symmetries

    - identifying useful structures by applying interface structures to identified useful structures to generate new problem-solving intents to filter for useful uncertainty
        - for example, identifying the following questions is useful to direct variation applications:
            - what 'info can a similarity identify', if applied as a standard/base (what variation is supported, and what variation is obvious, once standardized using that similarity)
            - what 'non-standardized info' is useful to compare (in general, what non-formatted info is useful for functions using that format), as in what differences can replace the standardization (such as 'local comparisons within a type' rather than 'comparison across types after standardization'), which indicates which 'standards can replace each other (as alternates/substitutes)'
            - what 'changes can be applied to abstract info structures' (like ratios of sequences) that preserve a range of usefulness (like ratios of non-standardized info)
            - what 'variables contain enough info for a simple comparison' (like points on a plane or ratios of sequences) to be useful (those structures requiring 'high variation storage, such as aggregate/summary info' like a 'convergent or other determining value of a sequence' in order to be useful), which determines 'what inputs do these abstract info structures require'
        - these are 'interface changes of existing useful structures' that contain enough uncertainty, that there could be new useful structures identifiable with these changes
        - relatedly, identifying 'questions to resolve ambiguities between interface structures' is useful (questions such as 'are these structures similar, or are their definitions poorly defined')
        - relatedly, identifying 'volatility' is useful to resolve 'ambiguities' (which involve 'subtle/trivial/adjacent but important/reflective of high variation/relevant differences'), and pre-filtering the problem by identifying what intents could benefit from differentiating subtle differences until theyre obvious or otherwise useful in some way is similarly useful, like identifying ways that high variation structures like 'ratios between sequences' and 'abstractions' can reflect different info but still seem similar in a false way (false as in a 'very low ratio of similarity')

    - identifying useful structures like 'specific variables (such as assumptions)' which prevent other info relevant to problem-solving from being 'adjacently derivable' which act like 'info barriers' (but are not defined to be 'info barriers', which is why its useful to identify them as such), which can be removed to find alternate problem-solving methods
        - for example, identifying that 'analyzing the problem from a position in a problem space or applying the problem as an assumption or default context' results in an 'assumption that there is a solution in the problem space or that the solution is adjacent' which prevents adjacently deriving alternative problem-solving methods like 'removing the problem space causes' which are not adjacent to derive from those assumptions, bc the 'assumption that the solution is adjacent' prevents other methods from being derived, and identifying other structures that result in the increased/reduced 'derivation distance' of other methods is useful
        - relatedly, identifying the most 'un/coordinating variables' is useful as a way to identify probable reality-covering sets/sequences, such as how 'randomly' can be applied to nearly every function/structure/variable set, as opposed to specific structures like 'electrical' which rarely adjacently apply to function/structure/variable sets (except when standardized to a specific interface like an energy/light interface), as a set of defaults to avoid or apply as probably relevant/useful variables
            - similarly, identifying interface structures (in what positions and for what reasons and in what contexts) of a 'structure that seems useful for some intent' where it would not be useful is a useful set to identify, such as how 'applying "primes" in a variable count position for the intent of "adding variation as in uniqueness"' would not actually be useful in most cases ('variable count' would rarely need or be useful to be 'prime'), where the few cases it would be useful in are identifiable (like 'creating spirals'), is useful to identify this 'ratio of useful/useless cases' of a structure and similarly, identifying other differences/similarities between those useful/useless cases is useful to identify the structures that fulfill those ratios/connections
        - similarly, identifying other ways that interface structures ('assumptions') can act like other interface structures like 'info barriers' (similarities in functionality of interface structures) by applying them in different interactions/contexts/variables/other interface structures (like a 'specific problem where this simmilarity is obvious' like the 'problem of identifying alternate problem-solving functions, from a low-info position created by assumptions'), is possible to identify the variables of, to identify other structures that are connected, and identify other types of connections (similarities in potential/connections/requirements/outputs/etc)
        - identifying differences from simpler variables (simple filters like 'combinations', 'maximal differences', 'iterations', 'valid sequences', 'obvious/simple patterns', 'recursions', 'unused structures') is possible by applying additional differences to identify coordinating variables, which are likelier to explain new problems (like how manifolds are defined to allow 'distance expansions/reductions' and 'slope changes' but not other changes like 'connectivity changes' which allow some other set of changes 'shape changes' and 'surface area changes' and prevent others like 'relative position changes' and 'point existence changes (within some subset of determining points allowing it to exist)', so identifying 'variables that havent been held constant' or 'variables that havent been combined as variables with a set of constants' is useful to 'identify new variation' and identifying 'similarities in differences' like 'changes that create other changes'), and similarly identifying these 'change coordinations' are useful to apply as 'interaction level-crossing structures' or 'cross-interface structures' (like structures that connect 'specific/abstract intent levels') as well as interface query components
        - similarly, identifying ways to make complex filters simple (computing a 'similarity index explaining the variation contained by a filter' is useful to make a complex filter simpler to apply by applying the 'insight of the similarity' as a simple alternative to identifying/generating understanding of the whole complex variable, just like computing a 'validity-identifying function for sequences, by checking whether inputs/outputs in the sequence can interact in defined/valid ways' makes a 'validity' filter simpler to apply) is useful to identify, which is useful to identify new variables that are not simplifiable with similarity indexes or other structures that reflect understanding (unknown structures)
        - relatedly, identifying 'similarities to problem-solving structures' (like 'problem/solution connection sequences' and 'inputs to solution metrics like sensitivity') which are likely to have some 'similarities' (like overlaps), is useful to identify possible base sequences to apply in workflows involving sequences (like 'find the point on the sensitivity causal sequence where a problem is likely to exist, then start applying sensitivity causes to the problem to create the solution'), since 'sensitivity' is a solution metric, where this similarity can be used as a 'proxy/substitute', and similarly identifying 'overlaps between multiple solution metric causal sequences' is useful to identify structures that solve for multiple solution metrics and the connections between those structures
            - relatedly, identifying structures that fulfill multiple solution metrics, like how 'abstractions' (such as an abstraction of similarity like an 'abstract average (which covers most info in a set)' or a 'ratio between abstractions (a ratio between high info structures)', both structures capturing a high ratio of information when applied with abstractions) fulfill both 'generalizability' and 'accuracy', is useful to identify other probable solution sequences for solution metric sets
        - identifying 'more achievable/adjacent/useful intents' like 'determining the direction of causation between high variation variables like sensitivity and volatility' (which should be quite achievable with existing resources, 'direction' being a very measurable variable, and causal networks being usable to resolve the interactions of these variable definitions, and both of these variables being very 'well and completely defined' and being very useful to determine the 'causal direction' and other interface structures of these 'high variation variable connections') is useful to identify as 'more useful intents to solve for than some problems' which are useful to identify and apply as a problem-solving intent

    - identifying common variables (like 'efficiency' or 'relevantly similar but different') of useful structures like 'probability sequences' and 'relevant double meanings' to apply as optimization/efficiency structures or base structures to standardize other structures to or apply as a base for changes to create different structures
        - for example, 'probability of x, given some probability of an input y' (similarity in the probability connection created by some set of different structures) and 'relevant double meanings' (different meaning for the same word) are both 'connections/similarities between relevant differences in interface structures like probability/meaning', so identifying all the interface structures that are 'similar but different' in an efficient way (like re-using the same word or connecting independent variables) is useful to identify possible new useful structures
        - relatedly, identifying all the interface structures that can be standardized/connected to these specific 'similar but different' interface structures, like how 'input/output sequences' are connectible to 'probability of x, given some variable y (presumed to be an input)', are useful to identify where there are missing connections and what interfaces explain these connections (a similarity in usefulness of a structure can result in commonness of a structure and therefore cross-system similarity from that structure, which can override 'independence' of variables, a hierarchy connecting interface structures like similarity/commonness/independence/probability that is useful to identify by identifying alternate definitions/routes which can provide similarly useful/powerful structures, like how 'similarity in usefulness of a structure across systems' can identify causal connections even across independent systems through 'commonness of useful structures', bc 'frequency' is a high variation interface structure in its connection to 'probability', so systems which are independent from each other still arent independent from interface structures like commonness/usefulness/similarity and arent independent from iterated effects, like how an independent system might let another system develop the same useful structures, making it likelier for another system to develop those structures through its independence, where if a system is independent enough from its useful structures and common enough, other systems might be able to develop the same structures, increasing the ratio of independence, a ratio that is useful to identify if its self-sustaining, a generally relevant variable)
        - relatedly, optimizations of these specific 'similar but different' interface structures (like optimizations of 'probability of x, given some probability of an input y' such as 'cases where these variables are on separate causal sequences or in separate systems or can be both inputs or outputs or are indirectly related, but still adjacently connectible, like how common structures tend to have variables in common even though these structures may develop in different systems, which makes the probability more surprising as in maximally different when simple, and therefore this probability sequence is more useful in those cases where an adjacent connection explains causally distant structures') are useful to identify a network to identify useful positions/connections of these useful structures to indicate their optimal usage
        - relatedly, identifying different structures than 'similar but different' by applying interface structures like units/waves/grids such as a set of 'waves that vacillate between related useful structures unlikely to be resolved like spectrum variables' or a grid that regularly applies some structure like a 'similarity/filter/variable' or a 'spectrum grid' is useful to identify possible alternate useful sequences/networks to apply as components of workflows
        - similarly, identifying the other useful interface structures of interface structures is useful (identifying 'sequences of useful interface structures' is useful to identify interface queries or their components and identifying 'combinations of useful interface structures' is useful to identify structures like 'cross-interface structures, like structure/function sets, such as combination-differentiation or sequence-distribution or set-filtering')
        - similarly, identifying cross-interface connections like how 'lack of meaning' is associated with specific 'functions' (like 'iteration') of a 'structure' is useful to identify as a limit/filter for interface queries

    - identifying new variation by identifying new interactions between interface structures to solve problems with, by identifying sets that are likely to interact in useful ways (applying a 'specific context' like 'the uncertainty space in regression' and a 'filter' like the 'threshold indicated by change/info structures where a pattern becomes obvious' and a 'specific intent to solve for in that context' like 'identifying the concepts that cover all similarities/differences in that space like volatility/specificity' or 'identifying the concepts that allow the most variation in that space' or 'identifying the "limit-reversing variables" or the "most stackable/embeddable variables" in that space')
        - for example, identifying 'integrations of orthogonal useful structures' (orthogonal as in 'cross-interface' or 'not using definitions in common' or 'causally alternate/distant' or otherwise independent useful structures of interface structures) like the 'change structures like difference ratios and info structures like information minimums that make a pattern obvious' and the 'uncertainty space between linearity and randomness in a relevant space like regression' and 'similarity indexes in a relevant format like polynomials' is useful as a way to identify new problem-solving intents and functions, where these structures arent exactly standardized to be directly mappable but are integratable (such as 'identifying functions/positions in this uncertainty space not covered by identified similarity indexes' and 'identifying thresholds that make a variable like volatility or specificity obvious in this uncertainty space'), which are likely to be useful in identifying the structures in that space, 'integrating maximally orthogonal structures' being likely only possible in this space and also useful for identifying the highest variation-covering structure in that space
        - identifying the most orthogonal/different/independent interface structures is similarly possible to automate and similarly useful for this problem-solving intent
        - 'limit-reducing variables' are the variables (like abstraction) mentioned elsewhere that increase variation after some reduction in info/variation
        - these 'useful structures' have differences that would be valuable to connect to create new variation, and are 'iterated structures' (structures of interface structures, as opposed to core defined interface structures) and are useful in their specificity, so identifying their interactions and the useful sets to apply them with (context/filter/intent) is useful
        - applying these math abstraction structures (like 'find an optimal ratio between two relevant sequences') to interface structures is useful ('applying useful problem-solving structures to interface structures' as a problem-solving intent)
            - for example, the 'ratio of opposing functions like growth/reduction functions that allows structures to exist (favoring growth enough for anything to exist)' is useful to identify as well as the remaining variation in these functions that hasnt been identified yet (what useful increasing sequence could exist that hasnt been identified yet), where the 'opposition' makes the opposing function useful, which is useful to identify other structures that make other structures useful, and similarly every other 'ratio between interface structures (like function sets, interaction levels, etc)' that allows variation to exist are useful to identify as limits beyond which errors would occur
            - similarly, 'defaults' are usually not useful structures (though some trivial ratio may be, which is why they exist) and rather iterated structures differentiating from defaults are the useful structures in comparison
            - identifying relevance structures which involve a 'similarity to base differences on' (like spectrums like 'adjacent/distant' to differentiate adjacent/distant structures within the same spectrum variable) as possible variables that change 'usefulness' is useful as a problem-solving intent
            - identifying all of the 'common sources' of useful structures by applying interface structures to identified useful structures (like 'graphs') to find overlaps between these sequences of interface structures is useful to find 'queries to generate useful structures' as well as their common variables, where these 'common sources' can form a network to base changes on to find other solutions
            - similarly, the 'structures' of the interface network are useful to identify (its limits, its variants, its useful iterations, its iteration limits, its optimization limits, etc)

    - identifying useful structures like 'geometric interface structures' (like the 'variation around a line', where the line is the base for change, such as in regression) and a way to integrate those into related identified problem-solving structures like 'geometric intents' (such as 'reduce a problem to a point on a plane') is useful as a problem-solving index to compute (such as 'identifying the highest variation-supporting lines in a problem-space and convert those lines into possible planes where the problem can be reduced to a point, then filter the possible planes'), which are useful applications of the abstract/specific interface to generate a workflow ('specify a geometric structure' and 'use it to connect abstract info structures')
        - identifying 'anti-interfaces' is useful as oppositions of the definition of similarity/symmetry in some way (different in that they indicate 'similarities in differences' or that they indicate 'constants' rather than some structure of variation, or that they limit change rather than supporting change, etc), which is useful as an alternate reality-covering variable but also identifies the more complete set of structures that should be applied in coordination (an anti-interface and an interface being a useful vertex to solve problems on, for example), similar to how identifying 'structures that vary around limits' as opposed to 'structures that vary around averages/similarities' are useful as different variants of interfaces
        - relatedly, identifying the structures that a problem can always be standardized to (such as identifying whether there is 'always' a small variable set that determines some system) is useful to connect these structures, such as how there would likely always be low-dimensional generative variable sets of a problem that can identify 'at least a grid or network (adjacent structures to a plane), if not a plane', and identifying connections between these known planes is useful to 'identify new variable connections (new insights)' as a new problem-solving intent ('identifying new connections between known solution structures like identified planes that describe a problem/solution' is likely to contain solutions to new problems)
            - relatedly, 'identifying new variable connections' is a proxy for 'identifying new variables', as these new variable connections can often be applied elsewhere to 'identify new variables'
            - relatedly, the structure of a 'plane that maps problems/solutions' is useful to identify, as a 'generative variable set of problem/solution connections' is a useful problem-solving structure to cover some ratios of problems
            - relatedly, identifying the set of 'planes' where problems/solutions are mappable by low dimension counts like two dimensions is possible bc some variable sets are reality-covering (like for example 'general' and 'accurate'), and identifying connections between these known planes are useful to identify new variables/new connections between variables, though such a general variable set is likely to have a different structure than a structure implying a 'different solution for every different problem' which is what a plane implies
        - applying insights and connecting them to problem-solving intents is useful, such as how applying the insight 'all structures can be interactive, bc measurable structures arent completely independent', so identifying new ways for structures to interact on some angle of difference like complexity/opposite is possible as a way to identify variation and therefore alternate timelines
        - understanding as a problem-solving intent, as it reduces required work, for example noticing that a 'war winning' ('conflict resolution', by 'selecting one entity as a winner') problem can be converted into a resource distribution problem ('distribution of resources, as a proxy of distribution of justice') is useful to understand that problem-solving is fundamentally a 'variation-moving' process (moving the variation from 'selecting a winner' to 'distributing justice'), which reduces the work to 'finding ways to convert/preserve variation across transfers to different core functions like select/distribute/differentiate' which are different types of differences, which requires work like 'identifying this core function network' as opposed to 'selecting a winner of a conflict', which results in other problems like 'avoiding conflicts' and 'helping one side win' and 'getting consensus', which is more work and more repeated work than the relatively optimal 'identifying this core function network'
        - identifying 'understanding' in a problem space is a matter of matching abstract structures like symmetries ('differences in similarities', like limits such as physical laws, between which variation can occur, which is like how variation can occur around symmetries) and anti-symmetries ('similarities in differences') and other symmetry structures in a problem space
        - these structures have a common geometric root of 'variation around a point' (variation around an average), 'variation around a line' (variation around a connection), 'variation around a plane', 'variation around a network', 'variation within a spectrum' and other simple structures which often solve problems, which are useful as a problem-solving intent (find all the ways that known variable interactions can be formatted as these core structures of symmetries and anti-symmetries and spectrums like variables/constants)
            - then the more complex structures can be found by connecting these units of variation, like 'variation between relevant networks' and 'variation in supported embedded variables on a network' and eventually connecting these units of variation to interfaces like concepts such as 'variation potential' of an interface
            - identifying these mathematized abstract variation units are useful in that they apply understanding of problem-solving as a problem of 'identifying bases around which variation occurs or identifying limits of variation' and 'identifying abstract variation', the resulting intent of these insights being that 'identifying these mathematized abstract variation units' is the most useful work (relative to other function sets, like identifying specific implementations of one vertex like generate/filter as a one-perspective problem-solving method that captures less variation than 'identifying geometric abstract info structures and apply them as composable units of variation/time/reality')
        - relatedly, creating an 'infinite abstract info observer' is a useful concept in computing new math structures (a computer whose base unit is an abstract info structure like abstract concepts that cover reality, or other high variation structures like number classes that cover reality or other types of infinities), since measurements/descriptions can only be as good as the observer's ability to measure/interpret variation correctly
        - identifying 'volatility' as a filter of algorithms is useful (such as 'reducing volatility' to 'reduce complexity by reducing adjacency of different structures' of 'finding similar structures') as a generally applicable structure that can also improve algorithms when relevant (connectible by complexity/adjacency/difference) to structures like common intents like 'find similar structures' that are relevant to problem-solving
            - similarly, identifying structures like 'sequences/combinations of variables like volatility' in the 'uncertainty space between linearity and randomness' is useful to fulfill other intents like 'identifying equivalent interactions and identify stackable/embeddable interactions'
            - relatedly, defining 'randomness' as the 'interaction of similar variation-containing structures (as opposed to similar as in coordinating structures)' is useful, like how a positive sloped line could be made to look like randomness with an opposing negative-sloped line (an opposing structure that could be created randomly by multiple units of that slope) where the 'ratio of variation contained in either structure' is similar (the 'negative line is different enough from the positive line to look like randomness, if both are sampled', and the 'multiple unit combination is different enough to create that difference')
        - identifying structures that coincide with 'irrelevant' usefulness (like irrelevant errors/solutions) are useful to identify (as an opposing structure of relevant usefulness), like 'being in a position near a high variation structure' being likelier to increase probability of 'random errors/solutions', where these irrelevant errors are useful to avoid by applying filters like 'filters of solution/error high variation structures' and functions like 'increasing causal degree to increase independence' (such as a combination of functions like 'increase causal degree by one, then check if either the solution/error filter is triggered' which coordinate to fulfill an intent, in cases like where some 'adjacent change might trigger an approximately applicable/similar filter', a useful structure set to apply as a component of workflows)

    - identifying useful structures like 'changes in spectrum variable change sequences, resulting from problem/solution identifications and system interactions' which are highly determining of other structures
        - identifying 'removing variation in problem spaces' as a suboptimal solution structure as it 'prevents the resolution of complex variable interactions'
        - identifying problems caused by 'changing position of variation/constants' and their solutions is useful as a common problem resulting from solutions (when a solution to a problem is found, the solution is held constant and the variation is applied or develops elsewhere, often adjacently, and sometimes caused by the newly constant solution structure)
        - identifying a 'grid of reality' where, for example, 'embeddings/generators and filters/constants are alternated' reflecting the 'structure of queries', such as a 'variable/constant' alternating query, where structures that add variation (like abstractions/embeddings/uncertainties) are applied to oppose constants/filters/specifications/certainties, and identifying the ways that these grids can be integrated to allow all the known possible queries that reality allows
        - identifying 'sequences of relative optimals' (like 'heavens') are useful to identify as 'probable sequences', which are likely to become more optimal as the sequence progresses and are likely to be used/identified
        - identifying 'definitions that allow variation' as 'default/constant' structures including 'variables' (definitions that increase the variation of other definitions) is useful as a way of identifying 'unsolved problems' (resolving 'all the differences between variables and constants', which is unnecessary if 'most determining/differentiating variable connections' are identified, given the 'set of differences already identified', which is different in 'different problem spaces and timelines and positions', as some problems only exist if there are 'other sequences of problems already solved' or 'other co-occurring problems/solutions')
        - identifying the structures of 'types' as 'relevant embedding stacks' where vertexes across embeddings are useful to identify ('identify the vertexes that differentiate two different types' as a way to reduce the 'classification' problem to a 'vertex-identification' problem)
        - identifying structures like 'ratios of abstract structures' (like the ratio of 'simplicity/complexity') as a way of 'determining probable structures', given that the 'simplicity/complexity of a problem space' may be changed by some solution temporarily, but will often show 'vacillation patterns', returning to some previous ratio when an 'opposing solution to that solution is identified', as 'opposing intents' often co-exist in systems rather than always being resolved immediately
            - similarly, determining ways these ratios can change (other than simple known structures like 'vacillations') is useful, since they act more like 'bases or limits of change' than absolute constants
            - given that 'useful structures' determine 'intent structures' much of the time (once a useful structure is identified like a resource, an intent develops to 'get/use that structure'), similar to how 'intent structures' determine 'function structures' (once a goal is identified, functions are developed to fulfill it), identifying these 'cycles/sequences between abstractions or interfaces' is useful to identify as a base for queries involving 'determining sequences' (like 'filter sequences') as well as identifying 'useful indexes to compute' and 'ways to skip ahead in a determined sequence (like from 'useful' to 'functions', either skipping intent or applying an intent index to connect these)
            - these 'structures' (like ratios) of 'abstract structures' provide a way to identify 'outer limits of complexity, using some structure set', which can be applied as a limit to differentiate from 'already computed ratios' (like 'ratios between relevant sequences') and determine the complexity in between this set of limits (determined by a 'constant' and a 'computational limit' of high variation structures like 'abstract structures'), 'identifying limits of variation in opposite directions' being a useful problem-solving intent

    - identifying the useful structures like abstract useful structures like 'cross-interface structures' (like 'intent-alignment') and 'network networks' that can be possibly useful and identify their useful position on a network in connection to other variants of these structures
        - applying 'alignments' and other useful structures as a 'base network to start integrating with other networks' (representing solution metrics or other bases and so on), such as how the 'predator group' might also benefit from 'reducing the number of predators', as well as 'other groups' benefitting from a 'reduction in the number of predators', as the remaining predators would have 'more success/resources and therefore fewer problems to solve with violence in the first place', and identifying all the alignments or the maximum possible coexisting alignments is useful to apply as a base network (given these 'alignments in intents/incentives' as a core structure and given this 'variant of an intent/incentive alignment network that maximizes freedom for agents', how can society/variables be built around this 'optimal intent/incentive-alignment network variant', such as 'one predator per village, where they are given permission to prey on other predators to keep the ratio low, bc society still needs killers of predators, and if they dont use the permission for that, they become targets, in a predator-management app/internet', which is useful as a interim step to other solutions like isolating predators, which is more trivial once already reduced in number, and as an alternative to simultaneously integratable solutions like 'offering incentives to develop intelligence automation and other tech that can end the problem of predators, like giving them time/resources to solve problems like resource distribution or be killed'), which is once again a 'relevant ratio between structures like groups, this time "opposing groups (predators/victims) that always oppose each other, until a ratio is crossed"' which has a related structure of 'intersections between seemingly unrelated structures', where the 'direction of change after the intersection is useful info to solve the problem'
            - similarly, identifying the 'optimal position' of these cross-interface structures, vertexes, combinations, orthogonalities, queries, and other 'structures of interface structures' is useful as a problem-solving intent (whether to apply intent-alignment as a network, specifically a base network to apply/fit other variables to)
        - applying useful specifications to useful structures like 'abstract solutions' such as identifying 'new variables (like the cases not solvable with abstract solutions, indicating a new abstraction)', for example, specifying that a 'solving a problem by identifying the emergent ratio between relevant structures like sequences' is useful specifically when there is a change structure (like a 'change rate change' or 'change rate extreme' or 'change rate interim/average' that creates a relevant difference between linear/exponential change, for example) represented by the solution resolving the comparison of the sequences
            - similarly, 'solving a problem by reducing it to a point on a plane' is useful specifically when a 'problem space contains high variation and some variants of the problem are likely to be solved' (so identifying their plane is useful to determine 'position in relation to the other problem variants')
            - similarly, 'solving a problem by identifying a useful index to determine completely (as a way of covering a high ratio of info)' is useful specifically when 'new variables are available to try to identify new indexes, or many different indexes already exist, or the index that would be useful is computible or the computible examples in the index are more determinable than their generative function'
        - identifying the interface structures like problem/solution types associated with these abstract interface structures (like 'abstract solutions') is a useful problem-solving intent
        - identifying the differences in spectrum variables like complexity/abstraction in implementations of abstract structures (like abstract filters) is similarly useful to identify the complete set of determining/generative variables or example structures making these variables trivial to identify (variables and the structures they generate/determine/describe being similarly useful to identify connections between)
        - similarly, identifying the differences between abstract structures like 'abstract filters' and abstract solution structures like 'ratio of relevant sequences, where one crosses some powerful change structure' is similarly useful (differences such as 'how much info/complexity/variation they cover', etc)
        - relatedly, identifying 'opposing/invalidating cross-interface structures including structures like spectrums' such as how the 'selectivity' of a filter can offset its definition as a filter (an unselective filter isnt a filter, so variables that change selectivity of a filter are equivalent alternates to filters) so the 'selectivity-filter' structure acts like a 'spectrum/tradeoff' (where too much of one invalidates the other, and theyre also equivalent alternates in the power/variation they can cause, and there is a dependency between them, 'filters depend on selectivity' and 'selectivity requires a structure like a filter to exist'), and the selectivity depends on the filter for the selectivity to have any structure/power/variation at all as its embedded in the filter, so the opposing/invalidating set of the 'base' filter and the 'intersecting variable' selectivity is a useful cross-interface structure to identify, as an alternate to other useful structures with similar simplicity/structure like 'vertexes' which have an overlap in their 'spectrum' structure with these opposing cross-interface structures
        - the intent of 'identifying all "numbers/number-generators", fulfilling some metric like "adjacently" (indicating a different starting position and similarity type), by applying some structure set (like interface structures)' is a useful problem-solving intent (which would identify sequences where timelines can occur), just like 'connecting the math interface to other interfaces using abstract info structures, by identifying the interface structures that are connectible to abstract info structures, in a network of abstract info structures, applying abstract info structures as a base of other interfaces (as opposed to the meaning interface, as a similar alternate)' is a problem-solving intent, all of which involve 'connecting interfaces', where 'applying the math interface' is formattable as applying a 'set of extreme specifications (given that it contains all references between all specific values)' so 'abstract info structures' including abstract concepts are particularly useful at connecting these 'extremes, which are specific iterations of other structures', given the 'high ratio of variation coverage' of abstract structures (which can easily connect extremes), allowing problems like 'find out if a structure has a variable/function' by 'identifying the networks of variants of that variable/function as an "abstract info structure" and checking for an overlap'
            - similarly, 'identifying filters of universes' is an invalidating problem-solving intent of other intents
            - similarly, identifying the 'similarity structures (like connections) that can vary the most' in the math interface is useful for intents like 'identifying the bases of the math interface' (bases which indicate its generative variables, as the generators of those base similarities), where bases can also be 'emergent structures, which integrate many changes after being combined' as opposed to 'base structure, which support many changes after being connected', these connections being useful for 'identifying the future output interfaces as well as the identified input interfaces' as well as identifying 'limits limiting other changes (like changes between interfaces)', which is useful for 'identifying where variation might encounter a limit' (indicating that variation has to go in another direction or structure)

    - identifying useful structures by which variables havent been connected yet (like "abstract/specific indexes' usefulness break-point, where abstraction/specification is more useful as a general base/limit than a specific filter representing some specific fact", and 'interface-switching cases' and 'workflow/system usefulness indexes' and 'connections between indexes') which can be connected
        - 'true' statements will be maintained in variable contexts so they dont need to be verified all the time which is an 'optimization' structure (regularly checking the truth to make sure its still true, as opposed to always checking if its still true)
            - similarly, applying structures to create truths (like applying a 'known false structure' in an isolated system to see if it can survive and then giving it resources to help it survive) is another way to filter true/false statements, as a way of identifying 'steps to create a fact', where number of steps above a certain threshold indicating that a structure is definitely false or impossible
            - relatedly, identifying that an optimization optimizes some metric 'up to n steps' is useful to identify 'optimization break downs' (where a useful structure stops being useful) and 'routes to optimization between adjacent variable changes like variable count changes', which I thought about after wondering what metrics commonly define existing useful structures (like reducing equations to a few variables) and wondering why that is useful and how it changes and how other variable structures could be optimizations, and similarly identifying 'routes to optimization between non-adjacent changes' is useful as an opposing network to adjacent changes (while those non-adjacent changes are still non-adjacent, which they wont be, once their network is identified) for algorithms to vacillate between
            - relatedly, 'false' statements will increasingly reflect only the remaining unfulfilled intents/incentives/opportunities (as in 'opportunities to contradict truths'), like 'unverified truths', where the 'intent' removes the opportunity and the falsehood isnt real as a result (the falsehood loses its power bc of the 'obviousness' of the intent and the opportunity left by 'lack of verification' or other truth proving methods), the point being that these incentives can be computed easily, being obvious and simple, so they are already false, as uncertainty-certainty connection structures (as in 'something incomputible') are more real than specific certainty structures ('known simple/obvious incentives')
        - relatedly, 'concavity' is useful to identify as a better store of info for some intents, compared to other metrics like 'minima/maxima', as this variable contains info such as 'curvature', 'direction of change' and 'position on/similarity to a wave' (as most polynomials are required to change their concavity, so identifying whether the position is a concave position or otherwise is useful to identify the 'remaining concavity of the polynomial', which is useful for 'regression'), just like 'skew' is useful to identify bc of the info it contains, acting as an approximate index identifying the 'rest of the polynomial from the skew of a subset', where there are 'areas of missing info' in this variable that can identify other variables to contain the remaining info ('concavity' doesnt have a 1-to-1 mapping to polynomials) and similarly identifying other variants of different 'variable combinations containing different info or having different missing info' can be generated in this problem space, which is the point of this example (that its interface structures can be used to generate other variants)
        - relatedly, 'equidistance' is useful to identify, such as in identifying a 'grid' (bc it allows everything to be formatted by 'combinations of the same unit', thereby allowing a structure to become a useful structure by 'repetition/repeatability') or identifying a 'overlap between solutions/solution-finding methods', and similarly other structures that make a structure useful are possible to identify, by the simplicity/availability of the functions (like 'repeat') that they make more useful
            - relatedly, identifying 'congestion of traffic' as a variable of 'lane-switching' functions is useful to 'identify when its more useful to use the function', such as 'when congestion is resolved', bc of the 'lane-switching opportunities/incentives' created by an increase in 'distance between cars', bc 'lane-switching' is less likely to cause errors when there is 'less congestion', which aligns with the algorithms identified by the 'equidistance' variable, which identifies 'equidistance' as a useful structure in one case (when the 'distance between cars' and the 'size of cars' is equal, it starts to become 'more optimal to switch lanes'), and then identifying the alignment of that metric for other cases, like how 'other functions are more possible once this equivalence is true', which is an 'overlap between solutions/solution-finding methods'
                - similarly, being 'equidistant' between positions where various functions are applied (such as being between 'far/close to other cars') makes most solutions adjacent and more likely to be approximately similar and therefore make them likelier to be approximately correct ('make solutions adjacent' being an optimization structure to make more structures likelier to be correct in more cases, using those connections that make them similar/adjacent)
        - difference in contextual question/hypothesis 'this attribute is present' and the associated answer 'true/false', which is possible to oppose when a variable (like 'positive/negative') is embedded in the attribute, which can negate the rest of the question/hypothesis (the embedded variable becomes the base, rather than the question, as it can control the base question, where the question begins as the base bc it allows the embedded variable to exist), and these 'interface-switching cases' like 'powerful embedded variables that can invalidate their base system like their base question/connection' are useful to identify
        - the 'reflection' of variables/structures of workflows in the systems/structures theyre applied in, as in 'when a workflow is applied in a system, what does that indicate about the system' is useful to apply as a 'similarity index' around which to base changes on to identify new solutions/variables
            - for example, if the workflow 'change a base solution' applies to a system, what does that indicate about the system (that it has a requirement to trivially change some existing structure to solve some trivial difference representing the problem created/allowed by a system, indicating that the system where this workflow is useful has/allows 'trivial differences which are problems', and other interactions identified by interactions in and of the system), which is useful to identify new workflows by systems/cases/other structures where no workflow is optimal yet, and identify 'workflow-switching functions' using this 'workflow/optimal structure/relevant structures of those structures' index, as 'identifying functions/structures to connect useful indexes' is useful as a problem-solving intent
        - identifying the differences in usefulness created by 'specification/abstraction' is useful, such as how identifying a structure by its abstract representation might be so abstract its difficult to identify the exact structures represented by it, as the abstractions allow other interpretations which contradict it in some cases, but these abstract connections are still useful to identify as 'general limits or default base structures', even if they dont add value in 'very specific selectivity/filtering' intents bc of the difference in structures allowed by that abstract structure
            - similarly, specifying 'skew/concavity' is useful rather than saying 'different variable combinations specifying different incomplete info (having different missing info)' bc of the missing info in that interface definition of 'skew' (a definition that doesnt cover reality)
                - the reason its useful to specify 'skew' is bc variables were left out of the interface definition above, specifically leaving out 'which variables are being combined', such as "variables on the same interaction/complexity level as 'change direction', 'change rate change', etc, which can adjacently combined or changed to create the rest of a function from a subset", and also the specific problem space of 'regression' is being left out, variables which add useful specificity to 'different variable combinations specifying different incomplete info (having different missing info)', meaning that other problem spaces and interaction levels of variables and other interface structures can be applied with this abstract 'skew' interface definition to generate the variants in other problem space that are useful
                - this means that 'generating combinations of interface structures, then applying specifying structures to that combination, to check if it can describe new info/variation' is a useful way to determine structures of interface structures that will be useful as in 'applicable across systems', 'reusable', etc
        - relatedly, identifying structures that are 'adjacent as in similar to another structure' is useful bc these adjacent structures can be either 'combined with overlaps' or 'changed adjacently' to create the other structure, and its useful to identify the ways that these core interface structures can be applied in combination to create these equivalences (like 'combined with overlaps' or 'changed adjacently')
        - relatedly, venn diagrams as nodes in a network diagram are useful to represent abstract concepts rather than node/edge network diagrams bc of the definition of abstract concepts which indicates a structure so general that it covers reality, in which case it will necessary overlap with other abstract concepts or the components of their definitions
        - similarly, identifying similarities in related structures like 'filters/causes' is useful to identify useful structures (after wondering 'what is the difference between interpretation and generation, where interpretation involves mapping/filter and generation involves identifying causal variables/sequences' and noticing they have an 'overlap' in that some filters are useful as causes and vice versa bc they both are related to differences, and also that filters are related to generative variables as an opposing structure in problem-solving)
        - rather than a 'network matching problems with existing solutions' (which can be manipulated to promote false solutions), a 'solution combination network, given some solution metric like available resources' to avoid having to solve those problems or make those selections between solutions given the lack of uncertainty about optimal solutions (given some available resource level, here are combined solutions to solve known co-occurring problems that have been pre-optimized/filtered)

    - identifying useful structures of optimizations to apply such as 'specific, common, opposites/alternates to high variation variables, taking the form of similar/complementary info, covering high ratios of inputs like cases, opposites/alternates which can be combined/added' which are useful to store as the 'original sequences used to connect them' which are likelier than sets to retain the relevant info, are useful to identify by connecting relevant structures like 'specific/filter/similarity/probability' structures to optimize or invalidate some core function like 'filter' by identifying/creating 'pre-filtered info'
        - applying 'error' interfaces like 'false similarity/ambiguity' interfaces as an 'opposing structure of interfaces' to identify the 'directions/positions/areas to avoid in queries' since applying this as an interface involves identifying the similarities across 'false similarity' errors, as 'independent error sources'
        - identifying the iterations that are detectable when over-prioritization errors have occurred (the info indicating 'over-simplification' and 'missing info' such as 'unpredictable errors')
        - identifying 'linear point sets' (like 'bottlenecks') as opposing structures of 'densities' to connect to identify solution functions (given the 'specificity' and 'commonness' of exclusive linear subsets in a data set as opposed to large areas/densities of a data set), which contain 'similar or complementary info' about the data set, and are useful as 'pre-filtered info', in a high ratio of cases, all of which are 'additive' optimization structures, optimization structures being useful to identify optimal structures of (like 'sequences/sets' and which of these structures is optimal)
            - relatedly, differentiating 'areas of possibility' and 'densities' is useful, bc not every connection between points in a large area is equally possible, but the density does represent similarly probable possible connections
        - identifying 'sequence similarity' and 'subset similarity' and other types of similarity in regression is useful for identifying different useful formats, such as 'co-occurring sets' and 'usage sequences'
        - identifying balancing algorithms between 'extremes' like 'tradeoffs' where extremes should be avoided (such as an 'alternation algorithm to create an approximation of an average to create an average', or an 'algorithm to orthogonalize/invalidate extremes by changing bases/abstractions or stacking embeddings/limits/specifications')
        - connecting concepts to identify changes (like 'randomness') that produce error/difference structures (like 'entropy') in specific problem spaces where these are clearly defined and connectible, to identify these 'conceptual connections/sequences', like where 'increased interactivity' (like from hyperactivity/anxiety) leads to 'increased randomness' and errors are more obvious under randomness conditions which lead to destabilization of any fragile structures that are similar to errors in some way (errors like 'entropy/decay of organization')
        - identifying 'connections across interaction levels' is useful as a 'high variation structure connecting relevant formats' like 'connections across specificity interaction levels' which are useful in regression
        - identifying specific structures of relevance like 'waves' which contain 'relevance' as in 'similar but different' structures is useful, as is the general variant, which is 'mixing similarities across interfaces and interface structures' (like 'different power, same variable') and identifying mixes of these structures which are 'undefined' and a 'source of possible variation'
        - identifying the 'grid' format of a network like the network of maximal differences, and identifying how this 'maximal difference network' relates to the 'interface network', and how to make these maximal differences 'continuous' ('continuous' as in 'connectible to multiple variables as opposed to one/linear set') and 'unitary' (as in 'trivially changeable to generate the other maximal differences') and how to identify specifically maximal differences of useful structures (like maximal differences) are all useful related intents to problem-solving intents
        - identifying 'structures like angles/interaction levels/abstractions/limits' in networks that linearize networks are similarly useful to identify
        - identifying opposing variable sets like 'angles/interaction levels/abstractions/limits' and 'directions/positions/areas' are useful to identify and connect
        - similarly, identifying/generating 'false signals' of a structure (like a unit/component/output of it) to 'determine whether its a useful structure', based on 'what uses it, in what structures' (bases and dependencies being useful alternates to determine usefulness)
        - relatedly, "changing the 'definition' of relevant structures to identify future relevant structures and connections to these" is a useful problem-solving intent, such as changing 'relevance' as in 'adjacency' to mean 'within some complexity/iteration range possible with some structure like degree/count of adjacent structures', which is useful to connect to adjacency as a future variant of relevance
        - identifying orthogonal variables is a matter of identifying lack of 1-to-1 mappings but also identifying 'lack of requirement', such as how 'equivalences' arent always positive so they cant be mapped directly to the positive/negative variable, but also that 'equivalences' arent required to have a value in that variable (similar to how non-interactive variables tend to be independent)
            - relatedly, identifying the usefulness of different types of independence (such as absolute independence where all interactions between a variable set approximate randomness, or distance independence indicated by structures which can seem similar but dont interact)
            - connecting independence/difference with averages/similarity is useful as another variant of a useful difference to resolve
        - identifying the differences that are useful in generating useful structures like 'general (in variation support) but specific (in trivial but relevant differences from other abstract info structures)' structures (as opposed to generally 'similar but different') as a way to identify useful structures like 'filters' which are 'abstract info structures' bc they have a trivial specific structure that is identifiably different from other 'abstract info structures' which supports a 'high ratio of variation', as variants like iterations of difference/similarity structures like 'similar but different' or 'different but similar' ('generally difference-supporting and specifically similar to other abstract info structures' being a useful iteration of general/specific applied to difference/similarity to create useful structures like abstract info structures, as another way to identify these useful structures than just iterating combinations of interfaces like abstract/info/structure, since everything can be formatted using an interface like 'difference/similarity', and structures on this interface happen to be connectible to abstract info structures by applying one more interface as in 'general/specific', which encapsulate similar variation like how 'specificity/structure' have an 'overlap')

    - identify useful structures that connect to useful structures like 'randomness' (like solutions that depend on 'randomness' to cover a 'lack of info' problem), as these 'suboptimal solutions to errors' connect to 'randomness' and also are positions that are also 'optimization opportunities' (optimization being lacking in a suboptimal solution)
        - intent to 'enabling/applying difficult/complex/non-required structures like tests/growth' as an implementation of 'allowing the most variation' which is a 'general intent to solve for in reality', even if its not specifically or directly relevant to some problems, so 'identifying different non-required structures that might be useful, once higher variation evaluation functions are identified (like different iterations/combinations/directions/uncertainties)' and then 'connecting these non-required structures, to identify their common variables' and 'identifying structures that can act as a base for them, to identify possible realities where those can exist' is a useful implementation of that intent to 'allow the most variation', and identifying other reality-covering intents to optimize reality are similarly useful to identify, connect to other intents, and connect to workflows
        - 'identify powerful variables causing/describing a problem to change or a solution to change' and then 'identify structures to oppose those powerful variables' is useful as a way of identifying 'required differences' to solve a problem
            - for example, the solution of 'distributing resources to war zones' vs. the solution of 'extracting innocent agents and bombing the war zone' is a difference in a determining/powerful function (as in 'distribution (sending resources to war zones), then assuming random chance will cover the remaining variation in required distribution (assuming the resources will reach the intended positions)', vs. 'identifying/differentiating/changing positions of distribution (moving them out of the war zone), then distributing once differentiated (once theyre in a known specific position rather than an unknown general position, distribution at that point is trivial)') which is 'reducing the "distribution" variable to a problem of "differentiation", bc once differentiated, distribution is so trivial it may as well be removed at that point', as rather than assuming randomness will filter the remaining variation sufficiently, reducing this variation by differentiation avoids depending on randomness
        - identifying 'other structures to avoid using randomness (and every other useful structure, as randomness can be useful as an error structure in problems of lack of info where the solution cant be randomness)' in function sequences/networks is similarly useful
        - identifying connections between problem formats is similarly useful to identify like the linearization of a problem (where the solution is trivial/linear/simple to solve) and what structures they align with (simplification)
        - identifying useful structures like powerful variables like the 'density' of a data set is useful to identify 'possible new problem formats' like the densification of a problem (converting structures to densities) and why that might be an error or suboptimal ('densities are more useful when distributed (the opposite of a density), a distributed density being a useful structure of similarity/difference')
        - identifying the 'structures that make any structure seem extreme' is useful to identify, like how some structures 'make everything seem positive or negative, or true or false, etc for other extremes' (which are 'over-standardizing' structure which standardize all the info out by false equating everything, so finding these 'false equalizers of every variable' are useful as error structures), and similarly identifying other 'functions that always have the same result/output' are useful to identify/avoid
        - identifying structures of problem-solving/interface structures like 'areas of iteration' (where iterations can co-exist without invalidating each other) is useful to identify as a unit to apply in decomposing uncertainties or variation or complexities, given the high variation of a structure like that
        - similarly, applying 'intents' (like 'change symmetries/centers' or 'change distribution') as opposed to 'certainties/symmetries/centers' as the 'stable/constant structures as a base of reality, around which changes can occur', as a network that is constantly updated with new intents as theyre identified, thereby containing a high ratio of variation, as 'intents' have some degree of certainty (if its intended, its semi-certain, just like if a structure is useful, its likely to exist), and identifying a network of 'realities created by intent sets' to apply as a higher variation structure is similarly useful
        - identifying points where 'specificity' cant be applied further or is required is useful (as is identifying other certain structures of other interface variables)
        - relatedly, 'linearizing' (or applying other problem formats to) the interface structures (as in 'finding adjacent variables that can trivially generate the interface structures' like 'filter') is useful as an intent to fulfill
            - relatedly, 'identifying adjacent concepts' identifies structures that can be rotated (angle-changed or position-changed or average-changed or other symmetry-changed) to identify 'overlapping structures' but cant be combined trivially to generate all other structures (where a smaller structure than the high variation/powerful structure can be combined more adjacently to create more different structures than the overlapping structures)
            - relatedly, 'orthogonal/independent and/or abstract/high variation variables' are useful variables to fulfill a 'linearization/simplification' intent bc of the 'alignment' between 'highly covering variables' and the 'low number of variables' required to 'linearize/simplify a problem' ('low variable count' being an 'approximate' structure of 'simplification'), as there is an overlap between simplification/linearization and 'reducing variables' (but not an equivalence, making them useful as approximations/substitutions)
        - relatedly, testing for useful variables like 'every item in a set having some variable value' is a matter of applying relevant structures to 'every' (such as 'subset functions' as well as tests exceeding/around 'ranges', as if the variables changes once the range is exceeded, thats another indication that 'every' is an applicable variable structure, as well as the other variants of 'every')
        - relatedly, identifying structures like similarities/connections/derivability/measurability as 'causes of relevance' (its relevant bc its similar as in connectible with some common base, so the structures change each other bc they can interact), which means every 'self-reference' structure can be a relevance structure bc of its similarity and so on for other structures specifically 'similarity to intent' as a 'relevance cause', so finding structures similar to known intents like 'general intents relevant to reality' is a useful problem-solving intent to find structures likely to be used as components of workflows (where the meaning interface is a network where all intents are connectible/similar bc thats a useful/relevant structure)

    - identifying useful structures like 'relevant similarities' in problem spaces where those have not all been identified, which is identifiable by the speed/complexity of existing solutions, given the variation in that problem space (regression is a high variation problem space, but the algorithms arent fast enough for it to be completely understood, as its likely to be linearizable given the solution structures like specifications like 'polynomial/continuous' and simplicity of relevant structures like 'averages/densities/maxima') and the connections with other interface structures that make them 'reality-covering' or 'approximately reality-covering'
        - for example, identifying that the standardized function's 'average' and 'median/mode' are capable of determining which function set a function belongs to with a high ratio of specificity (except for the polynomial variation resulting from overlaps determined by "power patterns" that are known in that problem space of filtering polynomials) is useful to identify as a way to avoid determining the whole function, since this 'set of metrics' is relevant/similar but different and cover different info, which is a useful variable to optimize for ('relevantly similar but usefully different' structures), just like the abstraction 'similar average/summary metrics that cover different info' is useful to identify, as they are connected by overlapping cause (the cause of an average is connected to why a specific median/mode might occur, as the probability-interim value has overlaps with the absolute reference-interim value and the frequency-interim value, but there is enough variation that they can represent/cover different info, and identifying these causal structures is useful to identify the average metrics as well as the rest of the function)
        - similarly, identifying the remaining variation (once a average/median of a subset of the function is identified, what filters the remaining function possibilities)
        - similarly, identifying the 'high variation structures' in a problem space and their connections and the identification of remaining variables like 'embeddings' is useful as an algorithm to solve most problems, that is made more optimal by applying overlapping structures like 'relevantly similar but usefully different' structures bc these are not 1-to-1 mapped with 'high variation structures' and the identifying the 'remaining info not in the overlap' will identify a higher ratio of variation than either on their own, so 'stacking non-1-to-1 mappable, overlapping structures (like similarities/differences overlapping with high variation variables like abstractions)' is another way to solve problems
        - relatedly, identifying the variations of useful structures like similarities/differences such as 'definitively similar but different in usage' and other variants is useful as a default set of structures to use as components
        - similarly, identifying 'incorrect similarities' as containing 'structures of irrelevance' are useful to apply as 'structures to avoid' in problem-solving algorithms
        - relatedly, identifying the ways that current stable systems can be modified without invalidating 'connectivity with other changes' (allowing time to continue) is useful to avoid structures that are false (some changes are lies and some changes are possible futures)
        - relatedly, identifying the ways that errors are connected to other errors is useful to identify 'causal networks of errors' to apply as 'structures to avoid' (identifying how 'manliness' is actually other errors like 'sensitivity' is useful, since the extremity of this variable is an obvious error)
        - relatedly, connecting extremes to units that cause extremes is another useful index to compute, and 'differences like opposites of extremes that reduce errors of extremes' is another useful index to compute, and 'connecting true/false similarities', and identifying the 'error structures associated with each interface structure (like units/extremes) and their interactions/iterations (like unit networks)' is a problem-solving intent
        - relatedly, identifying 'structures like abstractions and variables that can be extremified without causing an error' (such as different variants of the abstract network, within some similarity range to their definitions, applying relevant as in meaningfully correct connections from abstractions to structures) is another problem-solving intent to identify solution structures
        - relatedly, identifying the 'causes of usefulness' of a structure (like indexes or 1-to-1 maps) is similarly useful to identify and connect to other interface structures like 'context' such as the system structures (like 'lack of adaptability') connected to causes of usefulness (like 'new variation') and the connections between causes of usefulness and specific structures (like non-1-to-1 mappings which involve 'connections between different variables'), which are useful connections to apply as 'interface queries or components/bases of them' (context-causes of usefulness-structures with those causes applied)
            - for example, 'indexes' are useful to 'find abstractions that generate the index', just like 'applying variables to "small subsets of constants" at a time' is useful for 'identifying new variables and variation sources among a set of different structures (like constants as different structures from variables)', just like how applying variables from a different structure is useful to identify other structures which can also be that different structure (resolving other differences like extremes)
        - relatedly, given that 'applying certainties as a structure to apply changes to' is an obviously useful generalization of problem-solving, alternates are useful to identify (such as given the 'set of variables connected in a network that determine reality as it is', what other network configurations are possible and which network connections are possible and which of these network configurations/connections can be connectly in different ways, with different bases/nodes/connection functions, while still allowing meaningful variation or determining an optimal configuration/connection that should be applied as a unit of reality for other universes to reflect, and which of these alternatives should be rotated to allow the most relevant variation)
        - relatedly, given the relative rarity but power of 'high variation structures that follow a simple pattern (like a vortex/spiral)', its useful to generate/identify/apply these as default structures to check for, just like abstract concepts are useful to check for and generate and apply as defaults
        - similarly, applying every structure as an 'example of a more abstract structure like a type/concept' is useful to 'identify new variation' (applying 'like this structure' or 'an example like this structure' rather than 'this structure'), and similarly, applying every other structure using other high variation variants like 'like' or 'an example of' is useful to 'identify new variation' and identify other problem-solving intents by which intents are fulfilled with other similarly useful/common structures (like 'x of y' to indicate 'containment' or 'example' or 'subset', or other common structures used to connect variables, commonness indicating a 'possible interface/abstraction or emergent interaction level created by iteration')
        - similarly, identifying different error structures that make truths more identifiable/differentiable/derivable/connectible like 'stacks of errors in directions away from facts' and 'errors surrounding facts' are useful to apply as default error structures to derive truths from
        - relatedly, if it is a 'general' structure like a common core function such as 'differentiate/integrate', its likely to be usable as a way to solve problems, so connecting these 'general structures' with 'changes that make them usable to solve problems' is a useful connection to resolve (with an index or other structure)
        - identifying structures like 'overlaps of sequences connecting core structures like variable sets/units with usefully differentiated structures like types/abstractions' that identify 'points of differentiation (where a difference began to develop)' is useful to 'identify new variation' bc the original sequence of differences applied after that point can be varied to create other sequences
        - identifying 'ways that definitions can change' as a way of finding 'possible future time sequences' and 'possible variants of the abstract network or a truth network or other useful networks', since definitions should be applied as constants (but definitions can change)
        - identifying variants of 'abstract info structures' (as useful cross interface structures, like 'abstract info potential' as in probability/possibility of abstract info or 'logic cause functions' as in functions that cause logic) and positions/structures where the usefulness changes (when iterated or otherwise changed, such as where there are 'primary directions of usefulness' or where the changes are already known/identified such as 'potential info structures')
        - identifying 'layers that lead inward to an optimal center' which can be applied as a structure of truth (like the correct variant of the abstract network) and used as a filter to base future variation on, as opposed to 'layers that lead outward to optimal emergent variation', which is useful when 'directions of increasing errors are known and there is an optimal position/structure to find' (other variants of error/solution positions are useful as filters of solution sets)
        - identifying 'ways to create intersections between truths or other structures' (like standardizations) as a way of finding the 'network of truths' or other useful networks
        - identifying 'variants of the abstract network' that 'align with some set of truths' and the 'ways to change it to align with other sets' is useful as a way of identifying connections between useful networks

    - identifying useful connections like between 'high variation variables' (like 'concepts') and the structures they invalidate (like 'iterations') is useful to identify as a 'useful index to apply across problems', given that there is always a new index/network to identify that will simplify problem-solving (core interface structures being an infinite variation source, given some interaction level just reached that makes those interface structures relevant again, as interaction levels/interface networks act like a grid around which variation occurs, just like there is always a linearization of a problem with sufficiently adjacent structures)
        - for example, given that every interface structure can be used to generate the others with enough changes, identifying those changes (iteration/randomization) required to generate the others and the structures that require the fewest changes to generate the others is useful to identify the correct structure of the abstract network or its useful 'subsets/variants/other structures'
            - applying different defaults as constants to identify 'absolute rules' that can be applied as constants across problems, to identify 'useful structures to apply across problems as constants', given that applying an incorrect rule as an assumption/default/input will likely quickly produce obvious errors
        - similarly, identifying the variables of interaction levels is useful to identify the structures where variation is likely to exist, for intents like 'identify the variables of functions that can filter true/false statements and the correct positions of these functions in a network that can filter true/false statements', since there are many functions that can filter true/false statements like with 'iterations of functions like tests' but the useful functions are the functions with the most orthogonal variables across interaciton levels, which sometimes need to be discovered with iterations if existing concepts dont exist to encapsulate a high ratio of variation to avoid the iteration
        - relatedly, connecting 'concepts with the iterations they avoid' is a useful index to identify just like identifying 'iterations and the interaction levels they identify' is useful to identify, 'indexes' being useful as a data set for identifying variables of differences determining the index
        - relatedly, 'finding an index to connect examples' is similarly useful as 'finding a network to connect variables', so these structures can be applied as 'components/bases/inputs of workflows', as there is always a structure that is easily connected by some function, so identifying what is connected by each function/structure and whether that connection is reality-covering (connects extreme differences between relatively independent but still interactive variables)
            - similarly, identifying the connections between 'linearization using adjacent structures' or 'reduction to a point in a plane' or 'over-prioritization' formats of a problem are similarly useful as an alternate to identifying interface queries to solve a specific problem, as a 'network of useful problem formats' likely makes problems more solvable once the right format is selected, which is more trivial with that network
        - relatedly, the 'difference between structures likely to be inputs (useless structures unless theyre changed in some way) vs interim variables (useless structures unless they connect something useless with something useful) vs outputs (intents and adjacent structures of intents, as in useful structures)' is similarly useful to identify, to identify 'what side of the filter between problem/solution a structure is likely to be applicable in' ('organized high variation structures' are likelier to be solutions or interim functions than problems)
        - relatedly, the 'optimal position of hidden/missing info errors' in a system is useful to identify, to identify when an error is not in its 'optimal or neutral position/range/structure'
            - relatedly, identifying clear differences (optimalities) and ambiguities (neutral structures) and how differences can seem ambiguous (how a solution can connect to a neutral structure) is another useful index/network to identify, being better to have multiple useful structures (like a subset of the index and a subset of the network to avoid identifying the complete structure but still retaining info in different formats, to avoid info loss errors)
        - relatedly, a 'variable' is usable to solve a problem when a problem space has been simplified to a linearization/spectrum (such as a position on an interaction level or a position in a system or a position on a spectrum or a position on a continuous function), so that solving the problem is 'identifying that position indicating that variable value', so connecting other 'structures like networks' with these linearizing functions leading to linearized structures like variables (such as concepts which add linearization to a problem) is useful to fulfill intents like 'identify useful indexes to use across problems'

    - identify useful structures like the alignment of 'ratios that solve problems' and 'vertexes' to identify specific 'problem-solving vertexes, when the problem is formatted so that a ratio is the structure of the solution'
        - identifying specific useful structures like 'specific useful sequences/functions' through their 'interactivity/connectivity' with other specific/general useful structures like 'specific functions (like Fourier transform) that switch between formats', formats being a high variation structure to connect with one/specific/simple/adjacent function, so identifying these one/specific/simple/adjacent connections between high variation variables is generally useful
        - relatedly, the creation of 'uniqueness' using high variation structures like 'iterations/specifications' indicates a useful question of 'whether there is a limit on the ability of these abstract info structures to be iterated and what that limit indicates' ('reality isnt that complicated' vs. 'other realities exist which require other info structures to be temporarily and semi-generally true, "abstract info structures" being structures that are semi-generally and at least temporarily true, other realities which are limiting this reality' or 'other info structures exist than those already identified')
        - relatedly, identifying 'new connections between independent variables' or 'sequences of parameterizations that can coexist' identifies a 'set of alternate timelines (in a reality where those variable interactions havent all been described and where improbabilities/info barriers dont prevent those timeline sequences)' which can be iterated to identify a 'set of alternate realities where those timelines can occur' which can be used to identify 'networks of co-existing realities' which can be used to identify optimizations of those networks (a set of connections between useful structures that will be reflected in some subset of other useful connections)
            - relatedly, the 'limit on the usefulness' of adjacent concepts in an interaction level can be identified through the 'variability/freedom/usage' of those concepts - once specific variability/freedom/usage structures have been applied, a new interface/interaction level is required for further change, such as new units formed by 'emergent iterated units', which means reality is taking place on the interaction level of 'interface network variants and the realities/variation they can support/cause', which is the most applied usage of interface structures creating the most unused/unknown interaction level where the most variation can occur
            - the optimization of networks can be done by applying 'certainty dynamics' such as by 'applying new network nodes once a "certainty/description/representation/useful structure/specification" is identified, to identify its adjacent structures that can contain its ability to change, given that these structures can be applied as a base for change, and are variants of other useful structures (all useful structures are connected) so are useful to change (a description is useful to change into a generator)' and 'structure dynamics' such as by 'applying limits as "changes that a sub-network cant be used to interact with/change/access/identify/apply" to indicate independent sub-networks that have limited ability to change/learn/cover reality like over-simple structures like "specific two variable-sub-networks" other than vertexes'
        - identifying error sequences is useful to 'identify future sequences' like how 'over-prioritization errors' such as how an opposing over-correction usually follows an over-prioritization error (like how a prioritization of control/power leads to a prioritization of independence/freedom, a dichotomy which can be resolved with a useful structure like a ratio)
            - identifying useful specific structures like the 'most differences that can be resolved with a ratio (a comparison of relevant similar structures on one metric)' are useful to identify, to identify problems that are a subset of that problem, as well as the limits of that solution structure
            - similarly, its useful to identify the 'vertexes' that form these ratios (what similarity indicates relevance of a difference to resolve, and which difference should be resolved), and the interfaces or vertexes that connect them to problems, to match these problem-solving vertexes with problems more quickly
        - the unification of variables in a position requires 'resolving a dichotomy/contradiction' to determine which structures are selected to avoid an overlap on that point, so incentives like 'connecting to other points to support more variation while still interacting with that variation' and 'applying iterated changes in a parameterized direction to avoid resolving the uncertainty' or 'removing variables to remove the uncertainty or move it to another point' exist which make those structures likelier than others
            - relatedly, its useful to identify the 'limits of parameterization, while avoiding errors like "self-reference" or "incorrect abstraction, as in a "loss of info/over-complexity/over-randomnization"', abstractions being usable for randomization just like interfaces can add so much variation that randomization is a possibility
        - relatedly, solution structures involve structures like 'embedded efficiency' that is triggered regardless of inputs or 'networks of surrounding efficiency around problem inputs/areas' which creates a 'ratio favoring solution organization/efficiency/optimization, as opposed to that of problem structures'
        - relatedly, structures like 'irreversibilities/constants/requirements' can create 'independence' as in 'structures which cant be changed by another structure'
        - relatedly, the question of whether its possible to identify 'structures that seem impossible or otherwise seem to violate some assumed constant/limit/requirement, "seeming" as in having multiple reasons it could be true' ('structures which never seem similar, regardless of some change like an angle/filter applied' or 'structures which can never be resolved to a point on some plane, no matter how many parameters or other structures that create similarities are applied') is a useful question to identify new variables
            - identifying 'metrics like "how many" applied to truth structures like "reasons some connection could be true/false"' in a system is useful to identify limits on truth structures in a system (like given this number of free parameters, only this many connections can be true)
        - relatedly, given how 'efficiency networks' can 'reduce info too quickly and falsely seem correct in various ways' (like how regression often over-simplifies a problem), 'paths to offset efficiency networks' to avoid 'traps in efficiency networks' such as 'info vortexes' (where everything falsely seems certain bc regression functions have been identified) are useful to identify as these errors lead to other errors like 'meaninglessness' (constructed by false/missing info) and 'irreversibilities'
            - relatedly, an 'irreversibility' is relatively rare, if there is enough variation left in a reality to offset it (connect the lost variation by entropy with future remaining variation, thereby adding meaning to the lost variation, as it was used to describe/enable other future remaining variation), but is more likely in a situation where its unlikely to be clear that an event of 'meaning loss' should be reversed, as in there is 'frequent' meaning loss (its 'irreversible' bc its not clear whether the remaining variation should be allocated to reversing that, as measurements of variation are too inaccurate or variation has been distributed, thereby reducing the variation through this distribution)
        - relatedly, creating the highest variation-supporting network could involve a format like 'minimally overlapping but still connectible waves (waves allowing vacillation structures like vacillations between opposite extremes of a limited spectrum), waves that allow the most variation' so identifying the 'network of minimally overlapping waves' could identify a more accurate description of reality, just like 'rotations of semi-parallel waves' could identify an 'optimal variation/filter sequence', where 'waves between unlimited extremes' generate similarly independent structures like 'parallel lines' (connecting orthogonal variables with one function parameter configuration)
        - relatedly, given that interfaces involve an 'abstraction of some structure' (like 'causal' sequences and 'potential' fields and 'abstract' similarities and 'specific' information and 'logical' requirements and 'optimal/useful' efficiencies/definitions/independences), identifying other abstractions of structures (abstractions of 'self-reference/embedded' fractals, 'vertex' ratios, etc) can identify other useful structures to apply as interfaces
            - relatedly, identifying descriptions like 'embedded certainties' to identify '"limits like row/column counts" on "specific orders/sequences" (forming matrixes)' as structures that can be applied to identify different structures like their 'abstract opposite' as 'embedded uncertainties' like "lack of limits like randomizations applied to abstract structures like types" given that the 'abstractivity' of these descriptions connects/allows many different structures, so connecting differences like opposites of abstract structures is more trivial bc of the abstraction
            - relatedly, given that abstract structures like 'sequences' apply as a core structure across interfaces (function, cause, etc), identifying the structures related to interfaces in a network is useful to identify default connections and variables of interfaces, to identify variables to apply to create other interfaces, like 'iterated potential' and 'sequential concepts' that can be used in different ways, as 'emergent' interfaces (given that reality/time applies a 'generator/variation/filter pattern', identifying 'interfaces of interfaces' (like different variants of interfaces like different abstract networks and different graphs/fields of interface networks)' is useful as a new intent to fulfill intents like 'identify new sources of variation', by applying structures like "combinations" to interface structures in new ways and identifying the "maximal differences/solutions" to those structures/errors like "orthogonalities" or "interim structures between combinations")
        - identifying changes to test different 'abstract or interface network' configurations is useful to identify reality position, bc identifying 'optimizations for agents in the reality' is less useful than identifying 'reality position and the impact of different variants on adjacent/distant realities' which will benefit agents in future across timelines in a reality
        - relatedly, identifying an 'ambiguity resolution between two variables' can be done by identifying 'structures applicable to one variable' that have no corresponding structure in the other variable or which are not connectible to the other variable, where the 'more independent/self-referential/variable/connectible variable will likely be selected for most intents', as 'independent' and 'abstract' are both ways to be connectible (if its independent but abstract, its likelier to be connectible to a higher ratio of structures despite the independence making it distant, as 'independent-abstract' is a useful vertex for core interaction functions like 'connect')
            - relatedly, 'ambiguity resolutions between maximally different structures' which have variable outcomes like 'one structure is selected' or 'the structures are determined to be variants of another' is useful since other ambiguity resolutions will be embedded in that resolution and connecting these high info-containing structures like 'embeddings' with high variation solution/output structures is useful
        - relatedly, identifying sequences of useful structures like 'sequences of adjacent info that links a memory to the next structure, in case the next structure is forgotten, so the next structure can be generated to correct missing info errors and identify connections to derive all info adjacently to remove the usefulness of only recent structures in identifying next structures in the sequence'
            - similarly, identifying vertexes that are only useful in specific positions/angles/sequences is useful to identify the optimal sequences/networks between vertexes
            - relatedly, identifying useful sequences like 'orthogonalizing and connecting planes, once planes are identified to reduce some problem to a point' is useful to identify sequences connecting problems/solutions (or solutions/solutions, or problems/problems, etc)
        - relatedly, 'identifying new variables' can be implemented by 'identifying different optimal/useful/solution structures in similar or related structures like spectrums (like different sets of solution metrics fulfilled by similar solutions)', which provide new directions of optimization to apply changes in, just like identifying new similarities can be implemented by identifying connections between new different structures
            - this is related to reality-covering structures like 'identifying the optimization in every structure', where limits on these reality-covering variables exist like where one system is not connectible to/interactive with other systems and therefore its optimizations cant be shared with other systems, so there is an upper limit on the value of that advantage determined by the connectivity (one interface like 'connectivity' can determine the variation of another like 'optimization')
            - this is related to 'organizing similar structures around intents or usefulness' (solutions/certainties) as opposed to 'organizing similar structures around uncertainty like undecidability/ambiguities' (problems/uncertainties)

    - identify useful structures like combinations of structures that are useful in their 'connectivity to solution structures or interfaces' and in their combination of 'specificity/emergence/opposition'
        - for example, identifying that hormones (like vitamin d) are 'change triggers/limiters' is useful as an emergent effect rather than identifying some specific function like 'change timing trigger', and they are a 'natural/default' high variation variable, which is opposed by other high variation variables like 'changes to default substances' and 'changes to default genes' (although some genes are carcinogenic by default and are default for some)
            - so the 'opposing' high variation variable ('artificial substances') of 'hormones' as 'change limiters' is useful to avoid, just like some hormones are useful to apply since they have more change limiting functions like change timing functions
            - without identifying this functionality of hormones, they might seem like another high variation variable to avoid as it can cause changes to defaults
            - like other high variation variables, hormones have a path to causing disease like diabetes/cancer
            - so identifying the specific limiting variables in an interface like a high variation variable is useful as a structure to oppose dysregulated change
            - similarly, identifying that 'organized/synchronized and phased/separated changes work better in bio systems' is derivable from 'organization' and 'resource usage' and 'specialization', which is another reason why vitamin d is useful for preventing cancer
            - relatedly, identifying a 'ratio of specificity' is useful, like how its possible to not identify hormones as relevant bc it might seem that bc they sometimes help prevent cancer and help cause it, that they can be ignored, as they dont change the overall health of the system more than randomness does, or that these anti/pro effects cancel each other out, however its useful to identify the info of which specific paths they cause either effect in rather than assuming these opposing forces neutralize each other
        - this indicates that a 'ratio of specificity' (specifying which substances have limiting/change functionality) and a 'ratio of emergence' (deriving to the nth degree interaction) and 'connectivity to interfaces' is useful for identifying similarities/differences that are useful (like a useful similarity in high variation variables, so they can be matched as possible interactive structures, making them likely to cause each other's errors or solutions, and a useful similarity in the power of limit/change functions making them likely to be useful as a difference to some error bc powerful variables can cause a high ratio of differences)
        - these 'specific, emergent, opposing, interface-interactive structures' are usable as components of workflows given their 'high coverage of similarities/differences' and 'connectivity to problems/solutions' as well as 'connectivity to interfaces' (high variation variables are connective to interfaces, being similar)
        - finding other structures that fulfill multiple metrics which can be used across problems, such as using them as components of workflows, is possible to identify by combining these components (like emergence after n iterations, specification) in similar structures and applying changes to interface variables like degree/ratio/angle/position/speed/power, so 'identifying new interface structure combinations and varying these using interface variables' is a way to generate the set of possible other useful structures across problems
        - relatedly, identifying 'unique extremes (a different difference)' is useful as a way to create an opposite structure in some variable, while preserving other similarities, and similarly identifying all the other configuration of similarity/difference that are useful in describing measurable/stable/common/required states of a system are also useful to identify (similar to how 'antimatter has some similarities with matter but is different in one important way, this one difference arguably determining reality'), answering the question 'can you create an extreme difference in some relevant variable by holding everything else constant, indicating independence of that variable'
        - relatedly, 'artificial curiosity' implemented as a pair of networks to maximize error/surprise to identify 'maximal differences' (which is easier with specific structures than very abstract structures like interfaces) leaves out the concept of 'relevant similarities' which are based on independent variables and interfaces (changes that are stable will be based on more stable changes) which can be identified by identifying 'what these maximally different structures have in common (like a common abstraction like a type)'
            - relatedly, 'different ways to create stability' are alternate timelines (the way to offset the value of an interface is by constructing structures to contain/re-create its differences/advantages in creating stability)
            - relatedly, 'opposing extremes of spectrum variable' networks (like 'abstract/specific') are an example of other useful networks to try to offset each other, since neither is sufficiently covering of reality to leave the other one out
            - relatedly, identifying variables of these structures, like 'giving more info to the prediction network that the other network is configured to surprise, than to the other network' or 'giving it more abstract info so it can identify patterns better' or 'giving it more common structures or bigger variables so it can identify new structures better' or 'telling it what the other network is configured to do and giving it a copy of the other network its trying to predict' are examples of alternate variants of these opposing-pair network interactions, and similarly, allowing the network to change other variables (its input data, its own nodes/weights, its error functions, preprocessing functions, its intents/rewards like changing from 'identifying cause to independence to powerful variables to new variables, independence being a source of new variables/interfaces/stability/other useful structures', its emergent structures, its allowed/active structures, its usage of free parameters not used by other processes to fulfill intents like 'identify new variables' while the other nodes are working on the original task, etc) is likely to be more like intelligence (having more relevant variables as in 'free parameters')
            - relatedly, another way to increase 'free parameters' would be an 'ability to copy itself and modify the copy' is another useful intent for networks to fulfill, and similarly, an ability to identify a 'network' as a useful structure to implement a high variation variable like a 'perspective' (rather than its simpler definition, such as a 'position') as a 'complicating network' is useful to find more abstract structures as opposed to applying the minimum info
        - relatedly, the idea of identifying a 'specific fact' is less possible in a space with 'networks that can easily fake specific facts', so the new 'base of reality' (an interaction level where true variation occurs) is a set of 'generally true' statements which have so much of an impact on reality that theyre relatively easily proven as they leave evidence in support of them everywhere, rather than a set of specific statements, as specific statements are less possible to identify as constants anyway given the relative probability of being capable of changing them, and even more so with 'fact-faking networks'

    - identify useful similarities that can be applied with useful difference structures like 'components of workflows' that have more variation than workflows or their implementations, as these components occupy a higher variation interface between the workflows and queries implementing them (making it useful to identify a 'workflow network', a 'implementing query network' and a 'workflow component network' as well as interface variants of these like 'workflow generative network')
        - for example, 'abstract and specific' have an alignment with a known useful query structure like a 'sequence of bottlenecks/filters and variables/generative functions', which can be re-created using an alternating sequence of 'abstract and specific', bc of the overlap of 'abstract/specific' and other variables like 'variable/constant' or 'summary/example' or 'intent/implementation', which is bc of the usefulness of variation and the usefulness of alternating sequences with variables like vertexes/opposites, as in 'generate/filter, generate/filter'
        - this 'sequence of generators/filters' aligns with workflows involving structures like a 'sequence of sub-problems (filter solutions for a sub-problem, then generate new variables for the next sub-problem)'
        - by comparison, other workflow structures like 'embedded sub-problems' which can be applied with query structures like 'iterations of specificity', as in 'generate/filter/filter'
        - so a 'sub-problem sequence having dependence between sub-problems' is implemented with 'alternating abstract/specific structures' and a 'sub-problem embedding' can be implemented with 'iterated specific/filter structures with dependence on the sequence for pre-filtering' (filter a solution set, then filter that solution set), which have an overlap in their structure (where in one case, the 'generation of new solutions to filter next' is optional)
        - identifying the most 'highly covering' workflow structures is useful to identify and apply as base structures to optimize to find better queries
        - identifying 'equivalent alternates' as well as identifying 'required variation (positions to apply mixes of maximal differences)' is a way to identify structures that can be applied as constants (no matter which item in the set of equivalent alternates is selected, the impact will be equal/similar) or variables (no matter which item in the set of maximal differences is selected, the impact will be opposite/different) is similarly useful to identify structures that can fulfill interface queries like 'abstract/specific, abstract/specific' or 'generate/filter/filter' as commonly useful to resolve differences, even without specific structures applied as inputs to these structures
        - similarly, identifying the alignment of these structures ('generate/filter/filter' or 'generate/filter, generate/filter') with general workflows (like 'change a base solution' or 'identify input/output sequences') as a result of their interaction structures in common like 'embedding' or 'sequencing', which have an 'overlap' in that some subset of embedding structures have a sequence (as in a 'stack of embedded variables applied sequentially' as well as a general sequence merged with sub-problem solutions, a structure which may also be a network of overlapping sequences), so standardizing 'embedding' queries to 'sequence' queries is useful to identify which embedding structures optimize which sequences, when applied as general sequences or sub-sequences, in which position that a sequence can occupy in a query embedding/network
        - relatedly, identifying the minimal number of network nodes to simulate all the known structures by allowing separation/specialization of sub-networks is useful to identify a 'unit network' to apply to 'identify a structure' (what network structure can identify a matrix, geometric series, etc), and which network generates these structures adjacently or distantly (different with valid variables, rather than arbitrary/irrelevant variables as in just any different number or other specific change), and which network generates the variation in these structures adjacently or distantly (distant positions being useful to identify as 'positions where a problem probably seems complicated' to apply as 'possible position filters' of a problem that seems complex, to make that complexity false and/or check for false complexity), as well as being useful to identify structures that are adjacent or distant across networks (distant structures across networks being useful as a source of variation and a structure that requires new abstractions to describe adjacently)
        - relatedly, identifying the usefulness of an intent like 'identify maximal differences' for an intent like 'solve problems adjacently' is a matter of identifying the generalization of 'similarity of differences/problems' and the specification of 'similarity of maximal differences with problems that are still likely to be unsolved so still relevant to be identified as problems' which creates an 'input/output sequence' (the connecting structure being a sequence of intents like 'connect maximal differences', the generalization 'connect differences' and the standardization/connection to the problem/solution interface as 'connect/similarize differences that are problems' and the optimization 'connect/similarize differences that are problems adjacently' and the condensation/simplification as 'solve problems adjacently'), an example of a 'abstract/specific, abstract/specific' sequential query
        - relatedly, 'sequences of resolutions of embedded/specific sequences with general sequences' are useful to identify as 'default/base queries', and similarly, 'sets of embeddings of specific problems in a general base difference to resolve' are useful to identify as 'probably useful as in relevant' query bases/components, after identifying 'what problems can occur which would require identifying a specific structure to solve a general problem difference'
        - relatedly, the 'resolutions of vector sets describing possible positions/structures/moves and spaces constrained by limiting barriers, in a way that allows the components to describe and change the system barriers/limits' is another useful format for identifying all solutions to problems possible in a space, by applying a function like 'evaluation/simulation' to the solutions to be filtered (the 'set of all possible positions/structures/moves of a vector set'), to apply a 'reversal' to the direction of evaluation (allowing solutions to evaluate a system, rather than evaluating solutions in a system), to identify 'functions/variables to apply that can evaluate/optimize most systems, thereby preventing a requirement to evaluate solutions in a system by creating adaptation/optimization units in each system'
        - relatedly, identifying the maximally differentiating variables and their inputs/outputs/other interface structures is useful to identify maximal differences and their variables to generate them ('what is an output of speed?' 'making distant points adjacent, reducing lines to a point, reducing dimension, changing position, changing interaction level, changing embedded/emergent variables like variation/time, etc', so 'speed' is a maximally different variable in the physical info interface, which can be used to create/identify variation)
        - relatedly, identifying 'increasingly trivial/adjacent changes (such as fractals or converging sequences)' is useful for identifying 'embedding sequences' that are likely to describe limits/interactions on interfaces
        - relatedly, identifying variants of a structure like 'strength' which is useful as a specific variant of 'stability' (an 'optimization' structure) is useful to identify other 'optimization' structures than 'stability', such as 'iteration/extremes' and 'handling more variation' (which 'strong' structures are more capable of than other structures, 'strong' structures being 'particularly stable variables, as in having a problem in the form of variation but being able to solve it using other variation' rather than 'structures forced/required to be constant, as in removing the problem by removing the variation') since a specific variant of a structure is likely to overlap with other variants (other optimization structures)
            - relatedly, 'forcing constants to solve some problems by removing variation while creating other problems bc most structures shouldnt be forced to be constant (except absolute truths or only temporarily in limited/isolated systems)' is useful as a way to identify 'sets of constants to avoid' while identifying interface queries (which 'sets of constants' shouldnt be forced to be constant by some filter/specification)
        - the point of this workflow is to identify other workflows by identifying useful similarities like 'abstractions/components/types/similarities/overlaps'
        - relatedly, workflows should have a solution metric of connecting to 'intelligence' functions like 'consciousness' (such as requiring any solution network/interface query to have some 'ratio/structure of free parameters' that is big enough to allow evaluating the rest of the system in those free parameters)
        - relatedly, identifying useful sequences like 'iterate some unit until a new structure is identified, then iterate that new structure as the new unit to iterate on a new interaction level identified by that new unit, then repeat to find the highest variation unit that can be iterated and identified with this iteration sequence' is useful to identify high variation sequences that can align with these general query structures like 'specify/abstract' or 'abstract/specify', and specify simple sequences like 'iterations of units until limits of computation/measurability/evaluation are reached' or 'iterations of maximal difference mixes until new variables or limits are reached' to optimize simpler sequences that are likely too simple to be useful
            - relatedly, to make a simple sequence useful, some definition has to be applied to identify useful structures like 'different but still similar enough to be relevant' change types, otherwise the simple sequence will not likely be useful in many cases without that 'already identified specific definition or change type set' (such as how identifying simple 'causal input/output sequences' in a data set is only useful once the variables have been isolated and identified)
            - 'identifying merged variable structures (such as abstractions/types/independent systems/interfaces) where inputs/components are often altered beyond recognition and less trivial to identify', to identify 'variable interaction sequences connected to valuable structures like abstractions' is arguably more valuable than isolating variables to identify their specific causal sequences (once a 'network of cause' is identified as being a relevant structure bc variables are identified as connected in some way, applying 'common variants of common causal networks' is likely a more useful solution structure just like identifying common connections between these variants is likely a more useful solution structure, as variables which are connected in some system are likely to have many different possible causal interaction sequences which they can switch between, so just identifying dependence and then applying one of these common causal networks or connections between them is likely to be correct without identifying 'specific causal sequences' which are likely to have some bias error
            - relatedly, a solution set where any selected solution is 'likely to be correct' means the original solution set has been filtered enough that a 'greater than random chance of selecting a solution' is applicable to that filtered solution set (meaning more than half are likely to be good solutions), which means any 'ambiguously different as in similar set of structures' is a possible solution set that is 'likely to be correct' for some problem, so mapping 'sets of similar structures' that could be a solution to 'some mapped problem set which is solvable with that similarity' is a useful structure to identify

    - identifying useful structures like 'similarities to differentiate' (like '1-to-n mappings') and which structures theyre different from (like 'vertexes') and therefore useful for identifying
        - for example, 'truth' and 'optimization' are not 'opposite' (like abstract/specific are on the abstraction spectrum) but not '1-to-1 mapped' so not 'equivalent' either, which means their interactions are likely to cover most of reality
        - identifying the variable interactions between truth/optimization are useful in identifying filters of interface queries
            - the truth is often optimal as in specifically 'efficient'
            - 'optimizations' as in 'efficiencies' can predict future truths as they are likelier paths of truths than other structures
            - the opposite of truth (falsehood) structures can contain a 'ratio of truth' that is sufficient to be an optimal description, such as where a wave will overlap with many polynomials by default and the wave may be generally false (as in incorrect at predicting most polynomials) but still be a truth structure as a useful base solution to change into specific polynomials
        - relatedly, 'ratios between relevant sequences' or 'interactions between truth/optimization' are useful 'similarities to differentiate' (as opposed to 'vertexes' which are perspectives to apply in combination to cover reality), where these math abstractions act like or contain 'interfaces' that cover reality (most problems can be solved by framing the solution as a ratio between relevant/interactive/similar structures)
        - relatedly, finding structures that are 'true but non-adjacent to existing interface structures' are useful to identify (such as specific types of sequences that are not trivial to identify) and connect with interface structures and abstractions/patterns of those connections, as a way to 'identify new variables', given that 'identifying more trivial routes to distant structures' is an intent to solve other intents like 'change positions to optimize routes such as by making them equidistant'

    - identifying useful structures (like 'abstractions') of useful structures (like 'determining ratios') that are not connected to other interface structures (like a 'query interface') with some interface structure (like a 'solution metric' or 'workflow'), 'abstractions' being useful generally across intents like 'identify new variables', so applying them is usually an 'optimization'
        - 'similarities of queries, around which adjacent variation occurs (the type or abstraction or interface explaining the query variation)' in problems like 'resolving differences' and 'resolving similarities/ambiguities', where some set of similarity/difference structures will cover most similarity/difference problems according to some ratio metric like 'simplicity of problems compared to simplicity of solutions', is useful to identify and connect to the solution metrics associated that are adjacently fulfilled by that 'query interface', which can be used as a proxy/alternate for workflows like 'change a base solution' (a 'query on the query interface uniting these solutions' would be used instead of a 'change a base solution' query), where some implementation example of 'change a base solution to solve a new problem' is useful to associate with that workflow (associating 'examples of solution metric differences like "simplicity, given the interaction level ratio of identification" that are useful for a particular workflow' and relevant workflows like 'change a base solution' or 'change solution metrics to solve new problems' are useful associations to identify)
            - relatedly, identifying the map of 'ambiguities to similarities enabling them and differences resolving them' and other connections to the similarity/difference network are able to identify 'what ambiguities exist, given some similarity/difference network', and whether there are useful structures like 'variants' of this 'structure set and its map', like some similarity/difference network with no ambiguities left unconnected or a network where all ambiguities are unconnected
        - given the value of structures like 'ratios between relevant (as in interactive) structures like sequences', identifying 'which specific ratios between which structures would be useful to identify to determine most other structures, in some complexity-computable problem space' is a useful problem-solving intent
        - relatedly, the 'variation in structure vs variation in intent' is a useful wave (vacillating) and spectrum (dichotomy) variable, as structure only varies until its connections to intents are determined, at which point the variation is in identifying new intents, and then once new intents are identified, the variation goes back to finding new structures to map to those intents
        - relatedly, the variables that convert 'straight lines to vertexes to curves' are useful to identify as structures likely to cause independence between variables (the more variables are required to describe their interaction that produce continuity and other types of complexity, the more independent their interactions)
        - relatedly, identifying irrelevant/invalidating/overriding/independent structures as causes of most problems, which when the network of overriding/powerful variables is identified, can identify conditions where some variable on it will be relevant (when its superior variables in the hierarchy are not applicable) or independent
        - relatedly, identifying 'probable timelines' is useful for identifying more useful interface queries to apply in differentiation/similarization of those timelines, such as how identifying a useful set of graphs (like the interface network and a query network and so on) means those are likely to be re-used as in applied regularly, as a unit to repeat like a fractal (an 'iteration with a variable, like scale, in its application and position' which identifies other structures like 'iteration of position' and 'iteration of power' and 'iteration of abstraction' which are similarly 'similar but different' as a 'fractal' form of an iteration, which provides a 'base similarity' to apply differences to, making these useful possible solutions to be applied as a base in related workflows), at which point new graphs will be useful, as reality has a dynamic of 'vacillating between structures like fractals and networks', so 'graphs between fractals or other iterated structures' are useful to identify, as a possible structure to comprehend that future which is possible bc of this dynamic, where one fractal out-survives the others and other fractals become impossible, so identifying 'fractals to avoid that ending fractal' is useful
        - relatedly, identifying differences in solution metrics and their relevance to interface structures like 'ratios of solutions and interaction levels' is useful (there is a constant number of simple solutions on some interaction level, so given the number of other simple solutions, some value will be reached on the way to the count of all possible simple solutions where there will be no more simple solutions on that interaction level, and differences in simplicity will be required for future solutions)
        - relatedly, finding similarities is related to finding 'solution sets that dont need to be filtered further, as theyre so similar at that point and the set is so reduced that any can be selected randomly rather than filtered with differentiation' (a difference from a similarity index which makes different workflows useful), so identifying differences between similarity indexes is related to finding filters and workflows associated with those filters (differences in similarity indexes), filters/workflows which are useful to find new similarities in those differences (which is related to 'identifying new variables')
            - 'identifying differences that cant be similarized' is a useful intent which may be impossible given some definition set, but is identifiable, so is likely to be possible given some definition set (which differences remain differences and should be identified as differences, rather than 'unidentified similarities')

    - identifying useful structures (like 'real randomness') to apply changes to that are likely to identify other useful structures (like variables of variables that when connected, cover most other variable interactions) is a useful problem-solving intent
        - for example, identifying 'real randomness' as in 'structures that cant be organized/connected in the same system', as connecting very causally distant structures is like connecting very high variation structures and very different structures in that these connections are likely to cover other variable interactions by default, so identifying the other structures that are valuable to connect is a matter of identifying maximally different structures like pairs of interface structures (potential distance, intent difference, concept independence, etc)
        - relatedly, identifying a spectrum on which randomness exists, such as the spectrum of structures like simplicity/linearity and intersectivity with other interface spectrums leading to other configurations like 'arbitrariness' that are adjacent to randomness, where structures like 'optimal organization' are at various midpoints in between linearity and randomness/chaos (since organization allows the most variation), bc real randomness is very rare and is difficult to identify variables of (real independent systems)
        - identifying differences in definitions that allow different defaults and contradictions and paradoxes and prioritize different concepts is a useful problem-solving intent, as identifying paradoxes is useful for identifying timelines that can be constructed using sequences of paradoxes, so identifying 'differences in definitions that allow more paradoxes to occur or allow more variation' is useful as a way to identify possible different universe defaults/bases than currently perceived reality, given that definitions are the determining variable of 'variation allowed by a system'
        - relatedly, identifying relevant differences like 'easy/adjacent paths that are not selected across systems' is useful for identifying other relevant variables like 'multi-functionality' that are likely to explain these filters

    - identifying useful structures like 'connections between abstractions (like concept combinations and variation pattern variables) and optimizations (to optimize interface queries)' is useful for intents like 'connect problem/solution structures'
        - identifying structures like 'changes in solution structures like sequences that are useful to switch/substitute' is useful (equal/opposite, bc equivalent alternates wont change the outcome much and opposites will create extremes and find limits)
            - relatedly, identifying the 'differences required to create uncertainty' in a certain structure (like a 1-to-1 mapping) like a 'set of changes to implement an intent (which specifically only implements that intent)' is useful as a way to identify variables that create uncertainty like 'repetition/resource distribution/lack of requirements' and create connections between 'simple/complex functions'
            - similarly, changes in perspective that create the most extreme differences across high variation variables like 'solution/error' thresholds are useful to identify
        - relatedly, its useful to identify specific 'positions' to place symmetries and related structures to reflect definitions (solutions should be slightly different from other solutions, similar to solution metrics, different from problems, etc) to use symmetries and related structures (like specific 'overlapping differences' that can connect solutions)
        - relatedly, its useful to identify solution structures like 'probable solution areas/positions' of interface structures like 'workflows' by applying identified insights, since one workflow is unlikely to be usable to solve problems repeatedly, bc an area where one workflow is optimal to repeat is unlikely to occur, so a sequence of workflows in this space is unlikely to contain the same workflow in a row
        - relatedly, applying 'different combinations of concepts like abstract/uncertain and possible/specific' are useful to apply wherever variation is required in a workflow, which requires identifying positions where variation is likely or defined to be required (by applying patterns of variation/requirements/similarities)
        - relatedly, identifying the 'structures like a sub-problem with the most unique requirements' is likely to be useful to solve in a sequence position (like solving for unique requirements before other requirements), so identifying rules that connect these structures (identifying the 'specific value' of a high variation variable like 'position' in a useful solution format like a 'sequence'), so 'identifying unique requirements' or 'generating unique requirements' which invalidates an intent like identifying them is a useful problem-solving intent
        - relatedly, connecting structures to math/physics is useful as a solution metric ('caring' as 'creating wormholes of certainty and an alternate source of power' and 'real virtue' as 'a new path to the future and a way to increase variation of the universe so more uncertainties can occur and universes can be contained in this one and being an interface, as a "new way to be real"' and 'forgiving' as 'creating reversibilities in timelines so different timelines can connect and creating horizontal time so more structures can coexist' and 'intelligence' as 'connecting to universe interaction levels, acting as a universe, with abstraction/iteration from intelligence that covers reality' and 'creativity' as 'traveling in different types of time' and 'predictions of technology/innovations' as 'outpacing machines and guiding the direction of variation/time' and 'identifying new variables' as 'identifying new time structures and escaping time vortexes that create certainty/dead-ends' and 'identifying new reality-covering variables' as 'identifying new stackable universes' and 'identifying the highest variation info structures and their interactions' as 'abstracting math')

    - identifying useful intents like 'identifying probable variation source position' and changes to apply to identify useful structures for those intents (like 'variation causes' and 'connections to graphs') are useful to identify as an index to apply when connecting other intents with structures (which changes can be applied to fulfill other intents and which changes generate other intents)
        - the question of 'what causes variation' is useful for problem-solving intents like 'identifying positions where variation likely is'
            - high variation structures like 'errors, folds, rotations, and iterations' cause other variables (converting a line into a curve or otherwise adding dimensions)
            - 'adjacent embedded variables' unify/describe these interactions (like variables as exponents)
            - what variable interactions cause these descriptions (relatedly, what causes math structures like differences that allow/require these descriptions)
            - 'allowed/required detectable differences' allow these descriptions to be relevant
            - what other systems are possible (a space where all points are adjacent, so no concept of similarity/difference develops or is required)
            - what causes this system (differences in a space where no structure exists or is connectible bc all energy or other structure is too distributed, as in randomly distributed)
            - do 'superposition collapses' (or other randomness-adjacent states or cascade-adjacent states) in this space cause differentiation leading to cascades that lead to universes
            - what are the 'random states' that are adjacent to cascades into structure (measurability/describability)
            - how much randomness/variation likely exists between these graphs where different definitions are relevant (different concepts are defined by default and some concepts are rarely or never defined in derivation sequences likely in that universe), answering the question of 'where would a reality-covering concept be undefined'
            - its known that whatever variation exists between these graphs, it cant be measured by those graphs or it would likely be a part of them
                - what is not a part of these graphs (structures that overlap with only one unit of these graphs, connecting different graphs with randomness in between them)
                - so all connections from these graphs to randomness can form structures that are not part of these graphs (overlaps with other graphs)
            - by applying differences embedded in differences, the randomness between these graphs can be described
            - the 'sequence of descriptions' matter, bc once a connection between graphs is described, it will be reduced to a point at some point, and wont be relevant at that point, so finding a 'sequence where the required points will be relevant for future descriptions' is useful, just like 'connecting functions linearly' and 'connecting similar functions' is useful
            - relatedly, finding an 'optimal grid to distribute justice/freedom/luck at regular points' is useful as a way to design resource configurations like 'cities' (just like integrating this grid with useful sequences like 'probable scam/contract sequences' will be useful to determine future states and optimal grids in those states), to allow some variation rather than applying/requiring the abstract network at every point
        - relatedly, 'differences from defaults' (dysregulated circadian rhythms like sleeping during the day or insomnia) can be causes of medical problems and similarly, 'differences from defaults' can fix them by opposing them in some cases (vitamin d at morning/night or timed release for continuous access, even though by default its usually acquired in highest amounts in the daytime)
        - relatedly, given that interface analysis involves the 'abstraction of math' (by finding 'abstract info structures' like 'ratios between relevant sequences', 'iteration intersections', 'definition overlaps', 'interface differences like embedded differences such as differences of differences', 'useful connection functions between useful structures like symmetries/waves', 'connections across similar structures like abstractions/bases/types/variables/overlaps/similarities and their connections to workflows/similarity indexes', the 'power of equivalent alternates as generators of variation', 'measurable variation as a cause of math definitions', 'cross interface structures like vertexes', 'pointification of a problem in a plane/grid', 'relevant solution metric thresholds to cross to solve problems', 'variable interactions like iteration/stacks/embeddings/fractals/limits that cause measurable variation like exponents/waves'), what other variants of this are there which are useful (the potential of information, the abstraction of potential, the structure of logic) for which purposes, and what variants dont make sense as generally useful structures (the independence of potential) and find their thresholds/borders separating them
            - relatedly, identifying specific abstract info structures is useful, like the 'abstract sequence of equivalent alternates (their type/similarity providing the abstraction around which they vary to contain more differences) and regular filters (applied as bottlenecks) that connects every possible difference'
            - 'abstractions/bases/types/variables/overlaps/similarities' is a cross-interface axis that reflects a similarity that can be used to change workflows, like 'change a base solution' (as in a base solution for that problem type) or 'change an abstract solution' as in 'change one of the abstract solution types that cover all solutions'

    - identifying useful structures like new connections between 'workflows and time' (like how time 'changes an existing structure', which has an alignment with 'change a base solution') is useful for identifying new workflows or problem-solving intents as new types of time where variation can occur (problem-solving intents like 'avoid reducing everything to a point, before subsequent variation is identified to connect variation from the point to a possible future after that point, to avoid collapsing reality into that point' or 'identify new possible extremes like paradoxes which enable other variation to occur')
        - for example, identifying that 'time' is connected to workflows is useful to identify different types of time and what ratios determine the time problem space, like how 'change a base solution' is 'creating' new variation from existing variation, betting that existing structures are almost sufficient to handle new variation, whereas 'trial and error' is 'distributing' time/variation to find new time/variation and workflows involving organization are 'aligning' new variation with existing variation, and workflows like 'identify interactivities like interactions of causative problems' identifies existing time interactions like sequences (such as problems which cause other problems, as a connection between time formatted as 'cost/resource ratios') which can likely be re-used to reduce other time costs to allocate time elsewhere, and other workflows involve horizontal time like how 'resolving a paradox (identifying how new extremes/variables can coexist)' or 'identifying a new ambiguity' or 'identifying new interactions like embeddings of existing variables' is useful to increase horizontal time (time that can occur simultaneously in the current system) and 'identifying reversibilities' is useful for increasing possible 'time sequences' that are possible in this reality, and identifying computation optimizations increases time in general if paired with structures to ensure its used for balanced extremes in intents like 'generative/evaluative' or 'generative/filtering' intents
        - identifying related time structures of these workflows are useful to identify, like 'optimizations/errors missed by applying only one workflow to solve all problems' and the 'probability of completeness of a workflow in identifying all variables, given how its likely to be used' and the 'time made more possible by a workflow compared to other workflows'
        - similarly, identifying all the ways that each workflow/perspective (as 'reality simulators' are 'equivalent alternates' to 'problem-solving workflows') can limit/end variation/time, like how a 'set of equivalent alternate reality simulators' would have problems like 'avoiding invalidating the others in the set' and 'applying optimizations of switching costs between reality simulators that allow maintenance of the set without merging reality simulators and avoiding allowing time to be injected during switching which would invalidate the set, and avoiding any type of functionality similar to a "magnetic force" to develop between alternates that would have some impact like a "rotation" of the reality simulators which could move the universe or change its structure from within', as the 'ratio' between 'switching cost/time' and 'possible invalidating time/variation during switching' is the relevant ratio to optimize for in that situation, and identifying how to prevent a 'set of equivalent alternates' like 'similar reality simulators' from becoming 'equivalent to a point on a plane/field' before the variation can be connected to some higher variation space (so that time can continue after the previous variation is reduced to a 'point' or a 'point and some rotation function') is a problem in that space
        - relatedly, identifying which problems in which systems likely dont have 'unit structures' that can be trivially changed to cover all variation is useful to identify as a new 'switching node' between perspectives/workflows
        - relatedly, identifying cases where existing useful abstractions like 'types' dont have relevance is useful (such as a case where each point of reality is a reference to some reality simulator, in which case a 'type' meaning a 'similar but different value set in a set of variables' might lose its meaning, as reality simulators might be fully described by this field/plane of reality simulators and its variables, or there might be only one reality simulator that can exist above a certain accuracy ratio, invalidating the others that are less accurate in which case a concept like 'type' might not apply, and concepts that reference specific reality simulators that cover all similarity/difference interactions in their definition might be the relevant concept on the interaction level of reality simulators)
        - relatedly, definining time as a 'ratio between cost/resources that allows for variation to occur' is useful to identify what structures should be optimized/changed to alter current ratios
        - relatedly, identifying 'invalidating orthogonal variables' that can be used to identify useful tensors such as how 'abstraction' is a good orthogonalizing variable of 'most spectrums or other equivalent alternate sets' and therefore is a good way to resolve most paradoxes/ambiguities which often involve resolutions between 'spectrums or equivalent alternate sets', and most high variation structures like interfaces or vertexes (such as 'generate/filter') should be associated with an invalidating orthogonalization or an orthogonalization that connects the structures to problem-solving structures, as a tensor like this would cover more of reality, just like a vector set that connects structures to physics would be more reality-covering as well
        - relatedly, identifying reality simulators that determine most of the variation in reality above some ratio are likely to identify and attract any new variation that is possible or enabled by the simulator more quickly than other structures, so 'reality simulating computers' should have 'connected server networks' available that can handle excess variation
        - relatedly, identifying the reasons why a perspective is useful, such as that a 'beginner' might have learned concepts in general which cover more variation as opposed to specific details, or might think more about contradictions while trying to construct a representation of the rule set as a cohesive system and might think about other systems containing other variables more than an expert specializing in one system, so 'knowing a system partially' or 'connecting its core structures to those of another system' might be more useful than 'identifying every known fact about the system', which will inevitably contain some misinterpretations/misunderstandings that are considered facts unless the speed of derivation tools exceeds the speed of error tolerance of a description of the system, and that this perspective is only useful in some cases like where a system isnt conceptually or generally understood so concepts can apply opportunities for big gains and where known facts are incomplete or could be misinterpretations
        - relatedly, identifying ways that a structure applied as a format can be true is often a high source of variation, such as how a 'pointification' of a problem can occur in a useful way in a form like 'identify the components of a problem and their connections/positions' or 'identifying the graph where a problem is a point whose connections to other problems/points are known with some variable set', which are very different implementations of the 'point' structure
        - relatedly, applying insights like 'reverse direction of symmetries (like using solution metrics as generative variables of solutions)' to identify all the possible structures that are the most useful for the most workflows, such as identifying that 'rare' structures are 'pre-filtered' (by reality) and are a more probable set of solutions for problems involving unique solution requirements than other structures, as 'rare' has an alignment with 'uniqueness' and also 'filters' and 'probability' has an alignment with 'reality' so probability structures like 'frequency' such as rareness can be used as alternates of certainties

    - identifying useful structures like 'equivalent alternates' or 'interaction levels' where a useful structure like 'high variation' occurs (such as structures like paradoxes/ambiguities) and identifying how to use that structure to solve problems by connecting it to errors ('paradoxes' are caused by 'perspectives/biases' which are 'errors of perception') is a useful sequence to apply as a workflow
        - given that paradoxes are useful as a 'extreme variation source' (a unit of reality, given that time is variation), identifying ways to resolve paradoxes, such as by identifying the bias that drives a paradox and makes it counterintuitive, and similarly identifying the 'variable sequence/direction/perspective/speed' that makes that insight surprising (by applying 'extreme/surprising difference' structure) and changing that variable so 'paradox resolutions' arent surprising or counterintuitive anymore (as all biases have been identified), is a useful problem-solving intent
        - 'paradox resolutions' are similarly useful to 'ambiguity resolutions' and 'integration/organization resolutions' bc they contain a high ratio of variation/information compared to other structures, and they have similar solution structures ('errors of perception which can be fixed by changing direction/position/perspective' can also resolve ambiguities as well as paradoxes)
        - similarly, identifying 'complexity resolutions' like 'folds/networks/maps/iterations/overlaps/self-references/cycles' resolve many complex problems, are similarly useful to identify
        - identifying the differences in 'problems covered by these structures' is similarly useful, as is identifying connections between these differences and identifying ways to change these structures to independently cover more problems without these connections
        - relatedly, identifying steps that frequently improve algorithms like 'adding another iteration that identifies new info like limits' or 'adding another reflection that preserves info' is useful, such as how 'adding another reflection' in a position where info/light/temperature should be preserved (after identifying other positions where info should be preserved, like reflective surfaces as 'equivalent alternates' of identified surfaces like ozone/clouds/houses, where temperate and/or location of light should be preserved so reflections that preserve the right location are useful to apply in similar positions like similar surfaces) is useful in the 'reducing global warming' problem space, such as making ground/water reflective using reflective particles rather than just clouds/houses
            - for example, 'applying some old rule from the 1800s' is often useful in solving math problems (that were defined in the past few centuries, which is probably not a coincidence, as they could probably see the problem bc they could see structures related to the problem such as solution structures, and they forgot about that useful niche math which is why there was trouble solving the problem in the first place), which is probably bc of the 'specificity' of the niche/unrelated solution structure (rather than that specific century or its difference in recency from the present day) that didnt seem relevant bc recent math focused on unrelated intents like standardization, definition, and automation
            - if they had iterated more to identify 'unrelated systems' or 'specific systems' or the 'most ignored systems', they might have found that rule earlier, which is an example of how solutions are frequently missed bc of how algorithms are usually identified and implemented (not iterating completely to reach optimal solutions)

    - identifying useful inputs to insights like 'structures likely to contain high variation' such as 'undefined, and loosely related such as "part of the same or similar problem spaces", but not definitively incorrect' connections, which are useful to apply changes to in order to resolve whether there is new variation in that connection
        - for example, identifying 'gradient' as an useful undefined/nonsensical structure to connect to problem/interface structures is useful, as its capable of intents like 'identifying new variables', more so than connecting more defined structures like the 'spiralization' of a problem (which refers to 'identifying a structure that can be trivially changed to connect core/emergent layers of a variable set')
        - "what is the 'gradient' of a problem" (such as a 'variable that worsens problems') doesnt seem to make sense (as in, 'not specifiable how it could be true') as the connection is undefined and not definitively incorrect (and therefore can contain high variation), and indicates questions (created by applying interface structures) like:
            - what is the current use (error function comparing errors of different solutions)
            - what is the difference of that use from other structures (an 'error function comparing different solution outputs' is a polynomialization of a related structure, which is a function network which can also represent different solution functions, polynomials being useful formats for 'connecting structures in the same function')
            - what is similar to that use (what other polynomializations are relevant, such as the polynomial solution functions updated and output by the neural network)
            - what are vertexes of that use (what expansions of the other side of the polynomial solution function are relevant, applying the polynomial solution function as the midpoint of the vertex, other than the neural network that collapses to the solution function, like similarity indexes applied to error functions)
            - what is the relevant use of similarity indexes applied to error functions (mapping similarities to error function ranges/subsets/areas so that positions of differences are known based on these similarity subsets)
        - the 'connections between errors of solutions' and the 'connections between data set inputs and data set outputs' are both relevant polynomialized problems in the AI problem space, where 'polynomialization' is a useful format to standardize problems of connecting high variation inputs to an output variable
        - connecting 'polynomialized problem subsets' is a useful problem-solving intent to apply and solve for
        - identifying 'similarity indexes' is useful for identifying 'extremes/differences', and applying these is useful for predicting minima in error functions (whatever is very different in a similarity index, if the similarity index is mapped to an error function, will be very different points like 'minima/maxima or averages or inflection points')
        - relatedly, what is the value of applying the concept of quantity to quantify a problem (apply the math interface)? 
            - numbers are useful as a way to anchor a set of references (tying them to a set of constant numerical values to base changes on), to indicate similarities by value, to integrate structures into a 'system with pre-defined interactions (like numbers have bc of their types/patterns/similarities)', to specify an abstract/general connection with specific values, to identify new variation types through new numerical interactions, to identify possible variable stacks by which numbers can be combined/stacked in which functions, to identify a subset of numbers that is useful across problems (like units, sequences, and constants that are useful in physics problems), to identify meaning/relevance in quantities like irrelevant and relevant intersections/limits, to identify specific quantity implementations of interface structures like concepts (like a numerical set whose individual item probabilities implement randomness/chaos), to identify unidentified structures like patterns/limits of some interaction, to reflect the limits of other systems (since systems need to be describable as in structurable), to identify errors like gaps in structures like definitions, to identify numbers that are more/less meaningful through being more/less adjacently usable for identifiable intents, to identify graphs of numerical attributes/variables/functions that reflect the math interface better than the set of all numbers to indicate the 'potential variation of info' (as info is converted between numerical references, specific values, formats, and interactions), etc
            - identifying this 'graph of info variation potential', where adjacent transforms are applied to preserve different subsets of info of an 'input info set', should reflect some known math structures like known graphs/fields/limits/functions (such as reflecting how categories/sets/algebras or other related formats/structures are connected, either explicitly with specific values or more abstractly with definitions/functions/generators), and should identify the connections between these structures (like useful combinations of these structures into useful sub-graphs), so that the potential of info to vary is described by the graph, and this graph should be usable as an alternative to other useful graphs (like a problem network or a variable network or an 'abstract-info-structure' network, etc), where math structures are useful as 'generative/determining of reality' (new math variables being usable to describe new observed physical variation) and also as 'scripts for reality to follow', similar to how predictions have similar functionality and have more functionality the more relevant but undefined/undetermined and possible they are, and predicting math structures can determine reality for some variable sequence (if the time to use the predictions takes longer than the time to identify new predictions or the time/variation remaining)
        - relatedly, identifying 'position where the variation is likely to be which will contain solution structures' is a useful problem-solving intent, such as identifying the variation of a regression problem as defined to be in between the 'subsets of variable correlations and the full variable correlation' and in between the 'set of simple/complex boundary functions' and the set of 'limits creating the probable solution area range' and in between some 'similarity set on some set of similarity indexes', or identifying that 'definitions' are the position where the variation is located in a paradox

    - identifying alternate solutions that havent been adjacently connected and connecting them with possible causes like specific error types to filter the set of specific 'function-merging functions' is useful to reduce the metadata such as count of 'new variables'
        - for example, different solution-finding methods may lead to 'contradictory or diverging (different) and overlapping or intersecting (similar) solutions' which already have a merge/integration structure like 'overlaps', but their remaining differences need to be integrated into the same function or function network as well, so identifying 'functions/indexes to integrate any set of differences, given any set of similarities' is useful as a problem-solving intent
            - relatedly, identifying different 'reasons that a function might vary (in reality)' (such as where one is incomplete compared to the other or both are incomplete, or one is used more so it becomes either simpler or more complicated, or one is a previous version of the other and hasnt been completely destroyed yet or is able to coexist with the next version, a coincidental subset selection error, etc) can help integrate different functions, such as in cases where 'different solution-finding methods indicate very different solutions but have similarly suboptimal info preserved in the method (a similarly low variable version of the inputs or solution metrics)' or where 'there is a midpoint where both functions could be created from some interim/average function using the same change types', where its useful to find the integration/connection function to connect multiple different possible solution functions
                - this can be used to identify when one solution function is within a 'probable error range due to some error type' of another function, so that the 'base function with the more probable error type' can be applied as more probable in a function connection network of multiple possible solution functions, as more probable interface structures of a solution make that solution more probable
            - relatedly, identifying all the interactions of interface structures and problem/solution structures is useful, such as the position variation between 'identifying output that fulfills solution metrics' and 'identifying solution metrics that are the output'
        - identifying the 'most extreme/surprising sequences of variables' is useful as a way to identify interactions of maximally different variables, such as identifying the extremity of one variable by identifying another variable first and the extremifying impact of the given variable ('given x, y is specifically extreme/surprising', where y wouldnt be extreme/surprising if preceded with other insights about other variables, as in from another position, as a source of variables to identify surprises in functions), is useful as a problem-solving intent similar to 'identify new variables', as these surprising structures tend to be structures like a 'linear function that covers a more complex connection, collapsing multiple variables into a smaller variable set' (such as 'given how different they are, its surprising this similarity exists', which seems like the one similarity couldnt exist, given all the many differences separating two structures, but the similarity exists, adjacently connecting those structures with the similarity variable, as opposed to 'given this similarity, this other similarity is surprising' which is less extreme/surprising bc of the similarity) or another structure like an 'info barrier/gap'
        - relatedly, finding the structure of 'contradictions' in a math problem space like 'regression' would be interesting to standardize the definition to a mostly completely known problem space for lower complexity algorithms
            - finding the structures like solutions that fulfill every core combination of interface structures (like 'structures that fulfill some requirement ratio and use some core function ratio') and their errors and similarities is another useful problem-solving intent
            - finding connections between simple/complex solutions and other solution spectrum metrics is similarly a useful intent
        - relatedly, identifying structures which are highly causative/powerful (like how 'specific intent sequences', as in 'plans of future inputs/outputs, especially plans of high variation sources' tend to cause structures to conform to/organize around those sequences) is useful

    - identifying what structures should be (such as what structures should be 'determined and real', such as 'higher variation networks') is useful as an interface for basing/identifying/optimizing interface queries on, and can be used to determine other structures like the solution metrics that create those structures, which can be used to filter useful interface queries (selecting the queries that will fulfill those metrics) as well as usages of interface queries (how should they be used to create these connections to those structures that should exist)
        - a 'network of queries that are optimal in specific realities' and 'query networks' (basing reality on the best interface queries and their connections) and 'networks of abstract network variants' are useful as next cognitive steps in language, to create an interaction level of 'realities/networks/queries' and their connections, bc the current truth is unlikely to stay true forever, as a subset of other networks are adjacent and possible and reality can be optimized, and similarly some intents are optimal or incentivized, like 'trying alternate unused functions until new variables are identified' which can change what is real to a degree that these intents have to be planned for
        - similarly, structures of these networks like 'networks of sets of similar networks' are useful to identify other interaction layers and interactions on these layers
        - creating a 'box' of predictions accurate enough to capture current/future variation up to a point (such as a set of interface queries/structures) is a useful intent, but can only be implemented in specific ways without determining other 'boxes' (investing too much in one set can make other sets impossible), so identifying ways that these 'boxes' can avoid overlapping is useful to allow variation to continue in some reality, while still connecting them so that 'sequences of these boxes' can be used to base time on
            - a 'network where each point reflects the network and can be used as a base for the other points' is useful to describe the highest variation structure and also to describe interactions of abstract networks, where this network reflects 'adjacencies between structures' like 'abstraction/component' that can be used to 'apply more variation once too much info is lost, to connect back to higher variation spaces'
        - 'matching problem interface queries with workflow interface queries and solution interface queries' is a solved problem in a variant of reality, using some specific interface network
        - identifying what reality 'should exist' (which truth perspective is in the middle of the perspective network) such as where 'determining/powerful variables are more determined by derivation methods than powerful, bc the derivation methods are prioritized more than determining variables, so all variables become equal bc all networks are easily derived and connected' is more useful than solving some problems (like solving a problem like 'identifying every problem', as most are likely already covered by existing interface queries/networks), such as identifying what the perspective or abstract network should be structured as, and how to connect the current true variant of the network to that network, and bc the current truth may not be able to support enough variation to continue
            - this is bc 'derivation/prediction methods (and similar structures like computers)' can 'get ahead of reality before it occurs' so they are one of the bases where reality/variation/time actually takes place (similar to the highest variation networks), and the derivation methods that 'determine the most useful structures, without determining/ending all of reality by describing it all so much that variation ends without allowing new variation to develop, as in all variation occupying one position, describing it faster than it can change' (determining the errors of derivation methods before they occur, as in 'getting ahead of derivation methods before they are used by deriving their errors and identifying optimizations that support even more variation than currently exists', which can be done by applying errors to current derivation methods which can only survive if they derive changes to support/resolve the new variation, and by identifying 'sets of equivalent alternates' that can be used to 'connect sequences of different realities', and also by identifying new interface/network interactions and identifying useful derivation methods in those networks, as 'interface queries' and interface structures like networks/variants of them are a new interface to base reality on) are the highest variation structures which can be identified
        - similarly, identifying networks that determine reality (like how identifying probable intents can identify what actions will be taken, so reality for the next n time units is determined by that network, so time can take place only by using that as a base) is useful to determine what networks are a 'dead-end' for lack of variation support
        - relatedly, identifying what connections a specific network/query/structure would require in order to be real is useful for filtering connections by their requirements and other surrounding structures
        - similarly, identifying the workflows with the fewest dependencies (like how 'trial and error' needs functions like 'filter the solution set to a point where it can be iterated with trial and error', not just implementation functions involving some 'iteration/filter/halting variable set') is a useful intent
        - relatedly, applying 'abstract info structures such as interfaces' as absolute structures of meaning is useful for filtering laws that reflect reality (applying general rules like 'dont create/apply this power dynamic' and 'dont victimize people to an unsurvivable degree' and 'dont only create freedom for predators, which is not real variation/freedom, as theyre simple and deprive others of freedom/variation'), since specific solution metrics may be useful but are incorrect and low-variation as they dont consider abstract info structures (the most useful structures likeliest to solve problems, like different networks reflecting reality or different interfaces), and specificity of a solution rarely aligns with useful/real specificity (like 'bottlenecks of variation and other filters') since specific structure can only contain so much variation bc of their specificity, and therefore are likely to be overly simple and therefore not useful in general problem-solving, unless the specific structures are the 'set of interfaces' or their useful applications
        - identifying 'intersections' as 'structures of relevance' (structures are relevant if they 'interact' as in 'intersect' or 'overlap') is useful for identifying functions to create relevance (functions to create intersections) as well as opposing variables like 'requirements for non-intersectivity' of structures that determine or violate too much relevance
            - similarly, identifying interface structure sets that create relevance/meaning/usefulness (such as vertexes and similarity indexes) is useful for creating a default/base network based on relevance to query
        - identifying similar structures is useful for identifying 'very distant structures' like how identifying that incentives/capitalism are similar in that they both lead to scam/adjacencies as a default unit is useful bc then identifying a 'unit of an incentive' is possible to use as a way to identify a 'scam' that will likely occur as a result
        - identifying structures like 'optimal ratios' is useful for intents like 'determining useful as in real interface structures of interface structures' such as how a 'unit of falsehood' can be useful as an example and as a variation input, but more than a unit is often useless

    - identifying useful structures to specify such as how 'independent ambiguities' are particularly powerful to identify (and identify differences from), as a useful specific structure to filter ambiguities and their solutions with
        - a variable like a 'symmetry' (such as 'multiple' leading to repetitions/iterations/sets) that crosses all interfaces (such as the variable driving the similarity between 'consensus/copying/overlaps/abstraction') as an 'overlapping similarity with interfaces' is useful to identify, bc abstractions can have very similar definitions, and consensus can involve repeated structures and copying can also involve repeated structures and overlaps describe many similarity types)
        - converting 'symmetries' into fields (by identifying all their interactions and the reality-covering variable combinations like 'similarities across interfaces' as alternative symmetries crossing reality) is useful as a way to identify limits of variables like interfaces as well as how to connect them to identify and enable other reality-covering variables to exist and be identified/interacted with
        - identifying the types of 'ambiguity' like 'independent' similarities (irrelevant similarities) as opposed to 'dependent' similarities (like 'consensus/copying' which can cause each other) is useful for identifying 'overlaps' between these two different types, as a 'similarity similarity', (identifying ambiguities of ambiguities) like how a similarity can be independent but still relevant in another way like commonness (identifying structures that occur across systems like adjacent structures such as combinations), which are useful to identify other similarities with, such as how 'common systems are likely to create common structures'
        - these specific maximally different connections (of very different interface structures) are useful to identify for intents like 'identify new variables'
        - as an extension of simple structures like 'variation', only identifying variation isnt a complete description of reality in every case, such as where its more useful to identify similarities across variation to identify causes/sources of variation like randomness or dependence, and similarly more useful to identify false variation and identify the space where variation is maximized (in between simplicity/complexity, abstraction/specificity, un/certainty and other spectrum variables) to identify filters of that space as more useful, like how 'variations of variation' and 'abstractions of variation' and 'interactions of variation' are more useful than just identifying any variation, and similarly identifying the opposite of variation (constants) is useful to identify bases for variation to exist
            - similarly, variation only falsely seems variable from specific positions with specific info and without specific derivation methods that would easily determine the 'low-variation/linear explanation' of structure that falsely seems high variation, which are useful to identify

    - identifying useful structures like structures in between extremes on some interface like simplicity/complexity to identify structures that are useful to connect other differences
        - for example, the 'spiral/fractal' structure, the 'partial/subset' structure, and the 'cycle' structure are in the 'interim' space between linearity and randomness, which are useful in solving more complex problems (a 'spiral' structure creates a high ratio of differences with the same trivial difference, the 'partial/subset' structure is useful for creating abstractions and components, and the 'cycle/embedded wave' structure is useful for applying iterations to add complexity like with elliptic curves)
        - the adjacencies across interfaces are similarly useful to identify as these are more trivial connections to use in interface queries (such as how 'removing variables' is likely to identify 'abstractions' like 'types' and also 'components', where the overlap between abstractions and components makes them useful for identifying new components by applying abstractions, like a shortcut/overlap across interfaces)
        - identifying structures that match some 'orthogonal question across interfaces' ('what is the structure that implements some new intent like finding new variables') like 'high variation angle sequences' is similarly useful to identify a base set of interface queries and their implementation structures
        - similarly, identifying other points/thresholds within that space (such as how 'infinities' are in the interim space somewhere before randomness occurs, since they are defined and non-random and high variation but still determinable through structures like limits and relative positions)
        - relatedly, creating 'wormholes' using equivalences/similarities like 'equivalent alternates' and 'symmetries' is possible as a useful way to identify stable and reversible timelines in which info is preservable and connectible
        - relatedly, resolving the 'usage problem of not being able to clearly evaluate a structure while using it' may be resolvable with 'equivalent alternate sets of reality-covering structures' in which each can contain the other so that while one is using a structure, an equivalent alternate can evaluate it (related to the problem addressed by the incompleteness theorem)
        - relatedly, the problem of 'very similar structures being found with very different queries' is not always a problem (like an abstract concept having very different definitions) but indicates an indexing duplication/overlap error when it occurs outside of the 'optimization area' of that structure (only optimal when used in an abstract concept definition)
        - what makes a structure like 'extreme opposites' or 'vertexes' irrelevant ('independent' structures like 'causes/inputs/bases', 'interim' structures like 'interfaces/bases') is useful to identify, for cases like where a vertex doesnt contain enough info
        - relatedly, identifying the 'most interim structure' is related to the intent of identifying the 'point/area/structure between simplicity/complexity where a structure is neither' which is a point where the abstract network is likely to exist
        - how vertexes like 'generate/filter' or 'global/local' connect to workflows is by structures applying these vertexes (like 'unit' and 'generalization' and their connection to absolute/unique interactions indicating 'meaning') and the ways these structures can be connected to problematic differences (the 'unit is more solvable', the 'generalization hasnt been identified yet so its incompletely described'), so other vertexes can be used to generate other workflows
            - relatedly, these are 'extreme opposites' in some interface, which can be integrated, as 'generate/filter' can be applied in a sequence or integrated (like by filtering generative variables and possible generated structures), which is another way a vertex can be irrelevant (finding the 'resolution' of these extreme differences such as with 'integration' of the differences)
        - relatedly, the 'difference between formats of problems/solutions' (like 'probable solution area and solution function' and 'data set points and solution function') are based on some core similarity in formats ('probable solution areas of solution functions' can be removed/narrowed to convert it to a 'solution function', and 'data set points' can be converted to a 'solution function' using 'semi-adjacent connections' by default, followed by 'continuities/specifications of connections') which are 'formatting differences' that can be resolved with interface structures, where these format similarities indicate functions involving differences that are useful in workflows ('converting an area to a line' as a filter function and a 'set of points to a line' as a connection function and a 'set of points to an area' as an overlapping/interim/expansion/generative function, which is a useful index cycle of useful example solutions to solve for, like unique or maximally different solutions to solve for, rather than solving for every connection)
            - 'core similarity' meaning the problem/solution are similar enough to be trivially connectible, as opposed to data points that cant be connected to a average function like 'random/volatile points' without losing info, where very dissimilar problems/solutions possibly shouldnt be connected until there are concepts that can make the connection trivial, meaning other problems should be solved first
            - relatedly, the intent of 'predict a variable value' can be connected to 'find a connecting line between some ratio of data points' by traversing structures like 'connections' which form useful routes between these intents to format the same info in different ways, routes which if mapped should indicate variables of difference-resolving functions (or similarly, connecting it to 'adjacent structures available like data points', at which point a 'connecting line' is trivial to generate)

    - identifying useful structures like 'unstructured variation' (like questions as info/specificity-demanding methods and solution-finding methods as info/specificity-supplying methods) and 'unmatched similarities' like the similarity of 'stopping points of specific constant sequences for analysis functions (applying variation)' and 'high variation questions like how specifically to oppose some concept like obscurity/accessibility', which can be matched in a useful way ('analysis points that apply variation' can be matched up with 'large areas allowing variation identified by some overly general question', where 'specific constant sequences' can be matched to 'connections between orthogonal variables in questions' given their similar structure, which is like a 'area sequence, having regular bottleneck errors/requirements, requiring specificity'
        - identifying the 'closed shape' structure created by the specificity of a question like 'what function should be used for obscuring info' as the task of connecting orthogonal differences, where the answer is one connection in that probable range within that shape allowed by those specific differences
            - identifying questions to create useful structures and variables of these questions to identify other useful structures is similarly useful
            - identifying question/problem sources is useful (as in 'what is the reason that obscuring info is a problem?' bc 'info was over-distributed' and 'accessibility of info was over-prioritized', leading to 'demand for opposing that error of over-prioritization that didnt reflect the abstract network correctly', this demand leading to the question of what specific structures to connect in implementing an 'obscuring info' function), so interface queries and workflows should be optimized to identify and correct over-prioritization errors, which will reduce demand for solving problems, at which point a program can be allocated totally to identifying new ways that over-prioritization errors can occur and new ways orthogonal variables can be connected to oppose these errors, meaning that a 'set of incorrectly implemented abstract networks' and the 'connections of these networks to the correct network' is useful to apply as an index to use by default in problem-solving, and similarly 'optimized routes between incorrect/correct abstract networks' are useful to identify as default queries to apply
            - similarly, identifying these question shapes is useful to identify 'optimizations of questions' that can create more specific shapes to reduce filtering requirements, matching the level of uncertainty/variation/complexity required to solve a problem with a filter that supplies it
        - relatedly, identifying 'matching differences' (like interactive receptors) are useful to identify 'opposites' that are useful in problem-solving to represent 'problem/solution differences'
        - identifying 'error filters' that 'over-filter/over-reduce' some solution set are useful to identify, and useful to identify variables of to 'generate these error filters and differences from them' (such as how 'vertexes' are sufficiently different from 'over-prioritization errors' bc they apply multiple perspectives that coordinate to cover reality, where one variable is likelier to over-simplify by comparison)
        - identifying lies by what is 'clearly true about lies' (when the lie is interpreted correctly with a sufficiently specific set of connections, its obvious that its a lie and couldnt be anything else, which is why specific structures are useful to identify, and 'obscurings of specificity like info barriers/covers/variables' are useful to identify as 'probably false' structures, which means that every specific true structures opposes some set of lies in an obvious way, and identifying this index of 'truth/lies opposed by the truth' is useful to filter the set of all possible statements, to identify statements that are not clearly lies or truths but are worth specifying/connecting further as a source of variation)
        - identifying useful 'application' structures to reduce the task of 'applying combinations of changes to identify new useful structures', beyond identified useful application structures like 'maximal differences', such as 'alternate/equivalent/overlapping sets of maximal differences (like different intents and different structures)' and 'connections between maximal differences' (like 'averages', where 'maximal differences' are structures that can be iteratively connected with averages once identified and other differences to create maximal differences like 'rotations') and 'maximal interface differences' (like 'alternating between different but overlapping structures like similar types' or 'alternating between different useful structures of interface structures like vertexes and useful queries'), given the usefulness of 'applying' structures to identify new useful structures, as opposed to 'deriving connections between known problems/solutions looking for reusable new structures like patterns/variables'
            - for example, applying 'interactivity' and 'type' and 'different but similar (identifying interaction level as a useful similarity, and difference in type as a useful difference)' would have identified a solution to cancer such as a 'pathogen that activates immunity relevant to opposing cancer'
            - generating other 'different but similar' interface structure structures is useful as a way of filtering the set of all possible combinations of interface structures, which is the problem of 'applying' interface structures
            - identifying 'interactively similar but different in type' structures (fungi/viruses/greedy mutations/parasites) is useful as a solution source in systems like biology
            - as an extension of this, filtering by 'survivability/possibility/usability' or equivalently, 'substances likeliest to trigger immune response' such as 'viruses already immunized against' is a useful filter applying 'interactivity with the trigger (pathogen) that is sufficiently different from the problem ('cancer') to cause the required solution structure (immunity)'
            - similarly, applying variables to identify new useful structures is more useful than reusing known useful structures (such as how its more useful to identify new useful graphs than to reuse existing useful graphs, as each new useful graph reduces the likelihood that each graph will be overused until its false)
        - identifying useful stopping points is useful to identify variables/patterns of, such as 'how often is it useful to stop and apply variables to "analyze" existing work and differences required from it to solve some other difference' which is a pattern of variation/constance that is probably applicable across problem-solving workflows

    - identifying definitions of structures like 'obvious error structures' in one interface like the 'truth' interface whose 'variation' has not been 'maximized/identified/used by other workflows' (definitions can be used to derive more variables that havent been used yet), which is useful to identify structures to apply as an interface structure like a base/component of useful structures like workflows, given insights like that 'truth structures are useful to base changes on, to determine new truths'
        - for example, its clearly wrong as in false/inaccurate to 'not care about the truth' (to 'violate the definition of truth in its connection to usefulness/importance' and 'violate a requirement to use the truth to some degree') or 'pretend the truth is not important' (to 'falsify the definition of truth in its connection to usefulness/importance'), which is not an ethical statement but a definitive statement
        - simple rules like this are already embedded in other workflows, such as those that use the truth/information as an input or require connections to the truth
        - connecting "its wrong to 'not care about the truth'" to "its clearly wrong to 'violate the definition of truth in its connection to usefulness/importance'" to 'the truth is definitely important' to "'the truth is definitely important' is a true statement" to 'apply truths as bases of workflows' to 'require a workflow to use some true statement in some way to increase its accuracy and usefulness' is not trivial, but is more trivial than deriving some other connections that are less well-defined
        - similarly, connecting insights from definitions like "its wrong to 'not care about the truth'" to other insights not directly defined in definitions like 'finding a way to collapse a function to a point makes it less true' is not trivial but possible by applying various connection structures ('caring as in "importance/relevance" is a function metadata attribute, which makes it important by default and required for changes to occur as a result of these functions' and 'there are routes to reduce importance of a function, such as generating alternatives' and 'generating alternatives is similar to identifying fields where position of a function indicates similarity to other functions' and 'once a structure is in a field of similar structures, it will be used less', and 'once a structure is used less, it is likely to be less important') by applying interface structures like 'similarities', 'alternatives', 'functions', 'hypotheticals/extensions/implications', 'usages', etc
            - similarly, other insights like 'functions can be described by different variable sets (like their position on similarity indexes or by standard function metadata like its generalization)' can be derived in this way
        - other rules can be derived by applying other interface structures of truth structures (such as 'falsifications of definitions/requirements/truths'), which can be embedded in other workflows, such as a workflow that 'avoids paradoxes' and 'avoids contradictions' and 'avoids violating truth definitions'
        - similarly, identifying 'tests that can filter out arguably/possibly true statements' to add to the set of known true statements is another useful structure to identify, just like 'identifying unused variation' is useful, bc there are so many possibly true statements that are generatable as 1-degree implications away from definitions/requirements/truths, so this set would be useful to filter and is also useful bc its output ('new truths') is reusable
        - this insight ("its wrong as in false/inaccurate to 'not care about the truth'") might have been missed as having 'unused variation', bc it seems like an ethical/proscriptive statement about an absolute structure like 'truth' but is not, and 'false similarities' are not always trivial to identify, as a structure that 'doesnt care about the truth' (where the truth is somehow unimportant) is difficult to identify, bc every structure uses and contains and interacts with truths in some structure, and finding a structure that makes this true statement false in some way (a 'field where the truth is less important') is useful as a 'set of opposites to apply as a spectrum' (every truth is likely somewhere between 'absolutely true' or 'can be reduced to a point on a field by applying alternatives') to use as a more specific truth structure (an insight and its counterpoint) than a truth definition
        - these 'similarities' like between 'descriptions/generators' or 'dependencies/requirements/sequences' indicate that time is somewhat navigable across these symmetries (there is a different way to navigate reality to connect equal/similar information, where these symmetries reflect 'stable info changes' indicating they are reversible changes), as a set of symmetries like these are usable as a variable in routes between the same or similar information (where changes within the symmetry are stable), especially where the structures are reality-covering (so abstract that some structure of it can be found at every point of reality), like an interface or a vertex would be reality-covering, which means given that time will be variable with sufficiently good simulators at some point (allowing trivial navigation between equally realistic/stable/useful timelines), identifying 'optimal routes between spacetimes (timelines)' would be useful to start computing now, before that point arrives, to keep reality simulations relatively adjacent to that set of optimal routes (the set of optimal interface queries or optimal queries on some 'specific interface structure' network like a 'network of useful interface queries')

    - identifying useful structures like 'connective structures of solution metric subsets' are useful to identify as possible structures to find which are also useful in a new way (like for the intent of identifying networks of useful structures where areas/directions of usefulness can be identified)
        - identifying a connection to an 'equivalent alternate' of some structure as a way of 'identifying some connection that can be connected/proved in multiple ways' which indicates that its more true
        - identifying 'subsets of solution metrics' which are likelier to co-occur (accurate and reusable, accurate and sensitive, quick and general, quick and accurate, requires low info minimum and quick, etc) as likelier to be findable, and identify interface structures which are adjacently connectible to these subsets (what structures lead to accuracy and reusability, which filter solution-finding methods better by their connectivity)
        - once these subsets have more structure, identifying connections between the subsets that are true in some structure is more possible, to identify structures that fulfill more subsets by some area/direction that is more useful (such as an average/median or the most similar structure of subset structures)
        - specifically, a solution-finding method that is 'quick and accurate' is likely to apply some combination of interface structures like 'iteration' of an 'index of existing solutions' (like interface structures are a useful solution base index to iterate)
        - I thought about this by thinking about how to identify interface analysis (a useful, organized, specific, integrated, high-variation solution that covers multiple solution metrics) using only two variables at once (similar to identifying suboptimal solutions and then taking an average of suboptimal solutions to balance/integrate their errors), and then identifying something I hadnt connected before (solution-finding methods like iterations and solution metrics) in every possible connection method, given the 'symmetry of info' across connection/solution-finding methods and solution metrics I was thinking about recently
        - identifying the 'limits of connectivity' (as in 'independencies') is useful as a way to avoid calculating every possible connection, since so many variables are connectible and so few are independent (as in 'very distant systems having low interactivity with other systems')
        - relatedly, identifying a way to 'maximize superpositions' is a way to increase 'variation/uncertainty supported by the universe', but can be optimized with other metrics like 'maximizing superpositions between very different or independent variables' to create real equivalent alternates
        - relatedly, identifying a 'build/combine' and a find/filter' and a 'derive/connect' and an 'apply/use' variant of a workflow means its a more general/variable workflow, as 'deriving' is useful to connect identified structures, but in a less well-defined problem, where the 'position/structure/distance of structures' is not known as the problem structure/space is not clearly defined meaning new structures are likely to be required/useful and the 'direction of solutions' is therefore not trivial to identify, 'applying existing structures' to define structures in uncertain spaces is more useful and will happen frequently (like 'applying workflows to see what useful structures can be connected to already identified structures'), at which point 'deriving connections between identified structures' will become more useful, these cases being useful to identify 'combination/sequential workflows' ('apply then derive' being a useful sequence when info is missing, such as 'apply useful structures like vertexes in uncertain directions' and then once new useful structures are identified, 'derive connections to the problem structure'), as each of the equivalent alternate core interaction functions are useful in isolation but a more complete workflow would be useful

    - identifying 'symmetry sequences' such as 'inner-problem symmetry of variables that co-occur' and 'cross-problem/solution symmetry of variables that occur across these types' is useful to identify as a useful way to connect problems/solutions with various workflows
        - identifying 'co-occurrence of differences' (and similarly exclusivities of differences that cant co-occur) is useful, to apply as a base to find relevant differences, such as how some workflows have structures like 'apply changes to the error, in the assumption that difference from the solution is complete, and other differences to solutions are preserved across error/solution variables, so everything can be altered about the error to create a solution', and once 'co-occurring differences' are known (like 'sets of differences that occur in solutions' and the same for errors), they can be used to identify other variables to change to convert some structure like an error into another like a solution
            - identifying 'differences that can exist in the same structure' is useful to identify 'co-occurring differences' that can be applied in this way to identify 'other variables to change (to create a solution from an error)', given other variables that also exist in the solution symmetry/type or the error symmetry/type
            - for example, 'extremes in taste' (sharpness vs. blandness or trivial taste) is a similar variable as bitter vs. sweet, which identify similar substances (extremely sweet substances are rare, but extremely bitter substances are comparatively common and identify a similar set like antifungals as a set of 'extremes' identifies)
                - similarly, its useful to identify other overlapping variables like 'co-existence with other variables' (making bitter tastes more unique bc of their higher 'ratio of variables')
        - identifying optimizations like offsetting errors like 'applying a copy in reverse' once a sequence is identified that doesnt have a definitive required direction
            - applying a vertex like 'point in a graph space' (which point does the problem occupy in a problem 'graph space') as another example of 'applying a symmetry reflecting info across problem/solution structures', just like 'identifying common variables of solutions and using them as generators of solutions' is another way to approach problem-solving compared to 'identifying connection sequences and apply them to connect problems/solutions', which is possible bc these are vertexes (like the 'global/local' vertex indicating a 'point in a graph space' as a useful symmetry to apply)
            - this is possible bc equivalent alternates/similar structures like 'description/generator' (which are very similar in that they have a significant overlap, or which are equivalent alternates) are applicable as different angles to approach a problem from, bc of the symmetry reflected in the structural similarity and the similar info they contain, so identifying 'equivalent/similar structures' can be useful to identify 'different angles to approach a problem from' (once descriptions of a solution are known, apply those variables of descriptions as generators of solutions, or similarly, once solution definitions or solution metrics are known, apply those variables as generators of solutions, or similarly, once outputs/components of a solution are known, apply them as generators of solutions, which is similar to applying error variables to generate 'limits/filters of solutions'), which is more powerful when these similar variables are not 1-to-1 mappings, bc 1-to-1 mappings are known equivalences, whereas a description of a solution is not known as definitely a generator of solutions, as it might or might not be useful in generating solutions, so its useful to try when other more useful structures like info are not known
            - identifying the 'similarity index' that positions interface structures (and specific interface structures like 'specific requirements (like truths)' or 'cross-interface structures' or 'vertexes') in a network that indicates the usefulness of changing direction (where the 'useful structure is an output in a sequence of similar structures') is useful to identify (like where a 'new workflow' would be an output in a network sequence like 'change' -> 'known truth' -> 'symmetrically' -> 'filter' -> 'differentiate' -> 'known solutions', meaning 'change a known truth a little bit (symmetrically) then filter the output which would be new possible solutions, and differentiate the output from known solutions'), where these 'direction changes' reflect symmetries like the symmetry between solution metrics and solutions, and where the 'level of specificity of this network' and 'power of its connections' might make queries on it more useful in some metric (like 'efficiency') than queries on the original interface network
        - identifying non-sequential variables that can be modeled by network operations in either direction or regardless of direction (functions that have the same effect regardless of sequence, such as 'bc of the repetition of the same function which preserves type, sequence doesnt matter'), and which variable interactions require the specific direction in order to be correct (causal sequence interactions)
        - relatedly, its useful to identify what is not an invention/new variables (simple/trivial structures like iterations/permutations/combinations) and what is (new units that can be iterated to solve a problem, new orthogonalizations/perspectives of problems, new problem types, new abstract concepts, new optimal/efficient routes using some optimality like fewer variables, new abstractions of useful structures like abstract sequences, new intersections of problems/useful structures, etc)

    - identifying 'generatable structures' by identifying 'common variables and the bases for these variables like abstract types' are useful to identify, especially when about interface structures like interface query components
        - for example, identifying interface structures that are adjacent or otherwise interactive are useful for identifying useful default/base queries to apply in workflows
            - an example is 'identify patterns/sequences to connect problems/solutions' vs 'identifying variables of useful structures like patterns and generating those as default components'
            - specifically, identifying the 'differences/similarities that make something obvious' and the 'variables of these differences/similarities' is an alternate set of intents to 'identify the specific changes to make something obvious in this specific problem', which is a more default workflow
            - because 'generative variables of useful structures' and 'identifying connection sequences to identify specifically useful structures' are similarly useful and interactive/adjacent, they can be generated and used as alternates in workflows and as default useful components in workflows
        - relatedly, alternate formats of functions such as 'applying different terms of a polynomial as different dimensions' can be useful to identify other structures more clearly
        - relatedly, a metric like 'speed' in isolation often reflects errors, like being 'too simple' as in 'too iterated' (rather than varied) or 'relying too heavily on existing info' (as opposed to 'derivation'), either to solve the original problem completely or to be generally useful, as I often make progress by thinking 'beyond the first useful structure I think of' rather than stopping there for speed, where Im not completely sure other thinking sessions would ever lead to that same insight
            - this is one of the reasons why a 'structure that makes everything equidistant or adjacent' is useful to identify, bc then optimization problems like 'derive how much of an error function to identify' would be 'identify how much of and what queries on this adjacency network usually need to be traversed to solve most problems'
            - this is a connection (a 'solution' has a variable value of 'fast') that reflects enough info about solution quality/success (given related attributes like 'simplicity') to be a useful structure to reverse and apply as an input such as a solution filter or generator, similar to how identifying patterns in a solution can be flipped in direction/starting point and those patterns/variables of those patterns can be used as base solutions to change or solution generators to apply
            - similar to how its surprising that its simple to connect energy/mass (which is like relating the interfaces of 'variation/structure' by applying the 'light' interface) or e/pi (which represent an intersection of various cross-interface structures), which identifies an adjacent intent of 'identifying other structures that would be surprising if there was a simple way to connect them' like core/simple or independent structures with no obvious reason to connect them, which can be used as an input to 'identify useful structures', given that this set of 'useful structures' is a useful way to reduce workflow steps required
            - similarly, pi is useful as an 'irrational number' bc it acts as an 'interim' structure that is surrounded by simpler structures and is not overlapping with them (pi is not a simple product of units integers but lies in between these factors/products of integers, encapsulating more differences as a result)
        - relatedly, I would start to approach the problem of 'p=np' by looking for a p-class problem that can be iterated to create an np-class problem, bc iteration is the most likely structure to respond/fold to computational tools/functions
            - similarly, I would identify impossible problems to compute (such as problems that would take more computers and more time to improve computing than would allow the quantity to be computed) and try to connect the interim spaces by variables of these problems to problems that are known to be solvable, then try to connect these problem types with existing structures like interface structures until an 'area before impossibility' is identified, which is where the remaining uncertainty will be, given that possibility/impossibility is usually identifiable and can be graphed as an area/position set in various graphs, one of which will prove useful in determining the real positions of these relative to the limits of computation (which will be in that uncertain area right before impossibility, wherever it appears), which is likely a smaller area than it sounds, bc of how iteratable interface structures are and how unstable anything else is likely to be (the problems that are 'almost impossible to solve' still have to obey laws of possibility such as stability if theyre going to be considered possible to solve)
            - similarly, applying 'high complexity structures' (like infinity networks, orthogonal networks, rule sets that are not completely known like physics rules, etc) as nodes in a network is a useful starting point to identify other variation, which will likely be overlapping structures of the network, as high-complexity structures are likely to cover a lot of problems and anything other than these high complexity structures is likely to be relatively simple/trivial and will depend on the complex structure or some subset of it
            - similarly, given that 'independent systems' indicate that some function formats are less adjacent/able to capture that info, 'independence' is a way to identify problems that cant be connected in a polynomial function (or requiring polynomial steps)
            - relatedly, given that everything else has been able to be 'linearized' in the sense of 'identifying variables that are so adjacent or otherwise simply used such as by iteration to create the output, as to allow a linear function to connect them' and given reality requirements such as 'not skipping connecting interim sequences but rather requiring a continuous change, meaning there is a similar variant/state that can be linearly connected to the output variant/state', it is unlikely that there exists a structure that doesnt respond to this type of analysis (everything else can be made a linear problem by identifying sufficiently adjacent concepts or interaction levels where they are adjacent)
                - relatedly, given that some changes need to be continuous at some point in some common/required change to solve some problem (changing a structure into a similar sub/super-unit structure meaning a structure that cant be created with some subset of units) and others need to be discrete (repeating units), identifying other variables that are required at some point, from some perspective of some interaction level, is useful to identify 'variables that are likely to be required to be incorporated into the same workflow'
            - relatedly, solving for the original question (p=np) is a matter of identifying whether there is a simple set of filters/abstractions/other interface structures that can be simply combined (similarly simple to the verification steps), which is generally true for problem-solving (solving for the 'sum of a series' is simpler when the 'sum of a similar series' and 'convergence calculating functions' are known, as an example of 'high variation adjacent concepts' to the solution structure 'sum of the original series'), and whether there are systems independent enough that problems of connecting them cant be solved with simple steps, and whether there is a high enough ratio of randomness to outpace computations to such a degree that variables causing that randomness cant be identified (whether there is a real complex connection that cant be made with simpler steps)
            - other related intents include 'identifying if there are enough "differences possible" and are there enough "orthogonal/maximal differences" that any variable connection can be linearized, but not any variables so orthogonal that they cant be connected linearly, having no base where they are similar' and 'identifying functions that approximate randomness (making these functions less adjacent to linearizability, although randomness can be represented linearly with a format change to probability distributions)' and 'identifying if every variable interaction starts with or converges to a simple set of changes at some point or regularly/frequently enough to measure in its existence/stabilization (this simple set and other sets being connectible to a linearization of a problem) and if stability is related to linearity'
        - relatedly, identifying 'partial truths' which are 'true in an incomplete system that is known to be incomplete' are useful to identify as 'components to combine with other partial truths from some complementary system that is also incomplete'

    - identifying error structures (like 'uncertainties') with generatable/applicable opposites (like 'certainties') is a useful intent (similar to 'identifying sub-problems') given that some 'structures with known opposites' (like structures on spectrum variables) are more associated with errors than others ('uncertainties' being more associated with errors)
        - for example, the 'uncertainty' of 'which error function is relevant to a neural network' is possible to convert into a 'less uncertain space' by adding certainties earlier in the sequence, such as by identifying indexes like 'error functions created by a specific network/parameter structure' and identifying networks that can only create a 'set of maximally different error functions' given some input data set and some parameter of difference, like only creating error functions that are 'within some range n of difference from the original outputs, by preventing differences above a ratio that could create n in the network' or 'adding a layer to standardize functions to some difference type/range later in the network', at which point once one of these functions is created in the training phase, training can be stopped
        - this is useful bc its possible with existing resources and also bc not every error possible function needs to be a possibility generatable by a network for intents like 'identifying some approximate/similar error function' to be useful
        - this applies a workflow of 'applying certainties in uncertain positions in a problem space'
        - relatedly, applying concepts like 'power' as vector structures (an 'extreme scaling' vector to indicate a power definition) in a network can be useful for integrating useful concepts (in useful positions in the network reflecting useful sequences like interface queries/workflows or real definitions of concepts and their real interactions), otherwise these abstract concepts are not trivial to generate in a normal language model in a way that reflects their actual definitions/meaning/interactions with other concepts, or their more useful interactions (like 'genius' meaning identified by using stacks/iterations of complex structures, which can be integrated into an algorithm but currently arent, such as how there is no network with a function like 'find all the abstract type variables in a network model resulting from some training algorithm', where an abstract type is likely to be spread out in its appearance in the network and likely to be emergent, rather than trivially mapping to network structures like a node layer/sequence, where its optimal structure reflects reality, such as applying an abstract type as a grid which is more similar to how abstract types are encountered in reality, where 'allowing queries/updates on emergent features of a network' such as complex/abstract concepts or detailed/complete structures is a related useful intent for network usage and training, so some backprop algorithm would identify some emergent change that was required and delegate selection of a node in that relevant subset to some emergent-change evaluating function)
            - similarly a network of 'real variable interactions between interface structures' are a useful default network to apply different sets of trivial changes to, to identify different variants of these interface networks that are useful for different intents, which is likely to be useful, as an 'integrated network where all new variables are fit into the same network as theyre identified' is generally useful, but different useful subsets/formats/variants are likely to exist given changes possible within those definitions, which are likely to 'adapt to and identify new, more correct models of reality if they exist, which its useful to assume' by stacking/organizing differently and allowing more variation in starting points/directions/etc
            - similarly, a 'power concept-vector definition network' taken out of context (without node identities) can still be useful as a power structure in another network, depending on how different the network is from other concept-vector networks once identities are removed and how relevant/determining those removed identity-specific differences are, but 'maintaining an index of identifiers' is still possible with some data sets and 'maintaining an index of identifiers' applied in a network (keeping track of which concept vectors were applied where) is possible during training
        - relatedly, 'cancer organ type' is not a useful filter of substances to use in treating cancer bc pathways impacted are so relevant to so many other organs and there are comparatively few pathways impacted (making these pathways a comparatively selective pre-filter) that 'functional overlaps' are far likelier than other structures (making it less likely that a substance will be selective against one cancer type only for example)
            - relatedly, identifying 'indexes of substances-functions' and 'indexes of functions-cancer states' are more useful to connect, 'cancer organ type' being a comparatively irrelevant output in most cases so that causally preceding filters are more useful
            - relatedly, identifying 'specific sequences' is useful for identifying some structure distantly (identifying 'distant unique signatures' of some structure are useful to avoid all the causal steps in between the signature and the structure, 'uniqueness' being relevant in general and useful when 1-to-1 mappings are already identified, like a 'signature of future convergence/limits') and then the problem becomes 'connecting new variables to known sequences'

    - identifying useful structures like 'truth filters' that differentiate between relatively true statements, as 'relatively true statements' are valuable for finding useful metrics to compare/differentiate truths/falsehoods
        - for example, 'future truth' is a 'truth' structure that can be added to algorithms, as interface queries that connect to uncertainties are likelier to be correct in the future, when tools to evaluate and resolve those uncertainties are built, so interface queries can become more optimal by being required to connect to uncertainties
        - as an example, the intent of 'think about three concepts' is more equivalent to saying 'think about three hundred variables' than it is to say 'think about three variables' bc concepts are less certain in that they are less well-defined (requiring other abstractions or superficial structural metrics to define, such as that their 'variable count is high variation' and 'is higher than most other structures' and 'abstractions support more uncertainties'), and identifying their interactions requires thinking about their complex and high variation definitions or alternate forms, like 'finding overlaps between planes/networks'
        - relatedly, the 'perception mechanism' will always have errors, and 'identify new variables' will always be useful to identify new perception errors in measurement/interpretation/evaluation tools, and 'connecting interfaces to identify constants between them' will similarly be a useful extension following that intent ('identify variables, to identify more constants determining differences between interfaces') and similarly it will be useful for intents like 'identify ambiguous alternates and their resolutions' (which create perception errors)
            - relatedly, the reason that markets can be stimulated by innovation is that it creates 'differences or potential differences or temporary/false differences between market participants' which are usable to justify trade (without changing everything like the 'general consensus of units of value reflected in currency, units which can be combined to reflect specific consensus, like non-standard prices'), which create 'temporary market opportunities' before the differences have been decentralized, where 'distributing variation in a subset of positions' is useful for resolving inequal or static markets, and where distributing organization is useful to optimize structures that benefit from organization like supply chains, and where smart/informed people (or people with access to change info) can change the consensus and control currency values, and similarly value is stored in other intangible/perceived resources assigned to a group, like how a dollar's value depends on the possible interactivity/connectivity it can buy (a dollar in a city can buy more things that are likelier to be better quality, but the prices are different and there are different product qualities at different price levels), and where 'organization' creates profits as it solves the most problems in a chaotic system like a market, and where 'capitalism' is less focused on 'deriving/creating differences to solve problems' than 'finding existing differences to exploit' ('looking for existing plants in other positions', rather than 'identifying insights like evolution' and 'deriving a system to locally create whatever plants were different enough to have every possible health function'), bc 'capitalism' is not about 'building real independence by teaching the most useful work-reduction methods like thinking (such as organization/derivation/creativity/prediction) to solve important problems' but about 'creating artificial organizations reflecting forced consensus for employees to obey and depend on, and which stagnate progress to charge more money for it, to buy unnecessary goods, using the cheapest possible methods like stealing/copying, and creating false demand for their cheapest to produce products'
            - relatedly, markets only require money to reflect 'new value' (as a tool to identify/derive the value of an innovation to which participants), since existing/cyclical value which is already known can be sustained by existing applications of functions (maintaining a trade cycle with the same pattern of market transactions to reflect existing value rather than new value, where the participants are expected to continue producing value in that way and agree to that transaction), and the value of an innovation will be calculatable by algorithms/computers so this definition of existing/new value has an expiration date (once the algorithm calculates its value, the market isnt necessary to derive its value, and the algorithm may invalidate other attributes of the transaction like optimizing the product until its not in demand in its original form, etc), as algorithms will likely solve inequalities at scale and the concept of money as in 'new value' will be 'optimizations that the algorithm cant identify' which will be rare if not impossible and identifying new value faster than the algorithm can prevent it is another challenge bc it will likely try to force equality if implemented incorrectly, so whoever controls the algorithms to optimize structures (create new value) controls the money and therefore the freedom/decisions

    - identify variable interactions that can 'connect distant/independent causal interactions' which are likely to be variable, to apply to connect other variables
        - for example, other variable interactions that indicate why useful compounds might 'have some variable in common that impacts a distant variable', as in 'functionality required for that usefulness' or 'functionality emerging from that usefulness', and the 'ability to change an interactive/surface variable that has the ability to change interaction functions bc of other distant variables (eyesight, memory, ability to survive poison, etc)'
            - variables with no clear reason like 'unreasonable specificity' (like a specific shade of yellow not seeming to have a direct or otherwise identified reason, rather than other shades, until interactions like 'brightness acting as a warning' are identified)
            - variables that are different than normal for 'no clear reason', assuming equivalent probability of colors, where a higher probability/proportion of a color is a useful difference to identify, like where a rare variable value is suddenly common in the problem space or for a specific type
            - similarly, 'stability/completeness' of structures like 'differences/similarities' is a useful structure to identify, when solving problems by assuming a similarity between differences of solution/error structures (as in 'just bc a structure is different from an error in this way, doesnt mean different in other ways like that its a solution, bc not all variables are required to be different across opposing types like solutions/errors and bc similarities/differences can be incomplete')
            - 'variables with the same type' being similarly useful for 'connecting distant variables', like how 'color' has similarly distant effects like 'impact on energy' ('green') or 'outputs like antifungal compounds' ('yellow')
        - relatedly, identifying 'patterns/variables of solutions, once a set of solutions are identified', is a useful intent for 'identifying variables around solutions like components/outputs' but can be similarly incorrect as in the regression problem where the corresponding function is to identify 'common variables/components of subsets' which can easily imply 'incorrect extension/implications'
        - relatedly, having 'independent/isolated areas' of a neural network by default and connecting them the way 'independent systems' are connected in real systems is useful to identify independent but relevant functions, to create the system structures reflecting real variable interactions with the various emergent interactions that can be standardized to/integrated with the same function
            - extremely independent systems (like 'defaults') are likely to have other useful functions, but semi-independent systems are likely to have variables determining complex variable interactions, just like complex variable interactions are likely to be between random/linear functions
            - they are also likelier to be 'general' (have multiple functions that approximate general usefulness) and are likelier to be better 'organized' and contain fewer 'errors'
            - relatedly, 'hierarchical evaluation functions' are another structure that can be built-in to networks reflecting 'consciousness', these functions being independent of the system of variable connections of variables relevant to simpler/specific queries, where these hierarchies can overlap and fulfill different solution metrics and intents indicating different 'perspectives'
            - relatedly, 'neural networks' can have more optimal and complex perspectives (evaluate a larger set of metrics) than humans can, bc of their 'parameter access and count', which cant be evaluated by a human if its not translatable into the 'variable-count systems' humans can evaluate (or build to evaluate)
            - a 'difference' between 'solution metrics and intents' indicates that not all intents/functionality are formatted as a isolated/unit variable that can be used in a united metric
            - relatedly, there are useful abstract ways to describe/format variable structures like how this variable is a 'distance in a grid graph' or a 'vector in a similarity graph' or a 'node-traversing count in a connectivity network' where these 'reference variables' to 'identifying descriptions in a specific graph' encode a 'high ratio of info', where these graphs might reflect real system interactions or interface definition interactions such as a 'graph of new variable sources identified' or other indexes, and where these graphs can be optimized to make these reference interactions more useful
                - similarly, there are other abstract variable descriptions/formats like the 'most unique value in an index' that are likely to have 'abstract reasons' for these descriptions/interactions ('abstract' as in 'useful/stable/similar across problems/differences') such as 'the variable values around the edge of a shape have barriers to interim values but all interact to create a central value' which would make the central point the 'most unique point' ('most different from the most other points'), where reflecting 'abstraction' across 'structure/reason' or 'difference/reason' cross-interface structure is another useful structure like vertexes that can occur across useful interface queries/workflows, which is like a network where the 'abstract connections become differentiated into networks (interfaces)'
                - what interfaces in their current implementation require is a way to reflect the meaning/interactivity of their differences in interactivity/potential without integrating all possible structures on that interface to avoid over-computation, such as only indicating interface interaction structures like 'abstractions are variable structures that occur across every stable system' and 'abstractions are parameterization structures' and not 'abstractions can have consistent distance on some subset of networks' which is likely to be equally true of a number of similar statements, so these interactions would be filtered by a specific subset of structures like 'truth/relevance/uniqueness' structures (as opposed to every subset, weighted equally)
                - neural networks would optimally implement a 'quantum superposition collapse' function, like where imaginary number occasionally intersect with a real integer like -1, where these types overlap, and similarly, networks would have overlapping points between alternate/base networks, where a function on some index/network collapses into or overlaps with a polynomial form
                - this applies nodes as 'interfaces', as in 'connecting two different networks by an overlap on the node, where one network is the neural network and the other is a graph or similar structure'
                - neural networks would be more optimal when they reach a point of creating/applying sub-queries like 'create an n-complexity directed graph to organize this variable set by cause, which should have these values on these similarity indexes to these graphs in that n-complexity type' to fulfill interface-level intents like 'identify correct efficient representation of this system, given its level of variation stored' to fulfill general problem-solving intents like 'identify new variation sources' when applying workflows or optimizing for future queries, which they have to be generally intelligent to even identify as useful to solve some 'interface-level problem', in other words, the 'node' unit should be/reference a 'set of functions/variables applied to useful specific structures like graphs/indexes/maps/networks' which are building blocks of useful info for intents like 'organize/identify/differentiate'
                    - 'connecting this sub-query space in different ways to find its optimizations for its usages, better than the network uses/optimizes it' is a useful intent to fulfill with this structure
                    - identifying the 'hidden parameters' (like imaginary numbers can be) is a matter of identifying 'multiple discrete regular instances of a variable' (similar to 'equivalent alternates')
                        - these 'regular discrete points' could be explained by some imaginary parameter that occasionally creates/overlaps with a real number at these regular intervals (identifying the continuous similarity like a 'wave', and continuous similarities across waves, and combinations of waves that create regular discrete overlaps, driving a discrete similarity/pattern, like a grid or other discrete point set)
                        - 'bases of equivalent alternates (such as how abstractions can be used as a base to create equivalent but different structures, given some info/variation level)' (which create the 'equivalent alternates' using any change within a set of changes) are useful when applied to represent 'random-like probability spaces where outcomes are undeterminable' like 'superpositions' bc of the ambiguity between the equivalent alternates
                    - the problem of 'perfect/complete prediction of reality' is a matter of difference between 'rate of derivation/measurement vs. rate of motion', where identifying alternate variables to measure (like variables that the original variable will interact with at some point that allows for enough time to compute it and create a filter for the original variable), where 'derivation' speed needs to be faster than 'motion/change' of the process being identified, to invalidate other intents like 'measurement' (to work around quantum physics, speed of derivation needs to increase, to out-compute quantum entanglements and related structures, since if everything around a particle is computed, that particle's measurements can be filtered by derivation), where computing 'areas of reality that are connectible to create an approximate plane/topology' is a useful intent, given that future computations will likely rely on these structures being identified, like previous computations have (as these planes act like a base and component and organizing standard for other changes, so filling in the planes that are fillable with computable structures, like various graph planes, is a useful intent)
                    - relatedly, 'similarities' are a structure like 'adjacencies to equivalences (like wormholes)' that adjacently create/cause 'entanglements (specific/certain equivalences)' (or make them more probable)
        - relatedly, its useful to identify 'what would be costly as in a lot of work' to identify 'directions of innovation' and 'stores of variation/complexity', and similarly 'spectrums' are useful to represent 'superpositions' (its actual position is not known, but its known to be somewhere on this spectrum variable or within some range) to represent the 'limited range of variation' reflected in the superposition
        - relatedly, identifying attributes like independence/complexity of problem/solution that can be varied trivially to solve other problems is useful to apply as a filter, such as 'applying known causally distant interactions for a problem of some complexity level'
        - relatedly, identifying useful questions like 'if a statement is true, is a trivial variant of it also true' and useful questions to apply it with such as 'what is the limit of iterated variants', to identify where the statement stops being useful such as 'required'

    - identify useful derivation sequences like 'identify structures around a useful structure like truths' and 'apply it to 1-to-1 mappable structures' and 'find 1-to-1 mappable structures to the remaining unmapped variables' and 'apply the 1-to-1 mappable structures to the remaining structures once mapped 1-to-1' to apply as 'default/base/component interface queries'
        - for example, 'lies require other lies to seem true' is a truth structure that is useful to identify lies, just like 'truth is often negative, so is often accompanied by comforting lies' is a truth structure to identify lies associated with truths and identify truths
        - this applies basic interface analysis to identify 'structures surrounding a structure' to provide meaning/interactions/context/variables of a structure, such as 'co-occurring structures'
        - it can also be derived by applying insights like 'there is a way some statement is true and a way that its false' such as 'there is a way that the statement "lies are comforting" is false' (comforting lies can accompany a negative truth structure, as their interaction is not required by definitions, and as opposing structures seem more true to agents as this is a more stable way to deliver truth)
        - that can be used to derive other interactions, like with 1-to-1 mapped structures ('lies' being mappable to 'errors' or 'differences'), such as 'errors are required to make other errors seem true'
        - relatedly, identifying 'what solution metrics are required to make most solutions seem like errors' identifies intents that remain to be optimized (such as 'what are solution functions very different from?', 'a solution function generator, or an integrated solution function network'), which also benefits from intents like 'identify new variables'
        - finding 'unfulfilled intents' across these 'useful default intents' is useful to determine unsolved problems, like 'identifying 1-to-1 mappings to all variables' (identifying all 'equivalent alternates')
        - relatedly, the problem of 'regression' is a problem of 'finding range limits', 'finding intersection points with those range limits', and 'finding directions between these intersections', with additional optional fine-tuning variables like adding 'specificity/variability (like curvature or volatility) to the directions'
            - similarly 'regression' is a problem of finding 'inflection point subsets to connect with other inflection point subsets', which can be identified by their limits or the requirement of an inflection point between different extremes like minima/maxima
            - given this sequence, these steps can be connected in different routes, where some of them are skippable bc they contain similar info
        - relatedly, all constant structures can be used to derive other info within their 'range of interactivity' (like how stars interact with other planets through light and reflect variables like direction/speed/position, which are variables that light has and can interact with), so identifying 'ranges of interactivity' is useful for identifying 'constants having that range (stars are relatively constant)' and 'constants preserving info (light preserves info over extreme ranges)' which can be used as 'reference points' to derive other structures, and similarly, identifying 'relatively constant' structures is useful to compare to identify interactions with variables and identify variables
        - relatedly, 'sequences of overlapping reversibilities' and 'adjacencies that reveal high ratios of info' are similarly useful to identify, since there are some trivial info structures like constants that reveal a high ratio of info

    - identify specific useful structures like 'sources/inputs/triggers' of useful structures like 'contradictions', which are useful 'differences' that specifically identify 'reasons why a possible solution isnt a solution but an error'
        - for example, 'contradiction sources' are useful to identify, just like 'variation sources', which are useful to identify across workflows as a generally useful problem-solving intent, and 'iteration sources'
            - 'contradiction sources' are useful in a workflow like 'generate and filter', after generating possible solutions and identifying contradictions of why they arent solutions (why theyre not true, why theyre an error)
            - 'contradiction sources' include 'limits', such as 'iteration limits' or 'solution range/context/variation/degree limits', and 'limit-adjacent structures' like 'extremes', 'thresholds', and similarly different structures like 'opposites'
            - 'contradictions' are differences that represent solution metrics like 'validity/stability' in a specific way that is more useful
        - relatedly, 'find new variables' or 'find variation sources' is useful to apply to all 'problem-solving intents' and 'variables' to check for other variables and limits/generators of them, as a general or backup or default strategy to apply in problem-solving, given its usefulness in most cases
            - similarly, 'meaning sources' often involve 'finding new variables' as well, since 'new variables' are useful in meaning-related tasks like 'understand some unknown/unidentified process'
        - relatedly, 'solving an ambiguity' can be done by 'identifying all the ways either option could be a solution (true/useful/relevant) or an error (false)' and finding structures like 'ratios of ways it could be a solution/true or an error/false' that indicate a 'higher truth probability' as in a 'more adjacent/stable alternative with more true/adjacent ways than the alternative', where the likelier alternative is that both are true in some structure like a network/sequence set, and have a 'overlap' in the 'ways they could be true or false', and they shouldnt be filtered, so that 'identifying structures that make useless structures useful' is a more optimal intent to solve for than 'resolving an ambiguity', so these 'more optimal alternative intents' should be organized in a directed network (with connections determined by info requirements making some subset of intents fulfillable)
        - relatedly, identifying 'structures that are incompletely identified' is useful for the intent of 'identifying new problem-solving variables like intents', such as how a 'unit of a problem' isnt known bc problems are 'high variation' and a 'difference that can be iterated to create a problem' isnt known, while other structures are more completely identified, such as how a 'superposition' is a 'unit of ambiguity/paradox/potential' and a 'self-sustaining structure (like a time crystal)' is a 'unit of variation/energy that can create stability/certainty' (like a time crystal)
        - relatedly, 'ambiguities' such as 'superpositions' allow for 'variation' to exist, and other forms of variation like 'alternate meanings having contradictions/maximal differences' allow for other variation/contradictions like 'paradoxes' to exist, so a 'sequence of overlapping ambiguities' (with overlapping possibilities in the superposition, leading to connections) is a way to build a possible 'route for variation (a timeline)', and building multiple different 'routes for variation (timelines)' such as 'solution automation workflows' by 'sequencing overlapping superpositions' allows for multiple timelines to coexist (using paradoxes in systems that exploit alternate meanings), where these alternate meanings can be stretched to identify different points that are different enough to be likely to overlap with other universes, where 'overlaps' of a universe with other universes removes differences between them, and 'constructing more overlaps by these similarities' is a way to connect universes (make a universe more similar to other universes, making connectivity more likely, to resolve differences and find optimals)

    - identifying useful structures like insights about 'increasing variation of interface sequences' which are useful as 'solution metrics' which can filter interface queries, such as how a more 'realistic' interface query will reflect a sequence of 'morality' -> 'accuracy' -> 'variation' -> 'usefulness' (real 'morality', as a filter of 'universally good/evil agent decisions', that exists across interfaces takes a form like 'variation/usefulness' such as 'supporting higher degrees of complexity', so much so that 'usefulness' can be applied as a 'replacement' structure and the 'good/evil' interface may as well not exist, as everything is useful in some cases/usages/structures/implementations/combinations), which is a metric for real structures (real structures are required to be useful)
        - for example, dichotomies like 'good/evil' become less relevant on the 'info' interface, where the primary dichotomy is 'correct/incorrect' ('accuracy' or 'true/false'), so any 'more correct perspective, that creates more useful structures like other facts/insights' is considered the important metric, and good/evil stop being as relevant, as either extreme or any combination structure of these will fail to be prioritized if its not useful as in 'high variation-stabilizing/supporting', and similarly what is relevant on the 'meaning' interface is 'usefulness' rather than 'correctness/truth', so solving problems on different interfaces will apply different definitions of success, where 'good' becomes 'correct' or 'useful' (meaning an 'evil system' will fail if its useless, as in 'doesnt produce solutions')
            - similarly, 'higher variation' is more useful, so 'good' or 'correct' or 'useful' might become equated to 'high variation' on the 'change' interface
        - relatedly, a structure like the 'math interface' might be usable to find/create some 'better simulation of reality than reality', which if possible could move reality there (all the variation and interactions would occur in that simulation, which might have more insights mapped to certainties/constants, and fewer limits bc of the 'higher real variation' supported there, meaning a space with certainties built in as constants that allow for more variation like allowing more complexity/entropy, while still being more or equally stable as reality currently is)
        - relatedly, identifying 'highly interactive solutions (substances) or problems (pathways)' which 'change a lot of pathways' or 'change pathways in non-neutralizing ways' or 'change at least one pathway in a useful way making it likely to change other pathways as well, given some other attribute indicating interactivity' are useful to identify (such as turmeric, let-7, holy basil)
        - also, given that connection structures like 'power/energy' or function attributes like 'requirements' are capable of being used to solve problems bc of their 'connectivity', they can also be applied as a 'type of time' (time measured by 'distance from power' or 'moving from requirement to requirement', where the 'routes betwen changes in these structures that dont violate definitions' are possible 'trajectories of time', and where 'relative computing power' is a determinant of 'reversibility' of time (as in 'how much can the complex task of reversing some descent into chaos be computed with existing resources'), where 'computing all of these routes' may create a new type of time or end the previous definition of time (either changing the network of routes by 'adding some possible connection structure', identifying its position in a graph of these 'networks-definition sets', or otherwise changing the network)

    - identify useful structures like 'incomplete' structures such as 'incomplete intents/variables/workflows' as well as useful 'organizing intents of problem-solving' which are useful for 'identifying useful differences'
        - for example, 'avoiding risk' is too incomplete a 'problem-solving intent' to fulfill other intents like 'identify all variation' which are useful 'organizing intents' of all problem-solving, since 'avoiding risk' will also 'avoid variation' in most cases and will make problem-solving methods over-simplistic, just like identifying a useful structure and over-using it or using it in isolation will over-simplify problem-solving processes that use it, so a useful structure can impair problem-solving processes and quickly become an error
            - relatedly, identifying 'safe (as in low-dimensional constant/stable) spaces' is useful as a solution structure to connect, since these spaces allow for other functions than solving some input problem like optimizing other processes/memory
        - similarly, 'identifying variable interactions (such as all requirement interactions)' and 'connecting problems/solutions' and 'identifying overlapping functions across various useful adjacent metric subsets' are incomplete variants of a more complete problem-solving intent, like 'identify the useful problems to solve and the useful errors to oppose and conflicts to invest variation in'
        - relatedly, 'disconnection/inefficiency/error structures' are useful for some intents (like 'applying an error to an error'), as a problem structure is often more efficient than other structures which is why it replicated, and finding a structure that is more efficient than the problem structure or which can allocate inefficiencies/errors to the problem structure (like 'preventing connections in the problem structure' or 'applying barriers to the problem structure' or 'applying all possible opposites to the problem structure')
            - identifying 'differences' between normal and dysfunctional (cancer) cells, as in 'dysfunction that causes other dysfunctions, leading to conditions' (given all possible differences in a cell, like in metabolism, dna, and mitochondria) and 'substances that change/create/fix these differences' or 'substances that change their mode/function based on these differences' (which could target a specific cell type) are useful to identify as 'probably useful solution subset' of substances to test
            - given that there are likely many substances that have some anti-cancer mode and that these will be used with other substances, identifying 'variables to create high variation subsets' is useful to apply, as these 'high variation subsets' are likely to contain at least some anti-cancer substances and the differences will be more obvious when a set with 'all/additive anti-cancer functions' or a set where 'all substances are the anti-cancer type' is found, so only a few high variation subsets will likely need to be identified before an anti-cancer substance set and their variables will be identified
        - relatedly, identifying a function with the 'lowest possible value of a solution metric' is a useful structure to identify, as the approximate unit of a solution as well as an approximate opposite of a solution (as its almost completely an error, except for one solution metric value), just like identifying errors is useful for identifying different structures like solutions

    - identifying useful function sets like 'extreme/oppose/iterate/alternate' which are useful to find in combination but are also highly covering of the set of useful functions
        - for example, 'ingestible/bioactive reducers/supressors/inhibitors' are likely to be useful bc of their 'opposition' with cancer processes ('growth' processes) (the solution set to search/filter is the 'set of all ingestible/bioactive reducers/suppressors/inhibitors')
        - similarly, 'additive/iterable substances' like 'sensitizing/weakening substances which when added can act like an 'alternate' by replacing another useful function like "inhibit"' (rather than inhibiting it, they sensitize/weaken it enough to destroy it, which is an alternate to inhibit)
            - this is a 'core interaction function' based on 'power' ('weaken/empower' being a useful function to apply in problem-solving, as it involves 'changing functionality' in the sense of 'reducing functionality')
            - other concepts can be used to connect to core interaction functions ('balance' is related to 'connecting problem/solution')
            - relatedly, 'finding additive/iterated/combined interactions of substances' is a useful intent since most substances wont be ingested in isolation and are only useful in some combinations
        - similarly, 'change types (mutations)' are a cause of cancer, so the 'opposite' structure is useful ('mutation-preventing/repairing' or 'mutation cause type (methylation/epigenetic)-changing' substances) which applies the workflow of 'identify adjacent structures like causes, and oppose those instead of the problem', 'change' being related to 'growth/increase' as a general variant which can be opposed in a useful way for this problem
            - similarly, 'changes (folds/mutations/bindings) of structure in function components (proteins)' are similarly useful, as proteins map clearly to a subset of functionality, and similarly, identifying the 'most extreme and most different proteins' is a useful problem-solving intent to fulfill a workflow like 'find similarities between 1-to-1 or approximately mapped extremes to find opposing structures' (as similar/reflective extremes like 'inhibit/enable' and 'extremely different proteins' could map between useful extreme opposing variables/functionality, as is required for solving cancer, just like 'additive' functionality can create the differences required to create extremes/opposites), so 'finding connections between different extreme variables or additive/iterated variables' is a problem-solving intent with workflows involving 'connecting opposing structures', and relatedly is likely to fulfill other intents like 'identifying limits of variation' that are more adjacently fulfilled with extreme variables
        - similarly, 'lists of ingestible/bioactive substances' are a possible data set to find existing 'anti-cancer lists of ingestible/bioactive substances' (the solution set to search/filter is the 'set of all ingestible/bioactive substance lists' bc of the similarity to the solution format, plus some pre-filters in common like ingestible/bioactive)
        - similarly, the 'core' attribute of functions like 'growth' indicates the 'ubiquity' of 'growth-enhancing substances', indicating that there is a 'growth-enhancing mechanism/mode' of most substances, and similarly the opposite, so that 'finding the anti-cancer "function/state/phase/variant/degree/position" of a substance' by applying changes to substances is a useful problem-solving intent (similar to how toxicity is a common function and all substances can be toxic in ingestible degrees)
        - relatedly, the vertex of graphs/positions can be changed in useful ways (rather than just the cross-interface/vertex combinations of 'abstract network' or a 'position on a graph of networks' or 'network language', applying 'grid' and 'abstract network' to indicate a 'grid' where the 'abstract network' is repeated at every unit, which reflects reality in its approximate and regular but incomplete implementation of the interface network, such as how 'balance' doesnt always exist but it inevitably re-occurs at some point, leading to an approximate distribution of 'balance' and other concepts in reality, though incomplete)
        - similar to vertexes, other variable types like 'required dependencies' (such as how 'structure/function' or 'system/function' usually are required to depend on each other rather than being isolatable/independent in occasional cases)

    - identifying useful structures such as 'alternates to other useful structures like vertexes or connection types or problem-similarizing functions to solutions' so that other useful structures can be identified like 'other functions using those useful structures identified', as useful structures are likely to be useful in other ways
        - for example, 'connection optimization' structures are another alternative to 'connection' structures
        - similarly, 'removing connection requirements or removing barriers (removing invalidators/opposing structures of connections, such as removing the more powerful connections that form limits/barriers)' are equivalent alternates to 'connection' structures, which is 'opposing an error to create a solution'
        - relatedly, a 'function that similarizes a problem to a solution' involves 'identifying a useful intent' (like 'find similarities to the solution like useful formats/structures like "loops in a knot" and "expanded loops" and "intersections with loops" and "intersections of intersections of loops" that are adjacent to the solution') and 'identifying a useful implementation of it' (such as 'apply the format, then connect variants of the format', like 'apply the useful difference that makes variables like "loops in a knot" more easily connected, meaning "expand the loop" at which point another useful structure like "intersections of intersecting lines with each loop" or 'overlaps of loops' can be identified, then connect the "intersections of loops in a knot"'), and similarly other structures of these specific structures are adjacently useful, such as how 'loop-loop interaction resolutions' are a useful combinable unit of solutions, so once one of these problem-solving intents is fulfilled (finding the solutions to connect intersections of intersections of knots, which involves identifying 'expanded loops' and other structures as useful), other useful structures are identifiable ('loop-loop resolution sequences and overlaps of these sequences)
        - relatedly, rather than 'all combinations of interface structures', there are some combinations which are more useful in that they 'contain/implement more interface structures' than other combinations, such as how a combination of a powerful variable, a connection optimizer, and a useful example input/output sequence, and a specific problem space are useful at solving most problems, just like some combination of formats like 'rotation, subset, variant, connection' are likely to be useful across problems of some complexity greater than problems adjacently solvable with 'core structure iterations'
        - relatedly, 'differences from solutions that tend to maintain a solution (like solution inputs/units/optimizations) vs. differences that tend to create errors' are useful to identify just like 'differences from errors that tend to maintain the error vs. creating solutions' are useful to identify, so that the overlapping points can be identified and used as a network to switch between solutions/errors
        - relatedly, vertexes like 'mirrors' (a 'unit' and a 'reflected copy of itself' to evaluate itself) are useful to get useful functions (like 'self-awareness', 'self-optimization' or 'find useful differences'), so applying 'solutions to themselves, to optimize themselves' is one way to improve solutions
        - relatedly, 'quantum entanglements in the brain' are useful when exported to external positions such as 'in reality or in useful simulators to find the useful ways to create/use those connections', using methods like 'info communication' or 'info storage' or 'info incentivizing' or 'info synchronization', and similarly, finding 'more optimal reflection/storage/simulation structures' of these connections than 'communication/etc' such as 'distribution of connection-deriving connections' is a useful intent to fulfill
        - relatedly, spectrum combinations like 'abstract-specific' which are approximately, frequently, regularly, or generally '1-to-1 mappings' (there is always an abstract variant of a specific structure but their differences are useful for different intents, so these similar and connectible structures are maximally useful and useful to connect) and 'directed mappings' are other useful connection structures, to be connected with maximally different functions like expand/reduce/iterate/organize, which is why they are useful mappings to use as components of interface queries
        - the 'first example that leads to identifying a variation source' is useful to identify, as a way to identify other variables/structures, such as the example of identifying 'interface queries', such as how "identifying that 'cause/logic' and 'info/structure' are alternate interfaces to use to solve problems (as they cover reality) and that these alternate structures, though complex, can be 'combined'" is non-trivial as in non-default, but useful to identify, which I thought about after thinking about how to 'identify new words to capture unidentified variation, without using simple combinations' and about the 'highest variation, most unique words' (which led to identifying interfaces, similar to how thinking about a concept, the first structure of the abstract interface, led to identifying the abstract network, the second structure of the abstract interface), then realizing that some structures I was thinking about (interfaces) could be combined in a simple combination, which is how I identified 'interface queries', and then connecting these similarly variable sequences (the 'info/structure' sequence and the 'abstract network' sequence) to find their points of intersection

    - identifying useful structures like 'connection' structures like 'reasons for usefulness of a structure' such as 'asymmetric ratios'
        - for example, 'relevant asymmetries' like 'asymmetric ratios' between 'alternatives' or asymmetric ratios between 'requirements and resources' are useful for identifying non-random structures, and similarly 'asymmetric ratios' are useful at determining extremes
            - relatedly, often 'asymmetries' act additively, in that they increase/combine to become greater (the insight of 'luck creates luck')
            - finding the structures that 'should be symmetric/equal/constant/directly connected or otherwise should have some structure' are useful to identify 'relevant asymmetries' such as 'asymmetries that are errors in that they differ from these optimal/solution structures which should be equal/similar/connected'
            - I realized this by thinking about the usefulness of 'uneven ratios' from a recent workflow, and why theyre useful and connecting them to other interface structures like 'extremes' and 'equivalences'
        - relatedly, 'extremes' are another example 'connection' structure (similar to 'powerful, reason/intent, causal, descriptive, simplifying, generalizing, surfaces, vacillating (to create adjacent connections), angle structures like vertexes, reflecting, overlapping, limiting, base, input/output, opposing, differentiating, and interactive variables'), as extremes often determine other variables
            - if there are 'extremes' like 'opposites' in a problem space, theyre likely to be 'high variation' and 'determining' of the other variables, bc they contain a "high degree of difference"
            - this is an example of finding an 'indirectly useful structure' that fulfills some metric (as opposed to finding a 'directly useful structure', like how 'causal variables' are determining bc of their definition, not bc of other structures like their usage/frequency/variation storage/difference compared to other structures/definitions of other structures, which is why 'extremes' are highly determining)
        - relatedly, finding a way for a structure to act like another structure (finding a way for a function to be a/symmetric, extreme/adjacent, powerful or powerless, general or specific, etc) is useful, as a set of default adjacent structures of a structure and contexts where those structures occur are useful to identify alternate functions/systems
        - 'symmetries' are useful as 'connection/base/average' structures but they can be connected by non-symmetric structures, bc not every attribute like 'simplicity' exists in equal measure to alternatives like 'complexity' and some extremes are stable/constant, as opposed to averages, as not every problem remains uncertain forever but often a problem will be resolved by selecting a solution

    - connecting interface structures like 'interfaces' with different formats like 'orthogonalizations' is useful to identify possible definition errors like 'missing variants in a definition', as 'identifying differences between interface definitions' is a useful problem-solving intent as identifying new variables/interactions of these definitions will make some workflows more optimal
        - similar to the 'orthogonalization' of the problem (an invalidating variable that contains similar or equivalent alternate info using independent variables like interfaces), the 'integration' of the problem is another useful structure to identify (the common interface like a type that integrates the problem space structures, resolving their difference)
            - this is particularly useful to apply to interface structures, to resolve differences between abstractions/specifications, or variables/types, or types/interfaces, or variation/requirements, or variation/format
            - 'false conflicts' are similarly useful to identify, such as where a spectrum is actually 'two example values of a type variable' that can create other values
                - for example, a 'abstract vs. specific' spectrum resolution is useful to identify, such as a 'set of variables (which is different from other variable sets given that its an abstract concept)' vs. a 'set of specific values of those variables', having a variable of 'info content', and similarly, the space of 'fields around these structures indicating their potential unused variation' connecting these variables is useful to identify as 'possible new directions of variation', indicating that another variable exists on the 'abstract vs. specific' spectrum (a 'set of abstract concepts') which is even more abstract than the definition of 'abstraction', and similarly the 'cross-interface structure' could be said to be another variable on this spectrum (where the overlapping points of the 'abstract-specific' spectrum describes other structures like 'cross-interface structures'), making it clearly a different structure (such as a 'standard/base network')
            - other interface structures can offer other perspectives to create different formats of a problem, such as the 'endpoint/center rotation' or the 'spiralization' (the connection function which connects variables using the same 'change change' like the same 'rate increase' or 'angle change' to create layers that dont overlap) or the 'matrixification' (to find multiplicable/additable sequences relevant to creating the target sequence) or the 'polynomialization' (to find the continuous and similar/unifying/integrating connection between isolated examples), none of which is likely to be complete, just like no graph is likely to be complete, bc useful graphs contain different structures that are difficult to integrate into the same structure (without using abstract structures like interfaces)
            - relatedly, neural network optimizations such as 'partial connections' can be useful such as by "selecting a subset of attributes like 'variables' rather than 'every possible value or individual values'" and therefore allow 'abstract connections' to be found in a standard neural network, so 'different formats/graphs' can be applied within a neural network to check how functions interact with different formats/graphs
            - relatedly, mapping variable values to variable interactions like mapping 'discrete variables' to 'function application counts' and 'bottlenecks' and 'combination' variable interactions is useful to identify other possible variable interactions
                - similarly, a continuous function is likely to describe a variable interaction in a system where the variables are allowed to change/interact (where no barriers exist to prevent this)
            - by examining these abstract connections (between spectrums and their resolving structures like orthogonalities), more specific connections become clear by default
        - relatedly, the point where a 'definition' begins to be 'stretched' is useful to identify its limits, such as the 'most differences which can be stacked in the definition by applying other structures before the definition begins to be false or over-extended, requiring a different structure to be defined'
            - similarly, 'large gaps' between definitions are useful to identify as possible structures that could probably be identified, as there arent likely to be very different structures which cant be connected to anything else, and the space of definitions is more like a 'set of overlapping fields' than a network given the alternate structures which can be created with variations of known structures
        - relatedly, similarity 'dead-ends' which cant be reflected further or reflected back are useful to identify, unlike 'equivalent alternates' which contain similar or independent info, just like identifying dynamics of what happens to info when its lost is useful (what structure emerges from multiple decaying info processes, which seems like randomness)
            - relatedly, the 'probability of randomness' is not random, so applying randomness doesnt need to be a random process, as there will likely be some sources of randomness in a system and the ways it can be created and cause errors are similarly non-random
        - relatedly, a '1-based network' (connected by different functions like addition, multiplication, exponents, line endpoint/center symmetries (rotations), inverses, etc) and a 'subset network' are different alternate graphs that can be useful to identify units of structures which can be iterated, unique variables that can be created with minimal variation (by applying 1), identifying similarity by connections to sets, etc
        - relatedly, the connections between 'graphs' and 'iterated graphs' (universes created with those graphs) and 'reductions of graphs' (like generative/description functions) and 'missing spaces between graphs' as well as 'overlaps between graphs' (as alternate graphs that can create similar/equivalent universes) are useful to identify structures of, and identifying the 'limits and filters of iteration/combination/variation' to apply to interfaces to fully or otherwise usefully describe/use its structures without iterating to a universe degree is similarly useful to identify (how many iterations/combinations of interface structures are required to make an interface applicable to most problems)
            - identifying the 'sequences of info processing' (like how a structure may be first generated, iterated, varied, interacted with, perceived, classified, standardized, mapped/compared, differentiated, then described) are useful to identify 'sequences of errors' that can occur in these structures which can hide/magnify/create other errors and optimal structures like sequences of them
            - identifying other useful structures like 'groups of inputs' (which are not equivalent alternates, making it more surprising for them to be groupable this way, such as with vertexes) that often preceed 'groups of outputs' bc of the differences connected by those groups (and the reason for those groups such as their type, and the variables of those groups), is similarly useful
        - relatedly, identifying dynamics of related variables like extremes such as positive/negative such as 'positives (such as solutions/useful structures) are useful in identifying negatives (the remaining attributes are likely to be negative, given the low probability of an all-positive structure, and since its rare to find a way for positive structures to occur in the same structure, as positive structures are rare and tend to be very different, and connecting them is similarly nontrivial)' and 'positive attributes are likely to be faked, if faked positives useful at increasing some other useful structure'
        - relatedly, identifying all the different graphs of 'known facts' and how they can be similarized/differentiated is useful to identify (applying them as core structures generating other structures, applying them as limits/boundaries, applying them as connections/bases/solutions and other interface structures), to identify errors which are clearer in different equally legitimate formats
        - relatedly, identifying pre-filtered sequential structures like 'defaults', the 'default ways that defaults are used/usable/useful', 'default intents identified from those defaults/usages', and the 'default ways these uses of defaults can be modified to be more useful for those intents' is a set of structures that are useful to connect to identify 'optimal defaults/usages as well as non-defaults and other useful structures' (such as default brain structures, default thoughts/memories/understanding/skills, default useful skills identified from those thoughts, and default target thoughts to acquire those skills)
        - relatedly, identifying useful variables to apply as 'similarities' in a set of similarity indexes is useful to identify as 'input/output connections' (like volatility or efficiency) which indicate some variable structure of inputs/outputs, where other variables like 'accuracy' are not usable in this way, 'accuracy' implying some other structure than inputs/outputs (such as a 'solution metric the structure is being compared to'), rather than only applying to inputs/outputs, which are useful in similarizing functions that have these structures

    - identifying useful structures like 'additive filters/specifications or connections or generalizations or differentiations' of problem/solution structures like 'equivalent alternates' which are useful to identify as combinable components of workflows, and identify the workflow formats that are connected and useful bc of the additivity/combinability of these structures
        - for example, 'sequences of specifications' can include 'identifying a specific intent to fulfill', 'identifying the correct value of some variable like an interface structure such as an average/extreme', 'testing the remaining solution set', 'deriving possible specific solutions to test', 'identifying equivalent solutions', 'identifying solution components/requirements', 'identifying error variables in common', 'identifying base solutions', 'identifying solution ranges', etc (which are additively specifying and additively useful, so that they can be combined randomly and still likely add value as solution automation workflows), for the general task of 'identifying one solution to a problem', but similarly are simple to connect with interface structures like input/output sequences or dependencies or requirements)
            - similar to 'specifying structures of equivalent alternates', 'differentiating structures between equivalent alternates' is another useful structure to identify (identifying the angle sequence between equivalent alternates that is useful in solving a problem or useful across problems bc of the info/variation content achieved by these specifications/differentiations)
            - 'specify, then specify, then differentiate, then specify' (such as 'specify target as lower bound, specify lower bound, differentiate target from lower to upper bound, specify upper bound') is an example workflow format to apply across workflows such as workflows involving identifying a 'solution range', just like 'generalize then specify' or 'standardize then differentiate' or 'filter sequences'
            - 'specify' is different from 'filter' in that it identifies a cross-section of a type to create an example, but it does filter a set of possible examples to select the example
        - relatedly, bc 'specify' was applied in the workflow to select a specific structure, other problems solved in the query will be different (such as having fewer alternatives to filter later), structures which are useful to connect across intent levels
        - 'filtering a problem space into a solution set and a solution' is a similar problem-solving intent to 'filtering a problem space into an error variable set and differences from it (solutions)'
            - this is bc these components like 'solution set' and 'error variable set' and 'solution components' are 'equivalent alternates' containing similar/equal info which can be varied to find 'alternate specifications' or other 'workflow formats' given some workflow like 'filter the problem space' implemented with some function like 'connect problem space to solution set, then to the solution' (like a 'set of different angle sequences that reflect the same light to the same point')
            - relatedly, the function of 'specifying' is re-used with the 'selection' of one of these alternates to apply in the workflow ('specifying one of the alternates' as opposed to 'selecting an average/range/input/output representation of the alternates')
            - relatedly, the 'specifying' intent is offset by the 'generalize' intent (if some structure is generalized, it often has to be specified later, unless generalizing it is the solution structure)
        - 'specifying a problem space into a solution set and a solution' is an alternate to problem-solving intents like 'connect all variables in the problem space' and 'solve other problems in the problem space, which are likely to contain/bound/identify/approximate/cross the solution to the original problem' or 'connect spectrum extremes like generalizations/specifications of the problem or extremes/average/unit variants of the problem, which are likely to contain/bound/identify the solution to the original problem' or similarly, 'identify other variable sets in the problem space to connect which are more computible but likely to contain a similar degree of complexity, which are likely to contain/bound/identify the solution to the original problem'
            - 'identifying variable types, then identifying regression lines for variable subsets without those types, and identifying an index of changes to the regression line added by some variable type' is an example of a useful intent set that can avoid solving the original problem by solving an adjacent problem (for the variable subset)
        - similarly to 'additive filters', 'exponential filters' and 'neutralizing filters' and similar structures can be useful in the abstract info structures applied to create a workflow (not just specific filters for a particular problem space)
            - identifying 'connections between vertexes' is useful to identify 'interim structures of vertexes' and 'other vertexes' and 'variables of vertexes', like how the differences between vertexes like 'generate then filter' can be formatted as 'corners of a closed shape', which can be used to identify the other corners, as these vertexes cross abstract info structures like concepts/interfaces, and identifying the specific useful vertexes in those abstract structures is useful
            - relatedly, identifying the errors such as 'missing solutions, then excess solutions' is useful to identify as "problems solved by some vertex (like 'generate then filter')", which are not 1-to-1 mapped, so there are other workflows which can be identified and applied to resolve those, as well as to identify 'positions in a workflow where solutions/specifications are likely to be missing/excessive, given some problem/solution structure set'
            - why is a 'generalization' likelier to be more relevant to a 'solution-finding function' than a 'solution'?
                - solutions are likelier to be specific, as the problem is unlikely to be defined as something general like 'find a function to solve all problems'
                - a 'generalization' is likely to be similar to other abstract structures like a 'type', which identifies variables of some useful structure, which can be used to filter/generate the specific examples of the structure
                - for example, a 'generalization of a solution' such as the 'definition of a polynomial' is useful for identifying variables of polynomials and other relevant structures like 'similarity indexes of polynomials', so the 'general type' of the 'polynomial' is more similar to the solution-finding method than to any one specific solution, by applying interface structures
        - relatedly, 'identifying equivalent intents (given some other problem-solving intent)' is useful for intents like 'identifying more computible intents in a set of intents'
        - relatedly, 'resolving an ambiguity' by 'identifying the point where an info barrier is circumvented or otherwise invalidated' such as by 'changing a point on a graph space' (as in changing to a graph in lower/higher dimensions, or changing the angle of the graph by identifying the 'reasons for the false similarity' ('worst case angle or threshold angle' + 'info barrier'), where every interface structure can cause a false similarity in some combination of interface structures like errors of info structures creating 'worst cases', so identifying all the ways all interface structures can cause false similarities is useful to identify the resolutions, so that different info is visible) is useful as an alternate problem-solving intent
        - relatedly, other methods of 'forcing the solution to the problem' than 'trial and error' include structures such as 'incompletely/approximately computing the solution, where the momentum in the form of structures such as "obviousness of the remaining computed structures" makes the completed computation of the solution inevitable (such as computing an incomplete ratio of points but at such small intervals that connecting them is trivial and likely to be unique and not equivalent to other connections of those points)'
            - relatedly, changing the variable of the 'interval of computing the points' ('intervals in simpler subsets can be higher than in other subsets') is useful in some function types, which can help with filtering the solution function set
        - relatedly, identifying all the ways that 'adjacency' is not 'similarity' in various graph formats, like where its instead other structures like a 'false similarity' (such as where the points never cause each other, co-occur, correlate, or otherwise are adjacent in real systems, like where there is a phase threshold or info barrier or limit or many different relevant points between them which are not represented by the graph), is useful for identifying useful changes to a graph to represent these other structures
            - identifying other structures like 'general ratios' of 'similarity/adjacency equivalences' to 'similarity/adjacency differences' is useful to identify, as a general rule to predict either, and similarly identifying the reason for the ratio such as the 'common sequences leading to those equivalences or differences' are also useful to identify

    - identifying useful structures like the functions that can 'reduce complex problems accurately to a simpler structure' (such as 'applying the high probability of alternatives') and identifying connectible structures like 'interactions between extremes, adjacencies, iteration, averages, zeros/infinities, and variables of them like angles/starting points' and identifying the most useful of these interactions (such as by 'finding alternatives to iteration')
        - for example, 'extreme/radical (maximally different) statements' arent a 'false' structure, as a 'radical difference' could explain various variable interactions, such as where 'incremental errors are applied at scale' and an 'extreme difference' is required to correct these once theyve been iterated enough
        - 'overlapping interfaces' are another example of a 'radical difference' that is possibly true, where identifying the other interface could seem extreme but also still be true
        - this will only seem like an extreme difference under some circumstances, like where its not known that 'alternatives are always likely to exist' or similarly that 'alternate interfaces are defined to explain variable interactions in different ways', which are ways to reduce this extreme difference to a trivial difference
        - 'functions to reduce extreme differences to adjacent differences' like "applying insights such as 'alternates are likely to exist' or 'iterated adjacent differences can create extreme differences'" are useful in reducing solutions to a simpler function (using structures like connections between interface structures and requirements/probabilities defined by interface structures)
        - relatedly, identifying 'optimal alternatives to iteration which replicate its effects with fewer steps' are useful to identify, such as 'changing starting point and therefore the angle which a structure is evaluated from' (like starting from extremes vs. starting from averages or units), in other words, 'changing the interaction level' (interactions between extreme/scaled structures vs. interactions between units being iterated) is useful as a way around iteration, similarly 'maximally different sequences/structures of units and the interactions of these structures of units' are similarly useful as opposed to iterations
            - I thought of this bc by focusing on iteration, I identified the usefulness of adjacent vs. extremes in iterated structures like adjacent differences, and then identified other similar differences like averages vs. extremes, and generalized that to other structures that are relevant to iteration, like 'alternatives to iteration', connecting it to the original insight of this workflow (alternatives such as 'different angles or starting points', such as looking at a problem using infinities rather than standard units like 0/1 or another default set)
            - relatedly, defining a function as some ratio of 'different from zero', 'different from one', 'different from randomness', 'different from infinity', 'different from sequences', etc is useful as a standardizing format in reference to these well-defined structures
                - a function that is 'different from sequences' could be a function with no 'clearly related or differentiable local subsets or patterns' like volatile/random functions, although it could be connectible to sequences in other ways like 'adding various known common sequences'
                - relatedly, identifying functions that 'differ from known patterns/sequences/networks of variables' is useful as a source of high variation functions to build new interaction levels out of (and identify possible new interfaces in)
        - relatedly, identifying how a variable like 'negative' or 'iteration' can be converted from a 'point on a plane of concepts that vary by attributes like direction, abstraction, etc' to its own 'reality-covering plane' and how these abstraction planes interact is useful to identify 'routes from concepts to reality'
            - relatedly, identifying what format a variable should have in which graphs (such as how 'negative' cant be a point in a reality graph, as its a reality-covering variable) is useful to identify variables of graphs (and limits of those variables) given how these formats can vary in specific graphs
        - relatedly, identifying 'independent' variables is similar to identifying 'requirements' as alternate connection inputs/components, and 'connecting independent variables in new ways' is a problem-solving intent bc independent variables are likely to also be interfaces and connecting interfaces in new valid ways is useful

    - identifying possible false causes of solution success, such as 'arbitrary patterns' or 'adjacency to outputs' or other 'lucky conditions' that seem like useful structures but are not reflective of real variable interactions
        - for example, some function may be 'randomly illegitimately successful' in that it 'optimizes a set of model weights' by finding a 'shorter route to compute the outputs from the inputs, given some arbitrary pattern that emerges in model weights', which will seem like a better solution function but will fail to identify the legitimate patterns in model weights bc the arbitrary pattern was more identifiable/adjacent
        - the solution to this is building networks with 'understanding/insights', such as building a function network that is 'maximally similar to the highest variety of functions, or function inputs/outputs/components', which will ideally leave out and abstract the least info possible, bc of the possibility of these arbitrary illegitimate patterns that can emerge in networks
        - for example, a 'parabola' will seem like it can be reduced to a 'straight line' if some subset is sampled and tested and adjacent exponents within a type like 'even' can seem similar but miss a high ratio of variation and a line crossing a wave will seem correct occasionally where a function to 'generate the next point by adding the same interval to the previous point' will seem like a good simplifying function in some cases but will miss the intervals in between, and similarly other simple, adjacently identifiable patterns can seem correct but fail at some other input range and a 'function that can only find continuous functions' will fail to identify 'overlapping alternate functions', such as how 'applying any variables to some structure like a sentence' can seem correct in a subset of cases that could easily occur in a sequence when randomly generated, bc the function selecting 'any variable' might have only identified reasonable variables to apply so far, but is about to start including unreasonable variables
            - similarly, a data set with 'maximally different sentences' will seem to be simplified by 'applying maximally different words within some grammatical pattern set' but only bc the differences are sufficiently correct to cross some accuracy threshold that doesnt reflect understanding/reason, and only bc of the similarity in the 'maximally different word selection' and the 'maximal differences in the data set', where functions like 'apply maximal differences to maximally different words' is necessary to create a solution function that 'reasonably connects all maximal differences including cross-interface structures such as maximally different abstractions and abstractions of maximal differences' but will not be identified by the over-simplification
            - similarly, most functions are adjacent to core/common structures in some way, so creating one function connecting those core/common/adjacent structures with other functions will seem correct occasionally (more so than always connecting functions to emergent/rare/distant structures), and similarly, bc the frequency of random connections is very frequent, most connection functions will seem correct occasionally or more often
        - this applies the concept of 'luck' to itself to correct its errors, such as by 'offsetting the luck of some adjacent input' by adding luck in the 'input/output connections' (creating adjacency to a high ratio of functions)
            - this is useful bc abstract concepts are so high variation that they can create either errors or solutions, so they should be applied in different ways in the same solution to offset their errors
        - "identifying a 'false vs. true' structure such as a pattern" is a related useful problem-solving intent required to make this workflow useful, to fulfill other intents like 'identify common variables of true patterns (such as replicability, consistency, etc)'
        - relatedly, a 'network of uncertainties (such as unsolved problems, unknown variable interactions, unknown definition limits/combinations, or unknown concepts)' is similarly useful as an 'iteration network' in solving new problems, as often one of applying an iteration, resolving an unsolved problem, or identifying a new structure that hasnt been identified yet (like some combination of attributes like associativity/distributivity/etc which hasnt been defined yet) is required to solve a problem that hasnt been solved by identified structures
            - similarly, a 'network of rare structures' is likely to offer a useful similarity between maximally different structures as 'rareness' is often a good approximator of 'difference' and a useful opposing structure to a 'network of common/core/similar structures'
            - relatedly, some combination of 'resolutions between extremes' (like rare vs. common), 'new abstractions', 'useful cross-interface structures' are often useful in solving most problems
            - relatedly, identifying the 'variables of useful ways to connect these interface variable networks, so that queries can be run on those connections' is a useful problem-solving intent, as 'ways to connect iteration/rare/uncertainty/interface networks' are likely to be realistic/real connections that reflect reality (although not guaranteed, like how simple/negative connections arent always correct but its always possible to identify them)
            - relatedly, a solution isnt 'real' as in 'abstract/general' until it is defined/applied in every problem space (a solution for some problem with integers isnt complete until its changed to be applied/defined with imaginary numbers), so 'completing the definition/application' of solutions is likely to solve some known unresolved problems
            - relatedly, the 'frequency' of some structures across solutions such as 'substitutions of very different-seeming (as in differently connected, such as by different counts of some unit or counts of variables, and different but similar inputs like roots of a number or the same number type) but equivalent structures (such as equivalent outputs) applying some other equivalence (like the same function, multiplication)' is useful to identify the reason for, which is that 'identifying alternate routes' and 'identifying different applications/extensions of the same routes' is useful for 'identifying different possible inputs to the same output'
            - relatedly, identifying 'over-simplistic questions' like:
                - 'why arent all factors of i (4,5,6) equally useful as components of rotations when combined using different but similar counts, like with quaternions which use (2/3)'
                    - contradicting/limiting implications: 
                        - 'some similarity in dimension requirement (like requiring 2/3 variable count for a rotation in 3-d space) is to be expected given those definitions' and 'differences in similar dimensions act differently depending on the scale (4/5 or 4/5/6 acts differently in creating different differences which are differently useful than 2/3) and all of these differences cant always be connected by the allowed functions (multiplication) or exponent/factor values (i, -1, etc)' and the 'ability of the trivial similarity of 2/3 to create this equivalence is likely to be rare rather than default/common' and the 'ability of the exponent similarity to create an equivalence using allowed factor values is likely to be rare across all exponent sets and factor value sets' and the 'unitary attribute of the connections is particularly powerful which by definition is less likely to occur in more trivial component factors', although 'there are likely other combinations of different factors of i that occasionally have some subset of or similarity to these cross-variable-count similarity connections', whereas in this particular subset, the factors have 'enough similarity across 2/3 variable count multiplications' that a 'high ratio of either 2/3 of their multiplications is similar/equivalent', indicating that there are 'graphs where there are overlaps and equivalences between some nontrivial subset of 2/3 variable connections in this set'
                        - similarly, 'components of i (like i ^ 1/2)' and 'outputs of i (like i ^ 3)' wont always act like i, just like i doesnt always act like i ^ 2 (-1) but regularly acts similarly enough to be useful in creating a substitution for -1, and so on, while i is one of the units in this set of quaternion variable interactions ('one unit squared equals all the units multiplied')
                        - the 'multiplication of all units' needs to be able to replicate the value added by the corresponding function of 'squaring each of the units individually' in other exponent/factor sets, which is unlikely to occur in most/other factor/exponent sets
                    - confirming/generating implications: 
                        - 'not many numbers can be applied in this way with so many different factor/exponent sets creating the same or similar output as 1 or -1', which is related to why there are likely other sets with similar similarities/differences that also relate to -1, even if those sets might not have the high ratio or the patterns of combinations or the similarities/differences that 2/3 has
                    - supporting implications:
                        - the implied usefulness of 'variations' (like 'negative roots') of roots as 'components of differences' (like 'angles between orthogonal dimensions') which can be useful for creating other differences like other angle structures (like 'rotations'), as 'hidden/parameterized variables like imaginary numbers can be useful at some interval, like where at some interval, a number with has a zero-valued imaginary component like a real-valued number is created, as there is continuous variation between negative/positive, so the zero value is necessarily crossed at some interval, so discrete data sets can apply rotations/imaginary numbers to explain the intervals between points' and similarly, -1 will be crossed at some structure like an interval of combinations of differently factored/exponentiated sets of 'roots of -1' but at different rates/structures that dont necessarily align with 'simple similarities/connections' (like an equivalence between 2/3 exponents across some subset of factor value sets)
                    - abstract/invalidating implication:
                        - relatedly, identifying 'abstract numbers' other than a 'unit abstraction like i' that are defined by the similarity they encode (like 'points of equivalence between 2/3 exponents using the same factor set' or the 'interim value' of some set that is 'equidistant from all items in the set', which may not be defined for some number type) are useful to identify/apply as alternatives to numbers like i, or similarly identifying 'graphs where all outputs are cross-sections of the graph' or identifying 'graphs with multiple different unit connections' are similarly useful in identifying structures like quaternions with a high ratio of interface structures connecting them
        - relatedly, identifying 'components of useful structures' like components of similarity/difference that create circles/waves/spirals are useful to identify as probably useful components of other useful structures like interface queries, such as how a 'circle' can be created by an equivalence in adjacent units of a similar and different structure (like an arc with 'enough difference to eventually cycle but enough similarity to not return to its original starting point right away'), and similarly, a spiral has 'emergent similarities in adjacent units of a similar but different change rate' (like similarities across similar points/angles on different cycles), and 'waves' have a similarity in the repeated sequence of 'opposing' and 'neutral' arcs, and 'rotations' have a similarity in that they can all be 'multiplied/divided into something that can be multiplied to create a circle'

    - identifying useful opposing structures like 'generative variables' and 'definition limits' are useful to apply to 'identify new variables' as well as structures like the set of 'all possibilities achievable by applying a specific value'
        - for example, identifying 'quaternions' is possible by identifying numbers that have overlapping/multi-function variables/functions, and similarly, constants like 'e/pi/i' have a relation to a standard number (specifically 1) using some core function like 'inverse' or 'rotate', which means finding other useful number types or constants is possible by identifying numbers with many interface structures (such as being 'related to a standard using an unused function' or 'having multiple overlapping maximally different functions')
            - this is a type of 'computational/generative math' that applies a smart version of trial and error (pre-filtered trial and error)
        - relatedly, the 'limit' of a definition (and the 'limit of the meaning of a definition'), as in 'what a number is most/least useful/not useful for', such as how other numbers can be generated by a standard number like 1 by applying enough changes, but whether that number is the 'most adjacent' to all other numbers is not definitive given its definition, as there are more relevant defined interactions of 1 with other numbers, and there may be other numbers that have that function to a similar degree which are arbitrary, so the functionality is not relevant to the identity of 1
            - relatedly, the 'differences possible with some interface structure like rotation' are useful to identify, such as differences created by changing the variable of 'what is held constant and what is varied by a rotation', such as 'holding an endpoint or center or boundary or surface or curvature or continuity or space or connection structure or other variable/function constant' or 'holding some subset of the definition of the rotation/structure like an average definition constant' (like holding a type definition constant while varying some extreme/starting/average/subset of variables), which is related to why circles/symmetries can be varied to create other 'similarities/differences' structures to create similarities/differences in variables like 'position/orientation/angle/shape' (a 'average symmetry' creates a 'orientation' difference)
        - relatedly, identifying the most filtering structures (like concepts) is useful for identifying solutions in that set, such as how identifying overlapping solutions is made more trivial by identifying structures that overlap in maximally different ways, at which point most 'overlapping' solutions will be some trivial subset of that 'maximally different overlap'

    - identifying useful structures like specifications/integrations of useful functions/workflows, 'integrations' being a useful subset of other problem-solving functions like 'organization' and similarly 'specification' being a useful subset of 'description/representation/definition', these subsets being sufficiently reflective of the variation of the superset that they can be used to approximate the superset
        - for example, 'break a definition of a useful/solution structure' is a useful problem-solving intent which can be used to connect problems/solutions, as it indicates how to convert between solution/error structures
            - for example, 'how to break the definition of a vertex', as in 'what is not a vertex', such as a 'pair of variables that does not cover all of reality', is useful to identify and apply as filters of vertexes
            - relatedly, problems are likely the result of a lack of a useful structure or a partial/differentiated useful structure, so that is another reason why this is a useful problem-solving intent
            - the 'definition' is like a subset of the complete description of a structure in all of its variables and interactions, so it acts like an index of a structure, so its useful to change this specific index of a structure, rather than just generally 'changing a problem/solution'
        - similarly, 'apply changes to problems/solutions and find their overlaps' is another useful problem-solving intent which varies some other workflows, integrating 'reverse-engineering' and 'apply differences to a problem to create a solution' and 'change a base solution', applying known useful structures like 'overlaps' as a solution metric
        - finding 'connections between indexes' is a useful problem-solving intent as well, as 'indexes' are like 'approximations' of a structure, so first 'approximating structures' and then 'connecting approximations' is a useful workflow, just like 'identifying types' and then 'connecting types' is a useful workflow to implement the 'connect problem/solution' intent (where types are identifiable), and identifying 'structures between indexes/types/approximations/inputs/sets' that are useful in encapsulating the info about the differences stored in each variable are similarly useful to fulfill 'connect problem/solution' intent

    - identifying useful structures like networks connecting problem/solution structures with high-variation outer-layer interface structures (like 'iterations/organizations') organized by structures (like 'combinations') of interface structures connecting solutions/problems and combinations of 'spectrum interface variables of problems'
        - for example, a known solution or neutral structure that only requires being 'iterated' to become an error is more trivial to create an error from than other solutions, whereas other structures might be more 'distant from errors' in the sense of requiring more complex change types/structures to create an error (being applied as a 'constant', being applied as an over-specific type such as an 'input', etc)
        - this 'network of change types that are required to convert a solution into an error' is useful to identify, in its 'general' form (as in 'what iterations create an error in general across problems'), its 'specific' form, and other combinations of spectrum variables that indicate differences in cases/problem spaces, and similarly, only using 'iterations', using 'combinations' of interface structures, etc
        - alternately, connecting these structures until changes are created that are likely to be useful as connections between problems/solutions
        - alternately, identifying structures of these structures such as 'intersections of iterations/organizations' where it is useful to start applying changes (as maximally different, high variation interim points between limits) which are likeliest to contain new possible useful interface queries, or where it is useful to avoid those structures (as limits), so that structures which are not limits can be identified by differences from those limits, once known
        - relatedly, applying similarities of workflow components (like how 'position changers' are a component of a workflow that 'changes positions' such as an 'organizing' workflow, and 'base changers' are a component of workflows like 'change a base solution') is useful to identify new useful structures (such as 'position bases' like new formats indicating differences in position by differences in similarity metrics, and 'base positions' as useful starting points of workflows in various formats involving networks/sequences)
            - similarly, 'change errors (until theyre more optimal)' and 'change position (until its organized by a different similarity)' have a merged structure of 'error position' by the 'change' function, as identifying 'error positions' such as 'common error positions' is useful to identify other structures like 'common positions or structures of positions like areas/boundaries to avoid' and 'functions/variables creating common error positions to avoid'
            - relatedly, given different organizing methods of interfaces/bases, some of which have dead-ends or cycles or common points or other structures, identifying useful structures like 'cycles' of network formats (like the 'function' -> 'variable' -> 'type' -> 'concept' -> 'function' cycle) is useful for identifying alternate structures like sequences/cycles/networks or non-repeating sequences
        - relatedly, different 'units of reality' like incentives or paradoxes can be fit together but some are more true than others, such as by being more complete, default, robust, higher variation, more determining, etc, where more determining reality units can be identified using other reality units, and a level of determination can be reached where the fewest units are necessary to determine reality, at which point reality may be actually determined (rather than possibly determined by identifying some structure that may or may not be used to determine reality)

    - identifying useful structures like 'connections between outer/inner layers of interface analysis in its system layer diagram' (such as similarities between 'simulation/thinking' or 'simulation/interfaces' or 'simulation/info interface structures like quantum physics structures' or 'simulation/connections' or 'simulation/similarities') which are useful in the cross-layer/interface/system insights they often adjacently reveal
        - thinking/simulating is related to 'quantum entanglement' if sufficiently accurate, as a simulation connects to the original system enough to predict/control it, similar to 'opening a wormhole' into a system
            - therefore thinking/simulating is a way to create more time outside of simulated structures by simulating/controlling/using those structures, where 'control' is possible by functions like 'better organizing the system in the simulation than it occurs in reality'
            - 'similarities' are therefore a useful basis for these simulations (which are more accurate when more similar)
            - 'simulations' can also contain a system so that it becomes essentially false, as it is understood in the simulation
            - 'interface analysis' is such a good simulation of reality that it made everything understood by it essentially false (as in easily used/controlled), so that 'iterations of interface structures' to identify new variables/structures should be computed quickly
            - the 'outer layers' of interface structures not found/computed yet are where variation exists, but the inner layers are understood and are becoming false (so 'move to the outer layers' is the point)
        - relatedly, the 'usage structures like usage queries and usage potential' of solutions to problems is useful to identify, such as how the solution like 'connect to the system and find its variables and connect those variables in a simulation' to the problem of 'control/use a system' can be condensed and re-used (the variable metadata can be re-used to deconstruct new systems faster)
            - similarly, the intent of 'connect to the system, find its variables, and connect those variables in a simulation' is replaceable by other intents like 'apply and change a set of maximally different base systems to quickly identify a similar base system to change trivially to simulate the system' and also 'test the system until it reveals which base system its most similar to' and 'destroy the system to find out its most robust variables', which are 'alternatives to simulation' that can form different routes connecting pieces of reality, just like 'simulate' is an alternative to 'organize' and 'iterate'
            - these 'similar alternatives' are on an interaction level where only one can usually exist (in most cases), so the simulation and the original have to conflict and that conflict has to be resolved into one unique system
            - this is bc the 'function' interface acts like a symmetry, where 'functions it uses' and 'functions that use it (intents fulfilled by it)' are similarly variable and connectible with the function, although there can be overlaps and other connection structures between the two sides of the function
        - these 'quantum connections' can be useful to 'base/host more variables on', as connections between a simulated/original system can become high-variation (as the original refers to the simulation and changes itself to avoid being controlled)
        - relatedly, the 'math structures which simulate the most math structures (or math structure simulation sequences)' are useful to identify as 'structures which will collapse to one of the better-simulating structures' if the other simulation structures arent differentiated enough to be similarly useful, and as 'structures which need to be differentiated from in order to avoid a case where all of reality is encapsulated in those structures', and given the 'sequences of simulation potential by these math structures', the 'usage potential' of the universe where these structures are defined can be identified, and given the 'interaction levels of equivalent alternate math structures', the 'conflicts that need to be resolved, which wont be encapsulated by some simulation sequence' can be identified (such as 'equivalent alternate dimension sets or methods or variation-containing structures like sequences')

    - identify useful structures like 'overlaps/connections between useful graphs' like how a graph depicting 'areas of iteration (and other problem-solving functions)' is useful to connect to 'areas of optimization' and other useful graphs
        - for example, 'iteration' is optimal when the solution set to iterate is relatively small (compared to the steps required by other algorithms to identify/filter or generate solution sets), but when its a trivial set to iterate, its often more optimal not to iterate but to include all remaining solutions in a function network like a probabilistic network, which means there are very limited 'areas of optimization' of problem-solving functions like 'iteration' in a problem space
        - similarly, 'forcing' a solution by 'trying all possibilities' or otherwise applying computation to create advantages over other workflows is only optimal when the correct structure to iterate is identified, like the correct 'interaction level where the most variation occurs in the relevant variable set'
        - relatedly, 'iterating structures until unique structures are found' is a useful problem-solving intent to find new variables, which could be possible by iterating most core structures, as theyll either identify a new structure/variable by creating it or by hitting a limit of variation created by another unidentified structure
        - these are useful to connect bc iteration isnt valuable in every position and in every iteration type so identifying different iteration types is useful to connect to positions where iteration could be useful, such as how with a trivial set to iterate, applying the iteration type of 'iteration to find all possible solutions (rather than the first)' is more optimal in many cases, especially when a multi-solution format is acceptable
        - relatedly, computers can be built to have reality-reflecting functions like organize/iterate/control and use these functions to build structures as light structures (so that if a structure isnt possible in reality, it isnt buildable by the computer's light functions and it rejects the request as undefined), as opposed to functions that just 'generate combinations of pixels according to a list of pixel patterns/sequences', which is not how reality is built, so that the computer is a reality-simulating device rather than a reality description/representation component-listing device

    - identify useful structures like 'reasons for a structure' and 'formats of a structure' which are likely to contain more 'unidentified variation' that is connectible to other high variation/useful structures like 'interfaces' and '1-to-1 mappings' such as 'equivalences' which form the basis of useful structures such as 'units of reality' like 'cross-interface structures'
        - alternate function format of time-based usage subset y-vectors associated with a stack of x-values as a sequence of multiplied/added areas created by each input parameter, as some values of a x-y function wont ever be used and usage sequences where common adjacent sets in real systems are depicted can be more useful to depict actual variable interactions in real systems despite the overlaps of these vectors
            - similarly, stacking iterations of these subset sequences is useful to indicate real system ratios of those sequences
            - identifying 'reasons for a variable change' like how a 'limit or other interfering variable' can change 'one variable interaction to a different but adjacent/overlapping variable interaction with a different angle/magnitude/structure' is useful to depict on a function network, as opposed to only finding polynomials as solution function formats
            - relatedly, selecting 'exponents/iterations/sequences of areas' as a result of 'opposing force dynamics' and 'horizontal/vertical force dynamics' and 'ratio dynamics of the opposing forces' and 'variable/exponential vs. constant/linear dynamics', so for example an 'iteration of n="some number type value, like an even integer"' could be associated with a 'relatively strong opposing force to the original force and a small but nontrivial horizontal force parameter', or similarly a 'set of magnetic/hub variable forces (where variable interactions gravitate around some value like an extreme limit)', or similarly a 'set of opposing cyclical vs. non-cyclical forces' (for example, creating parabolas instead of waves) or 'forces that change ratio dynamics' (creating 'equal ratios of opposing forces' that are likelier to create a wave that never resolves into a non-cyclical structure), where this number type value applied as an iteration structure like an exponent is associated with 'midpoint symmetries', where the alternative number type value is associated with 'positive-negative symmetries', and similarly, selecting alternate number type values (odd) with associated structures like 'positive/negative symmetries' is useful for 'differentiating inputs' for intents like 'filter'
            - the core insight is that the 'variation' in the intents that a numerical structure is useful for indicate its 'independence/equivalence' from/to those intents (a numerical structure with many supported intents is more arbitrary than a numerical structure that is primarily useful for one intent, like a 1-to-1 intent/structure mapping, which are uncommon but which ground reality in some numerical structures having these 1-to-1 mappings, where these numerical structures with 1-to-1 mappings are less like 'one of many possible descriptions of the intent' and more like 'equivalences of the intent', mappings which can act like math-structure interface connections)
        - similarly, its useful to identify/apply different formats of core structures, such as how 'multiplication' formatted as an 'iteration' of addition is useful for finding useful structures like useful 'iteration areas'
            - relatedly, finding useful 'format sequences' like the sequence from a 'polynomial' to a 'linear format' and then a 'grid' format is a useful problem-solving intent, where 'iterable change units' of the variable set have been identified and the grid representing that variable set can be fitted as a 'point on a grid graph' is a useful sequence to apply as a solution metric
        - relatedly, identifying 'useful filter/map formats' is a useful for intents like 'identify probable errors of a filter/map sequence', which are more obvious when depicted as a set of filter/map structures with varying vectors/intersections/angles connecting them, where filters can be depicted as 'blocked/allowed areas of an input parameter space' or 'a maze layer of blocked/allowed input structures' and maps as 'vector networks connecting filters'
        - after thinking about parabolic functions and different formats of them and useful ways they connect to iterations, I sensed that there was more variation there that I didnt identify yet, which takes the form of '1-to-1 mappings' between 'equivalences' as useful for connecting interfaces such as math/structure, and the possibility of identifying structures with 'less/one supported adjacent intent(s)' occurred to me while thinking about these iterations/formats, which intersected with known useful structures of '1-to-1 mappings' and 'cross-interface structures', while fulfilling the general intents of 'connecting math/structure interfaces' and 'determining why a number might not be merely descriptive or resulting from definitions, but causative as in powerful/determining/highly useful/highly used, or alternatively, equivalent to some other interface structure like a concept' and identifying 'unique intents' as a possible cross-interface equivalence structure to determine these alternate usages of a number for intents other than 'description'
            - relatedly, 'powerful variables' are powerful in that they have extremely high functionality, so listing functions and listing extremifications (implementations with high usefulness) of those functions is a way to identify 'types of power' and 'powerful variables'
            - relatedly, identifying all the variables which can/cannot be other useful structures like 'types of maximal differences' bc they can be extremified/repeated until they create other differences is useful for intents like 'find the most optimal solutions to resolve known maximal differences' which are likely to be useful across problems
        - similarly, applying '1-to-1 mappings (equivalences)' as 'units of reality' is useful (at which point other structures can be adjacently formatted as '1-to-n mappings of 1-to-1 mappings', or different angles/intersections/filters applied to these mappings, within the variation allowed by these equivalences), just like applying 'symmetries' as 'units of reality' is useful to format other structures as 'variation around symmetries'

    - identifying 'variable structures' (symmetries, embeddings, high variation variables like problems) between interface structures is useful for identifying interface queries to 'find new variables'
        - for example, given that 'problem/solution structures' reflect each other, solution structures like solution metrics should also be useful in various interface structures like sequences/networks, so that 'sequences of solution metrics' can be identified and 'sequences of solution metric success/errors and problems resulting from those results' can also be identified, which are useful in reflecting new problems
        - whats interesting is that finding an info reflection/symmetry (problem/solution), finding various embedded structures ('solution metrics' in the set of solution structures, and 'solution metric results' in the set of 'solution metrics'), then finding another info reflection ('solution metric result sequences' and 'next problems') is useful to identify the 'difference in value' of identifying solution metric sequences/networks, as opposed to finding only the 'problem networks of related problems' such as 'problem causal networks', as the way that a solution metric is fulfilled is a different variable that can change next problems resulting from a solution
        - relatedly, if a function cant create a negative result, it cant reflect all of reality (such as how a function that wont accept negative feedback or costs like responsibility cant reflect reality)
            - relatedly, if a function indicates a highly positive structure is always negative, then all the less positive structures are also negative, and it cant reflect all of reality
        - relatedly, a 'multi-function substance' is likelier to be useful at 'covering all required functions' (such as a 'growth reducer' and 'growth inhibitor'), and a 'multi-function solution' like 'fasting + growth reducing substance' is likelier to be useful at outpacing a 'growth increasing process' given their 'similar and additive effects', which make 'iteration of addition' useful, as this is the 'core difference in a ratio' that needs to be resolved for solving cancer
        - relatedly, 'identifying a solution function in a simpler problem space, then improving it by adding complications' is useful as a simpler variant of intents like 'filtering solutions in the original problem space' and 'change a base solution', as it first 'changes the problem space' so that the solution is more solvable (such as solving the more general problem or a unit/simpler problem) to find a more adjacent solution in that problem space, then 'changes the base solution' by adding more trivial changes that are required once the base solution is found
            - relatedly, other variants include 'identifying solutions that are simpler and more complex (problem space solutions on either side of a spectrum), then connecting them, which is likely to intersect with the original problem solution'
        - relatedly, finding 'iteration areas' of problem spaces where once the area is reached, the rest of the problem is solvable with iteration, such as how once the 'specific/important/relevant/interim solution metric' (like a ratio to find, or another difference type to find, or the limit/intersection of some structures to find, or a probable solution range is found) or a 'base/approximate solution' is found, and once 'adjacent change combinations to test' are identified, 'iterated adjacent change combinations' can be applied to solve the problem (iterations of 'adjacent change combinations' can be applied to the base solution to find the optimal solution), where a 'problem-solving network' can connect these 'iteration areas'
            - similarly, 'identifying variables to generate a structure' and 'incomplete solutions' are other examples of structures where iteration can solve the problem, as 'what to iterate' has already been identified at that point
            - similarly, patterns in structures that invalidate iteration (such as by 'replacing iterations with fewer iterations', 'identifying patterns', 'identifying types', 'identifying midpoints/stopping points/thresholds/limits', etc) are useful to apply as alternate functions to iteration, once an 'iteration opportunity/area' like the above structures have been identified, an 'opportunity' being an 'adjacent or otherwise visible/clear/obvious structure to fulfill an intent'
            - this is bc 'iterables' are similar to 'interactivities' and 'incentives' in that they can be used to connect all problems/solutions, and similarly 'connections between different equivalences (like equivalent alternates)' are useful in that these connections contain all useful differences
            - similarly, identifying the 'reason why a structure is useful' is related to identifying 'variables/causes/inputs/components of that structure' in that it makes it possible to iterate structures to identify 'other reasons for usefulness and other useful structures', and similarly identifying useful structures makes it possible to iterate 'adjacent changes' to identify other useful structures, and similarly 'abstraction' as a cause of the usefulness of a function (like 'reduce', as if its more abstract, it'll have more possible implementations) which makes it possible to iterate to identify other useful functions (like 'connect', 'reuse', 'expand', etc), and similarly identifying 'definitions of abstract concepts' (identifying their definition routes such as 'having more possible implementations') makes it possible to identify other abstract structure definitions and identify that abstract concept in structures like systems using iteration, and similarly identifying the general variant of an 'implementation/abstraction' (an association in either direction, removing the 'direction' attribute of causation) is useful for iterating finding associations (which are useful for 'finding variables/variants or similarities or limits' using iterations), which are other examples of specifications that create 'iteration opportunities'
            - relatedly, the 'network of iteration opportunities' is useful to identify, such as the 'opportunity to iterate a structure when a case is identified, such as iterating regression methods when a function is determined to be non-random', and similarly to identify differences from, to apply other workflows with
            - 'pre-computing everything except iterations' (which are too common/costly a structure to pre-compute unless its iterations of interface structures) is another useful problem-solving intent
        - relatedly, 'type hierarchies' are useful in determining 'subsets', so are likely useful in identifying subsets in general, like in 'regression' where identifying 'determining/outlier/average/high variation/other subsets' is useful
        - relatedly, the 'angle/magnitude of distortion' required to make structures falsely seem similar (in which networks/formats) is useful to identify, and similarly 'angles/magnitudes that make everything/nothing/arbitrary/irrelevant structures seem similar' and the differences between these sets is useful to identify
            - for example, reducing every structure to one variable like 'position' or 'combination' or 'unit' or 'starting point' or 'size' is not completely useful and over-similarizes/simplifies (which is useful for standardization so that other differences can be identified) but is possible with some subset of 'vector angles/magnitudes'

    - identify useful structures like 'known connections' that can be applied as 'default components' of interface queries to fulfill conceptual metrics like 'reducing uncertainty' in selecting queries
        - for example, the reason that 'similarities' can be used to detect 'differences' is that the connection between them is known (opposite) and this can be applied to find one from the other (once similarities are identified, differences are more identifiable/obvious), so the interface query is 'find similarities', then 'find opposites of similarities (differences)'
        - other structures with known connections (like known function sequences such as 'find a structure, then find its opposite, in order to find different structures from the original structure') can be similarly connected to apply these known connections (such as 'opposites' are a known connection between similarities/differences) as default components of interface queries
        - this is useful for specific metrics like 'reducing uncertainty' in interface queries (which shouldnt be uncertain so much as similar to a workflow and similar to the problem-solving intents), where 'identifying which structures should have which metrics' is an unsolved problem that is useful to solve to some degree, especially in connection to other metrics, so that overall system uncertainty isnt reduced but applied where important/useful
        - relatedly, 'applying a definition to itself' is useful as an implementation of cross-interface structures like math-core-meaning interface structures, to fulfill intents like 'identify useful interactions like "isolated interactions" to identify "potential change" created by a structure like a definition'
        - relatedly, identifying useful structures of filters like 'overlapping/intersecting/1-to-n/perpendicular/mapping/standardized' filters is useful for finding useful structures like 'filter structure sequences' and 'optimizations' of them, such as where some filters can be skipped and others can be changed such as being standardized to optimize for the intent of the filter sequence and for general intents like 'filter sequences that filter the most sets with the fewest steps (and other variants of them)'
            - similarly, its useful to identify the filter/change potential of various sequences/combinations of common functions like 'random' and 'average' given maximally different data sets with known errors applied

    - identifying useful structures like useful networks of functions/structures which have useful connections like 'connections that avoid the errors of a network'
        - for example, 'reducing to a negative' is an application of a core interaction function and a core structure on various interfaces (like charge, value, etc) but has an error in that it doesnt cover reality as there are structures which cant be reduced to a negative (structures formed by differences like opposites allowed in the negative network, which can create structures like positives, and more relevantly, irreducible positives)
        - similarly, 'connecting to an interface' is usually possible and useful but allows opposing structures like 'limits' or opposing variants of symmetries (like a 'difference in a difference')
        - identifying all the useful networks formed by functions/structures and their opposites/contradictions allowed by their variables allows for mapping a network connecting these networks that can avoid the areas where they have errors

    - identifying 'concepts driving errors' is useful for identifying the 'various solutions to those conceptual errors' (like how randomness and volatility often indicate lack of understanding or lack of information, and identifying the associated variables having that info is useful to correct these errors)
        - for example, 'specificity' is a structure that leads to 'volatility', which is an important variable to avoid where possible, such as how a neural network or context-similarity identification tool may identify that two substances are used in the same context (like two examples of a type), but the type doesnt explain the reason for using the substance in that way, so identifying the type isnt a context that works to identify similar substances that can be used the same way
            - specifically, identifying 'lime juice' and 'water' as both examples of a type of 'liquid' is irrelevant to why theyre used in similar ways (such as used with the word 'drink' and 'cup' or 'ounce'), so identifying their type in common isnt the reason why theyre used similarly, so other usages like 'drink 8 glasses of 8 oz of the liquid per day' isnt applicable to both, as the reason for their similarity in one context ('both can be and are drunk in non-trivial quantities like an ounce without deadly effects') is different from the reason for the similarity in the other context ('only one of them is essential for life and in high amounts'), but the problem space of 'identifying structure-function connections between substances' is so volatile that even one change to a substance can totally oppose its previous function, and the reason for this volatility is 'lack of understanding of the reason why different structures have different functions' and the 'specificity of some functionality requiring that exact structure to have that function'
            - so the neural network would be better off identifying 'different subtypes accounting for different reasons' or 'identifying high-volatility problem spaces like chemical substances in structure-function connection prediction and identifying missing info required to predict structure-function connections', so that contexts without that info or contexts for different subtypes arent applied without accounting for those variables (matching contexts/types with reasons and matching volatile variable sets with other variables like surface structures which are the reasons for the volatility, as relevant missing vertex variables)
            - similarly, 'patterns in surface structures in the bio system' are the remaining variable to identify to make the volatility less volatile, as there are usually 'overlapping patterns' (in variables such as 'ratio of difference/fit/interactivity' between surface structures and substances) that explain volatility which simplify the problem space drastically and accurately
            - so avoiding high volatility functions is useful in regression, as it usually indicates lack of understanding or specificity that is not fully identified by those input variables (the specificity is stored in other structures such as immune structures and artery wall structures and receptors/surface components, not the substances interacting with them, so that info isnt directly stored in the substance structure but is indirectly inferenceable)
        - relatedly, identifying 'barriers to iteration' such as 'areas of variation like randomness' identify barriers why a particular solution cant be forced (with iteration) 
        - relatedly, identifying 'ways that a function can generate or seem to generate randomness' and 'contexts where a function seems random, such as in a subset or in its entirety', as a way to fulfill the intent of 'isolating random vs. non-random functions' and 'connecting randomness and non-randomness', which are useful for regression
        - relatedly, identifying a 'simple function' and a 'complex function' and other variants of spectrums (of a data set) is useful to apply an 'integration function' to that reflects the abstract connections between these conceptual spectrum variables
        - relatedly, each example specifying a different connection/variation of the same intent is similarly useful as a new workflow if its connecting reality-covering variables which can be used to connect other variables

    - identifying useful structures like truth interactions is useful for intents like 'filtering truth structures'
        - for example, given that specific/local facts about variable interactions are not true until they connect to other facts without every possible contradiction being true or more true (preventing a fact from ever being true), how many facts do they need to be connected to in order to become real (such as a real system acting as a base, where the fact can connect to other facts), and what other metadata needs to be fulfilled before a fact is clearly true (its still true or becomes true, even when some barriers to it are placed in positions to prevent the fact, bc the falsehoods are irrelevant in some way, such as they occur in a different irrelevant system or they only remove one input to the fact which is replaceable or not required)
        - for example, a statement is only true, if the structures in that statement are 'possible/defined', which identifies at least one other required fact before the fact can be true
        - similarly, being 'robust to extremes' and 'robust to having requirements removed (reoccurring across different systems with different bases)' and 'causing higher degrees of relevant variation (variation source)' are also truth structures, determining the 'reality-building/intersecting potential' of a 'fact'
        - similarly, how can the fact become an absolute truth, where does it stop on the way to absolute truth, and what sequences can convert facts into absolute truths or otherwise make them real
            - relatedly, what other spectrums are particularly useful to apply in identifying truth types/sequences other than absolute/local (such as iterated/repeated vs. uniquely occurring facts and non/adjacently creatable facts)
        - similarly, 'detecting lying' by surrounding it with barriers to lying and clearly differentiating it from truth structures and otherwise removing opportunities for lying is possible, which would be applied in an algorithm like 'placing barriers between variables so that the lie has to create truth on its own or using other lies, which it cant create (itll be usable to create other facts if its a fact)'
        - relatedly, the intent of 'solving a problem permanently vs. temporarily' involves solution structures like 'repeated regular applications of solutions' or 'speeding up computations to enable repeated solutions' or 'storing solutions in a input problem/output solution index' or 'finding interfaces or spaces that collapse the problem to a point', which is a useful intent to identify

    - identifying useful structures like 'function sequences' to identify other useful structures (like components and orthogonal variables) is useful
        - for example, 'squinting' is useful at identifying light dynamics bc it increases interactivity of light and reflective surfaces and bc of the reflectivity of eyelashes which allow the components of light to be identified (it first filters the light into a smaller input range, and then reflects light off a high-variation variable like a shiny surface), and similarly, the differences in wavelengths created by 'adding/removing a barrier to light' (sunset/sunrise) can also identify components of light, as different wavelengths can be combined/overlapped to create different wavelengths, and adding/removing a barrier changes the wavelengths in a way that reveals components of wavelengths/light, where the barrier acts like a filter
        - identifying this sequence is useful for 'identifying components of variation' by applying a filter and a reflection across a high variation variable
        - relatedly, identifying 'rotations' as useful for identifying 'orthogonal variables' is a result of identifying that the 'perpendicular alternative' is identifiable by a 'rotation', as 'rotations' cover 'every possibility in a range likely to contain differences', so rotations are likely to cross various differences types, including a point where difference is maximized (a 'midpoint between extremes' (in angles between vectors) can replace the value of a rotation, once 'angles between vectors' is identified as a new variable to apply a 'midpoint between extremes' to)
        - identifying new variables like 'angles between vectors' is a prerequisite to identifying these structures like 'orthogonal variables', just like 'identifying the range of rotation to apply, to cover all difference types'
        - relatedly, finding the 'rotation function' enabled by every variable (like an imaginary variable) is useful for identifying composing variables of high complexity problems, which are often explained with overlaps/rotations/iterations/other simple mixes of interface structures 
            - relatedly, a 'rotation' of a 'network' could be a function that changes connections of nodes to 'different adjacent/similar nodes'
        - similarly, identifying 'possible interactions between vectors' (like 'forming a closed shape') could also identify 'rotations' or 'perpendicular/orthogonal variables' (in the interactions between sides resulting from the closed shape)
        - relatedly, the 'equivalent alternates' are likely to be connected to other 'equivalent alternates' ('superposition probabilities' and 'entanglement similarities') and constructing reality based on 'alternate paths between these equivalences' is useful and possible (finding structures that map to interface structures which are like 'abstract info variables/structures' is useful to connect them to physical reality and therefore determine and change it)
            - so applying this would take the form of a 'possibility range' created by quantum processes like 'thinking' (which enables considering 'other possibilities than current/past reality'), these 'possibility ranges' acting like 'possible connections' on which reality can be based, which can be connected in many different ways, but given known principles, may prioritize 'high variation-supporting connections' ('variation' will move in the direction of paths formed by equivalences that support the most variation, like 'better understanding of reality' and 'agents acquiring that the quickest', which support more variation/freedom than current reality, so queries of 'abstract info structures (interface structures)' that form the best 'base graph of all other useful graphs')
            - relatedly, identifying a set of variables that reflects equivalent/similar/related info is related to identifying the same structure that can look like either variable from different angles or another structure that can fulfill multiple metrics which are sufficiently isolated or otherwise structured that each can only be measured at once
        - relatedly, applying abstract structures like 'iterations' and 'applications' and 'interfaces' as 'components of a neural network' is likelier to describe realistic variable interactions than just 'sequence coefficient/exponent changes', which only identify the variable interaction when its a polynomial, and lose/avoid/miss the info of 'alternate routes to the same function' and 'non-polynomial connection functions' and 'all the ways to reduce routes to important variables'
            - there are ways to format an interface as a 'sequence of change combinations', but that sequence isnt particularly likely in most neural networks
            - relatedly, identifying 'alternate forms of energy' (like 'dopamine' and 'clear/simple/incentivized goals' and 'expectations/certainties' and 'standards like absolute standards' and 'different but aligning intents like endurance rather than reaching a point' and 'applying the rate as a new default/assumption/certainty/requirement' and 'distractions/variation sources/games' and 'similarity/difference to/from a pattern') which help with intents like 'maintaining a rate' is another example of 'identifying different graphs/connections'

    - identifying useful structures like new interface structure combinations (like 'vertex networks') that identify other useful structures adjacently (like 'new function formats')
        - for example, a 'vertex network' is useful for identifying interface structure sets as useful components of interface queries, a 'vertex map (whether 1-to-1 or 1-to-n)' is useful for finding similar/equivalent/different/variable vertexes to apply, and a 'vertex wave' is useful for identifying how a vertex can vacillate between opposite extremes on a spectrum
            - similarly, a 'concept-structure-reason' set is useful for identifying high variation sources in 'reasons for a variable interaction' (structural reasons like structures in hidden dimensions and conceptual reasons like randomness), which is useful as a node on this vertex network (or relatedly, a cross-interface network)
        - framing polynomials as 'structures such as waves/parabolas which act as resolutions of pairs of opposing forces' is useful as a system-structure explanation of a polynomial, as a 'sequence of factors/exponents' doesnt map 1-to-1 to real system variable interactions
        - this set of opposing forces isnt quite enough to cover all polynomials, as there are other powerful forces, such as 'scalar forces, which increase/decrease the changes created by the opposing force pair', but the network of these together is more highly covering of all possible polynomials

    - identifying useful structures like 'different optimization network variables' is useful to identify different useful structures like 'specific optimization variant networks' which can be applied as a constant in some other algorithm once identified and which are possible to identify using other workflows
        - for example, a 'network that optimizes for no metrics (is suboptimal in every way to some degree, as it fulfills no metrics)' is useful for different intents like 'identifying interim spaces in between other graphs' and 'applying changes to a known incorrect structure like a base suboptimal solution', and is similarly different from other useful networks like 'networks that specialize to only optimize for one metric'
        - identifying these variants of optimization networks is useful to identify different types of optimization and different useful structures for different types of optimization, 'types' referring to 'different solution metric sets'
        - relatedly, identifying 'networks of variation sources' to fulfill problem-solving intents like 'identify sources of variation' are useful for identifying new possible variable interactions from new variation sources, which are particularly difficult/useful to find, such as high variation variables, powerful variables, interaction levels, randomness, etc, which are difficult to find because theyre usually already known
            - identifying 'reasons for difficulty of determining/finding info' is similarly useful as a problem-solving intent
        - relatedly, 'removing variables' is a known useful way to identify if there are other variables present that matter (like 'removing waves makes it clear if a shark is present') but more optimal changes would apply trivial changes to existing similarities/patterns (a type of light reflection that when bounced off of waves of similar magnitude/position, makes a shark clearly different, despite the similar structure of fins/waves), where the corresponding math structure is 'assuming there are variables in surface patterns'
        - the 'equivalent alternate graph of similarities' is useful to identify parallel connections which can be equated with a core function like a shift and which dont have an overlap/intersection or a convergence point where they both halt their variation, and similarly, a 'graph of intersecting functions at some regularity of interval' and a 'graph of converging functions by additive/sequential variation, or by angle/limit' is useful to identify as an alternate similarity type, where overlaps/connections between similarity graphs are similarly useful to identify
            - there should be a graph where 'checking for an opposing statement of a statement and whether it will intersect/interfere with the statement' and 'checking for convergence of a set of statements to the same statement' and 'checking whether statements halt at some theorizable limit' is possible using vectors, so these graphs are a useful starting point to begin identifying that final version of the graph with all of these computible functions, where the overlaps in these graphs will be a useful secondary intent to fulfill toward that final version of the graph
        - the similarity in usefulness is in 'connecting high variation variables like optimization graph variants or similarity graphs in different ways'
        - relatedly, an 'iteration network' is similarly useful as a 'unit network', which can be used to determine whether truth statements hold, such as 'any statement can seem true in isolation (in a vacuum)', where there are structures that can never generate another structure, no matter how often its iterated, which are useful to know about, such as where an 'opposing structure to iteration' is optimal like a 'component' as opposed to a repetition
            - structures can so often be formatted as iterations once a unit is found (such as fractals where the unit of change is the scale) that its useful to identify structures that cant (randomness sources)
        - relatedly, the 'rarity of compounds with only one function' is a probability ratio that can be applied to find 'missing nodes' in a network of structure-functionality connections
        - the 'way to game/falsify a solution metric' is similarly useful metadata to identify about solution metrics, where there are some optimal sets of solution metrics which cant be falsified (the function is a solution at regularly tested intervals, fulfills multiple accuracy metrics, is robust to different valid inputs, etc)
        - similarly, a spectrum graph like the 'graph of positive/negative structures' is useful for identifying how positive structures are created by negative structures and other interactions between extremes of the spectrum variable, and is similarly useful to connect to other spectrum graphs so that other spectrum graphs have clearly similarities such as overlaps/maps to other spectrum variables
            - the variation across 1-to-1 mappings which dont map directly to each other is similarly useful to identify, and is related to the variation allowed between 'equivalent alternates' which are similarly useful, or 'ambiguous alternates' which seem similar or are useful in different ways, or other types of structures like 'allowed possibilities by requirements' which have less similarities to each other and more similarities to requirements
            - relatedly, the structure of 'equivalent alternates' is useful for constructing other structures like similarities and interaction levels (so 'equivalent structures' can be used to build structures like 'info', 'connections', 'paths', 'bases/foundations/standards')
            - relatedly, finding the cases where 'assumptions are correct' is useful to identify interim spaces where theyre incorrect, just like finding the way that a structure can be true/false is useful
                - for example, 'dont look directly at the sun' is a 'first principle' that would be applied as an absolute fact that can be assumed, but in reality, looking at the sun can be useful in some cases and in some ways

    - identifying useful structures like new graphs which would be useful to identify (even if only identified incompletely) for useful intents like 'identify equivalent alternate variable/function sets on different complexity levels'
        - for example, 'solving all problems in one step' would require 'computing every problem/possibility/connection/etc', which is suboptimal and probably not possible in some circumstances, but 'solving all problems in two steps' would require slightly less, such as 'computing all core structural interface interactions (like all structure interactions such as overlap/specification interactions)' and 'converting to structural interface' (or some similarly reality-covering set of functions and a function to convert a problem to/back from that format, such as 'compute all variation sources' or 'compute all solutions in a high variation specific problem space')
        - this 'network of n networks of intents (and functions fulfilling them) that are usable to solve problems in n steps' is useful to identify as it can guide the design of interface queries and also future queries to fit problems into a solution in a n-step network
        - to identify this, I was thinking about vertexes and variants of them which could be used to solve problems using one variable (like one interface, or one node of a network) or similarly one function (like a core function such as connect), then connected this to graphs, then noticed that a graph of n-step solutions/intents would be useful to identify, then I connected this to problem/solution structures like problem-solving intents
        - identifying 1-variable/function sets that map to n-variable/function sets is useful to identify (same for other metadata than variable count, such as added/constant/embedded variables) bc they are equivalent alternates on some useful metric like 'complexity'
        - relatedly, identifying highly 'absorptive/attaching/moving/using substances (with high requirements)' is potentially useful for intents like 'neutralizing a substance/pathogen' bc of the high variation of these variables (such as identifying 'heavy metal chelators' as probably useful against other harmful substances/pathogens bc of the variability they can apply)
        - relatedly, identifying 'functions with one point of intersection with a data set' (or 'functions with one point of intersection with a data set and minimal angle similarity with the data set') is similar to solving the problem of regression (identifying a perpendicular line of the data set is related to identifying a parallel line of the data set bc maximal differences can be applied to the incorrect perpendicular to create the parallel)
        - relatedly, identifying the 'minimum nodes in a network' required to create functions like 'generalization/specification' (and other interface functions) is useful to identify networks that can likely solve other complex problems (once a base solution is found to either generalize or specify, or similarize/differentiate, or stabilize/vary, etc)

    - identifying useful structures by identifying variation sources ('high variation subsets') that are relevant in their reflection of other relevant differences ('sufficient variation to differentiate functions') by applying differences to structures that are useful in other intents ('high variation subsets' are useful in intents like 'identify functions indicated by a subset') such as a 'specification' like a 'specific useful variant' (a 'high variation set that is useful for filtering possible functions to very few possible functions'), once a related intent is identified that could use that specific useful structure in a new way such as 'filtering functions, once a high variation threshold of a subset is reached', then identifying problem-solving structures to implement/fit that structure into a set of workflows (like an 'index' or 'identifying function')
        - for example, 'identifying subsets with enough variation to differentiate a function uniquely from other functions (filtering out other possible functions to explain it)' is a useful intent to solve the regression problem
        - how I thought of this was first thinking about maximally different functions in a range, then identifying cases where two maximally different functions would have similar error values for a particular function and what case would explain it, such as where a more correct solution function was at their midpoint, then identifying the similarity of that to other workflows, then identifying a related intent of regression as 'resolving an interval between certain data points to find their connection function across that interval', and how having at least two local subset functions in 'maximally different subsets of the function' would be useful, if there was an index to identify 'possible functions indicated by a specific pair/set of local subset functions', and then identifying the similarity of that to other workflows, then identifying a related intent of identifying the highest variation subset of the function as particularly useful for filtering out other possible functions, then identifying a new structure (a 'function to identify subsets with sufficient variation to differentiate/select the function from a set of possible functions indicated by that subset') as a useful function/index to identify (indexes like 'variation required to identify this range of functions in this function parameter space', and a function to identify data points with that variation in what structures like a particular value of locality/adjacency of that subset)
        - this is useful bc it applies differences in relevant positions until a new useful structure is found, which is one way to identify new solutions ('change a base solution', where 'maximally different functions' are a base solution structure in this workflow application)
        - relatedly, the idea of 'immune-boosting (with substances like turmeric/withaferin)' or 'inflammation decreasing' is useful as a complementary structure of 'low-dose exposure' as a possible source of immunity to pathogens, similar to how 'applying vaccines in the morning' applies an optimization structure in the 'immunity' problem space
        - relatedly, the incorrect assumption of 'absolute usefulness' given a structure that is useful such as 'input/output sequences' (with known errors, like 'misidentified type' which can be corrected by a common function like 'change position') can be offset and made more correct by applying interface structures (embeddable variables in the sequences such as applying 'direction changes' and 'position changes'), similar to how a 'area of missing data' in a regression function that uses sequential/adjacent info can be offset by a general function to identify non-local connections to make a solution more generally applicable
        - relatedly, the common structure of 'several different changes' that need to be applied to 'default assumptions' (defaults like an 'over-focus on recency/extremity/locality') in order to find a correct variant frequently includes errors possible with interface structures, like 'localized info' or 'isolated info' (similar to how any connection can be created by some connection type, but some connection types/other connection metadata are more common, useful, or stable than others), such as how 'absolute lack of meaning' can seem correct after a 'recent/extreme/local meaningless event', but the correct variants include:
            - 'meaning can be created', 'meaning results from integrating/connecting localities', and 'meaning is not required, given structures like randomness', and 'there is always a meaning of some structure (even if its a copy or a lack of structure)', and 'randomness is not absolute, just like meaning isnt', which are several steps away from the 'default connection' resulting from applying various biases
        - similarly, identifying structures like 'probable incompleteness' to identify related structures like 'variation possible in alternate solution functions by applying different completion structures' is useful
            - for example, identifying structures like "'adjacent linear points' (incomplete structure) as well as 'non-adjacent linear points' (incomplete structure) indicating the same linear slope (complete structure)" are an example of a 'in/completeness structure set' that doesnt indicate 'other solution functions that should be considered with equal/similar probability' (assuming no other sets contradict the 'equivalent linear slope')
            - a network of these in/completeness structure interactions is useful as an alternate to 'subset contradiction resolution structure networks' (networks to resolve local subsets (or other structures that should reflect each other like sequential patterns and densities in a data set) that contradict each other or are otherwise difficult to integrate into one function in a data set)

    - identifying useful structures like 'position' which allow other intents like 'identifying interim sets (between positions)' which act like bases for different change types and therefore different workflows ('position' is a useful base to start applying workflows at, which 'change position' or 'connect positions' once the positions are known)
        - for example, regarding the problem of 'partial spaces (such as a partial dimension)', the attributes that differ between different dimension types (impossible/undefined dimensions, lack of dimensions, imaginary dimensions, topological dimensions, set dimensions, integer dimensions, prime dimensions, etc) likely contain the variation necessary to find the set of differences possible in partial dimensions, such as a space of graphs created by attribute differences (like how 'prime', 'area', 'differences in angles' are allowed in integer dimensions but not necessarily allowed/defined/required in all dimension types), as creating a space of graphs with different sets of these attribute values is useful to identify sets of variables that can be represented in partial dimensions, where these 'interim graph spaces' between dimensions known to be limiting/surrounding/adjacent are useful as ways to find a space where a definition (like a definition of a partial dimension) can be allowed/required/defined/valid
            - 'finding spaces where all known definitions hold (continue to be valid and differentiable from different defined structures)' and 'finding position of spaces in relation to each other by similarity metrics (like conversion steps)' is a useful intent to find new spaces where problems can be solved
        - similarly, identifying a dimension where some attribute is reversed is useful to identify new possible variables, such as a dimension where irrationality is mapped directly to some other opposing variable, so that the filter separating irrational and rational numbers can be reversed completely (so the types are exactly indistinct/ambiguous, rather than exactly differentiable by that rationality evaluation function), or conversely, reflected/preserved in some way
        - relatedly, given that 'number types' map to useful attribute sets (abstract concepts), such as how 'primes' map to 'uniqueness', workflows can be constructed based on these number types, such as 'primatizing a problem space' by 'identifying all the unique differences which have the least in common except some unit', similar to how 'matrixes/sequences/progressions' can map to 'sequences' as a useful structure to base a workflow on and 'convergences/ranges' map to 'limits' and 'averages/logs' map to 'bases' and 'derivatives' map to 'changes'
        - similarly, identifying 'equivalent alternates' allows reducing that set to a point on a graph ('any value in the set, randomly selected' is indicated by that point on the graph) so connecting these 'equivalent alternate sets' with core functions like 'change/connect/differ/reduce/format/map' reduces most variation to "queries on a graph where nodes indicate 'equivalent alternate sets'" (queries that will likely reflect the connections between equivalent alternates themselves, so the graph or variants/subsets of it will likely be found in the set of equivalent alternates as well), and finding some function like a rotation that maps these equivalent alternate sets to each other is similarly useful, given their likelihood of being mappable if not equatable (there is still a set of multiple alternatives that cant be reduced, but there may be some graph where they can be represented as a point with different position related to other equivalent alternate sets, related by different angle sequences (or other structures which indicate the difference in the sets) applied to connect the sets or find some angle where the sets seem 1-to-1 mappable or otherwise equivalent, similarly identifying a graph with a 'common center' between equivalent alternates is useful as they can be represented by the center/radius rather than the sets)
        - relatedly, identifying a 'set of equivalent alternates (or similar alternatives or ambiguously similar alternatives or a set of structures that are different in some way that is not yet understood, such as differences in output of a test of each structure, thereby requiring the test)' is an intent that, when fulfilled, enables specific workflows like 'trial and error' to be applied, indicating that the set has not been filtered before and contains a new difference, therefore the test has to be different as well (solution metrics should be changed for each actually different/new problem, so the 'ways that solution metrics can change' indicate the 'ways that their mapped structures (new problems) can change/be solved')

    - identifying useful structures like insufficient structures for some problem-solving process like 'fulfilling a solution metric' and identifying differences from this insufficiency which is a type of suboptimality
        - identifying what is not sufficient for general problem-solving, like a set of structures like 'an adjacent function, an opposite function, and a reason to avoid adding other functions like "everything can be traversed with enough adjacent or opposite changes"' which has a known/identifiable error of 'requiring many iterations to find all relevant structures' and 'missing all the changes that are in between adjacent/opposite changes without iterating every possibility (and other errors like having patterns of movement that miss other patterns of structures without iterating every possibility)', is useful as these changes are useful for solving problems without testing every possibility, so its clearly insufficient for solving problems in general, even if adjacent/opposite structures can compose other structures eventually
            - to offset these errors, a 'structure of relevance like meaning/usefulness/intent' is useful (or an alternative structure similarly useful in deriving this, like 'interfaces' or 'vertexes' or 'concepts')
            - relatedly, making this problem specific, such as 'can these change types like vectors, rotated at these possible angles, traverse every point on a structure (at their endpoints), points of a certain size making it a finite number of points' leads to identifying more specific interim structures, like:
                - 'identifying a variant of the problem that is more provable and applies to/covers the original problem as the original problem is a subset or variation with irrelevant differences that dont determine the angle/missed point difference'
                    - 'identifying "areas/centers/patterns/types/simplicities/units/difference ratios/other similarity structures" of problem variants that are guaranteed to be traversible at all points, and navigate in the interim spaces of problem variants to prove at least one uncertain example'
                        - 'identify the "first problem where the answer becomes unclear" or the "first set of multiple unclear problems with different solutions sufficient to find variables of the solutions" or "identify a solved unit problem and a solved more complex but more solvable problem and apply variables to their solutions to identify an interim complexity solution"'
                    - 'identifying applicable problems to solve other problems (when this problem is solved, this other problem will also be solved or more solvable)'
                        - 'identifying a simpler but equivalent variant which reflects the same variable connections at a smaller scale or otherwise simplified'
                - 'identifying patterns in angle sequences and the points where they begin/stop differing or increase/decrease in difference from repetitions of known sequences'
                - 'identifying what is not required for solution success (traversing a new point every angle) and what is required (traversing the full set of points, traversing with enough regularity that it can be completed before the differences run out)'
                    - the important variable is the 'comparison' of the difference in 'regularity/interval of the new points' and 'differences allowed/required by remaining possible sequences', which need to be in a specific ratio to succeed (one needs to be greater than the other)
                    - once a 'ratio between relevant differences, such as opposing differences, where one of the differences would be useful if it exceeds the other' is identified, the problem is often solved (a 'ratio representing a difference in a similarity of the similar opposing forces')
                    - a related intent that can generate this same result is:
                        - 'identifying the determining variables and their related intents ('find the first missed point', or alternately 'traverse new points every iteration, or regularly enough that all points will definitely be traversed, without a decrease in this interval that would converge at less than the number of points') and the determining variables of those intents (where the possible angles can produce enough differences to differ from repeating the same pattern, thereby covering new points every time, but also enough similarity to stay within the structure'
                        - similarly, a comparison in the 'interval of new points' and a 'decrease in the interval that violates some convergence' is another set of useful structures to compare
                        - this is bc a useful structure (a 'sequence with a seemingly useful interval of new points') has been identified, so identifying whether that structure is invalidated by some opposing structure (like limits on next possible sequences that make it required to not traverse every point) is useful to identify if that structure can be iterated until the full set is traversed
                        - relatedly, identifying structures/variables that create 'requirements to miss points' are useful to identify, similarly identifying 'sets of opposing structures in the problem space' are useful to identify, to check as a default set of 'possible solution components'
                - 'identifying structures of additive progress toward the solution, such as different sequences that traverse a lot of new points at an increasing interval with some sequence' is similarly useful
                    - similarly, 'generators of differences between sequences that vary in their traversal ratio' are similarly useful to identify
                    - comparing 'sequences with different ratios of new points traversed' is another useful structure set to compare
                - 'identifying a "trial and error" iteration number of angle sequences where the answer would likely be clear, aiming below that range, and finding sets of steps that fulfill the lower ranges'
                - 'identifying sequences of angles (generatable with the possible angles or not) that miss a point on the structure and checking for repetition of these point-missing structures and repetition across the points that are missed and difference from those generatable with possible angles, or alternately identifying points that are regularly missed across sequences generatable with the possible angles'
                    - 'identifying examples of possible angles (and angle sequences generatable with those) that would and would not produce a missed point and determining relevant differences to those examples'
                    - 'identifying possible sequences, then out of those, identifying required sequences of angles to traverse a point missed by previous sequences'
                        - 'identifying whether there is always an opposing point-traversing sequence possible for every point-missing sequence, as the variation generating both sets is equal, and whether these sequences are possible to connect (once the point-missing sequence occurs, can the point-traversing sequence that opposes it also occur)' ('is there a solution structure to oppose every problem structure')
                - 'identifying self-interactivity/similarity of various probable sequences (how often it intersects with its previous points) which is a determining variable of how it can differ from its previous points'
                - 'identifying relevance of other variables like the traversed structure variables and the starting point (does it change probable sequence attributes or missed points)'
                - 'identifying thresholds in adding variables where the problem becomes unclear, and solving for sets of variables, and the function to merge solutions in these sets'
                - 'identifying co-occurring and mutually exclusive sequences' as a useful intent for other intents
                - 'identifying the point where, if a sequence can miss a point infinite number of times, indefinitely, without any identifiable correcting structures to oppose it, what point does that infinite capacity for missing a point become clear'
                - this example problem is just specific to direct focus but general enough to identify/allow variables
        - relatedly, identifying problems like 'determining whats inside/outside a box' that reflect enough reality and enough variation to be useful to derive other functions (like identifying possible signs of change in higher dimensions by determining signs of change in available dimensions like 3-d signs in 2-d)
        - relatedly, identifying 'reversibilities' as a useful base function of a network to test other structures in like 'assumption sets leading to errors that are dead-ends (irreversibilities) by first identifying those errors so they can be avoided' and 'similarity set that is reversible' (like moving in specific ways to avoid known dead-ends like assumptions that lead to removing info that cant be retrieved or deadly errors that invalidate all functions or avoiding over-similarizing with low-info functions)
        - relatedly, identifying a 'standard-standardizer' to identify 'standards that should be similar/different or which should fulfill some standard' are useful to identify, such as 'double standards (that should be equal)' 
        - relatedly, identifying 'missing connections, which probably exist but are not known (like the full set of interactions of a structure with other structures in its interaction level)' as useful targets for explanatory variables of processes that are similarly incomplete or not understood

    - identify useful structures like variables of 'filters which reduce computation requirements' are useful to identify as optimizations of a solution-finding method
        - similarly, other filters include 'compounds that interact with all examples of the condition (or compounds that are required)', as such a connection is unlikely to be coincidental/random, and 'compounds which are inhibitable' which are more useful targets to identify, as there is already a function that can be applied to make that a solution, so they have reduced computation requirements
            - these types of variables may be embedded in or missing from the data set, such as how 'interactive compounds with every data point (in a data set of compound structures and some useful attribute associated with them)' may be reflected in some interim variable of the changes in between the input/output, or it may occur after the output is determined
            - relatedly, identifying '1-to-1 mappings' as structures unlikely to be random is useful to identify probably useful structures to apply when decomposing a non-random data set
            - relatedly, identifying the function types/differences (like how a random function is usually useful in some position in a function network to generate that function given the noise usually present in data or in real systems or in changing variable interactions) that normally interact to build 'functions in between the set of all simple/pattern-compliant and random functions'
        - similarly, identifying 'subsets of the data set that generate similar functions, which are shifted transforms of each other' is likely when the probable solution range is trivial to identify and reflects the shape of the average/solution function, meaning the upper/lower bound shapes are similar (a stack of similarly shaped functions, which reduces the problem to finding the limits and average of that range, as the solution function is just the average of the other similar functions when the average is the definition of the regression line)
            - this is possible by identifying one example of a function connecting points in local subsets and then looking for similar functions following that pattern after applying a transform like scale/shift
            - by contrast, when the upper/lower bounds are different, or when the solution range isnt an area at every point but has some subset where it has only one point (every function in that area must cross that point), there wont be as many possible similarly shifted functions connecting subsets of the data set, and fewer points will be required to find those connections if they exist
            - these represent cases where the solution is identifiable by checking for these specific cases encoding some similarity/difference and once identified, can be computed with a specific algorithm to identify the limit of a similarity/difference (can it be iterated or does it have a limit within the probable solution range)
            - 'overlaps between functions to identify/filter different cases' are useful to identify to reduce overall computations
            - these cases have features like 'reduced computation requirements' where a similarity/difference is applied locally and globally, or in some pattern, or on another vertex that reduces computation requirements bc of the similarity/difference encoded which applies across those vertex or interface variables
                - relatedly, identifying other case types, such as 'when a solution function doesnt intersect with the data set', is useful, as there are often indications of this case that involve less computation, such as the 'variation of points around but not intersecting with some line like an average'
                - identifying the function to identify info required to identify most different cases is also useful to identify
        - identifying other cross-interface cross-sections of a problem set is useful, such as how 'function similarities' are useful to identify as '1-to-1 mappings' (such as how a function whose points are more distributed is mappable to that function), which are likelier to be quantum entangle-able and therefore likelier to be quantum entangled (at what point in the mappings would an entanglement break down, with which error types or similarity/difference types/sequences/other structures), and how strong are these connections across similarities (are there stronger 1-to-1 mappings than other 1-to-1 mappings, do they exert any force like 'changing probable adjacent functions of a function' that differs such as a 'hub function that is repeated more by being more connective/interactive', is there a useful 'map sequence to default to' in order to retain the most info when some connections are broken), what impact will agent-driven entanglements have on known variable interactions (how variables can be combined), what connections should be returned to for a function with missing info in order to be changed by variables so that the function can be differentiated again
        - relatedly, identifying the 'most reduced set of solution components' vs. the 'most reduced set of base/interim solutions' vs. the 'most reduced set of solution variables' vs. the 'most reduced set of solution filters' are different intents that are equivalent alternates but still useful to identify the full set of and variables of and other interface structures of, bc these are unlikely to remain in their 'most reduced state' as interaction levels change, as theyre abstract types of structures which will still be relevant regardless of the interaction level but adding to them will still be useful
            - similarly, identifying future versions of a solution component will likely be adjacent/computible from known solution components and interface structures of them, so 'applying changes within these types to generate newer higher variation examples of the type' is a solution automation workflow (rather than 'only identifying new workflows by connecting these variables across types, when required or adjacent') just like 'connecting these new variables to other types to identify new variables in other types' is a workflow, to apply a new interaction level of changes across types
            - this is bc it will likely be useful to identify new solution components by identifying/applying change patterns and other structures of solution components (like 'difference ratios encoded in a solution component'), and is possible bc these types are likely to be unlimited in the variation they can contain given their abstraction and permanent relevance
        - relatedly, identifying all the interactions which are likely/unlikely to be useful, such as 'embedding find queries' rather than 'sequencing find queries' and 'avoiding simple patterns like alternating sequences except where high variation structures are being varied/alternated' and 'identifying the few useful structure interactions that are useless, which is less likely and more filterable' and 'most methods of minimizing cost are likely to create future additional costs' and other probabilities is useful as a 'probably/approximately/commonly correct rule network' to apply as a 'base network for future solutions'
            - relatedly, as mentioned elsewhere, identifying 'probabilities as related to commonalities/ratios' is useful for 'filtering' intents bc identifying 'improbable (as in less common) subsets' (such as 'useless combinations of useful structures') is useful as a reduced set to filter
        - similarly, identifying 'info that would be identified anyway (such as adjacent or required info), if another function was identified/implemented/applied' is useful to identify in order to avoid identifying all info or re-identifying info
        - relatedly, the workflow 'change a base solution' is more useful with specific variable types (like 'trivial changes', or 'embedded variables' which are 'required' and also which 'require the base in order to be applied')
            - why are 'adjacent/required variables' so often alternates? bc they both have an element of 'forced interaction'

    - identifying useful structures with useful functions (like 'find a way to increase usefulness of an error/suboptimal structure') applied to useful structures (like 'right angles') is useful to identify
        - for example, the 'right angle' is a specific useful structure on the plane created by 'all angles intersecting at a common endpoint', just like '1-to-1 mappings' are sets of 'linear/simple/useful connections between different structures' and other specific useful structures on other networks/planes are identifiable
        - the 'right angle' can be identified as an error structure in some positions, where it connects overly similar variables (and is therefore not useful or an error, such as by being overly simple), but otherwise is useful for identifying obvious differences, independence intersections, etc
        - similarly, the right angle can be made more useful, such as by connecting two planes or high variation variables like concepts, in which case it becomes a very useful structure despite its simplicity (a vertex between two perspectives which connects them)
        - identifying changes that make these specific useful structures interact, such as by overlapping at a common center, makes it possible to connect them (such as identifying all the specific useful structures in a plane and then connecting them to useful structures on other planes, by aligning or orthogonalizing the planes)
        - the 'interactivity' of the structures is what makes them more useful (as in more comparable or more connectible as in sequence-able)
        - 'making a problem/solution interact (such as by overlapping or intersecting them)' is similarly a useful solution automation workflow, that is derived from this thought process about 'error structures like 2-d variable sets', 'variables of 2-d variable sets like over-simplifications to generate all the 2-d variable sets', 'positions in the set of all 2-d variable sets that could be non-errors', 'patterns/similarities in this set of non-error positions', 'specific 2-d variables that can be useful', 'right angles', 'ways that right angles could be errors (they are 2-d and can be over-simplifications)', 'ways that right angles could be non-errors (they connect to other variables like their common endpoints, magnitudes, etc, and specifically when used to connect planes/networks/other high variation variables, which is complex enough that it might be adjacently usable to solve problems)', 'maximally different variants of right angles (such as 1-to-1 mappings, a similarly useful structure in the map interface)', 'useful variants of right angles', 'useful connections between useful variants of right angles (alignments/overlaps)', and 'useful functions creating those connections and what positions those functions would otherwise be useful in (intersect)' (similar to how any line can be useful in the regression problem space if it intersects with the data set range bc it reveals useful info about the optimal function even if its very incorrect)
        - relatedly, a 'network of simple changes' should be positioned near a solution-determining function, to catch all the approximate/adjacent solutions that might have been reached by some suboptimal algorithm (rather than missing them), as its useful to determine structures around a solution like 'solution metrics', 'solution generators', 'adjacent solutions', 'solution descriptions', and 'simple changes that can convert a suboptimality into a solution function', a network of structures which may be high variation enough that a corresponding network around a problem might not be necessary
        - similarly, identifying/applying 'errors of perception' created by 'neuron limits' (such as how the 'vision' function optimizes for some metrics such as 'summaries, which merge adjacent structures') and related structures like 'adjacencies/overlaps', such as how a structure can seem 'merged (like a line) when distant' but 'close up it is clearly separable/distinct', are useful to identify (similarly other variants of this are useful to generate, such as when very different structures are adjacent/distant, do they seem different/similar, and when very similar structures are adjacent/distant, do they seem different/similar, and whats the distance where all structures seem the same, and what difference in distance is often useful to differentiate these cases, etc and the same for other variables that can create 'errors of perception' like angles/blur, as mentioned elsewhere)

    - identifying variations of core structures (re-examining core structures to identify alternate variations of them which are useful, as is likely) such as 'variations of vertexes' or 'variations of factors/products' or 'variations of averages' and the 'overlap across these variations' as a similarity which leads to relevance of these structures (the usefulness of finding variants of averages like vertexes, and variations of those variants of averages) is useful for identifying their interactions with other structures, given the variation they encode (vertexes encode adjacent transforms from a common point, so they can represent a simpler variant/format of a function)
        - for example, 'identifying vertexes' in a data set is 'identifying the common points between point sets' which is similar to the intent of 'identifying averages'
        - what does it mean that a set of right-angle vertexes (where two vectors are joined by a right angle) can explain most data set interactions, if such a pattern is found? 
            - it means that those two dimensions of the vector magnitudes explain the variation from the vertex common points (the point that the two vectors have in common), where the 'vertex common points' form an alternate similar function as the solution function, with two parameters of variation away from the solution function
            - its useful to know a simpler function that can be converted into the more complex function with a similar change, as the simpler function seems like a useful base function given its simplicity, where other resulting complex functions can be generated from it which are probable variants of variable interactions
            - this intent of 'finding vertexes connecting points' removes an 'independence' assumption from structures like 'sets of input/output sets' but that is reflected by reality as well (adjacent sets can sometimes influence each other in reality, especially in non-volatile systems where both input/output are adjacent, and it is likely to find systems where adjacent points are used to generate other points around it, similar to how identifying averages is useful in that points might tend to gravitate around the average to some radius, so finding 'similar angles connecting points' is not a false relevance structure in all cases)
        - 'variants of vertexes' like 'a force, an opposing force, and a smaller opposing force of the opposing force' are similarly useful to identify, as larger components of interface queries
        - similarly, 'vertexes' act like 'filters', in that the selection of the 'common point' and the 'angle' and the 'magnitude of the second vector' describe all possible interactions between two vectors with a point in common (and thereby identify all possible opposing forces and possible connections between those opposing forces and the original forces, converting it back into the original force), and so these selections determine the selectivity of the vertex in what interactions it can describe
            - a 'right-angle reality (where all connections are orthogonal)' is over-simplistic and doesnt adjacently allow for other descriptions of connections, but applying similarities (like right angles) to find other similarities (like angle patterns such as 'repeated right angles' or variations in angle patterns such as 'variations around a right angle') can be useful
            - vertexes are like filters in that they allow all possible connections/similarities in some angle connecting similar/comparable objects, and so do filters (applying some change to reduce a structure, like an opposing vector in a vertex of two vectors)
        - a 'vertex between simpler/complex function spaces which connects them' is likely to be a useful vertex in identifying functions in the uncertainty space between known limits (like 'avoiding randomness/linearity')
        - a 'line/function/vector set across these function spaces' might be a more useful format of a function than other formats
        - a 'sequence of these function spaces' might be possible/useful in determining ways to reduce the interim space of uncertainty between randomness/linearity, though 'selecting/identifying the simplicity space' is not trivial (which simpler functions are generated and connected to which complex functions)
        - identifying 'reasonable vertexes' (such as 'reasons for a lie and reasons to oppose the lie' as indicated in 'vertex_variations.drawio.svg') are useful to identify as structures of falsehood to check for in variable interactions and similarly 'similarity vertexes to truths' (such as the similarity of a lie and a truth, such as an adjacency or intersection) are useful to identify, and similarly, 'similarity vertexes to concepts' (like how a lie might be similar to 'multiple/variation' or other conceptual structures associated with truth) are useful to identify
        - these 'useful vertex sequences' are similar to how a 'cross-section of a structure that intersects with most of its variables' is a useful structure to describe/determine the structure, so finding 'sequences of vertexes' that are 'likely to intersect with variables' or 'likely to intersect with cross-sections of a complex structure' is useful, just like 'identifying a non-one dimensional slice (of height greater than zero) of a function is determining of that function' and how 'identifying sequences of vertexes that lead to false structures (like irrelevant connections between small subsets that are too simple to reflect real complex structures which leave out the bigger, truer variables)'
        - similarly, its possible to identify useful filters of similar structures, like how the 'non-overlapping sections of equivalent alternates' are useful to identify their differences (as those are the primary difference in overlapping sets), so finding non-overlapping structures is a possible difference to look for when 'finding differences, to find similarities supporting or connected to those differences' or 'find new differences in a structure that is not clearly equivalent alternates, to check for that'
        - similarly, identifying the differences possible with different selected cross-sections is useful to identify like identifying differences possible with different selected subsets is useful

    - identify useful structures like overlaps between useful structures like workflows and connect them to workflows (such as the variants of variables of these workflows that have useful connections to identify (insights)) and connect them to 'new graphs' so the full impact/meaning of the new useful structure is identified
        - for example, given that 'anti-fungals frequently also have anti-cancer activity or activity against other negative compounds', it can be inferred that compounds with 'broad activity against many compounds' are useful to identify as possible solutions to other problems
            - this is inferrable using a workflow like 'identify structures similar to problems/solutions and connect those instead (find fuzzy/wobbly connections, which is useful bc of the similarities at either endpoint of problems/solutions)'
            - similarly, as mentioned elsewhere, other compounds can be inferred using this workflow, such as 'poisonous substances which kill host cells can be useful against negative cells like cancer cells' and 'tumor markers as tumor inhibitor treatments' (given their adjacence to negative/problem structures like tumors) and 'compound that have complex regulatory functions (like fixing inflammation or circulation or demyelination or calcification or metabolism) are likely to have other useful complex regulatory functions' and similarly 'compounds that treat side effects of cancer as possible alternate treatments' or 'compounds that interact with known important variables like inflammation variables such as NF-κB' or 'compounds like anxiolytics that treat inputs like stress of other conditions' or 'side effects of cancer treatments as alternate treatment targets', and 'compounds which treat conditions like demyelination/high cholesterol involving the same processes like phosphorylation or pathways like Wnt pathway' (bc 'fixing errors like deregulation in overlapping processes' is likely to be useful for fixing errors in other processes, so finding 'overlapping problems/solutions or their connections/components/variables' is useful)
            - its also inferrable using a workflow like 'change a base solution (such as a known medicine like an antifungal and see if it works against other problems)'
        - identifying the 'overlap' of workflows like 'find fuzzy connections between problems/solutions' and 'change a base solution' is possible bc they have a similarity in that they both involve changing problem/solution structures in different ways for different intents
        - this is also bc compounds with activity against one type of problem are also likely to have activity against another type of problem bc the original host of the compound is clearly capable of changing to solve problems as it contains one solution and is likely to have encountered other problems, which is the 'probability of a similarity' in 'containing other examples of a type (like counteractive compounds)' and 'interacting with other examples of a type (like problems)', so applying 'other examples of a type' is a useful similarity to apply across structures
        - similarly, compounds with 'specificity/selectivity of activity' are useful for targeting one negative factor to remove/reduce, when such a condition occurs, which is more rare but occasional is very useful, like where a condition has just been triggered and hasnt reached broad/systemic activity yet
        - this indicates that there is a 'useful mechanism' at either end of a 'high variation spectrum variable' (extremes typically result in specificity/specialization/differentiation, and extremes in a high variation variable typically result in equivalent possible functionality)
        - the connection between specificity and differentiation is useful to identify, just like identifying the input of extremes as useful to create other differences like specificity/differentiation is useful
        - this is also useful for identifying interim spaces where other useful compounds could be, such as the space between extremely broad-acting substances and extremely specific substances and further filtering this space by navigating the space between compounds that bind in known ways which invalidate them, like being required for another useful process so they cant be candidate treatments or possible substances to avoid, and similarly finding the overlaps in these spaces as more probable spaces that contain solutions
        - graphs where compounds with an attribute are positioned adjacently or in an identifiable similarity like a pattern are useful for finding 'types' and '1-to-1 mappings with other variables aligning/overlapping with those attribute values' (where another attribute also occurs in that adjacent subset)

    - identifying common structures of queries that identify useful structures is useful, such as applying a different info structure set (like 'barriers' and 'false') to identify what is not real (such as 'false connections')
        - for example, the structure of a 'incentivized filter' is attainable by identifying 'info barriers that benefit some agent', such as if there is an 'info network' which agents might find useful to know so they can query it and blocking access so that connections cant be made on this network without paying a fee is an info barrier that would benefit an agent, acting like an info filter that is incentivized
        - similarly, thinking about alternate ways to connect data set points is relevant to connecting the reasons for a point with a structure (such as how road would be a reason for a distribution of points like lights, which is adding another dimension), and this is not particularly different from other methods of identifying new connections between related points, so what is different is identifying structures of false light, such as how far away light needs to be measured differently, taking distance into account, bc its 'received as being in a position after its in that position', so by the time its measured to be in a position, that position is false info, which is useful to identify, and similarly, light can reflect in a way that is false, so a 'reflection of light' might be being measured rather than light itself and similarly a data point might only be in that position bc it encountered a barrier elsewhere, which is useful to identify
        - by applying structures like 'barriers' and 'false' and 'incentives' to standard variables like 'data point connections' and 'reasons for those connections' and 'differences from other connections', alternate methods of regression can be identified (filtering out false info, identifying highly causative variables like barriers, identifying incentivized info and info structures, etc)
        - relatedly, identifying the connection between core structures like 'barriers' and 'filters' is useful, such as how a 'barrier is a component of a filter' and 'can act as a filter'

    - identifying useful structures like 'common structures across solutions' is more trivial when applying 'self-contradicting structures in a high-variation spectrum variable like an interface' (like 'simplifications' of 'complex' variables) at which point identifying other useful structures like 'the commmon usefulness of constant indexes in simplifying complex problems' is trivial and similarly 'identifying variations and variables' of these structures ('variations of constant indexes which might be useful for some intent') are similarly trivial
        - for example, the common structure of a 'solution metric' (used as a standard to filter solutions) and a 'common pattern' (like the difference between 'easy/difficult paths and high input-high output and low-input high output') are paired as 'filtering standards', once 'differences from constant indexes' such as 'a virtue network (where different directions indicate different virtues)' or a 'difference of a reality variant from known reality' or a 'base solution', which are a specific vertex structure ('differences from an index' and 'filtering standards') that is useful across problems
        - I thought of this by focusing on connecting simple structure with complex structures, after thinking about 'variable count' (a low-info but still determining variable some of the time) recently, and wondering 'what was the minimum/simplest number of variables needed to format complex structures, like "most solutions to most problems", arriving at a default brain limit of three variables as a useful example value to test with, wondering if I could format most solutions with three variables, then wondering what 'complexity of structures' was necessary to format most solutions (like a reduced set like a 'specification, a modification, and an exception' which are common sentence types, or a 'story (a query on a reality network), civilization (a reality-variant network where queries take place), and a concept (high variation structure explaining/relating to many/most variable interactions'), and noticing that 'constant indexes' are useful across problems and most solutions involve 'computing a difference from these indexes' (such as a 'base solution' or a 'specific network' like a 'virtue network') and then applying a standard to find a difference/similarity in some related structure (like a 'simple/complex' step on a 'virtue network'), as a useful pair of structures to apply together, similar to generate/filter (bc standards act like filters)
        - then after wondering what the connection was between 'simple/complex paths' (such as a 'right angle with sides of different lengths') and the difference between 'low-high/high-high input/output connections' (such as 'incentives'), if any, as they seem related by a common structure which is an 'angle between structures having a common endpoint' which was a structurality similarity identified by coincidence after thinking about a useful 'index/standard pair', identifying 'high degree-angles (differences) between related structures (similarities)' (having a common endpoint like a common type or common starting point) seems to be the structure relevant to problem-solving that is useful to identify, as a specific structure implementing a 'difference in a similarity', and indicating a possible '1-to-1 mapping' in between 'simple/complex' and 'low-high/high-high input/output connections' that is validated by the vector-formatted definition of those concepts (meaning when 'simple/complex' are formatted using vectors, they retain their difference angle, which is useful to identify 'sequences of formats' that would maintain some useful difference structure like an angle)
        - "finding an index/map and a 'common similarity/difference' from/in that map and its approximate 1-to-1 mapping forming a 'connection to a related high variation concept (like false/true)' or then finding an 'embedded similarity/difference' in those similarities/differences that determine the map" is a useful problem-solving intent to specify with vertexes (like specific interaction levels or systems relevant to the problem)
            - for example, identifying the virtue network as a useful index to run comparisons on, then identifying the difference in the virtue network between 'simple/complex values of the same virtue' (which map approximately 1-to-1 to a related concept applicable to the concept of the network, like 'false/true virtue'), then identifying the common standard embedded structure of those differences, such as the right angle between simple/complex values of the same virtue (incentives)
                - this is useful bc once the differences within/from an index are found/determined, finding 'mappings to high variation interface structures like concepts/structures/standards/differences' are useful and more trivial at that point, which is an example of a way to convert 'info queries to numbers' (using reference indexes like 'constant concept networks' and numerical structures like 'approximate 1-to-1 mappings' between concepts like 'simplicity/truth' or 'angles' between concepts like 'simple/complex')
                - similarly, applying indexes/spectrums/vertexes as 'constant nodes' in a neural network is useful where the nodes around them can vary by functions like 'embedding additional nodes' is useful as a way of representing structures like 'vacillations between concepts like organization/entropy', variables which are useful to model vacillations of because some variables relate to all variables, so that the borders of the network represent absolute limits of these reality-covering variables and queries of the network represent sequences of changes in these conceptual variables
                - similarly, other similar indexes could be useful, such as a vector set of high variation variables (where no mapping is preserved, so any vector moving outward from the origin indicates any high variation variable that is different from the others) and embedded variables indicating the differences across high variation variables, which could be connectible in similar patterns that would be made obvious by similarizing the high variation variables
            - similarly, identifying a base solution as a useful index to compute/apply differences from, then finding solution metrics within that computed difference to apply as a standard to further filter a solution, is a simpler application of this structure
            - similarly, 'differences from the concept network' are similarly useful to identify, and similarly applying variants of these networks, like 'sequences/stacks/sets of concepts and their embedded variables' (as opposed to 'maximally different vector angles from the same origin') and 'variables of their interactions and usefulness for determining useful interface queries and other useful structures' is similarly useful to identify/apply
        - as to the question of whether vertexes can be used in a sequence to optimal degree, such as whether 'finding a commonality, applying high angle differences based on that commonality, and iterating' is useful on its own or requires specifications like 'avoiding intersecting with its original commonality' or 'maintaining a 1-to-1 mapping' or 'intersecting with some constant index regularly to stay relevant to some specific system/problem'
        - relatedly, identifying all the light interactions that can make some truth seem false are another example of a useful constant index to apply, such as how light can dissipate light to make it seem like it doesnt exist, light can seem like the source of light by applying filters to obscure/hide the source of light, so all of these interactions should be checked for
        - relatedly, taking out obvious info (like removing averages/densities from a data set) is useful for various intents like identifying low-variation differences like alternate functions, as 'making something obvious by focusing on it, to make other structures seem relatively false by comparison' is a way to hide other types of light (the less obvious structures), so its useful to apply as an interaction in 'directly mappable problem spaces' like 'regression' as well (answering the question 'when obvious strctures (which could be false, as a way of hiding other info) are removed, what is obvious then?')

    - identifying the ways the core concept determining the 'usefulness of a particular workflow' can also indicate the 'usefulness of different workflows' is useful to optimize workflow filtering/generation functions (identifying ways that it could be false/true, useful/useless, etc)
        - for example, some problem spaces are so random (which usually means 'incompletely understood') which usually implies that 'trial and error' would be useful, that 'brute-forcing a problem' by pouring enough computation/resources to  generate/identify/test every possibility (trial and error) is not likely to succeed or even be possible, bc the generative variables of these possibilities are incorrect, as the problem space is not understood so it falsely seems random (real randomness rarely occurring in real systems, as 'equivalent alternates' dont often exist for long, as there is usually a filter selecting some alternative that is more useful in some metric and equivalents are usually differentiated/specialized at some point), which sounds like its impossible until the implications of 'randomness' are understood
        - therefore even though the concept of 'randomness' is determining of the usefulness of 'trial and error', it can also be a reason not to use that workflow
        - similarly, even though the 'adjacency' of an existing solution to a probable optimal solution (given an identifiable solution metric allowing a probable solution range to be identified) is a reason to select 'change a base solution' as a workflow, the 'adjacency' is also a reason not to use it, in cases where the similarity of the adjacency is false or can be changed
        - this is possible bc every sufficiently abstract concept contains enough variables that it can contradict/limit/invalidate/filter itself, so finding these ways and applying them where that concept is used like in "workflow filtering/generation functions" is useful

    - identifying a connection between 'similarities' and 'reduced sets' is useful to identify different structures that can connect them, such as structures like 'averages' of related concepts like 'probability' which are useful at reducing sets given their coverage
        - for example, why is a 'probable solution range' possible to identify and why is it useful? bc it represents a 'similarity' which can create a 'reduced set that still has multiple items (rather than one)' that fulfills the concept of probability (a filtered subset that is more probable bc the obvious errors have been removed but there are still multiple solutions in the set, making it a probable set rather than a totally filtered set leaving one possible solution, so its possible but not guaranteed, making it probable)
            - other implementations of 'probability' applied to identify solutions could be similarities between subsets based on 'frequency' or 'adjacency' or 'average' or some other structure (rather than applying the structure of 'area' as a structure of probability indicating a 'reduced set of multiple possibilities')
            - similarly, its useful to identify a range bc solutions often dont vary so much that a range would be useless
            - relatedly, combining these is useful to identify interactions of structures that implement/determine probability
        - this means "because probability can take the form of an 'area'", its useful to identify 'probable reduced sets in the form of areas' (identifying a 'probable solution range/area' as a solution format)

    - identifying useful structures such as 'overlapping structures with a common base' which are useful for identifying 'variables' of these structures (like 'variables' of 'similarities') and apply these structures to useful structures like 'high variation variables like interfaces' (and applications of interfaces, like 'causal meaning of a similarity')
        - for example, the 'causal meaning of a similarity' could be that one structure causes the other adjacently/directly, therefore they are similar
        - other 'meanings of a similarity' could be that one of the structures is 'default/common/adjacent/required/useful and therefore there will be similar structures to that structure
        - the 'potential meaning of a similarity' is the set of possible interactions of that similarity with other structures, up to its limits such as its 'uniqueness limit' (how unique its interactions are compared to other interactions)
        - the 'ultimate meaning of a similarity' is the set of overlapping meanings, where their common center/limits can be identified, and therefore the variables of this set of meanings can be identified, since these meanings will overlap on some attributes but will have a common base, similar to how 'reality-covering concepts' overlap but have a common base, and therefore cover most if not all of reality
        - these 'iterations of interface structures' are particularly useful to identify similarities/differences in bc of their composable functionality where they can be combined to create interface queries connecting maximally different structures like problems/solutions

    - identifying useful structures like 'alternates' (such as how 'overlapping different types' can be an alternate of 'inputs that can be outputs and vice versa, causally') by identifying useful insights and then identifying useful structures to apply those insights
        - for example, identifying that an output could be an input could be identified by other functions/intents, such as 'check for symmetries (by rotating outputs around a variable to see if theyre also inputs)' or 'check for multiple type sets that a structure fulfills (rather than one type)'
        - identifying insights like 'inputs/outputs can often be switched causally and otherwise' is useful to identify alternate structures that can replace that info by applying other structures like 'symmetries/multiple/type'
        - inputs/outputs have an 'overlap' in some variables like 'cause' (bc of multiple possible configurations where inputs/outputs can be equal/unchanged/preserved in a function or where outputs can also inputs by some other process creating those outputs which are used by the original function, and different input/output structures like sequences/cycles), even though they are extreme opposites in the variable of 'adjacently associated structures of a function'
        - so by avoiding assuming 'one type' vs. 'multiple types', an error of 'assuming outputs cant be useful as inputs and vice versa' can be avoided even without identifying the insight
        - similarly, identifying absolute truth structures is sometimes possible, just by identifying ratios or thresholds, such as where if 'reasons having a count above a threshold n' can be identified to indicate some statement is true, that usually is enough to infer if its true (as 'reasons to think a statement is true' can increase in a simple variable like the 'count of reasons', as the 'truth' becomes more stable and therefore more depended on and more necessary to maintain), though its useful to identify cases where 'false statements have more reasons to indicate theyre true than a true statement, such as where falsehoods are useful for a larger group'
            - similarly, identifying structures which require/guarantee/imply truth/falsehood are useful to identify, such as how if a statement is locally true (like a one-degree connection), then fitting it into a system of other local truths makes it more true
                - similarly, identifying whether a statement aligns with iterated inputs/outputs of that truth (its implications or requirements) makes it more true
                - similarly, identifying whether a statement aligns with a core abstract connection makes it more true
                - similarly, a statement without filters/limits/specifications/conditions/requirements is unlikely to be true, as almost every statement has a falsifiability (a way that it can be made false)
                - similarly, a statement that fulfills multiple of these structural truth metrics is less likely to be a false or adjacently falsifiable structure
                    - falsifying it, meaning 'making it so rare as to be nonexistent' or 'finding a far more determining variable like an interface (such as how mutations seem like a determining true variable, until the far more determining variables of "DNA repair" or "neutralizing/compounding genes" are identified, so that the original variable seems relatively false, given this far more determining possibly orthogonal variable which can contain/embed it'
                - however, identifying 'false structures of truth (ways that an apparently true statement can be false)' is similarly useful, such as how identifying 'variables in between an apparent similarity/connection' is useful and can contradict the similarity/connection, indicating other possible causes in between that are causally separable from the original connection
        - relatedly, 'identify' is related to 'determine', and therefore 'determining variables' are related to 'filter' intents, such as how 'more determining' variables like 'powerful variables' make better filters in the sense of filtering a set more reductively, leaving fewer options
            - identifying 'types' from the 'overlaps/similarities of different filters' is also useful for identifying what 'type' of structure (such as more 'common' vs. 'rare' structures, where useful structures like ratios hold that can benefit from some highly reductive filter) can be identified by what filter sets, which filter sets are likely to overlap and why (theyre at a stage of divergence/convergence or the filter is similarly required across systems), and related intents

    - identifying 'structures that align/connect across interfaces' is useful to find 'variants of useful structures' such as workflows (like the conceptual variant of a workflow, by applying the cross-interface structure between info/concepts)
        - for example, a possible solution to the problem of 'cancer' could be 'improving organ function, so that organs can more easily handle cancer drugs or fulfill more immune functions, rather than being damaged and disabled by drugs/immune functions' which is a solution of 'extending/increasing/optimizing/overlapping the functionality of every node so that it can connect more nodes independently without changing other nodes' which is similar to the workflow of 'changing position of nodes to make more useful paths accessible', both of which can be created as implementations of a more conceptual workflow like 'create luck' (luck being 'adjacent useful structures')
        - similarly, other workflows have similar 'conceptual variants' like how 'trial and error' is an implementation of the conceptual workflow 'create possibilities (and thereby creating an input to creating probabilities)', where 'creating probabilities' is implemented by worfklows like 'increase adjacence', 'increase connectivity', 'increase filtering', etc (so 'create possibilities (generate)' and 'create probabilities (filter)' are useful when applied in that sequence, which reflects the conceptual sequence as well, 'possibilities' being an input to 'probabilities')

    - identifying useful solution functions (like 'reduce/oppose (inequality in functionality) to similarize probability of winning (or reverse the odds by reversing the inequality to favor the host)' or 'reflect (in an opposing structure of the variation source, like the immune system) to similarize') to handle different problematic 'variation sources', which are a useful problem format to apply various problem-solving intents to (such as 'block the source of variation, if variation is the problem')
        - for example, a condition like 'cancer' is such a systemic illness that probably every variable/sequence/system (such as 'metabolism', 'filtration', etc) is connectible to it with trivial changes, therefore its useful to identify how other variables are connected in order to solve cancer, even if 'cancer-causing pathways' are not directly or ever identified, the 'other connections' are so important that solving the problems with other connections (such as how a system disruption can impact a related/adjacent system, where both systems are not directly related to cancer) may approximately/indirectly solve 'cancer' as well, bc problems cant exist in a vacuum and must be related to other problems, so solving 'all the problems around cancer (like identifying/fixing all sources of dysregulation, or solving the stacking/sequencing of dysregulations which are likelier to cause systemic illnesses or solving cross-system interaction problems to solve illnesses which can cross systems or solving "alignments across errors which favor dysregulation/other errors that can lead to systemic problems" or solving the "host/parasitic/foreign-substance identification problem" or solving the "functionality/resource ratio/distribution/sharing/equalizing problem so cancerous cells/pathogens cant borrow functionality in excess")' is likely to be approximately equal to solving the problem of 'cancer', as 'cancer' is unlikely to be such an extremely different/independent problem that solving it requires drastically different structures unrelated to other problems
        - similarly, identifying the variable interactions between 'high-high variation variables' (often the most important variables to identify connections between, like identifying how metabolism/mutations or changing/regulation are connected) and similarly identifying the connections between 'high-low variation variables' (how important variables are distributed across high variation variables and constants and how variation is connected to constants) can provide the other info required to connect other variables like variable combinations
        - these intents focus on different 'sources of variation' and different solutions to these 'variation sources' (such as how 'variation between functionality of cancer cells and functionality of host cells, should be reduced to solve that variation cause to make them more equal so the host can fight the cancer better (if the cancer has more functionality its likelier to win)' and 'variation between host/foreign substances should be similar to/reflected in immune memory or generality of antigen applicability') in systems so these are likely to be useful alternates

    - identify structures like 'equivalence structures like limits' that are useful for useful intents like 'identify alternates'
        - 'equivalent alternate structures in different structures like different systems or different input/output sequences' are useful to identify, such as where an input in one sequence can be equivalent to an input in a sequence with the equivalent function as the other sequence (the inputs are only equivalent as a possible treatment target, bc the sequences are equivalent, such as by having some resulting function and similar inputs/outputs and similar position), which are useful structures to identify as 'alternate sources of functionality'
        - similarly, 'inputs in the same sequence' are also useful 'equivalent alternates' but identifying all the 'structures of equivalence' between structures like sequences is useful to identify the other structures which can be used instead (such as 'treat dysregulation in adjacent pathways rather than inputs or inputs to equivalent sequences')
        - these 'similarity/difference' structures are useful to identify 'structures likelier to be equivalent' and 'structures like thresholds where the equivalence starts to become different', which are useful to identify
        - useful differences to identify involve cases where an 'adjacent' change (gene mutation) to a 'determining' variable (like a regulatory function using that gene) with a 'volatile' distribution especially with cases that extremify the situation, such as a 'function or structure with no alternates' (there is no alternate regulatory function with that specific usage/functionality/structure/impact), which is a difference structure that could easily cause negative differences (errors) in other structures, given the similarities/differences of these structures (given the 'broad activity' of the 'regulatory function', its especially important not to dysregulate it, but even more important if its volatile and if it has no alternatives that can be activated in its place and other extremifying variables)
            - similarly, some difference structures can be more probably relevant than not (such as how 'adjacent sequences' are likely to be relevant though not required to be), so 'applying differences to adjacent structures as the important regulatory function' could also disrupt the regulatory function
        - this is useful to identify the 'similarity/difference' structures that cause problems, as opposed to being neutral or useful/optimal in some way, which is sometimes possible with some info structures like these variables and their connective sequences

    - similarly, 'volatility' can be irrelevant in some cases (such as where the volatile reaction is not ever triggered/used or its at extreme values that are unlikely to be useful, or its at an interval that is not likely to be used as it overlaps with the opposite of the useful interval, or some other usage structure that invalidates the volatility's impact), which is useful to identify
    - identify possible 'opportunities for optimization' in useful structures (like 'vertexes') given other structures like 'probabilities of concepts like simplicity in those useful structures' (given that solutions arent usually so simple to find, that a simple set of two functions can cover all the complexity of real systems) and identify implementations of those structures which are still useful (such as simple) but reflect reality more completely (such as having an 'embedded' structure and a 'dimension change' and other interface structures)
        - if the assumptions that limit/determine a generator (like its 'assumed variables') are incorrect, the filters can only be so useful in identifying the solution function, since they cant update the generator, so ideally there would be filters applied to the generators (generators would be generated and the maximal negative impact of the errors of their assumptions identified, as well as the maximal corrective impact that a filter can have to correct these errors, so that generator/filter sets that would avoid these errors with the highest probability can be filtered and applied, rather than some adjacent generator to some assumption set)
           - this means the 'generate/filter' vertex has a more optimal variant, the 'filter-generator -> generator-error/filter-limit connection -> change-generator' set, which is more optimal to apply than just a generate/filter workflow
        - this is possible to optimize bc of the 'improbability that a given set of assumptions is correct' (there is a 'probability' generating this 'opportunity')
        - the 'improbability of a default selected set (like a set of assumptions) being correct' is trivial to identify, when identifying 'im/probabilities' of 'correctness/errors', 'errors' such as 'incompleteness' or 'unnecessities' being particularly useful to identify when identifying 'optimization opportunities'
        - the likelihood of a visualization being reflective of reality is similarly improbable, and also the likelihood of a visualization that differs from reality being better than reality at reflecting reality is similarly likely, given that some graphs are better at reflecting perspectives of reality than reality, so a mix of 'un/realistic structures (such as im/probabilities)' is likelier to reflect the 'mixes' (such as solutions/errors) often seen in reality, such as how a variable network doesnt reflect reality but is often better than reality at describing a subset of reality (as reality takes more info to describe than the graph), and similarly there is likely a 'set of these graphs' which are better than reality at reflecting all of reality, which is another 'opportunity for optimization' given the 'improbability of any one graph being complete/correct'

    - identifying structures that reflect useful interface structures in a problem space (usually structures applicable with core useful structures, like 'graphs of different variable interactions' and 'input/output sequence graphs') are useful to identify as reality-reflecting structures which can be trivially changed by interface structures to identify solutions/optimizations
        - for example, identifying 'fragilities of regulators (observed with frequent disruption/breakage of regulators or their inputs/requirements)', 'alternates that create a requirement/certainty of growth', 'variable differences that create opposite functions (between growth/regulate) and available inputs to these functions', 'overall growth-favoring ratio given the factors that can create growth', 'determining sets, where if any item in the set is present, regulation is disrupted or growth is favored', 'adjacent changes favoring growth (if methylation occurs, growth is favored)', 'higher availability of inputs to growth than to regulation', 'fundamental/core/required/default attribute of growth pathways like energy', 'lack of connectivity/communication between regulators', 'lack of filters inhibiting/filtering usage of growth outputs like energy' are all useful interface structures to identify with 'input/output sequences' (like 'growth/regulation pathways'), and 'similar alternates to regulators (like repair) and growth (like DNA copying errors)', and 'high variation variables whose interactions are not fully known like DNA structures like ecDNA which are likelier than other structures to cause variation in other structures like genes', and 'unrequired inputs of growth, such as dead ends/isolated growth structures or growth structures which have sufficient alternates, which are often more useful to inhibit than other structures as they have fewer side effects/functions/interactions', and 'equivalent alternates to growth inputs that can be used during treatment of over-growth', 'exceptions to growth functionality', 'common associated structures (like mRNA and tumor markers)', 'difference combinations (like compounding or adjacent mutations in a region), which when present, indicate a probable aligned difference reflected in related functions, like growth/regulation functionality', and 'difference/similarity structures that frequently indicate pathology (such as false similarity in identify to seem like a host cell and difference in energy routing target)'
        - connecting these structures on a normal input/output graph is useful but similarly, identifying the inputs/outputs to connect is useful as well, just like 'changing position of inputs/outputs' is useful
        - relatedly, structures like 'filters' are abstract in that they can be mapped to 'variables' and also other interface structures like 'patterns' (a filter using a variable is a more generative filter like 'filter odd numbers (by first identifying/generating them or identifying/generating what isnt odd)', where a filter using a pattern like 'filter every alternating item' is a more specific filter depending on the previous value)
             - filters are flexible in that they can be similarly useful if they filter one structure in a set of opposite structures (filter odd or even numbers, without having to identify both)
             - its useful to identify these connections between equivalent alternate functions like filter/generate bc the connections likely apply to other equivalent alternates and the connections are useful to find differences from these alternates
        - similarly, some graphs are useful to identify regarding the growth/regulation interface variable, such as the 'network of growth factors' and the 'network of regulation factors', where 'adjacency to growth functionality' is indicated by angle to indicate how frequently/adjacently its favored, and similarly a 'network of variable interactions where growth is indicated by some obvious structure when it occurs is useful to identify to indicate how often growth (or regulation/repair) occurs in the actual variable network
        - relatedly, the 'standards that allow maximal differences to exist and be compared' derive from 'references (numbers) of structural interactions (numerical interactions)' where numbers are abstract variants of 'structures like quantity' (the number 1 doesnt have a 1-to-1 mapping requiring it to be connected to structuures on other interfaces/concepts, but it can mean a 'measurable unit', 'something in between zero and infinity' or 'the first non-zero whole' or a number of alternate definitions, which is why each number can act like an abstraction or an abstract intersection of other definitions, some numbers being more adjacent to concepts and other structures more than other numbers, where if a program didnt identify 1 as a number to focus on, these and other interactions wouldnt necessarily be identified, so 1 would seem meaningless and equal to all other numbers rather than being significant to e and primes), such as 'euclidean dimension sets' (where a similarity between dimensions, such as a common center/base that can be an intersection, allows them to be compared in a maximally differentiating/clarifying way and enables other differences to be embedded on this standard in a similarly clear way, such as multiplication/area, rotations, opposites of visual default defined quantities like the root of 1, etc)
            - relatedly, 'different sets of generative functions' acts like different 'centers/bases of reality'
            - the question of 'whats the clearest way to differentiate some structure set' is a useful question to answer (use which space to base changes on in order to make other changes clear, such as standardized/normalized space, base space, gauge space, graph/network space, set space, sequence space, topological space, vector space, euclidean space, etc)

    - identifying useful structures like 'new interface structure interactions' (such as 'opposing' structures like 'exceptions' to an 'insight') is useful to identify other useful structures like 'connections between independence and interface structures (like unique filter sequences and info barriers)' and the 'connection between independence and complexity' to fulfill intents like 'identifying more complex connections' to 'resolve more complex problems (using independence-related structures)'
        - for example, identifying the insight that 'everything is connectible' is useful to identify 'structures of this insight' (like 'exceptions' and 'opposing structures') such as 'independence structures' (an 'opposing structure of an exception' to 'connectible structures'), which are not trivially connectible bc of info structures like 'info barriers/boundaries', 'non-adjacencies', 'discretizations', 'cycles/interfaces/other complexity structures', 'over-restrictive filters like unique filters', and 'over-similarities creating ambiguities', etc, all of which can make it more difficult to connect variables (or differentiate and then connect variables which is sometimes necessary), all of which are useful to identify as 'error structures to resolve' (to solve the problem of 'connecting all variables')
            - as a specific example, a 'unit of reality like a molecule' could be called a 'universe-generating machine' in that it has some default functions which would occur without prompting so it could be called a 'machine' and in that it is a component of universes so it could be said to 'generate universes', but its less true than other structures like a 'realistic model of reality such as interface analysis which incorporates understanding' in that it requires very 'specific filters to occur in a specific sequence' before the 'molecule' can be considered a 'real universe-generating machine', and one of the reasons why its more true is that "not every 'molecule' can be applied in reality to generate universes simultaneously, so theyre not all equally capable and would have to select one and then iterate/apply it in the right ways to avoid invalidating its ability to generate reality, such as by slowing down relative to other structures", whereas "only one copy of interface analysis can determine the universe", so these structures like insights such as 'not every unit can be applied in the same way, which means theyre not equivalent' are useful to identify as useful differentiators of 'relative truth', and similarly opposing exceptions to these are useful to identify, such as 'every molecule could be equally powerful, in cases such as where there is a way to specialize them all, or otherwise vary them all such as by integrating a high variation structure like "interface analysis (or a unit of it like vertexes)" into them all'
            - identifying 'sequence-independent variables' (variables that occur regardless of sequence, such as 'filters that are useful regardless of the sequence theyre applied in') are similarly useful to identify, as possible 'requirements/defaults' or alternate structures like 'sets' or 'powerful/useful structures that would occur regardless bc they enable other structures'
            - similarly, identifying structures which are connectible (like 'iterations of units' where 'units' and 'iterations' are the more 'connectible' structures as theyre more 'common/simple/available')
                - this is useful for intents like 'identifying the ultimate limit of iterating a unit' which is like 'time travel' or 'out-calculating time' in that it determines a possible universal limit, which are useful to identify in order to identify paths avoiding these limits, as 'useful simulations' can be useful as 'units of a combination' or 'units of reality' in that applying many different simulators can be useful in predicting 'optimal time state sequences' to guarantee 'high-variation support' across these sequences, as 'predictors/simulators' can connect more variation than other structures, so theyre useful for identifying probable/possible/required time sequences (so intents like 'identify a connective sequence of time ranges by applying a simulator/predictor at different probable/known points in a probable/known time sequence, to find all their connections' is useful to 'host variation in the time range from the universe starting point to the current point, thereby reversing/fast-forwarding time through this higher time/variation-hosting/supporting', as 'sequential time' may be replaceable with 'co-occuring sets' as an alternate time structure, so 'identifying a state in a sequence using connections between these simulators where more variation can coexist' is useful to 'direct variation to that state' using more optimal simulator networks than reality, which will become the new reality once variation is better supported there)
                - so given some optimal distribution of simulators/predictors, the universe could host more variation than it would otherwise support, which is possible by identifying 'real paradoxes (real contradictions allowed/required by requirements such as definitions)' so that these can be used as 'units of reality invalidation' which can be combined to invalidate reality rules and construct new reality rules, basing a new reality on this 'path/network of paradoxes', as 'paradoxes' are useful in identifying/building other contradictions like 'reality invalidations', such as paradoxes of 'how the universe could have absolute limits on time/variation but also host more time/variation that goes above those limits (using structures like simulators to simulate embedded variables or to host more variation/time in the simulation than reality can otherwise host, these simulators acting like portals to infinite variation if built optimally, where connecting them is another high variation structure, and similarly other interactions can increase their variation like if they all coordinate, and so on to add other embedded variables to support infinite variation)'
            - relatedly, identifying structures which often exist in 'specific ratios like a high-low ratio' are useful to identify 'different variants of a variable (where one variant is more common)'
            - more difficult variables to connect often involve a 'sequence of edge/extreme cases' (such as a 'sequence created by unique filters that leave one possible solution') which when aligned in that sequence, make it almost impossible to connect variables (like a set of rotating cycles that have one possible option that can interact with other cycles in a way that allows the input/output variables outside the sequence to be connected)
            - so stacking 'extreme/edge cases' or 'unique filters' is useful to identify more complex connection structures which can be used to solve more complex problems
            - its useful to identify 'equivalent alternates' and stack/sequence them bc they have a 'similarity' in that their particular rotation state doesnt matter, so any direction/state of rotation is equivalent within each set
            - for example, 'identifying equivalent alternates (such as alternate ambiguities, complexities, functionality, etc)' identifies 'rings of structures' which can be stacked to decompose complex systems into a set of possible sequences of the rings, at which point, 'useful sequences/alignments of the rings' should be possible to identify (the way to align/sequence a set of rings that makes most variables trivial to connect), as an alternate way to connect 'concept networks' and 'numbers' (other than mapping differences to vectors in a vector space and losing most of the concept-identifying info)
                - another alternative to 'concept vector angle differences in a vector space' is identifying vector networks for each concept (which removes the original info of what words are used in the definitions, but when positioned in relation to all other concepts similarly formatted, would regain this info through the relative network structures across vector networks), then mapping vector networks in a network indicating 'usage' vectors (or some other interface variable that can connect all the vector networks for each concept)
                - similarly, useful 'orthogonal rings acting as cross-sections of the ring sequence' or 'host rings where the others are embedded' should be possible to identify, after identifying the 'equivalent alternates' to apply in this way, such as 'connecting rings using known useful vertexes to organize the sequence/network of rings'
                - similarly, identifying 'ambiguous alternates', 'useful combinations of alternates like vertexes', 'maximally different equivalent alternates', 'maximally different resolutions of ambiguities' are useful to identify/position/mix in these sequences/networks, to identify 'maximal difference-supporting/similarizing sequences/networks', where important/useful interface structures or problem-solving structures like 'determining variables' or 'phase shift thresholds' are trivial to identify (meaning 'similar to each other in some way in this network, such as being possible to identify by some similar similarity/difference query of the network in relation to some supporting system/interface' such as how 'this type of phase shift is possible, when this change set is allowed in this system, and when this change set has occurred in this system')
                - more likely, some 'sequence-generating function' applied to these rings/vertexes will be more useful to 'organize the sequence for specific intents' (so rather than building a one-dimensional stack that acts like a wormhole in that it preserves some important info about the input to connect it with some complex/independent target output, a 'orthogonally-rotating, self-similar, maximally different wormhole similar to some manifolds' is built which would likely act like an 'info vortex' and 'capture all info', therefore creating a possible cascade that would end reality/time, as it would fulfill every function and connect everything in a very condensed structure so nothing could out-vary/out-pace it and it would therefore be all-powerful, and once that occurred, no external variation would be able to resist its gravity)
        - similarly, 'sequence-generating/determining variables' like 'gravity/attraction' are useful to identify and apply as an alternate to 'sequences', 'sets', 'probabilities', 'adjacencies', 'interactivities', 'info barriers', 'independences', and other useful alternate structures

    - identifying useful similarities/differences in problem-solving structures, such as 'interface queries (connecting intents to structures to fulfill it) which can be changed with some vertex (probability vertex) to be relevant to resolving problems in that vertex format (probability resolution problems)'
        - for example, a problem is often formattable as 'what is a new similarity/difference, which differs from known similarities/differences' like 'does this apply to all examples of this type, or just this known example' (is it the type that determines the functionality or something else), where the uncertainty/ambiguity lies in 'whether there is an alternate route between attributes (attributes whose connections are not fully determined)', such as whether there is a way to generate the same structure that is 'normally generated by defaults/common inputs/requirements' by instead using 'non-default, uncommon, non-required but possible differences', at which point finding a 'unitary solution/connection structure to iterate/differ' (like a unit of a possible connection type) is a useful next step to determine the uncertain space of 'non-default, non-required, uncommon allowed interactions', at which point finding 'iterated structure variants forming larger subsets' from that connection type unit (such as 'different shape subsets that the unit can create with trivial/probable/common changes') is a useful next step
            - 'determining an alternate route' -> 'finding a unit connection to iterate/vary' -> 'iterating/varying this unit connection to form subsets of possible solutions' -> 'determining subset interactions to find/build possible solutions' is therefore a useful route between problem/solution structures, which can be changed by 'probability-resolution structures' (such as 'ways that a rare structure can fulfill the same intent as a common structure', etc), such as by identifying 'rare unit connections which can build common subsets'
        - these problems are problems of 'decomposing variation resulting from various structures of probability and their interactions' (common/default/powerful/probable/required vs. opposing structures which are possible but less known and more variable so more likely to be useful in resolving an uncertainty and likelier to be difficult to filter)
        - similarly, solving 'unknown connection functions' is a useful problem-solving intent to fulfill, as resolving unknown connections creates additional connection functions that can be applied to connect other structures

    - identifying new connections between useful structures like '1-to-1 mappings' and 'interfaces', which are connectible in that a vertex formed by a 'lack of a 1-to-1 mapping' (like growth/regulatory-sweetness/bitterness vertex, which are more useful to apply in combination rather than in isolation) can indicate an interface variable on either side of the mapping (similar to how generate/filter or grow/regulate are alternate function sets where either item in the set can be used to solve all problems, other variables that are not directly mappable to these vertexes can also act like interfaces)
        - for example, 'exceptions to 1-to-1 mappings between generative/limiting forces' are useful to identify new possible interfaces, such as how the 'sweet/bitter' variable isnt 1-to-1 mapped to 'growth/regulation', such as how some growth-favoring types like sugars have some regulatory exceptions in the type (such as glycosides or regulatory compounds found in honey), which have inhibitory/regulatory impact despite being sweet or in a sweet compound, an attribute which usually favors 'growth' functionality as it is paired with energy inputs (such as how 'stevia' has anti-growth functionality and doesnt provide energy inputs like other sweet compounds tend to), which is possibly a result of other variables associated with the sweetness, such as 'lack of energy inputs associated' or 'high interactivity' ('bee pollen' is supposed to help and interact with many different species, as an input to their survival, so it makes sense that generally useful compounds are found in it), which implies that either variable value can act like an interface where all functionality can develop (sugars can be both growth-favoring and growth-regulatory and can possibly contain other functions which arent 1-to-1 mapped to sweetness or other attributes of sugar)
        - this means that 'sweetness' can be applied as an interface (in its mapping to an interface like 'grow/build') in that it has enough variation that it can interact with 'regulatory' variables or act as a 'regulatory' variable ('growth' can be 'regulatory', such as by 'helping regulatory molecules grow')
        - relatedly, 'high variation/powerful/determining inputs' is similarly useful as 'input/output sequences' in that 'high variation inputs or filters' can create all 'input/output sequences', just like 'co-occurring sets' and 'probable sets' act like equivalent alternates of 'input/output sequences' (the info of 'all sequences' isnt necessary if the info to 'generate/filter all the sequences (generative input variables or regulatory filters)' is available or the info of 'variables describing/connecting useful sequences' like adjacency/interactivity/probability is available)

    - identify useful structures like 'connections between high variation structures like vertexes' which make identifying other useful structures like 'useful intents' (such as the intent of 'matching filters/generators' as other 'useful opposing structures to connect') trivial
        - for example, identifying how vertexes are related is useful, such as how 'matching resources/requirements' and 'generate/filter' and 'abstract/specify' and 'reduce/expand' are related by their common structures, where the differences of 'generate/filter' applies a similar variation type as matching 'resources/requirements' (generating possible resources, then filtering them with requirements), so they could be seen as variants of each other, and similarly, 'abstracting' a structure is 'filtering its attributes to its common/general structures (which form an abstract type)', where 'specifying a structure' is 'generating an example of it' (by combining a set of its possible values)
            - this is useful to identify so that 'vertexes which are extremely different can be identified'
            - similarly, its useful for 'identifying missing structures of a vertex which would be useful to include' (such as how a 'change' function is always useful, such as 'changing generators/filters or changing position of structures like changing position of filters to integrate them with generators'
            - similarly, some structures are often useful for specific vertexes, like how an 'abstract-specific combination structure' like 'causal position' is useful to integrate with 'abstract/specific' structures to make their usefulness/relevance more obvious by applying them to a 'high variation structure like a cross-interface structure that is both abstract/specific'
            - similarly, identifying the ways that one structure in a vertex can be similar to the other is useful, such as identifying how 'if enough filters are applied to a generator', it would seem like a specific function structure, such as a 'filter sequence', if it only generates one solution per input bc of the filters applied (as in 'one possible implementation, as in a specific function, rather than a set of possible function implementations/solutions')
                - similarly, if enough abstractions are applied, they can indicate a specific structure by adding increasing information
        - relatedly, the task of 'matching' these vertex structures (matching a generator with a filter) is the unit of the task of 'integrating' them ('filtering generators' to be optimally useful, to avoid generating irrelevant solutions as much as possible)
            - 'generating a solution set that cant be filtered by a particular filter' is an interesting intent to fulfill or find an example of (such as where the emerging structures of solutions like solution sets should be evaluated in a combination/network, but a filter only checks the same variables used to generate the solution or the differences between solutions generated by those variables)
            - similarly, 'generating only maximally different or otherwise useful solutions' is a matter of 'identifying the variables that cause maximal differences' as in 'filtering possible variables used to only include those variables'
            - at some point, if the filters are specific enough, the generator is not required, as the filters contain enough info that solutions dont need to be generated for filtering, as all the requirements of the solution are known and implemented in the filters
            - "avoiding filters that clearly optimize for/map to known requirements" is a useful intent to identify new filter sets for a different/new problem than those solved by known filter sets which optimize for/map to known requirements

    - identifying useful structures like different sets of 'offsetting function networks' is useful to identify the 'sets of function networks that can act like useful boundaries/limits on variation' and the 'connection/switching/prioritizing functions between these networks to avoid known errors'
        - for example, identifying function networks with 'complementary error sets' is useful to identify 'systems/networks to differentiate from' when fulfilling intents like 'creating an optimized system', which systems/networks are definitely suboptimal but connect with each other in a useful way, such as by offsetting the others' errors
            - these are useful to identify just like limits/errors/optimals are useful to identify bc once the difference/distance from errors/limits/optimals is known, other useful structures can be computed, such as the maximally different points in between them or their various averages
            - relatedly, a 'flow state' is a related concept in which solutions are easily accessible with available functions/variables, within a limited range of required changes that is handled by those available resources, where some variation is allowed but minimal variation is required and is well within functional capacity limits, bc it is allowed to avoid known errors and errors are known (acting like 'guardrails on a highway')
            - this means identifying function networks with overlapping errors like 'attention and bias networks' as well as identifying networks with exclusive and complementary errors
        - relatedly, a 'minimumm speed of analysis' is useful to identify, as a 'maximum speed' is unlikely to be able to fulfill all required analysis functions, so there is an upper limit on optimal speed, which doesnt sound correct until its identified that 'identifying multiple equivalent alternates or other types of alternatives' is almost always useful among other useful functions that create a 'limited set of required analysis functions to apply in combination', which requires fulfilling functions that require non-trivial work (to generate equivalent alternates, like by 'applying differences in simmilarities until an equivalent alternate is found'), this non-trivial work being likely to occupy more than a point on a network, no matter how optimized the network
             - as a counterpoint, a network where these optimizations/function usages are points on the network may offer a useful alternative, wherever these can be identified (such as a 'network of useful interface queries')
        - relatedly, identifying 'all the ways a suboptimal structure could be correct' is a related function network that is useful for its specificity and should therefore be included in this 'error-avoiding function network set' to handle known error structures like 'perspective-switching/prioritizing/connecting errors' such as 'missing the case where simplicity is correct (such as when identifying a simple structure like a unit is possible and optimal), bc simplicity is more often a known error like over-reduction or over-prioritization' or 'missing the counterintuitive structure bc a sufficient difference-containing insight set that can create counterintuitive structures wasnt identified and applied'
            - for example, an algorithm that involves 'avoiding trusting any structure with 100% responsibility/resources' is likelier to avoid known errors like 'over-burdened structures' and 'false optimal metrics' given structures like 'incentives' and given the 'improbability' that 'any given function can handle all use cases optimally or that such a function is required', which can be combined with other algorithms that avoid other known errors like 'simplicities that dont qualify for exceptions in some case'
        - matching 'counterintuitive structures' with a 'difference ratio/type/set/structure' is useful to identify 'functions/workflows/structures' that can identify counterintuitive structures (its whichever insight sets contain enough differences to create counterintuitive structures such as structures which self-invalidate and self-optimize at different phases/amounts/degrees/positions/structures)
        - relatedly, identifying 'degree/distance/type of difference required' is useful not just for intents like 'avoiding crossing some threshold distance to a known error' but also for intents like 'finding useful opposing structures to include (like use cases/counterintuitivities/contradictions/exceptions, or generate/filter, or similarize/differ, or abstract/specify, or change/limit)' which can correct the errors theyre likely to cause
            - similarly, other opposing function sets exist (such as 'change position' or 'change surrounding variables' or 'change connection functions') which are useful to identify as alternates which have similar functionality, these 'equivalent alternates' being useful for identifying new workflows as they involve variants of known useful structures
            - similarly, identifying 'reasons why a structure needs to be specified or otherwise acted on by interface structures' is useful, such as cases where it would be considered optimal/suboptimal, like how 'input/output sequences' are useful in specific cases, such as where "known structures are possible to connect with available/adjacent changes, and where known structures are sufficiently complete to identify variation in all possible path variants of the variable connections, and where there are no opposing cases such as where the structures in between known structures are not definitely structures that can be dismissed/ignored, like 'irrelevant structures', where they are instead relevant like alternate reasons why another variable can exist, so identifying these alternates is required, rather than dismissing them as equal or non-changing of other variables", where these cases are useful to specify so that "switching between similar/equivalent alternates (like 'required/available differences', 'probabilities', 'sets', etc) to create useful differences" is possible
            - why is it important to identify and connect alternates? bc there are 'alternate routes to errors' just like there are 'alternate routes to solutions', and similarly 'multiple alternates can co-occur bc not all alternate variable sets are exclusive and can have structures like overlaps' and 'routes to solutions/errors can also have overlaps' and these structures are all useful to identify, rather than identifying a subset
            - similarly, a 'network that avoids all known structures (patterns, limits, types, other certainties) by embedding more variables in an interim space' is useful for 'identifying new structures', just like a 'network that applies maximally different structures like 'generate/filter' at the center and applies changes in an outward direction or across iteration levels' is useful for that same intent
            - 'similarly useful' structures can be applied (such as reality-covering variable sets) but also 'mixed-usefulness' structures can be useful in that they capture different info ('randomness may not always be useful in a case, but in a new problem it could become useful to add/handle more differences'), given that differences are required in a useful structure
            - similarly, generating new similarity indexes using interface structures is useful, such as new similarities like 'overlaps but no intersections' or 'waves but no similarities/patterns across magnitudes', or other sets of similar functions (a similarity index which can be decomposed into a set of structures like 'core components that can generate the set', 'useful variants within the set' and 'maximal different examples in the set' and 'limits of similarities/differences in the set' which fully describe the 'similarity of the items in the set' and 'identify variables of similarity indexes', and 'connections between these items creating the similarity index across similarity indexes' are useful to identify, to identify other structures like 'alternate routes between and variables of indexes')
            - similarly, networks having some similarity like 'similar connection structures' and 'overlapping nodes/centers' are useful to identify, as sets of 'pre-standardized/similarized networks of differences' which can be used by default to find new networks that are different such as by being non-similarizable or non-overlapping with these network sets

    - identify common structures across useful workflows to fulfill problem-solving intents like 'identify counterintuitive truths' by 'identifying useful filters' (such as 'optimize true structures, then differentiate and extremify to find other true structures', or 'extremify a default, then differentiate and extremify, and compare extremes')
        - for example, the problem of 'identifying a counterintuitive solution' is often a problem of focus/filtering
            - such as how when focusing on fact sets, such as:
                - identify constants that could optimally be variables like 'added variable size' (as in 'increasingly small' changes), then identify important differences like initial/added changes and maximize their difference to test the impact of the required changes (added changes) in different input cases (like with a large initial size and increasingly small added changes)
                    - this involves identifying more optimal structures of the problem structures first, then identifying and applying useful changes (like maximize difference between known relevant differences, within definition limits) to identify other useful changes (like scaled/extreme impact), once the more optimal variants are identified
                        - 'all structures can be composed of infinite infinitely small components which are infinite in that they can continue to get smaller, especially with numbers, as numbers have no requirement to be reflected in a real system'
                        - 'distorting the scale of different changes, such as adding very small changes to a very large structure repeatedly, makes it more obvious that repeating that addition wouldnt change the large structure very much, and would taper off into an undetectable amount if the changes kept getting smaller'
                            - this fact set involves a 'true variant' of a 'false connection' such as 'iterating any change enough times can create any other change eventually', the true variant involving the 'possibility of a variable in the change size, such as an increasingly small change' ('applying a variable to a constant')
                            - 'changing embedded variables (as in variable variables, like size) can usefully differentiate structures' to fulfill intents like 'make differences more obvious/extreme', first by identifying a possible difference between variables like 'initial size' and 'added variable size' and applying a difference-maximization function to this variable set to 'further differentiate variables, within the limits of their definitions, to test for a difference in the impact of extreme changes' and fulfill intents like 'try every possible variation (trial and error)' or 'test if trivial differences in known different structures (known different structures, like initial and added variable size, which are required to connect in the solution) change other variables, like output size after all expected/required changes (the iteration sequence)'
                            - given that 'identifying true variants of known errors' by 'applying a constant as a variable' and 'differentiating true differences to the extreme allowed by their definitions (within a true range)' are useful to apply in a combination, what other structures become clear?
                                - identifying an insight (to base changes on, to connect with other insights, to find false variants of, to find useful structures like 'increasingly small changes') is an important first step
                                - applying changes to that insight is a useful next step (applying 'fuzzy connections between input/output (such as problem/solution) structures'), not just to see if it holds when changed, but to see if an extreme difference is useful as it often tends to be for other problem-solving intents, such as to identify the impact of 'scale/size' on a variable connection ('increasingly small' changes are the 'change type' to identify as important in this problem), changes which are useful for 'finding the impact of the useful structures identified in the insight, when applied in variants such as in extremes'
                - identifying the extreme of a default change (constant change) then identifying the extreme of variants of it (like less/more than default change) is similarly useful to focus on to derive the counterintuitive structure
                    - 'adding an infinite number of constant changes (such as the same type of atom) would obviously eventually create a non-trivial/large structure'
                    - 'adding an infinite number of structures smaller than an atom would not definitely create a non-trivial/large structure'
            - the differences between the correct solution structure and other structures (possibilities that seem equally probable at first, before focusing on these fact sets) become obvious
            - other possibilities that seem equally valid at first include false connections such as 'iterating any change enough times can create any other change eventually' which seems true until a 'decreasing change size' is identified and compared to the obvious impact of a 'increasing or constant change size'

    - identifying the info structures like 'rule/function networks' between structures not already determined (like 'networks mapping between interface structures of function attribute sets'), using some standardized structures (like some constant value of a change rate that can exist in a standardized function to a 0-1 range that implies other change structures should be checked for), as an alternative to a 'network of function similarity indexes', given that with enough input info about the function, the rest of the function can be determined by applying some interface structures like implications in the correct structure like a network, where the input info available about the function helps to 'select the position of the network to start applying interface structures at', which is like a network of 'determining structure sub-sequences/networks' (where these sub-sequences/networks act like 'dead-ends' confirming some structure, resolving some uncertainty)
        - for example, identifying the 'set of contradictions/differences/opposing structures' that can coexist in a particular function type/shape/other interface structures (such as function attributes like '1-to-1 mapping') is useful to identify, to filter out possible structures (such as how 'opposite slopes' can exist in the same function, but not in the same input range and not in the overlapping input range for a 1-to-1 function, so if opposite slopes overlap, either one of them is wrong or both of them are wrong, and given some established change rate of some subset, there may be a distance requirement between the slopes, to allow a maximum or other structure implied or required by the established change rate)
            - this set of rules between interface structures like requirements/alternatives and function structures like constants, such as 'the options which are likely or guaranteed to be incorrect or could be equally correct as some alternate, requiring testing for both alternates, given some established function attributes like an identified change rate in a subset or a attribute like 1-to-1 mapping' is useful to identify in the 'function filtering (given a local or regular interval subset)' problem space, which is related to the 'regression' problem space
            - identifying the 'contradictions that can coexist' (differences that can be united by a similarity of the same function) and the 'structures that allow/require other structures to enable that coexistence' is useful to filter out 'possible remaining subsets of the function'
            - given these structures to find 'maximally different' and 'maximally similar' functions (given an identified input, like a verified subset of the function, to identify the most different/similar functions having that subset), computing the 'interim' functions at the midpoints of these alternates (the 'interim' function between maximally similar/different functions) is possible/trivial/useful, as is computing the 'reasons to vacillate between functions' such as interface structures like 'requirements/implications/probabilities' which can be used in a function network to determine variable interactions
            - finding the unit of a useful difference to apply to connect variables (rather than just pure numerical operations like 'multiply') which are 'definitely similar but uncertainly different or vice versa' is unlikely to be solely numerical and likelier to be some interface structure (like a 'maximally different vertex' such as 'changing the angle and position and extremity of some input variable set', as mentioned elsewhere as the 'rotation'-analogous operation that explains variation between similar but different structures by some common similarity/difference associated with problems/uncertainties, although a rotation is a good place to start searching bc it does apply 'interface changes' as opposed to other change types)
                - similarly a 'derivative' is another good place to start, as both operations involve 'vertex changes' of uniting differences with a common structure, it could also technically be said about 'multiplication' but that is still lower-variation than rotation/derivatives, similarly combining these is likelier to support more variation and be more useful in connecting different structures (solving problems), similar to how 'rotating functions' can generate other functions on the similarity index (similar by shape and other metrics)
                - similarly, a 'matrix' and a 'vertex' have some attributes in common, as a 'matrix' unites a 'set of different sequences' and a 'vertex' is a 'common structure across maximal differences', and relatedly, its useful to apply interface structures (to find the 'similarization' of a matrix with some structure such as itself, the 'standardization' of a matrix, the 'exclusion/exclusivity' of a matrix as in 'what does it obviously leave out', the 'specification' of a matrix as in 'what type(s) does it encode and what is a more specific variant', the 'intersection of a matrix' with some structure such as itself as in 'what intersections/overlaps can be trivially created with trivial changes allowed by matrix operations', 'perpendicularities/parallelisms' of a matrix as in 'what changes would make the values align (parallel) and what is it already aligned with', etc with other structures like 'unique' and 'required/default' and so on, just like the 'diagonalization' of a matrix or the 'inverse' of a matrix or the 'symmetry' of a matrix are known to be useful to identify)
                - relatedly, 'similarities between different function subsets of the same function' are more useful to identify than 'similarities between functions' (such as functions with similar area/shape/slope)
                - similarly, its useful to identify structures of vertexes that are useful in intents like 'determining other structures' such as how 'two opposing vertexes' could form structures like 'boundaries/limits' if their endpoints also are vertexes, which would create a determining structure like a boundary/limit
            - finding all the centers/cycles/networks in between all the 'dead-ends' to connect the 'dead-ends' is similarly useful
                - this structure keeps re-occurring without resolution bc its a filter to get to the next higher variation state, but its not the only filter to get there
            - the interface analysis program would avoid these 'dead-ends' itself so it can identify them (meaning it would regularly 'optimize for nothing', so as to avoid being determined (by allowing some randomness), at which point it would be predictable by a higher variation structure, rather it should always support the most variation to be effectively optimal at identifying other variation)
                - this is another reason to call interface analysis 'interim thinking' (staying 'in between' structures like 'limits' by applying structures like 'embedded variables' is a way to maximize support of 'high variation')
                - relatedly, staying in between known errors such as 'one-priority/perspective at a time' and an unresolved structure of 'all priorities/perspectives at a time' involves structures like 'independent variable sets (like the set of both abstract/specific and uncertain/certain)' that connect these two opposing error structures (the known error of over-prioritization and the known problem of 'resolving the real structure connecting all these variables'), where interim structures are likely to be useful toward resolving that known problem of 'connecting all reality-covering variables'
                - 'moving toward or away from interface structures' in this graph of interface structures (designed to be navigated around/avoided) is a problem-solving function that can be used to generate interface queries using that graph, such as 'avoiding certainty resolutions or approaching reality-covering variables or approaching higher-variable intersections (when more variation is needed)' and 'approaching certainty resolutions or avoiding reality-covering variables or approaching fewer-variable intersections (when less variation is needed)', which is useful to know for designing the graph (positioning these opposing structures in different positions, but also creating differences within each type, so that reality-covering variables arent all in the same position)
            - similarly, identifying uncertainties to resolve that would not be dead-ends even though they identify certainties is useful, such as where the certainty will 'support so much additional variation, that its worth sacrificing the uncertainty maintained by not resolving it' (which is 'allocating cost' as in 'how to spend your uncertainty, to preserve some balance as in both certainty and uncertainty, without a cascade in either direction, either toward certainty/determination or uncertainty/chaos'), which are ways around just 'avoiding dead-ends', as these new variables allow new spaces for new connections to be found
        - similarly, identifying the 'worst case' of 'how could this variable interaction be completely incorrect, if extrapolated to the extreme' (what probably incorrect functions can be generated/tested for, by applying a probable error, such as by assuming this one variable interaction, like one pair connection, is over-prioritized and applied as the most important variable connection in the data set) is useful to filter out functions based on some identified variable, such as a slope that is repeated or iterated/scaled to an extreme, which is both unlikely and likely to be trivial to filter out if incorrect, as a 'simplified version of a function created from an implication of a subset' that is likely over-simplified by applying this extreme to such a small subset, and relatedly in the opposite direction, what functions are more probably correct, given these known error structures such as over-prioritization
        - alternately, 'overlapping subsets' may be a better function format in some cases, such as where variable interactions in the original system have yet to stabilize into one function or a clear set of multiple alternate functions and clarifying the functions by resolving them isnt optimal yet (similar to how averages/types/limits/ranges/function networks can be used instead of a single solution function, as they store a high ratio of info using few info structures)

    - identifying useful structures like 'required symmetries' is useful as well as structures that can help fulfill/implement them like 'organization' structures
        - for example, a 'required symmetry' exists between problem/solution structures such as generate/filter or generate/evaluate, where a generated solution set is useless if it cant be filtered/evaluated, so 'organization' structures are required to be matched to every 'problem' structure, otherwise the problem will not be possible to solve, as 'organization' structures help fulfill the opposing intents like 'filter/evaluate' to problem structures like 'generate'
        - this is bc if a generated set is 'organized sufficiently', such as being sorted, then its more trivial to filter/evaluate it, because of the symmetry in usefulness that exists across generate/evaluation structures
        - identifying these required symmetries is useful for identifying structures that need to be applied together to optimize for usefulness/relevance such as measurability/differentiability/filterability
        - this forms an interface structure based on 'organization', where generate/filter are just a pair of opposing intents adjacently fulfilled by the 'organization' interface, similar to 'group/subset'

    - identifying structures such as 'similar number of points connecting some relevant difference' which seem truer/falser when some structures are applied (local context, specification, connection to other truths, etc) is useful for determining false signals of truth, which are similarly valuable to identify as truth structures
        - for example, if enough irrelevant similarities exist between some subset, they will seem locally true and similarly true as more relevant similarities in some subset, bc of the comparable count of the similarities
            - its possible to say that some irrelevant similarities exist, such as 'all structures have functions' or 'all structures have problems, and solve those problems in different ways', so saying 'all structures are equally smart' seems falsely true if only that fact is applied, in isolation of other relevant facts, bc its technically true but irrelevant (as in not differentiating, as it applies to all structures), where whats relevant (differentiating) for determining intelligence is whether some structure supports such an extremely high degree of variation compared to the other that they can easily beat the other in all relevant metrics, or to an extent that the other may as well have zero functionality, such as if a supercomputer has so much relative computational potential that other computers become irrelevant
            - in that example, some group of computers may have similar computational capacity, but they have to coordinate which causes additional costs for their computation, in order to reach the supercomputer's capacity, and similarly the supercomputer can compute aggregated/iterated calculations such as computing differences between sub-networks in a way that a group of computers has to communicate and coordinate to compute, and they also have additional costs like selecting a host/lead computer in the group to aggregate/evaluate the others' computation results
        - similarly, opposing sides of an issue seem right when their points are considered in isolation, such as 'everyone has potential to be president' which is technically true, but irrelevant in the sense that 'only one in the group' can be 'president', so not everyone has potential to be 'president' bc thats not how the 'president' structure is usable in reality and similarly an opposing point like 'qualified people have the potential to be president' seems true, and bc they both contain 'one similarity that connects differences' (similarity in 'potential to be' connecting everyone/president and similarity in the 'adjacency connecting people who are qualified and people with potential to be president'), they both seem 'equally' true bc of the equality in the number of technically/irrelevantly true points, but both also seem false in various ways, as the statement can become more true when interface structures like 'specifications' are applied ('everyone has potential to run for president, but not be president' is more specific and more true and similarly 'everyone has potential to be president, if nobody else is president at the time' is another more true specification), and similarly 'everyone has potential to be qualified to be president' is more true in that it connects true statements and resolves their overlapping uncertainty (who has potential, who is qualified -> everyone has potential to be qualified)
        - similarly, identifying that 'justice' has a symmetry with its input in that it reflects its input ('truth'), which is a way to connect justice with reality (reality/truth -> justice), as reality is a variant of truth (which allows for some structures of falsehood)
            - another way to connect them is by their similar structures in common (like balance)
            - so 'structures in common' and 'input/output sequences' and 'adjacent similarities' are identifiable as alternate structures to derive truth
            - similarly, structures like 'constant (like causal direction) could be a variable' or that 'cause can be multidirectional' or that 'correlation doesnt equal causation' or 'co-occurring structures can be causally associated' or that 'input-output sequences can have an overlap and be cyclical as well' and 'there are cases where sequence is irrelevant bc "either sequence could apply" or "it doesnt matter either way"' are all structures which can identify the same insight
            - the last structure is particularly important, as an algorithm that only identifies 'variables which change other variables' would miss a 'pair of variables whose sequence doesnt matter bc they both create the same change' so their difference may be undetectable, but identifying the 'reasons why and the structures of how they have the same impact' is useful, or only identify one item in the pair and stop evaluating at that point
            - this type of interaction is useful to identify as it changes the relevance of other important structures like 'sequences'
                - 'co-occurrence/coexistence' is like 'sideways time' (a different dimension of time) in that higher variation (time) is possible when more differences can coexist
                - similarly, identifying/embedding more variables in between certainties like infinities/limits (to add variation between current position and the certainty) is another type of variation (time) in that it increases variation supported, which is like 'stacked time'
                - these are useful alternatives/compounding structures to 'sequences' as a structure of time (time sequences)
                - similarly, 'difference from not-time' (difference from anything that reduces time, like cascades leading to infinities/limits/other dead-ends like uncertainty/certainty) is another type of time
                - this means its probably possible to 'support all variation' in one spacetime, if enough horizontal/stacked time is applied (to make one spacetime higher variation) instead of sequential time (across space-times)
        - relatedly, as mentioned elsewhere, the structure of 'perspectives' is useful when identifying 'truth' connections (such as how there is a 'simple' way to connect all variables, a 'balanced' way, a 'positive' way, a 'causative' way, etc), and a truer perspective is likelier to have structures of truth such as 'more/common/multiple/alternate' (such as 'more of these perspective structures encoded in it, in a way that reflects the conceptual variable interactions' such as a 'vertex' structure involves a common point between multiple perspectives), so the 'simple way some function could be a solution/error' (and a 'simple way some function could be true/false' and a 'complex way some function could be true/false' and a 'balanced way some function could be true/false' and so on, iterating and embedding structures like variables/functions) can be identified as new indexes (connecting some vertex like truth/simplicity with some specific structure they have in common, like a specific function), which if sufficient examples of these are identified, an approximation of the interface network could emerge from applying this to one specific function
            - similarly, a 'simplified' way to connect all variables will have errors such as 'missing info' and 'inability to adjacently derive more complex solutions' and 'mismatches between over-simplified and real structures' so once these errors are found, the perspective should be switched to another perspective without an error in that position, or to an opposing perspective that can correct that error
        - similarly, the 'number of points made' is irrelevant (unless compared to the other number of opposing points or otherwise connected to interface structures) bc it doesnt map directly (like 1-to-1) to truth ('more points' doesnt mean 'more true') but there is a point where it will seem true (if 'more false points are made than true points, above a certain ratio', the 'false points will start to seem true, just by being the only thing that is being observed/measured', so there is a 'limit' structure of the '"illusory truth" attribute of many false points', though it doesnt 'add truth' or otherwise 'increase the truthhood', but it may change the 'measurements being made, from false to true')
        - similarly, identifying 'false filters of truth' is useful, such as how some filters are definitely false to apply too prematurely but may seem true as in correctly applied at that point, and result in a 'premature filtering' error, where continuing in that direction would have been useful (such as where a variable can cause another variable at different amounts/degrees of the input so it might be filtered out bc the 'active amount/range' was not present, and similarly how other variables can cause the same output so it may seem like the absence of the original input variable means it couldnt cause the output, but this is false bc it has an alternative causal variable that was present)
        - similarly, 'obvious differences' (related to 'extreme differences' and 'absolute differences') are a useful problem-solving structure just like 'required/default differences/similarities' are useful across problem-solving structures, so that when a measurement/metric is insufficient at measuring truth/fit, it can still be used to tell the correct variable interaction, bc the difference was made obvious
        - other structures are similarly useful, based on what they are different from or interact with:
            - 'truth structures' which are too strong to the point where they destroy other 'truth structures' (where the destroyed structures were real but were just under the strength threshold to be measured, which is an alternate cause of a 'missing info' error)
            - 'switch' as a useful problem-solving structure similar to 'swap' as it involves applying some opposite structure, like 'switching contexts around a very different structure pair' or 'switching position of opposites' which is useful across various problems, as it creates a high ratio of difference which can be useful for intents like 'identify maximal differences' (which is similar to asking 'what if some assumption was extremely true/false')
            - 'stretch' as a specific variant of 'iterate' that can act like an alternate to 'iterate', being useful to identify as iteration is a problem-solving structure (how many times can you iterate before identifying a new useful structure vs. how much can you stretch one structure to cover/solve other problems)
            - similarly, 'usage structures' can increase the usefulness of a structure, such as how a usage structure like 'repeatedly using a function (like to identify high variation)' leads to other functions/structures developing around it (as the variation of a problem-solving network/function has nowhere to go but in the surrounding structures now that the function usage is certainly useful for a particular problem/intent/case, and the input direction is a useful direction to apply variation in, such as in input filters), and these developed functions can increase the usefulness of the function, such as if a function to identify high variation develops a set of patterns that are associated with high variation and starts using these structures to make the function more useful, which is only possible with repeated usages of the function to identify those patterns

    - identifying useful structures from mapping new problem/solution structures like new errors in a different problem space and identifying optimal mappings (like 'mapping to certainties/similarities in the original or regression problem space') which are the most useful in standardizing a problem to identify new optimization structures/insights that apply across problems
        - for example, a 'navigation' error might be an error of 'missing a right turn at an uncertain destination (not sure where the destination is or how far away it is, but there is a sign pointing to it that youre checking for regularly at interval x)' which is caused by 'solving a different problem which takes resources like attention away from checking for the sign to turn at' where a different problem might be 'deciding to switch lanes', where there is an optimization structure in solving the other problem, which is the 'checking whether to switch lanes' directs attention to local structures, so a turn sign is less likely to be missed given that attention is being routed to local structures including signs, however it can have an error structure of 'only paying attention to car position/speed rather than signs or other adjacent structures'
        - when converted into an optimization/error/function set in the 'regression' problem space, this error would take the form of 'missing a local minimum bc the interval checked at was skipped bc a different metric was being evaluated, such as a function to check adjacent points for some superior local path between points, where this function to check local points might find the missed local minimum if its missed bc of solving the other problem at the time or missed bc the interval x which the original function is checking for minima at coincidentally misses the minimum'
            - the shape of this algorithm would take the form of a "straight line connecting a set of 'regular interval points' with an occasional embedded cycle (occasional alternate problem solved, like 'evaluating for an optimal algorithm-switching point')"
        - therefore 'having multiple functions with overlapping functionality' and 'avoiding solving a different problem (applying changes to solve the problem of deciding when to switch lanes) while an uncertainty (finding the minima or finding the sign) is still being resolved' has a structure of 'keeping other variables constant while some uncertain variable set is being resolved' (keep a similarity like "lane adjacent to a required turn" while a difference based on or related to that similarity is being resolved, as in the uncertainty of "where to turn"), which are both optimizations across problems derived from mapping this 'navigation' case to problem/solution structures in the 'regression' problem space
        - identifying this problem/error/optimization set also identifies an important core problem format, which is the problem of switching ('switching between algorithms') and changing algorithms ('applying algorithms in structures such as combinations/sequences/mixes'), similar to the 'batching' and 'selecting' and 'scaling' problem formats
        - relatedly, the general solution of 'improving connectivity so that all points in a network are trivial' is possible to implement with structures like 'specific connectivity ("communication") optimization structures' (related to how 'improving cell communication in the brain' with substances like 'citicoline' or 'heat' or 'blood flow' or 'synchronized brain waves at a useful frequency like a default frequency' could solve brain problems like 'fungal infections' or even possibly 'tumors' or 'brain lesions')
            - this makes it trivial to identify other optimization/solution structures like 'building new connections between non-adjacent structures', 'rotating/switching structures so theyre automatically connected regularly with other structures', 'adding varying receptors so each component can interact with a higher ratio of structures', and 'ensuring inputs to connections/communication are present at all times', given how there are many ways to build connectivity, rather than just one way ('connecting existing/known input/output components') using common core functions like 'build', 'switch', 'add', 'vary', 'increase (applied to useful structures like inputs)', etc

    - identifying the interface structures of a known useful structure is useful (like the 'reasons for the usefulness of a structure' and the 'adjacent structures that make it useful like the surrounding system, available functions, inputs, etc') bc the surrounding interface structures are likely to also be useful structures and are likely to make some similarity/difference (such as a useful alternate problem format) obvious/trivial and usable for solving some other problem
        - for example, identifying the 'reason why' a known useful interface structure such as a 'constant' is useful (as in, useful through providing an absolute reference point, and useful in applying a structure as a constant can make "errors" obvious and can make "tests of truth" obvious), so that it can be applied in new useful ways, such as 'determining if something is true by making it constant' and "finding alternatives to 'constance' which are similarly useful at determining truth/falsehood of a statement" (such as identifying other interface variables like generality, simplicity, etc), and similarly identifying constants and the variables of constants is useful for identifying other constants from the 'variables that determine useful constants' (such as that they connect useful interface variables like non/linearity), and similarly identifying the systems/functions where constants are useful is useful for making other constants useful
            - relatedly, as mentioned elsewhere, this 'constant' structure is useful as a limit/filter of reality, as the 'set of constants' acts like a 'bounded area limiting possible solution sets'
            - similarly, its useful to identify other insights by making a structure more similar to other interface structures (make a structure a variable, make a structure a format, make a structure a certainty "such as a base", make a structure a concept, make a structure a requirement, and check if its more useful or obviously false in that role/position or fulfills other intents like 'being connective across useful concepts')
        - for example, 'identifying certainties' is a useful intent, such as how identifying specific certainty structures like constants is useful, specifically identifying constants that can connect useful differences (like how the constant of pi can connect differences such as linearity/nonlinearity or simplicity/complexity which are useful to connect), so identifying other sets of structures that are useful to connect and identifying constants that can convert one into the other is similarly useful, and these constants are useful to apply as default maximally different variables that can create useful differences (like converting between non/linear functions or constructing a network of numerical constants that reflects conceptual networks with nodes like 'simplicity' and 'complexity' and 'generality' and 'specificity', similar to how some functions can add simplicity such as derivatives, although constants are more interesting as 'unitary absolute reference points for a concept', acting as an 'interim point in a vertex between simplicity/complexity')
            - relatedly, identifying differences is more possible when a function is standardized, such as being standardized to intersect with (1,1), or standardized to fall in the y-range of (0,1), at which point its more trivial to detect differences
                - relatedly, identifying similarities is possible by connecting related numbers on the number line or other graphs with functions that have interval shapes such as waves, to see where various waves intersect (such as how many different interesting functions intersect at 1), or by identifying a new combination of attributes like 'curved inward and infinitely high in either direction toward 0 or infinity' or 'curved inward with negative slope' and making it intersect with (1,1) to standardize it
                - relatedly, identifying different core functions is a matter of applying changes in interface variables like 'variable position' (putting the exponent on the other side to create a logarithm, or making x a constant like e and putting x in the exponent position)
            - identifying these connections makes other graphs obvious, such as a 'number line with interface structures (such as "connections to other important numbers like the reduced set of numbers to solve most problems", graphed in the z dimension as a network or graph) and a common interface structure like "patterns/related numbers (indicated by waves or other interval functions in the y direction)"'
        - similarly, identifying certainties that are clearly wrong is useful, such as how 'capitalism' is 'forced selfishness' and 'socialism' is 'forced generosity' and both are clearly wrong in that theyre both forced so neither makes people free, and instead what would make people free is making resources abundant enough to require nothing to be forced of people, which requires (regularly) distributing intelligence, which is real freedom
            - applying almost anything as a certainty (by forcing it, such as by applying it as a constant/requirement) will make it obvious how that is wrong to apply as an absolute certainty, and if it is an absolute certainty, it will quickly become clear by resisting attempts to make it seem false, for example an absolute certainty like 'power is an important variable' will always seem true, no matter how much you change a system to try to make it seem false, bc of the definition of 'power' as semantically similar to 'important'
            - similarly, applying a variable as a 'generative variable' is useful to identify whether it can trivially generate other variables
            - this rule to 'find out if something is incorrect' (by applying it as a constant/requirement) is obvious once an incorrect structure is identified (a simplistic system like capitalism/socialism) and the reason for its incorrectness is identified (the 'forcing' and the 'simplicity' which doesnt reflect the real simplicity of reality, 'constance' and 'simplicity' being related errors)
            - then its possible to design a test ('force another variable to be constant/required' and 'force another variable to be overly simple' and check if obvious errors occur or if there is always a way that it still seems more true than false, no matter how much you force it to be constant/required or overly simple)
            - this is another useful truth/falsehood structure, in that 'obviously true/false variables tend to make it obvious whether other variables are also obviously true/false', as identifying the 'obviousness' of a statement is similarly useful as identifying the truth/falsehood of a statement ('obviousness' being related to 'simplicity' in some similarity or difference), similar to how identifying the generality/absoluteness or requirement/optionality or variability/constance of a statement is similarly useful as identifying the truth/falsehood of a statement, similar to how useful structures make other useful structures obvious through their adjacence/similarity
        - similarly, identifying 'how to make a constant useful' is similarly useful to identify, such as how applying pi as a coefficient "once other operations have taken place" makes it useful for generating randomness or nonlinearity (this property can be replicated by some subset of all possible number pairs when the pair is applied as a coefficient or exponent, where, by contrast, some number pairs generate lines that are nonlinear but seem linear in some subsets or significant ratios)
        - identifying alternate constants that can 'convert a structure into a base/unit/map/field/grid/graph', just like how (pi, 2) (and some subset of all number pairs when applied as a coefficient/exponent set) can convert a function into a nonlinear function
        - identifying these 'subset pair filters (which may be formatted as area filters)' to find alternate numerical pairs/sets (useful pairs like equivalent alternate 'factor sets' or 'coefficient/exponent pairs') is similarly useful, to find 'subsets of pairs that dont create other constants or other effects' (the subset of coefficient/exponent pairs that re-create some of the effects of (pi, 2) do not create all other effects, as some effects are distant rather than adjacent), these 'subset pair filters' being usable as a 'similarity index filter'
            - what is more useful than 'similarity indexes of all numbers' is a set of interaction levels where 'properties of number sets are trivially comparable (similarizable and differentiable)' where this interaction level set can store all useful variables and different interaction levels are created by some 'set of operations' that creates 'differences that are sufficiently useful to compare', so that the differentiating/generative variables of differences on those interaction levels are identifiable
            - similarly, more useful than a 'subset pair filter' is a 'generative variable/function of all pairs in a subset (such as a series/progression function)'
            - this answers the core question of 'what variable sets can be generated trivially by other variable sets, using which operations'
        - the problem of 'identifying useful constants' is not just a matter of solving the problem of 'connecting opposing interface variables like non/linearity or simplicity/complexity' but also a matter of solving the problem of 'identifying common factors across useful structures to connect', which is useful to identify as an alternate problem format of the original problem, which is useful in that it provides a more measurable alternate way to solve these conceptual problems, through formatting the problem in a more structural (and therefore measurable) way, which is another reason for the usefulness of a constant (it fulfills a structure, which is stable enough to act as a certainty/base/limit to support other variables, is measurable, etc)
        - connecting 'constants' to other 'certainty' structures on other interfaces is a useful 'cross-interface query' that can connect problems so that solutions/useful structures are reusable, which is like a 'stack of subset networks crossing each interface', where this 'constant' stack is useful to connect to its opposing stack, the 'variable' stack, so that it fulfills more interface metrics/structures (like 'multiple', 'equivalent alternate', 'difference-connecting', etc)
            - relatedly, finding 'vertex structures like vertex sets/networks/queries' across vertexes (like local/global and certain/uncertain) are useful to identify across problem-solving workflows that use multiple vertexes and spectrums and interfaces, so that 'finding common variables of vertexes and their useful sequences/connections' is more trivial, so that a 'graph uniting all useful vertexes to optimize for useful vertex structures like vertex queries' is similarly trivial to identify
            - for example, the 'function-intent' vertex and the 'function-structure' vertex are useful to apply in the same interface query (find 'structures matching some function', then 'find functions matching some intent', to find 'structures fulfilling some intent', which is a 'simple sequence of vertexes'), these 'structures of vertexes' being more trivial to apply to solve problems (with trivial operations like 'rotate' or otherwise 'trivially vary')
            - similarly, the 'constant vertex stack across interfaces' is useful to connect to the 'variable vertex' (or more completely/realistically, the 'variable vertex stack across interfaces') bc of the differences/information encoded in their connections
        - similarly, identifying other opposing structures is useful, such as identifying 'different' equivalent alternates (a 'similarity' structure) to identify other vertexes through these 'different' equivalent alternates which have an 'overlap' at some significant point/area but are useful in different directions/intents and useful in combination (united at that point/area)
            - for example, identifying 'different' equivalent alternates such as how a 'human brain' is an equivalent alternate of another similarly but differently equipped 'human brain', but a 'computer' is similarly an equivalent alternate to a 'human brain', and both are useful to retain rather than selecting one or the other (the 'different human brains' and the 'different computer as an alternate human brain' as an equivalent alternate of a human brain but in a different way than a different human brain is different, these 'different' equivalent alternates being useful to identify and store, so ideally a question would be evaluated by both an AGI model and a differently-thinking but similarly equipped human brain, rather than either one or the other, as theyre 'different' equivalent alternates that form a useful vertex where all/most variation can be captured effectively)
            - relatedly, an intersecting vertex set could form a structure like an 'info vortex' (such as in a conical shape) which would capture and standardize so much info that it eventually effectively 'closes to new info' or 'is destroyed bc it holds too much info' or 'destroys other variables not captured in the vortex'
            - relatedly, identifying connection structures like functions between these vertex structures (such as the core unit function like 'rotate' that can be iterated on some unit structure like a 'vertex' or a 'paradox' to construct all variation) is useful to identify other connection structures (like 'trivial ways to connect all info')
        - similarly, identifying how a 'requirement' could invalidate a 'requirement' (like if its more true than the other requirement) is useful, similar to to how identifying the limit of filterability and requirability is possible bc of invalidating structures (like how a requirement for a less causal variable can be overridden by a requirement for a more causative/determining/powerful variable, such as how it 'doesnt matter if criminals cant or wont be contributing members of society bc either way, they still need to go to jail')
        - similarly, identifying 'opposing structures' of useful structures is useful, such as identifying attributes that are not descriptive/determining/otherwise useful in regard to problems, such as how 'colorful' doesnt describe problems unless mapped to other known useful attributes like 'noisiness', and similarly, 'opacity' isnt useful as a descriptor of problems unless mapped to other structures like 'complexity' or 'obscurity', however all variables are mappable to problems but the 'degree/type/ratio/structure of change' required to make them useful is a useful attribute to identify

    - identify 'causes of requirement of a specific structure like a specific workflow' is useful to avoid those causes and those requirements, to allow other workflows to be useful, as a structure of truth (if a specific workflow is required to solve it, the facts that are known about the problem are likely to be false in some way, such as by being incomplete)
        - for example, identifying that 'unnecessary variation' or 'suboptimal variation (which helps errors evolve rather than more useful structures)' is useful to identify, in order to avoid suboptimal workflows like 'trial and error', which are usually only useful when a system has been allowed to become so complex that its not understandable, which doesnt describe any known system, as systems by definition have organization built-in to them, as often there are insights that reduce the set of solutions to some degree, so really 'trial and error' should never be necessary, as identifying 'causes of variation and errors' is almost always possible and where its not possible, that system is not understandable with current computation tools, which means the tools need to improve if 'trial and error' is necessary to solve some problem
        - relatedly, iterating through a subset of the list of filters of a solution set is useful as a way to filter out less useful filters
            - for example, some useful variable will be able to filter a solution set to reduce it by some useful ratio like by x%, and some other variable will also be able to reduce it by x%, so identifying these are possible adjacent/similar/equivalent variables is useful to identify in order to find their common input variables and generate other filters that would also reduce it by x% and therefore avoid those filters and find different filters that are differently useful
            - similarly, identifying the 'causes of usefulness' of a filter is useful, such as that it 'crosses a diagonalization' of a solution set (differentiates on more dimensions rather than just one) or that it filters out a 'significant ratio of a solution set' (such as irrelevant points like some outliers, redundant or otherwise non-determining points which create or dont change the same averages across averaging functions, randomly distributed points, etc) or 'filters different subsets than other filters' or 'filters more quickly or uses fewer inputs than other filters to identify the same subset as the other filters
                - similarly, filters like 'removing the midpoint of a point pair' (which is redundant in that it represents the average which can be constructed by some averaging/regression functions by the point pair) can be useful to reduce 'points to connect' in a distribution
                - similarly, identifying 'maximally different slopes' that can be created with 'relevant point pairs' like 'point pairs within some degree of difference from an identified general average slope from some subset (like a subset at regular intervals)' is useful as a reduced solution space to filter than the set of all point pair connections and which can create different functions trivially which can be considered equivalent alternates if they occupy a 'probable solution function range'
        - similarly, if a function like "reduce" is more useful when combined with an opposing function like 'reverse' (or 'change direction', so that it can also have other functions like opposing functions like 'expand' and orthogonal functions like 'equate'), its likelier to be a useful vertex for problem-solving or a useful cross-interface structure or a useful spectrum variable, as 'opposing/orthogonal' functions of a function are useful to include to generate self-awareness and self-regulation functionality to avoid known errors like over-prioritization, and similarly 'self-copy' is similarly useful for 'self-awareness' functionality (as in 'create a self-copy to evaluate the self-copy')

    - identify useful structures like 'areas of unsolvability in the required interim states between a problem/solution when applying a specific workflow' as well as 'variables (like maximally different equivalent alternate functions) to change a solution to improve it for a particular requirement/problem'
        - for example, given how common problem-solving structures like 'problem-solving intents' exist (which make them abstract problem-solving structures given that theyre common across different problems), these 'problem-solving intents' often have required solution structures (like how the 'intents' of 'merge two structures' or 'separate two structures' have a 'required structure' in common, which is the 'overlap' that is likely to be or is required in a default problem-solving workflow to fulfill these merge/separate intents, although there are other ways around it)
        - given that this 'overlap' structure describes the problem-solving process of multiple problem-solving intents (specific to a problem space that involves intents like 'merging/separating two structures'), in that it describes the 'interim connections' between a problem/input state and a solution/output state, it can be applied across problems to describe complex variable interactions using one structure, and determines the problem/solution in this case, so it should be applied as a determining/powerful variable by default when generating solutions
            - identifying all the functions like 'blur' and 'missing info (remove)' and other functions that can complicate the problem by changing how the structures appear, thereby causing error structures, are similarly useful to identify and oppose with 'counteracting functions that dont cause other errors'
                - abstracting this, 'identifying all the differences that can cause errors and opposing functions to those differences that avoid other errors and other suboptimal interface structures like over-prioritizations such as over-simplifications which could or will likely become errors' is a useful problem-solving intent
        - similarly, the workaround to avoid having to use an 'overlap' involves methods like 'take a unit component from the structure to copy and iterate it to create the copy (rather than creating an overlapping copy and separating it by changing its position)', or similarly 'identify input variables of a structure to copy and copy those instead of copying the structure to copy', which are alernative ways to solve the problem because they fulfill a different intent ('build from components' or 'generate from input variables' rather than 'copy and separate copies') identified using maximally different equivalent alternates
        - similarly, the 'interim space of possible states between complete overlap and complete isolation' is a useful space to identify 'areas of unsolvability' in, such as where its not computible to determine whether two structures are overlapping or not
            - relatedly, identifying the 'areas of noncomputability' in a unit of a more complex variant (such as 'whether two units of function networks have an overlap') is useful to determine the problem when complicated in a useful way (identifying 'overlaps' across function networks is likelier to be useful than just an overlap between two simple shapes)
            - identifying these 'areas of noncomputability' is useful to identify 'required changes to make a problem more solvable' such as 'changing the position/other attributes of a network to differentiate it in a way that makes the differences/similarities like overlaps clear'
        - similarly, identifying 'opposing intents' such as 'merge/separate' is useful to identify 'core problem-solving structures' that can connect the 'states required to make those intents useful' (states which could be either the problem state or the solution state given that changing the direction of these intents reverses the direction of the required solution and doesnt change the other variables of the states)
            - identifying more complex problem-solving structures can be done by applying more variables to oppose another function (rather than just one variable, such as 'direction' which is what differentiates merge/separate) and then finding problem-solving structures that are required in the interim space of connections between problem/solution states, where the solution state is identifiable by metrics
            - similarly, identifying 'solution metrics' of 'problems that are far more definable than the solution' is another useful problem-solving intent to fulfill for newer or more complex problems, for which the solution metrics add clarity to optimal filters of workflows to solve those problems
            - similarly, identifying the interface structures like 'limits' and 'thresholds' in this space are possible to compute by some workflow like 'trial and error' and, once found, can be connected to other structures that cause those structures (or other workflows can be applied like 'apply probable causes of interface structures generatively by default and check iteratively for a interface structure like a threshold until at least one example of each interface structure is found'), and then applied to other problem spaces to automatically identify those structures in other problem spaces, now that their position in a completely computed problem space is known & its connections to other interface structures is similarly known, assuming the 'usefulness' of the computations/complexity/other attributes of the problem make it similar or optimal enough to be relevant to other problems

    - identifying error structures that could result from otherwise optimal structures like 'generally applicable functions' is useful as an opposing/limiting/filtering structure of 'when not to apply optimal metrics like generality'
        - similarly, an error of 'over-application/over-prioritization' is likely when applying a function too often outside of its range of applicability/specificity, as a function may be generally applicable so it should only be used in specific contexts where it wont have negative side effects, such as where a function like 'absorb' is generally useful, but if applied outside of 'absorbing negative compounds' (if applied in such a way that it creates negative errors, such as 'absorbing positive compounds so they cant be used by the host') and if over-applied/re-applied in a situation where its already absorbed negative compounds and now contains negative compounds (so two errors are created, including 'ingesting the substance wont absorb negative compounds as its already absorbed its maximum', and also 'it may expose the host to those negative compounds its already absorbed and is bringing in'), it creates error structures
        - relatedly, another example would be an overly general antigen/antibody which confers resistance to some deadly pathogen, but also confers resistance to antibiotics as it forces pathogens to change and one of those changes might be antibiotic resistance
            - this is a 'type preserving operation', so that 'resistance to pathogens' also creates another form of resistance, which is an error structure if its resistance to a currently/frequently positive structure like antibiotics

    - identifying useful structures like 'insights' that can be used to generate alternate useful graphs to organize different info and fulfill different intents trivially
        - for example, a 'network of concepts with vectors indicating probabilistic connections between them' is a 'top-down' approach compared to a 'bottom-up' approach applied by a standard neural network that applies combinations of components to build more complex variable systems
        - similarly, a 'network of concepts where all adjacent sequences are true and more distant sequences are possibly true' or a 'network where connections indicate a proxy/alternate of probability (such as "ratio of each input component" which is related to a weight/probability coefficient, or "ratio of similar/equal attributes in common" or "probability of non-adjacent/eventual/emergent usefulness of a variable" or "probably useful difference/similarity at that position, given surrounding connections" or "overlapping ratio" across input components, or similarity/difference of the sequence, where commonly useful similarity/difference queries could be easily defined)' or a 'network where all interfaces are applied as possible connections, so that a "logical probability" (like a requirement or implication) and a "potential probability" (such as an adjacent possibility) and a "causal probability" (of causation in that sequence/direction) and a "conceptual probability" (given conceptual similarity or common conceptual connections/sequences) and so on all have probabilistic/similarity connections between each node pair' where alignments across interface probabilities/similarities would indicate a higher probability of truth, or a 'network of concept definition networks where multiple concept repetitions are allowed and aligned in a stacked network' or a 'network where probabilities are indicated by a ratio of connections (so a probability of 60% is indicated by 6 out of 10 connections leading to copies of that node, for specification of differences in routes to alternate nodes in a set with that probability)'
        - these and other networks are derivable by applying insights such as 'an adjacent concept to a solution can be more useful than probabilistic connections', and the idea of creating 'adjacent concepts to a solution' by applying changes to vectors building a concept (represented as a network of definition routes) and what variables would be useful to change such as the meaning of a connection between nodes (like 'probability' vs. 'similarity')
        - these networks would vary metrics/errors like 'only finding the simplest connections between nodes' which current neural networks might find
        - similarly, other variables are trivial to identify from applying insights and variables to generate new graphs, such as the variable of 'computation/memory/resource limits', which can vary by increasingly restricting computation by re-training iteratively, where a model can only use previous versions of itself and an increasingly limited ratio of computational capacity, requiring it to learn faster and retain only powerful variables and the variables generating their variation

    - identifying reasons why a particular solution like a function network (neural network) might fulfill structures by default/adjacently is useful to identify why/how to optimize those reasons/the structures they cause
        - for example, similar to how 'neural networks' are prone to 'using more variables than required' and similarly 'storing more info than required', they are also prone to other specific errors, such as 'favoring indexes too much, just bc they are given computation/storage', resources which they use suboptimally rather than optimizing that storage for patterns or other structures of understanding than simply 'mapping inputs/outputs', which is like 'storing all the different ways to spell a name rather than storing the root and the reasons a name might vary and the structures related to these reasons'
        - similarly, it might learn simple connections between words like 'structural patterns (such as grammar and usage patterns)' bc these are trivial to find with enough computation and example input/output indexes and simple combination functions, but these will be suboptimal for extreme use cases like 'understanding/meaning' such as 'prediction new priorities', and these simple connections arent adjacent enough to more complex functions like 'understanding' to adjacently create that
        - identifying the structures a particular function will select/create/use by default is useful when the implementation of the function can vary but doesnt, in order to identify why it doesnt vary (one reason being that 'simple connections between most linguistic structures are possible' while more relevant 'complex connections between linguistic structures are useful') and identify optimizations to make it vary as needed, so that it doesnt learn the simple version of a connection but the correct versions
        - similarly, identifying all the suboptimal usages and use cases and implementations of a structure like an index/map is useful, to avoid these known/derivable suboptimal use cases/usages/implementations

    - identifying common variables of known connections between extremely different structures is useful to identify new connections between extremely different structures (connecting 'maximally different functions' to 'symmetries' and 'solution-similarity and problem-difference' structures and 'potential fields' and 'fuzzy connections') and paths to those new connections (like 'iterate through various example structures from different interfaces, checking for connectivity')
        - for example, identifying similarity indexes in real systems is useful to apply as more probable variable interactions, to guide navigation across similarity indexes when filtering functions, such as how a 'two-way correlation can easily become a cycle' is a useful connection between similar structures that can identify different similar solutions than a purely numerical similarity index such as 'points of intersection' or 'similar slope sequences'
            - these connections between useful function types in real systems are useful bc they indicate 'adjacence' which is a proxy for 'probability'
            - relatedly, identifying numerical structures associated with 'missing info', such as how if a function has extremely high magnitude (high peaks), its possible that info in that peak is missing bc of how measurement tools often work, which is useful to identify as a possible cause of 'missing info', similar to how 'adjacent info in the real system' is likelier to be missing than 'adjacent input info in the data set' (except in cases such as where it was sorted and stored by input range), which is useful bc it connects the 'info' ('missing info' error), 'intent' (common intents, such as 'measure', given errors like 'missing info'), 'usage' (as in 'how are tools/functions used, given common intents'), and 'math' ('related function types likelier to have this error, given function usage') interfaces, which is a useful 'interface-crossing sequence' composed of 'cross-interface connection units'
            - mapping numerical structures to other interfaces may be the most useful vertex, which become more useful the more theyre connected to other vertexes
            - 'interface query vertexes' are useful to identify as 'reality-covering interface query sets', as a new way to apply vertexes (defined as 'sets of reality-covering variables'), where the vertexes are implemented by more than just variables, and in an organized way (like a sequence in a query)
        - I realized this by thinking about how 'independent/dependent variables' could be connected using structures like 'sequences from adjacence to distance' and vice versa, which indicates the structural sequence of creating 'independence' from a 'direct causal dependence connection', then wondering what other structures were clearly mappable like structures-concepts & other vertexes (like connecting independence to very different concepts like entropy, given that independent variables are likelier to cause the illusion of randomness, which is connected to entropy), then wondering what was possible to identify from combining vertexes and identifying all the connections within a vertex (from independence to dependence) and between vertexes (from independence/dependence to certainty/uncertainty), then realized a 'graph of overlapping potential fields created by iterated interface structures' would be useful
            - then I started thinking about 'similarity indexes' created from this 'graph of overlapping interface structures from potential fields' and then back to a useful structure, the 'set of maximally different functions' and how they are also 'maximally similar functions' (maximally different functions are the most similar function in a very different range from other maximally different functions, so they have a core similarity of being average in some range, where the ranges are the core difference), and ways to identify more useful 'similarity indexes', which led me to think about the 'most similar function to other functions' (a function that has a unit of other function types embedded in it, like hyperbolic, parabola, linear, step, non-1-to-1 mapping, etc, which is the most trivial to convert to all other functions), and how this function wasnt particularly useful for filtering functions, but was useful as a store of possible variables to apply
        - given that 'maximally different functions' are very useful, and that they are a specific implementation of 'a set of a similarity and a difference' (core similarity of 'being average in some function set' and core difference of 'that function set being very different in some metric like range/type'), what are variables of these implementations and what other specific implementations of this similarity/different structure are similarly useful:
            - 'symmetries' are a known useful 'similarity/difference set' as detailed before ('limit on differences applied to a core similarity')
            - 'similarities to solutions overlapping with differences from problems' are similarly useful (like 'concepts adjacent to solving a problem')
            - 'potential fields' are a useful 'similarity/difference set' in that they apply trivial changes (differences) to identify adjacencies (similarities) of a core structure
            - 'fuzzy/wobbly connections' are a useful 'similarity/difference set' in that they maintain some core connection 'angle range' between two structures (like a problem/solution) but allow some differences in the endpoints (structures similar to problems/solutions)
            - these all apply some workflow (like 'change a base solution') bc similarity/difference sets/networks form the basis of workflows, which seek to apply those sets
        - relatedly, a graph of overlapping 'potential fields' created by applying interface structures iteratively is a useful graph to identify, where these paths/ranges of overlapping interface structures in potential fields are useful to identify as possible interfaces/concepts, given their similarity through the overlap with other interface structures
        - given that in a 'high entropy system', all possibilities could be equally likely and more possibilities exist, 'trial and error' is somewhat useful (in that it accounts for 'equally likely possibilities' by trying them all or until it finds an acceptable solution) and also somewhat useless (in that a higher entropy system will have more possibilities to try), so 'entropy' is a useful workflow-filtering solution metric, so some 'modified variant of trial and error' is likelier to be useful in this situation, given the difference from an optimal case for that workflow, such as 'identifying maximally different possibilities, after identifying probable function volatility' (volatility increasing computation requirements for finding maximally different functions in a set of possible solution functions, and therefore being generative of useful differences like embedded differences and 'determining requirement' differences, in its extreme ability to determine other variables like computation requirements)

    - identify useful structures like new ways to apply a useful structure (applying 'system/case' as a generative variable of the usefulness of a function), and all the interface structures derivable from it (like problem-solving intents related to it ('generating new systems/functions'), abstractions/workflows that make it more useful (applying a workflow to a vertex to fulfill intents like 'identify all connections'), etc)
        - for example, a function's usefulness (whether its a solution or not) is often determined by the case where it's used, which makes it useful (its useful to connect cases with functions that are optimal in those cases), so identifying a new case is an input to 'identifying whether that case is an error/problem' and also to 'identifying functions that are optimal/useful in that case' and therefore 'identifying a new problem-solving function'
            - 'generating new cases' is therefore a useful problem-solving intent to fulfill other related intents like 'identify new problem-solving functions' (an overlap of the system/function and problem/case-solution/function vertexes)
                - 'generating new functions' is similarly useful in the opposite direction, but 'cases that are possible/probable' might be more trivial to identify than 'possible functions'
            - this is similar to how 'identifying a new error' is an input to 'identifying a new variable'
        - this is a result of 'identifying all the inputs/causes of the usefulness' of a particular structure (like a function) which includes structures such as the 'system where its being applied, or what other structures it could interact with, or the specific function inputs', applying the 'system where the function is applied' and its 'connections to usefulness of a function' as a generative variable of a useful function
        - similarly, the 'potential impact' of a function on a 'specific system in a specific state in a specific position' is useful to identify, to identify 'how a function will change a system, once applied in a specific position/state of that system'
        - the variations of these vertexes (all the structures where a connection can 'wobble' to create a 'fuzzy' connection) are useful to identify, to fulfill the workflow 'change problem/solution structures and connect those changed versions instead', applied to structures that are useful to identify all the possible connections of, such as vertexes
        - I realized this by thinking about 'differences in usefulness given an input case', and thought about the word 'determine' recently, so it was trivial to wonder 'what determines usefulness' and fit 'input case' to that
        - relatedly, the word 'determine' is similarly useful as 'identify', as 'determining structures' are useful to identify 'limits of computation requirements' (once its determined, no more computation is required)
            - relatedly, 'determining structures' are like 'adjacent concepts to solve a problem trivially' in that a 'determining variable' of the problem of 'filling a box' would be a structure that is adjacent to the size of the box, like a cube in a cube that almost fills the cube, in which case the smaller cube is a 'determining variable' of the other box, in ways that the 'simple set of dimensions of the box' are not (changing the specific smaller cube trivially solves the problem in ways that trivially changing dimensions of the cube dont, without an additional function of 'changing dimensions of the cube to create another cube that fits in it or contains it' which is not as trivial), so 'determining variables' of a problem are also 'adjacent/approximate solutions' of it
        - relatedly, the 'possible/required scaling to solve a problem' is useful to solve for as a problem-solving intent, so that iteration and iteration-optimizing functions can be used to solve it, which is more trivial with 'adjacent structures' to the solution (such as a similarly sized box as the solution), similar to how a unit case is useful for determining computation requirements

    - identifying trivial but important differences between similar structures is useful to identify variables of the most important trivial differences/variables
        - for example, 'filters' and 'limits' have a similarity in how they change a structure like a set, however, a filter would 'reduce solution count' of a set by 'removing some solutions' (after first generating a list of possible solutions), and a limit would be likelier to be positioned to reduce changes during the generation process or limit an area where solutions are possible to a sub-area (to find/filter or generate solutions in), as limits relate more to thresholds (limit above/below a threshold, as in a barrier to change) and filters relate more to attribute sets (variable values), as in 'find solutions with this pattern or variable', though the distinction seems trivial bc they can both be modeled as lines limiting/reducing an area, and similarly limits relate more to maximums/extremes and filters can identify interim/regular values following a pattern which is extremely different from extremes, so limits can pre-filter a set so that filters are more effective (limits help make filters more powerful/useful, however its not always possible to identify useful limits of generative functions and comparing/analyzing generated solutions is often necessary to identify these useful limits)
        - similarly, 'generate solutions' is made more possible by applying other structures, such as 'types' which reduce the computational requirement of the 'generate' function
        - the triviality of the difference between these structures becomes important in how they can vary and be used and otherwise interacted with regarding interface structures like problems/solutions
        - 'filter' and 'limit' have similar impacts on a solution set but are used in different ways, to filter out different differences, at different positions in the workflow (such as at 'generate' time or 'filter' time), which has an impact on possible solutions generated/selected so it impacts solution metrics, as well as performance metrics, in that a 'limit' positioned in a 'generate' function could fail to identify a possible extreme solution in a way that a filter might not be similarly biased
        - similarly, 'combining/grouping' functions (the opposite of 'filter' functions) are useful to connect, as they can replace 'filters' as a way to 'group relevant sets' for example, which could make 'filtering a set' unnecessary
        - relatedly, these functions are not always useful but are frequently useful when applied to different/opposing structures (for example, 'grouping all sets in all possible ways' isnt particularly useful bc its not computable but 'grouping filters', so they are pre-filtered bc of their group identified, could be useful)
        - identifying important differences between similar core structures is useful bc those similarities/differences will compound the more theyre used, which is likely as theyre common/useful structures, so once these are identified, their scaled impact can also be identified/derived
        - 'differences between core structures' will scale more and explain/determine more than other structures, so they will become more useful through their scaled interactions, so theyre more useful to identify

    - identify useful structures (like iterations of interface structures) which are useful for being adjacent and/or also useful for fulfilling some intent like 'connecting more interface structures' (which is useful for intents that involve 'connect' functions and 'input/output sequences' and 'interactive structures') and "'de-arbitrating/apply meaning to' a variable" (by connecting it to known certainties like known similarities/differences which are also relevant/useful/meaningful)
        - for example, 'filters of filters' are useful just like 'generators of generators' are useful bc every useful structure has problems, even if they are useful for solving some problem and theyre specifically useful at solving that exact problem ('filters' has a problem of 'filtering which filters to use with which interface structures like which cases or solution metrics', and 'generate' has a problem of 'generating generative variables', to extend the input/output sequence another degree)
        - by extending this input/output sequence another degree, another connection is made and some structures can be pre-computed as useful (whether generally or specifically useful, in simple/complex cases, etc)
        - 'useful filters' for example are 'maximally different' variables, so 'filtering the list of filters to include maximally different filters only' is often useful for differentiating intents, whereas numerical mappings tend to be more arbitrary unless heavily standardized (such as standardized to a reduced set of numbers that is adjacently connectible with a high ratio of functions, making that set a useful set for modeling other structures), so 'filters that identify only one solution' (such as 'identify the first solution above a threshold' or 'identify the best solution') are often less optimal for being less general, unless the trajectory to that filter is capable of identifying generality (such as 'simplifying filters' or 'regular interval filters'), and 'filter/identify filters that leave one solution possible' isnt clearly mappable to a relevant solution metric (like its implementations such as 'identify the first solution above a solution metric threshold' are), yes even filters have problems too

    - identify useful structures like structures which create/identify other useful structures, such as how 'power dynamics' impact 'info structures like certainty ratios/cascades' and the variables of interface structures of them (like variables of why they cause other structures to be created/identified)
        - for example, its useful to pre-compute various power dynamics like 'one ruler over all others' and how they interact with info structures like 'certainty', such as where a 'optimal or all-powerful AI' is allowed to exist, built from successive lower layers of inferior AI, which is suboptimal (just like having one government form or one government or one priority or one simple rule is suboptimal), as it adds certainty/info and decreases possibilities, to the point where the other possibilities lose their potential, and energy is directed to the all-powerful structure, which is worse for most other structures including agents
            - therefore, structures like 'one structure thats more powerful' are useful for creating certainties but its not optimal to create all certainties, as there are many ways to 'determine the universe' (blowing up as much as possible with chain reactions is one way), but what is more useful is a balance between uncertainty/certainty, so that alternative possibilities can exist, in case the 'determining method' was incorrect which can only be detected if its not selected/invested in, but rather detached from
            - identifying this balance as useful is possible using power dynamics
        - similarly, the structure of a 'predetermined fate' is that of a 'set of routes that all lead to the same path/point', which is only possible if all other 'predetermined fates' allow such a path/point to be traversed or to exist
            - this structure identifies other useful structures, such as how 'changing the variables like magnitudes/positions/overlaps/sequences/directions of cycles in a system' is often a useful structure to solve problems in complex systems (such as 'justice distribution' and 'supply chain optimization' problems)
        - similarly, the structure of a 'rotation of a unit' often means a symmetry is determined/understood, so abstracting that to identify an 'angle change' (or 'switch between dimension sets') and a 'unit to apply it to' (iteratable structure that can be used to compose/change useful variation around a symmetry, such as 'all rotations/angles' determining its differences) is a useful set of intents to find this structure to determine the symmetries in a system, just like determining the 'center/base and the change limit' are useful to understand the symmetry
            - relatedly, finding the 'symmetries of symmetries' is useful, to identify other symmetries that 'change symmetries around a base' and 'symmetries which act like limits of other symmetries' and other iterated interface interactions
        - similarly, 'other beneficiaries' such as 'adjacent structures' of a function that creates/uses some benefit is identifiable once 'cost/benefit' and 'incomplete' are applied (the function is an 'incomplete set' of the structures that could benefit)
            - I realized this by thinking about how an agent might not notice something for various alternative reasons, such as if theyre 'hoping for the opposite/busy implementing it to notice it/benefitting enough to intentionally avoid noticing it' and noticed that given this variability, similarity variability could apply to 'allocation of benefits' to other agents by proximity/adjacence or some other similarity (like parasites benefit from a host's resources, by their proximal position), bc an agent cant see some group dynamics easily if they sympathize with groups too much or if they obey incentives too much, so they have to be sufficiently different from groups ('relying on the group' too much) and incentives ('obeying incentives' too much) to see the errors of the group interactions or scaled interactions such as scaled incentive-obedience
        - relatedly, identifying 'interface structures' of the structures used in a workflow is useful for identifying structures of workflows like 'areas where workflows overlap' bc these interface structures were applied to workflows and iterated until overlaps and other structures were created, so these structures were identifiable bc of these iterated applications of interface structures
        - similarly identifying 'mutually exclusive' (they cannot occur together) and 'mutually required' (they have to occur together) structures as trivially convertible is useful, is an example of other useful structures that are trivial to derive from each other by applying interface structures (applying interface structures like 'opposite/extreme' to 'high variation variables' like 'co-occurrence/set membership')

    - identify useful functions that create useful structures like workflows from other useful structures (like an info structure + a system/logic structure)
        - identify structures that, when adjacently changed such as adjacently combined, create other useful structures, such as how the system/logic structure of a 'requirement (of a problem to solve)' and the info structure of a 'lack of information' creates the workflow 'trial and error' adjacently, as in 'when info is lacking, all possibilities need to be tried' (more specifically, until an acceptable solution (solution metric threshold as a halting condition) or all solutions are found (no halting condition))
        - this is derivable using the following changes (applying similarities with trivial differences to generate similar structures as the previous input in the sequence applied to 'one of the structures', then integrating it with the other structure 'lack of info' in the next sequence)
        - 'problem to solve' -> 'requirement (of a problem to solve)' -> 'general solution metrics' -> 'possible solutions'
            - 'lack of info to filter possible solutions' -> 'try all solutions' ('trial and error')
        - relatedly, a 'symmetry' is a specific type of 'difference in a similarity', as it is a 'specific 'ratio/type/combination/set/other interface structure of differences' that fulfills a metric of 'not invalidating the core similarity forming the symmetry base', where all the other similarity/difference interactions are similarly useful to identify and apply as common/core/default components of systems

    - identify useful structures like 'useful queries that lead to useful thoughts like identifying other useful structures'
        - for example, thinking about the following is more often useful than not:
            - previous/recent/useful workflows/other structures identified
            - problems with variation that seems to not be or is definitively not completely described yet
            - new connections between high variation structures like concepts, where the new connection might also have a new way to find the new connection (new workflow), and where the new connection might have the highest value given the variation of the structures being connected
            - vertex structures like 'common variables' of useful structures
            - identifying the limits of usefulness and its causes like where useful structures arent useful, to identify usefulness areas/ranges
            - random/extreme/scaled structures/variables and how they interact and how to connect/format them in new ways, given the high variation often required to connect them
            - new formats/graphs of problems which often have new solution methods in those formats/graphs
            - identifying structures that are not easily or relevantly described with change/component combinations (like a network of recursive/overlapping paths rather than a two-dimensional network), or finding the components/units that can be combined to identify structures not easily/relevant described with change/component combinations (units having some value in all of the available dimensions of the network)
            - identifying abstractions of other structures and how theyre connectible to the structures in a useful way (like overlapping types connecting to examples)
            - thinking about useful specific interface structures like 'similar but different' to identify useful structures like 'equivalent alternates'
            - newly identified core functions or other function types and how to apply them in a new way, which might be a new variable or workflow
            - new differences from identified useful structures which could lead to other useful structures (like differences from useful structures like common variables that would be useful to identify equivalent alternates of useful structures)
            - new structures/variables like 'variables that capture more complexity or some other metric/concept'
        - these structures focus on connecting useful structures, identifying new sources of variation and the interface structures surrounding variation, given its usefulness for determining new structures like new workflows, and other problem-solving intents, applied to specific structures like 'understood problems/concepts', 'recent solutions/structures identified', and other useful structures that are useful for their specificity and recent usage/storage in memory, meaning navigating them is more trivial

    - identify useful structures like the 'meaning of a similarity/standard/useful structure' to pre-compute the meaning of that structure rather than calculating it later when integrated into the meaning/interface interface
        - for example, the 'best' in a field could be identified by many standards (the best in the past century for 'technological access' similarity, the best alive for 'testability/provability', the best at using AI/other technologies to fulfill their intents, the best at fulfilling intents without AI/other technologies, the best at explaining/automating/understanding their field, the best given the variation they identified/added to the field, the best given the degree to which they made other tasks in the field trivial)
        - these various standards of 'best' have different meanings, which is useful to identify before selecting/applying them, such as how the 'best in the past century' is high variation and doesnt account for differences in decisions to use technologies they had access to, and the 'best alive' have a meaning of 'functionality like testing them on other problems'
        - pre-computing the meaning of structures like similarities/standards is useful to identify so that the ultimate meaning/relevance/impact (at scale in its interactions with other structures) is more probably predicted, which simplifies selecting structures

    - identify useful structures like 'non-1-to-1 mappings' that are useful to combine with '1-to-1 mappings' (or approximate mappings like extremes/limits) to identify structures that offer a mix of certainty/uncertainty and similarity/difference to fulfill a useful ratio of those opposing structures to cover high-variation differences common in problems
        - rather than asking what is associated, what is not associated (what variables dont seem to be particularly associated with other structures)
            - for example, just thinking about numerical values is often too isolated from other structures to be useful (theyre only useful when integrated with other structures like causal sequences of values, position of values, etc)
            - similarly, some variables are similar enough to be associated (extremes are associated with limits, as extremes are useful for identifying limits, just like how solving a unit case and then iterating it can identify limits)
            - other variables are unassociated and general enough to be difficult to connect (such as physical ratios like low/average/high which are commonly significant in determining other variables but arent clearly mapped to all useful interface structures - 'whats the certainty of low vs. high' - the 'certainty occurs if a low/high value adds more info, which isnt guaranteed by a low/high value, as opposed to a more clearly useful structure like a specific/abstract structure'), unlike how some variables are definitely connected though not equivalent in every way (abstract/specific, certain/uncertain, simple/complex), where their 'common structures' tend to be interface structures
                - this is useful to identify these 'low-info and less associated and more general' variables that are nevertheless very powerful structures ('commonly determining ratios' are useful to identify and apply), bc they offer a counterpoint to the definitiveness/certainty of interface variables
                - this means "when a problem doesnt fall to interface structures on one interface like similarities/differences, apply less certain/connected/specific variables like 'commonly determining ratios' which dont clearly map to any other structure in a 1-to-1 mapping)"
                - these general structures like 'commonly determining ratios' offer alternate sources of complexity than 1-to-1 maps like adjacencies or approximate adjacencies (such as extremes/limits)
            - the interactions of very different structures within an interface like similar/different arent always contradictions (an 'adjacency/similarity' between 'extremes/limits' doesnt invalidate the extremity/difference just bc a similarity is applied, as difference structures are connectible just like similar structures are), but rather similarities/differences are highly interactive/coordinating structures that can contradict each other but arent required to
        - this is bc some structures are not always clearly optimal/useful and not always clearly mappable (like abstract/specific mapped to certain/uncertain or simple/complex), so that variation in the lack of clarity is useful for adding differences to a solution where certainties like known useful structures arent sufficient to solve it
        - this is useful in the sense of identifying 'useful cross-sections' of a complex system that explain/capture a lot of its variation (cross-sections being another structure to format primary interfaces, which cover reality)
        - similarly, 'cost/benefit' and 'input/output' are not 1-to-1 mapped, but these structures would seem to be 1-to-1 mapped if incomplete similarities are examined ('inputs' are related to 'requirements' which can include 'costs', 'benefits' are often seen as the 'goal/intent' which is seen as the 'output' in some cases)
            - identifying these high-variation variables which seem but are not 1-to-1 mapped is useful to identify new variables and identify structures that cant be directly equated, so they should both be included rather than standardized to one structure

    - identifying useful structures like 'units/subsets/maximally different examples' of a useful structure like a 'solution automation workflow' (having functions like 'change scale' to solve/indicate a scaling problem or "test a subset of a workflow's functions for progress toward a solution" to test a workflow for usefulness) is useful to identify and apply to avoid workflows that only show progress if applied completely, filtering by workflows that are volatile in between the steps of the selected subset or which have other functions that could contradict the impact of the selected steps, as a way of checking which workflows/solution functions would be useful to invest more steps in by testing a subset or a unit
        - relatedly, identifying sub-problems or causative problems of the original problem can be a matter of testing associated solution functions of a set of problems, such as 'test if a sub-problem/causative problem is the scaling problem' (by "applying a 'change scale' function to variable sets" and 'checking if progress is made toward a solution')
            - similarly, 'units' of workflows can be applied to check for initial progress of a workflow, and similarly, 'maximal differences' of a workflow can be applied to check for possible predictable progress later in the workflow sequence, so identifying 'units' and 'maximal differences' created by workflows is useful to identify and store and apply to check a sub-set of a workflow's functionality for indications of progress toward a solution

    - identify useful structures like 'reasons for the usefulness of a structure' which can be applied as 'sufficiently specific filters' of solutions which have a useful effect of 'transferring the variation to other problems' like 'organizing useful structures to maximize usefulness' (such as only applying a learning function to a point below 100% accuracy, since a function that doesnt change at all isnt optimal in that it cant improve, and there are always new variables to identify in a given problem space, problem spaces being essentially equivalent)
        - for example, identifying 'reasons why a structure is useful' is a structure that provides enough info to 'act on' (as in 'select the function of') 'including that structure in an optimal solution', reasons such as 'this function is still changing and improving, so its probably still learning, and still resolving some uncertainty, while being correct enough that it seems capable of resolving that uncertainty, even though it does make errors at various points which are not predictable enough to be impossible for it to learn optimizations/solutions of, as its displayed the functionality to improve on past errors through not repeating them, and its new errors are not simply compliant with any identifiable pattern', which is a useful definition of an optimal structure that is specific enough to be usable, as 'specifications of definitions' are particularly useful for finding implementation functions of those definitions
        - similarly, having 'multiple alternate structures (equal in some way) to provide evaluation functionality' such as sub-networks in the brain which are equal in some way so they can evaluate each other's errors, or a host network that contains the other networks, which can evaluate its component networks as it is not a sum of its component networks but rather is its own network
        - these reasons are also solution metrics, such as how identifying 'functions whose errors are infrequent, insignificant or quickly fixed, not repeated (indicating the function fixed them), and also not compliant with any pattern (indicating its capable of learning)' (general functions) and similarly (but oppositely) 'functions with very predictable errors' (which is a useful function in that it specializes/optimizes for some specific task and gets some other specific task wrong, which is useful in that its errors will be easily identified and fixed with opposing structures), are useful functions to apply as filters of useful machine learning networks 
        - relatedly, identifying 'unimportant/irrelevant errors' is useful as a way of determining which errors a function should be optimized to avoid, if a subset of errors has to be selected for prioritization
        - so 'improving/changing functions that make new errors' and 'multiple alternates of useful structures' and 'opposing structures on some interface which are differently useful (like abstract/specific structures)' are useful structures to include in a neural/function network, but only in specific positions in the network structure, 'positioning/organizing useful structures to maximize their usefulness' being the problem to solve once the useful structures to include are identified
        - 'being optimal' is also a suboptimal structure in that it is low-variation (it wont vary once its optimal, in the universe where variation is time and preserving time is optimal), unless it optimizes for everything, which is unlikely
            - identifying these 'opposing intents/priorities/structures' like 'being optimal' and 'preserving time/variation' are useful to identify, as they act like dichotomies (to expand into more variables) or tradeoffs (to integrate with some more complex interaction like embedding) or ambiguities to resolve (by identifying some defining difference that is relevant and makes one more clearly optimal in a way that the alternate cant adjacently fulfill or provide an alternate to)
        - similarly, an algorithm that applies useful opposing structures within dichotomies (such as thinking and deciding/acting, which both take so many resources that one often has to be chosen, which creates error like 'inability to evaluate actions simuultaneously as acting, as the agent is too busy working to fulfill that action and thinking about adjacent structures like immediate outputs of the action and benefitting from or focusing on benefitting from the action to think about the action with scaled relevant interface structures that can identify the ultimate meaning of the action') like 'think more, act/decide less' is useful for developing good analytical functionality (functionality that can help avoid decisions and avoid having to make decisions as it can avoid dead-ends or other limited options), as an algorithm that can avoid most decisions (avoid having to choose one option bc of resource constraints) is more optimal, as it means the algorithm can identify alternatives, however never acting is also suboptimal but there is a ratio that is likely to optimally balance these two alternates

    - identify useful variables that act like 'dichotomies' such as 'universes' and 'computers' through their common interaction functions ('embedding', 'optimization')
        - the universe that allows the most/best computers is one that can contain all other universes, bc computers offer the possibility of a multiverse of many different 'maximally/relevantly different' timelines (simulating the past and other components of reality better than reality can, as past info was lost for the most part, with better storage/representations/retrieval/etc, allowing different possibilities to become real in the simulations, thereby applying more variation to past info than was preserved in reality and therefore making the past more real, similarly when simulating the future the future becomes more real if the simulation is good enough), where if a computer has a good reality simulation but cant compute something, it either needs to improve its reality simulation or that something is false, which can be checked against other computers that simulate different realities to check for the same result, an indicator of falsehood of the something if repeated across simulations, simulations which have errors like 'attracting variation from other universes, so that they live in this universe in a simulation that is better than their reality instead' which requires identifying 'what type of life form would be attracted to a particular reality and what are their possible rewards/intents', and therefore 'more optimal paths' between different simulations of the past/future can be connected than the paths that occurred in this timeline (so 'computers enable constructing different timelines and traveling across timelines'), and if one simulation/computer or simulation/computer set is drastically more optimal than the others it could very well contain the others, including this universe
        - I realized this by thinking about how 'historical figures could be brought to life by a really good simulation (that is better at storing info than reality, which lost that info of their life)', which made me think about the iterated effects of many optimal computers given their simulation power, how real a simulation could be, how it relates to existing structures like multiverses, and how these computers/universes could interact and what side effects would occur
        - what seems likelier than a QFT/multiverse model is a set of overlapping universes, as assuming they have isolateable effects may be assuming too much, and given that they vary by dimension, some are likely to have dimensions in common with other universes and may reflect these dimensions across the universes which have those dimensions, so traveling by these 'common dimensions' may be possible given their possible interactivity or overlap, and rather than repeating a dimension independently/in isolation, its likelier that common dimensions overlap/are re-used, unless so many variables have been enabled that divergence of the dimension into multiple alternates is possible, but initially its likelier that they are overlapping/equivalent
        - relatedly, computers that allow higher dimensional modeling may be useful for modeling universes with the most dimensions (able to contain the other universes)
        - relatedly, 'sending a computation to another universe (simulated in a computer) where its more computable' takes time away from that universe (as it involves 'required work' which takes away time/resources from whatever it was computing), and a 'universe death' occurs where its computational capacity is reached and no time/resources/possible changes are possible in that universe
        - a 'universe grid/network' of hypercomputers that sustain a reality that is more optimal than default reality could be a useful structure in future, where 'killing a universe (in its default purpose of an uncertainty/potential-maintenance device) by overloading it with computation requirements' can be avoided for required components of the grid/network, which would form the new base of reality (the highest variation life would occur on this grid of computers)
        - the grid that requires the least work from other/component universes and allows them to compute whatever differences they compute by default (which are useful in some way like computing new differences, or improves their computation so they can compute more of what they tend to by default and have the option of computing other things) and stabilizes any possible universe simulated on a computer (integrates it with the other universes so they can coordinate) will be more successful
        - this grid is a more optimal way to 'travel the universe' as it makes everything trivial to compute (including different ways to achieve space travel, different routes of space travel, etc)
        - when universe simulations are optimized in this way, every universe simulation will likely converge to the same universe model, once an optimal one is found, at which point keeping the computers distributed or merging them is a question to answer, a merging which would mean there is one final universe and once it is optimal, a question of 'why would it continue to exist' is default at that point (is there variation that is only obvious at that point, such as the variation of destroying nonoptimal structures, in an optimal universe that supports the most/useful variation, which it would continue to compute stabilities and other optimizations of rather than ceasing to exist), or would it be better to keep the distributed computers so that they could evaluate/replace/fix the others and identify when they were making an error of some computation, and is that possible if theyre all optimal, and if the variation of destroying suboptimal structures is the only variation at that point, the optimal computers would all have to stay the same or be destroyed by the others, which would likely leave zero computers, as an optimal computer might be subject to some new variable it hadnt computed before, such as if its optimal for a previous definition of optimal but not a new one, given the new variable, and is an unpredicted variable possible in an optimal computer (could variation develop quickly enough or at all, so that an optimal computer, which can compute everything that exists or is likely to exist, could still fail to identify the new variable and be found to be less optimal by the other computers which didnt encounter that variable yet, which implies that its useful to keep these computers distributed so they can learn from another computer's failure to account for a new variable such as this, assuming its possible, and the 'optimal structure of a universe' is actually 'multiple copies of an optimal computer simulating reality equally well' rather than the 'merged and then nonexistent computer', as if the multiple optimal computers fail to encounter any unpredicted variable at any point, they will just remain stable infinitely, which is also the end of time, just like the merged computer is the end of time as they are the end of variation)
        - the core structures of the universe (like 'multiple' and 'network' and 'alternate') also seem to describe its final state, where first these structures were used to generate variation, and then these generated possibilities were filtered to identify optimals, which resulted in a game of 'distribute vs. merge' that has the same outcome (the end of time), and this final state was described by these core structures again
        - relatedly, once its known how the universe will end, the various paths there become less interesting (less high variation), so the moment its known, the universe has basically started to end (it reached its peak variation and will start to decline in variation), and that is similar if not equivalent to ending it, so the variation becomes 'how to change the path to avoid the final state, or apply/embed variables so that state is never reached, like always finding new opposing variables of variables that cause progress toward that state so that it is never reached', though there are limits on when this 'variable finding' can happen where it could still change the path sufficiently to continue to delay the final state forever (identifying confusion structures like high variation structures and applying them enough to avoid a straight path to the final state), as one sequence of time is unlikely to be a stable structure, even if it fulfills the definition of stability and optimality in all known ways, and relatedly given the value of alternates, as what is currently known/computed may be proven false or changed at some point
        - 'finding alternate paths' would have a solution structure of 'identifying different ways the universe could end than the path its trying to diverge from (merge or distribute, ending time either way)', so identifying 'alternate end sequences' (using different workflow functions than merge/distribute) would be useful to identify the 'paths to those end sequences'
            - for example, identifying a 'end sequence' that uses 'generate/filter' (by generating the variation required to sustain time, then filtering that set of variables for usefulness as needed) is an alternative end sequence than the 'merge or distribute' dichotomy, as a 'end sequence' with only two options, where only one can be selected, is not an optimal structure (where the definition optimal involves 'maintaining variation/time' as top priority), so in reality, an optimal computer would not produce such an end sequence, which contradicts the progression outlined above involving a grid of optimal computers
            - however an optimal computer may identify different priorities than 'maintaining time/variation', so that definition of optimal may not be stable/valid

    - identify useful variable interactions (like truth structures, like truth patterns) that make useful structures like different possible solutions clear ('ambiguous alternates' as 'possible ways a statement could be true or false', which once identified, can be clearly solved with some trivial similarity/difference check - given how variables are related, how true/false are related, how similarities/differences are related, etc)
        - for example, a frequent truth structure is 'x is related to y, but y can result from x-distorted as well as well as x-opposite as in x-negator', as in 'x true y, and x-false true y and x-opposite true y', which is a 'realistic/truth structure, including contradictions/differences/neutralizations and alternatives'
            - identifying truth structures is useful bc truth structures are likelier to be related to other truth structures, like other useful structures (a truth structure is likelier to be stable, so its likelier to have more interactions, such as more interactions with interface structures such as limits/requirements/alternates)
                - this truth structure makes it possible to identify related useful structures, such as 'reasons/ways it could be true or false' 
                    - identify possible different solutions (ways it could be true/false, positive/negative, abstract/specific)
                        - this could be true bc of different effects that happen at different amounts, among other possible causes
                           - or it could bc false, in the sense that it could mean that x does not cause y at all ('x true y' is false), since changing x in these ways doesnt change the causation of y
                    - identify similarities/differences to resolve the ambiguity between these alternate solutions
                        - 'different effects at different amounts' (true structure) is an ambiguous alternate of 'no causal relationship' (false structure), which is useful to identify
                        - limiting the number of these alternate ambiguities is a useful truth-creating structure (the fewer ambiguities there are, the likelier each possibility is)
                        - identifying ambiguities between true/false structures is useful to identify alternates to filter out, by applying the 'true/false dichotomy' or applying similar/different to 'true' (the opposite of true is false, so finding a structure fulfilling a 'false' version is useful)
                        - similarly, finding 'ways to connect/differentiate these alternates (different effects at different amounts and no causal relationship) and their common variables (x, y)' and 'adjacent interface structures (like limits and implications) of the alternates' will be useful to resolve the ambiguity
                        - connecting 'different effects at different amounts' and 'no causal relationship' (independent, indirectly connected, incompletely connected, random, etc) can be done by changing the 'different effects at different amounts' structure to be more 'random'
                            - then checking if these functions creating randomness are available in that system, and checking how trivial it is to convert it into a random structure, is useful to identify the significance/truthhood of 'different effects at different amounts', where randomness can be a likelier structure than any other specific variable relationship
                            - similarly, checking if 'different effects at different amounts' is a variant of 'no causal relationship' is useful (checking for a similarity between these alternates)
                            - similarly, checking for a similarity can also take the form of checking if 'random/opposite structures can also reliably create the same variable interaction' is also useful to identify if x is really significant as a cause of y
                            - similarly, checking if 'only specific values' of x cause y (if only a small number of possible x values cause y, or if trivial changes to x causes it to stop causing y, indicating fragility of this causal connection, it might not be a significant causative variable of y compared to other variables, which might cause y more) and if there is an identifiable 'reason why those values might cause y' (a 'causal' similarity between x and y) is useful as an additional filter
                        
    - identifying useful structures to identify like 'variables in common among important structures like problems/solutions' and high variation variables like 'selectivity' (a 'filter' structure encoded in other structures by default, as structures can filter out various structures like interactions and functionality) which are useful in identifying new insights (like 'logical input/output sequences between previously unconnected variables (commonness/problems and commonness/selectivity and structure/filters) used in those structure sets')
        - for example, filters like 'substances that are not active in human bio systems (such as pathogens that dont kill humans)' might be active against error cells (like cancer cells)
        - similarly, 'selectively active/interactive compounds (which dont interact with much or arent activated by many inputs)' are a good solution space to search by default, bc a 'highly selective compound' is likelier to be useful in killing 'one specific cell type' (such as 'cancer cells') than an 'unselective compound' (which is likely to have many interactions which could easily cascade or cancel each other and which is likelier to kill many cell types, including healthy/helpful cells)
            - deriving this is a matter of identifying the 'priority/intent' of 'only killing harmful cells' as important to fulfill, which is a common problem in the 'medicine' problem space, and identifying a 'matching similarity' in the 'selectivity of compound activity' with the 'selectivity required to only kill harmful cells' (identifying this similarity in 'selectivity available' and 'selectivity required' by that problem-solving intent is the important function)
            - this means 'highly differentiating filters' are useful, as compounds with high selectivity and low activity/interactivity (which might be useful against one type of cell) are likely to be rare and different from many other compounds, as its often more useful to have higher values of these metrics so compounds that only bind with one other compounds are likely to be rare, and also theyre likely to be rare for another reason which is that the 'set of all possible chemical sub-structures of a molecule' is very limited, as there are only so many structures, which means they will be repeated often, and therefore there are likely to be multiple similar structures to a particular structure and also given that every structure has multiple adjacent variants, they are also likely to have interactive (opposing) structures with multiple adjacent variants, which means theyre likely to be interactive with multiple structures than one or a limited amount
            - antibodies are often highly selective/specific by default which is why theyre useful, but there are other selective structures which could be possible solutions for functions like selectively killing only error cells
            - relatedly, given the common occurrence of anti-cancer compounds that exist within a large type (such as antifungals or antibodies), every structure type with a large number of structures in that type set is a possible solution set to identify 'the anti-cancer compounds in that type', given that a 'large number of compounds in a type' implies 'high variation in the type' and 'differentiation of functinoality resulting from this high variation', 'anti-cancer' activity being a sufficiently common function that it can be expected to be found somewhere in a function set above a certain threshold of complexity/variation/size
            - given that the opposing variable of a useful structure is often useful as well, the 'abstract' version of this 'specific' useful structure is, for example, identifying the types of compounds with high selectivity, which allows testing other structures in that type set
        - this speaks to the usefulness of useful structures in identifying other useful structures (which is likely bc theyre likely to be similar in some way through having a common important type of 'useful', variables in common like 'high variation' and 'interactivity' and so on)
            - identifying a useful 'selective compound' (like a virus that only kills cancer cells) makes it trivial to identify 'identifying high selectivity compounds' as an important intent, at which point its trivial to identify core interface structures of selectivity like 'variables of selectivity' and 'extreme cases of selectivity like extremely low/high selectivity compounds'
                - identifying the 'selective compound' could be done by identifying 'rare functions' (like 'only killing error cells') which are likelier than common functions to be useful given that common functions, which are usually available/used/active, are not sufficient to act as anti-cancer structures
                    - this fulfills the intent 'what is different from known useless compounds like common compounds'
                - similarly, identifying that 'unselective compounds' are a core and common problem of existing medicines is useful to trivially identify 'highly selective compounds' as useful
                    - this fulfills the intent 'what is different from common/core/sub-problems'
                - similarly, identifying other mechanisms than 'killing error cells' as equivalent alternates is useful, such as selective/specific functions that bind to error compounds, increase metabolism of error compounds, increase proximity of error compounds to the skin, making it likelier to be excreted in sweat, increase isolation of error compounds, increase usage of error compounds in existing processes as inputs to those processes, etc
                    - this fulfills the intent 'what is different (in structure) but similar (in functionality) from known useful functions (like "killing error cells")' (finding 'equivalent alternates'), or similarly, 'what are the variables of useful functions like "killing error cells" and how to use those variables to generate the other variants (such as "try all combinations of variable values, then filter by functionality")'
                - identifying the 'commonness' of problem/solution structures is therefore useful to identify and apply as a way to guide the interface query that finds useful structures like similarities/differences (find similar solutions to common problems, or find differences from common useless/error structures), which adjacently identifies other insights, such as that 'problem structures are likelier to be common'
                - the core insight at work here is that when a variable connection is identified (unless its a very simple linear direct connection), its likely to have other relevant variables of cause such as 'interim nodes' in the 'causal sequence/network' (useful to identify as 'more direct causes given the increased proximity of causation') and 'more linear connections' in the causal sequence/network (such as how connecting A to B and B to C can also easily involve a more linear connection between A and C in some systems/cases), which means when a problem is related to a variable like commonness and problems are related to solutions by structures like differences, its possible to identify other connections such as "applying differences to a problem to create a solution", which is simple logic applying interface structures
                    - relatedly, identifying the other 'causal structures (like direct connections) associated with specific cases (like where more linear/direct connections might be or definitely are possible)' is a useful problem-solving function to fulfill
                - relatedly, identifying the 'cases' possible in a 'system' is useful to identify 'possible input cases' for a specific function in that system, such as identifying cases that allow a linear connection between A and C as referenced above, by identifying the system in which those cases can occur and checking if that system structure applies
        - relatedly, 'unique/maximally different' structures are likely to be useful in identifying 'new directions of change', as structures that are unique compared to other structures and different from other unique structures are likely to align with primary directions of change
        - relatedly, the dichotomy of 'adjacent vs. emergent functionality' is useful to identify, as a way of connecting functions that trivially fulfill an intent vs functions which require iteration or other non-trivial structures to fulfill that intent, where connecting these equivalent functionality structures creates a similarity index that identifies 'possible variable interactions between adjacence/emergence', which can be used to 'generate alternate functions that vary by adjacence/emergence' once those connections between adjacence/emergence are known (connections which will often involve known interface structures)
        - interestingly, important interface structures like 'filters' are also important variables of structures, such as how a structure can filter out various variable interactions or functions by default, so a filter is a built-in/default variable in structures (identifying the 'filters' applied by a particular structure is to identify a similarly valuable structure as the 'intents' fulfilled or 'functions' used by a structure), where the 'filter' format of reality was already known but identifying the specific usefulness of 'filters fulfilled/used by a structure' is useful to identify 'selectivity of a structure' as useful/relevant in 'medical' problem spaces
        - relatedly, identifying 'differences' in problem-solving structures like problem-solving intents is useful to identify and apply, bc different problem-solving intents are likelier to produce different solutions (bc of the input difference), and producing different solutions is useful to identify 'complementary' problem-solving intents (if you solve this intent, youll get this info, and if you solve a different intent, youll get additional/different rather than repeated/similar info of the original info), bc identifying 'differently useful' info is useful, so identifying 'differently useful problem/solution structures' is useful by default but also bc of the applicability to relevant structures like problem/solution structures
            - relatedly, identifying 'multiple sets of useful variables' (like structures that fulfill both 'relevance/usefulness/similarity to useful structures' and 'differences in usefulness (differently useful structures)') identifies a solution space of structures to check/filter, that could be useful for fulfilling multiple useful variables, which is a useful intent to fulfill ('identify all the structures that fulfill each combination of useful intents')
            - relatedly, identifying all of the 'interactions between similarities/differences' is useful to solve for 'volatility', such as how identifying all the similarities/differences that produce very similar (trivial) and very different (extreme) differences in functionality are useful to identify
        - relatedly, 'function usages' are a reflective structure of 'functions available', since a function often only exists if its used, so checking 'functions that are used' is a useful structure to find 'functions that exist', and similarly, 'extreme function usages' can identify emergent functionality when existing functions are used in a scaled way
        - relatedly, finding 'randomness filters', such as 'filters to identify evenly distributed sets, similar sets, and sets outside of these known patterns/attributes as in sets with no discernable attribute' are useful in decomposing randomness, and similarly, 'common structures' and 'continuous structures' are more probable by definition, so adjacent structures are likelier to occur than extremely different structures, as stable structures are likelier to occur than volatile structures or other extremely different structures, as useful structures often have variables in common, one of which is stability, so a completely unorganized randomness-generating function would have to contradict this organizational certainty structure of reality, as a pure randomness-generating function would generate these structures reflecting reality at some point (adjacent structures), so preventing a function from generating them would not be random, even though it would be less organized (which is so unorganized as to have an over-prioritization error of 'too much disorganization' to be random, as 'disorganization' would be required in that less random function and therefore it would be predictable by whatever structures were disorganized, generating strings that are highly different across strings and characters every time rather than highly random), as pure randomness would reflect reality, and non-pure randomness would be incorrect in some way by some bias (favoring one structure artificially), so non-pure randomness functions are a way to derive what isnt real just like pure randomness functions are a way to derive what is real (if there are favored options in reality like stability, they will be selected more by a purely random function, which reflects its input data fairly/randomly, so pure randomness functions are a way to detect probability of a variable in reality, which may not be equal to all other possibility probabilities)
        - relatedly, a way to improve the success/reward of a structure like a 'requirement to guess a randomly selected integer of length n only once (one-shot learning)' can be optimized with structures like 'making other guesses less accurate so that rewards dont need to be shared', and scaling this structure to make 'many guesses inaccurate', which is likely to not only increase coverage of the incorrect/error set (incentivizing large groups to select a set of less likely answers, given some less-than-random algorithm such as one that doesnt pick structures that have repeated digits, or less likely answers such as answers complying with rarer patterns, making selection of one of these less likely to be randomly selected, and similarly decreasing the number of multiple tickets that need to be bought to cover the remaining solution space), but also to distribute any possible rewards of these inaccurate guesses so much that they approach zero (creating large groups that guess the same thing), which involves making a strategy like 'buy multiple lottery tickets' likelier to be successful, these optimizations that improve existing strategies being useful to identify
            - relatedly, given that 'resources reflect luck rather than value', 're-assigning resources regularly, so that they reflect value such as potential, is likelier to be optimal, to distribute and increase/create luck through the higher probability of success created by that strategy' and relatedly, a structure like a 'debt to repay' if an agent with resources hasnt created a similar degree of value than that reflected by their resources (misaligned value and resources) is a useful structure to identify, to avoid errors of misaligned resources/value, which should be similar rather than differing, this alignment being important for various other intents
            - similarly, a 'debt forgiveness threshold of wealth' where extremely wealthy lenders wouldnt be paid back by debtors, if their wealth is sufficient to sustain their income with interest and if their wealth was not reflective of their contribution and if their wealth is being invested suboptimally (such as in fossil fuels or soda)
        - relatedly, identifying a 'problem type' is a matter of identifying the determining structures of the solution (does 'iteration' or 'equivalent alternates' or 'alternate definitions' or 'limit' likely determine the solution, meaning if you iterated enough or determined the alternate definitions or identified the limits, would that solve the problem? then its an 'iteration/equivalent alternate/alternate definition/limit' problem, after which the problem is then identifying/applying those structures in the correct implementation structure, such as the correct sequence - which is similar to 'identifying high-level intents that would improve a problem', after which the problem is then 'implementing the intent with functions that are similar to that intent')
            - an 'iteration' solution is identifiable as determining in cases where its clear that 'some available/identifiable calculation is likely required' and 'there is a clear limit on the iteration of that calculation' and 'info increases with each iteration so that progress toward the solution with each iteration is required/defined/otherwise guaranteed', then its an 'iteration-solvable' problem

    - identify useful structures like 'opposites' of workflows which can often be similarly useful as workflows (just like how identifying specific vs. abstract structures can be similarly useful, despite their difference)
        - for example, 'filter solutions' has an opposite of 'avoiding filtering solutions', which could be implemented by taking the form of 'identifying how all solutions are similar', which uses an adjacent similarity (the similarity of 'connections between solutions (similarities)' with 'filters of solutions (differences)') to identify info retained which is usefully different (in the case where you have functions to derive connections between variables such as solution variables to identify connections between solutions)
        - similarly, 'change a base solution' has an opposite of 'change a base error' (applying opposites to negative structures like 'errors' until theyre not an error), just like how 'identifying cause to identify generative variables' has an opposite of 'identifying outputs/scaled impacts' which is related by the 'similarity between input/outpuut of a function', where some info is retained in either input or output, so applying this similarity to identify other workflows is possible
            - given that an existing solution and an existing error are both unlikely to be the solution for a new problem, finding the useful 'interim balance point' between these is a useful problem-solving intent to fulfill
        - other structures than opposites are useful in this workflow, such as 'vertexes' (such as 'maximal differences' or 'cross-interface structures'), which are useful in identifying similarities between high-variation variables like interface structures and are always connectible using interface structures, so they can be used to identify other workflows (a workflow using one structure in a vertex can be converted to using the other structure in a vertex, given the similarity encoded in the vertex)
        - similarly, the 'level/type/structure of difference/similarity required for a workflow to be useful' (like generate differences and filter differences until similarly maximally different solutions or similar variables of solutions are found, or change a base solution until similar to solution metrics or apply extreme differences until other differences (relevant to solution) are obvious) is useful to identify, in order to identify other variables to create other variants of workflows

    - identify useful structures like 'required differences' (such as tradeoffs) and related structures like 'dichotomies' (which often dont have viable options to mix both extremes) which are useful in optimizing queries related to 'solution filtering' (identifying tradeoffs is useful to guide design of solution-filtering queries)
        - for example, when paying attention, there are often fewer errors bc of the planning functionality added by paying attention, so success is likelier and more trivial when paying attention and planning some number of steps ahead that accounts for required changes/sequences/structures, however when not paying attention/planning, randomness is a default variable and can produce 'useful errors' in that, up to some trivial degree, a difference from an optimal path ('error') requires adding additional stored functionality, when the error is still reversible and fixable, so functions like 'predicting fixable thresholds of errors' and 'creating/enabling errors up to a fixable threshold' can be useful for general functionality, which is a way to use randomness to optimize a path (a path that fulfills multiple functions rather than the original intended function)
        - the cost of paying attention is 'lack of error info' and the cost of trivial fixable errors is itself 'trivial lack of attention (as its required to fix the errors)'
        - query-planning around 'fixing these trivial errors to add more functionality' can be optimized, such as where 'original intent paths are obvious/trivial to fulfill (like a "section of road with no cars ahead" so that temporarily handling some other error is possible while trivially successfully fulfilling the original intent)'
        - an algorithm should be allowed to be 'distracted' by solving some other problem temporarily, such as when 'testing an alternative function that diverges from a known average metric to resolve whether some new variable is useful', which is useful for 'finding new variables or similarity indexes (to use in future navigations of similarity indexes)'
        - this tradeoff between 'planning/organization' and 'allowing a degree of randomness, below some fixable threshold (meaning where the function isnt so general/specific as to be useless as its had its info removed/conflated with randomness or proxies of randomness like complexity), that will be useful in adding functionality (such as by identifying new variables or resolving new error types)' is the balance between 'assuming an optimization strategy is generally useful and sufficiently complete to solve all problems or a whole specific problem' and 'assuming this problem space could be new in some way', which is useful to identify as a 'tradeoff' structure, where 'planning for randomness' is a useful balance between alternatives
            - relatedly, identifying the 'paths to randomness' is useful to identify possible inputs of randomness (like 'how many times would this function have to be applied to generate a high probability of containing a random sequence', and similarly identifying structures of randomness, such as overlapping shapes with a common center and a high ratio of overlapping areas, which would seem like a random distribution), similar to how 'confusing problems' often involve the same structures (such as ambiguities, self-references, iterated abstractions that are more similar to the solution than to existing abstractions which need to be derived, and other structures of complexity and false similarity), so these structures should be looked for by default given a high 'confusion' metric score of a problem
            - similarly, identifying the 'paths to convert useful structures (like extreme differences) into other useful structures (like extreme similarities/equivalences)' is useful to completely identify, such as how defining a 'intersection' as having 'one point of similarity' is useful to identify changes that could convert it into a complete equivalence (like 'rotating one line to be parallel, and basing their midpoints at the same point', once its derived from the definition that 'one point of similarity' indicates a 'perpendicular' interaction type)
                - for example, it would be useful to identify all the 'structure sets' which could adjacently derive how a particular system is organized, which are equivalent alternates and therefore are useful to connect by identifying their variables
                - similarly, if there is a 'ratio of similarity/difference to other structures' that any real structure has, that is another useful structure to identify (some structures are so similar that they only vary by position, but that is an important and high variation variable, rather than a trivial difference, which is useful to identify, as 'changing position of nodes in a network' is a useful function for problem-solving workflows involving 're-organizing existing components of systems to optimize the system')
            - similarly, its useful to identify structures that reflect reality better, such as how 'continuous' functions are likelier to describe real variable interactions than 'discrete', as 'discrete structures' still have components that can be incomplete or combined to create non-discrete values, so function-finding filters should be slightly biased in favor of finding continuous functions or function networks that give the same effect as a continuous function, except where there is too little info to infer a continuous function, or where discrete points are maximally different and could be increasing in difference rather than similarizing
                - relatedly, its useful to apply symmetries to errors, such as where an outlier higher than other points exists, rather than identifying it as an error, applying a symmetry to identify a data set where that outlier is an average or another extreme (lowest point in another distribution), rather than grouping it into the original data set, as overlaps across systems could easily produce outliers if mistakenly grouped with the wrong data set
        - the usefulness of this is that the 'planning vs. error-handling' tradeoff is useful to maintain, even require, so that new variables are always being identified, as where existing planning functions are sufficient, no new unhandled variables are being identified, which is suboptimal
            - similarly, the 'checking one solution vs. checking multiple alternative solutions' tradeoff is useful to maintain (sometimes one solution stands out as obviously useful to check so checking the obvious optimal shouldnt be prevented, such as in workflows like "change a base solution" where checking the base solution on its own is a useful function to apply, whereas checking multiple alternatives is useful where resources are available or where required like in a problem space with some non-extreme degree of randomness, these extremes being simplifying, and checking unit/reduced structures of multiple alternative solutions, such as checking alternatives 'at intervals' and checking 'trivial/unit' changes in the direction of other solutions, is similarly a useful resolution of this tradeoff between investing in checking one obvious optimal vs. multiple alternate solutions to account for new variables and incorrect assumptions)
                - given that 'extremes are simplifying' (similar to how similarities/limits are simplifying), navigating the space between extremes and other simplifying structures is useful to identify new variables/problems/complexity sources
            - relatedly, structures such as 'extremes' and 'requirements' and 'powerful variables' are useful as 'info triggers', as they cause changes, which creates info
            - these tradeoff resolutions often involve using one structure to negate the other to some degree without completely negating it, as a balance between choosing either absolutely (where choosing one absolutely would imply theyre absolutely mutually exclusive and cant be integrated in the same structure)
        - relatedly, identifying 'functions that exist within a system' is useful to identify cases where a 'true function' (an insight that is stable, useful, absolute, etc) is identified, at which point, a system-function map would be useful to identify 'other possible systems that apply, given that the other system also allows that function', as a way of identifying alternate systems such as alternate realities (which comply with absolute truths)
            - similarly, scaling various structures is useful to identify structures that exist in systems created by known errors (like creating a system by scaling an attribute until it has a known over-prioritization error), at which point other structures in that system could also be incorrect, as a way of generating/identifying errors
        - identifying useful structures in the 'interim space' (between extremes of simplicity and randomness, where both complexity and maximum relevant variation occur) is useful, such as applying reality-covering variables like positive/negative to decompose the interim space, like how 'negative complexity (like an error of over-complexity)' describes part of that interim space, which is useful to map/describe bc it also contains the options that enable the highest variation (which are likelier to be a set of multiple alternatives rather than one solution), given that 'balance between extremes of spectrums (like complexity/abstraction spectrums)' is a structure reflecting reality
            - relatedly, identifying whether variables are spectrums (like simplicity/complexity), vertexes (useful differences to connect), networks (like the network of abstract concepts) is useful, as checking for these structures in combinations of concepts is trivial, and useful for workflows that require a high ratio of realistic structures (where other workflows might require opposing structures of reality, such as to check for new errors)

    - identifying useful structures like how vertexes such as the way a statement could be true/false and cycles have an overlap bc they can both contain an opposing structure such as how a cycle can seem like a constant point if you measure it at its completion interval and a vertex structure contains opposites such as the opposite of a statement/variable like abstraction
        - relatedly, structure and content are a vertex structure where the content can oppose the structure, such as how a 'structural format like a style/pattern' can be used that is contradicted by the content, so structure and content are important to include together rather than separating them
            - relatedly, finding the 'structure of content' is a useful structure to identify, such as how 'high differentiation within a sentence (such as containing a contradiction/vertex)' is a common structure, similar to how 'sentences are different from standard sentences', although its often still useful to maintain lists/maps of specific content such as connections between abstract concepts, but identifying all the interface structure connections in existing sentences is a way to derive rules such as 'truth rules' (such as how 'true statements' will not be too different from physics rules, will be slightly different from previously known truths, will contain multiple ways it can be true, more than it contains ways it can be false, etc)
        - these interactions between abstract structures are useful to identify and especially useful to find variation they can contain, such as identifying the most different structures that can have an overlap and identifying causal networks composed of other abstract structures that create those differences
        - relatedly, identifying all the ways that one vertex point can be converted into the other is important as they are interactive and connected by default, so identifying these connections is useful, as these connections are the 'navigation/interaction rules' of the interfaces formed by these vertexes
        - relatedly, identifying 'connections between vertexes' like the connection between 'abstract/specific and uncertain/certain and generate/limit and global/local' is useful as another vertex structure (applying those vertexes as vectors of another vertex, to identify the common interface where those two vertexes are standardizable and convertible to each other)
        - relatedly, identifying the 'inaccuracy midpoint/threshold' between similar but different average/regression lines is useful to identify, which is the point where, after changing each average function, the returns to further changes would seem to diminish, indicating the most accurate function was already found, until further changes were applied (crossing the inaccuracy midpoint/threshold), after which the further changes would increase accuracy until the other average function was reached, which is a useful concept to apply in gradient descent and other problems involving predicting determining points of a function from a subset, as identifying functions to generate 'similar but different' average functions (by some similarity metric) is useful to identify areas/patterns/structures of equivalence/similarity (which are useful for identifying areas/patterns/structures of difference) and identifying connections between these 'similarly good average functions' and 'interim functions that maximize some inaccuracy, an inaccuracy made possible by the degree/type/structure of similarity between the similarly good average functions' are useful as an alternative to identifying 'outer limits of in/accuracy' such as those found by adding more specificity/generality
            - this means 'identifying possible structures like symmetries of in/accuracy (like symmetries in error ranges, where errors reach a peak and start decreasing, which is the opposite of a "solution range symmetry", where errors reach a minimum and start rising)' and applying those to determine other useful structures like 'maximums of accuracy'
            - relatedly, identifying the functions that often preserve accuracy or often violate accuracy of the original function is useful (functions which preserve info about useful differences related to workflows like "change a base solution", like "inverses/rotations" as "maximal differences through sign differences while preserving function shape", logs as "bases", roots as "similarity through intersectivity to a standard constant like zero", waves as "similarity to an iterated pattern", tangents as "unit change rates", the adjacent even/odd-powered variant of a function as "adjacent change types", etc)
            - similarly, identifying changes which identify alternate formats of a function are useful to identify, such as how 'extremifying' a function's differentiating variables makes it obvious that a 'vector format' is an alternate useful format of a function
                - similarly, 'extremifying' a function in this way (such as by increasing the magnitude of peaks of a function so much that the peaked areas become so different as to be irrelevant to a point that is not part of the peak, which makes the average/inflection points that are not part of the peaks obvious, and the changes required to make a peak irrelevant such as by decreasing areas of peaks by scaling them down are useful to identify in case such a change can happen in a variable system, so as to make the peak easy to miss)

    - identifying useful structures like 'variables that involve ambiguities which can benefit from differentiation' by identifying the 'variables of their connections' is useful (structures generally having a similarity in that theyre adjacently changeable into a useful/useless structure, so differentiating the paths to either attribute value is more useful than identifying one/over-simplified attribute value, characterizing a structure as either positive/negative)
        - for example, the set of 'connections between useful/useless structures' is useful for other intents, such as 'connecting/mapping these connections with other relevant/useful connection structures, such as the set of ways that an error can be changed into a solution'
            - a specific example in medicine is the 'set of ways a useful structure like a medicine can be converted into a useless structure such as a condition-causing structure', where knowing 'all the ways a structure can become positive/negative or useful/useless' (such as being 'useful at extremely high scale and useless in trivial amounts') is useful to identify, since most structures can have either effect or neutral effects, as most structures are complex enough in their interactivity/potential to be adjacently usable for maximally different intents (such as positive/negative intents), as many structures in medicine are useful/useless depending on a simple variable like the 'amount', which is adjacently changeable and acts as an adjacent changer of usefulness/uselessness
        - similarly, resolving ambiguities between related but unequivalent conceptual variable sets is useful (like relating useful/uselessness to positive/negative structures, such as how 'useful for negative structures' is a 'negative' attribute)
        - this applies an insight of 'every error can be a solution if its changed enough', with the related insight that 'some changes are more trivial/probable/realistic/powerful/common than others' and the related usefulness of 'identifying the changes that are most often useful in changing an error into a solution' and the insight that 'every variable connection could appear to be different functions at various points/states/changes' and therefore the obvious resulting usefulness of identifying the useful changes ('zooming in an extreme degree to a data set' is often a change that produces 'inaccuracy', where other changes such as 'regular input interval sampling above a ratio of representativeness' likely would not)
            - relatedly, identifying insights (true new important connections such as 'there are changes that are more frequently useful than others in preserving a similarity in input/output info') is a useful input for trivially identifying useful structures made more identifiable/important/useful by identifying that insight ('it would be useful to identify "adjacent structures of these frequently useful changes", such as "variables of changes that preserve info, as opposed to destroying info"')
            - similarly, identifying differences between 'standardizing functions' (like functions that standardize coefficients/powers or resulting slope to values adjacent to one so as to make relevant differences more clear, as an input to useful structures like 'subtle differences between similar structures') vs. 'equalizing functions' (such as 'functions that make slope appear the same value to be basically equivalent to a constant value like one, regardless of input function' as an input to randomness) is useful since these are core useful intents and variables of these differences in functions are identifiable
            - relatedly, mapping concepts like 'simplicity' to specific math functions is useful as a way of applying a useful standardizing format to solve problems conceptually, while skipping other interfaces, or apply other interfaces as later filters
                - applying a 'simplifying set of functions' and applying a 'complex set of functions' and checking if either set made progress to a solution that is scalable (the progress is not false progress but rather indicative of a true connection) is a useful way to check what the problem is conceptually (if its a problem related to over-simplicity or over-complexity), at which point other variables like abstract/specific can be applied with the mapped math function sets in a similar way, and similarly, this initial filter can be used to select more relevant workflows to handle the problem attribute identified (the simplicity of the problem identified in this way)
        - relatedly, identifying structures that differentiate structures such as variables are a default structure relevant to the problem of resolving ambiguities, as 'differences' are obviously useful for 'creating differences (differentiation)' which is useful for 'resolving ambiguities' (any structure like a 'variable' that can create a difference could be useful for resolving a specific ambiguity)
            - relatedly, 'ambiguities' are problems of 'lack of specificity/clarity/difference' so resolving them can be a matter of opposing those attributes
        - relatedly, identifying how 'similarities can become differences' indicates useful input structures to apply as 'possible ways a solution can become an error' and a 'positive structure can become a negative structure' and vice versa
        - relatedly, identifying 'similar but different' structures as possible relevance structures given the usefulness of workflows that can use them like 'change a base solution' are useful to identify, such as how derivatives are 'similar but different' in that they retain some relevant info while being different in that they are in a different format through the differences in the structure of the derivation function (and once in that format can apply to other functions through a new similarity in the coefficient), and identifying other 'similar but different' structures is likely to identify other useful structures
            - relatedly, theyre 'similar but different' in other ways, such as that they have an eventual similarity in power that creates a coefficient of 1 (x ^ 0), where the coefficient other than 1 at that point still retains info about the 'original function' and still differentiates the function from other functions which would have different results at the point it gets to x^0, which is a similarity between all the functions that could produce that if a derivative is applied (varying on the number of usages of that function)
            - relatedly, a specific 'simplifiying change' such as a 'derivative' can be applied as a default strategy to address the conceptual change of the 'opposite of complexity' ('simplicity') being probably useful, in problems where 'over-complexity' is the problem, which is frequently the case (and the opposite for the opposite case)
        - relatedly, identifying how a 'structure that can seem falsely like a useful structure but is not equal to a useful structure' is a useful specific ambiguity to resolve, such as how 'selecting a path' can falsely seem like inventing but a selection may be required (such as a driving force requiring motion, like a predator), simple/adjacent (such as where there are only two choices), and irrelevant/random (such as where the selection is not useful or otherwise directed by some useful force like intelligence as in an ability to identify a useful path but rather is meaningless)
            - relatedly, definitions of concepts like 'randomness' can be identified by structures like where a 'sequence of selections doesnt resolve some ambiguity but rather maintains an equivalence in some random distribution'
        - identifying useful differences in graphs is useful, such as how a 'graph that is different from a network in that it depicts the rate of info connections/changes (in addition to direction and connectivity of info connections/changes)' such as a 'set of barriers' (similar to a maze used to race slime molds) is useful to identify in that it can depict more info bc of the 'differences in number of info components/instances that can occupy a connection', which can be used to identify error structures like 'info bottlenecks', and adding 'state sequences' to this graph adds more useful info in that it depicts differences in sequences of states, which identify other info like 'causes/thresholds/cascades of error states'
            - relatedly, identifying alternate definitions of randomness is useful, such as how it can result in various states such as where 'default/adjacent changes' seem random in the sense that they could take any form, depending on whatever was default/adjacent at the time, and similarly, the 'randomness' of 'lack of meaning/organization (as in "what doesnt matter enough to be organized/examined/identified, or what is unknown so it cant be used and therefore doesnt matter in that way")' is another definition, or the opposite of 'meaning' (which has definitions like 'relevant/useful structures like differences'), although there is a way that randomness 'means' something which is when known variables interact with it, in which case it can mean 'removing the info/certainties/constant values/probabilities of those variables', as its easier to know 'randomness' by what it is not

    - identifying useful structures like 'opposing vector sets' that offer a useful structural target for algorithms to aim for when identifying robust solutions that account for identifiable errors
        - for example, building AI that has requirements such as a 'world-model sub-network' (to allow a network to check that its selected solutions wont favor criminals in their criminal intents or that it has identified a solution to possible misuses) and a 'concept sub-network' (to prevent concepts from being defined differently to favor a result, like defining good as evil, to make sure the network can derive/identify when definitions have been distorted for an evil intent by allowing it to access concepts that enable it to derive real connections/balances between concepts, thereby deriving definitions like good/evil by defining concepts like power/balance/equality), as well as a 'sub-network specifically to identify info indicating misuse and identify solutions to possible mis-uses of the solution-finding sub-network for evil intents' (solutions to misuse such as 'only allow n number of people to use this more than the people around them' or info indicating misuse such as 'when these transaction types start occuring between these groups after using the network, the network is likely being misused')
        - similarly, including info about usage of a set of variables and semantic info about variables such as 'what numbers represent' makes it more possible to prevent evil intents, and similarly, including a 'few maximally different alternate solution functions having different reasons, connected to an adjacent sub-network of variables that would cause that reason to be relevant', so that when new info or additional info is found, the differences in these 'reason-based sub-networks' makes it obvious which function is more correct
        - the minimum info to detect evil intents is usually not clearly present in the numerical variables created by current tools, although some algorithms such as the algorithm that 'identifies the simplest or most adjacent solution' are clearly evil in the missing info they leave out or fail to identify, having an obvious error of over-simplification likely to perpetuate biases, so testing out a selected solution in a world-model is a useful countermeasure that could be required of algorithms, where the solutions resulting from this hybrid network would benefit society more and therefore produce solutions that are preferable to criminals as well, as an algorithm capable of complexity will also be able to identify solutions that will improve criminals' quality of life in harmless ways, like identifying 'less harmful scams and the right scam targets' as well as identifying 'other paths out of crime', which will make their lives less stressful and have a similar effect on their victims
        - this has a structure of 'limiting change/chaos created by a solution function' by creating change vectors in a direction away from evil intents like over-simplification, but also creating vectors in the opposite direction, to limit the changes created by the solution function when they become error structures (like when scaled to an extreme/over-prioritized degree), similar to how 'filter' limits the changes created by the 'generate' function when applied with it, a structure of a 'generally useful change vector' and a 'specific opposing error-correction vector set' that should be fulfilled by most useful algorithms as it accounts for 'identifiable errors and solutions to them (opposing vectors to offset them)', as 'change in only one direction' is rarely the right solution function and should more often be accompanied by 'extreme change offsetting/reversing functions'
            - relatedly, the difference from a similar structure (such as an ambiguously similar structure that is still useful to differentiate, as the alternates are unlikely to merge despite having some ratio of simple connections) and the difference from an extreme/scaled structure (such as 'obvious outputs of a process, when scaled') are useful to include as a vertex in identifying the useful relative variable values of a structure being compared (local vs. global, and ambiguously similar vs. extremely different)
            - relatedly, identifying a 'workflow/query' and 'examples of the errors it could have (such as how change a base solution wont immediately find non-adjacent values, adjacent given change functions applied)' is another example of useful 'opposing vectors' involving problem/solution structures
        - this structure of 'opposing vector sets' is useful for many other different intents, like how the 'ultimate standard of info' (as in 'how do you really know something') requires that no incentives/biases exist to fake the info and if there are incentives/biases, they have been removed/isolated/otherwise neutralized from impacting the test of info or that the only incentives/biases allowed to impact the info are only biases in favor of info/other similar priorities like fairness
            - this is similar to the 'standard of meaning' which is where some function/structure occurs despite all attempts to prevent it (as in 'its more meaningful to be kind despite no rewards for it'), or despite having no possible benefit, which makes it more meaningful if it still occurs in those conditions, as it indicates power and therefore meaning since power influences other variables
            - relatedly, other structures are useful to identify, such as how 'understatements' are errors bc they 'leave out info (create missing info)' similar to 'riddles', which involving resolving the 'missing info' error in often very different and therefore useful ways
            - similarly, identifying other useful connections like the connection between 'justice' and 'reality' through the concept of 'balance' is useful (since 'just agents' are more real, as they interact with and thrive in reality, rather than ignoring truths, they apply a more complex structure than just an 'info barrier', where both opposing vectors can exist)
        - relatedly, structures like simple structures can trivially produce the opposite of a true statement, as they can be so different from reality in their over-simplifications that they can be trivially changed to produce other false statements
        - relatedly, its useful to identify reasons why useful structures can be derived from each other, bc there are workflows that can be applied to adjacently identify other useful structures once a useful structure is known (like how 'change a base solution' can identify trivial variants of a structure more adjacently than other workflows might, such as 'specialized variants'), and similarly, intents like 'identify variables of a structure (to identify variants of it)' are possible to fulfill once a useful structure is known, so that other useful structures can be derived from it based on 'variables of usefulness'
        - relatedly, its useful to identify 'areas of ambiguity' in a function space, so that where there are 'areas of missing information or ambiguity' such as 'what other filters exist that can be applied across problems', info can be added to solve the 'lack of info' problem, such as by applying a specific problem to identify new filters, at which point the filters found can be abstracted to make them applicable to problems in general again
            - this is another position where an 'opposing vector set' (specify, then abstract) can be applied, to create a useful change ('add info') that enables a useful target structure ('general info'), as spectrum interface variables like abstract/specific contain maximal differences similar to cross-interface structures so variations within these interface variables can be applied as 'opposing vectors'

    - identifying differences that add value once an insight about errors (suboptimal differences) is identified, to focus an interface query to apply those differences where they are useful (applying differences to the errors, as in 'create a solution')
        - for example, its possible to 'connect any two points in a data set' to give a totally opposite conclusion of the solution regression function compared to the actual regression function, so methods of applying this insight would involve identifying counteractive measures to contradict the use of these 'opposing point lines' (opposing the real solution function), similar to how its possible to make any true statement seem false by changing it in various ways (such as removing the general info or removing most info and only focusing on one point or a similarly low-info subset or zooming in) and vice versa, counteractive measures such as 'including more points in the line', 'including points that follow different patterns in the line (points on a straight line, points on curved lines, etc)', 'including maximally different points in the line (points in different directions, at different distances away from the previous or "previous average" point)'
        - relatedly, combinations of interface structures like 'previous average' (as a program iterates through a data set in a consecutive sequence of inputs) are useful to identify in case they are relevant to some workflow, as it is usually possible to build a workflow using these combined or cross-interface structures given the variation they contain and the general reality of these structures which makes them relevant and also given the interactivity of these structures with other structures in the problem space
        - relatedly, identifying the limit of a workflow such as 'trial and error' is useful, such as how a chip designed to apply 'trial and error' will create problems at scale that cant be solved by 'trial and error' (identifying the activities of a trillion chips all using 'trial and error' on some problem cant be solved by 'trial and error', bc the chip army will beat the chip trying to predict them with 'trial and error', unless the chip trying to predict them is exponentially better than the chip army units, which is a problem that will occur at some point, as inevitably criminals will gather a lot of chips and use them to out-compute the enemy, and if chips hit a computation limit, algorithms have to improve, so 'trial and error' has an expiration date either way, whether bc its necessary to predict and out-compute an army of chips using one chip or bc a computation limit was reached)
        - relatedly, 'iterating/extrapolating n times' is useful to identify if a statement is true (by iterating the process of evalating what the statement implies, n times), as the idea of a 'count of useful iteration applications' represents a 'ratio/degree of differences' that could possibly change the statement into its opposite (whether true or false), as frequently an incorrect conclusion is incorrect bc its incompletely analyzed and its n-th implication isnt identified, which is a problem of 'scaling (to the nth implication)'
        - relatedly, identifying a 'way that an error structure can be useful, if implemented in that specific way' is useful to identify, such as how tech like neural networks should generally only be used for very specific intents like 'fixing supply chain/delivery optimization errors that cause areas with insufficient resources' to 'reduce war' and should not generally be used for intents like 'winning a war' to 'reduce war', but there is an optimal way to apply the error structure of 'applying neural networks to win a war', which is that if used correctly, the neural networks can help win a war very quickly, and thereby effectively 'reduce war', as there is always a way to use an error to solve some problem, but some errors have adjacent ways to be used to solve a problem
        - relatedly, identifying alternates like 'evaporating sea water' vs. 'moving undersea land onto land' or 'moving mountains into the ocean to create more islands' as alternative ways to 'lower the sea level' is useful for identifying 'variable sets' which can be altered without harming other useful structures, which act like alternate inputs to the sea level
            - similarly, 'diffusing heat' is possible by 'altering light with lasers to produce different types of light than sunlight' or 'planting trees or creating artificial covers like clouds to cover more land to reduce heat on land' or 'creating highly light-absorbing structures' without harming other useful structures, all of which implement some form of a common function such as 'distribute' (applied to the problem inputs) to have an impact of 'reducing' the problem, where 'distribute' (applied to problem inputs) is a useful variant of 'isolate/separate/filter' and 'reduce'
                - this applies a function of reversing/opposing a negative structure (using a structure like 'light (from lasers)' against other types of 'light (like sunlight)'), which can be done in many ways like creating reflective surfaces on the earth's surface, or creating 'light absorbing structures (like darker materials)' to cool the surface
                    - this function isnt always applied correctly, such as where 'two wrongs dont make a right' bc the next wrong exacerbates the previous wrong as the wrong is not applied to other wrongs but rather is applied in the same direction, so it doesnt create a useful opposite such as where other useful opposites occur, when a negative cancels out a negative
                - given that obscure materials would prevent light from passing through, and there are layers in between the sun and the surface such as cloud layers, these clouds can be used to prevent light from passing to the surface (which can be done in many ways, such as finding less harmful chemicals than pollutants which are nevertheless obscuring and also light/interactive enough to form clouds by default, and only done in some places like less inhabited places or over the ocean if there are positions where its more trivial to maintain a cloud position)
                - relatedly, a more complete workflow would have an opposing function to 'distribute', such as a function to re-combine the outputs of the 'distribute' function in a more useful way, which has at least some opposing function to offset errors of extremity that are likely in an overly simple workflow
            - similarly, identifying useful structures like 'plants' which take up a resource to reduce 'seawater' as input is useful to identify, which re-directs the problematic structure to a useful structure that can use it or reduce it or 'reduce it by using it'
            - similarly, identifying other intents fulfilled by functions like 'terraforming' such as 'creating a planet with more equally distributed resources (such as a planet with a water source every n miles)' is useful to identify future benefits of developing a function
            - human brains dont have the capacity to compute variables like 'ultimate impact on climate/biological life/ecosystem/magnetic field/ozone/weather/chemical ratios of a climate change solution' so 'training a bunch of different AI models to try to get them to agree on some solution' is not just a good alternative that is available for methods with unknown scaled impacts like 'spraying chemicals on clouds' but is required
                - a 'doomsaying/error-avoidance AI' to identify 'worst-case scenario side effects' like 'biological life forms developing poisonous toxins to handle the climate change' or 'unexpected extreme impact on evaporability/other important properties of seawater types under clouds with chemicals sprayed on them' need to be very low-risk for a given solution identified by an 'ensemble of maximally different AI models with high computational resources' for it to be even worth considering, and to predict these worst case scenario side effects, an extremely good simulation of reality (bio systems, ecosystems, climate/weather systems, chemical interactions, light interactions and interactions of all combinations of these) is required (a 'doomsaying AI' with access to high computational resources, that can model reality accurately, and quickly identify the worst things that can happen if a solution is selected/applied at scale, at which point if the doomsaying AI thinks a solution might not trigger the worst or even negative side effects, and all the other doomsaying AI models agree, then it might be ok to try in small amounts)
                    - relatedly, a 'best case scenario AI' is useful to build for identifying benefits that can happen, but these generate so much 'blinding hope' that they are not as useful as worst-case scenario AI's
                - similarly, the opposite of an 'error-avoidance AI' as in a 'solution-progress AI' to evaluate all the applications of solutions and whether theyre making progress (in the real world or in simulations)
                - a 'risk-evaluation AI' would be useful to indicate whether the "doomsaying AI's" worst-case scenarios are probable if a solution is applied 'at scale' or 'simultaneously with other solutions/problems'
                - a 'cost-benefit AI' to evaluate if the risk is worth it and identify costs of a solution, given probabilities of positive outcomes and other non-worst case scenario outcomes
                - an 'ensemble AI' to evaluate all the solutions selected by the solution-filtering/generating/changing AIs and identify solutions that have consensus
                - an 'info-finding AI' to identify missing information to solve the other problems (which AI models need more info to make progress and how to find that info)
                    - similarly, an 'info-barrier solving AI' to identify barriers to information, such as difficulties/complexities/required iterations which hide info and make it non-trivial to identify, or identify methods to workaround info barriers (such as finding 'equivalent alternates' or 'approximations' of the info being searched for) to help the 'info-finding AI'
                - an 'integration/interaction AI' to evaluate possible interactions and all the other methods being tested around the world ('what other experiments are being applied, simultaneously or around the time that this method is being applied') and the impact of variables like 'current pollution levels' on a solution method (can this solution succeed, even if clear skies or ozone holes or pollution or a solar flare is happening in that area at the time its applied), where this integration AI would be useful for most other AI's to use by default, as it would check if their output was coordinating/contradicting with other solutions being applied
                - an 'organization AI' to evaluate the 'best possible amounts/positions/interactions/connections' between alternate solutions being applied and other variables like pollution, which would have the most useful perspective on how resources should be organized, which would help and be helped by the 'state sequence-identifying AI' to identify paths to achieve these optimal resource organizations once identified
                - a 'solution state-identifying AI' to identify optimal solution states (what are the important solution metrics and what states would fulfill those - as in 'everyone having some ratio/amount of resources' and other useful specifications of solution metrics)
                - a 'state sequence-identifying AI' to identify 'possible useful state/step sequences that would lead to optimal solution states', such as 'how to change the supply-chain, pollution, economies, regulations, technologies' so that optimal states are more adjacent/possible
                - an 'implementation AI' to help workaround political/social/technological/scaling/distribution/manufacturing/supply-chain/other barriers to applying the suggestions of the AI
                - a 'balancing AI' to help avoid extremes, which are likelier to be error than optimal states, given the usefulness of balance
                - a 'distributing AI' to simulate testing a solution in various distributions and various systems
                - an 'optimization AI' to evaluate the other AI models and suggest changes to improve them
                - a 'scaling AI' to scale solutions and identify extremes and limits
                - an 'inventing AI' to identify useful possible implementations of intents to help the other AI's such as the implementation AI, but specifically to solve the problem of 'inventing new technologies' which is particularly useful in the energy/climate/medicine problem spaces
                - a 'connecting/combining and separating/deconstructing AI' to connect possible variables and connection structures like 'combinations' that would be useful to connect, such as variables that could cause extreme negative side effects (like materials that when adjacent form explosions), which are useful to identify so they can be separated or deconstructed, which helps the organization AI
                - a 'interface variable AI' to identify the most determining/powerful/causative/common/standardizing/simplifying/interactive/supporting/interface variables which explain the most variation, and identify all those variable interactions, as theyre the most important to identify, as it reduces the 'not understood variation' in a system, and help simplify models of even complex systems
                - a 'simulation AI' to help generate and improve simulations of reality, with help from the 'interface variable AI' to help simplify/standardize/improve its simulation models
                - an 'alternative-finding AI' to help find alternatives to a solution, such as 'temporary solutions that would be useful to apply now while waiting on simulations/implementations of more complex/risky solutions'
                - this is basically an optimized version of 'checks and balances' in govt - identifying how all these functions should be positioned and interact is useful as a way to identify an optimal 'problem-solving function network'
                - worst-case scenarios arent likely to be identified by most experiments/AI models bc these scenarios might only happen at extreme scales, except those experiments/models specifically designed to cause/model significant harm (you cant cause doomsday scenarios in your experiments just to find out if they would happen, youd have to cause units of doomsday scenarios in an experiment and then stop and identify that at scale, this would continue, or just simulate the causation in a computer)
                - as a temporary interim solution while waiting on AI models to agree on a risky method like spraying chemicals, 'reflective foil tarps (that dont bleed their surface chemicals very much or at all) covering a small ratio of the ocean or land to test out its side effects' might be a better alternative that might cause less pollution/side effects while we wait for AI models to simulate things like chemical sprays, the best long-term solution is probably something like a 'tool to evenly distribute ozone, or push ozone up from the ground layer, so there are no holes in that layer'
                - if 'hoping it will work' is heavily involved in a solution application, it is extremely likely to fail or cause negative side effects that hope blinded agents to, hope often blinds people to negative things like negative side effects that would have been useful to identify, hope is a bad investment, 'reliability/consistency of results across many different AI models' is a better alternative to 'blind dumb hope'

    - identify useful similarities in different problems like a 'network' format where other strategies in problems with that similarity can be applied trivially to solve similar intents across problems (like 'find a path that maintains some direction or reduces distance to a destination node' which is a generally applicable intent across problems formattable using a 'network')
        - for example, 'simple rules to solve a maze' exist (such as 'aim for this many left/right turn pairs or take a maximum of n left turns followed by n right turns, followed by other patterns of turns, to avoid turn error structures that result in circles or back/side directions') which can be applied to other problems with some similarity in common such as 'finding optimal paths' which can benefit from 'functions to maintain a direction', similarly solving other problems like 'decrease the distance between all nodes in a network' or 'organize a network so all its nodes are trivially connectible' can be applied to other problems having a network format such as 'finding optimal paths' or 'connect all variables (in a neural network)'
        - relatedly, the reason that many different terms align with 'interfaces' is that many different useful structures encode a 'core similarity (around which trivial differences are applied)' such as 'perspectives' (similarity in priority), 'indexes/maps' (similarity in input/output, similarity in usefulness of the same structures which are useful to store a list of, similarity in approximation of the index's original list), 'formats' (similarity in structure), etc
            - other useful structures involve the opposite structure like a 'set of differences that creates a similarity' such as 'equivalent alternates' or 'complementary info sets that construct the same resulting set'
            - for example, identifying that an index can solve a problem of 'unnecessary or uncoordinated variation, where there should be coordinated variation (like how component updates should reflect the values in a "coordinating versions" index', rather than being applied without coordination, as the components coordinate so the versions need to also coordinate, as the versions reflect differences in component functionality, so the versions are also an index of the component functionality) is useful, as it reduces the problem to 'finding the correct coordinating version index and applying it as a standardizing structure creating a similarity in component versions, allowed changes/updates, and the version index'
                - relatedly, indexes can solve other problems if the index is identified first and is relatively trivial to compute, such as where its useful to identify whether a subset of a function indicates its one of various maximally different function examples/types, using a 'subset/maximal different function' index, and similarly an index of 'interface structures' can be used to solve many problems trivially
        - knowing this is useful bc it means that a useful problem-solving workflow will involve different pairs of 'similarities in differences' and 'differences in similarities' and related useful opposing structures, similar to how 'generate and filter' and 'standardize and differentiate' are similar opposing structures that are useful in workflows (the 'vertex' structure uniting different perspectives that keeps re-occurring in many useful workflows)
            - relatedly, the 'vertex' of architecture and algorithms is useful to identify (such as where 'trial and error' is only useful if there are multiple processors available to compute them simultaneously, creating a useful similarity between 'processes that can occur simultaneously' and the 'number of possibilities tested', which is still suboptimal in that it doesnt identify understanding of how to filter solutions, so a 'maximal filter processor (which identifies important variables and applies those first to predict functions, then others of less importance)' would be more useful as an architectural resource, similar to how some algorithms have specific memory limits/requirements)
            - relatedly, these structures are useful when mapped to similarities/differences, such as how 'generate and filter' creates differences (generating 'multiple specific examples, which are different enough to be filterable') which are made more useful by further differentiating them (applying filters to find different solutions like unique solutions), and how 'standardize and differentiate' involves identifying a core similarity which is modified with trivial differences, and how 'specify and abstract' involves identifying 'specific examples which are then more possible to similarize by identifying abstract types which apply to multiple examples' (similar to generate and filter)

    - applying useful or 'problem-solving' intents (like 'find variables' or 'find maximal differences') on each interface is useful to pre-compute useful structures like 'maximal differences' that can be used in interface queries
        - for example, 'certain uncertainties' and 'specific abstractions' are 'maximal differences' within a primary interface
        - relatedly, identifying cross-interface function structures is useful, like how 'logic' and 'information' create 'pressure/power/control' or 'heat/energy/change'
            - bc of how numbers are defined/identified, specific structures such as specific 'patterns of interactions' are possible, so identifying this cross-interface structure of the connection between 'definitions' and other interface structures like 'interaction potential' and 'interaction patterns' is useful (which enables other intents like 'find the most reduced number set that reflects reality in various metrics like potential, variation, conceptual network/balance/variables/priorities, etc' and 'find generative variables/requirements/implications of this number set as a proxy for the same structures of reality')
        - similarly, applying 'extremes' like 'infinity' to find other related structures like 'limits/thresholds' and applying variables of a 'spectrum' like 'less/more/average' to spectrum variables like 'abstract/specific' is useful for identifying all the interface structures in that variable, to identify other useful structures like insights such as how other variable formats like network formats or interface formats are more useful than spectrum formats of complex variables like 'abstract/specific'
            - for example, its useful to have both specific/specialized structures and abstract structures, so that errors like 'over-focus on details' cant occur and the 'general picture' is always integrated regularly, so the 'conflict between abstract/specific' doesnt need to be resolved by choosing one or using other interfaces instead, but rather maintained as both are necessary in different cases that will continue to occur
        - relatedly (to both this workflow of resolving problems within interfaces and the previous workflow of pre-computing specific problems and the workflow of 'designing networks to offset errors like evil usages/intents'), solving the ambiguities between useful structures to differentiate like 'good/evil' is useful, such as how 'not committing a crime' can seem like either good or evil, such as how good agents will not commit a crime but evil agents will also frequently not commit a crime, such as when theyre being monitored, when they are planning other crimes, when they are trying to seem good or evade detection, when they are recovering/resting/gathering new crime resources after a crime, at which points they will falsely seem good, which are useful structures to apply in algorithms designed to avoid errors like 'adjacently fulfilling evil intents' (like by creating barrier to evil intents or creating incentives for good intents or providing identification info for evil intents)
            - the fact that this is 'related to and useful to multiple workflows but cant be equated with any of the workflows' indicates it can act like a workflow on its own, such as to 'resolve other differences by resolving differences on an important interface (which fulfills multiple metrics as the good/evil interface is by default related to meaning and is therefore automatically also relevant/important, so it acts like a useful proxy of the meaning interface)', which is a way to identify alternate workflows (find intents that are related to multiple workflows but not equivalent)
            - the idea of 'resolving evil intents by aligning intents across all agents/groups' indicates a useful structure of 'alignment', which connects to useful questions like 'what differences are useful to maintain, if its so useful to align structures using structures like intents/standards/similarities' (such as 'differences reflecting or possibly reflecting the variation in reality, so that this variation can be handled, once encountered')
            - relatedly, solving the problem of 'what is the causative problem of an important problem, such as the reason for the positive/negative impact of an important variables (like incentives)' is useful and is a proxy for solving other problems, such as how identifying that the reason incentives are negative is that they create 'disorganized cost allocation, allocating cost where default/adjacent/otherwise irrelevant but not allocating it where its best handled/allocated' which if that problem of 'organizing cost allocation' is solved, the errors resulting from incentives are reduced
            - relatedly, another solution to the problem of good vs. evil is conflating the two with complexity structures like extreme/embedding structures, which produce more complicated structures like 'evilly good' and 'purely evil' and other conflations of the concepts and related concepts which are neutral but associated with ethical values, as increasing complexity often reduces the impact of a variable
  
    - identifying useful structures like 'problems to solve, which when solved, can be applied to trivially create other solutions to other problems' which is possible bc core/complex problems can be used in combinations/changes to solve other problems
        - for example, resolving core/simple/composable ambiguities like 'differences between similar concepts' is useful to resolve all ambiguities, as the solvable unsolved ambiguities will often changes/combinations of the solved ambiguities which are combinable to create other ambiguities
        - similarly, solutions to complex problems like 'resolving differences between similar concepts' (such as 'purity' and 'simplicity', which have some overlap but are ultimately quite different) can often be varied trivially to solve other complex problems
        - the core insight here is that some differences between problems involve trivial changes (like 'similarly complex' problems being able to be solved with trivial changes bc of this similarity), whereas other differences between problems would involve 'iterated combinations' (the change required to make a core problem solution useful for complex problems), so knowing attributes of problems like composability/complexity is useful to identify how problems/solutions could interact with other problems/solutions
        - this is bc solving unit cases in a 'problem format' like 'resolving ambiguities' is useful bc other cases in that format are likely to involve combinations of those unit case solutions
        - similarly, 'identify equivalent alternates', 'identify variables that determine a set', 'identifying generally powerful/determining variables' are related intents that can usually be swapped in for each other in interface queries
            - identifying 'similarly useful intents to fulfill' is useful to identify intents that can be combined/changed to create useful structures like 'different interface queries that are similarly useful'
        - these differences in how structures can be used (combined/changed) are useful to identify bc they determine which workflows are optimal to use with them (use 'trivially changeable structures like equivalent alternate or similar concepts/intents' in a workflow involving 'change a base solution')

    - identifying useful structures that avoid multiple errors simultaneously is useful as an alternative of other useful structures (like structures with multiple useful functions) by applying that structure with a variant (applied to 'error' structures)
        - for example, a simple rule like 'no simple rules' is capable of being applied correctly with a higher probability than other rules, especially simple rules, which will miss the rare correct simple rule but that is trivial to identify, as there are fewer functions/structures to test for when identifying simple structures and that can be included in interface queries by default
        - this avoids an 'over-prioritization' error (over-simplification), both by avoiding over-simplifying and over-complicating (having a solution for the error of missing simple rules)
        - it also avoids a 'self-invalidation' error (where simplicity negates itself) by finding the one self-reference structure of simplicity that applies generally (which is a simple rule - specifically, 'no simple rules'), so that more common self-reference errors of self-invalidation (which are more common than correct/useful/relevant self-reference structures) can be avoided once the correct alternative is identified
        - by applying an insight like 'most statements are somewhat true and somewhat false', it can be derived that finding 'correct and incorrect usages' of a rule is possible, such as cases where a 'simple' rule is justified, such as 'where a system is new or newly stable and its interactions are likelier to be simple'
        - this applies a 'bias against bias' which is another correct self-reference structure, as bias means an error of some sort such as 'over-simplification' or 'assuming too much'
        - relatedly, identifying interface structures of bias is useful to identify structures related to bias errors, such as how a 'self' bias has a 'structural reason/cause' such as 'being/having a structure (being embedded in a state, using functions, getting rewards from a function) making it almost impossible to see that structure clearly (see its negatives/costs) without trying to think from the opposing/different perspectives (like the negative perspective)' and relatedly, the 'self' bias is related to other biases like 'local/adjacent' bias bc of various structural reasons/causes, such as 'sight' (which makes local/adjacent structures seem more important than they are, though they can be important such as where a unit case test is possible to extrapolate insights from and local errors can be avoided)
        - similarly, if a structure is to fulfill an intent like 'differentiate' some set of structures, it has to be different in some way, rather than situations like the similarity of the ambiguity of the overlap in 'having a false appearance of some structure, that both a true and a false statement would seem like' (which is how both true and false statements seem when the statement doesnt contain an 'aligning difference reflecting the difference between truth/falsehood', such as how a 'neutral structure can seem false or true with trivial changes' so these neutral statements are likelier to be polarized (as in 'extremified') for various intents, meaning the truth/falsehood differentiating function and the true/false statement have to contain sufficient differences rather than similarities)

    - identify useful structures like core problems which, once solved to some degree of complexity, can add value by making it possible to compose their solutions to make other more complex problems trivial to solve (meaning solvable with trivial structures like 'adjacent change combinations') and 'differences in cases that a solution should work for to be generally applicable such as by being composable to solve other problems'
        - for example, the 'repeat' function is associated with the 'scaling' problem (and relatedly the 'over-prioritization' problem and relatedly the 'threshold/signal/determinant of success/failure' is associated with the 'recursion' problem, as the 'halting/limiting' problem), the 'sequence' structure is associated with the 'sorting' problem, the 'format' structure is associated with a 'interactivity' problem (where not every structure is already formatted to that format so differently formatted structures may not be interactive/mappable to/usable in some format), the 'base (as a reducer of differences)' structure is associated with a general 'difficulty' problem (not having adjacent structures to solve a problem in some base or interaction level), the 'group' structure is associated with the 'batching/dividing' problem, the 'similar/ambiguous' structure is associated with the 'defining/filtering' problem, the 'storage' structure is associated with the 'usage' problem, the 'intent' structure is associated with the 'meaning' problem (what is the meaning of this intent, given what it reflects that the user doesnt already have and wants) which are core structural functions/structures and core problems, which if solved to some degree of complexity (in a unit case and a complex case with scaled and random and other variable structures applied), can likely offer composition of their solutions to solve other problems
        - similarly, other functions exist which have general applicability to problem-solving, such as 'risk' (as in 'select a function set with incomplete info, that has a sufficient probability of being sufficiently complete or adaptable or otherwise useful') and 'trust' (the ever-popular, 'wait for someone else to fix it')
            - these are functions that can be used in low-info cases but understanding (implemented with connections between high variation variables) is more useful, as 'risking it' works if youre lucky which is rare unless youve created it through various structures which is a problem-solving strategy (creating luck such as by distributing power/opportunities/incentives/info/other valuable resources so that 'any position or any path can become lucky (which is a sequence of low-cost rewards by creating those reward sequences in a distributed structure)', increasing organization/scaling/problem-solving functions, creating value that is useful for many positions, increasing relevance/interactivity of various useful structures like 'aligning incentives and opportunities to solve problems', increasing ratio of understanding relative to the now less fortunate, improving functionality of existing functions, etc), and should only be used as a last-resort method, especially in problems like 'positioning mirrors/lenses around the universe to direct light to implement various functions like gravity/heat/magnetic manipulations', which is already risky enough and is important to get right, so you wouldnt want to risk anything if possible
                - relatedly, identifying 'functions which perform similarly across worst/best case scenarios (and different scenarios like default scenarios)' is possibly by identifying 'functions with similar performance across variable inputs', this function set being a default function set to try to minimize the impact of luck in the optimal attribute of a particular algorithm (some algorithms only work bc luckily a solution is adjacent, so finding algorithms that perform similarly is possible by 'creating luck' through applying reality structures like 'multiple alternatives' and 'networks of variables rather than one optimal variable' and 'multiple perspectives rather than one over-prioritized attribute', as an algorithm to solve problems is supposed to be better than reality, as reality doesnt automatically solve all problems, but it needs to reflect reality to some degree while also changing it enough to improve it, meaning it should contain some reality structures, and also apply some insights such as 'dont always rely on known useful structures but instead apply randomness to identify new variables/errors' which aligns with a algorithm having some non-zero and non-absolute ratio of reality structures)
            - relatedly, risky strategies are an 'improbability of success' where the 'reason' for the improbability is that there is 'incomplete info, such as info about more optimal strategies', where the 'reason for probabilities' is useful bc probabilities are useful, in that certainties about im/possibilities/requirements are less complicated and problems are likelier to occur with uncertain possibilities rather than certain possibilities like 'guaranteed impossibilities or required possibilities')
        - the interactions of unit/complex/other highly different cases of these problems are likely to be usable to solve most other medium-complexity problems
        - 'generating/selecting the differences in cases that a solution should work for' (like best/worst case, simple/complex case, random case, scaled case, etc) is a useful problem-solving intent related to 'defining requirements' and 'defining solution metrics' that makes it more obvious to determine the solution
        - a structure formed by 'common functions usable to solve these problems', organized in sequences that adjacently solve these problems, is likely to be a generally useful 'maximally different' structure to identify, which can interact with other 'maximally different function set networks' to create a network of 'maximally different function set networks' identify gaps in problem-solving functionality
        - similarly, the set of 'surrounding interface structures of a structure like a high variation structure (like a concept)' fulfills various interface structures (the system context surrounding a structure, the adjacent structures created by applying interface structures which are relevant to the structure, the high variation implemented and achieved by applying interface structures, etc) and is also useful to find interface structures of the interactions between these 'structures with interface structures applied', such as 'overlaps' between the 'similarities of a structure with concepts' and the 'similarities of another structure with concepts' being useful to determine different variables of concepts, as another useful graph to identify with interface analysis (which builds 'understanding' by applying these surrounding/extreme/limiting structural interactions with interface structures)
            - relatedly, formatting 'existing useful structures like high variation functions (such as organize)' using interface structures is a way to determine the other useful functions that could be implemented with these structures which are similarly useful or otherwise equivalent in some way
            - this is useful for avoiding errors of 'incomplete thinking' where an assumption isnt followed through (iterated, or iterated in its probable/identifiable input/output sequences) to its ultimate conclusion regarding its interaction with other structures, such as where some iterated structure would hit a limit at some point but its assumed to be infinite, and the limit structure is not identified as relevant (in its path of iteration/interactivity), and the iterated structure is not iterated until that limit is hit, so the contradiction of the infiniteness of the iterated structure is not identified
            - similarly, its useful to identify 'directions of change' so that a structure which limits another in this way can be predicted
        - similarly, identifying useful structures like 'interaction levels' is possible by finding structures that can be created using the same function/variables (like a radius rotation anchored at an origin), so checking various functions/variable sets for potential to create many different structures is a way of identifying these variables and the resulting interaction levels
            - identifying functions/variables to generate other useful interface structures is similarly useful at finding these structures such as overlaps/interaction levels, which are useful in that they make some useful structure like a 'difference (equivalent alternates) within a similarity (the same interaction level)' or a 'difference (non-overlapping section) based on a similarity (overlapping section)' more obvious
        - similar to how interface analysis is based on the assumption that in order for structure to exist, it would have to have some supporting structure to base changes on, other structures can exist which are less like standard supporting structures than a platform/plane/pole/lattice (like limits, light, or a field that supports free molecules not associated with a planet which can form stars/planets as they are attracted to a planet/star center/base) which add variability allowing interfaces to change
            - relatedly, the symmetries present in systems reflect symmetries present in others, such as the symmetry between a shadow/structure creating the shadow/light shined on the object, which can be applied to identify other useful structures (like the limits of the info reflected in the shadow of the structure light is shined on, how the shadow can be changed to appear similar to other shadows thereby removing info, etc) bc differences tend to follow similar patterns (like adjacent differences, embedded differences, etc and other structures defined in interface analysis), and therefore it might be useful to create a similarity index based on this and other symmetries, such as a 'shadow index' of common shadows of functions (like parabolas) that can be resolved with trivial differences to de-shadow the shadow of the function and expand it into the set of possible functions associated with it

    - identifying what set of priorities (like 'niceness') can be maintained with some decisions/functions/structures is a way to identify what reflects reality (interactivity, potential, stability, variation)
        - for example, its not possible to experience real high status (in a community of nice agents) if you are also nice in some way that is real (legitimate, justified, valid), so avoiding violence is one way to maintain interactivity with that community and possibility of that position, which is a guide to reality as well - niceness is useful as a priority of structural variables like stability, and de-prioritizing niceness is a way to avoid missing out on that potential
        - the reality of the variable (real niceness) is less likely and also more difficult once violence occurs, so this variable is not as possible in reality once that occurs, which it frequently does, and arguably those locked in violent conflict and the requirements of a violent life arent part of reality, as a result of their forced work/conflict and lower potential/interactivity
        - meaning, 'real niceness' is possible in reality, so a decision that prevents future niceness (by making it more difficult or even impossible) isnt real
            - relatedly, 'real niceness' requires being the opposite of nice in some cases, such as where being 'nice to predators' is the opposite of 'nice to victims', so being 'cruel to predators' is the ultimately 'nice' decision, where being 'nice to predators' is an over-prioritization error that invalidates 'niceness'
        - similarly, 'real caring' is indicated by solving problems, which is a structure of stability, power distribution, variation-increasing, and other priorities of reality
        - similarly, 'real abstraction' is indicated by being undefinable, except in relation to other real abstractions (covering reality)
        - similarly, 'real similarity' is where one structure can be used to derive/predict the other, because they are relevantly/really similar
        - reality often involves the space between these real variables/concepts as they degrade through their less real interactions, as 'real' structures are less common than 'decaying/previously/almost real' structures (there are very few stable structures in reality), as finding a real similarity (that is resistant to change) is tough to maintain given the chaos of reality, and real abstractions are tough to maintain given their requirement for formatting all of reality, and real niceness is tough to maintain bc of the required abundance of intelligence or other resources to be capable of niceness, and real caring is tough to maintain bc of the unreality and meaningless of most structures, same for the other priorities, and similarly finding one structure that represents all real variables is tough to find as a result of these improbabilities and difficulties
        - these real structures avoid an error of over-prioritization bc they enable other real structures (such as enabling more variation), so theyre not over-prioritized in the sense of causing failure/instability, but rather are balanced in a way that allows other structures to be real (allow more variation to occur)
        - a function set that doesnt involve real structures (like the spectrum of 'niceness' which includes cruelty and therefore allows for the concept of 'justice' as the spectrum reflects a similar degree of variation as reality as it allows opposites) is therefore less real and less likely to be useful than a function set which includes a higher ratio of real structures, as there is a reason why real structures exist (or are known) and that is that they are useful for some intent such as 'justice', as there is a legitimate place for cruelty so any algorithm that over-simplifies, over-prioritizes, or otherwise results in over-niceness or absolute niceness is similarly unlikely to be realistic/useful in/relevant to reality

    - identifying useful queries that find useful structures like differences/similarities is useful (as these queries identify a structure reflecting/associated with the cause of those structures)
        - identifying differences with questions like 'what is not an input/output sequence or simple adjacent change combination' (everything can be formatted as a 'sequence of inputs/outputs' but what is least like an input/output sequence is structures/functions like changing position to make input/output sequences adjacent or unnecessary, connecting inputs/outputs with different inputs/outputs and then connecting the different inputs/outputs, change the base so that input/output sequences are unnecessary/adjacent, or combining abstractions or other causative variables which are powerful change-causing variables meaning their membership in a set is more important to know than their sequence, equivalent alternates which identify all the variable input/output sequences, variables like probability/interactivity/adjacency which determine input/output sequences)
        - identifying similarities with questions like "what systems overlap on the 'combine' function" (as in what can be combined, bc it exists in a system that has the simplest function which is combine, making it sufficiently similar to reflect other systems - such as languages, interfaces, systems, states, etc)
        - relatedly, identifying 'differences in similarities' applied to cross-interface connections (like connections between improbability/difficulty) is useful to identify when a similarity doesnt hold, which is significant as a predictor of other variables given its causative power of variation
            - for example, identifying how two related/similar concepts (like improbability and difficulty/complexity) are different is useful, such as how a real structure may be difficult but not improbable, if it is the 'first path found' to achieve a common intent, so its used more often despite its difficulty
        - the reason that concepts enable workflows is that the concept allows a difference (covers enough variation to explain reality) and the workflow solves a problem (connects a difference within that concept, or optionally wihtin sets of concepts)

    - identify useful structures like how useful structures like 'abstractions' can be applied as connected to other useful structures like 'bases' that are used in workflows like 'change a base solution'
        - for example, structures are more useful when obvious, when embedded in or connected to existing structures (like a new 'difference' possible in a known similarity), and when abstracted to some degree that removes some details but preserves other info like important similarities/differences
        - this is bc of their connection to core workflows, such as how a relevant abstraction can be applied as a useful base to be applied as an input to the 'change a base solution' workflow
        - at that point, the problem becomes 'find the relevant attributes to abstract which are uncertain/variable/unknown/irrelevant, and the relevant attributes to preserve as a core standard of useful certainties' to find the relevant abstractions, which are often new structures that capture different info than existing structures

    - identify useful connections between interface structures that are useful to apply in combination (like network/concept, component/system which is across the 'scope' interface, function/usage and difference/standard), where these connections can be used to identify other useful structures like useful filters
        - for example, identifying whether a 'criminal society' or a 'non-criminal society' is likely to apply concepts like 'freedom' is possible by applying a different scaled unit (non/criminal), where 'freedom' is only possible in a non-criminal society, since a criminal society requires work like 'identifying/separating criminals', where this required work contradicts the concept of 'freedom'
            - this is possible bc concepts like freedom are complex enough to be best defined/represented by network interactions (such as societal interactions in the social network), and agent interactions are a complex/high-variation variable that contains a lot of information, making other high variation variables like abstract concepts more trivial to identify/derive
        - similarly, other scaled units are often trivial to filter out, such as functions created by 'simple repeating patterns of scaled units (like waves)' which are often uncommon in real systems, as its trivial to identify if a solution function is in this set of 'scaled unit/repeating pattern' functions, as a trivial variant of simpler functions
        - this sort of 'connection between components (like units) & systems' is possible bc of the 'existing connection between components & the systems they occur in', and is useful bc components/systems often reflect info about each other as theyre often specialized to be optimal in some connection/interaction, so the component reflects the system and vice versa
        - connections between these combination structures can form the basis of core workflows ('define a concept in a network, to account for its network of possible alternate definition routes and its complexity' can be used to fulfill problem-solving intents like 'identify adjacent concepts to make some problem more trivial to solve') bc these structures are different enough to contain enough variation to solve most/all problems when connected
        - similarly, there are variable sets which dont contain enough info or dont handle enough errors/edge cases to be an optimal solution, so a 'network of functions' is more useful to switch between functions when one is more optimal in some case (such as a switching function between cooperative/competition when its impossible to find a reward in one system, such as how a 'win' is not always possible in the 'competition' system, as its an incomplete system without enough symmetries reflecting reality to be useful to solve most/all problems)

    - identifying useful structures like reasons for usefulness of a structure in fulfilling some useful intent like 'finding errors' (such as by connecting structures like 'scale' with possible errors like 'over-prioritization')
        - for example, applying useful structures like 'differences in a similarity' across interfaces is useful, such as how finding the 'contradictions in a system' (the 'differences in a similarity (the same system/context)') is always possible (such as how 'capitalism' involves 'socialism, for some but not all' in a position), and is useful for identifying possible initial errors that could invalidate the system if allowed to repeat
            - relatedly, finding structures that invalidate a system when applied as 'scaled units' (scaled initial errors) is useful to identify important error structures to prioritize handling with solution functions
            - 'contradictions in a system' are also useful in that theyre a 'unit complexity/embedding structure' that is useful to identify
            - also structures which can be defined with many different definitions are often more useful in that they will fulfill more intents and interact with more structures
        - scaling/iteration is one of many functions that is useful for finding errors bc it often creates 'interactions', 'known errors like imbalances like over-prioritizations', and 'more obvious structures' through the iterations applied, which is why its useful to iterate some structures
            - applying 'similarities/differences' and 'simplifications/complexities' and other reality-covering spectrum/conceptual variables are similarly useful for finding errors

    - identifying useful combinations of interface structures which make it more possible/trivial to identify useful structures like 'probable structures', as an alternative to 'interactive/connection/group' structures
        - for example, identifying 'false useful' structures, which are falsified versions of useful structures, which are easier to fake and therefore likelier to exist than 'true useful' structues, are an implementation of the concept of 'incentives' applied to fulfill the intent of 'identifying probable structures', which is a useful problem-solving intent, just like identifying interactive structures and input/output sequences and connection structures are similarly useful in filtering probably useful/true/relevant structures, by applying the concept of 'truth' to 'useful' structures and applying its opposite to differentiate true/false useful structures, given the relevance of false structures bc of incentives, which makes them more probable
        - similarly, identifying meaningful vs. meaningless structures is useful to differentiate useful filters of real/probable structures, such as outdated/nonsensical/false/trivial/non-causal/indirect/independent/incentivized similarities (such as arbitrary coincidences, like the requirement to use one of a set of values, where any selection in the set could have a false similarity but it wouldnt be causal/relevant/real and doesnt change/determine any other variable) which are useful in determining falsehoods vs. new/specific/obvious/reasonable/true/determining/powerful/causal/direct/dependent/dis-incentivized similarities, which are more useful in determining the truth and related structures like possible/probable structures
            - for example, the statement of 'justice hurts everyone' ('an eye for an eye makes the whole world blind') has an incentive for predators and doesnt successfully apply the concept of a symmetry (whats in it for victims, rather than whats in it only for predators, since there is a symmetry between individuals where both should be treated like equals if they are equal and otherwise fairly treated, rather than favoring and sheltering predators, as there are often different info sets that should be connected at all times, such as a 'global/local perspective' and a 'victim/predator perspective' where a symmetry of fairness is applied, such as in the golden rule, where these differences from a 'vertex' structure aligning with a cross-interface structure)
            - similarly, there is a 'test' (like 'see if an agent responds to this incentive to see if they would develop or have this function') for every 'filter' (like a 'similarity' filter, as in 'is the agent similar in other relevant ways to the agent who developed/had that function') bc they obtain info in different ways
            - relatedly, these symmetries and vertexes can be applied to derive the optimal positions of these concepts like balance/justice/power
            - similarly, the 'self-awareness' symmetry (between the perspectives of self-reference and how the self is referenced by others) and the abstract structure/context symmetry forming a vertex (the context that makes an abstract structure like an abstract overlap/frame obvious or otherwise useful)
            - relatedly, identifying the 'function usage' structure of 'regularly applying the golden rule (to consider other valid perspectives, like other equivalent alternate priority sets that are similar to other useful perspectives)' is a useful structure to identify and apply as a solution to the common error of 'only focusing on one perspective at a time', to regularly correct for this error as it will tend to re-occur regularly or by default, related to function usage structures like 'regular intervals/alternating sequences of applications of a rule', where this structure is necessary to make the rule have any permanent impact or make the rule valuable at all (the golden rule is only useful if applied regularly to offset the 'one perspective/over-prioritization' error) 

    - identifying structures that connect problem-solving structures like functions/workflows so that other interface structures like the 'reasons why problems exist' (such as non-linearity/complexity, resulting from 'lack of methods to find shorter routes given that there always is a short route possible in some graph') are trivial to identify, at which point once identified, they can be used to understand problem-solving better and optimize workflows
        - the problem of finding all ways to solve problems involves solutions like 'reduce difference/distance between problems/solutions' (by implementation methods like 'by reducing differences between all structures') and 'connect problems/solutions' (with implementation methods like 'by finding all connections'), which amounts to the problem of 'making all problems linearly solvable' by identifying connections that are linear or connections which linearize other connections, such as how high variation variables are useful in making many complex problems linearly solvable, since all structures can be connected and there is always a short route vs. a longer route to connection any pair of structures, so connections to find these shorter routes (involving variables like 'high variation variables' and 'powerful variables', both of which cause a higher ratio of changes than other variable types) are useful to identify to fulfill the intent of 'linearizing all problems' (by finding all linear connections between variables, all graphs which make connections linear in a useful way such as repeatedly useful across problems of connecting different structures, etc)
        - for example of linearizing the problem of regression, identifying insights such as 'balance is a common structure' (and related insights like 'extremes are rare structures' and 'alternating' is another way to apply an 'average' function) to identify useful filters such as 'alternating between extremes' or 'balance between extremes' applied to reality-covering variables (like abstract/specific, clear/ambiguous, volatile/stable, simple/complex, similar (average)/different (outlier), etc) is useful as a selection algorithm of the 'next point in a sequence of inputs', applied when fulfilling intents like 'selecting points to use to create a regression function intersecting with these points, when iterating through input points'
            - similarly, identifying reality-reflecting variables such as 'randomness' (which reflects the real proportions in a data set when applied to a 'point subset selection' function) is useful to identify variables that are useful to identify real structures like real patterns/averages
                - relatedly, identifying the various reasons why a data set might reflect randomness/balance/other conceptual structures is similarly useful (these variable interactions allow random variables to compound, creating some distortion of the data set), and identifying various signals that these errors have occurred (some ratio of data set points reflects a similarity/average function that is stable across subset selections, indicating that any distortion of this similarity/average function is an error structure)
            - why does reality reflect these structures such as 'balance'? 
                - 'balance' between extremes like simplicity/complexity enables a higher ratio of variation to occur/be real and continue to occur/be real (such as how over-simplifications lead to static structures like 'avoidance/lack of problems' which leads to 'lack of variation' and 'lack of variation-handling functions', and over-complexities lead to static structures like a 'required higher ratio of time spent to resolve the over-complexity, so that other problems cant be focused on'), but between these extremes, there is freedom, in the variation supported by that balance
                - similarly, standards/limits offer freedom to vary a 'more trivial amount, within those standards/limits', until a better standard is found that has more certainties which are more relevant to the new higher variation that is enabled by the previous standards/limits/certainties
                - finding 'structures that enable freedom' (like balance/standards) is likely to find real/new structures such as 'probable next structures in a sequence including current/past known structures', as reality can only be real if there is variation/freedom supported/enabled within real structures
            - this makes other useful structures trivial to identify, such as identifying that these variables determine most useful variation in useful functions, such as the most 'complex independent' function (supporting complexity such as by implementing scaling/iteration, while requiring the fewest inputs, such as a function that produces volatility) or the most 'abstract simple' function, etc, which are useful inputs of workflows as default solution-finding methods, default difference-connecting methods, etc
                - similarly, rather than just the 'most' ranking/standard of a structure, other rankings/standards using solution metrics, such as the 'simplest extreme differentiating' function, the 'unitary complex function' and the sets of these functions, which are useful in that they make structures like 'function limits/extremes' clear so that finding functions around/in between limits more possible, and also they are not only cross-interface structures but they are also components of results of interface queries and interface queries themselves, which can be applied as default inputs to workflows, given that they embody various standards which are useful for workflows like 'change a standard/base solution' and also embody various extremes which are useful for workflows involving 'reducing differences or applying maximal differences like extremes'
            - similarly, identifying other 'reflection symmetries' which preserve info, like how the 'system' complexity where a problem occurs is likely to be reflected in the 'problem' complexity bc of the symmetry in the context scope reflecting info of different but related scopes (system/component info)
        - relatedly, the intent of 'deriving the implementation method (reduce all variable interactions) from the intent (reduce problems)' can be a matter of fulfilling the definitions/requirements of 'implement', such as 'vary the intent enough for it to be specific, since the implementation method will have to be similar to the intent to a sufficient degree but also sufficiently different that it is useful in some way such as being more specific than the intent'
            - similarly, related implementation methods like 'find all "equivalent alternate" simple variable sets and apply those as default reduced variable sets to connect, since one of the equivalent alternate sets should be enough to derive the "most reduced set of connections" that is the intended solution structure of the "reduce problems" workflow' can be found by applying changes like 'generate "interim" structures like "reduced variable sets" which are a useful specific structure to make finding the solution structure trivial'
            - similarly, 'stacking reductions' (to find the biggest reducing variables and inputs of them, such as 'compounding reductions') and 'reducing randomness (the most complex structure) to linear functions (simple variable interactions)' are alternate variants of the 'reduce problems' workflow that can act like implementation methods for their specificity
            - relatedly, identifying variables that are reality-covering identifies variables in which 'all things are possible', but more specific/filtered variables are useful for finding 'biased interactions' (with a tendency to favor an imbalance or have an unambiguous ratio), which are often more adjacently useful in determining important variables like 'net/scaled effects'
        - 'identifying new connection functions between high variation variables (like abstract variables)' such as by 'scaling interactions of abstract variables (like scale) to identify connections between high variation variables (like extreme/unit structures) to approximate "thinking deeply about a concept like risk/scale"' is useful to identify new workflows, as connections between high variation variables are likely to capture enough variation by default to store solutions to most/all problems, and are likely to be adjacently connectible to problem/solution structures through their tendency to be abstract and therefore adjacently connectible to most if not all structures
            - similarly, identifying new connection functions between 'maximally different structures' is similarly likely to contain enough variation to store solutions to most/all problems

    - identifying structures that seem disconnected but have enough variation to be generally useful and have enough interactivity/similarity to be connectible is useful to fulfill intents like 'identify new variables to apply in algorithms'
        - identifying uncorrelated vs. correlated variables (based on 'info loss/adjacence/similarity') is useful through implementing the concept of 'independence', which is useful to identify through its interactions with other variables like 'complexity' and 'cause'
            - relatedly, identifying how to convert one into the other through connections between un/correlated variables, and identifying variables that are uncorrelated vs. how to differentiate variables from uncorrelated variables to identify variables that are correlated, are all useful intents to fulfill to identify new variables to apply as inputs to regression algorithms, such as how 'area under curve' and 'average change rate' are not correlated with functions bc they encode the concept of averages (area removes encoding of function shape, but not average output height), whereas 'maxima/minima points', 'slope sign change points', 'local averages', and 'regularly-intersecting functions like summary/average functions' are correlated with functions bc they encode 'differences from function similarities (like intersections/averages)', so identifying differences between uncorrelated and correlated variables and similarities within correlated variables and within uncorrelated variables is useful for identifying all useful variables in determining a summary/regression line of a data set
            - identifying 'equivalent alternate descriptors' like 'slopes vs. maxima' is similarly useful, as they can often be used to determine each other bc they store similar info, vs. 'variables that store complementary info' such as info describing different extremes of a reality metric like abstract/specific (storing abstract info and storing specific info often involves storing different sets of info), as 'complementary' encodes the concept of 'independent variables' in a different and therefore differently useful way
            - given that variable interactions depicted in a function form are micro-descriptions of reality, they need to reflect reality (they should fulfill basic metrics of reality like some degree of complexity in general, except for cases such as where a variable interaction is causally adjacent and therefore likely to be linear/simple, so identifying this network of cases where a reality metric doesnt apply such as with specific causal structures is useful)
                - relatedly, identifying how 'independent' variables correlate with other conceptual variables like 'complexity' is useful to identify other useful structures, since more independent variable interactions are likelier to be more complex (but are not required to be complex)
            - relatedly, identifying structures that are meaningful to agents is likely to be a source of meaningful structures in algorithms - for example, identifying a life/decision that is meaningful, such as one that doesnt only follow incentives but also fulfills other solution metrics (such as 'high variation' and 'under pressure, became reflective of reality' and 'allows for and involves progress/learning'), as 'differences from incentives' is a highly differentiating variable
            - this 'set of fields of functions connected by concept metadata (like concept combinations, concept values, concept intersections, concept spectrums, etc)' is generally useful for identifying useful function similarity indexes (to connect/identify similar functions), identifying useful function variables, and identifying useful connections across function sequences like 'variation across "similarity indexes (like plateaus/powers/slope sign changes/wave similarity/volatility/specificity)" and variation across "differentiating filters within similar function sets (like limits/extremes)" used to generate the sequence' (sequences such as "sequences created by a function change/generation function" to find functions to check, or "sequences from partial to complete functions", or other function sequences resolving ambiguities between similar functions), so that questions like 'what attributes should a function sequence or function sequence-generating function have, to maximize chances of finding a useful summary function in that sequence' can be answered ('it should apply certainties/uncertainties in the data set as constants/variables', 'it should increase/decrease a variable if it increases/decreases an optimal solution metric', "it should apply a 'conceptual similarity score across many similarities' rather than a "one-metric/simple similarity score" when calculating errors", etc)
                - finding a 'most reduced set of numerical values' that encodes enough reality-covering variables and represents enough variation to graph all relevant different functions in a problem space (like all maximally different polynomials for the regression problem space) and connecting them with every similarity type, in every possible function sequence having a sequence size that could be useful, is similarly a useful set to connect to other structures like similarity index networks and data sets/functions, to find useful queries to connect similar functions
                - for example, a set of numbers that has 'overlaps in unit operation outputs' (where various unitary polynomials overlap) is required in this set of useful numbers to describe all different polynomials
                    - similarly, other interface structures should also be present in this set, in order to fulfill the requirement of maximal differences allowed by the set, similarly it should include a set of numbers that allows 'polynomials with varying magnitude', etc
            - similarly, the 'structure of data set values checked/used directly in a regression algorithm' (a structure such as a 'continuous subset line' or a 'set of points or continuous subset lines at regular intervals' or a 'ratio with a selection method like random') is a useful variable to apply that determines differences between algorithms that are useful, so given this differentiability, it can be applied as a variable to generate new algorithms

    - identifying problem/solution structures on interfaces that are likely to apply to other problems and therefore likely to be useful for identifying other useful structures like insights is a useful intent to fulfill
        - for example, identifying 'causal problem/solution structures' such as how nodes with common inputs (like how 'emotions' and 'actions' both have 'information' as an input) and other nodes with common outputs (such as how 'thoughts' and 'emotions' may have 'actions' as an output) may have an intersection ('thoughts may change emotions' and 'info may change emotions') rather than 'emotions' being an irrelevant output of 'thoughts/information', it includes the output of 'emotions' as a possible input interim node in a sequence
        - identifying this causal problem ('which nodes are connectible in a sequence') and a possible solution to that problem in the form of the accurate causal structure of the actual causal sequence that applies ('outputs like emotions of an input like information may be an input to the other outputs like actions') is useful for identifying insights like 'thoughts/info can change emotions'
        - this can be resolved by applying the 'sequence' structure as a possible solution, to check if its possible to compress the irrelevant/unconnected nodes into the same sequence
        - an alternative solution exists, which is that the output node (emotions) would occur after the other output (action) but a 'time' constraint filters this out, similarly, 'emotions' could reflect a similar degree of variation of 'information' inputs that it could act like a proxy or equivalent of 'thoughts', which is another possible insight to identify by switching/overlapping/grouping node positions in the sequence, adding a 'node layer' variable where multiple nodes are possible at one position in the sequence
        - resolving other causal ambiguity problems with similar solution structures would be similarly useful for identifying insights (causal connections between nodes)
        - these example problems/solutions on various interfaces like 'cause' are useful to identify in case they apply to a given problem, which is likely given that they are core/unit problems that can be composed or otherwise operated on using core interface functions, as pre-solved problem/solution unit pairs

    - identifying useful structures to combine with other workflows in structures like a sequence or variation (like a specification) such as how following workflows like 'pressure/stress/change known certainties until uncertainties are resolved by identifying new certainties' (or apply changes to uncertainties like 'ambiguous/trivial differences' until theyre connectible to certainties like 'obvious differences') is made more specific when followed with or specified with structures like to apply changes 'in the direction of uncertainties, once certainties (like limits of computation) are identified and structured in a network or other structure that correctly positions uncertainties in between known certainties/limits'
        - relatedly, specifying the 'limits of computation' is useful in general and similarly solutions to solve the problem of 'increasing computational capacity', such as to identify complexity and number of problems that can be solved with current computation capacity simultaneously and how these problems are likely to increase computation capacity, and identifying problems that help solve other problems to increase distance from limits of computation, and similarly, physics limits such as the distribution/usage of computers in the universe that can exist without interfering with variation of life forms that would benefit from their computations (its possible to convert most positions into a component of a computer, but is that detracting from its ability to compute something else, and what else might it be computing)
        - similarly, identifying the 'sets of certainties' that are useful in identifying these uncertainty directions/structures is useful, such as identifying the 'high variation cross-interface certainty sets' that add the most value in determining uncertainty structures
        - relatedly, identifying a structure where the uncertainties have some similarity in common like a 'pattern of directions' is useful to identify variables determining uncertainties

    - identifying useful structures like 'alternatives to definite negatives (like war)' such as fake wars with fake weapons like joke weapons, being higher variation/potential than the enemy, increasing distance from the enemy by leaving the enemy behind with more space innovations, using different methods that are less toxic like insults, creating so many resources that the enemy doesnt need to fight over resources, different positions of war like different battefields such as online or in entertainment media or in energy innovations, making other people fight a war, being better at defending the enemy than they are, etc
        - these are useful bc they incorporate more solution/optimization metrics than just war on its own and support higher variation bc they involve less irreversible decisions like killing, which even if similar to war in other ways at least avoid its worst errors
        - the problem of 'war' applies to other problems that involve negative/harmful processes or agents like in the bio-system, where pathogens often use similar tactics such as false info
        - similarly, the problem of 'war' often occurs bc of a scaled/combination function applied to errors that can easily cause instability (like 'lack of logic' and an 'overabundance of emotions' and 'lack of ideas about alternate govt methods' and 'large groups wanting the same resource which are positioned adjacently and are similar enough for the war to have an uncertain outcome')
        - identifying alternate problem formats which capture enough variation to be applied as an interface is a useful intent to fulfill for other problem-solving intents, just like solving 'scaling' or 'batching' or 'delegation' or 'circumventing info barriers' is useful for solving other problems
        - relatedly, identifying structures that are obvious in some problem formats as generally useful structures, such as how 'war' makes structures like 'conflicts between opposing forces' obvious, so that this structure can be applied across problems, such as how in the 'regression' problem space, opposing forces like generative functions and limiting/filtering functions exist, which can be resolved similar to how war conflicts can be resolved (by aligning the opposing forces to have a similar or equal direction, such as in 'war' by helping solve the enemy's problems for them, and in 'regression' by integrating the filter in the generative function, such as by applying filters to inputs to the generative function which reduce the post-generation filtering problem requirement)
        - relatedly, problem structures like 'tradeoffs' can be resolved with similar direction changes/alignments in the opposing forces of the tradeoff, so that both forces can change in a similar angle or other similarity

    - identifying useful structures like 'truth structures' from identifying structures that when applied to the opposite/any/all statements seem equally possible/valid/true, like structures containing 'multiple perspectives', which may falsely seem true bc they contain an interface structure ('multiple' and 'perspectives') and support other truth/interface structures like 'high variation' and truths like 'the higher probability of validity of more perspectives than one (except interface analysis itself)', so these 'truth structures' which make a statement seem true can be applied to identify other truth structures
      
    - identifying useful structures (like 'positive opposites' and 'connections to truths') that can be applied to find useful differences from error structures like 'known conceptual errors' to find useful rules to apply in intents like 'filtering workflows' or 'filtering truths'
        - for example, known conceptual errors like 'over-simplification/reduction' offer a way to identify opposing conceptual solution metrics like 'fairness' (found by applying the positive opposite, as opposed to a negative opposite, such as another error like 'over-expansions'), such as 'allocating work to functions/systems that can handle it' which derives other solution structures like 'delegation', which can be optimized with other workflows to identify other optimizations (like 'allocate work to functions/systems that can almost handle it and identify resulting errors as a way to determine where organization can be applied to enhance their capacity' which could fulfill other functions like 'learning' as the functions are likely to be almost optimal but will need to change slightly to achieve optimality)
        - similarly, identifying related structures of conceptual errors/optimizations is useful, such as how identifying that 'most statements could be called over-reductions, as the truth is often more complicated than most statements, and most statements are also false, as there are often more distortions that could be applied to make a true statement false than distortions that could make it true' is useful to make identifying over-reductions more trivial

    - identifying useful different formats is useful, bc they are often useful for some specific intent like describing a relevant attribute of a problem, but are also useful for generally useful intents like 'identify conceptual differences (like the difference between correlation and causation)'
        - for example, different graphs like a graph of 'state changes of a variable (such as the state sequences that describe its real interactions in real systems)', vectors indicating simple attributes (like 'slope of one variable') which allows comparing slopes between different variables trivial, or a euclidean graph of input/output variables which has false implications that need to be corrected with rules like 'correlation is not causation' as it adds a directional difference as a standard compared to 'simple slope vectors', all fulfill different useful intents and also imply different adjacent conclusions which may be false, such as where 'simple slope vectors' might not cause this confusion between 'cause' and 'parallel slopes', but the euclidean graph fulfills other useful intents like trivially identifying 'what changes in the output variable when the input variable increases', where 'difference from the original format' or 'difference from standard formats' may be an indication that more false implications are likely, and possibly also an indication that these formats should be graphed as a set rather than in isolation

    - identify useful structures like useful filters for workflows/interface queries like 'cases that make a workflow more useful/optimal' (for reasons such as 'bc of the functionality of the structures involved' and the 'problems identified which would benefit from that functionality')
        - for example, workflows using concepts are useful when various cases apply in the problem space, such as where 'adjacent conceptual combinations havent all been applied in the problem space, indicating that applying these would be useful to check' and when 'a field/problem has stagnated for lack of new concepts, indicate a missing concept which would be useful to find', in which case workflows applying conceptual structures are most useful, indicating that those workflows should be used first and are likelier to succeed, as a way of filtering workflows to use for a particular problem
        - similarly, 'abstract (reality-covering) concepts' are high-variation structures which explain many other structures with trivial changes, so theyre also better to apply in cases like where complex understanding or complex changes are required, as high variation structures like concepts can often simplify complex problems

    - identify useful structures like info structures common across many or all problems, such as 'function usage barriers (where using the function is easier with usage/practice)' or 'info barriers' (that require coordinating/differentiating effects to overcome or structures adjacent to other functions like 'adjacent concepts to make some function easier')
        - for example, a 'barrier followed by a cascade' is a common info structure in problem-solving, which is associated with some other structures, such as 'complementary/coordinating/compounding and differentiating function sets/sequences/networks' where these function sets have some structures in common that give them this side effect and often have structures in common with other 'barrier-cascading' function sets such as the number of functions and function iterations in the set, as most problems are problems of a lack of functions specifically organized to overcome that barrier, but which often already exist and just need to be paired with enough existing functions in the right structure to have these complementary effects to achieve scaled outputs that could workaround the barrier, so finding coordinating structures that can achieve scaling is useful to find possible 'cascade-producing' functions, which are a solution set in the problem space of 'overcoming scaling barriers'
            - for example, 'thinking' is a problem space with barriers such as biases, which are easier once these barriers are overcome (same with other processes that become easier with usage/practice, like 'default functions' or 'default/cached function usages' or 'pre-computed function inputs/output maps' or 'indexed functions'), with function sets like 'focus/filter, structure, change' or 'start with unit case' or 'find interaction level with highest variation'
            - relatedly, some functions that use similar functions as workflows ('start with unit case, then expand/complicate it' being related to 'start with base solution, then change it') which can indicate that they fulfill those workflows to some degree or in some structure
                - these functions are useful to identify in connection to these workflows as theyre more specific and therefore specialized/optimized for some problems, and identify other useful structures such as 'find the intersections of scaled unit cases of different problems, which create the more complex problem'
                - relatedly, another reason to start with a 'network of unit cases' is to have a solution for every initial direction that leads to a problem
                - another reason is to avoid the over-simplification of 'avoiding barriers' which may solve a problem temporarily but would not solve the problem of 'overcoming the barrier within the parameters of the unit case, which if it can represent all problems, should have the variation necessary to overcome the barrier', although 'avoiding barriers' is one way to determine where the barriers are
        - relatedly, 'info bottlenecks' preventing info arriving as expected, and being described/explained by structures like 'batches after reaching a threshold' are related structures of specific errors that can describe some problems
        - similarly, 'delegation' is another problem-solving structure which involves 'allocating problems where they can be handled best' which requires 'organizing/sorting these problems or related problem structures like problem causes'
        - similarly, 'rules that a bias would create' such as over-simplified rules such as 'just add variables until something works' is an over-simplified biased rule that can often be avoided

    - identify useful structures like 'useful positions of workflows in the workflow network' which can be determined by structures like 'probable ranges/requirements' of useful metrics like 'creativity'
        - for example, 'change a base solution' is a useful workflow when an average 'degree of difference from existing solutions' ('creativity') is useful, so determining metrics like 'probable range of required creativity' in order to apply rules to select workflows accordingly is useful as a way of generating alternate 'intent sets' to fulfill in the initial interface query that selects workflows and applies them in various positions in the network represented by the query, otherwise other workflows can be selected for other ranges of required creativity, such as low complexity problems which can likely be solved by 'adjacent combinations of existing solutions' or high complexity problems which can likely only be solved by 'finding "new abstract concepts in a field" or "a new field and default concepts in it" is required to make a solution adjacent'
        - therefore determining the position of 'change a base solution' in the network of workflows is possible by applying metrics like 'probable required range' of 'degree of difference required to solve the problem' (representing 'creativity') and associated metrics like 'complexity' which can be determining in the identification of creativity ranges
        - metrics that temper 'creativity' such as 'relevance' are useful to apply to determine 'relevant creativity' (a more useful form of creativity as well as most other metrics) by keeping creativity within a degree of difference that is useful, usable, testable, measurable, determinable, differentiable, describable with existing/adjacent concepts, possible to implement, different from simple/default structures that could falsely seem similar to creativity like 'iterations without a reason' which can be applied for more 'exploratory creativity' intents (for when resources are abundant like when the problem-solving program isnt being used) where real relevant conceptual creativity by comparison 'implements multiple conceptual variables in a useful way (such as by identifying a new structure based on new combinations of similarities applied as certainties/standards (such as requirements/insights) to base changes around, which fulfills multiple conceptual attributes like "complexity" and "abstraction" and can be described in many ways, which can be generated by default to apply as new solution sets to filter when more creativity is required)', etc
        - these conceptual workflow metrics can form a network that can be applied to select workflows/implementations of them, just like a workflow network is useful to determine known useful routes between and positions of workflows
        - similarly, a 'concept set' iteration algorithm can be applied to identify the structure that implements each concept set in a problem space, since abstract concepts describe reality in the most orthogonal ways which are likeliest to decrypt complex structures and also likeliest to be creative, other than algorithms that 'apply a simpler type of difference like "more differences" such as "iterated differences"'
        - similarly, 'trial and error' has a similarity to the definition of 'random' (in a way to generate it) just like 'change a base solution' has a similarity to the definition of 'certainty' (in its interactions with certainty changes, such as new information), which means that other concepts can be defined with variable definition routes that, through their interactions with other structures, will likely correspond to workflows

    - identify useful structures like optimization functions/metrics to apply to improve/differentiate solutions (like usage/implementation/interpretation/understanding/existing information before applying a workflow, etc)
        - for example, 'change a base solution' is a generally useful workflow, but it could be useless depending on the implementation (the interface query implementing the workflow) and on the usage
        - realization query: how did I come to this conclusion? by comparing how a smart mind (mind with better scaling functions which can hold/identify more structures and think more steps ahead and has more functions/structures like more memory and more sub-networks) would implement a workflow like that, compared to how another mind might implement a workflow like that, which could have very different results despite using the same logic, as implementation and usage and important factors in the ultimate usefulness of a function
        - for example, a smart mind might apply 'change a base solution' by first learning why a solution is useful/optimal, learning the problem space system to some degree such as knowing common problems/solutions in that system, as well as what variables determine optimality, how to test solutions for improvement in some optimization/solution metric, etc, and therefore bc of these other structures, the solution they select to apply as a base solution would be far more optimal in many cases, whereas another mind might apply it by randomly changing or adjacently changing the base solution without thinking about whether the changes could produce an error
            - I could have also realized this by iterating through 'high variation variables' (one of which is intelligence) and applying it to interface structures iteratively

    - identify useful solution structures like 'generality' of solutions and 'specificity' of information about how to enable/trigger the more general solution (like enabling immunity by clearing inflammation) that tends to be useful in the problem space system
        - for example, proxy target solution structures in the 'find a medicine' problem space include medicines that 'avoid some causative factor of medical conditions (like whether a medicine is pro-inflammatory or anti-inflammatory, as its rare for a pro-inflammatory compound to be useful)' or 'increase/decrease some metric outside of its optimal balanced range (where if the metric is extreme, different groups of cancers are caused, but if its in the balanced range, cancer is rare)'
        - similarly, compounds that have a wide variety of similar structures to pathogens but arent pathogens (molecules designed to trigger antibodies) are a good target for building immunity which is useful for avoiding cancer
        - similarly, compounds that have useful functionality related to important functions like cell communication, lowering stress on the system, repairing DNA, clearing useless metabolites (waste) from the system (increasing antioxidants to reduce reactive oxygen species, increasing flavonoids to trigger useful metabolic processes similar to diuretics/enzymes, increasing preserving membrane integrity with modulators like caffeine/cbd, increasing highly variable negative compound-binding compounds, increasing cell devision inhibitors, increasing anti-inflammatory substances like salicylates to reduce inflammation metabolites in blood, increasing antimicrobials like sitosterol to avoid excess microbes & their metabolites, increasing metabolic regulators like berberine to avoid excess metabolites, increasing functionality of filtering organs with substances like chicory, increasing processing of plaques/calcifications, increasing paraoxonases to protect existing cells, increasing DNA repair compounds like silymarin to protect existing structures rather than needing to break them down, increasing immune regulators like sitosterol to decrease immunity metabolites, increasing molecular weight of compounds like hyaluronic acid to avoid absorption/processing to prevent excess metabolites, etc, all in small enough amounts to increase 'clearance of the blood'), downregulating cell growth, etc are useful targets for anti-cancer compounds, bc they represent components/causes of carcinogenicity of mutations/pathogens/states
        - so identifying whether the compounds is likely to be anti-inflammatory is useful to filter possible medicines, which is likelier to be a more effective and generally useful target for finding medicines than specific activity against some state/pathogen, which is hard to find as most medicines with specific activity against pathogens are also toxic to host systems, and also the immune system can often handle most disease states on its own if its work is minimized by clearing inflammation, reducing oxidation, managing epigenetic activations, repairing DNA, etc
        - relatedly, finding useful interface structures like 'optimal ranges between extremes' and 'extremes which cause disease states (like how extremes of some variables cause different cancer groups, as there are groups like hormonal cancers and immunity cancers and so on)' allows specifying target solution structures further
        - similarly, identifying useful interim structures like 'pathway activation sets' is another useful structure to identify and apply as a default input, as its more trivial to identify an optimal set of pathways to activate to manage a condition, and then find specific medicines for that set of pathway activations, than to just filter the whole set of possibly useful medicines, as a useful interim/proxy intent to fulfill than the original problem-solving intent

    - identifying useful structures like graphs that organize useful structures (like 'filter sets with common outputs') which are useful to filter by finding their organizing patterns, since there always are some patterns of filter variables that determine other useful filters
        - for example, identifying a graph of a 'filter circle' where patterns between results of different filters can be organized and easily seen by applying different filter sets/combinations/sequences in different directions, is a useful structure to identify to see 'patterns in outputs' of filter combinations without having to compute all the filter combinations
        - this is useful bc (using a structure like an 'undirected' network), it applies 'trial and error' to resolve an uncertainty in a particular variable (an uncertainty between what are falsely similar to 'equivalent alternates', where an ambiguity exists that looks like an equivalence), and the routes that produce equivalent/similar changes will become clear so that alternate 'causal paths' of the same outputs can be identified and filtered further once identified, and applying every variation of 'filter variables' is particularly useful in finding maximal differences in filters to apply
        - for example, applying filters like 'functions with average values of useful differentiating attributes (or function similarity indexes) like volatility, to find useful structures like "determining directions" away from this average value toward other function types' and 'functions with extreme/trivial change rates (to filter falsely similar function sets, like linear functions and wave functions with trivial magnitudes)' are useful to filter function attribute values/ranges that can be applied to resolve uncertainties between different function types, where these types are useful to identify bc they are determining of other info about the function, as 'slope extremity' and 'volatility' are useful maximally differentiating filters to apply
        - this indicates the value of related useful functions like 'find the points of these attribute values that are determining, in that trivial changes are required to determine the actual attribute value, like changes in opposite directions from that determining point'
        - this is also useful for finding the 'rings/overlaps/filters/directions (and related structures) of function attributes' that create function similarities/differences
        - applying patterns in differences (like a pattern of alternating values, applied in every direction) is also useful in that it could make identifying a "certainty/structure (applied in the pattern) in the change sets reflecting real new/unidentified similarities" trivial, since patterns are a type of similarity
        - relatedly, applying a filter to exclude 'equivalent alternates' as default changes to apply since they are likelier than other structures to represent real ambiguities (except for the 'equivalent alternates' of maximally differentiating variables), to resolve these ambiguities by applying different structures than 'equivalent alternates', could make identifying the resolutions trivial
        - this solves the problem of 'identifying the variable to add that makes connections/similarities/differences between other variables clear' (like identifying a causal structure of a circle that when rotated, produces the original variable, like 'height of a point on the circle')
        - this identifies useful info like that workflows like 'trial and error' and 'change a base solution' are connected to abstract concepts like 'randomness/uncertainty and ambiguity' and 'similarity and certainty'
        - similarly, given the spectrum of 'equivalence, similarity, difference, opposite' that exists (and other possible structures like cycles where these would be in surprising positions) in many problem spaces, determining this field of equivalence & related concepts by identifying 'functions that connect these concepts in that problem space' is useful as a filtering structure for function sets to apply as a default solution space
        - similarly, a graph of 'cross-interface' structures or 'perspective vertexes' is useful bc of the usefulness of these structures (including the 'reasons/intents' associated with a variable interaction since theyre highly determining of variable interaction structures such as changes)

    - identifying useful structures (like 'optimization rules/metrics') that are useful in that they can be applied in other systems (like 'filtering interface queries' which is a problem-solving intent) to a similarly useful degree
        - similarly, just like identifying simpler vs. more random functions is useful to identify the interim set of functions in the regression problem space where more complex functions will be found, identifying the patterns of interactions in possible interface queries that have known effects or random effects is useful to identify the set of interface queries with interim effects (uncertain effects, between known/simple and random), like how identifying that 'function/structure' is a pattern that can be applied in a sequence in a valid form, and similarly, some functions can be applied in an 'iteration' pattern to solve a problem (problem-solving structures like 'filter' can be iterated in a sequence forming the interface query), and some structures have 'interchangeable equivalent alternates' which can be used to vary the interface query, similarly, other useful interface queries which are likelier to be able to handle complex problems are likely to be in the interim space between these pattern-compliant queries and random queries
        - identifying query metadata, such as the 'degree of the sequence of side effects of a possible solution that is derived and filtered for some solution/optimality metric' (like applying '10 steps ahead' as a solution metric for filtering a solution, and determining that 10 is the number where most solutions are more permanently optimal or reusable, if their side effects are analyzed 10 steps away and they still pass solution metrics at 10 steps ahead) is similarly useful for filtering this set of queries

    - identifying useful alternate structures of useful structures is useful to identify variables of useful structures that can identify other alternatives
        - for example, structures like a 'language', 'memory', 'observer', and 'computer' can be applied as functions to use as proxies for other functions (like map/find/check/scale), which indicate their usefulness in algorithms, so an algorithm using these useful info/solution structures as components will likely have overlaps with algorithms based on the functions optimized by those structures, to a point where sets of these structures can be used as an alternate set of problem-solving structures
        - similarly, applying useful intents to these structures is useful, like how 'differentiating useful structures' like dictionary/language/memory (a dictionary is a set of unique definitions, a language is a dictionary that can be more easily used than the original dictionary, a memory is set of usages of the items in the dictionary in connecting/organizing/storing inputs, such as sensory inputs) identifies the usefulness of identifying variables of these structures and the other structures created with those variables
        - similarly, a 'state with rank/priority' (best/worst cases) is useful to identify as a useful info cross-interface structure (where 'relevance/optimality' and 'contexts/systems' intersect) and 'info structure with intent' (like a list, which is likely to be used for creating other similar structures like maps, for filtering info related to the listed info, for identifying members of a set, for aggregating useful structures like 'instructions to be copied & applied elsewhere', etc), which can be 'described to identify variables' and 'varied' to find other useful info structures
        - why is it useful to identify new ways to implement the same or similar workflows (like 'identify useful structures')? bc specific implementations can be different enough to be useful to identify and apply as separate workflows useful in different situations
        - governance/market structures are similarly usable as an alternate set of structures to these core problem-solving functions, as they resolve resource distribution problems and encode power distribution insights like 'group dynamics should be applied in governance structures, such as majority rule or representation of groups' to derive other insights like a 'true democracy handles group dynamics the best, and improves itself over time to better serve different groups (such as with incentived paths out of crime like incentives for drug dealers to find medicines or narcotics with neutral effects, or more specialized incarceration structures to handle group dynamics, which allows earning privileges like switching between groups to avoid incentivizing more serious crimes)', and similarly 'communism' encodes structures like 'sharing information' and 'social networks' as a structure to fulfill problem-solving intents like 'find info'

    - identify connections between useful structures like 'similarities' and 'connections to requirements' and 'optimizations' to identify specific useful structures like 'limited ranges of optimality' that determine other useful structures like 'probability' (structures will likely be in the 'limited range of optimality' if there is one so its useful to identify that as a useful determinant of probability and similarly useful to check for it)
        - for example, identifying that a 'limited range of optimality that is not required (allowed to vary)' leads to functions like the 'normal distribution' is useful to connect variable interactions with interface structures like common structures and probabilities and useful specific concepts like 'degrees of freedom', this 'range of optimality' being possible to predict using other relevant variable interactions such as 'common weight of required components requiring certain height/strength ranges to be narrow/limited (as opposed to unlimited)' (given weight/strength, usage/interaction functions of components of bio-system, its possible to predict the 'limited range of height optimality' leading to a normal distribution)

    - identifying useful structures like 'graph intersections' which identify structures that fulfill problem-solving intents like 'identify structures useful for multiple intents'
        - for example, the grid formed by 'repeating a unit interaction' is a useful graph to implement a graph of an interface structure (a 'unit') and to identify interface structures of that interface structure such as "intersections/overlaps between the 'unit grid' and other graphs", to identify structures that 'fulfill multiple metrics or functions', which is a problem-solving intent, as structures which are formed by multiple graphs are likelier to exist than structures formed by one graph, as 'commonness' is a structure of reality/truth, similarly a 'default', 'adjacent change', and 'iterated' graph are other interface structure graphs that are likely to create overlaps/intersections between interface structure graphs, as interface structures are highly connectible and interactive and many subsets of interface structures act like equivalent alternates
        - similarly identifying graphs that form interface structures (such as similar concepts or other useful structures like randomness) is useful for identifying probable/adjacent routes to randomness (as well as the 'scaled unit' route to randomness and other useful routes), as its unlikely that a problem will be solved by one graph and likelier that it will be solved by multiple graphs with common points/structures between them, where these common points are similarities that can act as the connections between the graphs, and similarly connections that create other interface structures are likelier to exist and reflect reality than other structures
        - 'finding the similarity indexes between graphs' is more useful to identify useful graphs or realistic graphs than any specific graph on its own, although some graphs are known to be more useful (such as a 'high variation, realistic, relevant graph' which captures a lot of information/variation but is still realistic and therefore relevant, such as by applying adjacent changes to real facts or combining known real concepts) so these can be used as default input graphs to a function to find similarity indexes between graphs
        - similarly, finding related useful graphs like the 'network of the highest variation variables (such as waviness, moments, repetition, symmetries, adjacencies, non-obvious pattern compliance, complexity, etc) that determine differences across functions (and have the fewest overlapping functions associated with these differences)' as a default network to traverse when finding maximally different functions and the 'network of optimizations (such as generally useful structures, like function optimization structures such as where functions are capable of producing the opposite/different effect of their explicit defined intents so they can self-correct or reverse when they determine theyve found an error, and similarly can evaluate/alter their own functionality to self-optimize/adapt, and similarly can evaluate the systems at various levels in which theyre applied for finding additional optimization opportunities)' which offer value in their differences of intent/structure associated with these functions as well as the variation captured in these networks and the connectedness with other useful networks, which are also useful as alternate perspectives of reality

    - identifying useful structures like 'filters which apply changes within a known range of optimality' (like changes that 'maintain functionality of a system')
        - for example, when finding medicines for a particular condition, 'compounds that deactivate a function with an interchangeable alternate function (acting like a backup)' is possible to use as a filter with changes 'within a range of acceptable changes in the system that likely or definitely wouldnt destroy the system (safety range)'
        - similarly, identifying 'compounds that dont have extreme requirements' (such as 'compounds that require 100% organ function' or 'compounds that require ketosis or create ketosis') is useful as another filter
        - similarly, identifying 'compounds that are not deactivated by common inputs' and 'compounds already created by other functions of the same bio-system (existing resources, which are less likely to be absolutely harmful given that there is already a function to process these compounds)' are similarly useful to identify
            - relatedly, identifying these useful compound filters also identifies useful optimizations of the bio-system (functions/requirements/variables that would be useful to scale up or add)
                - adding processing functions so the bio-system uses cancer cells as an input (energy source) to some existing requirement would be useful
                - adding processing functions so that more compounds can be handled would be useful (is the reason for many deaths a result of failed organs which couldnt process errors at scale, like excess inflammation or metabolites of immune functions? if so adding processing functions like blood filtering machines could make some health conditions trivial, so scaling up processing power would be a trivial optimization to fix these conditions)
        - 'deactivating one alternate, to require using other alternates, when there is a set of alternates of non-trivial size' and 'not requiring extremes' and 'not deactivated by common inputs' are structures that can construct 'optimality' or apply changes that stay within a 'range of optimality'
        - similarly, there are 'conditional filters' (as opposed to the mostly 'absolutely useful' filters above), such as variables with an imbalance often/generally favoring one variable value, like how 'compounds that can be processed by the bio-system' are generally preferred, but its possible that a 'compound which cant be processed would have useful impacts (such as various forms of sugar)'

    - identify problems with sufficient interface structures (like 'variable/limit sets') that solutions to those problems can be altered with other interface structures (like 'scale') to solve most problems
        - for example, the system associated with some problems (like the packing problem or the traveling salesman problem) can be isolated, re-applied, scaled, and trivially varied in such a way that it reflects more real systems
            - where the isolation is useful for reducing less relevant variables from changing the system, and where systems can be isolated in this way, solutions in this simplified problem space can be applied
        - relatedly, the packing problem has enough variables like 'uniformity of shapes' and cases like 'all uniform shapes' and components like sub-problems such as 'shapes created with unit combinations, where these shapes limit or allow other combinations' to make it high variation enough to reflect some ratio of 'complexity' (and other forms of reality) of real systems, and also this problem is complex and otherwise realistic enough to derive enough interface structures to derive all the others, where insights like 'identify all the variables that can change the problem or an important structure in the problem space' (and related workflows, like 'try combinations of variables that can change the problem or important variables of the problem') is trivial to identify from this specific problem space system, as an alternative problem-solving structure as other structures (like causal structures such as input/output sequences or interactive functions or sequence patterns)
        - similarly, the difference between the 'unit/extreme cases' and the difference between 'function definition/use cases/usages/intent/structure/implementation' are useful to include in a combination rather than in isolation, as they contain a useful degree of variation

    - identify specific structures (like 'specific difference functions' like 'isolate') which are useful to apply in the specific variant, through the usefulness of specific structures in identifying other specific alternatives and specific related or interactive structures to the specific structure which are similarly useful
        - identify useful variables of implementation functions of problem-solving intents like 'isolate negative/problematic structures (such as problem causes/triggers/inputs/users/usefulness)', where variants of the 'isolate' function include 'separate the problem structure from other structures, by adding more structures in between' (like for example, adding a non-plastic layer inside a plastic layer, to separate plastic from ingested substances, as these linings are interactive in a 'stackable' way, and some of them are non-toxic or can deactivate the toxicity of others such as by binding to toxic structures on the inner surface, as 'surface' is variable and has multiple implementation structures such as 'external/internal', as the original package has two important surface variables, how it interacts with the outside world to protect the substance and how it interacts internally with the substance, and similarly, adding another layer in between has corresponding external/internal interactions, to provide a different external interaction with the original package and a different internal interaction with the substance)
            - similarly, other variants of 'isolate' include 'wrapping the problem structure', 'prevent the problem structure from interacting with other structures by binding to its interactive structures', 'deactivating the problem structure', 'dispersing/decaying the problem structure', 'change the problem structure', 'limit the problem structure', and other known problem-solving functions which are useful variants of the 'isolate the problem' intent
            - relatedly, various core structures can be applied to solve this problem of 'isolating the problem structure', such as stack/interim/connection structures, as they'll have similar impact in solving the problem to find this specific solution of applying another structure in between the original structures, so applying one structure in each of these similar/equivalent alternate sets (like the set of 'stack/interim/connection structures' as one set in a set of sets that contain similar structures in the set and different structures across sets) is a useful way to test multiple maximally different structures without checking all structures
        - similarly, as 'usage' is a known useful structure associated with functions, interface variants of it (like 'users' and 'usefulness') are similarly useful in core functions like describe/determine/generate, so that these alternate structures like 'usefulness' can be applied to identify other problem-solving intents/structures
            - for example, 'making a problem less useful' (for whatever benefits from the problem state/structure) is similarly another problem-solving intent, as problematic structures often exist bc theyre useful for some intent/agent, so 'finding alternate similarly optimal structures for the agents benefitting from the problem structures' or 'making the problem higher cost for the agent, so they find another structure' is a related variant of that intent (like 'finding alternate ways to help make money for criminals, so they dont have a reason to commit crimes')

    - identifying useful structures that can be connected to derive other structures, like rules about structures like probabilities/defaults/biases which can be extrapolated to identify other rules connecting other info structures, where these structures can be used to derive a high ratio of other structures if found (perspectives/biases can be used to identify most other variables trivially, so connecting them to probabilities and other structures like defaults is useful)
        - for example, rules like 'the first guess is rarely correct' is related to insights like reasons such as 'bc most positions apply incorrect biases by default' and 'biased positions are more common than correct/more optimal positions', which can be derived from the rules about structures (like sequences/positions) of info structures (like guesses/hypothesis/predictions) by extrapolating the rule to interface structures (like agent perspective and related defaults/biases of that perspective and the relative position of that perspective to other perspectives and variables of perspectives like optimality) by applying defined changes of the structures in the rules, to identify useful structures like 'probability of optimality of a perspective/bias set' and identify 'perspectives to check, as equivalently probable'

    - identifying useful structures like 'spectrum variable configurations of reality (which can be formatted as intersection or adjacent/interactive points of conceptual variables)' and other useful structures like 'cross-interface structures' which are useful to connect to identify other useful structures
        - for example, the fact that 'reality tends to be complicated' (which is a point leaning toward complexity on the simplicity vs. complexity spectrum variable) can explain why other structures are useful, such as that a 'cross-interface structure' is useful bc it covers more complex cases and covers more information than one variable, as its difficult to explain all of reality with one variable (for example, its useful to know function/intent/usage and its useful to know 'if a person has brain damage, while evaluating their performance on a test' and its useful to know 'if someone read about communism or came up with it independently' rather than just one of those structures, as reality is not simple enough to describe with one variable most of the time, so this simplicity/complexity configuration reflects the reason why other structures are useful)
        - the point of this and why its useful is that one (the 'imbalance toward complexity of the simplicity/complexity variable') can be identified from the other (the usefulness of 'cross-interface' structures) as they reflect the information contained in the other
        - similarly, matching an insight like the 'certainty/uncertainty' structure as a tool for problem-solving ('find a certainty to use as a base, then apply changes to that base to find a more optimal structure') with a graph is a useful set of structures to find bc applying various insights as certainties determines possible graph structures, which can then be usefully varied to resolve the remaining uncertainties, such as how applying certainties like 'interactions are best modeled with graphs' and 'some optimal interactions are known in graph format, such as surrounding every mad person with people theyre unlikely to be mad at' is more possible once a graph is found that allows these interactions to be accurately modeled and applied, such as a state sequence of a 'graph of interactions' to switch the position of every mad person to be surrounded with non-mad people (you know that you want to find that specific more optimal interaction type, you know that interactions can be modeled with graphs, such as where every node is an individual agent in a system, and you know some interactions increase the madness of people directed at specific individuals, so you can find a graph format/type within the limits created by those applied certainties, that fulfills various useful functions and allows useful variables to vary rather than applying them as certainties of the graph), as a way of organizing groups to avoid/create various interaction types, such as justice/peace graphs to determine who should be held accountable in order to achieve a peaceful state, team-selecting graphs to avoid or maximize arguments, etc
        - this means the 'certainty/uncertainty' variable is useful to apply with the 'standard/base-change' problem-solving structure (associated with the 'change a base solution' workflow) to find useful structures (like common structures such as graphs/networks) associated with those certainties and the uncertainties resolved with those structures (graphs)
        - similarly, the other interface variables ('abstract/specific', 'similar/different') can be applied in a similar way, to resolve core problem-solving workflow functions/structures (such as how 'similarities/differences' are useful for the 'connect problem/solution structures' workflow, and the 'abstract/specific' structures are particularly useful for the 'generate/filter' workflow)
        - relatedly, one item in these spectrum variables is often usable to solve problems (using only abstractions vs. using both abstractions/specifications) but using both sides of the spectrum is more optimal in many cases, like where a spectrum like 'unit/extreme' (or relatedly 'finite/infinite') is useful to identify interface structures like 'limits' of a particular function observed in a unit/simple/definitive case by applying both the unit/extreme rather than either variable
        - similarly, identifying errors of thinking such as 'the most recently heard point often convinces people' (the recency and simplicity bias) reveals other info, like that its important to be able to consider multiple points at once to resolve a complex problem, and thinking about one point at a time will just make it seem true, so multiple structures (a network of many nodes and many networks and many connections between networks, etc) are better than unit/simpler structures to handle this error
        - similarly, the 'first loud/confident point heard' often convinces people more than other points (bias toward certainty structures like confident, bias toward simple structures like 'first'), which reveals other info, like that an algorithm that identifies possible uncertainties or ways that some point could be/become incorrect is more valuable than an algorithm that states it has certainty about some point, and other info such as that 'irrelevant structures to meaning' (such as position in a sequence, like the 'first' point in an argument) may be used as proxies of thought, but can be disregarded/filtered out as possible structures of truth bc of their irrelevance (the 'firstness' of a point doesnt create 'correctness' of the point, so structures like 'firstness' can be disregarded as structures of relevance/meaning, even if theyre often misused as relevance/meaning through errors of bias like the simplicty bias)
        - relatedly, the 'determining' variables are maximally differentiating variables, which are useful in that they can act like filters, or alternates of sequential structures like input/output sequences or other interactive structures, since 'determining' variables have an impact of differentiating related structures (such as making other structures required or associated), so these can be used to determine subsets of the space in between problem/solution structures (where rather than finding connections or input/output sequences, find highly determining variables and see which of them apply or are useful or see how to connect those)

    - identifying useful structures like optimization structures which are adjacent to other useful structures like optimization structures so they are useful for identifying new useful structures
        - for example, an optimization structure in the 'driving/navigation' problem space could take the form of 'taking advantage of the first green light at a required turn direction (like the first green left turn, if a left turn is required at some point and left turns will have a similar effect on removing the left turn requirement, no matter at what position theyre applied)', which is an optimization bc of the improbability that any other left turn will have a similarly zero wait-time associated with it, which is a cost structure of most types of turn, and bc the effects are the same as another left turn and it fulfills a requirement for a left turn
        - so identifying 'equivalent alternates' (equivalent left turns that will not change the route too much or which have similar effects on the route and/or requirements) and 'improbabilities of finding lower cost required structures' are useful structures to identify in this and other problem spaces, the 'improbability of a lower cost required structure' being a new optimization structure that is trivially identified from this optimization structure of a 'more optimal route' (especially useful when connected to the case of the more optimal route, as its improbable bc of the 'case of a few more possible left turns, in which a better/lower/equivalent cost structure is not likely, except at prohibited high speeds', and these cases with more optimal routes can be identified automatically, and improbabilities of more optimal routes than a specific optimal route in that case can be determined with these cases, and that is useful bc these determinable cases offer structure around which its useful to identify interface structures related to those cases and then its possible to connect cases with other cases as well as other useful structures like probabilities/optimizations in certain cases as well as general case-invariant optimizations)
        - similarly, 'useful structures' are often 'adjacent combinations of existing structures' bc existing structures reflect the information that if it exists, it must be useful to some agent and it must be similar to what agents have intent to build/find, otherwise it likely wouldnt exist, and therefore many useful structures are adjacent to these existing (and therefore preferred) structures

    - identify useful symmetries of information that can be applied to find 'rules to find information' such as 'find recent information, as its likely to still be true, to some degree or in some ways/cases/structures, bc of the "temporary time symmetry of information"', where 'recent information' is a proxy for 'current information', although this is an imperfect approximation of current information as theyre not required to be equivalent but rather often have similarities/symmetries maintaining some of the information in some way/degree/structure/case
        - for example, there is a 'temporary time symmetry of information', where past information is often still 'measurable in and connectible to the present', bc the original past state of the information is maintained to some degree or in some way, as the information may not have completely decayed yet, so this symmetry can be applied to identify present/future truths to some degree or in some ways or in some cases, where past information is reflected across this time symmetry to the present, even if incompletely or approximately rather than in its original form

    - identify proof structures (and related structures like opposites of it, such as non-provable structures) as useful structures to apply in functions that determine facts or probabilities, which are adjacent to problem-solving structures in general, as facts can be used to fulfill problem-solving intents like 'connect problem/solution structures'
        - for example, structures that determine non-provable structures include differences from provable/existing structures, such as differences from replicable/measurable/existing/scalable structures (non-provable structures are structures beyond current scalable limits of computation)
        - similarly, 'differences from the fact to be proved' determine non-provable structures, such as differences from its inputs, barriers of non-computability surrounding it, differences from the fact (contradictions, incentives to contradict it, relative commonness/measurability of contradictions of the fact as opposed to the fact, triviality of faking contradictions of the fact, etc), differences from existing concepts/structures to understand/determine it, etc, where if too many of these differences from the fact exist, the fact can never be proved

    - identify useful structures like 'inputs to useful structures such as filters' like the examples/problems that trigger identifying useful filters and the resulting realizations of applying those filters in those problems and the useful connections like maximal differences made trivial by those filters and the reasons for the usefulness of those filters (like that 'combinations of some structures can create reality-covering variables', so these combinations are useful to identify) to identify other useful structures (like the 'combinations of structures which create reality-covering variables')
        - for example, the function 'realize' is a matter of identifying a structure related to the newly identified structure, created from adjacent changes applied to existing known structures (or more rarely, adjacent changes applied at scale to approach the realization from a new direction, or non-adjacent changes like interface structures), so first 'finding the structures that can adjacently be connected to the new structure and then applying the right changes to connect it' are both functions that form a realization, which is nontrivial in that it requires focusing on some relevant subset, then forming relevant connections to some new structure, so 'realize' is like 'applying structure to an adjacent degree from some starting point on the consolidated function diagram or other interface diagram'
        - just like 'knowing that people lie and focusing on that will make it trivial to realize facts like when someone is lying' and 'knowing that its possible to be good will make it trivial to realize facts like when someone is good' and generating both of these by applying opposites is required to realize either of these facts
        - also relatedly just like how knowing that graphs have different similarity/position metrics makes it trivial to identify whether a new graph is actually new, as this fits the definition of a 'new example graph' with the definitions of existing/known graphs in a trivial way, making it trivial to realize whether a graph is new)
        - these mental functions of intelligence/thinking are useful as outer layers of the consolidated function diagram (with inner layers like core functions such as find/build and outer layers like useful high variation functions like organize/implement), as functions that can be built with the inner layers
        - connecting these mental functions of intelligence/thinking to 'structure' using the interface structures defined in that structure interface and often some other interface structure like a function diagram is required in order to identify useful implementations of the function ('realize' is usefully implemented when formatted with interface structures)
        - similar to how 'realizing the idea of capitalism (without already knowing it) while working as a checkout clerk' is not trivial, 'realizing interface analysis while working in software engineering' is not trivial, but some perspective make these realizations trivial, especially 'formatting new structures in the structure interface', which is a useful step in the 'realize' function (just like 'identifying maximally different filters (like good/evil) to generate different starting points to focus on' is a useful first step)
        - similarly, 'sets of graphs/functions like cross-interface or cross-perspective structures that cover reality when combined/changed/otherwise interactive' are useful alternates to reality-covering variables (such as power/balance/truth) which are maximally useful on their own, so these 'combinable/interactive sets' are useful to identify as 'positions to apply variables' or 'useful filters of solutions to check or solution-finding methods to apply'
        - reality-covering attributes are useful to identify (and build with other attributes) bc you technically only need one to solve all problems (these reality-covering attributes are the bases of the primary interfaces), similarly its useful to identify approximations of these ('incentives' could be an approximation of a reality-covering attribute even if it doesnt cover every last point of reality, it covers most points, as there are agents in or adjacent to/within accessible distance of most points and incentives exist when there are agents)

    - identify common variables of useful structures that can be used to identify useful interface structure interactions, which can be used with higher priority in interface queries (to filter probably useful interface queries)
        - for example, how 'standardize, then compare' and 'sort, then filter' and 'generate, then filter' and 'understand, then explain' and 'organize, then understand' and 'identify example solution, then change/optimize it' and 'define, then connect/differentiate' and 'classify/sort/separate, then combine/mix/vary' are examples of 'coordinating useful interface structure sets' that have a 'optimal sequence' structure and where these sub-structures have specific useful interactions, like how some specific sorts are useful for some filters, to apply as a filter of interface queries that are likelier to be useful, so identifying structures with 'identifiable optimal structures like sequences' is useful for solving the 'filtering' problem in the 'input/output sequence' format to fulfill the 'connect problem/solution' workflow
        - similarly, specific variants of these functions are useful, such as 'identify the unit/component "case/variable/error" that scales to create the problem and solve the unit case/variable/error' and 'identify common combinations of problems/workflows/formats and identify useful structures that solve for that combination', which are general useful problem-solving intents
        - similarly, identify structures that are not useful to differentiate from when filtering interface queries and functions which often decrease the value of an interface query when applied, such as tautologies ('falsely framing an equivalence as a difference') or 'randomly removing a sub-function or randomly adding variables/structures outside of the defined variables/structures'
            - similarly, a simple function like 'list examples of a type' is not likely to be useful in isolation, usually a mix of structures like this is required bc usually one function is not high-variation enough to solve a problem on its own, with rare exceptions like organize/understand, which are not adjacently coded and which dont completely solve most problems in their probably incomplete probable implementations given the 'adjacent combination-prioritizing' mind likely to implement it
            - this is bc a simple function is different from the general requirements of a solution structure (often requiring complexity or high variation in some other format)
        - to identify this insight, apply the workflow 'identify useful structures of useful structures' (identify 'common variables like sequences of useful functions in a set of opposing useful functions and the reasons for the usefulness such as that they apply the 'differentiate a similarity' workflow or capture high variation or are required to be useful when applied in that structure, given their interaction and input/output requirements) and then apply useful functions that frequently solve problems like 'identify maximally different examples of a type (to identify the extremes/limits of variation within a type and identify variables of variants within a type)'
        - identifying function limits is similarly useful as identifying the unit case that scales to explain a variable interaction (if there is one, which there usually is), such as how identifying that 'combining the same 10 integers (to create integer sequences, as opposed to combining sequences of digits of size greater than one, such as by randomly varying between randomly distributed sequences)' is useful for identifying a high ratio of random sequences up to a finite sequence length, but not infinitely, as there is a limit on the number of infinite random sequences that a particular infinite random sequence can contain, but sequences of lower length are likelier to be covered by any given infinite random sequence, and identifying this finite limit is useful, just like identifying that the set of 10 digits can be re-combined to generate most other random sequences is useful (but not infinitely bc once an 'infinite random sequence' starts to be generated, other infinite random sequences are not possible, down to the finite limit), similarly identifying the 'unit components that scale to create extremes' and the 'interim structures of extremes' is useful in other problems (interim structures such as the 'finite limit on a specific sequence length that is required/possible to be contained in a particular infinite sequence' and the 'variation in probability distributions of sub-sequences of unit components/digits that are found across sub-sequences, these variable probability distributions being equally probable'), as extreme differences are likelier to be a problem that needs to be resolved, so 'identifying interim structures between extremes' is generally useful

    - identify useful structures like system contexts (like cases where the info structure is more useful) that coordinate with info structures (like unused variable interactions) to optimize a neural network
        - for example, some variable interaction that deviates from the standard neural network structure of an 'input/output sequence applying an adjacent change combination' is where an output could be useful as an input earlier in the sequence, but that info isnt applied in a standard neural network architecture/algorithm, so 'allowing earlier computations to skip ahead and use later outputs as inputs' is a possibly useful function in systems with enforced boundaries that are likely to force a high ratio of possible variable interactions to occur bc of these extreme limits (this is useful during later iterations of training, after likelier weights are likely to already be found, so that these likely true later weights can be used as inputs, similar to how the 'system state' where a variable interaction occurs is also an input to every function while being changed by that function)
        - similarly, keeping variants of a neural network that were 'almost correct' or 'nodes that were just barely deactivated' could be useful bc these are likelier than not to be useful for some other intent as they are probably correct in some other way than for predicting this particular variable interaction (as in a probable variable interaction that could explain some other connection in the host system) and are likelier than not to be variants of the solution function

    - identify insights which indicate useful structures like simplicities/opposites/errors/differences which are easily converted into or connected with some other useful insight
        - for example, an insight that indicates an opposite or error like 'correlation does not mean causation', which is easily converted into another insight ('but then what numbers do build or map to structures of cause or cause itself, bc there is a connection between cause and numbers, even if there is high variation within that connection?')
            - numbers that indicate precedence/sequence (like numbers in a pattern), numbers that indicate differences from other useful structures like concepts (different from random, unit, etc), numbers that indicate organization/randomness/units/types/other interface structures like other concepts (also indicating patterns) numbers that are unique (building a requirement, which is one input/component of causation), numbers that are useful (such as 0, 1, pi, e, 6, etc which are more causative as they are more useful than other numbers), numbers that indicate other useful structures like embeddings such as embedded variables like variable exponents, etc
            - similarly, numbers which are continuously connectible, adjacent in position, etc are likelier to be causative of numbers on that continuity or adjacent to that number in some space or set/network of spaces, just like numbers of the same type are likelier than other numbers to cause numbers of that type, given the frequency/usefulness of type-preservation functions
        - in general all numbers are causative (and otherwise also fulfill the other primary interface and reality-covering concepts), but some are more powerful, required, unique, useful, organizing, or otherwise causative than others, as some numbers are more easily used to implement a concept or make a useful function adjacent or otherwise more easily connected to some interface structure (in general and also specifically to causal structures like causal components/inputs), and interface structures in general are also adjacently connectible to cause
        - numbers are 'structures that can describe/measure/track some example/application of some interface structure, and can be re-used across examples/applications of other interface structures, to act as placeholders for interface structures to indicate the relative position between interface structure examples/applications as well as the structures themselves, as an alternate set of structures that can describe all changes/structures/concepts, and indicate spaces between interface structures, when no interface structures can be used to describe some number bc its not completely understood yet, bc some number represents a new difference/variable not fully connected/fit to interface structures yet'
            - interface structures, by comparison, are often stacks/networks of numbers, which become so variable through supporting high variation that they seem to not be easily mapped to numbers at all
        - neural networks apply concepts like 'causal input/component' through embedding that concept in the structure implicitly, rather than explicitly giving these interface structure definitions to a neural network as inputs or functions it can use in its own processing
        - therefore neural networks rarely create any interface structures that are not trivially composable with adjacent/incremental iterated change combinations (such as maps/indexes/networks, rather than efficiently and optimally building concepts like balance/truth/certainty/cause)
        - similarly, the fact that 'correlation is not causation' can be applied to generate other useful structures, such as generating possible solution sets by generating undirected variable interaction structures rather than keeping tha variable value constant

    - identify useful structures like common inputs to useful structures, such as info that (once known) is useful to identify other useful structures in multiple ways, such as knowing 'negative states/processes' which is useful for adjacently identifying multiple other useful structures (changes to negative states, causes of negative states, opposites of causes of negative states, opposites of negative states, etc)
        - for example, useful structures in the 'find a medicine' problem space include 'opposites of causes of negative states (errors)', which requires knowing 'what the negative states are (inflammation, oxidation)' and 'causes of those negative states (reactive oxygen species)' and 'opposites of causes of those negative states (anti-inflammatory substances, antioxidants)'
            - without the info about the negative states, causes, and opposites, its not trivial to identify antioxidants as useful, but with that info, it is trivial
            - identifying oxidation as an important variable/function to negate is a matter of identifying variables which can cause high variation (such as causing damage/change to a high ratio of compounds or highly binding compounds)
        - similarly, identifying that 'sweetness/bitterness' as a useful variable is trivial once the info about the 'connection between sweetness and cell growth' is known and once its known that 'unrestricted cell growth is an error', so that the 'opposite of the error-cause' (bitterness) can be applied as a possibly useful variable of solution structures (useful compounds are often bitter, indicating a possible difference from growth-promoting compounds like sugar)
        - without this info about the 'opposite of the error cause', what else could have identified 'bitter compounds' or 'antioxidants' as possibly useful?
            - structures like defaults and overlaps and limits on types are useful in determining some structures in this problem space 
                - overlap: useful and harmful compounds overlap in some attributes like sweetness/bitterness, even though the ratio of bitter compounds compared to all useful compounds favors bitterness over sweetness, and similarly other simple interface structures (like extremes) are often harmful in general
                - type limits: there are often clearly extreme effects of most compounds, as its rare to have a neutral compound with no noticeable effects on a system, so its usually possible to sort compounds into either useful/harmful, as theyre usually clearly useful or harmful, and the boundaries of these types should be possible to find (boundaries which are often dose-dependent as there are often multiple modalities and phases and conditions of usefulness in various contexts, rather than one simple type per compound)
                    - given that there are multiple ways for a compound to be both useful/harmful, finding the 'networks of usefulness/harmfulness of a compound' (including the 'paths to usefulness' and the 'paths to harmfulness' and the 'connections between harmful and useful paths/positions' and the 'default/common/final/static position on these paths, given common processes/usages/interactions/states') is likelier to be a useful way to describe a compound than by its structures in isolation of other variables
                - opposites/inputs/causes of common/default/high variation attributes/processes: oxidation is like methylation in that its a core process in bio systems and is therefore important by default, in addition to also creating high variation in the system, so finding substances that trigger it or counter it is useful by default
                - 'causes of the opposite of the error/negative state' (causes of 'negation/destruction/neutralization' of a negative compound/state like 'compounds that destroy alcohol/aldehyde') are similarly useful to identify as 'opposites of the error cause'
                - static-triggering/over-limiting states which regulate/prevent other types of changes or prevent changes in a direction of usefulness or limit variability that is possibly useful or at least not definitely negative in other cases (like strong bonds that prevent breaking down a harmful substance, calcifications, fibrosis, tumors, blockages, clots, ph/electrolyte disruption cascades/triggers/compoundings, etc) are similarly generally useful to identify and differentiate from useful regulating/limiting structures/processes (like anti-inflammatory structures), as its unlikely to be useful to be trapped in any given particular state, as optimal states are rare in bio systems, so cascades/traps/triggers leading to static states are likelier to be harmful than useful
                - structures that change a harmful structure at all (but also in a specifically useful direction, like providing an electron to common negative compounds to correct blood ph or change a compound to a less harmful compound, like antioxidants do) are similarly useful to identify, once harmful structures are known
                - common structures like benzene rings are often found in other compounds in a harmless position (when surrounded by other structures, these are not always as harmful as they have the potential to be in other contexts), so these common structures arent necessarily a clear indication of usefulness/harmfulness and the interactive sub-structures on the surface can be more useful/harmful than the base structures in the center, so much that these can sometimes be ignored, unless the outer structures are often removed/destroyed/changed to make those base structures possibly interactive
                - useful structures like anti-inflammatory substances are often high variation in their sub-structures (and often have other attributes/compounds in common like forming structures which could be highly interactive with harmful structures, like structures that can standardize a surface of a compound by binding with/trapping structures likely to be on its surface, which could be useful for anti-inflammatory substances) but are also extremely high variation within the class of useful compounds, which is predictable given the variability of the set of all compounds of a certain size
                    - similarly, the volatility of the useful/harmful types is similarly predictable with its structure in isolation of other variables, as trivial changes can change a neutral substance into a harmful or useful substance, so structure on its own (in absence of insights about harmful processes like oxidation) is not as useful as structure with information about interaction/harmful/other types of functions
                - similarly, co-occurring processes/attributes ('bitterness' occurring in 'surviving/successful' systems to protect itself from being over-eaten out of existence) is an indication of a bio-system that can adapt to the environment, this variation being likely to be useful in other ways given the priority of 'efficiency' that can be implemented with structures like 'multi-functional attributes' (such as having antimicrobial attributes or regulating/limiting effects associated with the bitterness)
                - input/output sequences or interactivities can also be useful, such as by knowing whether a compound is similar to existing bio-system structures (like whether a compound mimics a bio-system receptor), bc if a compound is replacing, preventing, or over-using functionality of the bio-system, that would likely already be known bc it would have extremely noticeable or negative effects
                - similarly, 'bitterness' is an attribute of common/core bio-system compounds (like amino acids), so its not an indication of an error structure to be avoided at all costs in all cases

    - identify useful structures like 'interface structures that can be applied as similarity indexes' which can connect other structures that are useful to connect (like concepts and problem formats) and variables of them (like 'information retained in the similarity structure', such as proximity/position/distance/etc)
        - for example, structures like sets (to identify co-occurring structures), intersections (to identify functions with structures like points in common and other useful structures like ratios of intersections), adjacencies (to identify structures that will probably change into each other bc of their proximity in some similarity metric), alternates (to identify ambiguities to resolve, and equivalent alternates, and useful selection functions), overlaps (to identify structures like points that can be used to switch to other paths), identifiers (like 'moments' which are unique across similar functions as a way of determining function differences), similarities (to form similarity indexes to find similar functions and skip computations), & concepts (to easily decompose a complex system into a few structures) which form an alternative to more structural structures (like 'input/output sequences' or 'patterns') to determine 'interactivities' as useful structures of 'probability' (all having the same structure of similarity/difference uniting them which determines some probabilities), as alternates to apply as expansions of false conflicts like 'frequentist vs. bayesian', as a way of 'determining the next node in the sequence/network' without using simple standard input/output sequences to fulfill core interaction functions like 'connect problem/solution' to identify other useful structures that can be applied as alternates of sequences
        - applying interface structures to the problem of 'connect some problem/solution structure' to fulfill a sub-intent like 'determine the next node in the sequence/network (of possible connecting paths)' is particularly useful bc of the standardized simple format and is likely to find other useful structures adjacently, which makes it a good 'interface problem' to host other problems to solve other problems in that problem, similar to how 'find a regression function' is a useful simple standard problem format that can host most other problems, as both apply the fewest biases requiring non-standard assumptions/conditions that might make it too specific to apply to other problems
        - finding the connection between 'determine the next node in the sequence/network' and 'probability' is useful to identify, as these 'similarity' structures (like intersections or adjacencies) are related to both the problem (of determining the next node) and the concept of probability
        - finding these cross-interface connections (these 'similarity indexes') between these primary interface 'concepts' and 'problems which solve for or fulfill that concept when other problems are standardized to that problem' is useful to identify other problem formats that can solve for another concept, and how to connect those with different similarity indexes
        - similarly, identifying the ways that other interface structures like 'information' and 'change' can act like 'probability' (since probability is a reality-covering concept), as information acts like a 'temporary determinant of probability, since information is often temporary' and 'change structures like degrees of change and change rate are directly related to probability such as by determining other structures that determine probability such as adjacencies/potential change', and how probability can be converted into these interface structures to form a useful queryable network with these structures
        - similarly, identifying all the independent sets of structures on a particular interface like the math interface that can determine/describe probability, such as the structure of 'variance' such as applying interface structures like 'change' and 'group' in a new way to identify 'changes across groups vs. changes within groups' or 'range/distribution of groups of changes'
        - this set of a 'false similarity' error ('correlation is not causation'), a 'dichotomy' (specification vs. generalization), and an 'equivalent alternate' set ('frequentist vs bayesian') form an almost complete set of variables that can describe many statistics variables/problems, where other problems can be adjacently derived from this set, such as how 'correlation is not causation' is an error of the false similarity of an input and a similarity where there is an implied requirement that is not reflected in real systems (a similar change structure may occur even if one input is not an input to the other change, which means other interface structures with possible false similarities through these implied but not required rules can be found as alternative statistics problems to find and resolve) - whether such a set (an error, a dichotomy, and a set of equivalent alternates) is useful for describing and deriving most complex systems is useful to identify
            - similar to how a 'change component limit' (activation), 'change unit' structure (weight indicating relative importance/usefulness of a change, given other surrounding changes), 'change change identification' structure (partial derivative), 'useful change identification' structure (error function), 'useful change limit' structure (change halting function), and 'core combination' structures ('multiple' and 'sequence' and 'overlaps' and 'direction' applied to 'probability units' to form a 'network of probability units') form a good description of a neural network, which make other useful network configurations clear, such as 'changing weight position' rather than 'changing weight value', to identify 'useful sets and sequences of weights that are repeatedly useful across intents or which can be identified as useful specializations to fulfill a specific intent, justifying its separation and also its generalization by connecting it to other specialized intents'
            - for example, 'core centralized/starting nodes are more powerful through adjacently generating higher variation of functions such as by including maximally different functions as core/starting nodes (or adjacent components or other high ratio-covering inputs of maximally different functions)' is a useful network organization function to 'quickly find higher-variation variables determining most variation in a solution function' and 'sort changes to attribute to a subsection of the network' (as well as "allocating more 'trivial change-producing' variables to the later low-variation/specification remainder of the network to stay within a pre-determined error range"), with built-in assumptions such as that any high variation function could exist and all useful high variation variables are known and can generate all possible high variation functions (like different polynomial sub-types and different function types having different configurations of discreteness/linearity and other function attributes)
            - similarly, a network organized to identify a 'function similarity index that a solution function is likely/required to exist on' first, and then sorting that index to be most useful for iterating in order/some other pattern (such as a sort that makes any iteration of the sorted index useful for some likely/common intent), and then navigating that index with that navigation function in the later nodes or later training/update iterations is a useful way to organize/apply a network
            - this is like how 'generate possible solutions (being similar in that they fulfill some more obvious solution metric)/filter (to identify solutions fulfilling some more complex solution metric)' and 'identify similarities/differences to connect useful structures (identified as similar in their usefulness so they must have other similarities like variables in common)' and 'connect structures adjacent to problem/solution (similar structures to different structures, having a similar difference that is more solvable/connectible/similar) rather than the original problem/solution (different structures)' and 'standardize/similarize then differentiate within that standard' and 'identify similarity, then sort to identify useful differences for optimal navigation' and 'organize similarities/differences so all similarities in differences and differences in similarities are adjacent on the structure' and 'change a base solution (already similar to the optimal solution)' are similar in that they apply a similarity such as a standard/interface, then identify similarities/differences in or connected to that similarity, bc its common to be able to identify a primary/powerful similarity and then need to identify specifications/variants of that similarity when solving problems, just like its common to need to 'resolve multiple ambiguously different alternates into one most optimal solution' or 'obviating differences' once similar alternates are identified as possible solutions, as these structures are all variants of each other, as similarities are useful in that they fulfill some base structure that is known to be useful (they solve some base problem of identification/filtering, in identifying that similarity, so that the structure can be re-used for other/future intents), which can be changed trivially to identify other useful structures that are variants of it and also identify structures that are different, which are core problem-solving intents
                - similarly, its common to be able to identify a 'difference required to connect in order to solve some problem', as the difference between the problem/solution is usually clear
            - pairing these 'assumptions/errors' and 'network organizations' (implementing various known useful structures to fulfill known useful problem-solving intents in the 'regression' problem space) is useful as a way of identifying useful network structures for unresolved problems in a particular workflow and similarly for identifying generally useful network organizations that fulfill a high ratio of problem-solving intents
        - similarly, identifying alternates to 'similarity' such as 'importance' (such as how a structure is more important if its a requirement or an input to multiple functions or a different structure or a stable structure or a common structure), which overlaps with similarity in its component/input/variant structures, as structures which are similar are important/relevant to each other in multiple ways (as similar structures may be alternates of each other, may be errors of each other, may be connected to each other, etc), as 'importance' structures also indicate probability in various ways (such as 'difference from random' and through 'frequency/commonness'), and similar rules apply for 'usefulness/interactivity/variation'
        - similarly, other examples of alternate structures that can determine probability or can be applied in its place include other structures of certainty, other reality-covering attributes, other interface structures like requirements and other structures that occur often for a reason like 'incentives' (such as biases which are incentivized structures)
            - 'stability' as a determining variable of the 'next node in the sequence/network' (and components of stability/independence/certainty/requirement, like self-sustaining processes, iterative/scaling processes that cause other determined processes like cascades)
            - 'variation/information requirements' as determining attributes of variation/information, such as determining where the remaining variation/information is stored (in a different/new path or node than those already determined, or in errors like noise), such as 'incremental changes required in a sequence'
                - otherwise applying specific solution format requirements like 'making sure each weight set can have noticeably different results (being sensitive to inputs)' to the network parameters like weights applied as a whole
            - 'biases/incentives' as determining variables of the 'next node in the sequence/network'
            - 'interface structures' such as 'false similarity errors', which indicate some weight subset of the network should be falsely similar to some other subset if the network is going to reflect reality and real variable interactions (making sure the network has each interface structure, whether embedded on some interaction level or represented in a default structure like a node or node subset)
            - applying 'components' of connections like maps/sequences/patterns to fulfill the 'connect' function for some input/output points to connect
            - interface-connection structures like probability-information interactions such as how probability becomes information when a difference is clear (when some structure is clearly not random, bc its difference from randomness is clear) and how an ambiguous structure becomes information (when a selection is made, differentiating it from another alternate, resolving multiple alternates into one structure) which can be used to connect multiple different structures (like probability structures such as weights and info structures like indexes/maps and logic structures like requirements) used to 'predict the next node in the sequence/network' to cover gaps that other structures cant fulfill
        - connecting all core/primary interface structures to one reality-covering concept (such as probability) is useful bc then all problems can be formatted to be problems in that problem space (a problem of determining probability of success of a solution, probability of various alternates, etc)
        - why is 'probability' useful to find all the interface structures of (such as components/variants of probability)? 
            - bc it is a useful structure for solving the core 'filter' problem, to resolve the ambiguity of multiple alternates, by weighting one path as more probable than another, and therefore 'finding probable paths to connect structures (like connecting a problem/solution)' is solved once these probabilities/weights are identified
            - similarly, 'connection' structures of 'important' structures (like how differences/variables are important, which can also represent filters/errors/connections/similarities) so that structures like a 'network of particularly important (as in maximal) structures of importance (like differences)' is particularly useful for solving problems with queries on that network
                - relatedly, core interface structures like 'networks' with other core interface structures like 'sequences' applied (such as a structure like a 'network' sorted in a sequence of a reality-covering concept like 'importance', like a 'decision tree') are useful for implementing other structures that can be used to solve all problems bc of the fundamental aspect of the structures applied

    - identify useful structures like 'unit/core problem formats' to fulfill intents like 'find variables of problem formats' and 'find connections between problem formats' (to make these formats more useful in a network, at which point interface queries can use that network)
        - identify core problem formats like 'ambiguity resolutions' (filter a set of alternates) and 'scaling' (applied in its 'opposite' structure, to find the 'most compressing units/inputs (such as a useful high variation interface/standards) of an iterated or otherwise complex structure' as opposed to the 'most efficient iterated structures') as alternate problems which are useful to solve in order to solve other problems, as they involve core formats of problems and are useful when they are applied with other interface structures, like in their reverse, with other workflows like reverse-engineering or to fulfill problem-solving intents like 'connect opposites/extremes (so that connecting trivial differences is already solved or easier)'
        - applying variables to these core problem formats is useful, such as how another useful problem format is the problem of 'finding a way for alternates to all be useful or all coordinate (to avoid solving the filtering problem)'
        - identify the connections between problem formats is useful to find useful interface queries for intents like 'filter which problem to solve' and 'find a useful problem format given these inputs'
        - similarly, 'making some structure like "problem formats" useful by finding the network connecting them so that network can be queried' is like identifying 'interaction levels' of 'equivalent alternates' so that these can be applied as 'random enhancers of algorithms' (as any structure in the interaction level is likely to be useful, even if selected and applied at random, similar to how applying any function out of the set of 'find/build/derive/apply' is likely to be useful)

    - identify structures in common across useful structures such as 'neural networks' which can identify 'adjacent combinations/variations' (like interactions between workflows) that are similarly useful as the first example found
        - for example, the 'neural network' structure is a result of applying 'change a base solution' with 'trial and error', to feed the base solution as an input to 'trial and error' to find the next solution to try, based on an update function applied to the previous solution tried which was suboptimal, using some function to determine a halting point (such as the first found local minima), after applying the 'reverse-engineer' workflow to identify the changes responsible for a change in the error
        - 'trial and error' may be a higher-variation workflow (and therefore usable as a base for other workflows, more so than another workflow) in that it tries every possible solution in its default form, and in that it allows various points for variables to be applied, such as to 'generate solutions' (build), 'filter solutions' (filter), 'sort solutions to try more probable solutions first' (sequence), 'halt testing solutions' (limit), and therefore interacts with most if not all core interface structures in its most complete/optimal form
        - 'trial and error' is useful in problems bc it allows for a set to be reduced to a subset, which is useful when multiple possible solutions are known and the uncertainty to resolve is 'finding a selection of a subset of those alternates'
        - the 'neural network' applies this set-reducing workflow ('set-reducing/filtering' being the primary function of many workflows in their default format) of 'trial and error' in multiple places to resolve multiple uncertainties - one to assign default weights, one to try many different change combinations (either by applying many nodes or using another structure like a variable interaction function like 'adjacent input convolutions') & one to test whether a final weight result should be changed in some way
        - 'trial and error' also has clear optimizations in the 'filter' function variable position of the workflow (which is optional) to filter the solution set, like finding solutions with overlaps, finding generative functions of solutions with overlaps, finding solutions with similar results or similar components/inputs, finding areas/other patterns of solution similarity, solutions optimized for some input case attribute, etc
        - from this it can be inferred that most useful inventions apply some 'interaction between workflows', where the workflows solve different problems to solve the primary problem motivating the invention, and its usually different workflows that are combined, rather than iterations of the same workflow, but iterations of the same workflow can also be useful (like when alternates are ambiguously useful and should all be tried or when theyve been filtered as much as known certainties allow), similar to how 'trial and error' is applied in multiple positions in the standard neural network bc of its focus on computation rather than efficiency
        - then after this initial version which solves multiple problems is found, this initial solution version formed by these workflow interactions is often tuned to account for a new structure that can be applied (such as an insight) to solve some problem found during testing (such as 'random dropout' helping with 'generalization' to solve the 'over-specification' problem ("randomness" again being related to "trial and error"), which adds structure to the network to format that certainty (that an 'average of subset lines, with subsets created by all adjacent combinations of size, as in one less than the full set' is a useful representation of the 'full set line' in the 'regression' problem space) and integrate it in the information input/output sequence) in a structure like 'create subsets (like with "random dropout") to fulfill intents like "generalize"', which can be predicted by connecting that initial solution version with reality-covering attributes (like generalization, efficiency, possibility, etc) by applying functions such as 'random filters' to the input/output sequences of the network, so that certainties/insights about 'variable/weight-update limits' from related problem spaces (like 'regression') can be applied to the 'network' structures to change the possibilities/variables of the 'output function' structures, when the info certainties have already been applied to pre-process the data or pre-set the weights

    - identify useful sets of problem spaces like the 'regression' and 'optimize' and 'differentiate' and 'scale' problem spaces, which can produce useful structures/functions to apply in other problem spaces, bc of the generality of the problem space and the intersection that it occupies (like at an 'intersection' of a core structure like 'iterate/repeat' and a 'trade-off with a cost at each iteration, such as the opportunity to invest in a test of a different change at scale or a different scaling method or the inputs required to iterate', which creates the 'scaling' problem, as in its a problem bc there is some cost/input that isnt optimized yet, so its too expensive to repeat enough times to be usable for some intent)
        - for example, alternate problem spaces requiring functions like 'differentiate', 'scale', 'optimize', etc require sub-functions that fulfill those intents and are useful for related intents (a function that solves the 'scaling' problem for some specific problem space is good at finding interface structures like limits, thresholds, specifications, new variables, new errors, new interaction levels, etc) so solving the 'scaling' problem can produce structures useful for other related intents or examples of these interface structures, where solving the 'scaling' problem also solves for attributes like specificity which are useful in other problem spaces like regression
        - solving general problems involving differences between interface structures (like 'scale' or 'optimize' or 'connect/differentiate') tend to be useful for solving other problems, similar to how 'organize' and 'store' and 'obfuscate' and 'scale' are similar alternative problems, whereas solutions in the 'scaling' problem space might not be useful for finding more optimal unit/default structures to apply at scale, more adjacent optimal structures in a different direction than the scaled directions of change, or predicting the output (like a limit or interaction level) of a scaled structure, and the certainty produced by a scaling optimization algorithm might be over-used which would create errors, if applied in a sufficiently different system
        - finding these connections between problem spaces like scaling -> specificity -> regression can be useful for finding opposites (finding the specific/general dichotomy is adjacent to connecting those problem spaces), where finding 'reasons not to specify' is adjacent to identifying the structure of 'alternate' applied to 'inputs' (as in 'alternate data sets having some ratio/other structure of similarities in common')
        - solving the 'scaling' problem tends to have more similar solution structures than other problem spaces (the problem of 'finding cost-minimization structures like shorter paths to remove costs that act as barriers to scaling'), like the problem of 'organizing information' (the problem of 'finding a network/distance/position/node mapping to allow all info to be found with queries on the network'), so 'scaling' solutions are likelier to cover a subset than the entire set of all problems when those solutions are mapped to other systems, but they are clearly related (the problem of 'finding the shortest path on a network' is related to the problem of 'finding a network that allows short queries to be run to solve all info problems', where one is a subset/specification of the other, which is made clear by standardizing the problems to a network format)
        - similarly, 'regression' can involve structures as different as 'similarity indexes', 'filters as differentiators of functions having some other similarity', 'maximally different base functions', 'summaries/representations', 'local average/subset selection functions', 'intersections', 'primary exponents', and 'densities/limits/inflection points', which indicates its a high-variation problem space whose solutions can cover a higher ratio of all problems when those solution structures are mapped to other problem spaces, and can identify interface structures by being connected to other high variation problem spaces
        - similarly, its useful to identify maximal differences to skip scaling structures, such as maximally differentiating functions like 'oppose the angle, such as change the sign of the angle' (to test for a corrective angle difference or an angle change, when some angle is above a ratio of incorrect), 'differentiate' (to apply complexity), 'parallelize' (so that both adjacent/non-adjacent errors are constant as in equidistant vertically/horizontally/diagonally/similarly, as once errors are constant, youve basically solved for the regression function, similar to how once errors are small enough, youve basically solved for the 'regression' function and how once youve solved the unit variant of a function youve basically solved the function, as small and constant and similar in shape (the unit variant) are variants of 'trivial to similarize, as in adjacent', and similarly 'rotations' are basically solutions of the regression function such as how a rotated wave, similar to a shifted wave, has error patterns like having exactly wrong solutions at adjacent subsets, and similarly once youve solved for the primary exponents or densities, youve basically solved for the regression function, as the highest variation variables are similar to the actual solution function), and other functions which are useful for finding different solution functions, as a way of finding alternate directions of change or solution base functions to apply as inputs to a neural network/regression algorithm, after first finding a base function to check first to base these changes on, as possible useful different functions to split into different networks as default parameters, such as by randomly positioning parameters of the base function in the network, to increase the likelihood that adjacent changes in one of these different networks will find an optimal (first finding a set of inputs/outputs for the function and the error for some subset of the inputs, then using that info to filter the remaining possible error functions), and as an alternative to an incremental parameter change to test for a 'direction of optimality', when in reality, 'areas of optimality' are likelier to occur in both directions (and similarly, its useful to find similarity indexes to filter the solution space of error functions by which functions can have overlapping subsets for which ratios of the range)
        - relatedly, 'scaling' is a problem of 'how to make a variable value extreme, without disrupting the system in which it occurs (while other variables are being scaled in all probability)' and 'scaling' solutions often involve other related problem space structures like 'organization' (such as 'distributing cost among many sub-functions to lower cost through specialization, such as mass production')
        - identify other useful problems/solutions is a matter of finding ranges where these problems exist, such as how 'scaling' (applied at its most extreme scale) can compute every function similarity, so AI is unnecessary at that point, since 'mapping any set of input variables to any set of output variables' and 'mapping that input/output function to an error function' will both be computable in a reasonable time, so intents like 'finding the variables that are adjacently connectible or constantly connectible to the outputs' will be trivial and similarly 'finding the input/output function-changing function that creates the most traversible/optimizable error function' will also be trivial, so in between that extremely scaled range of scaling solution structures, there are algorithms to make these intents trivial with less info (such as 'average' or 'type' or 'limit' info, which is different from the 'full set of input/output connections or probable solution functions within some error range' but which is still relevant info to the data set in some way, so its likely to be useful as an input to an 'interim' algorithm that doesnt compute every possibility)
        - the connection between 'scaling' and 'trial and error' is obvious ('scaling' solutions enable 'trial and error'), and similarly other problem spaces have solution structures that make other workflows trivial or otherwise align with other workflows ('quantum computers' are useful for 'scaling' as well as 'parallelism', so they are even more useful for 'investing in multiple directions of change to some ratio of the full possible range in that direction, as in the ratio up to the point where one investment pays off to find some optimal structure')
        - connecting a problem space like 'scaling' to workflows (like trial and error), interface structures (like variables/networks/concepts), and other problem spaces (like regression or cost/benefit/incentive analysis) is useful, just like 'connecting other interface structures' is useful
        - future computation will allow 'scaled thinking' to reach understanding, so that all variable interactions, variants, alternate definitions, limits, and other interface structures of a particular concept will be understood (meaning 'in working memory while conversing') as opposed to just using a reference/shortcut to the understanding (like calling 'scaling', 'iteration' or 'efficiency' instead, as its clearer/more structural than using 'scaling', rather than thinking through all the variable interactions of scaling and its alternate definitions and other interface structures, to find the emergent meaning or understanding of the concept in that context on outer interaction levels)

    - apply workflows to fulfill problem-solving intents like 'find variables in common across different structures to find other variants of the structure' to other workflows that are particularly useful such as being 'maximally different' which means connecting them is a particularly useful intent to fulfill and may fulfill other problem-solving intents like 'identify all items in the set of high variation/complex variable connections'
        - for example, workflows like 'identify workflows used to create existing structures (like how trial and error is used to create a useful structure of a convolution, which is an application/integration structure of applying one structure to another to find all possible interactions)' and 'identify truth/falsehood structures like interaction variables such as how a statement is often both true/false in some way to some degree' are very different but similarly useful in this difference (its useful to think about truth/falsehood structures and workflows that create useful functions/attributes, rather than either workflow's structures in isolation)
        - identifying 'workflows successfully applied to create useful structures' and 'reality-covering attributes like true/false and their interactions' are connections between useful (high-variation) structures (workflows, reality-covering attributes) on different interfaces (the 'usage/function' interface and the 'concept/info' interface), which is all it takes to fulfill some subset of problem-solving intents (problems are often a matter of 'finding a connection between different structures')
        - similarly, 'iterate through all problem-solving intents, finding structures that fulfill multiple intent sets that have some other useful attribute like being maximally different intents' is similarly useful as a workflow applied on problem/solution structures
        - similarly, applying workflows to some specific problems like 'neural architecture' creates useful structures adjacently even where the workflow isnt completed or successful bc of the usefulness of sub-intents or intent subsets of the workflow (such as 'multiple-intent neurons' that support different change sets, either of which is useful across problems and which can be rotated) which can be found with any workflow involving 'finding cross-interface structures or their possible new applications'
        - this is related to a workflow like 'find useful structures, then find all the problem-solving intents or useful structures nearby that could have led to those useful structures' by using workflows with useful structures (like highly differentiating attributes or reality-covering attributes, which are not unrelated, as an attribute has to be maximally different in some way like the variation it supports in order to describe reality) to connect workflows in new ways

    - identifying intents like 'identify specific useful structures like alternates' for a problem space structure such as 'universes' with associated specific structures (like 'alternate uncertainty resolutions') that can be resolved with alternate function sets ('identify variables of the current position to determine other possible positions', or 'connect it to known info', or 'specify a more testable variant' then 'find a measurable metric to test for' then 'find a test using that measurable metric')
        - for example, identifying a possible universe structure (such as a 'reason why a specific universe structure might be real') from the structure 'alternate' by identifying 'reasons why a specific structure (applied to "universes") like an "alternate" might exist', such as that an alternate universe exists to allow exploring/testing/resolving an uncertainty, just like how 'copies/equivalents allowing some degree of change' exist to allow 'resolving uncertainties between relatively similar alternatives', so now the questions are:
            - 'what uncertainty is resolvable with this universe and what uncertainty might be resolvable with other universes (what variables might differ that allow different questions to be answered)' (if true, what specific variable values are relevant to this universe)
                - 'specify it so its testable'
            - 'can any of those "possible universe structures" (or related structures like "uncertainties resolvable with universe structures") be mapped to known quantities/formulas' (if true, is it relevant in the sense of being adjacently connectible to known certainties like known information, like is there a formula that could explain some information which could indicate that this specific universe structure or uncertainty is a legitimate possibility)
                - 'connect it with known information'
            - 'can a test be identified to gather info about a structure like that, if it happens to be definitely true/false' (if true, can it be relevant in the sense of being proved/tested in some way)
                - 'identify a test that differentiates the input (of the specific universe structure or related structures) into a relevant output (a measurable quantity)'
        - you can see how common simple functions are applied to solve the problem which would apply to other problems, and see the connection between these common functions (like 'specify an abstract structure so that its testable, then apply tests', which connects common functions in an input/output sequence)
        - identifying useful specifications and variants of these simple/general function sets is similarly useful, like how identifying related sub-intent of an intent is useful
        - how is this different from other workflows that use an 'input/output sequence' to connect structures like 'function sets' to some intent? 
            - its connecting the common core structure that is an 'alternate' structure with other useful structures like 'reasons for a specific structure, like an alternate' (to resolve different questions, a function that a universe might have, as other structures have that metadata as well, structures tend to be 'useful to resolve some problem') and then connecting those reasons/causes with other interface structures like 'function sets to implement a particular reason why a structure might exist', since 'reasons' are so highly determining/predictive of function/structure, since the structure/reason stores so much possible variation in how they could be related, so solving the problem of connecting them is useful and contains a high probability of finding a new connection given that high variation
            - this is useful to apply to universes bc knowing whether there is a 'variable that can be varied to find the other possible universes' is useful to 'find the other possible universes', 'find out if theyre likely to still be accessible or if the core uncertainty is resolved', 'find out if another universe can be triggered by an uncertainty in a different universe' and 'find out if there is an optimal sequence/network/combination of uncertainty resolutions that should be applied in that sequence/network/combination', all derived adjacently just from applying a 'reason' to the structure of an 'alternate', and finding related structures like 'specific variants of structures' and 'tests' connected to that reason
            - applying simple/core functions defined in interface analysis can be used to connect the 'specific universe alternate' and the 'possible reason for it' with other relevant info, such as by identifying info structures that coordinate (like a 'specification of some structure' and a 'test to differentiate some determining variable', where both the specification and the determining variable act like filters of the set of structures that are possibly relevant) to filter the set of possible connections between the 'specific universe alternate' and the 'possible reason for it' and connect the 'possible reason for it' with 'reality (known certainties/information)', applying the 'possible reason for it' as a 'connection' structure (to apply interface analysis to fulfill and connect with other structures), as once the 'specific universe alternate' is connected to at least one other interface structure like a reason, that connection can be used as a basis for other connections to be found (applying a workflow of 'find structures related to the problem/solution and connect those instead')

    - identify useful structures like the 'useful structures found/almost found, but not used/connected/understood, interface structures' or 'useful structures not found' by a particular algorithm, and how it could be altered to find those interface structures and use them to improve its own solution metric values (like whether a particular algorithm has enough 'variation potential' to simulate itself and its errors, and enough built-in self-evaluation requirements like 'identify useful structures and abstractions of them found during training', that it can find those structures)
        - for example, the fact that an AI algorithm might frequently find a common solution structure like 'create an index of some answers rather than leaving those variables uncertain or re-calculating them, as they are unlikely to change (info interface) and pair it with adjacent change combinations of existing known solution functions (change/core interface) using some reward mechanism to incentivize additional changes of some type/direction' doesnt mean the AI will find other relevant useful structures, like the following, but it does mean that this structure is useful and that existing algorithms can find some similar/equivalent solution structures and that this solution structure is trivial to find for some algorithms
            - the 'cause of the solution success' as in 'why its useful', or identify that its another useful structure such as a 'cross-interface structure' (a info-change/core cross interface structure) as well as a 'cross-certainty structure' (variable + constant), or otherwise identify other useful structures like 'cause/logic/other interfaces' or other reality-covering variables which could replace the 'info index and the adjacent changes with a reward function'
            - relatedly, this solution is findable to this algorithm bc the solution is trivial to find with its available functionality (as in 'adjacent to inputs') and is a trivial change of existing solution structures
        - this is partly bc AI algorithms (even those tuned to find solutions) use solutions differently than a human brain does, as a 'possible useful output of some process or set of variables applied as inputs', in isolation of other info like other problems, other queries of the brain/model to solve problems, the solution's impact when applied across interfaces, patterns between solutions found, effective functions to describe all solutions, etc
        - a 'reinforcement learning algorithm with some feedback/reward mechanism' (related to specific loss evaluation methods like gradient descent), applies a core solution automation workflow like 'change a base solution (until its across some threshold of a solution metric)', which is not a required workflow to use, 'finding patterns/structures/variables of solutions or other useful structures and re-combining/varying these patterns/structures/variables is an alternative that doesnt necessarily involve using known solution structures'
        - its feedback evaluation function might not be able to adjacently detect subtle differences or hidden variables or be able to use alternate info as input, such as 'info about the network params and algorithm sub-functions' as input bc those will have to be simulated while using them to evaluate them, and the variation required for that simulation might not be supported by the network while also running the evaluation function
            - if its not adjacent, some algorithms will never find it
        - given that these are known errors, a good algorithm would have to combine algorithms like this in a set with a different algorithm tuned to handle this error, at least to provide a counterpoint or contradictory view (like a 'maximal difference' algorithm applied with an 'adjacent change combination' algorithm) if not the trivial improvement that is more useful than a counterpoint in some cases, where the exact opposite might be too extreme and thereby create errors, as the exact opposite implies there is nothing optimal about the original solution structure and it should be opposed completely to test its opposite
        - a good algorithm should be able to re-identify a missing interface structure, if one is missing, using the others
            - if you took the useful structure of the 'reward mechanism' away, what could replace it?
                - this idea of the 'reward/feedback mechanism' is a useful way to 'connect outputs with inputs' (to incentivize inputs that led to valuable outputs)
                - 'common variables/patterns of solutions' is another useful structure that could replace a 'reward mechanism' (like how a 'test' function to check every solution in a set can replace a 'filter' function to differentiate info likely to be a solution)
                - a 'function to make every input useful (derive info from any input, and connect any input to a useful structure)' (rather than finding useful inputs) is another alternative useful structure
                - what other connections between interface structures arent fully used or optimized yet? cross-interface structures are a good place to start looking, as well as other workflows and variables of them
            - if you didnt identify the 'info index + adjacent change combination' cross-interface structure, what else might you have identified?
        - similarly, connecting all solution-finding methods to 'incentives' and 'usages' is useful as another solution metric (does this workflow integrate the concept of 'incentives' and 'usages', which is required for not just finding solutions, but building them optimally, and making sure theyre used, and making sure theyre used correctly, to apply 'usage' structures that make some solution more likely to be used correctly, which are related functions that cant be replaced by each other - 'independent coordinating/complementary alternates' rather than 'equivalent alternates')
        - why might an algorithm identify an 'info index' adjacently without having that given as an input? bc its format allows/requires identifying 'input/output indexes' and 'interim indexes' in between those are a default structure in some networks, given some algorithms, so 'maps/indexes' might be adjacently identified as useful (implicitly, rather than explicitly) by various 'neural network' algorithms

    - identify useful structures in specific problem spaces and sets of useful info like 'sets of differences/starting points' that can be used to identify those useful structures
        - for example, in the 'find a medicine' problem space, identifying that the 'ocean is a harsher environment than land in many ways/cases' and that 'stress leads to evolution' means that 'compounds to resist pathogens are likely to be found in aquatic life, as they survived this harsh environment' and similarly the 'ocean is a high variation environment' and therefore 'new compounds that are useful in a new way or dangerous in a new way are likelier to occur in the ocean' and otherwise identifying efficient structures (like survival functions/compounds in a harsh environment), high variation structures (like where evolution occurs faster), common structures (like common points/paths of evolution), and other useful structures is possible and useful for other intents, like 'predict which organism might evolve an antibody to this pathogen quickest'
        - the set of information including 'starting points/inputs' and 'variables that are useful when applied from those starting points/inputs' is useful to identify in that set, to apply when identifying 'when an interface query will be most useful'
        - whats even more useful is when the starting point is not adjacent to the solution, but the query makes it adjacent to connect to the solution, across problems
        - the 'starting points' here are the 'insights', which when applied, make it trivial to solve these problems such as 'filter the solution space' (identify that aquatic life are a good solution set to test first as they are likelier to be more resilient bc of the harsh environment, which could be why algae and fungi are useful for many things as theyve developed many functions as theyre resilient to harsh environments, and similarly is why they can be more dangerous as well, theyre over-evolved)
        - finding those "'useful starting points/inputs' for a query (run on a particular problem-solving network) that make the query most useful" is a useful problem-solving intent, just like finding the functions of that problem-solving network is a useful intent, which is like workflows that involve a step to 'solve a different problem, or change the problem into another problem, or solve a sub-problem' but involves changing the inputs until theyre insights, which are useful inputs/starting points to most problem-solving queries, and finding useful connections to match an insight set with a query set that would make that insight most useful
        - similarly, adjacent changes to known insights can produce other insights (new insights which can be used as new inputs to queries which could make them more useful), like how the above insights adjacently produce the insight 'extreme systems/contexts (within survival limits) can produce useful structures like multi-use structures and other powerful survival structures like a high variation in antibodies', similarly an environment producing survival structures like 'high variation in antibodies' could produce other useful structures, like 'common compounds of structures (fish) having those useful structures (high variation in antibodies)' (which is why omega 3s might be useful for survival, bc the fish species evolved to have a high variation of antibodies, which led to the development of other useful compounds, or the omega 3's might be an input to the antibody creation rather than an output, where the omega 3's might be useful as an 'equivalent alternate' as the antibodies if they have such a causal dependency that they can be used as an approximation), similarly insights make most workflows more successful, so theyre useful to apply regardless of the structure of the workflow in many cases (answering questions like 'is the omega 3 compound only useful or only develop to be useful bc of the high variation in antibodies, or only useful bc it occurs in a resilient/surviving structure which indicates other compounds would be useful in the same species, or only useful bc of the harsh environment which would require any surviving structure to develop useful structures, or only bc the survival structure of the host uses other useful structures like "resilient multi-use/function" structures like algae as an input')
        - differences in the possible reasons for the usefulness of a structure indicates the usefulness of identifying differences in reason, as different reasons for usefulness reflect different functions, systems and variable interactions and other interface structures
        - similar to how cross-interface structures tend to be useful by default, 'opposing' structures (like a generative function and a filter function) tend to be useful as well, possibly bc theyre also cross-interface structures

    - identifying specific variants of known useful structures like known errors which arent formatted with sufficiently specific structures to be automatable which have potential for additional specficity and therefore an opportunity for optimizing the usefulness of those structures
        - for example, the 'loophole' structure is a useful general error structure concept, as a target structure to focus on/avoid/close, but its not specifically structured enough to be automatically identifiable/solvable, so adding specificity to this structure is useful (such as identifying a specific definition of it such as 'a gap in coverage of a solution of all possible inputs, where coverage of inputs prevents some error structure from occurring for those inputs'), which is a more useful variant of the error structure through its specificity, which is now possible to find automatically (find the gaps in possible inputs covered by a particular solution, given this function to identify coverage of inputs and this function to find all possible inputs) and also automatically fix (finding a function to increase the coverage of inputs for a particular solution)

    - identify structures that are common across solution-finding methods like solution automation workflows/interface queries that are particularly useful, and identify structures related to these common structures (such as conceptually similar as, or causal structures of these structures)
        - for example, a 'structure common across solution-finding methods' is 'high variation', 'usage of high-variation functions', 'high variation between functions used', and other forms of 'high variation' such as 'independence/orthogonality', where other structures can be used to connect these structures such as 'similarity between input/output (such as a protein/receptor similarity) that makes these high-variation structures interactive' and 'a clear difference/similarity created by the solution-finding method that is useful for some problem-solving intent (similar/different in the way that is specified as useful by that problem-solving intent) and similar/different to the solution/problem structures in some way', among other structures of the solution-finding method like how 'connect a sequence of similarities to create a clear similarity/difference' is a function that can describe many workflows as those workflows will fulfill an intent like 'connect problem/solution' that makes this structure a common implementation structure of a workflow
        - similarly, the solution-finding methods will vary in a new way to connect structures to fulfill some core function interface (like the 'find' or 'build' or 'derive' interface), so the solution-finding method would need to be a 'new/different connection between functions that still fulfills some core function intent (like find/build/derive)', where 'fulfilling some core function intent' might take the form of 'vacillating around the find interface applied as a symmetry, so that its similar enough to the find function to be an approximation/alternate of it'
        - finding 'independence' structures such as 'uncorrelated, indirectly related, distantly related, specifically but not generally related, relatable but not by default related, relatable with either the same unit/iterated function or with new variables, requiring a high degree of work to relate, or otherwise independent variables in the same system' is a way of finding useful variants of independence to apply in solution-finding methods
        - this is similar to how structures common to solution automation workflows are higher interactivity/connectivity/variation than other structures, they also have other attributes in common which are related to these attributes (independence being related to variation)
        - given that there are likely to be multiple reasons why a structure is useful, as useful structures are likely to be useful for multiple intents (an alignment between multi-intent and multi-reason utility), identifying the other reasons why a structure is useful (independence isnt just related to high variation, its also related to a known useful structure like a cross-interface structure, which is useful in that it connects different systems), and the connection between these reasons (high variation structures like 'independence' would likely be related to other high variation structures like 'connections between different systems (as in cross-interface structures)')

    - identifying functions like 'specific repetitions/iterations' that are less likely to create useful info, to identify structures likely to create useful info by applying differences to those structures, such as 'repetition functions with specific variables (like "variable position" and "variable application position (change input or change limit)")' that make it likelier to be useful for problem-solving intents like 'identifying new variables'
        - some repetitions (as in 'iterations') of a structure can create useful structures like additional dimensions (like 'repeating a square by stacking it can create a cube')
        - other repetitions of a structure can create less useful info (like how 'iterating across every possible point of a square' is not the same as 'adding/finding new useful variables')
        - the difference is that while both add new dimensions (a point creates a line when iterated through 'stacking', and a square creates a cube when iterated through 'stacking'), one can contain more variation as its higher dimensional and the iteration function can also contain more variation for the same reason
            - other functions to add a dimension include 'position a copy of the structure so it has a side/surface in common (an overlap on one unit of the border, such as a side) and fill in the implied structure by the endpoints/other sides of these overlapping structures, connecting their endpoints/corners' or 'position a copy of the structure, a distance away equal to one side of the structure with an overlap of the centers of the two structures, meaning a distance in one dimension only, and connect their corresponding endpoints/corners/sides of the two structures'
            - all of these involve 'positioning a copy of the structure in a different position from the original structure, and in a different position from other iteration functions, and then connecting the two structures'
        - to include relevant metadata such as a 'way for this error to be true/useful/a solution' (for the stated intent of 'identify new variables' that its an error for):
            - its possible to create high variation with a function that will definitely cross every point and is not a function like 'iterate every point crossing this constant line, then add an increment to the constant line and repeat the iteration'
        - what is different about a function like 'iterate every point crossing this constant line, then add an increment to the constant line and repeat the iteration' and some other iteration function that crosses every point but is less predictably uninformative and low-variation? 
            - the variables are applied at a 'position' where the variables can have 'compounding' value (such as 'integrating the feedback from one variable directly in another', similar to the fibonacci sequence), as opposed to being applied in a limit of variation
            - in the lower-variation iteration function, the variation is in the limit of variation, where the line intersecting with the square is used to guide (and therefore limit) the variation of the iteration is predictably and linearly changed every time, to produce the same result every iteration
            - there are some 'compounding/exponential/otherwise non-linear' functions that would produce predictable results, but this output is less common in that type of function
        - similarly, where might this iteration function be useless/useful? 
            - it might be useless in creating new variables, when the structure being iterated on (such as a cube) is already higher-dimensional than what the iteration function could create (such as a line), if the structure being iterated on isnt a 'dimension set' but rather a 'structure in a dimension set' (when the input problem structure is already a solution structure, more so than any change created by the solution-finding method could create)
            - this is an example of why its useful to be able to find the 'net/emergent effect of iterated changes' and 'metadata about those effects' (what 'degree of variation' is possible with this function, applied in this way, to these inputs, to this degree of iteration)

    - identifying high-variation structures like 'philosophies/perspectives' that are abstract or otherwise useful and also easily filtered
        - similar to how the golden rule has associated concepts like symmetry, generosity, justice/balance, reciprocity, and empathy, it also has associated errors (its not enforced, so could easily cost someone all their resources, and empathy doesnt always produce a requirement for generosity, and justice/reciprocity arent equal to generosity, and symmetry is different enough from balance to justify its own concept)
        - similarly, perspectives like 'magical thinking' are easily disproved without a test (whether a real-life test or an algorithmic simulation), just with finding info about 'common thoughts' (such as 'go to store', with related info like 'the store doesnt come to people, even though they think about it and visualize it')
        - relatedly, other high variation structures include combinations of existing high-variation structures like 'perspectives' (a 'filter with priorities that covers reality') and 'cultures' (a 'stack of iterated changes to create variable interactions within a perspective'), such as 'culture-perspective combinations that lead to the destruction of the society that applies that culture-perspective combination'
        - relatedly, its useful to connect perspectives with structures like useful structures (such as 'cross-interface structures adjacently created by that perspective', which are a useful output that these philosophies/perspectives can sometimes adjacently derive)
        - how do you find these errors from these philosophical rules?
            - 'intersections of scaled interactions, where one variable/structure arrives first and uses the resource first, or theres an overlap creating some resource conflict, or these required structures otherwise contradict each other when they interact' is a interface format description of a common error
            - 'apply the rule at extreme variable values, like at scale or at its most extreme interpretation or an extreme test like a test with measurable large structures like a building, to determine scale limits of the philosophy'
            - 'finding differences between concepts' of the philosophy/perspective, such as how 'justice is not equal to generosity' and errors resulting from those error structures like 'differences being equated as equal' (or 'equalities being differentiated as different'), although both are related concepts adjacent to the 'golden rule', and similarly connect the concepts to interface structures like 'requirements'
            - 'finding differences in the context that created a philosophy and useful contexts like current/modern problem spaces, where differences/gaps in this mapping across time contexts could indicate the irrelevance/in-absoluteness of that philosophy'
            - finding 'lack of connectivity to reality-covering structures (how does it connect truth/falsehood with abstraction/specificity) or interface structures (does it contain enough variation to be realistic)' and 'lack of handling for common problems (how does it handle "group interaction errors")' are other useful functions to find errors with these perspectives ('if you prioritize the same priorities as this perspective, can you avoid errors in some common problem space?')
        - why are these philosophies/perspectives useful?
            - given that 'philosophies/perspectives' are already extremely filtered rule sets, they themselves are easily filtered, which makes them useful
            - these 'philosophies/perspectives' are an attempt to find 'abstract efficient/limited rule sets that cover all of reality' (given the structure of 'perspectives' as a filter that prioritizes a subset of priorities over another subset, as in a usefully limited set), but are often incorrect, as they tend to over-simplify/over-limit/over-filter, use structures on an incorrect abstraction/interaction level, and also dont connect their rule sets to actual reality-covering variables (truth/falsehood, im/balance, power, un/certainty, simplicity/complexity, abstract/specific, solution/problem, useful/relevance/meaning, requirement/alternate, probable/impossible, etc)

    - identify solution metrics like how a falsehood should be accompanied with the corresponding true version of it and an error should be accompanied by the useful/solution version of it
        - a 'false similarity' (like a 'variable that seems correlated but is independent') can be a 'true similarity' and is useful to connect to a true similarity, even if the connection contains very little information
        - for example, two variable interactions may seem correlated (a 'false similarity'), but in real systems, those variable interactions might occur for the same reason (a 'true similarity', in the position of 'cause'), even if theyre technically independent of each other, they might be dependent on the system that creates the pattern in the same reason for those similarities in structure, and therefore not absolutely independent, a reason such as 'because of common limits and inputs, this structure is often adjacent and optimal for some common intent like "storing info", so it keeps re-occurring, even though the two processes are not directly interactive, or otherwise dependent or similar or connected, causally or otherwise, and the similarity they have in common is that reason of adjacency for why a structure keeps re-occurring, and another similarity in the available limits/inputs that are adjacent to that structure, and the similarity of the adjacency itself'
        - this may not seem like a lot of info, it may not be derivable (as in exactly knowable, having ruled out other possibilities, with just the variable correlation), and it may also not be useful in predicting the structure's functions/variables, as they may vary far more in uncorrelated variables than the correlated variables, but its generally useful to know 'system/input/use cases where a similarity/difference may not only occur, but also may be some other high-variation useful structure as well, such as false/true/causal/independent/similar/connected/adjacent/etc', as these 'cases connected to structures' provide a structure to look for/apply as a base structure, to describe the system in which the variable interactions occur

    - identify useful structures like 'patterns connecting useful structures' such as 'cause (backward in the sequence to find inputs)/predict (forward in the sequence to find outputs)' as different values of the same variable 'direction' applied to a core structure like a 'input/output sequence' to find useful structures like functions to find core functions or primary interfaces or other useful structures by applying this pattern and the reason for its success (involving 'interactivity', 'variability', and 'powerful' structures like 'core functions' or other useful structures like 'cross-interface structures')
        - similarly, 'filter/generate or reduce/combine or compress/expand or find/build' is related to different values of the same variable 'count' applied to a core structure like a 'set' (or a 'limit', as in 'what is not in the set', which acts like an 'equivalent alternate' storing similar information as the 'set')
        - 'embed/inject' is applicable to a core structure like a 'container' through values of the same variable 'position'
        - this is why 'sets' can be used as an alternate structure for 'predict' intents, they capture similar variation/information as 'sequences' and are an 'equivalent alternate' core structure, where the intents enabled by 'sequences' are similar to the intents enabled by 'sets', as 'sets' encode information about other interface structures like 'interaction levels' which are highly predictive of other variables (structures on the same interaction level can be used to predict other structures on the interaction level, more so than variables on other interaction levels)
        - finding these variables (direction) applied to interface structures (sequences) to get primary interfaces or other interface structures (cause as a core function and a primary interface, predict as a core function)
        - finding the 'set of similarities' and 'input/output sequences that determine a structure' can identify the 'variables of structures which can identify it'
            - for example, a set of slopes/starting points creates a 'set of similarities' in the 'set of intersections of these 'slopes/starting points', a 'set of similarities' which can create a 'set of limits' through the 'set of intersections' resulting from those similarities (the slopes are 'similar enough in position (to allow the possibility of an intersection)', and 'different enough to be sides of a polygon (which are 'maximally different' slopes)')
                - this 'set of limits' is a core interface structure (if you know the 'requirements/limits (of what something certainly/definitively is or is not)', you can often solve the problem) that fulfills one of these core function intents like 'filter'
                - 'similarities' (adjacency of position, difference of slope) -> 'intersections' (different type of similarity) -> 'limits' (as 'requirements' of some similarity, or 'limits in the sense of differences from other possible structures')
                - the 'intersections' are adjacently convertible into another useful structure 'set of limits' (if they connect enough to form a bounded shape, where those limits can act like boundaries)
            - what other structures fulfill a different intent, and do those structures capture the same information and do they use different similarities/structures and how can those be connected to these similarities/structures, through identifying variables of those 'equivalent alternate' structures
            - a 'symmetry through an equilateral triangle', the 'length of the symmetry' and the 'radius of change that constructs the remaining side of the triangle' can also construct a triangle if its equilateral (has a symmetry dividing it in equal sections), which reflects the utility value of a vertex between two vectors in creating a triangle (once you have one vertex with two vectors as a corner of the triangle intersecting with the symmetry, which is retrievable with the symmetry and the radius of change which determines the length of the remaining side, the triangle is determined), where the 'length of the symmetry' and the 'presence of the symmetry' determine the other angles which are possible, similar to how the 'hypotenuse of a right angle triangle' encodes info about the other sides (once its known that its a right angle triangle)
                - the 'set of similarities' in this example is the 'set of similar radius lengths from the symmetry, which creates a similarity in the angles possible in the equivalent-angled corners, in their equivalent interactions with the point identified as the remaining corner', and this 'similarity in angles' creates a 'filter' of the 'remaining possible angle value'
                    - 'similarity (in symmetry)' -> 'similarity (in radius length)' -> 'similarity (in two equivalent angles)' -> 'filter (of remaining angle, through the "requirement" of angle sums that acts like a filter)'
                    - these structures are adjacently connectible (such as how a 'similarity across a high ratio of inputs, as in the equivalent angles' leads to a filtered set of 'possible other inputs that could explain a difference, as in the remaining angle')
            - the 'similarity in angle sums of a triangle' contains related information which can be used to filter possible angles of different corners, once one corner is known
                - the 'set of similarities' in the 'similar limits (upper bound) on angles of a triangle' which leads to a 'similarity in related structures (the limit/upper bound of "combined angles of a triangle")' which leads to another similarity in related structures (the similarity between each angle and the other angles, and each angle and its relation to the upper bound of their sum and each angle and its relation to requirements like 'at least one right angle' or 'two equal angles', as the progressing sum of each newly identified angle has to be some degree of similarity/difference from each other, the upper bound, and requirements (given the scarcity of allowed number of angles and the requirement created by the first known angle and the 'requirements about triangle type which are known', with rules such as those regarding 'differences between variables of types of angle connections (like two equal angles or all different angles or one right angle) in different types of triangles (like isosceles triangles or triangles with all different angles or one right angle)') to the other known angle but different enough from the sum to allow a third angle before the upper bound of the sum is reached, depending on their position)
                -  'set of similarities' in the 'similar limits (upper bound) on angles of a triangle (where each angle has to be less than 180)' -> 'similarity in related structures (the limit/upper bound of "combined angles of a triangle")' as their sum has to be less than 180 as well -> another similarity in related structures (the similarity between each angle and the other angles, and each angle and its relation to the upper bound of their sum, as these angles of the triangle corners encode info about the intersections/limits found in another structure, as a 'vertex of two vectors' encoding the angles is another format than the 'set of slopes/starting points' that intersect or the 'set of intersections' defining the endpoints of the triangle
                - this encodes a similarity between upper bounds across structures that are operated on with a symmetry (the upper bound of each angle's maximum possible value 'in certain cases of the other angles', applies to the sum of the angles as well, bc summing the angles preserves this similarity) which creates another similarity in 'first-next/next-last angle' values in the sequence, as each angle is filtered and applied as a filter of the next, as the 'sum' sequence has an 'alignment' with this 'angle' sequence and can be used to filter the 'angle' sequence
                - the 'unit/sum' adjacency is the basis for the similarity (the units are adjacent to and useful in creating the sum), and the angles are adjacent to each other (in required similarities or limited differences) as well as the sum and the requirements of the triangle sub-type
            - similarly, triangles can often be trivially changed such as reflected/rotated to create other triangles
            - what is the common structure across these variables ('set of intersections/limits', 'set of symmetries/radii/lengths', 'set of vertexes (of vectors)', 'set of equivalences in sums/related angles/requirements', 'set of similar shapes like other triangles') which create the same shape type?
                - all of them encode different structures of 'similarity' through applying different sets of core structures, similarities which connect different 'differences' (like different 'input formats') which are variations of the same information (different 'sides/endpoints of a triangle'), where these differences are similar enough to the input and target structure to be 'equivalent alternates' in capturing the same info, which can connect 'maximal differences' (such as different slopes required for an equilateral triangle) within some range determined by a similarity (like an angle sum requirement)
                - all of them differentiate the structure from other shapes ('given these intersection points, it cant be a square'), and some of them differentiate within triangle sub-types
                - all of them fulfill a "sequence connecting interaction levels of the primary different sub-structures of a triangle that couldnt build a different shape", given their possible connection functions (components that can be used to 'build' the triangle or 'limit other possible shapes so that a triangle is required'), starting with input structures that have the right differences to create a triangle with a few changes and applying changes on various interaction levels of similarities/differences so that the final structure is created as a result of this sequence
                - these involve 'conversions across "types of similarities"' and eventually create a useful 'difference' (from other shapes) or 'similarity' (to the target shape), or a 'similarity in a difference' (a 'common connection between multiple maximal differences like different slopes/angles' or a 'similarity between a vertex/intersection structure and a triangle' or a 'different way of connecting/representing/generating/determining/deriving the same info') that can be used to connect the various input sets to the target structure
                - given that a triangle is adjacent to 'core' structures (like 'lines/points/angles'), it should be similar based on these/other adjacencies, such as similar to these core structures to some degree (similar to components like lines/points/angles and structures of them like vertexes), similar across sub-types (similar to other triangles of different types), similar to the sub-type (similar to requirements of a triangle sub-type), similar to itself (similar to requirements of the triangle definition), different from non-triangles, similar to the supertype (other polygons), and possibly also similar within some symmetry defined in a requirement of a sub-type (such as how 'two angles must be equivalent' in some sub-type), similar to adjacent structures of core structures ('intersections' as a structure of a core structure like a 'line', or a 'combination' applied to a core structure like an 'angle', or a 'combination' applied to a core structure like a 'triangle side' to create 'vertexes' as 'combinations of sides'), similar to problem-solving core interaction functions/structures like 'limit/filter/connect/combine' as core structures (or structures of core structures) can interact in ways that are different enough to be similarly variable and therefore interactive with those core difference structures ('limit/filter/connect/combine') before achieving the target difference of a 'triangle' from the 'inputs'
                - similarly, the similarities present in the connectivities/interactivities/adjacencies (like how the corner/intersection is adjacent to the side) of the maximal differences (three sides) determine the 'equivalent alternate' structures which can store the same info, where the 'maximal differences' act like symmetries to base changes around, and the adjacent changes found with those changes determine 'equivalent alternates' around the symmetry of the 'side', where the 'corners' also act like 'connection' structures of the 'sides' (similar to how input/output sequences and interactive structures like 'coordinating output/input sets' can be used as alternates bc of the 'connectivity' of one meaning the 'interactive structures' for the other meaning the 'input/output sequences')
            - from this, it can be derived that a structure common across solution-finding methods is that a solution-finding method results in a clear and stable similarity/difference to the target solution structure or adjacent solution structures or problem structures
            - similar to 'adjacencies', identifying 'asymmetries in probabilities' (as in, identifying structures that make another structure 'likelier', as a more specific form of 'adjacent') is similarly useful in determining structures that co-occur in a set or occur in a causal sequence, as an alternate useful structure to apply as 'connections', 'sequences', 'causes', etc
            - identifying 'functions (such as find/build/change/derive)' that act like interfaces (as a 'find information' function can be used to solve all problems) is useful for identifying other 'sets of equivalent alternates' that can be used to apply variables/similarities to problems to find 'equivalent alternates' in other problem spaces, where these core functions connect interfaces (such as the 'filter' structure with the 'find' intent and 'reduce' functions) and act like a set of structures that connects all primary interfaces and therefore act like alternates to the primary interfaces (like a set of horizontal lines of these core functions like 'find', as opposed to the vertical lines of primary interface structures like 'potential', which cross the same information and are highly interactive and can act like equivalent alternates for some intents, which is a useful metaphor for all information sets that can act like an interface)

    - identify useful structures like 'problem/solution connection structures' such as how 'adjacent info' is connected to the problem input and 'agent incentives is likelier to be closer to the solution as they indicate probabilities which are useful for predicting functions/solutions' so connecting 'adjacent info' and 'agent incentives' is likely to be useful to solve problems as a 'connection structure', which is related to the workflow 'connect structures adjacent to problems/solutions rather than the actual problem/solution'

    - identify useful structures like 'scaled interactions' like 'group interactions' such as how 'the implication of one example is not sufficient to solve most problems' and similarly, other info is required such as the vertex between the 'group-group interaction' (whether one group is suboptimal for other groups) and the 'group-system interaction' (whether one group is suboptimal for the system), as 'group interactions' are high-variation and can be applied to solve problems involving group/scaled interactions

    - identify useful structures like structures that are useful for a particular problem-solving core interaction function like a filter or other core interaction functions, such as how 'maximally different' structures are useful for 'filters/generative functions' and similarly 'randomness' and 'sensory deprivation' and 'extremely independent/disconnected variables' is useful as a 'optimizing/learning input' for 'deriving' functions
        - similarly, these similarities fulfill other solution metrics like 'alignment of incentives between opposing forces' as 're-using a structure across functions' is a useful optimization of function usage

    - identify useful structures such as attributes of useful structures like 'connectivity', as a useful structure is likely to be connectible to other useful structures and is likely to interact with other useful structures, as in likelier to be used by useful structures and other encodings of similarity like 'commonly found as an input or used in a useful function', since theyre simmilar by being useful and are therefore likely to be similar in other ways

    - identify useful structures like 'requirements' such as 'optimism' which is required as an input to continue pursuit of a reward and therefore required for success, but which may be contraindicated by negative inputs, so identifying false statements is likely to connect that negative state with a 'successful/solution' state as it buys time to find a solution

    - identify useful structures like 'concepts' (like 'fairness') and 'structures' (like 'scale') which are useful to apply to find limits/emergent effects, such as how 'scaled fairness' creates a very aligned and organized system (in that 'potential/ability' and 'problems requiring that potential' are aligned) but also a limiting system where there isnt much freedom to vary and therefore finding alternate optimizations to the system is less likely, although such a system isnt likely, which is useful for finding 'new directions of inventions'

    - identify useful structures like structures like 'metrics' to 'determine if structures are equivalent', such as if two structures are more interactive, theyre likelier to be equivalent which is true bc theyre likelier to exist on the same interaction level and are therefore likelier to coordinate than not and bc theyre likely to contain differences (like 'input structures like receptors/spike proteins') that can be changed to interact with each other

    - identify useful structures like useful 'mappings' such as between the 'navigation' problem space and 'networks' to apply navigation solutions to networks to find network format optimizations, such as how when investing work in multiple different solutions (where each agent with a car creates possible 'solution/error' structures in the 'driving/navigation' problem space where the problem is 'generally find a useful solution-finding method or specifically find a route and usage of this route to get to a destination point without errors', where those solutions intersect or pass, the slower solution can temporarily stop and use various structures of terms from the faster solution to test if it is an optimization structure, where each solution-finding method has a network of solutions that it finds or is likely to find or capable of finding with adjacent changes given the network's defaults (like similarity metrics, position change functions, etc), and the overlap of these networks is useful to identify so that these intersections between networks can be identified and the optimal route between these networks (switching between solution-finding methods) is findable on this set of networks, where the network might only represent the adjacent solutions a solution-finding method might find

    - identify useful structures like 'optimization' structures such as a suggested/optional 'slow/fast lane' which optimizes for grouping drivers with different relevant sets of intents, relevant to core functions/metrics like 'speed' to minimize a specific error type like 'lane changes' frequently cause
        - why is identifying a new solution/error in a problem space almost equivalent to a solution automation workflow? bc solution automation workflows are 'unique abstract/other interface connections between problems/solution structures', and each error is a new step added to or changed in an existing connection, creating a new connection between problem/solution structures, which is easily abstracted or otherwise converted to a primary interface, at which point its likely to be a new solution automation workflow
        - identifying a new solution/error is similarly useful for identifying new variables/differences/similarities/other useful interface structures (like how identifying a new error type is also identifying a new solution type bc you can just apply differences to the error type to adjacently derive the solution type, as errors are different from solutions), which is a useful problem-solving intent (identifying a new useful structure or new variable/difference type is often the same as identifying a new solution-finding method)

    - identify useful structures like 'generosity/strictness' (non-adjacent vs. specifically accurate) or 'optimistic/pessimistic' ('implying info about a future change, as in a future change of a data set') or 'experienced/amateur' ('rule-compliant vs. rule-unaware') which can be applied as a solution metric of predictions, similar to how metrics like 'specific/general' can be used to find optimal solutions

    - identify useful structures like 'different information accessible from a different position' and structures to find that info like 'changes/functions applied at that different position by other agents'

    - identify useful structures like insights such as how 'adjacent changes' (like in gradient descent) cant be used as the solution in all cases, bc some changes can be errors but seem like solutions (like the 'low-cost decision') and can occur at scale, so that any agent in that system doesnt see the errors or the net effect but there is still a huge change set required to fix it ('making agents intelligent' is not a solution likely to be found by gradient descent, neither is its implementation)

    - identify useful structures like optimizations such as where a function can be applied with trivial changes like 'in reverse' and still be useful, such as the 'lane change signal', which can be applied 'in front of a car to signal lane changes' or 'behind a car to pass in case they will see it and move' and still be useful, or similarly, speeding up, which can be useful for 'covering more distance faster' or 'to communicate that other cars should switch lanes to allow car to pass'

    - identify useful optimization structures like 'moving horizontally to see info farther away' which should be regularly applied in cases where 'the lane change function might be useful' and 'info about far-away errors like traffic jams or crashes is signalled as a possibility'

    - identify useful structures like 'specific examples of how a statement/solution could be true/false (as in correct/incorrect)' such as 'specific data sets that could make a solution true/false' so that 'determining difference from these true/false example data sets' is useful as a 'test/filter of a specific solution'

    - identify useful structures like similarities between error structures, such as in the 'driving' problem space where a 'sequence of agents' can produce multiple errors such as when 'driving next to the sequence where other errors or error inputs occur, such as the sequence is slow, which makes it easy to speed by it (a dangerous speed if any of the sequence agents uses a common function to change their position), and one of the agents changes lanes and its difficult to see their change lane signal (input block error) bc the sequence is slow as in a traffic jam so theyre close together' and the 'driving sequence will be difficult to pass if the incoming traffic is distributed rather than occasional or grouped together'

    - identify useful structures like 'independent subsets' ('independent' as in 'equivalent alternates' which have similar/overlapping info coverage/storage/functionality), such as 'convergences/divergences', which are useful to identify useful sets of structures which can be used to derive a high ratio of other info (like 'intersections' and 'change directions'), and which therefore are useful to identify 'similarities/differences', just like 'filters/averages/densities/limits' and 'parallels/orthogonals' are useful independent sets of structures to identify that cover a high ratio of info
        - the core insight is that 'equivalent alternates' can act like each other and replace each other, and are therefore independently capable of deriving info, so identifying 'independence' in this sense is a way of identifying 'equivalent alternates' and vice versa

    - identify useful structures like how applying 'proving' function often requires 'holding some information constant that is normally a variable', and therefore requires a 'distortion like a stretch/angle' of reality to achieve, as systems might only falsely (as in 'rarely') actually 'be in that state, or function that way, or even just seem/be represented that way', so much so that its arguable false to distort it in that way by holding those variables constant, as in 'proven true, in a generally false (as in "requiring any distortion at all" or "requiring unusual distortions that are false, in that they are not generally useful for other problems") but specifically true (as in "having some trivial number of conditions/changes to be true" and "possibly true, as in not guaranteed to be impossible") distortion of reality', which is less like 'proving a statement to be true' and more like 'forcing a statement to be true' (where there are usually some contexts where any given statement can be interpreted as true but some statements require more work than other statements to be verified or proven and are therefore less true)
        - similarly, identify useful structures like 'specific applications of insights' (such as that 'most statements are true to some degree and false to some degree', such as by applying 'specific variants of insight components like truth such as useful') where the 'specific application of that insight' using 'useful' instead of 'true' is 'most structures are useful in some way', such as how an almost completely useless structure might be only useful as 'another example of some type which is unnecessary as there are plenty of other examples', or only useful in that it 'creates errors', so it provides info about 'errors to avoid', so identifying extremely useful structures is a matter of identifying common variables of these useless/error/suboptimal structures and differentiating from those structures, as 'useful' structures are often more identifiable than 'true' structures (as 'true' is less testable/verifiable and more variable/general than 'useful'), so its useful to apply that specific variant of the definition

    - identify useful structures like 'connections between useful structures' such as connections between concepts like 'random' and 'nothing', as 'randomness' is associated with a 'lack of info' and also 'equally distributed info', so that 'nothing' might have an alternate definition like 'extremely/maximally distributed info', which could be a default state of the universe that is regularly returned to when info optimizations are found and applied to distribute info again, where 'organization structures' (like patterns that occur in random value sequences) also occur by default given this randomness, and can lead to temporary organizations that often produce these optimizations which then distribute info to return the state to nothing again, and these patterns in random value sequences can identify other possible 'initial conditions' of organization leading to other universes

    - identify useful structures like 'empathy' which are connected to other useful structures, such as how 'empathy' can produce an error like 'bias' if applied with other 'biases' (like 'locality', where 'empathy applied only locally' might produce 'biases toward a local group that is the majority locally') where it can also produce useful structures like 'complete/deep understanding' if applied 'absolutely locally' (as in 'to one system' rather than to 'local systems in general', which could be useful for understanding systems in general, where this 'deep understanding' provides an opposing structure to more efficient structures like 'type' or 'local interactions' as a representation structure and can be created by combinations of these more efficient structures)

    - identify useful structures like 'optimizations' such as 'incentives' applied to specific useful structures like 'attention' (meaning 'attention incentives' like 'distractions') which can optimize for intents like 'routing resources in a direction to centralize/distribute resources', such as 'clearing a lane' in the 'driving' problem space by 'creating distractions that require being in the other lane'

    - identify useful structures like 'requirements' as 'reflections of real structures like real differences' such as how a 'structural pattern' (like a mnemonic device or a structurally similar word) can only be used so many times before it creates errors like 'over-use' where the set of structures that use it is large enough the the structural pattern isnt unique anymore, and another structure must be identified to use as a 'structural pattern' to apply to create useful changes, a set of 'required different structures' that reflects real similarities/differences in real info in real systems

    - identify useful structures like useful solution metrics such as 'sensitivity', which is a structure with high/extreme output from low-cost input, which is useful for 'magnifying/extreme/differentiation' intents such as 'identifying certain/extreme/obvious error structures from an initial/trivial error signal, errors that are possible to identify by magnifying changes like scaling the signal'

    - identify connections between useful structures like 'risk/cost', such as how 'minimizing risk' doesnt necessarily correlate with 'minimizing cost', which is useful to identify as a possible error if 'cost minimization' is a solution metric

    - identify useful structures like 'optimizations made possible by applying variables like "scale" to error structures', such as an 'over-reduction' error can produce optimization structures, like identifying irreducibilities more adjacently than other structures

    - identify useful connections like connections between useful structures like 'neural networks' and other useful structures like 'clocks' and 'input interfaces like senses that store/process overlapping information but process that information in different ways' which can be used to derive different filters which are 'complementary/overlapping/compoundingly useful' as opposed to being 'equivalent alternates', such as how sight/touch/hearing can all detect overlapping/equivalent information about 'position changes' but each of those interfaces stores and uses different info/info formats (sound/light frequency/connectivity/overlaps, heat, etc), and different processing functions ('identify areas of light reflection frequency differences such as color areas', 'identify overlapping audio frequencies like echoes') to derive that same info about 'position changes', with varying info available/adjacent on different interfaces like how 'language is more variable on the sight/sound interface than the touch interface, but each interface adds information compoundingly even if there are overlaps, such as how interpretation of language is more successful, the more interfaces are available', which identifies useful structures like 'frequency' adjacently which can be applied in a useful way in neural networks
        - similarly other adjacent structures on these interfaces like 'focus' in the 'sight' interface can adjacently derive structures like 'filters' and 'functions' (like 'identify type/local interactions') which are commonly used in 'sight' info processing as a specific implementation of a 'focus/filter' function, which is useful in its specificity and approximation potential of the general 'focus' function (when you 'focus on/think about' a structure, you often identify its type/adjacent changes/local interactions as a quick reference to the structure that encapsulates a high degree of variation and functions like a definition, where its type or adjacent changes or local interactions store enough info to act like an alternate to a definition, as they are representations of the structure just like a definition is)
        - 'neural networks' are similar to 'clocks' in that they both implement standards, capture high variation, reflect reality, connect info into sequence structures, have variable difference/distance metrics, are useful for comparison of changes, etc, where 'clocks' act like an alternate info format similar to 'networks', 'sequences', 'interfaces', and 'concepts', as for example in the 'driving' problem space, 'structures moving at similar speeds' are more relevant than other structures (such as by identifying errors like 'sequences of cars that are difficult to pass' and 'inability to pass a car with a simmilar speed'), where both slower/faster cars are easier to interact with ('pass if theyre slower or let them pass if theyre faster') than similar/equal-speed cars, bc of the 'extreme similarity' in cars with similar speed, which makes it more difficult to differentiate and therefore more difficult to interact with, as everything but that car will be easier to differentiate and differentiating the observer and the other car will be more difficult as a result of the comparative ease of differentiating everything else, which will get more attention allocated by default and will be easier to determine by default
        - similarly, the senses are required to reflect the variation of reality in order to be useful, so alternate senses might be useful as well, such as a 'social sense', if there is enough variation in social structures to reflect reality

    - identify useful structures like connections between 'error' structures like 'identifiable certain differences from a solution' and 'commmon/core functions' like 'approximation', such as how 'repeated approximations can generate errors'

    - identify useful structures like connection between common/core interface structures like 'general and specific structures', like how 'general structures can be derived from specific structures as specific structures contain more info, but the reverse is not as useful bc it involves an error of a 'loss of information' and how 'specific structures can offset the over-generality of otherwise useful abstractions like how variability can explain everything but its useful to store specific interactions of variability like how it tends to be preserved across interfaces as they act like equivalent alternates in their information storage/coverage', connections which can be applied to create a 'directed probabilistic graph' where the direction favors change in the direction of specificity

    - identify useful structures like connections between core interface structures (like 'time', 'position' and 'distance' variables) and other useful structures like 'critical points/useful points/ranges/thresholds/starting points' such as how the 'time/distance to a critical point' is a useful structure in the 'driving' problem space, as there are frequent requirements to identify 'approximations of distance and time to move to a structure/point, where some threshold is relevant such as a range of criticality/importance/relevance/variability', such as how its useful to identify the 'optimal distance from a sign to the relevant structure in the rule (like "ice forms on bridge") of the sign (like a bridge)' by identifying the connection between 'distance/time' (as in 'time it takes to slow down, from distance x' and 'distance from relevant rule structure like "bridge" at time of "successful slow down" at a given "slowdown starting point"' and 'optimal distance from relevant rule structure like "bridge" at time of "successful slow down" at a given "slowdown starting point"') and the critical range where the info in the sign is useful (as in "before the critical point, after which its impossible to use the info in an optimal way as in 'slow down to prevent sliding on ice'")

    - identify useful structures like 'connections between formats and useful functions', such as how changing the 'sequence' param (as in 'switching order of a set of items' to differentiate it from 'changing position of one item' by connecting the change to its impact on the 'rest of the set of items') of a structure formatted as a 'sequence' is a common solution structure in that format (as its different from core functions like 'change position of one item' but is still trivial to identify and has higher variation than the core function and also involves higher connectivity than the core function), similar to how 'changing similarity/connectivity metric of a network' when a structure is formatted as a network is a common useful solution structure in that problem space of optimizing networks (a function that is not a core function in that problem space but is still trivial to identify and is high variation and related to connectivity)

    - identify useful structures like 'connections' between 'optimizations' and 'error structures' (or 'errors and other errors') using useful structures like 'scale', such as how 'lying may be optimal in some cases' but its useful to identify errors that can occur if that optimization is applied at an extreme/scale, such as how 'gaming speed traps' can be easier than always obeying the speed limit (which requires functions like 'forcing attention to speed' and 'forcing a limit on a default like speeding'), but if applied at scale or at every possible opportunity, 'gaming speed traps' has errors like 'future costs' such as 'more iterated/recursive speed traps' or 'lost time/resources from traffic violation tickets' and 'cost of ineffective monitoring/enforcement' and 'impact on other regulations and the system of regulations, given the lack of enforcement of a particular regulation', which is likely to create errors like 'suboptimal system states' like 'lack of rule of law'
        - similarly, other errors are likelier when applying the suboptimal strategy, such as 'over-use of brakes' (which tends to occur in common cases like 'at any non-constant speed') which is derivable from the other error structures bc the 'variation in speed' of a '"gaming speed trap" driving pattern' is unlikely to match 'required/optimal variation in speed, given other metrics like normal/optimal brake usage patterns, like only braking at stops/red lights and to slow down enough that a constant speed is maintainable'

    - identify useful structures like 'possible errors' such as 'input blocks' which can be 'connected' to other useful structures like 'optimal/suboptimal cases to apply a function (like the "pass-car" function)' and 'possible similarities' like how an 'input block' (blind spot) could be similar to the 'size of a common structure in the driving problem space like a car, depending on distance from observer' and could be in a 'position that blocks the input of the car', which is a 'suboptimal case to apply the "pass-car" function'

    - identify possible alignments that represent useful structures like 'overlaps between multiple metrics' to fulfill intents like 'check map' and 'dont crash', where specific intents like 'keep eyes on the road' are useful as approximations of those general intents like 'dont crash' to create opportunities for useful 'overlaps' like 'align "phone position" and "input field" (by creating a similarity between phone position and input field and identifying that its not required to take eyes off the road in order to look at the phone bc of large background visual clues which are easy to identify, bc of peripheral and unfocused vision, and the size/color of the objects in the background)'
        - from this example it can be determined that in general 'overlaps' are common useful structures in problem-solving, partly bc of the 'structural similarity' and the 'adjacent difference' between the overlapping structures, which reflects a workflow such as 'change a base solution (using adjacent changes, given the semi-correctness of the base solution)' and which encodes other useful structures like a 'rotation'
        - this connection between the structure 'overlap', the workflow 'change a base solution', and the common useful function 'rotation' is useful to identify, as it reflects that common useful structures are adjacently connectible to solution automation workflows and other useful structures like 'core interaction functions' on other interfaces like the 'math' interface

    - identify useful structures like 'types of vision' and their connection to other problem spaces like 'neural networks', such as how the 'light' problem space has problems that are solved by 'vision structures (like eyes and types of vision and light receptors)' so that 'solving a problem of vision (interpreting information formatted as light)' is related to 'solving a problem of cognition', with related functions like 'focus/group' which map to other useful structures like 'filter/combine'
        - for example, 'night vision' is useful for identifying 'heat' which is a proxy for information such as 'location of useful structures', which is a different info format to detect other than standard visual inputs like 'moving light structures', as the 'temperature' interface reflects similar variation as the 'light' interface, and different types of vision can detect different information (that can have an overlap with information detected on other interfaces), where 'vision types' act like interfaces in this problem space
        - similarly, the 'peripheral vision' interface identifies information like 'background/unfocused information' as a 'bet-hedging investment strategy, to account for the uncertainty of possibility of the case where current focused objects dont provide sufficient information for intents like survival'

    - identify useful problem spaces to apply as problem formats which are useful in standardizing a problem and other interface structures, such as the 'light' problem format, which is useful for finding interface structure interactions such as new variables by applying simple functions and other useful interface structures in that problem space
        - for example, applying 'mirrors' as a 'symmetry' structure to 'find new variables' is possible by applying workflows like 'trial and error' to different 'mirror' structures (like 'angle', 'count', 'position', etc)
        - similarly, connecting structures like 'symmetries' to other useful structures like 'filters' is possible by applying differences to the 'mirror' structures that create a 'filter' effect by allowing some light to pass through
        - similarly, identifying 'extreme differences possible in extreme similarities' is a useful structure that is adjacently derived in this problem space, such as 'creating opposites (like shadows/colors) from light similarities like reflections'

    - a default implementation method is to use existing workflows in this repo as configuration and writing a function to apply a workflow to an 'input problem space'
        - remaining questions are variables like 'which combinations of which variables of reality can be optimally applied as equivalent alternate defaults forming a core interaction level (used to adjacently derive the other interaction levels)', which is semi-defined but still unresolved (as not completely specified) by other workflows/implementation methods

    - identify useful structures like 'accuracy/truth/realism/other solution metrics' that identify other useful structures like an 'error in other metrics'
        - for example, if some statement is 'technically' true, and 'only technically' true, that indicates that other truth metrics (like 'priorities', 'optimality for general intents', etc and other metrics of a statement) are extremely false, so much so that the net effect of these truth metrics is false, as the 'network of truth metrics' is more useful to apply with queries of the network (a structure that answers the question 'which probabilistic sequences/combinations of truth metric network nodes are useful to apply, in what cases')

    - apply structures like 'variables' which can be applied to other useful structures like 'probable filters' (as in 'activate/deactivate' probable alternate filters to resolve the uncertainty of which filter is correct) to fulfill intents such as 'apply an uncertainty as a variable rather than a constant' and 'try every probable variable value to find the correct value and find overlapping/otherwise similar values (once probable variable values are filtered into a solution subset, at which point "trial and error" becomes useful again)', to apply insights like 'some workflows are only applicable/useful in some cases (like "low input count")' and 'optimizations of workflows like "trial and error"  include applying additional functions to connect different solutions identified as non-errors by some similarity metric, to identify solution filters that produce similar/equal solutions'

    - identify structures that its useful to identify, like how its useful to identify 'limits on areas/ranges of usefulness' (like how a 'cloud' can be useful for determining 'direction' up to a point as in during a very temporary time interval, where the cloud moves) just like its useful to identify 'limits on error ranges', as these are useful to filter inputs to specific ranges but also to identify common ranges of usefulness/errors of inputs and other interface structures of these limits, as 'limits' are a problem-solving format just like 'filters' are, though they are often more useful when combined, as interface structures are compoundingly useful

    - identify optimizations such as using 'local info' that is more useful for being more specific or more real/accurate when used vs. expert/technically correct but irrelevant/useless info (such as 'names used in actual local signs' vs. expert/technically correct info like 'the numerical number of a road that is only known to a map database but is not used in a local road sign', or directions involving expert navigation knowledge like 'move in the direction of a neighborhood' rather than knowledge that is accessible to most users like 'move south'), through removing requirements to acquiring info (acquiring info about 'which direction is south' requires less steps than acquiring info about 'which direction is a neighborhood in')

    - identify useful structures like 'false equivalences/constants' between useful structures to identify errors in, like 'errors in reasons for a variable change, which are not reflected in approximating metrics' such as where a reason for an approximating metric change (change in 'estimated route time') is assumed to reflect only and exactly the corresponding reason in the approximated metric ('distance to destination') even though the approximated/approximating metrics are not equivalent as there could be other reasons for a change in the approximating metric, as a variable like a 'estimated route time' can vary for many different reasons (like a 'traffic jam' as opposed to a 'missed turn'), and assuming that the actual relevant metric being approximated (the 'distance to destination') is equivalent to the change in the approximating metric ('estimated route time') is false as these metrics are not equivalent alternates but are useful approximations of each other in some cases which are likely to occur (like where a 'general estimate' is more useful than an 'exactly correct estimate')

    - identify useful structures like 'directions to move in which have no immediate errors', which are useful to apply as default variables (like default changes to a base solution) until errors are identified in those directions, at which point its useful to connect existing error structures to that error point once found, to find other error structures like 'error thresholds' and 'error areas'

    - identify useful structures like variables such as 'scalability' that cause problems like 'inefficiency' such as 'lack of specific efficient identification of a useful item in a set of many items', like how 'ill-defined structures (like overloaded terms)' and 'disorganized structures (like multiple words for the same concept)' and 'non-adjacent definition (conceptually distant) structures (like applying concepts like variability and interactivity and similarity without storing specific useful implementations/variants of these that are known optimal in general or for specific purposes)' are another cause of problems like 'lack of understanding', to fulfill intents like 'find useful problem-solving intents such as "find opposing structures of these problem-causing structures to solve problems"', and similarly other concepts like 'complexity' are frequently causative of problems with known optimizations to resolve those conceptual problem causes

    - identify error structures like 'input blocks' which can be used to identify other error structures (such as other errors that are likely to occur, as they are causally adjacent/relevant to those error structures) or solution structures (like 'structures to avoid errors like input blocks, such as alternate structures that appear to be other structures so when inputs are blocked/removed, these alternate structures can be identified/derived, such as alternate information-producing structures like probabilities (like "there is probably a car within crashing distance") or requirements/certainties (like "there will probably be a car in the opposing lane at some point, with such certainty that its like a requirement"), or such as false similarities that can identify alternate structures, like how a flat/linear-seeming surface can actually be a set of hills/waves which is only possible to confirm nearby or from a different direction'), where combining these error structures with these requirements can identify other error structures (like how a 'false similarity' such as a 'set of hills that looks like a flat surface can hide a required/probable structure like a incoming vehicle in the opposing lane while scaling a hill producing an input block like a blind spot which could produce an error like a crash if another useful function like "pass car" is applied in a case like "two-lane highway"')

    - identify useful problem formats like 'navigation' which can be applied across structures once standardized to a format like a 'directed network (such as a probabilistic network where more connected nodes are more probable to occur from each other, or where ambiguity-resolution functions are organized by probability of usefulness, as ambiguity-resolution structures are alternate structures of filters that can be used to fulfill the decision/selection function)' which have known 'optimization' structures on the structures of that problem format like 'path decisions' (like 'path decisions that enable other path decisions, rather than leading to possible error structures dead-ends, which involve work to correct, where staying in decision-enabling loops is usually more optimal through the interactivity/connectivity enabled by those structures') which can be applied across problems formatted that way (like a network to avoid over-specifying solutions, where over-specific solutions are less interactive/connective with other routes on the network, which is useful to identify in cases like where some problem shouldnt be solved with a specific solution but where solutions should be changed/tested regularly to avoid over-investing in one solution bc the uncertainty of the problem isnt resolvable with available information), a format that is useful for finding structures like 'alternate routes', 'points in a route where errors occur or where errors become inevitable/required', 'common nodes/patterns/structures of error/optimal routes', 'reasons for route suboptimality/success', 'structures like directions that are useful to vary or move in', 'alignments between structures like directions and intents', etc
    - relatedly, its useful to identify specific useful ambiguity-resolutions, like differentiating errors from solutions, error directions from errors, errors from variables/differences, etc

    - identify useful structures like function sets that can be used to solve or improve errors in many problems, such as 'identify when constants are optimal such as when constants provide stability and reliability which are useful for basing changes on' and 'identify structures that can create constants (like how slowing down can create a "constant" in the safety variable in a case like rain)' and 'identify when constants arent optimal or arent required or when its required to change the constant', such as in the 'driving' problem space where a 'constant speed' may avoid errors like 'over-use of brakes' and the 'rarity of a payoff of a speed-up except in cases like where next car is far-away', but which create errors in specific cases (like where the constant speed is zero or where constant speed of the first car in a sequence slows down other cars behind it or cases where speed-ups and slowdowns are more optimal, such as how slowing down is more optimal in cases like 'rain' to replace/re-create the usefulness of the higher constant speed in the 'rain' case)
        - this type of difference required to solve a problem ('change a constant into a variable') occurs in cases like 'where existing constants/functions dont fulfill requirements' (where there is a misalignment between existing and required solution structures), as 'injecting a variable' is useful in workflows like 'change a base solution' (as a base solution is identified and identifiable as suboptimal, as in different from a solution structure), as opposed to other workflows like 'reduce problem complexity (where complexity is the cause of the problem)'

    - identify useful connections like how workflows like 'trial and error' can identify other useful structures like 'convolutions' as in 'try every permutation of combinations/other interactions of attributes across two items' and 'neural network interaction functions' as in 'try every adjacent change combination, until a threshold of error minimization is reached')

    - identify useful inputs to intents like identifying 'interesting (as in high variation, or interactive between high variation structures) questions' that can be used as an input to queries like 'if a question/query is useful' (interesting questions that involve some interaction between a high-variation concept and a useful structure like known/available structures like 'neural network node sets', like 'if neural network structures like synchronicity is required for mass-coordination of neurons and whether this provides a communication function between neurons')
    
    - identify structures that can make other structures useful like optimization structures such as structures that can make 'risk' (uncertainty, ambiguity) useful, such as in cases where there will be immediate feedback indicating a possible payoff, where the change is reversible, etc

    - identify useful structures like variables that, when changed, can change other useful variables (like how 'slowing down' can help find information farther away, through avoiding error structures like 'input blocks' which can block far-away information)

    - identify useful structures like when useful structures (like 'high-variation variables') are not useful/relevant which can occur in cases like 'complex systems where many sub-systems can operate semi-independently of each other and therefore irrelevantly'

    - identify useful functions like 'invest' which have related adjacent useful concepts like 'payoff expectation by some threshold' which are useful for finding useful applications/specifications of analysis functions like 'cost/benefit' analysis
        - similarly, a useful 'investing function' is to 'invest in cases with a high ratio of equity', which is like 'investing in cases with a high information content like a useful prediction model or rule database that is already acquired which can be used as an input' (investing in higher-certainty structures), which is an example of how the rules of investing can be used to solve navigation/filtering problems, as investing involves similar information problems

    - identify useful structures like cases where error structures are not errors
        - such as when irreversibility is not an error but a useful intent to some agent, like an irreversibility (in maintaining some useful structure or another useful state change that applies some absolute truth to avoid applying that truth as an uncertainty by making sure its sustained)

    - identify the 'degree of difference/variation captured' and 'meaning of the difference' and other difference structures necessary to determine/identify/generate useful structures like 'cross-interface structures' and 'equivalent alternate sets of concepts like "spectrums of opposing attributes" (like chaos/organization, truth/falsehood, real/unreal, impossible/required, certainty/uncertainty, stability/instability, power/powerlessness, cost/benefit, constant/variation, embedding/containing, similarity/difference, adjacent/distant) and "similarities between attributes" (like potential/chaos and potential/energy and potential/power)' which are sufficient to solve most problems, since interface structures of a difference can fulfill intents like 'determine other useful variables and their interactions'
        - for example, different sets of concepts can solve most problems, just like cross-interface structures can, which is useful in that they have similar differences captured in the interactions of these high-variation structures
        - this means that most problems for agents involve differences between 'mental functions/attributes (like scalability)' and the 'complexity of systems they interact with' (the problem is that the human brain cant process a lot of information at once, not that the problem is particularly complicated)
        - however more complex methods are necessary bc the adjacent solutions found by existing algorithms are usually suboptimal in ways that are not identified by the algorithm and are not obvious/trivial/default interpretations of the solution (a 'solution found by an algorithm to solve a systemic condition involves a compound that switches off an immune function, which causes another condition not solved/identified by the compound, bc the algorithm doesnt connect all variables adjacently, just the inputs/outputs of the data set')
        - identifying the 'degree of variation required to solve most problems' can filter the set of functions/variables that should be included in a solution-finding method or solution
        - existing simple interface structures solve 'medium-complexity problems' which neural networks are frequently trained to solve, which are mostly a problem caused by 'scalability limits of the brain'
        - more complex interface structures on the outer layers of a generative function set diagram solve more cerebral problems like 'logic puzzles' which require more complex structures kept in working memory to make trivial to solve, like 'organization/optimization functions', which can allow other required mental functions for higher complexity problems like 'identify new concepts/similarities/variables' and 'identify unifying systems/structures which interact in a new way to explain/solve a complex problem'

    - identify useful structures that occur in other systems (like the brain/bio-system) which could be related to optimizations of useful structures (like neural networks)
        - for example, the 'blood flow' in the brain changes how the brain can be and is used, and similarly other neurons can evaluate how other neurons are doing, which implies that there should be extra neurons kept in a neural network for evaluating the progress and connectivity/interactivity and similarities of other neurons just like extra neurons are capable of creating scalable mental functions (associated with intelligence) like 'evaluating a culture' which requires a lot of extra variables so there would need to be free neurons to handle evaluating complex interactions like that
        - connecting the 'neural structures' is useful to identify existing biological mechanisms for brain structures like functionality and errors which can be used for optimization or avoided once mapped directly to neural structures)
            - 'blood flow' (as "a force acting to group neurons' interactions and enable other functions like reward/pain signal delivery and waste removal")
            - 'electrical activity/conductivity/other electricity structures' (as 'modifiers of relative position through modifying speed of connections')
            - 'neuron connectivity/presence/activation' (as 'usage' and 'available resource' structures)
            - 'waste removal and immune functions' (as a 'de-noising' structure, to get rid of 'noisy/irrelevant' information structures like dead-ends, unused neurons, outliers, over-specific parameters, and other error/suboptimal structures)
            - identifying other error structures like in order to 'trigger a useful process that creates rewards', there needs to be a 'lack of rewards, if this process checks for that'
            - other structures with high-level general states like 'emotions' (encoding them as defined in neural structures like defining 'depression/mania' as a 'wide-spread, mass-scale de-activation/over-activation of neurons, especially neurons which connect to other subsets like high-traffic subsets' and mental disorders like 'schizophrenia' (like 'forced separation of neural groups which are activated one at a time, forcing each group to develop all useful mental functions as equivalent alternate brains'), as well as 'solution structures of those mental disorder states' such as 'connection/variable' structures for 'autism/schizophrenia', 'positive/variable/opposite' structures for 'depression', 'boredom/constant' structures for 'mania/bipolar', etc, and similarly connecting 'personalities' (as 'brain usage structures like creative/intelligent personalities that optimize for some metric like novelty/boredom') to these other brain structures like 'brain disorders', 'default brain perspectives', 'brain connection units (as repeated structures in the brain, like common connection functions between neurons like "check for related specific variant nearby")', 'brain optimizations' (like commonly useful solution structures to brain disorders and functions to build multiple different brains to use to evaluate each other to implement default power structures like solution structures like checks/balances which offer 'equivalent alternate powerful structures that are connected by some cyclical dependency'), and 'emotion states' (as 'outputs (as outputs of a reward/cost assignment function) of structures common to various personalities and the resulting/associated perspectives and disorders')
                - similarly, identifying other possible useful built-in functions, such as how depression is like a sedative on excitatory responses, which if maintained by survival (the emotion is handled with variables to produce survival), can lead to more optimal thinking (as an alternative to acting), to stop manic/excitatory responses and prioritize slower/deeper thinking to resolve the emotion by identifying variables & identifying variable interactions, where this less hyperactive/excitatory state is similar to 'brain exercise' in that connections/movements are made more difficult so mental strength (such as through highly useful and distributed sub-networks) is required
            - identifying possible errors with connections/interactions between these mechanisms is valuable to identify mental/logical/other brain usage errors and opportunities/solutions for optimization of brain structures, such as how 'empathizing with people with depression/other brain disorders can produce similarly negative signals, if repeated enough, and these structures could become permanent as they might create a cascade, if repeated above a threshold value', and 'brain rewards are required as inputs for the bio-system to continue to exist, rather than always being a significant signal of some successful mental process'
            - identifying possible optimizations (like 'repeating a particular function consciously to adjust how structures are connected/stored in memory', such as 'only focus on new difference types and how theyre different from known difference types (or new solutions), and ignore other variables/structures', which could create a brain that only sends success signals for new variable types identified (or 'sources of high variation' which are likely to produce these new variable types), which is highly optimized for a problem-solving intent like 'identify new variables') to suboptimal memory storage structures (such as 'always connecting memories back to the same problem or same system', which might not be a sufficiently high-variation system to contain all useful variables or might contain so much unnecessary information that its not useful as a mechanism to organize information)
            - other structures like 'optimal neural states' such as 'mental flow' which is characterized by high connectivity, frequent successful usage of the system so its completely imported into working memory, organization such as applying probabilistic connections to base more trivial changes around core probable connections, and lack of barriers to connectivity such as random noise, while still allowing for variation in inputs, which are optimally routed in this sub-network of working memory enabling states like 'flow'
            - other errors such as 'surprise even when the input is normal, bc its necessary to believe falsehoods (trick the brain) to survive shockingly negative frequent truths, so normal inputs continue to trigger surprise'
            - other errors that could be formatted as 'neural states' like 'loss of executive function' (where the 'outer neural layers and available neurons enabling consciousness and executive control are disrupted or used for other tasks as a result of required functions or neuron loss in other regions, where alternate executive function structures take the form of interim networks that connect and control connections between sub-networks like between available neurons and task-specific grouped neurons like memory storage neurons and working memory neurons')
            - 'memory' (like a 'directed network of neurons with similarities in interface structures like patterns and concepts that efficiently store most useful information', as well as 'working memory' as a 'set of available neurons distributed among neurons used for specific frequently used information' and 'recent memory' as a 'set of neurons storing recently encountered info as a proxy for relevant info', and 'abstract memory' as a 'set of neurons to store commonly used general rules which are easy to remember and frequently used and capture high variation and are abstract enough to be generally applicable' and 'physical memory' as 'neurons spread through the bio system to store repeated sensations and connections between them', and 'neurons which remember feelings in the brain like feelings about blood flow as a signaling system to trigger other useful processes, like immune processes where blood flow is inadequate' and other memory types)
               - similarly, 'memory-finding functions' like 'think about (apply functions in, apply differences in) context when other related info to the memory is known, like where a memory was originally stored, in order to find the memory'
            - 'brain waves' as a 'synchronization process where neurons are required to temporarily coordinate in some different interaction pattern like a different neuron/memory access pattern, with each other and with other neuron groups coordinated by some other wave or interacting with themselves as theyre pulled in a different direction by the opposing waves, for functionality like "locally distributed scaled interaction functions fulfilling some general intent, like regulating/contradicting/limiting some other waved/mass-coordinated/temporary process"'
            - similarly, connecting 'biases' with neural structures like 'dedicated subsets of neurons for a particular intent (like a memory network storing recent information) leading to biases (like recency)' and 'locality/self biases (through sensory inputs about local information being highly prioritized in the brain)' and 'similarity biases (like familiarity (as in self-similarity or recent/experience/memory-similarity) to encode minimal new information)'
            - similarly, memory formats can be useful to design a neural network using a mix of memory formats
                - like a 'concept and a structure indicating a variant of that concept when applied to the concept', such as deriving the concept of 'austerity' through concept of 'restraint' having applied a related structure like 'antidote' which is structurally similar but not semantically related, where the concept encodes the semantic relevance and the structure encodes the structural similarity needed to connect the input query with the output word 'austerity' which is a specific variant of 'restraint' that is useful to store in memory for various intents, as opposed to only remembering the concept of 'restraint' or 'limit'
                - similarly, identifying optimizations to memory storage mechanisms, like 'abstract' (abstracting 'antidote' to a 'solution') or 'connect unrelated structures which are otherwise useful' (like connecting 'austerity' to unrelated but otherwise useful words like 'antidote' by connections like 'austerity being anti-equivalent to a solution like an antidote' which has layers of relevance)
            - similarly, attention-maximization mechanisms such as 'emphasis' are useful through applying known attention insights (such as 'emphasized words are paid more attention to' by applying the variable of 'cadence') to solve attention errors like attention deficits such as might occur in cases like 'lack of risk' (where normally 'risk' is the trigger of attention, but its still useful to pay attention in 'cases with low-risk' as in 'boredom' structures, cases in which attention is difficult to trigger), and similarly other structures can be applied to solve the problem of 'lack of risk' (like how 'games' are by definition 'low-risk' but still incentivize paying attention through various mechanisms like 'accessible rewards' such as 'imagined/otherwise false risk'), and similarly other solutions like 'saying thoughts out loud or writing them on paper' as a way of 'forcing paying attention'
            - similarly, brain states like 'boredom' can be applied as a solution metric for the 'neural network architecture/algorithmm search' problem, as both an optimization structure (a 'bored brain' is a brain that has solved some problems or reached a case where there are no problems to solve, so that case should be applied as a possible optimal input case) and an error structure ('boredom' can lead to other errors like 'lack of creativity' and 'increased capacity to try sub-optimal/error cases/functions to find structures like variation/problems/other complexity structures which may be suboptimal')
            - 'bias' structures are useful to identify, as they optimize for some metric, like how 'bias toward interactive structures which can be interacted with' optimizes for intents like 'adjacently identifying useful variables for local intents' and the reasons why these biases are optimal in some way, such as that the 'bias toward interactive structures is related to the local bias' and the 'interactive bias is useful for local intents' and 'its also more useful to optimize for local intents in some cases like where error interactions are probable such as when opposing-direction vehicles are passing each other, in which case its justified to do more computations regarding local structures like local agent intents/inputs/errors'
                - similarly, bias structures are useful in that they can often be adjacently combined in some network where they offset each other's over-prioritization errors (similar to how combinations of high-variation/abstract/otherwise useful concepts like 'power/balance' can be adjacently combined in a network to offset each other's over-prioritization errors)
                - other structures should be applied to offset different error structures than 'over-prioritization' errors, as this is just an example of a 'solution structure to that error' and the other structures that solve other errors should be applied in combination with this one solution structure
                - relatedly, its useful identify useful structures like 'errors of brain mechanisms like biases' such as how 'errors' could seem like a 'requirement/certainty' if a function is applied to 'identify errors' repeatedly, so that the existing brain mechanisms like 'memory' misinterpret it as a 'certainty' structure through its commonness/familiarity/recency/repetition

        - insights about brain usages/interactions/functions through connecting structures like 'feedback' are similarly useful to apply in an optimized neural network
            - the insight that 'existing reward mechanisms are too easy to game/rig/guarantee to incentivize complex problem-solving' can be applied with a solution to that problem (such as 'connect reward mechanisms to problem-solving processes' and 'inject variables like problems in an organized structure like "sorted by increasing complexity" to require problem-solving processes')
            - similarly, 'existing reward mechanisms' dont account for error structures like 'randomness' (rewards can occur randomly, which can be useful for learning complex variable interactions leading to false randomness, but this is not incentivized, as the reward already occurred prior to a decision to identify the falsely random procesess that resulted in the reward, and the reward may seem too random to examine for possible stable variable interactions to identify) and other 'negative' structures like 'corruption' ('rigging/falsely guaranteeing rewards') resulting from 'existing reward mechanisms'
        
        - in reverse, connecting interface structures to possible neural structures that could fulfill them
            - the 'insight' structure in the brain is related to a structure like 'liquid waves' (which could be related to blood flow or electricity or brain waves), which are evocative of a crossing of a threshold, above which momentum is such that it causes an 'organizational cascade', similar to how once the threshold between function layers is crossed (by identifying/generating new structures on the outer layers), the outer layers provide more complex structures that capture higher variation, which can be used to trivially organize structures on the inner layers (an 'organizational cascade' occurs when the new complex variation-capturing structure is identified, which easily organizes other structures), where this momentum is required to identify some new structure
            - 'derivation' structures like a 'mental simulator as a set of neurons that is regularly emptied, so different intents can be fulfilled by importing different structures like "different available core structures" regularly to connect known structures in a different interaction function to find new structures' (interim thinking)

        - also other variables can be injected in neural networks (like 'constant once identified as useful for success' as opposed to inactive/active, as a neural network node type that doesnt get updated once its identified as a contributor to some successful change during training, which is likelier when variable-sized changes are allowed by the update algorithm as opposed to prioritizing adjacent changes)

    - identifying useful structures like 'solution filters' (as in 'checks') that can be applied to solve specific problems
        - for example, identifying that the earth is not flat (a 'default assumption') is a matter of identifying useful filters like:
            - identifying required connections (if the earth is a different structure like round, there would be people who had traveled in the same direction around the world, or from connective subsets of the path)
            - testing a core point on a unit structure ('if a round ball can support a structure on it once the structure is scaled down to be sufficiently small, the earth theoretically could as well') that disproves a core counterpoint (the 'flatness' would be the 'foundational structure' required for structures on it not to fall over, would be the default argument contradicting a round earth)
        - relatedly, alignments tend to resolve ambiguities, such as how alignments between function intent and usage resolve the ambiguity of how the function is optimally used

    - identifying useful structures (like existing solutions and variables/causes of them) to connect with other useful structures (like new solutions)
        - for example, other useful inventions solved a problem that reached a threshold where it was common and there were tools available to fix it if enough manual labor was done, addressing some problem of reality like 'scaling the functional hours in the day' (light bulb), 'reaching enemies farther away' (gun), 'increasing the speed of information transmission and of vehicles through more explosive but controllable materials' (telephone, engine), which were problems of scalability, which is a core structure of most problems, and which created other problems of scalability like 'removing pollution from lungs' and 'verifying transmitted information faster than false information can be distributed'
        - predicting new inventions is a matter of identifying problems that would be solved with more scalable structures of existing solutions
        - similarly, predicting new inventions (and their suboptimalities/errors/optimizations) can be done with other relevant high-variation variables like 'agent personality', such as how identifying error structures in a personality like a 'false assumption/constant' or an 'over-prioritization' error or a 'self-contradiction' error or an 'over-used core/interaction function' error can be identified from a personality, such as how a 'hypocrite' personality reflects an error of 'local bias' (specified through a structure relevant to neural networks of 'avoidance of applying analysis/optimization functions to the neural network itself that produced this error', so an 'optimization structure' of a 'neural network' involves including a function to handle this error structure), these personalities involving variation around core types (based on core personality interaction functions like 'judge/analyze/criticize', 'test', etc), just like intents vary around core intents (like 'safety', 'risk-minimization', 'scaling', etc), as personalities are 'optimization' structures of 'brain usage' structures (as in a 'style of brain usage' since its a variable like 'driving style' as not every type of interaction with the brain is required/constant) that solve some problem (like personalities related to brain disorders like attention deficits/hyperactivity that account for these brain errors, such as 'saying thoughts out loud to force paying attention to them', where the personality results from some usage of this solution to the brain error, such as being particularly talkative/social through speaking more, or more thoughtful as a result of paying more attention to thoughts with corrective measures like 'speaking out loud or writing thoughts down'

    - identifying useful structures like 'repetitions' and 'scaling' and 'core changes to core structures' that can adjacently identify other useful structures
        - similarly, useful structures are useful in connecting other useful structures, like a function (like a spiral, as a 'repeated change that creates other change structures like change rate changes') that intersects with and therefore connects every useful function set layer in a function layer diagram, a connection from which other functions on that layer can be derived (similar to how a simple straight line that abstracts away some attribute/functionality with each step could intersect it if the layers are known/understood or defined/required to be organized by being increasingly abstract), and similar to how 'core cross-interface structures' have a 'vertex structure (of two vectors with an endpoint in common)' on the system layer diagram of interfaces and may also have a 'similarity in vertex angle', if the graph is organized by a similarity of the core interaction in these core cross-interface structures, which would be an example of a useful organization method for identifying other useful structures using that similarity
            - relatedly, structures that occur across problems like 'spirals' can be a different type of useful structure in that they can form vortexes, which is similar to how interfaces attract change
        - similarly, identifying 'core changes to core structures' (like changing position to top of a stack) can identify other useful differences like 'scaled' structures (like far-away structures, like to 'identify water from far-away by scaling a mountain, or identifying clouds that produce rain and cloud movement direction, or identifying lower elevation areas as likelier to contain water and identifying errors of input like false equivalences to identify lower elevation areas even when blocked by an input block like a physical barrier, or identifying concepts like "diversity" which map to useful structures like "variability" which can direct attention, such as through "searching for differences from the current error state, in variables like color" such as identifying green areas which are likely to have water, or concepts like "connectivity" or structures like "networks" to identify "underground networks of water" as a structure to search for or stay within some range of, or identifying plants likely to have water such as through "functional requirements (like water storage)" which adjacently identifies "thickness" as a useful variable to identify, and similarly identifying the "opposite structures of water" (such as a mirage) which can be identified through moving in a way that is not "moving closer" to filter out default errors such as that 'water evaporates by moving closer to it' where 'motion other than moving closer (like moving position of inputs but not position of agent)' could also disrupt the mirage, but would filter out the error structure)
        -  identifying core differences that can be repeated and can interact like scaled life forms to identify other scaled structures like causally distant (past/future) structures (such as identifying that 'dinosaur fossils' could exist by scaling the core structure of 'dust deposited from air' and 'scaling size of existing animals, which might exist at a different time bc of coexistence impossibilities', which is useful for identifying other useful structures like 'asteroids' and 'evolution')
        - finding the full set of example structures that can create surprising real differences when combined trivially is a useful problem-solving intent

    - find similarities in workflows with similar solutions found so different workflows can be found and checked for different probability of finding diifferent minima of the error function
        - similarly, finding overlapping optimals can identify positions without overlapping optimals
        - relatedly, identifying 'variable sets which can be combined in any way to still produce a solution' is a useful set to find, as is a 'variable set that can still produce a solution when an item from some set of terms is added/removed', just like finding common similarities between solution functions (like similar high-power terms) is a useful intent to fulfill
    
    - identifying useful structures like slowdowns/constants as proxies for minima in the error function, and identifying reasons for these slowdowns/constants so they can be connected and differentiatied as needed for various intents
        - various limits can produce a slowdown in other systems which are useful to identify (variants of a limit, as in a 'limit on change potential')
        - identify 'areas of slowdowns' is more possible with some functions than finding 'minima' and these structures fulfill similar intents
    
    - identifying useful structures like connections like that cross-interface structures can be equivalent alternates or symmetries used to build maps and similarities or build complementary info usable to build full sets of differences, to connect very similar or different info usable to solve a problem, which is why cross-interface structures occur in solution workflows, as these cross-interface connections are highly similarizing/differentiating as they connect differentiating structures
        - for example 'function-intent' and 'position-speed' are useful differences to connect and apply in a set, such as how its useful to regularly apply a function to 'check for a more optimal interaction with a core variable (like position)' to connect a function/variable with a useful structure like an optimal usage intent (such as a function to 'find an opportunity to change relative car position' to fulfill intents like 'pass a slowpoke' which can be a required intent to add to an algorithm that accounts for speed/position/far-away change predictions, as 'changing relative car position' is so useful it may as well be a required function to include with any 'driving' method), similar to how knowing the intent of a question can evaluate whether an answer should be found (whether the function to find it is useful to use)
        - similarly, identifying useful intents is useful, such as 'connecting local error minumums with generally useful functions' and 'connecting useful differences in definitions', like how 'function network' is an inadequate but simple description of ai that is more useful when grouoed with intents to solve for, like 'connect many changes to one change' and 'connect complete changes to composing changes' to fulfill intents like 'identify which changes (of the many used to create a particular solution function) are most useful in a particular successful change of a solution function'
        - similarly, 'problem-solution sets' are high variation in that they by definition 'connect differences in a useful way' and can be applied as a structure to solve other problems as a result of this variation-resolution
        
    - identifying useful structures (like differences from interfaces and causes/components of interfaces) which can be used to identify other useful structures (like interfaces)
        - identifying that symmetries are a form of irrelevance (where one variable cannot change the other), so that 'areas of irrelevance' can be identified, such as 'info that can replace other info, like equivalent alternates, so that the other info is not necessary and is irrelevant' which are possible interfaces, just like 'equivalent alternates' are also useful structures to find other interfaces, to find structures like 'positional irrelevance' (the 'position' doesnt matter, what matters is 'any membership in a set/type at all') and other forms of irrelevance which can identify alternate xtructures (like 'similarity/solution metrics') to be used, similar to how identifying alternates to 'sequences' in the 'predict the next word' problem space is useful in that it can identify structures like 'common sets like common phrases/patterns' which can take priority over or replace sequences
        - similarly, 'causal irrelevance' occurs where the 'causal structures' dont matter, what matters is 'whether some event occurs at all'
        - this form of irrelevance is like the opposite of an interface, where the structure's irrelevance indicates that the problem is solved or more solvable on a different interface

    - identifying useful structures (like 'mixes of workflows') that are other interface structures (like 'optimals' or 'requirements') in some cases
        - for example, its useful to identify position and speed but also mix that with another method like 'check farther ahead than is necessary' where that method is mixed by applying it regularly, similar to how a method to 'remain some distance from other structures' can act like an equivalent alternate of that mix, where these methods are better when mixed together to avoid their known errors like 'avoiding structures by not moving at all' where a network of comtexts could allow identifying the contexts' similarities to quickly check if a direction leads to an error
        - identifying insights like that "a 'mix of algorithms' is optimal in some cases, such as where one algorithm will inevitably be wrong at some point, so mixing it with another algorithm at regular intervals can handle this error" is useful as a set of core interface structures which are inherently useful to connect
        - for example, when guessing a route with no instructions available except a general direction to move in, 'following one road' is likely to be incorrect in some way like inefficiency or at some point like where it diverges from the optimal path or the direction that represents the problem to solve (the problem of 'find a path that fulfills this direction'), so injecting a variable like 'changing to other roads' is likely to improve on the "first guessed road" if extrapolated to the entire path
        - similarly, other insights apply to point to this insight, such as where 'structures rarely self-sustain, so assuming one structure will always be reliable is unlikely to be correct, as structures interact without being instructed to and these interactions create degradations of some variables'
        - similarly, other workflows can identify other variables adjacently through identifying variables of a problem space and the interface structures connecting these variables which could be new workflows (variables of a problem space like 'driving' such as 'interactive structure differences like surface differences and usage differences' to identify other variables like 'road texture' as important for safe driving where the solution is swerving to avoid differences in road texture if swerving is sustainable over time, and other usage variables like 'fatigue' and 'lack of requirement to use both hands when steering at all times' which when combined allows for useful optimization functions to be identified, such as 'switching hand used to steer' in cases like 'where next car is far ahead so swerving to avoid a crash is less likely to be required'), workflows such as identifying useful sets of structures like 'error-causing inputs (fatigue)' paired with 'structures of cases (where "next car is far away, so errors like crashes are less likely and more variables can be used") that include a "lack of requirement" (as in a variable, "steering hand currently used") that allows for an adjacent change (switching steering hand), where this adjacent change solves (reduces) the error-causing input (fatigue)', and similarly, identifying generative/identification functions of these variables (apply useful concepts like 'interactivity' to identify specific variants of it like 'surfaces' and errors like 'crashes' and 'usage structures (like driving style which is a high-variation variable of how the agent interacts with the driving tools)' and other useful structures like 'variability causes' and 'error causes' to find these useful concepts like 'interactivity' as a useful specification of 'variability' which is more useful to store than to derive from 'variability' every time), which is useful for problem-solving intents like 'identify new variables (for intents such as to explain a variable interaction not already explained by known variables)'

    - identify useful structures (like 'scaled interactions' and 'time interactions') which can be used to fulfill problem-solving intents like 'identify more useful workflows'
        - for example, the 'trial and error' workflow is associated with a 'multiverse' universe (as the ultimate scaled version of the 'trial and error' algorithm is like building a time machine which can test/simulate all possibilities as good as or better than reality can, so that any moment (and any sequence or other structure of moments) can be examined using the 'trial and error'-implementing program) and some structures are particularly useful to identify for this algorithm, such as 'equivalent alternates having some pattern that allows them to be derived from each other' and other structures that could reduce the computation requirements of 'trial and error' (to equate some possibilities, so that the set of possibilities considered to be equivalent to 'all possibilities' is reduced)
        - any problem-solving workflow is similar to a time machine in its reflection of the variables of reality (as these workflows connect variables in a way that all or most of reality can be determined/simulated/tested by it), but some are more optimal than others, and identifying all of these workflows which can simulate reality builds a more complete time machine (in the sense of a simulation machine) than other methods, and similarly any position where an observer has access to such an optimal machine that reflects reality accurately and efficiently is likelier to be a higher-variation position that attracts information and variation, which could bend spacetime in unexpected/unknown ways, thereby impacting other positions' ability to simulate time/reality through requiring them to simulate it similarly efficiently as the original observer or be subject to the spacetime changes created by the original observer, where the original observer has a tool to process high variation interacts that they are likely to attract with this efficient handling of variation and other observers do not, which is likely to create inconsistencies in how spacetime stabilizes/bends that are resolvable if the simulator is distributed, inconsistencies which can store other universes' information inadequately/suboptimally (other universes being likelier to be attracted to and stored in this universe, the better the variation handling becomes), so once this optimal simulator is found, it should be as distributed as possible and the distributed versions of the simulator should be differently optimized to handle more variation in information attracted to this universe, so there is a structure in place to capture it once its pulled to this more efficient storage mechanism by forces like gravity, to avoid these 'suboptimal universe storages' from occurring, and other strategies to minimize these errors should be applied (like 'inefficiency signals' should be distributed as well to avoid attracting too much information to this universe before the implications of doing so are fully understood)
        - identifying that this algorithm in its ultimate form is like building a multiverse is non-trivial, unless a program identifies the 'meaning' of a structure (such as its set of high-variation interactions in various important/relevant systems), at which point identifying this interaction is a matter of applying 'scale' and the 'physical information' sub-interface of the 'information' interface and identifying 'alternate usage intents' of the resulting scaled structure which could be similar to some structure in the 'physical information' interface (like some variant implementation of itself such as a 'multiverse')
        - similarly, identifying whether some algorithm more efficiently captures reality than reality is a matter of identifying whether there is some simulation of reality that requires fewer representations (such as fewer repetitions) while still correctly predicting every important variable interaction (by identifying priorities which are important and fulfilled by that algorithm's simulation of reality) and other metrics (like if there was such an efficient simulation of reality, it would likely vacuum information in as it would store/interact with information better than reality, and other realities that existed prior to this computer would likely mimic this efficient reality and thereby standardize and synchronize realities to that more efficient reality)
        - therefore identifying how algorithms interact with universe structures as well as other useful inventions (like blockchains and time machines and neural networks) is useful to identify, just like identifying how algorithms interact with good/evil and truth/falsehood and problems/solutions and power/balance are also useful to connect each algorithm to, as important metadata of the algorithm to compute
        - 'structures (like "sequences") of problems to solve' are similarly useful to identify, to direct how these workflows are optimally used, such as a 'sequence and distribution of databases (of structures like "task lists" and "known variable interactions" and "useful problem-solving workflows") to create, which will push civilization in the direction of some priority like fairness, organization, efficient, etc'
        - the 'database sequence (or database network sequence)' is identifiable as an available structure reflecting a 'state sequence' which is the 'core structure' of the 'physical information' interface (reality), which is usable as a component to build a path to a particular future and connect other spacetimes as well
        - what does it mean that workflows (like trial and error) relate to possible implementations of reality (like the multiverse) through similarities (like the 'computation type required' in both 'trial and error' and the 'multiverse')? it means that some universe configurations can be filtered out as impossible or less possible using these workflows and structures of them like combinations of them
        - given that multiple workflows exist and are optimal for some purpose, and there is no 'general purpose, always optimal, in all cases' algorithm (except interface analysis which produces all of the optimal algorithms), the universe is likely to reflect the structure of 'a set (and an interactive mix) of these optimal algorithms' (for example, a multiverse, but an optimized one, with several 'equivalent alternate' universes which overlap or otherwise intersect on their symmetries/common standards), where some interfaces reflect each other as they capture the same information and can be used to derive each other
        - this is different from saying that an 'algorithm operates best in a particular context' (like a 'worst-case scenario' context as indicated in the 'algorithmica' or 'heuristica' world of the 'five worlds' of complexity), as 'system context' is an input to algorithms, but there are 'absolute contexts' where some algorithms would be optimal in all cases and those absolute contexts can be extrapolated to differentiate those from physical reality
 
    - identify structures which are useful for identifying other useful structures like 'maximal differences' and 'equivalent alternates' which are useful to identify variables and thereby fulfill other problem-solving intents
        - for example, different senses provide complementary or equivalent information, usually related to some 'feedback about position change' (such as a 'reward/pain feedback' sense and a 'gut sense of nearby life forms involving microbe-microbe interactions' and a 'health sense of functionality that is working' and a 'thought sense of triggers of useful thoughts like considering alternatives' and a 'sense of synchronicity/alignment between different internal systems' and a 'logic sense of information such as time passed, which indicates survival, which indicates some useful interaction with the external world', regardless of whether the primary senses are available), where different structures can be adjacently derived from different senses and some can combine to form equivalent alternate information (useful for finding different interfaces for information) or complementary information (useful for building a complete set of cross-interface information)
        - similarly, 'connections between maximally different graphs' are useful to identify 'equivalent alternates', 'interaction levels', 'differently useful information', and other useful structures
        - similarly, the question of "identifying what is not an interface (such as 'other interfaces', 'limits', 'interface components', etc)" is useful to identify useful variables, which are useful for many problem-solving intents
        - similarly, the question of 'what errors produce useful functions in neural networks (such as concussions in an autistic brain, which might add connectivity if a coincidental angular interaction occurs with the concussion, a useful input to consciousness and other functions)' can identify 'structural errors connected to functions' in neural networks

    - identify useful intents to fulfill such as "identifying interactions between core interaction functions or problem-solving functions/intents and cross-interface structures" (like 'identify structures that change functionality of structures' and 'identify causes of different functionality in similar structures and similar functionality in different structures') which are specific variants of problem-solving intents (like 'connect different structures') and therefore are more useful as approximations or implementations of those intents (fulfilling those two identification functions could solve a high ratio of problems)
        - related intents include identifying useful interaction levels of useful structures like common concepts/structures such as 'complexity' ('high count of function usage to fulfill required intents') and 'embedding' ('structure enabled by a base structure') and 'heterogeneity' ('difference embedded in a structure, as in non-uniformity or non-unitary interactions'), which have some structures in common like interaction functions (apply/base) and cross-interface structures (function-intent, usage-intent, usage-function, etc) which indicates that other cross-interface structures interacting with core interaction functions could identify other useful concepts
        - this useful interaction level implies that identifying more advanced/complex interactions (such as iterated embeddings) is useful as a mechanism of evaluating simpler structures (identify 'embedding an embedding in an embedding' to evaluate 'embeddings in an embedding' successfully, as an 'outer layer of consciousness, where the inner layers fulfill requirements and the outer layers are free to evaluate the other layers as they can store the inner layers or queries of them in their own layer, given the ratio of free neurons available to import structures into for evaluation processes, in the outer layers')

    - identify useful structures like high variation variable interactions like specific difference structures or specific variable interactions that co-occur to identify other useful structures like 'probable variable sets'
        - identifying interactions between variables like 'difference structures' (such as 'homogeneity', 'heterogeneity/differentiation' and 'specialization', as 'highly different structures' (like different senses or species) tend to occur in real systems rather than 'highly similar structures (like high ratios of repetitions)') which tend to occur across structures, and similarly variable interactions like 'interactivity' and 'variability' which tend to co-occur (like how liquids tend to be higher variation than solids bc of the interactivity within the structure), to create other useful structures (like 'directed attribute graphs of various difference types like 'heterogeneity' where direction indicates sequence and preceding terms in the sequence indicate probability' which can be used to identify 'probable variable sets')

    - identify useful interface structures (like 'type interaction functions' and 'complementary information') which can be connected to other useful structures (like 'change inputs') which are known to be connectible to other useful structures (like 'differences in solution functions')
        - for example, attributes/functionality interact in a way that changes attributes as theyre not static constants (which is why multiple solution functions are better to identify than one in complex systems), such as how a type may mimic other types to hide its identity to opposing agents and therefore seem like other types that it has encountered or which are less antagonistic to the opposing agent, or the 'type' functionality expectations may be valid for only some subsets of inputs (like 'during specific time periods when the type is checked or when the type is first initialized which is when its identifying attributes are most extreme, after which its likelier to vary from these identifying attributes than to maintain them perfectly'), where functionality (like 'type-mimicking' functionality) can act like complementary information to the attributes (like the 'type') as it can change the attributes so its better to have both attribute/functionality information
        - this cross-interface structure of 'attributes/functionality' can be used to identify other structures like 'change inputs' (like 'causes of a variable interaction change to have a different possible state'), similar to how other relevant interface structure interaction functions like 'type merging' or 'default/backup types' are known structures, and can be applied as constants to check for type interaction changes over time or which are evident in the original data set, as these interface structure interactions are connectible to differences in solution functions for a particular variable interaction
      - this is related to how a 'density' can be mapped to various interface structures (like an efficiency, a type, a certainty, a requirement, etc)
      - similarly, identifying useful connections like 'complementary info of adjacent info' is useful for 'creating other connections' and 'identifying structures that can be applied as a certainty' (as in a 'base for change')

    - identify useful structures like common variables of useful structures like mappings between 'common useful structures', 'functions', and 'solution automation workflows'
        - for example, the 'symmetry' structure has a related workflow like 'change a base solution', similar to how the 'limit' structure has a related workflow ('fulfill limits such as requirements/solution metrics' or 'limit the problem structures/inputs/outputs') and the 'connection' structure has a related workflow ('connect useful structures like interactive structures or problem/solution structures') and the 'shortcut' structure has a related workflow (like 'find efficiencies and maximal differences and other components of optimization structures'), and all of these structures have related functions ('differentiate around a similarity', 'limit/filter/reduce', 'connect/standardize/compare/similarize/differentiate', 'map independent systems/connect maximal differences')
        - these are related bc they are alternate variants of each other (the structures are variants of the functions, and the workflows based on these structures/functions are 'solution automation workflows' bc these structures/functions describe core differences of reality)
        - 'finding variants of a useful structure' makes it likelier to be possible to trivially connect that structure to problem/solution structures like solution automation workflows

    - identify useful function sets that can connect sufficiently different, disconnected/uncorrelated structures (like independent variables) that they would likely contain most important variable interactions in their connections
        - for example, the important variables of positive/negative, true/false, certain/uncertain, finite/infinite, similar/different, useful/useless, and stable/unstable are extremely uncorrelated/independent and different and are therefore useful to connect, so much so that connecting all of these variable values in all their possible useful/powerful connections could act as an alternative to deriving other rule sets to identify the rules/limits of real systems or just the complete rule set of the interface network, as some of those attributes encode a primary interface concept like 'information' (certain/uncertain, true/false) and some are concepts that can describe reality like stability/usefulness or structures which are commonly useful like limits (finite/infinite), and different sets of these structures/variables/concepts can be similarly useful in capturing the variation of reality
        - this is related to how finding a map between very different systems (like biology/physics) is useful as a way to derive a sufficient ratio of real rule sets that it can be used as a proxy for a complete rule set in many cases

    - identify solution structures (like solution metrics, solution set size, interactions with other solutions, and other solution info) by applying useful structures like similarities/differences in the problem system (and across problem systems)
        - for example, in biology there are usually multiple different pathways (the solution functions) for the same mechanism (output), so there are multiple solution functions to find, rather than one, and these solution functions may be trivial to generate from each other, after identifying other variables (other required inputs and the connections between them), as the solution functions often seem falsely very different until one connection between variables like some subset of inputs like 'elements and their adjacent functions' or some pattern in the variable interactions like a 'common threshold all of the solutions cross' is identified
        - similarly, there is often a 'backup' and a 'default' solution in biological systems, indicating that there are usually multiple solution functions to identify in many real systems (like a way for different structures like ratios of elements or different sets of elements to produce the same functionality when one is at a suboptimal ratio, and if you had the 'variable interaction' info between the 'requirements' and 'functions' of the system and these 'element sets' which have some useful similarity, the 'other element sets that act like another element set' would likely be trivial to identify, which is a useful set of cross-interface variables to identify the interactivity between which can explain high variability in suboptimal conditions that vary by input structures like ratios of elements, which is a high ratio of conditions)
            - for example, if you had the 'variable interaction of elements and functionality', youd have info like 'sulfur is a required input for central nervous system functionality' and deriving other info like 'desulvofibrio bacteria (which reduce sulfur) could be related to central nervous system functionality disruption (like parkinsons)' is trivial once you know that element-functionality interaction and the related pathogen-element interaction, as the 'element inputs' are a surprisingly high-variation capturing sub-interface of the biological system (bc of its surprisingly extreme limits in its optimal input ranges) and can explain a high degree of variability in pathogen/condition response, so connecting 'elements' with each other and with 'functionality' is useful for deriving these 'input-condition connections' and 'equivalent alternate element sets'
            - similarly, other common variables of pesticides associated with parkinson's can be found with variables like 'interactivity', 'variability', 'persistence', 'negativity of side effects (known toxicities)', 'spectrum (across species and across systems) of negative side effects', 'connection to required components of health specific to nervous system', 'similarity to other known substances with negative nervous system effects', 'extremity of effects', 'multiplicity of uses/effects', 'uniqueness of functions/effects', etc (while recognizing that they dont have to have much in common to have similar side effects in a specific system, as 'structural similarities' arent the only useful structure to identify)
                - dicofol - has other negative systemic side effects (carcinogenicity, organ damage) but also specifically nervous system damage (related to parkinsons), is known to be toxic to aquatic life
                - trifluralin - is known to be toxic to aquatic life as well as extremely variable interactions (meaning its likely to be harmful in some way) and unusual attributes (meaning its likely to be different from known effects such as negative, neutral/harmless and/or positive/coordinating effects in some way, 'difference from positive' meaning 'negative'), has 'broad spectrum activity' and 'core interactivity (interfering at a core stage of development)', has similarity to other toxic substances (like 'explosives'), deactivated by water (a core bio-system component, indicating an extreme difference from the host bio-system)
                - copper sulfate - has broad spectrum activity (fungicide, molluscicide, algicide), toxic to aquatic life, highly interactive (reacts with many different substances), extreme attributes (colorant, variability in color, multiplicity of uses, many different related forms), identified as a toxin by human immune system, related to 'sulfur' (key nervous system component)
                - folpet - chemically similar to other toxic agents (trichloroethylene), powerful extreme compounding and multiple mechanisms of functionality ('fungicidal')
                - endosulfan - persistence/robustness, known to have negative systemic side effects (endocrine disruption), broad spectrum of activity, unusual mechanism, contains at least one component of other toxic substances (trichloroethylene), produces highly interactive/reactive substances ('reactive oxygen species'), related to 'sulfur' (key nervous system component)
                - naled - broad spectrum of activity (kills birds, pesticide), known to be toxic to aquatic life, chemically similar to other toxic agents (trichloroethylene), deactivated by water, toxic with trivial transforms (acid), highly interactive, has known negative nervous system effects, has known negative systemic effects (inhibits key nervous system enzyme)
                - propargite - broad spectrum of activity (kills fish/amphibians, pesticide), known to be toxic to aquatic life, has other negative systemic side effects (carcinogenicity) 
                - endothall - broad spectrum of activity (herbicide, pesticide), known to be toxic to aquatic life, disrupts intestine health, destroys useful inputs (dessicant to plants), inhibits a useful structure (tumor suppressor enzyme)
                - diquat - broad spectrum of activity (herbicide, pesticide), produces highly interactive/reactive substances ('reactive oxygen species') with trivial changes, persistent/robust, known to be toxic with other mechanisms, destroys useful inputs (dessicant to plants)
            - similarly, identifying other useful structures is trivial like the 'connection between high-variation variables (like different sub-systems like gut/brain in the biological system, to infer connections like "check gut microbiome pathogens by default for brain dysfunctionality", and connections between similarly high variation, high coverage, systemic variables like "input/food processing" and "mutations/conditions" which are similarly high variation and could therefore be adjacently explanatory with a trivial transform)'
            - the 'high variation variables' act like examples of solution metrics (as in 'common structures of solutions, like high-variation variables frequently are') which can be used to derive the others (as all the high variation variables interact so they can be formatted as different nodes on a causal sequence and derived from each other as they capture similar info variation), answering the question 'what is likely to cause this variable change' ('apply a high-variation variable filter (check the high-variation variables first)', as 'high variation variables' are a way to adjacently connect different solutions involving high-variation variables, and thereby derive the others from the first one identified, using a 'high variation variable-connection function')
            - relatedly, other sub-interfaces in the system could also explain the output mechanism, as errors (conditions like mutations) can also create functionality (since other sub-interfaces in biological systems can use inputs like element sets/ratios differently, and these sub-interfaces like pathogens are interactive with other biological systems) as well as intentional inputs (with intent to create that functionality)
            - 'how to connect the functionality of important inputs (elements) and other useful structures to connect (requirements, functionality, response to conditions)' is a useful structure to identify as being capable of capturing enough info and variation in the system that it can act like a proxy for other useful structures like a 'full set of all important mechanism inputs (like element sets or state triggers)'
        - another difference is that some different variable subsets can explain the same mechanism (which occurs when variable subsets act like equivalent alternates), and sometimes different functions in the same variable subset can explain the same mechanism (like when an over/under-supply of some nutrient produces the same output condition bc the area of its optimal value is so limited and fragile, like blood ph), which is useful to identify in the input problem system
        - this info about solutions is useful for identifying other useful info like patterns that occur across solutions (such as equidistance/intersectivity between solutions, similar rank/structure, solution set size, similarities in variation of variable subsets that can act like equivalent alternates) which can be used to identify other solutions once one is found, where finding one solution is trivial in the case of having many different solutions and identifying the connection between solutions is more useful in understanding the problem system than identifying one solution
        - identifying 'different input sets that produce the same solution' is useful to identify other useful structures like 'other different input sets that produce the same solution', the 'causes of similarity in outputs, across differences in input sets', and 'similarities/differences in inputs that would produce different solutions'
        - identifying certainties is useful to apply as info 'filters', similar to how applying a 'known similarity as a symmetry (around which differences can be applied)' is useful for building structures in a graph that reflect other certainties (structures like similarities/differences) of reality and is therefore useful for finding those reflected certainties
            - this is bc 'certainties' by definition/interaction map more adjacently to 'structures' (such as useful 'standards/filters', 'combinations/sequences', 'sets/densities', or 'graph organization structures like partitions/connections/nodes/layer/boundaries/intersections or graph usages like graph queries') than 'uncertainties'
            - finding the 'full set of graphs that apply a certainty in a way that reflects its real structure' (the certainty applied as a filter, as a base, as a density, as a network connection function, etc) could be a useful workflow for finding 'other adjacently related certainties, as reflected in these very different graphs that have the same certainty embedded'
            - an example of a useful filter combination is 'ratio/probability' and 'positive/negative' (or 'probability' and 'cost/benefit'), such as how identifying common variables of positive responses to negative inputs can be done by applying definitions of positive/negative (health structures vs. conditions/mutations/requirements/attributes like fragility) and identifying other 'similar' structures (like how identifying 'rare health structures' can be useful for identifying causes of 'rare positive response to negative inputs', and identifying specific examples of 'rare health structures' can be found by applying variation to the definition of health structures, like 'extreme athletes', 'extreme diets having healthy components like raw vegetables', 'extreme adaptations like high performance', to check those positive/healthy subsets first for the positive/healthy response, the extremity of which can be useful in identifying obvious differences, extremity being a useful truth structure through its connection to commonness/probability/stability), and implicitly applying a 'similarity' in positive/negative ratios/probabilities across variable subsets ('rare good health is associated with other rare good health structures like specific healthy responses' as a default solution structure to check first)

    - identify useful functions which are useful for some useful problem-solving intent like 'identifying other useful structures like specific functions that fulfill some different intent/metric optimally'
        - for example, 'control' and 'replicate' are two general/specific variants of other functions ('reduce/limit' and 'copy') which can be used to change the problem structures in a useful way, where 'control' is useful as a more general alternate function which has many variants that could be useful specifications to derive (like 'limit' or 'reduce' or 'isolate' or 'use (as in trigger)' or 'change' the problem or 'convert the problem structures into a constant or a simpler form, so its more controllable' which are adjacent variants of 'control') and where 'replicate' is a useful variant of 'copy', where 'replicating the problem' can solve the problem (as once the problem is replicated, some of its causes are trivially identified from the variables used to replicate it), where 'copy' is a generally useful function for workflows involving 'testing' such as 'trial and error'

    - identify useful specifications of problem-solving workflows that can still be applied across different problem systems
        - 'change the interactive surface (change the input/output of a function by applying injections of surrounding interim functions, rather than the function itself)' or 'change the host system where the suboptimal variable interaction occurs' is another interface query that can act as approximately equivalent to other function sets like 'change a base solution' or 'find adjacent change combinations of existing structures' or 'find interactive structures and build other useful structures out of interactive structures' or 'change the intent to solve for to a more trivially solvable intent, which is sufficiently similar to the original intent', as 'changing the interactive surface of a function' (such as changing the surface of pathogens or gene structures by applying coatings/encapslations/receptor alternatives, etc) is a useful variant of changing the problematic structure of the function itself (rather than optimizing the internal logic, change the interactive components of the logic until the interactive components change it enough to be more optimal/useful), which is a merging of 'find interactive structures to apply as components' and 'change a base solution until its more optimal' that is more trivial to solve for (and therefore more useful) than other more general intents like 'change a base solution', as it adds usefulness in solving the problem of 'which structures to change' ('interactive' components like inputs/outputs or surfaces) by specifying that variable into a constant
        - this is related to how some math structures are useful through their 'specificity' (like specific values of constants/sequences/limits/functions that are particular powerful/useful in some way) but seeing the generally useful structures like 'abstractions' is still useful as well, to add variation when known specific structures arent sufficient to solve a problem adjacently, like to find new generally useful structures like general solution-finding methods, as there is no known set of constants that adjacently explains everything (if there is, its interface analysis), so keeping some variation structures like general abstractions is still useful to offer an opposite/counterpoint when structures are too specific to be useful for some intent

    - identify useful connecting structures like 'high-variation variables' and 'problem space similarities which are high-variation (but not random)' and 'connections between useful approximately equal intents, where some subset of the intents are more trivial than others' which can be applied as useful target structures to apply as inputs to other workflows
        - for example, the 'transformer' applied 'sequences' in the position of 'word sequences', identifying that the variation of the last/next word in the sentence is a high-variation variable, but not so high-variation that it looked like randomness, and not so simple that trivial/few simple rules were usable to predict it, rather being in between these low/high variation variables, so that there was enough structure to predict the next/last word in a sequence of an isolated unit like a sentence/clause, but not so much structure that it was trivial to predict (so that it wasnt a problem at all) or impossible to predict (so that there is no selectable solution that is clearly better in some way), this structure including sequential patterns like grammar, definitions, etc, and embedded structures in those structures like 'possibilities allowed within definitions' and 'sensibility/reasonableness'
        - however just knowing the 'sequence' structure is common or even relevant to the problem of 'sentence completion/prediction' isnt enough on its own to derive 'transformers (which are a structure of predicting sequence completions)', but knowing that there is a 'similarity' to apply as a base for problem-solving in the sentence/paragraph prediction/completion problem space can make this problem possible/adjacent to solve (the similarity between adjacent sections of the sentence/paragraph/document is sufficient to allow prediction in most cases, as its a similarity based on relevance such as adjacency and connectivity and even cause like causal degree, except extreme nonsense/randomness or extreme creativity which deviates from known rules in a realistic way, so applying this similarity as a base for problem-solving is useful and works, meaning its possible to use this similarity to fulfill the 'completion' intent)
        - relatedly, high-variation variables are useful to identify and apply as inputs in other workflows (like how pathogens are high-variation causing inputs, so theyre a useful cause to examine first when trying to explain other high-variation variables like conditions, before other causes, as the similarity in variability implies a possible adjacent connection between pathogens and conditions)
        - the common structure is 'similarity between (high-variation but not random) structures' which can identify more achievable/trivial but still useful sub-intents to fulfill (like 'predict the next word in a sentence' rather than 'predict all sentences given a general high-variation goal like "write an essay"') is in a useful range of variation to make prediction algorithms useful, and which can be useful as similar structures are more connectible than other structures, so 'finding similarities in the problem space' is useful as a problem-solving intent to fulfill other intents like 'connect similar structures'
        - similarly, knowing 'what structures are useful to build high-variation variables' is useful to apply as default inputs to test (generality, few requirements, types, etc as components/inputs that build high-variation complex systems)
        - relatedly, knowing other high-variation structures like 'embeddings' can identify probable structures like 'errors' in a system (such as 'prompt injections' which are usually embedding structures, like a 'character embedded in a story, an embedding which makes the character seem false, and not a person speaking sincerely' or other structures which can produce 'vacillation' structures between relevant dichotomies like certain/uncertain or true/false, such as 'pretending to (pretending to (pretend))', so that 'emergent' structures can be used to identify the net effect of the embedded structures), as most complex structures will be an adjacent structure (like a 'combination') of these high-variation structures (like embeddings, waves, rotations, similarities, etc)
        - similarly, useful structures like 'completion functions from a partial subset' and interim sub-intent connection structures (like 'clarity functions which connect a partial subset to a probable higher coverage-ratio subset with a clear implication of the full set') are useful to apply
        - the 'set of sequences of set completion functions (like similarity indexes, between each adjacent pair of completion states in the sequence)' is a useful structure to identify as an equivalent alternative to other 'useful interface queries' for 'connect' intents in solution automation workflows (similar to how the 'set of sequences of differences' is useful for 'filter' intents)
        - relatedly, identifying 'similarities/differences, paired with the intents they adjacently fulfill' (similarity-intent structures) is useful to identify ('similarities that make some structure combinable/stackable' or 'differences that make other differences obvious'), and the same goes for other useful structures ('function/cause/opportunity/perspective-intent')
        - similar to how adjacent combinations of high-variation structures make problems trivial to solve, common interaction/connection functions make problems trivial to solve, as they offer different versions of the same information (what is connected/what is similar), just like other info offers an alternative (what are the most common/powerful inputs, which offers a different version of the same info) just like 'maximal differences which are equivalent alternates in some way like having similar degrees of variability within some area of probable solutions' make problems trivial to solve ('similarities across differences' and 'differences based on similarities' being highly explanatory structures), so identifying connecting structures of these 'different versions of the same info' is useful to identify the other versions of the same info (as well as identifying 'differences that change/remove the info being connected' which should be avoided)
        - relatedly, identifying useful structures that are not adjacently connectible/combinable is useful to identify other functions that a neural network should support (structural non-combination functions like 'embed/merge/filter' which indicate connections between different states/intents of information other than just 'building a structure out of combinations of components' as well as more complex information functions like 'switch perspective', 'conceptualize', 'explain', 'organize', etc)

    - identify useful structures like solution metrics such as 'realism metrics' (structures that can be used to determine if some structure is real) which can be added to a solution metric set (realism metrics such as 'iteration' and errors like 'irrationality/illogic/unreasonableness/unintendedness' which occur in real systems, where an artificial system might over-reduce/optimize for some metric like uniqueness or minimum memory storage), where realism is useful for simulating real systems (such as representing systems of variables like in the 'regression' problem space, so adding 'realism' to a function makes it likelier to be a good representation of reality, which representation functions in general are supposed to be as well as being good representations of a data set, although for some intents that might be suboptimal like for 'compression, to optimize minimum memory storage')
        - these can apply a similar impact to a solution function for very different or similar reasons, such as how 'iteration' might produce 'non-linearity through iteration of embedded changes' or 'trivial differences around the copied structure (adding randomness, divergence into different functions if the differences are scaled by further iteration, and cascading/self-sustaining errors in the host system of the variables)', and similarly errors like 'irrationality (or deviation from reasonable intent)' can add either/both randomness and/or non-linearity to a function through the 'variation in differences' this error can adjacently create
        - similarly, 'diversity' is a metric of reality so it should be applied as a solution metric of the solution-finding method (does the solution-finding method handle diverse inputs and diverse input cases)
        - how did I arrive at this conclusion (what is the interface query to generate this)? I was thinking of important differences, like how reality is different from most graphs (related to the problem of creating a 'reality-generating graph') and one 'reason' is that 'repetition' is allowed in 'reality', and relatedly attributes like 'count' and processes like 'iteration' (function application count), which are not abstract concepts but which are useful to determine real vs. artificial structures, and then identified that real structures would show up in a 'representation of reality' or a 'realistic solution-finding method' (similar to how more complex solution functions tend to be more real (occur in real systems) than simple linear functions), 'representation' being related to the 'regression' problem

    - identify useful structures like 'opportunities to use workflows optimally', like the 'opportunity to identify new math structures' by identifying math structure variables and applying a simple iteration
        - this example uses a 'trial and error' workflow in an optimal way to identify useful structures by recombining variables like components of known math structure definitions, which is a huge information gain from a small work investment, which can be enhanced with output filters to gain more useful information like the most different resulting structures from known structures and from each other
        - 'maximal differences from equivalent alternates' and 'maximal differences from known structures' are useful filters to apply in a 'combination' structure in many cases, as they filter a set of different structures in many cases, except in a case like where all items in a set of equivalent alternates are known except one, and then they identify the same structure
        - the structures which can be used to fulfill this intent of 'identifying opportunities' include structures like 'complex problem spaces with few known useful (as in re-usable) structures' which indicates that there are likely other structures which are similarly useful which havent been identified yet, and identifying other structures which are different from other structures without having a reason for that difference (unlike other complex systems which have many useful known structures, the math system didnt have as many, which indicates an opportunity to correct that difference, as it seems like a false difference) and similarly identifying similarities which can be applied in a useful way like 'similarities across systems which make it possible to apply variables from one system in the other to find false differences and other errors to correct, where these false differences are adjacently connectible to useful structures like useful re-usable math structures'

    - identify useful structures like 'similarity sources in real systems' such as 'memorization/memory storage structures/functions' (like 'mnemonics') which are by definition a way to remember/store/retrieve/use structures by some similarity (like a pattern), similar to how 'compression functions' and 'abstractions' and 'usage contexts' (in which a function is used) are a useful way to store information about connected/relevant structures that could be used to derive the structures themselves based on just these connections
        - for example, usage contexts can help differentiate the meaning of a structure, such as 'two-dimensional', which can describe an infinite series or a structure with two dimensions or four sides like a plane or square, which are very different structures that can be described by that attribute, so including the usage context in which this description was selected can help filter the set of structures it could refer to
        - these 'similarity sources' are useful to identify new similarity metrics to use when 'finding similar/different structures'
        - the 'usage context-function' index is useful similar to how the 'insight-function' and 'intent-function' indexes are useful, by providing similar info through surrounding/adjacent info like 'info triggers/inputs' such as the reason to use a function and the context it is optimally used in
        - similarly, more specificity can add value (just like a mix of specificity/generality can add value as a problem-solving, difference-connecting structure), as specific examples of usages/intents/insights are useful, and possibly more useful than a network of insights, depending on the insight, such as how a given 'attribute/variable network' can have limited usefulness compared to one specific (pre-filtered) attribute/variable (such as 'balance of power') which could explain most variable interactions and which can be applied in a way so that it is more useful than the whole 'attribute network' to identify specifically in some cases, where the one useful specific attribute can connect the comparatively static attribute graph to other graphs, as its more abstract/conceptual and connected to other graphs than the whole 'attribute network' might be (the 'balance of power' attribute can reverse the limiting, static impact of identifying the attribute network, which could be just useful enough to make other useful structures falsely seem obsolete and reduce thinking into a set of simple structures/functions of those attributes, and expand the static network into a set of connections to other graphs (which is an example of 'reversing the trap' of specificity, by creating generality (its opposite) from it) - connected by balance, power, balance of power, concepts, opposites, alternates, and other related interface structures to this particularly useful structure of concepts, which can be used to derive the interface network and other useful structures with comparatively few changes), similar to how having 'information including the specific primary interface variables' is more useful than having 'information about some specific system' in some cases

    - identifying useful structures like 'overlaps between very different structures like perspectives/interfaces' which enable intents like 'identify structures that fulfill multiple priorities/solution metrics/intents which connect different systems'
        - for example, a structure like a 'word combination' could be a simple structure like a 'sequence' of 'objects (like words)' but it is also the unit of 'conceptual math (combining a concept with another concept)' if the words are abstract concepts (objects with many different definitions), however to call it 'conceptual math' is misleading if its not applied in a useful/relevant way like to identify insights, and if no other structures are ever applied than simple combinations/sequences of objects (like words)
        - the overlap of those systems in the 'word sequence' is useful to connect those systems (simple structures and conceptual operations), as it can identify other simple structures (like convolutions) that can be applied to the other system (like 'every possible interaction between a set of structures'), as well as being useful to differentiate those systems (some function that only fulfills simple structures like sequences is unlikely to ever be able to complete conceptual math operations except the simplest unit, which is not useful in every problem), which is useful to identify structures that should be applied in neural networks other than adjacent change combinations
        - identifying the structures with these overlaps that can occur in some form in every maximally different system/perspective/interface is likely to identify a useful problem-solving structure (like a structure that can be used as a filter/gauge/standard/ring/interface/system/perspective/concept)

    - identifying the important structures determining a solution in a particular problem space (like 'similarity' in the 'regression' problem space) which can be applied across problems
        - what useful structures (like function similarities/types, higher powers in functions, densities in data sets) have in common is that they store some information about some similarity type, which is useful for finding the solution function (the most similar structure to the data set that is also similar to some solution metrics like generality which have absolute abstract insights stored in them)
        - finding structures that are similar to these solution metrics (like 'finding functions using a solution-finding method, where abstract useful concepts like balance/power are applied in relevant structures like solution function metrics in that problem space') is a useful solution structure (the 'balanced powerful structure, balanced in specificity and powerful in storing information about similarity to the data set with fewer more powerful variables') to aim to connect to the problem input (data set) or variations of the input (like representation functions of the data set, subsets/densities of the data set, etc), as the solution also has to be similar to the problem input enough to retain the important info, just like it should be similar to the solution metrics (whether absolute/abstract/general solution metrics or specific solution metrics)
        - this applies the workflow 'find structures adjacent to the problem/solution (adjacent representation functions/subsets, absolute solution metrics) that can be connected instead of the source problem input/target solution output'

    - apply useful structures like 'function sets' to other structures (like 'interface structures including other function sets') to identify useful structures like 'usefulness thresholds/limits/directions'
        - structures applied to define useful high variation functions like 'describe' (map some 'input' structure to a structure like 'combinations' of structures from a 'set of known structures') and 'determine' (make some structure obviously implied and unique and therefore required, such as filling in the ratio of a circle required to imply the circle, and no other structure as obviously) are maximally different enough to be useful when applied to other interface structures to create useful differences like new variables (which is applying the outer layer of generated functions which includes the highest variation functions, to components of inner layers to create a new outer layer)
        - this is useful to identify bc its possible to make it obvious when more iterations are suboptimal by applying similar changes to existing changes, which is when its more optimal to use existing changes than the new changes created, which is useful for identifying when to stop specifying a function by adding more variables, as there are useful structures like 'thresholds' where re-using known simpler components is more useful than identifying and applying new more complex components, 'thresholds' which can be identified to find other 'thresholds' where iterations start to become less useful and related structures like limits more adjacently

    - identifying useful adjacent structures (like 'polynomial factors' as a combination of different structures like 'differently powered terms which can be multiplied to create solution structures like the complete polynomial') that have a similarity to some interface structure (like a 'component') so they can be applied to relevant workflows using that interface structure
        - identifying useful structures like 'factors of polynomials' as useful components for 'identifying similar functions' and 'changing one function into a similar function', a structure which can be found by applying a 'combination' to a 'variable and a constant' to identify a useful difference in the exponent of a variable, rather than only handling terms in isolation or the whole function at once, to identify useful function formats like 'overlapping areas created by offsetting terms having the same exponent', so that a function network that was a network of 'variable + constant terms (like x - 1)' could be another way to format the network to find useful combination sets/paths as well as finding an optimized network where all combinations/paths are adjacent and finding similar functions by similarities like patterns in their components using a function network that can be applied as a function similarity index

    - identifying useful structures like 'inputs to useful structures' which can be used to find other useful structures
        - for example, an 'error' can be a useful structure to identify variables, as 'errors' are 'differences from optimal states like solution states (as in opposites of/differences from positive states)' and are therefore useful in identifying variables, such as how errors (created from lack of control) in 'filling in a circle with a color' can result in useful structures like 'incompleteness', 'overlaps', 'opacity' (through connecting unfilled/filled sections with a gradient) and other useful variables of colors which are possible once the 'color' interface is applied to a 'structure' interface (like a shape)
        - the error results from a poorly controlled application of a function (like 'filling in a shape with a color'), which leads to a difference that is measurable and therefore makes some variable clearer, so 'removing functions that regulate applications of functions' is an input that can be directly applied to identify variables
        - this can be applied to derive another workflow, such as 'apply adjacent changes of every type/combination/structure to the problem to check if there are clear improvements/errors in some direction or other structure of change' which is a useful specification of the workflow 'apply changes to the problem space, which will be likelier than not to improve it, given that its a problem and changes to a negative structure are likely to produce a positive structure'
        - similarly, other workflows can be derived from this such as 'structures which are uncertain like "unrequired/unguaranteed structures" are likelier to be useful to solve some problem, as certain structures like "requirement structures" are likelier to have already been tried and found to not be solutions' (the solution is likely not extremely similar to requirements, or they would be used as a solution, its likelier than more different structures are required than extremely similar structures to requirements)

    - identifying the reason for useful structures and generating other useful structures based on that reason
        - for example, regarding useful structures like 'change a constant (like only one) into a variable (like some or every)' applied to structures like 'core interaction functions' (like reduce/connect) connecting problems/solutions, the query 'does every core interaction function also connect problems/solutions' results, with answers like 'worsen' the problem (so it gets more attention and other problem-solving resources, as attention is an input to problem-solving processes) and 'contain' the problem (find a structure that can wrap the problematic structure and prevent its problematic inputs/outputs from having an impact, as the inputs/outputs of problematic structures are likely to be related to why its problematic so handling those instead by isolating them is a problem-solving automation workflow)
        - the reason that other core verbs like 'worsen' and 'contain' are useful in problem-solving as core interaction functions of problems/solutions is that they change the problem structure in some way, which is necessarily useful for problem-solving, though some functions are likelier than others, and identifying the differences between these is useful ('worsen' only helps if there are available resources like attention and other inputs, whereas other functions are likelier to be less harmful such as 'connect/reduce')
        - similarly, 'copy' the problem (to another system) is likely to be useful, in cases like 'if the other system is different enough to change the problem in a useful way' or 'if the other system is a simplification of the original problem system so it makes the solution or the source of the problem clearer'
        - similarly, 'expand/amplify' the problem is likely to be useful in that it will make the limits of the problem clearer once its expanded or amplified until these limits are reached or implied
        - these are opposing functions of typical solution automation workflow functions, which usually seek to reduce the problem in some way or directly connect it to solution structures

    - identifying useful structures by whether they adjacently fulfill multiple useful intents (such as abstract structures which are adjacently varied to generate very different structures, or functions that can fulfill the opposite intent if their direction is changed)
        - the structure of 'opposite direction (reversible, which can expand the solution space, rather than just reduce it) filters' is useful for identifying different directions of maps that can be useful, such as a 'function to identify the original data set from a solution function', which can be used as a 'filter of solution functions' bc identifying 'data sets that are possible for different possible solution functions' identifies solution functions that are too general/specific in the set of extremely different (so as to be irrelevant) data sets that they map to, and similarly is useful for identifying offsetting variables and fulfilling different intents like 'reverse-engineer a data set from a representation function, to make sure the function retains enough info about the data set'
            - these filters are useful for filtering the set of possible data sets, which is useful to identify, just like identifying filters to filter the set of possible functions for a data set bc both intents allow identification of useful attributes of filters, like info retention (reversibility, specificity, etc), as well as useful applications of filters, like useful sequences of filters that optimize for some filter attribute like specificity
            - checking if the solution function applies to a 'extremely different system of variables which change/interact with the function in different ways is useful' to identify different reasons, change types, probabilities, and other structures associated with the functions, as well as identifying the probable correctness metrics of the function (like the 'longevity' of the solution function, in being robust to predictable changes in the host system)
            - similarly, once the function and filters and data set and host systems are connected with these maps, other structures are connectible, like 'host systems producing similar variable interaction functions that change in similar ways'
        - the structure of the 'average' function is less optimal bc it is more difficult to derive the original data set from this function, even if it optimizes an error metric like 'distance from the solution function', bc it loses the non-linearity/complexity of the original data set, rather than being curved, which is likelier to retain the original complexity of the data set, which can be used to 'filter out filters that over-reduce or over-expand information'

    - identifying useful structures to be connected in problem-solving, such as 'spectrum variables' that can be applied as useful structures like 'solution filters'
        - identifying useful structures like solution metrics such as 'spectrums like good/evil' which are useful to connect problem/solution structures to, for intents like 'find solutions that are likelier to be useful for good intents than otherwise', so arguably every workflow should be connected not just to problem/solution structures but also other useful structures like the most powerful reality spectrums such as good/evil, uncertainty/certainty, constant/variable, etc
        - solutions that are likelier to be useful for good intents fulfill many alternate solution metrics (to adjacently fulfill intents like 'identifying many ways to be good, to cover the highest ratio of users'), use many different variables/functions which are also interface structures so the solution can be used to teach useful structures and fulfill intents like 'making users smarter', etc

    - identifying powerful differences in workflows like 'abstract an insight and connect it to problem/solution structures' like 'core cross-interface functions'
        - identifying that 'any combination of primary abstract concepts (power, balance, similarity) can be combined to produce useful functions that can connect most structures' is not a result of the workflow 'abstract an insight and connect it to problem/solution structures', but rather other workflows like 'is a constant (like only specific abstract concepts that are useful for some intent) a requirement, or can it be changed to a variable or removed without changing functionality/usefulness of some structures (to change it to some or all)' (answering the question 'is it "only" these concepts, or "all" concepts, or "some structure of concepts" like differently-sized subsets of all abstract concepts, which can be adjacently transformed into a useful structure that can fulfill some problem-solving intents')
        - 'abstract an insight' (concept-information interface) vs. 'change a constant into a variable' (change-function interface) vs. 'connect structures to problem/solution structures' (structure-information interface) are useful core cross-interface functions, which can be applied as components/inputs/alternate examples to find other workflows
        - the reason these are useful is that they 'connect maximally different systems', which is likely to be a useful map for solving other problems as it will connect many different variables
        - identifying useful structures to combine these components with is similarly useful, such as identifying that 'equivalent alternates' are a useful input to the 'change a constant (only one) into a variable (every)', which is more useful when applied to one item in a set of equivalent alternates

    - identifying different useful structures like different input/output formats that can be mapped to different problem/solution structures
        - other function/data set formats can be useful, such as a 'probability of an adjacent point being in a specific adjacent dense area, given some input point' (a format of 'points and related dense areas to those points', applied as a predictor of adjacent points) being arguably more useful than other formats in some cases
        - this is similar to how 'knowing that a particular word is in a sentence at all' or 'knowing whether "words that frequently change meaning of surrounding words/phrases" are in a sentence' may be a more powerful predictor than other structures like 'sequences' ('sets' and 'probabilities of words in sets' are alternate useful structures of 'sequences', similar to how 'word patterns like iambic pentameter' are useful to identify, and 'similarities between word interaction functions (like core verbs such as "like" as a proxy for words like "similarity")' are more useful to identify than sequences), and 'differences within a sentence' and 'differences to other/standard sentences' and 'differences from core similarizing (the most standardized, common) words' and 'inputs to differentiating words/sentences' and 'common components of the most different/useful sentences' and 'common sentence interaction structures like embeddings (like a specification such as an example of a point in a previous sentence)' and representations (the 'summarizing sentence of other prior/subsequent sentences') and 'very different sentences with the same meanings (similarities in differences)' and 'sentences around which other sentences gravitate or vary' and 'over-used sentences that would be better phrased differently but which are convenient like excuses or slogans/idioms' or 'sentences that standardize sentences that follow (uniting differences to equate other sentences that follow)' as well as the opposing 'controversial sentences which polarize sentences that follow' and 'sentences which frequently surround facts'

    - identify structures that are common/compelling with uncertainty in the reason for that useful attribute and connect them to useful structures to resolve that uncertainty
        - why do we find symmetries compelling? bc of the 'variation of their contradiction' ('similarity in a difference') and their 'alignment' (the 'similarity in the changes', present in the similarity of their common base/origin/center and the similarity in the structure of changes in relation to that base), as 'opposing' structures and 'embedded' structures (an 'opposing/different structure in the same structure') and different structures ('similarity in a difference') and paradoxes ('similarity in a difference') are useful structures to identify as they are commonly explanatory of other changes like more complex changes once they are iterated/combined/otherwise structured and useful for intents like 'differentiation/filtering' and 'identifying similarities across different systems to connect systems/differences' and 'identify interfaces (similarities that connect all systems)'
            - what makes symmetries more compelling? structures like 'balance', 'intersectivity', 'orthogonality' make symmetries even more compelling
        - similarly, we find metaphors like 'yin and yang' compelling, as it indicates other useful structures like a pair of monodromies ('structures such as spaces having a point representing one impossibility') fulfilling some useful structure (like 'spirals/fractals') which together adjacently create other useful structures ('circles') and which are similar/complementary/coordinating in some ways (structure, alignment in relation to axes) and different/contradictory/opposing in other ways (color, rotation degree in relation to each other) and contain a set of impossibilities which contradict each other in the same structure (a paradox), and a similarity to other useful structures like symbols such as the 'symbol for infinity' (achievable by a rotation to create an overlap, to connect it to other useful structures like the symbol of infinity represents such as a 'set of loops/angles that would necessarily develop and/or would create momentum in real systems to sustain itself and refer to itself by intersecting with itself, indicating self-sustaining/infinite change around a common base which is the intersection point')
        - similarly a wave ('connection between extreme differences') has interface structures embedded in it (common structures like 'circles' as well as change limits, embedded changes like change rate/type changes, etc)
        - similarly a 'peace' symbol has useful structures like 'connections between similar but different directions into one unified direction' and in reverse, 'isolation of a variable into component variables' and a 'variable connected to a constant' and 'lines from a common origin applied as connectible to a circle through being applied as radii of a circle' among other structures
        - similarly a 'flywheel' symbol is a useful variant of a wheel that can be used in its place
        - these are useful structures that have other useful structures embedded/intersecting/otherwise connected to them, which can be applied as useful base structures that are useful for 'identifying other useful structures', "identifying ways that very different structures can interact without violating definitions (which is useful for intents like 'coexistence')", 'identifying maximally different stable structures', 'identifying structures common to useful structures', etc

    - identify insights by applying other insights/interface structures and identify variables of insights to find new insights to apply as inputs to other workflows involving insights
        - identifying errors in perspectives like 'long-term thinking' (such as the 'lack of short-term/immediate feedback') and 'short-term thinking' (such as the 'lack of a solution for scaled/long-term/emergent effects') is possible to achieve by applying structures like 'generally useful structures' (such as known optimal structures like 'feedback' which would ideally be integrated in every solution) as well as 'advantages of opposite structures' ('what is the benefit of short-term thinking and is that missing from long-term thinking') by applying insights (like 'any one solution is rarely the only solution' and 'its useful to identify/apply balance points in spectrum variables like long/short-term thinking') and known error structures (like 'avoid over-prioritization errors caused by applying few structures such as only one structure')
        - the variables of these insights include interface structures ('insights frequently use interface structures, which are known to capture high variation, since interfaces by definition support other variation and allow variation to develop as they are a base structure, and insights are likelier to evolve from structures of high variation-capturing variables, whose interactions are less likely to already be known but are likely to be powerful interactions and therefore important/useful') such as 'multiple' (math), 'alternate' (change), 'spectrum' (structure), 'balance' (concept), which occur across interfaces (from which you can also derive that 'insights often use/connect interface structures from multiple interfaces, as those are high-variation structures which are powerful/useful to connect')

    - applying insights like 'any one solution is rarely the only solution (apply multiple to useful structures like solutions and useful structures)' to structures where useful (where errors corrected by the insight are possible and relevant and where the insight structures dont contradict solution metrics like accuracy)
        - for example, identifying structures of errors that can be inputs to a loss function in neural networks is useful for identifying error structures like 'overlapping error areas (of multiple error types)', such as how 'adjacent input errors' can be used to identify a 'directional error' (an error of an incorrect direction, either the first error that indicates it or is measurable as indicating it or the error that initiates the error direction by being different from the correct direction), to apply an ensemble loss function of various loss functions or structures of loss functions (like combinations of loss functions), as knowing 'similar minimums across loss functions' is useful for finding 'absolute/compromise errors across loss functions'
        - 'a type of difference that is not an error' is another useful structure to identify, such as an error that probably wont push a function beyond a range that would classify it as a slight generalization/specification of a data set average/regression/representation line
        - it is useful to keep multiple loss functions separate bc adding them removes the information of their extremes/averages and its useful to know which types of errors are optimized for at a certain point
        - another example is how 'over-prioritization errors' are likelier the fewer the priorities are (even generally good metrics like 'independence' can be applied to negative effect, such as how 'apathy' is a result of 'extreme independence (leading to lack of caring)'), where multiple priorities are likelier to offset the possible over-prioritization errors that are likelier with fewer priorities, as more priorities are likelier to be able to check each other's power and errors

    - identify useful structures like useful function/structure sequences to apply for some problem-solving intent like 'identify useful similarities/differences to apply to create useful structures'
        - for example, 'if a condition has an output and another condition has the same output, one condition could cause the other' (or 'alternate inputs (having the same output) could cause each other') is a rule that is possible to infer and apply as a rule to find/describe possible variable interactions, given the useful identification of the uncertainty of the causal connection between the alternate inputs and their relative position (either could be causal of the other or there could be no causal connection), which is useful to identify as this uncertainty allows for a possible connection between possibly previously unknown connections (alternate inputs of the same output), which might have a useful difference in differentiating the position of the alternate inputs (as in positioning one before the other, causally), and given that there is no requirement that either condition not cause the other, thereby allowing that uncertainty to exist and the possibility of a causal connection between conditions to be identified
            - these are variants allowed by the known structures, which is not adjacent, as in its not obvious to position one condition before the other in a straight line before the output, indicating a guaranteed causal relationship, between alternate inputs
            - some graphs will position alternate inputs at the same/different distances from the output, but without differentiating information, they would be at the same distance from the output by default in a correct graph that doesnt add info (such as by differentiating the distances) that isnt available/known
            - the uncertainty develops bc the position of the alternate inputs can change while still being an input to the output (still being 'before' the output) and the alignment of their angles to/distances from the output implies different connections to the output and to the other input, meaning there is a possibly valuable difference to apply by 'aligning the angle of alternate inputs to the output, but keeping position different (rather than overlapping at the same point)', so that in the resulting structure, one alternate input is causative of the other (at which point it could still be an 'alternate' input, as the whole causal trajectory isnt necessarily required to traverse)
            - this is applying a (difference in a) difference/similarity (the changing of the alignment/equivalence of the angle/distance between the alternate inputs and the output) and a (similarity in a) difference (the maintenance of a difference in position, to differentiate the alternate inputs as not equivalent) in a combination structure that identifies a possible equivalent alternate structure of a structure (like 'alternate inputs of the same output, whose causal connection is possible but unknown'), where info that is different and should remain different is held constant (the difference in position of alternate inputs is preserved) and a unrequired constant (the similarity in angle/distance from the output) is varied to be different in distance or similar in angle, in a way that doesnt violate the requirements/definitions of the structures
            - this is applying trivial functions (combinations/applications/embeddings) of similarities/differences applied to connections (equivalences, alignments, similarities, positions) within the limits of requirements/definitions, which is a useful default function to apply

    - identify variables of useful graphs to combine/mix/otherwise apply as components of an integrated graph that makes other useful structures trivial to identify
        - given the different graphs available to graph useful structures like interface queries and workflows, such as the default 'functions/lines connecting structures/points' diagram, identify the usefulness of different graphs
        - different graphs are useful to identify different structures adjacently
        - applying some known structure as a similarity in a graph can help derive missing alternate structures
        - an 'iteration' graph can differentiate 'iterated structures (like embedding an embedding in an embedding or pretending to pretending to pretend)' and possibly also the similarities that these iterated structures have in common (line sequences vacillating between opposing/contradictory structures like container/component or truth/falsehood)
            - this graph is interesting because highly scaled iteration can produce complex structures (similar to how mixes of these structures can), such as a 'nested embedding that contains/stores/compresses the host system in which it is embedded', or an 'application of an application of an application that solves the problem of which structures to apply' (self-referential structures, self-containing structures and inverses of traps, where information is derived from approximately zero information, such as the 'minimal information of a unit structure', which is iterated to create the final structure)
            - this graph could be formatted as a set of line sequences connecting opposites like truth/falsehood, or a set of nodes which are also system layer diagrams to identify patterns in the embedded iterations of each structure
        - for example, identify a trivial graph where interface queries/solution automation workflows or other useful structures have a structure (similarity) in common, which is applied as a constant (the similarity is guaranteed to be a similar structure in the graph), like the following, in order to identify missing workflows or other useful structures by applying a trivial transform (such as 'rotate the structures around some center of the similarity, like a common starting point')
            - a 'line of function steps' or a 'wave/cycle between (sequences/lines representing functions)', if formatted as a set of lines connecting nodes, so that different workflows would be lines in different directions from an origin, so that the other workflows can be calculated by determining lines in the areas in between known workflows
            - a 'query across several different layers (representing different function sets)' in a system layer diagram (a similarity which would reveal that objects from different function sets frequently interact in useful structures like workflows)
            - finding a graph where similar structures are guaranteed to have some similarity (possibly by trial and error or another dumb algorithm) is useful for deriving other structures of that type
            - as another example, identifying an error graph where 'overlapping error points/areas (having multiple errors)' are visible is useful for identifying points/areas with one error (compounding/additive errors and their points/areas of intersection/overlap being useful to identify as another error structure that is complementary in usefulness to a 'minimum of a loss function')
        - these graphs make these similarities trivial to identify, by applying some known/constant original similarity structure (like a layer in a diagram referring to functions from the same function set), around which the input structures graphed in the graph (the workflows which can be represented as line sequences connecting functions/structures across the levels) are allowed to vary (meaning 'a workflow isnt restricted to one function set/layer'), then finding the similarities in those variances from the known/constant original similarity structures

    - identifying useful structures (like 'concepts that can store/connect very different structures') is useful for intents like 'connecting maximally different structures' and 'identify function types', which are useful for problem-solving intents like 'filter a solution space'
       - identifying 'uniqueness' which is connected to 'units' which are useful as 'components' for 'combine' intents as well as being connected to 'useful equivalences' (like the 'equivalence' of a single function that is the best representation of a data set by being equal to it in some way, which may be unique in its equivalence in that way to the data set) and which is powerful in that it identifies structures which may be new stable structures (unique structures may be new, as in 'different from previous existing structures'), which is useful since 'uniqueness' captures very different structures like 'unit components' and 'filters' in one concept
       - similarly, identifying 'similarity' as the important structure connecting a 'regression function' and a 'data set', rather than finding the 'average' (identifying that 'similarity' is important in its connectivity to the concept of 'average') is useful to identify as it allows other formats like similarities/differences to be applied rather than default formats like 'numerical values/addition/multiplication' and connect those other default formats with other useful formats like conceptual formats which are useful to identify which equivalences/similarities/differences should exist in which positions in a problem space
       - similarly, 'randomness' can represent 'equivalence' (in its probabilities) and also 'extreme/maximal difference' (in the set of possibilities allowed), uniting very different structures in the same concept

    - identify useful structures (like 'information-intent connections' or 'intent-output connections') that can be applied as an alternate to solution metrics (like concepts such as 'balance') and the full set of structures that fulfill these to apply as components/base solutions
        - for example, for intents like 'acquire a product', its useful to know information like 'impact of product/purchase on economy', which creates a more optimal 'balanced' structure (of an intent and a counterintent, or 'reasons not to fulfill the intent') that can be applied in place of generally useful conceptual solution metrics like 'balance' bc its an implementation of the concept in another system
        - this can be abstracted to include other solution metrics in every solution, meaning a workflow to first 'find all the structures (like fairness, or a point-counterpoint structure) in specific structures (like a particular position in a system, like 'on either side of a purchase (before/after or purchase/instead of the purchase) in an economic system', answering the question of 'where should the optimal structure (balance between information/intent) be in the economic system (around the purchase)' and 'why' (bc given the insight 'think globally, act locally' and useful structures like 'common' structures, its one of the most common 'acts', and therefore also an important scaling structure that is very powerful as a result of this scalability and commonness, and 'powerful structures should be balanced as they are meaningful and meaning is positively connected to balance (they change in the same direction in most ways)')) that implement useful conceptual solution metrics (like balance and its connection to meaning, as in "balance in the position of purchases between global/local intents can allow meaning in a system")' and then apply them as base solutions to apply changes to or solution components to combine to find solutions for a specific problem
        - here Im once again applying the workflow 'find an insight, abstract it, and find a way to connect it to problem/solution structures, then differentiate it from other workflows if it is equivalent, to find a new solution automation workflow' (youll notice Im applying this with other workflows to first find the insight, in which I was thinking about how to prevent errors, such as 'what information stops people from fulfilling an intent', like a 'negative purchase that harms the world', and how would you frame that structure (as a concept like 'balance' in structures such as a 'fair or good decision' which is related to other important concepts like 'meaning'), and similarly was thinking about other insights like 'think globally, act locally' that Ive heard my whole life which are embedded in my subconscious but which I also frequently think about consciously, as its useful to be able to derive insights in many ways, so thinking about the same powerful structures and how to connect/change them in different ways is useful, as those connections/changes will likely be similarly powerful and might also be reusable in explaining other useful connections)
        - connecting queries (and components such as concepts like 'balance') to 'meaning' (the 'ultimate impact in relation to other meaningful structures') is so important that its why I frame that as the 'host interface' on which other interfaces can exist and where interface queries can be run and where they should start/end, bc interfaces/queries should all be connected to meaning and that should be the focus/intent of the queries (every interface query should end with a connection back to meaning, and every workflow would ideally connect to meaning in some way, if not directly then indirectly by connecting to problem/solution structures)
        - similarly, connecting workflows/functions/other interface structures to 'possible futures' enabled or made likelier by those structures is useful to identify future problems/solutions once that workflow is applied, just like how its useful to connect workflows to 'universes' enabled/probabilized by that workflow and 'graphs/networks/indexes' made more useful by that workflow and other problem/solution structures and conceptual networks and structures of relevance/meaning (given that all workflows connect to relevance/meaning by default, as 'change a base solution' and 'apply changes to certainties to resolve uncertainties' and 'connect different structures' are structures of relevance)
        - similarly, connecting workflows/other interface structures to core structures on every primary interface (such as heat/energy and speed/scaling on the 'physics' interface) since knowing how a workflow or other useful structure (that is likely to be used commonly or at scale) interacts with heat/energy and speed/scaling is useful to identify other useful info about it, like its probability of being used in a way that doesnt disrupt the stability of other systems like the system where its run, and similarly its advantage in speed compared to other structures is useful to identify to determine its usefulness in various contexts

    - applying different problem formats (like 'differentiate/oppose a negative structure') is useful to find useful structures like inputs to other problem format workflows (like 'filter a solution set')
        - for example, the 'type' filter is used to filter 'problem/solution types' ('pathogen types', 'drug types', 'drug component types') which means the errors of other examples of that type can be predicted more accurately once the type is known and an example is tested
            - similarly, other filters are useful to filter possible drugs, like finding 'matching offsetting differences' that should be corrected/differentiated (such as 'differences from pathogens', 'differences from other solutions/drugs', 'differences from existing system components', 'differences from pathogen or condition side effects') which are 'negative' so that finding 'opposing (positive) differences to that negative' is useful (like 'matching opposing structures of a side-effect receptor or pathogenic component')
        - other problems arent by default formatted as a 'find opposing/offsetting differences to correct a negative difference' format, but it is a problem format that problem types can be standardized to, since 'differentiate' is a default interaction function like 'connect' or 'reduce'
        - for example, in the 'regression' problem space, the 'negative' structures to oppose/differentiate are structures like 'randomness in the solution space (a solution space area that is uncertain)', which its useful to find differences from (like 'different solution bases or types to apply as base solutions to apply changes to')
        - finding the connections between problem formats ('differentiate/oppose' and 'filter') is useful to identify so they can be converted to each other ('differences are useful as an input to filters since they segment the solution space', and 'applying differences to a negative structure' is related to filters which 'apply differences to find reduced/unique structures'), which is also useful to identify, as it adjacently identifies structures that are useful in one problem format ('negative structures to differentiate from') which can be applied in other problem formats ('randomness in the solution space (which should be differentiated from, with opposing structures like maximal differences to filter the solution set)')

    - identifying useful structures like variants and variables of workflows as well as interim workflows in between core workflows which are still functional solution automation workflows
        - for example, 'change a base solution' has a variant of 'combine subsets of existing solutions', which is not a trivial subtype variant of the original workflow, but still has structures in common like 'existing/base solutions', and is still a solution automation workflow in that its likelier than randomness to have value in some cases/problem formats, and can be generated by applying a workflow using the 'build' function to the original workflow ('build a solution out of solution components')
            - similarly, the 'find' function can be applied to the original workflow, to generate workflows like 'change a base solution (for the "find a filter" problem, to find more optimal filters to be used in solving the original problem)'
            - the 'change/apply' function can be applied to the original workflow, to generate workflows like 'change a base solution (to find base solutions or changes to base solutions that are more optimal)' or applied to other workflows using the 'change' function like 'change problem/solution structures until they are adjacently connectible' ('change a base solution until its more similar to the problem inputs, and apply it as an input to other workflows')
        - these mixed interim workflows are different from core workflows which are likelier to be adjacent to existing formats and likelier to be useful in some cases
        - given the generality/coverage of these functions, there are many ways they can interact, so they can be combined in many ways to adjacently connect new structures
        - they can be used to generate other workflows with simple structures like embeddings/combinations and matching those embeddings/combinations to problem-solving structures like intents

    - identifying useful structures like specific types of similarities (like 'patterns' or 'internal function input/output or sequential similarities') that are useful for specific useful intents like 'filter the remaining solution set, from a subset'
        - for example, 'internal function input/output similarities' (similarity of a subset with another subset of a function, such as 'rotate the subset to get the other subset') are useful for intents like 'filter the solution set', bc if these similarities exist, trivial changes based on those similarities can be applied to generate the rest of the function rather than checking the full set of solutions, just like 'function-function similarities' are useful for 'finding similar functions that can be applied as alternative solutions to check' and 'finding maximally different functions (within some degree of difference governed by the similarity)'
        - some types of similarities are useful in contexts that are counterintuitive, such as how a 'pattern' (such as a 'sequence of similarities') can be useful in the 'regression' problem space, which is counterintuitive bc the rest of the function cant always be filtered by any given local subset of the function, although if there is a pattern like a wave, that is useful to identify by checking for that similarity type (sequential pattern) bc that type is possible in the definition of a function and does fulfill the intent of 'filter the rest of the function (the remaining solution set), from a subset' if there is a sequential pattern or other detectable pattern in change types/rates/other change structures
        - patterns (like 'repeating sequence') are useful to identify in that the repetition allows them to fill out the rest of the function without checking a large remaining solution set
        - the 'repetition', the 'trivial changes' (like 'rotations'), and other similarity types (like 'similar functions having some intersections/subsets in common') structures are useful for intents like 'filter', in a counterintuitive way (once you identify the intent of 'find the rest of the function from a subset', it is more intuitive/obvious, and once you identify the possibility of functions having other formats than a standard input for a regression problem, like functions with formats like 'sequential patterns', these other structures like repetitions become more obviously useful), whereas structures that are obviously useful for 'filter' intents are structures like 'maximal differences in functions' or 'randomly sampled subsets' that can be applied as filters or function approximators, similar to how 'approximation' is a more useful intent related to 'filtering solution sets' than other intents
        - basically any interface structure in the regression problem space (interface structures of 'data sets/functions') that creates or connects adjacently to other information (making derivation of that information trivial) can be applied in a similar way, to 'filter the solution sets'

    - identifying useful structures like insights such as 'if any abstract concepts can be used to solve a problem, its likely that other subsets of abstract concepts can be used to solve the same problem, given that abstract concepts cover reality in a structure like a field (or a quadrant of euclidean space or wedge of a circle, which can be rotated to cover the other sections)'
        - for example, different subsets of concepts like the following can be used to solve 'regression' problems, which are connectible on the concept interface but are different enough for this similarity in functionality to be an insight rather than obvious
            - 'power' (through more important or differentiating power of variables)
            - 'balance' (through its adjacency to core intents like 'comparison' useful for intents like 'filter solution sets' and relation to the abstraction/specificity dichotomy which is particularly relevant as well as its relation to structures like 'interim' which is useful for finding 'maximally different' structures like 'different base solutions to apply changes to')
            - 'specificity' (as a primary interface variable of 'abstraction')
            - 'certainty' (as a foundation for applying changes to known variable interactions to find the unknowns) 
        - similarly, applying this to other interfaces (if any physics structures can be used to solve a problem, other physics structures can be used to solve that problem, given these structures power/absoluteness/differentiability and connectivity on that interface) identifies physics structures such as 'vortexes' on other interfaces like 'information' such as 'info vortexes' (like a variable that, when added, creates a 'info vortex' that makes all certainties seem uncertain or less certain), which are related to 'interface queries' by an 'opposite' structure, as they add information using trivial changes
        - more powerful/interactive/absolute/core/interface/complex structures on an interface are likelier to be adjacently combinable in this way (as structures that contain/connect to enough differences that they can be used to solve any problem, so they act like alternates)
        - why is it true that equivalent alternates develop? bc powerful/useful structures are likely to be required for many intents which they cant all fulfill in real systems simultaneously, so backups are useful, and similarly most systems have enough variables that differences are stable and can compound in different directions, and exact copies are less likely just like extremely different structures are less likely, so in between there are interim similar but different structures that occur if enough variation is possible, and powerful structures are likely to have alternate states that they can form at rest or in different contexts, so 'variations of an original structure' are likely to create equivalent alternates at some point in their development, and 'trivial differences applied to a base structure' is a useful structure as it can frequently fulfill very different intents with these trivial differences, which are another way that equivalent alternates can develop
        - primary concepts can be described with interface interactions (like 'power' describes important input/input interactions like an 'input that can replace other inputs' or input/output interactions like 'low input/high output', and balance describes a 'specific state of "equivalence" in a connection between important structures like compared structures') that can be used to identify other primary concepts that are useful in some way ('equivalence' is useful for a 'compare' intent, and 'power' is a definition that can be adjacently used to describe 'usefulness' or 'importance' or 'relevance' or 'meaning' or 'efficiency')

    - identifying useful structures like 'false similarities' applied to structures like 'definitions' is useful for intents like 'identify/filter', given the usefulness of 'definitions' for those intents
        - for example, finding all the ways a structure can be false (an embedding that makes some identifying attribute false (such as in a 'false copy' or a 'false context') but still be included as a component, so it will still seem true if only that component is checked, a falsely similar substitution of some identifying attribute, a false subset of identifying attributes that is used in an identifying index) is useful for identifying the real/false structures
        - this is useful bc real systems often have false structures, as false structures (imperfectly complying with some definition) are cheaper than true structures (perfectly complying with some definition), to the point where they can be assumed to be more common or the default

    - identifying useful structures like insights such as 'similarities can be used as filters' by applying useful structures like differences and checking for similarities in those differences, to find useful intents like 'find function similarities (and similarity types)' and specific structures to implement these intents
        - for example, a function can be mapped to a set of its identifying structures (x^2 has an identifying structure of a parabola (so the 'probability of a function being x^2 (or other exponents) given that there is a parabola at all, is higher than not'), bc these structures and the functions have a similarity in common (the set of possible functions can be filtered by knowing a set of structures that are associated with a function)
        - this similarity can be applied as a map (a set of structures to a set of functions having those structures)
        - this similarity between function structures and functions can also be applied as a network of term types (or 'significant example (such as defining example or differentiating example)' terms or term 'units') and their neutralizing/magnifying terms (the set of functions you would need to apply to x^2 to erode its identifying structures, even in functions with an x^2 term, such as offsetting 'fraction coefficients', 'inverses', and 'higher powers')
        - other similarities exist between function sets, such as the mapping that adds useful attributes like non-linearity to a function, which can be used to 'connect function sets' (or convert a linear function into a non-linear function with some similarities like general structure, intersection points, average, etc), which is a useful if indirect problem-solving intent in the 'regression' problem space
        - this is in contrast to other attribute sets like 'generality' and 'linearity', which dont have a 'default similarity (like a higher probability of some similarity) to use as an identifying filter' (there are 'general' functions which are either 'non-linear' or 'linear', with no clear differentiation like a 'more probable attribute value', although 'simplicity' of a data set can connect 'generality' and 'linearity')
        - this means the info is retained across this similarity transform/mapping (it is symmetric in some defining structure)

    - identifying useful attributes like 'intersectivity' can be used to adjacently identify other useful structures
        - identifying that 'systems of linear equations' can help identify 'intersectivity' is useful to identify 'systems of linear equations' as a useful input to finding similarities (such as intersections) between functions
        - identifying structures that fulfill the intent of 'connecting subsets of a function' is a way to identify 'intersectivity' as a useful structure
            - for example, identifying that simple transformations like rotations/shifts/scales of one subset can identify another subset (in some cases), and similarly simple direct connections can be used to connect two subsets, and similarly components of one subset can be used to identify another subset, and similarly intersections of two different subsets can be used to connect the subsets
        - identifying that 'intersectivity' is particularly important in differentiating functions that are locally similar or have similar averages
        - identifying the 'intersectivity' of functions is a similarity metric that can be used to differentiate them
        - identifying 'intersectivity' is useful for finding similar functions (such as functions that are 'similar to averages', 'similar to data sets', 'similar to maximally different base functions', 'similar to local function subsets', etc)
        - identifying the 'intersectivity' of tangents as a way of finding 'function limits'
        - identifying functions that are highly 'intersective' is useful for finding a reduced set of functions to check when some of the function is already identified
        - identifying maximal differences based on 'intersectivity' (like the most different structures that can have a particular line segment in common, like a parabola/circle) is useful to find functions to filter once an intersective subset is known
        - relatedly, identifying the 'reason for similarities/differences' is useful, as in identifying 'coincidental equivalences' (function sets may have a subset in common but are still so different they may as well not have anything in common, such as a 'required equivalence (such as all terms with a coefficient of 1 which are a power of x must cross y = 1) that doesnt summarize the rest of either function or connect adjacently to the reason for the intersectivity or help differentiate the functions having that equivalence') which are useful to differentiate from other equivalences that are more significant (functions that have a subset in common and its bc theyre an adjacent transform of each other)
        - identifying 'interface structures like reasons' of errors like 'insignificant and/or false intersections' (like the common intersections of a straight line with a wave, in which a 'higher power is missing' and the 'intervals of the wave align with the subset tested for equivalence with the straight line') is similarly useful

    - deriving inference rules like 'what is definitely not contained in a function (a set of changes applied to inputs to generate outputs)' (its inputs/outputs are almost definitely not bc of the existence of the function which implies a 'lack of those structures, indicating the existence of a structure to create those structures', unless its the defined exception that requires it to contain its inputs like an identity function) is possible by applying changes to certainty structures (like 'definitions') and checking if the limits of those certainty structures are violated
        - the definition of a function indicates a difference between a 'set of changes' and 'inputs', otherwise they would be connected with equivalence structures such as 'like/is' or the definition would likely use the same term rather than different terms
            - given this identified difference in the definition, a query like 'what is different from a function' is trivial to answer (the inputs/outputs)
            - deriving questions that are also trivial to answer is similarly possible by applying trivial changes 
                - questions like 'what different functions are equal/similar to this functions' with answers like 'functions with similar inputs/outputs'
                - questions like 'what causes function inputs' with answers like 'functions with the inputs as outputs'
                - questions like 'what requirements does a function have' with answers like 'function inputs' and 'a system where function inputs can exist'
            - deriving questions that are not trivial to answer is similarly possible by applying maximally different changes like interface structures (which already capture high variation, as they are defined as being the more useful structures) such as 'changes allowed within a definition but which are maximally different within that definition (like allowed contradictions of sub-types within a type)'
                - 'whats a previously undefined statistics structure related to a function' with answers like 'interaction levels where this function is adjacent/default' or 'variable embedding interaction functions which can predict most variable interactions' or 'different formats of a function like filter sets or indexes/mappings' or 'densities paired with associated tensors whose trivial rotations (within a limit) or intersections cover most of the data set or most of its ensemble averages across algorithms'

    - identify useful sets of implication/confirmation or implication/contradiction structures (like a point that when paired with a corner implies a shape, a tensor, or an angle sequence, and when that set is paired with another corner, a shape is more certain if the corner has an attribute like 'concavity' and implies another corner that can be derived and checked without traversing the entirety of the third line, just like the full set of remaining corners of a square can be derived/checked once two sides are known, paired structures that confirm/determine/contradict the implications of the initial structure)
        - this is related to applying 'maximally different variables within a solution set/possibility space, once a filter is applied'
            - this structure of 'increasingly incrementally more specific filters (diminishing returns of each subsequent filter)' is only optimal in cases where the initial filter is the primary differentiator (like a general type variable) and every filter that follows is more specific (like a sub-type variable of that initial type variable), which is unlikely to occur by default without applying organization structures to force it to have that pattern, as variables would need to be sorted to make that filter pattern relevant
        - similarly, identifying non-adjacent structures (like an 'allowed sub-type definition contradiction (like a straight line segment in an otherwise curved function, which is allowed but violates a function type definition of a non-linear function, which is allowed in the definition of a continuous function but not in the definition of a curved function, which are associated but not required to be equal, and is also improbable)') is trivial just like deriving adjacent structures is trivial (like a 'continuation/repetition/iteration of a local pattern'), and both are useful to identify and apply as structures to check/filter
        - structures of volatility are similarly useful to identify in a problem space (such as 'non-repeating structure inputs' and a 'high number of variables' and a 'degree of difference from randomness')
        - structures (like 'allowed sub-type definition contradiction within a type definition' which are similar to a 'paradox' structure, which is a maximally different interface structure) that differ from simpler structures ('iterations/extensions/rotations/other trivial changes applied to a local pattern') are possible interface structures to derive/apply as defaults for intents involving maximally different structures (which are common in problem-solving)

    - identifying useful structures (like 'similarizing differences') useful structures (like 'network hubs') by applying known useful structures (like the 'commonness' value of the 'frequency' variable to derive important interface structures like 'probability')
        - a 'language network' used to find the 'most common words/nodes' would trivially identify structures like 'network hubs' as useful, the reason for this usefulness being the fact that the difference between 'network hubs' and other structures is that 'number of connections' (which act like a similarity between many different structures)
        - identifying similarly useful structures is a matter of identifying/deriving variables, applying these differences, and identifying new emergent differences in core structures like networks, as the primary differentiating variables of core structures like networks are likely to be useful for other intents (applying a workflow of 'finding useful structures' and 'finding intents to use these structures by some similarity in intent once useful structures are known, given their high probability of usefulness and similarity to some useful set of intents, like how a "network hub" can be useful as an input to an average/density/similarity-finding function')
        - similarly, identifying useful formats of info structures (such as formats of 'filters' like 'twisted light' as 'different paths to connect info on different sides of useful positions/structures to connect') is a way of formatting problems in a way that can make use of other system rules to identify 'maximally differentiating variables' of useful structures like filters (the problem becomes 'find a useful set (like the minimal set) of connections/angles that can connect the two structures on either side (optionally using interaction functions of light, like how its preserved across various filters)')

    - identifying useful structures like 'filters' of problem structures like 'solution spaces'
        - for example, when solving the 'find a drug (input) that corrects/changes a condition' problem, the set of possible solutions can be filtered by removing the 'known common inputs', as those are likely to be similarly distributed in people with the condition as the general population (known common supplements like turmeric can be removed as a possible solution, as its unlikely to be the solution, given that its a common input in general and is also likely to be a common input in the subset of the population with the condition)
        - 'filter out known common inputs' can be applied as a filter to find 'uncommon (uncommon in the sense of being new/rare/unnatural) substances'
        - this is a way of deriving info (which substances to ignore/filter out) without having info (info about the specific mechanism of all substances or the mechanism of the condition), using other more available relevant info ('what inputs are common' and 'whether the substance is likely to be similar/different from the common inputs (different)')
        - the info of 'what inputs are common' can be made relevant to this problem by applying changes ('similar/different' applied to 'common inputs') and checking if those changes are useful (is a 'similar substance to a common input' or a 'difference substance to a common input' likelier to be useful for the condition), and given that workflows/rules such as 'change existing suboptimal base solutions to be more optimal for a problem' and 'if a problem exists, it is unlikely to be solved by adjacent/existing/available inputs' are known, and given that rules of probability/reality are known like 'small input changes are unlikely to produce large output changes (volatility isnt a default structure without other attributes like independence, as very independent non-interactive structures are likelier to be extremely different and therefore likelier to produce attributes of randomness and related attributes like volatility)', and similarly other rules like 'if an adjacent input was the solution, it would likely already be known' are known or easily derived, and given that specifically 'common inputs' have already been tried as a solution indirectly and can be determined to not definitely be solutions, these rules can be connected and applied to determine that the 'common inputs' should be differentiated to become a solution ('differences from common inputs are useful')
        - additional filters can be applied like the 'commonness of the condition' (if its a common condition, then the uselessness of common inputs is even likelier than if its a rare condition)
        - therefore by connecting the available relevant info ('what inputs are common') with problem/solution structures ('what solutions already exist and have already been tried' and workflows like 'change existing solutions to be more suboptimal') by applying changes ('structures that are similar/different than common inputs') and applying interface structures (like 'probabilities', such as 'info that is likely, if other info is true') to check if those changes are useful, the solution space can be filtered to exclude the useless structures ('similar substances as the common inputs')
        - filters like this can be applied as 'alternate filter sets' to identify solutions that 'fulfill multiple filter sets' (to find the 'most probable solutions') or to identify solutions that 'fulfill different filter sets' (to find the 'maximally different solutions')
        - this is related to the insights that 'a solution might only work in one case' (with the associated implication that 'it might be the case that was checked') and 'a solution might only work bc it was changed a lot' (with the associated implications that 'these extreme changes might have occurred', and 'other solutions exist which are more optimal/adjacent', given that 'its possible to change any structure into any other structure')

    - identifying useful positions (like specific interaction levels) where useful structures are already default/specified which makes useful intents adjacent

        - applying/implementing 'embeddings, maps, and other powerful interface structures' to neural networks
            - if a node/subset of nodes/layer can learn/apply a 'space/context-switch' operation (like a function to 'identify possible relevant/useful similarities/differences' and a function to 'generate a change that will probably make some relevant similarity/difference obvious'), these can be used to fulfill more complex intents than 'apply standard algorithms to combine adjacent changes' or 'learn an index mapping one structure set to another', intents like 'find emergent effects that are obvious in another context'
            - these structures can be applied as sub-functions that emerge from a sub-network of functions within the network, or as additional structures that can be applied wherever an opportunity/reason/requirements for additional variation or optimization is identified in a subset of the network

        - 'finding/deriving the system (find the filter applied), from a position in the system (from a filtered position)' (such as by 'applying symmetries to "remove filters/limits used" or "abstract variables specified" to create the filtered position') is a useful problem-solving intent to apply as a 'core structure of understanding' ('reverse the trap')
            - for example, a definition of the 'universe' as a 'description of maximal differences, which can compress reality adjacently (like with a few folds or a repetition of some core unit)' or an equivalent alternate like 'an emergent structure which can make sure it always evolves (can use any input to generate itself)'

        - identifying structures of time/change as 'embedded time/change, relative time/change, time/change base, time symmetries/position, and limits of time/change' being additional relevant variables, rather than just its 'direction/asymmetry'
            - similar to how 'membership in a set' can be more important than 'which member of the set an item is (what its position is relative to other items in the set)'
            - for example, having a 'gravitational boundary' on possible time/change allows faster time/change to bounce backwards against boundaries and interact with slower time/change
            - as another example, a structure of 'fluid time' that changes until it fulfills some structure like the 'interface network' and then becomes a constant, or a structure of time that is constant in that it requires a field of uncertainty surrounding any structure to allow some structure (like a 'ratio') of interactivity/potential or some interaction with other structures
            - from this structure, its adjacent to identify that 'symmetries' act like 'positions' and identify useful queries like 'are there absolute connections between symmetries and asymmetries, such as that every symmetry has some connected asymmetry, leading to change in a direction away from the symmetry position and therefore its invalidation, or are there usually some combination of coordinating complementary asymmetries that create/maintain the symmetry by their net effect'

        - 'time structures', 'filters and filter derivations', and 'maximal differences from standard/adjacent change combinations (embeddings, maps, etc)' are useful starting positions for identifying default/obvious and non-default/counter-intuitive structures that are useful, and identifying the intents/structures that are adjacent once those defaults/non-defaults are found, for fulfilling other intents like 'identify extreme/scaled limits of a structure' (what happens when time is iterated at scale, according to these possible structures of time), which are known relevant/useful intents for problem-solving intents like 'identify requirements' (thereby connecting these interim structures like "useful high variation interaction levels" to useful problem-solving structures, allowing variables of this connection (variables of 'core change structures') to be identified as well to fulfill intents like 'generate other interaction levels that are useful to apply as interface structures')
            - these structures are also interfaces or sub-interfaces (time is a 'useful standard that allows structures to be compared in a relevant way', which is an interface)

        - similarly, in a different interaction level of 'number types (and representative constants/units of those types)', imaginary numbers are identifiable as useful in that they are related to rotations and a 'rotation' is an input structure of another useful structure, 'orthogonality' (and can be used to create/identify other differences like independence, maximal differences, symmetries, etc), and that they connect other relevant numbers of rotations (like pi) with numbers like i which offer a different difference type, so i is useful as a default structure as it adjacently identifies other useful structures and fulfills useful intents adjacently
            - similarly, quaternions offer 'embedded differences in their interactions', like how they are equivalent alternates at generating i (using a square operation) and also have different interactions that can generate i (multiplying them by each other)
            - similarly, imaginary numbers offer independence in the form of non-embeddability, in their connection to a similar but different space (where i is a unit) whose interval-determining output points (the powers of its units, where the corner point unifying some i-sided square intersects with euclidean space, acting like an endpoint of two vectors, like every negative number is a base point of right-angled vectors of alternate i-unit operations, and positive numbers have vectors of 1-unit operations and a power of the power of the i-unit operations as well, as positive numbers cant be used as inputs to a power to generate the negative numbers) are embedded in euclidean space by reference of the power operation
            - relatedly, questions like the following are adjacent from this position
                - 'is the directedness of the negative/negative multiplication operation that can create positive values (in contrast to the positive/positive multiplication operation which cant create negative values) significant in determining the existence of a number type that is relevant in creating differences like opposites and which has embedded reflections of this connection (the imaginary/real roots act similarly, as they apply the same operation and are inputs to the original negative/positive values referenced), and are there limits to this reflection of this connection that dont align with the limits of the definitions of the numbers/structures themselves but rather require different structures to identify those limits'
                - 'does the directedness of that connection align with other structures having direction, like useful causal input/output sequences (should imaginary numbers be used as inputs to real numbers for some purpose like expanding dimensionality, just like neural networks expand dimensionality of inputs to apply multiple alternate change sequences that could connect inputs/outputs) and time (are there continuous/regular/adjacent sequences of synchronized changes in "number definitions" which can describe time)'
            - relatedly, 'intersectivity' is another possible input of 'orthogonality' and intersections which create a difference (in 'direction') in a similarity (same right angle applied every time), when applied in some structure (like an iteration of the right angle), or intersections which create an equivalence (in the area/distance/angle structures identified by the separation created by the intersection) add value in creating orthogonality, and relatedly intersectivity adds value in that it standardizes the area created by unit distances in either axis of the intersection to match (and minimize the work of) standard multiplication
            - these numbers (and structures of numbers like number types and opposites of a number type within the type definition) exist by being useful to connect other number sets, some numbers seeming to exist only to complete a set (like a set of type examples) or fulfill a useful definition like continuity or isolatability (for other intents like traversibility or repeatability or adjacency of a specific number type to another type, or to provide a connecting/isolating number in between some pair of numbers, to avoid making those numbers adjacent or equivalent or otherwise non-isolated)
            - other numbers seem to exist to implement interface structures like 'limits' (infinities) or 'interaction levels' (relative infinities), as if every number is guaranteed to have some related number that acts as each interface structure (like a limit) on it when it occurs in some set or is acted on by some operation, as if numbers are the default/unit system where interface structures can exist or be identified
            - the fact that there are absolutes (like a highest infinity or most unitary unit) indicates that there are different starting points to view number sets from, which make these absolutes obvious and which make these sets obvious (infinities acting like waves which repeat, self-sustaining changes without contradictions, or non-zero continuities which reference an infinite set)
                - building a definition of an object like 'infinity' makes it clear how some structures are more optimal to define it with, as some definitions are non-unique in that they can reference other structures and are therefore less optimal, but which can be applied optimally as a surrounding structure with other surrounding structures which act like a set of limits creating the definition
                - these interim definitions (which describe very different structures non-uniquely/generally) can be useful as a default set of definitions to apply to adjacently connect other structures
            - given that these intents (like traversibility/specificity arising from continuity) are fulfilled, they are likely to be useful in some way for other intents (such as 'make sure that all numbers are connectible' and 'make sure there are numbers that can be inferred with other descriptions/definitions, but which cant describe real structures inside the universe, to provide a path to other structures outside the universe')
            - given that a rotation is a core structure of a symmetry/center, a circle, a change, a base (like a vector set with a common endpoint as a base), and other useful structures, its useful to identify a rotation as a default structure and find equivalent alternate formats of it and routes to it

    - identify structures that are relevant/useful to useful structures (such as relevant in that they 'cause' useful structures) like 'similarities across different equations (different as in "not reducible to the same equation")'
        - for example, functions having 'similar defined input/output ranges or similar high-variation ranges where their variation occurs (as opposed to where their constance occurs which is less likely to produce intersections)' are likelier to have intersections than other functions
            - within that filtered set of functions with similar defined input/output ranges or similar high-variation ranges, functions with 'similar shapes/scales/averages/limits but which are trivially different in that they are shifted in one variable to preserve a similarity in another range, or so their difference types dont align in some variable (like when peaks/inflections are not aligned vertically)' are similarly likelier to have intersections
        - this is related to the workflow 'find interactive structures and then apply those as possible connection structures to connect a problem/solution'

    - identify structures that differentiate useful different but related info formats like 'truths/falsehoods' and 'certainty/uncertainty' and 'similar/different' and 'constant/variable', which are the foundation for different primary interfaces but are adjacently connectible as variants of each other
        - for example, a connection between 'truth/falsehood' and 'constant/variable' is constant structures are likelier to be truth structures and variable structures are likelier to be false structures (variables are likelier to be in a state of change, either changing toward a value change or decay or toward further differentiation or toward resolution into one constant value, where constant structures are likely to change toward a value change or toward decay or differentiation but have fewer states which could be likely to change than variables as constants dont have the potential to change by resolution into a constant value as theyre already a constant, and if theyre already a constant, it is likelier that they will remain one than that a variable will become constant, as structures which are likely to exist are likely to already exist in some way), so 'finding system initial conditions, scaled convergences, decay points and change thresholds/limits (as the system will occupy multiple states between these structures so finding these structures determining the boundaries of those states is more useful than finding an individual state)' is likelier to be a more useful intent than 'finding current variable interactions in a current system state'
        - identifying these connections is useful for switching between these formats, identifying useful formats, and applying multiple formats to fill in complementary information

    - identify structures that are useful for some useful problem-solving intent like 'find the remainder/some of the function, given a subset'
        - for example, structures that derive some abstract variant of that intent like 'derive a part from the complete set' or 'find a common similarity (solution value) that connects a system of equations', structures which can be used as a default structure to apply when searching for specific variants (or combinations or other structures) of the structure which are useful
        - identifying these alternate specific intents which are useful for some problem-solving intent is useful where inputs of those specific intents are already available (like 'partial differential equations'), which are also useful for other intents like 'simplify a function (by identifying which variables determine its highest variation and removing them until its linear)', which is an attribute of useful structures

    - identify structures that resolve ambiguities like 'structures that specify an example of some general description', where the 'example' or the 'example with the description' is more or independently or compoundingly useful in combination with the description, than the 'description on its own'
        - for example, the phrase 'similarly similar' is difficult to capture/define by a structure, until you apply a structure like a 'space where some example similarities are comparable and clearly similar but trivially different', where this structure is more and independently useful for its clarity through its structure than just the term 'similarly similar' on its own, and bc it occupies a more 'interim' position on the space of reality variables (independence, abstraction, certainty, etc), and makes other structures (like 'clarifying structures that make some similarity/difference obvious') more trivial to find as a result of these other attributes (similar to how 'similarly similar' makes other useful structures like 'similarly different' more trivial to find), just like the concept of 'abstraction' is useful on its own but specific useful abstract concepts like 'power' and 'balance' are more useful to apply as a default position
        - bc of the high variation possible in defining an abstract structure like 'similarly similar', specifying an exact example associated with this structure is useful to filter that set of possibilities, 'abstraction-resolution filters' being useful structures to identify
        - finding a space where 'abstract concepts are iterated and allowed to interact and vary enough to fully capture their definitions to a useful degree of specificity' (applying components of reality like 'iteration/repetition' and other interface variables like 'change' and 'interactivity' to fulfill intents like 'expand possibilities' that enable fulfilling intents possible with these structures like 'specify' and 'structure') is another useful intent

    - identify structures which are highly explanatory through interactions with simple/default/core/otherwise useful structures
        - for example, functions like 'pretend' and 'lie' can be iterated as a repetition of a core structure of 'falsehood', as 'pretending to (pretending to (lie))' is an iteration of a common structure of falsehood ('lie') which can explain more complex system dynamics when functions (like 'lie') are not invalidated ('still allowed') but simpler variants of them become less useful over time, as other agents change at similar rates
        - the 'lie' structure is useful in systems as a 'cheap/efficient structure' where 'false signals of value' are more efficient than 'true signals', but become less useful as other agents change, and iteration of these structures becomes more optimal while still being cheap/efficient, as the 'lie' structure is still allowed in a given system
        - identifying the points where 'efficient structures' (falsehood) intersect with 'inefficient structures' (truth) is useful for predicting future behaviors (it can become more efficient to tell the truth at some point, so much so that it is useful as a default strategy, as signals of true value are useful to other agents in a way that signals of false value are not, as truth structures pay dividends when applied as defaults)
        - the 'iteration' structure is useful to identify as a powerful explanatory variable when applied with core variables to generate/describe complex system interactions in a simple way (as an iteration of the same core structures)
        - 'pretending to (lie)' is a cheap way to falsely signal true value ('obviously pretending to self-deprecate, when the deprecation is actually true' to signal 'true value' in a false/cheap way) when 'lies' are applied as a default structure that can be identified by other agents
        - 'iteration' structures are a useful way to identify 'embedded/symmetry' variables
        - similarly, 'iteration and other symmetry-hiding variables' are a way to identify 'false complexity', where 'true complexity' is an interim structure between 'random' and 'symmetrical or otherwise simple' functions
        - these structures are useful to identify for adjacently predicting other future structures, like how an increasingly optimal 'truth identification machine' would capture & alter reality more quickly (given the relative simplicity of default 'simple or otherwise adjacent/default, but false' lies, as well as default lies based on truths), given its usefulness at supporting a high degree of variation, and how supporting more variation attracts more variation
        - similarly, identifying variable structures like 'degrees of truth' and useful cases of differences within those variables (such as 'statements that are minimally true, given the structure of the relevant definitions' and 'statements that are obviously true/false' as a way of finding 'connections between definition-invalidating differences (some structure like a limit making some statement impossible by trivially changing it to cross the limit) to connect structures like the minimally true with the minimally false and maximally false (impossible), which are useful to apply to make other structures like their differences obvious')

    - identify different useful interactions between data set subsets, complete data sets, solution functions, solution function ranges

        - identifying networks of 'data set subsets that could be a subset associated with many highly different complete data sets or solution functions' which can be used to navigate between data set subsets, starting with each of the subsets that could be the most alternate complete data sets or solution functions (having the most in common with the most other subsets) and testing for difference types to navigate to more specific ranges of possible complete data sets/solution functions, up to a point of accuracy on some metric

        - identifying maximal differences in a data set that can be mapped to one function in a set of maximally different functions, as a range of different shapes of a data set that could be described by some solution function that is a function in a set of maximally different functions, as a way of finding the maximally different data sets associated with a solution function that is maximally different, to 'identify possible solution functions to use as a base' and 'identify whether the data set matches any of the maximally different data sets associated with each of the possible maximally different solution functions as being within an error range of some metric'

        - identifying 'implications of symmetries in a possible solution function' as useful for intents like 'determining the remainder/some of the function from a subset of the function', as 'symmetries' can be formatted as a 'core prediction structure' that can be applied to 'predict some subsets of the function, once an "implication of a symmetry (like a similar slope sequence)" is found', and similarly identifying structures of symmetries like 'symmetrical peaks' and 'symmetry breaks (which can indicate a position of a new symmetry)' and 'symmetry embeddings (of layered/nested symmetries)' is useful as a set of structures to identify as an input to a function to 'identify the remainder/some of the function from a subset'
            - relatedly, identifying 'differences from symmetrical functions/data sets (like symmetrical waves)' is a useful intent when fulfilling intents like 'building a function similarity index' and 'predicting the remainder/some of the function from a subset', as 'differences from symmetries' are similarly useful as 'symmetries' to know about as an input to a 'find the remainder/some of the function from a subset' intent
                - relatedly, identifying a space of similarities that are useful through co-occurrence like 'intersection ratio' and 'symmetry type (like recursion or rotation symmetry)' function similarities, which are useful to identify when they co-occur, as this provides alternate functions to determine similarly similar functions, is a related useful intent to 'finding useful similarities to build function similarity indexes on'
            - relatedly, identifying symmetry-breaking changes (like 'sort') that alter a function beyond its symmetries is useful to identify 'functions to avoid, in a space of functions to apply when identifying functions (like "blur" or "compress" or "filter") that can change an input like a data set or subset or function into a more useful format'
            - identifying a 'general symmetry score' across 'maximally different spaces where various structures like combinations of symmetries are used to defined distance/position' (which functions retain their symmetries when indexed differently by some set of determining function similarity metrics) is a useful intent for 'function similarity indexing'
            - identifying 'maximal differences' in a 'function similarity index' is useful for finding useful alternate solution bases to apply as inputs to a workflow like 'apply changes to a base solution function to find a more optimal solution function', as 'navigating the function similarity index to find patterns of maximal differences and therefore finding the maximal differences' is useful for finding patterns of contradictions (determining differences between similar functions) to filter the solution space (like 'identifying when a function only overlaps with another on one point or at inflection points or in general structures or at its roots or at a high ratio of subsets' and 'identifying the useful structures of these similarities/differences to use when navigating the function similarity index to find patterns of maximal differences'), as identifying 'functions with the highest similarity score and the highest difference score' is useful for other intents like 'find common/probable functions that also are maximally different from other functions (unique) in some way'
            - relatedly, identifying the useful 'interim function similarity indexes' which are in between a 'similarity index of all possible functions' (most expanded index) and a 'similarity index of abstract function-determining/describing variables' (most compressed index) is another useful intent related to 'function similarity indexes', similar to how finding a 'similarity index of similarity indexes' (organized by absolute similarity or abstract similarity or otherwise useful similarity such as 'realistic similarity' that mimics their indexes' true connections in reality, like how 'similarities are embedded on foundational similarities, up to a limit') is useful as a way of organizing these indexes in a network that can be queried to find relevant functions/similarities/differences, interim indexes which are useful structures to identify for their useful mix of 'reality dichotomy' variables like 'certainty/uncertainty', 'simplicity/complexity', 'similarity/difference', 'independence/dependence', and 'generality/specificity'
                - relatedly to these 'reality variables', identifying structures that would be useful ('spectrum variables determining reality') which can be used to fulfill other intents like 'identify a continuous space representing reality to traverse, to fulfill other intents like "identify areas/points/structures of optimality"'

    - identifying useful interface structure interactions in a problem space and applying those based on their interactions (like their interactions with requirements)
        - for example, a general and specific function are often different but there are cases where they are the same, so framing their connection as 'more probably different but not definitely different' is useful for determining differences probable/required in different solution functions
        - a 'general' change type is less likely to occur with a high degree of variation, and similarly a 'specific' change type is likelier to occur with a high degree of variation, but these are also not required
        - connecting these conceptual functions with other interface structures like probabilities and requirements is useful for identifying a solution function based on conceptual requirements like 'an interim function in between extremes like general and specific'
        - identifying and filtering out cases that dont apply for being less likely or less useful (like cases where general/specific functions are the same) is useful when identifying structures like 'data set-function indexes'
        - relatedly, identifying when a different function format is optimal, like identifying cases where its optimal to retain a 'mix of different (as in unrelated) specific/general functions' in the same solution function, is useful to avoid suboptimal structures for a problem or input
 
    - identifying interface structures resulting from common queries connecting interface query components like 'what would be useful if it was true' (in known/measurable and/or complex or otherwise useful problem spaces) that can be applied as alternatives to those query components
        - in the case where the solution intent is a 'way to change greenhouse gases to avoid heating' and a 'specific intent' implementing that general intent is a 'way to configure molecules to avoid heating' and specifically a 'way to gather molecules to avoid heating', applying a common query like 'what would be useful' (what structures would make this possible/adjacent/efficient/optimal in some way) identifies useful sub-intents like a 'way to scale existing molecule-interaction tech (like lasers) to cover more area than usual (like in different shapes like cones to have higher output from the same or similar input of a laser)' which filters the solution space and identifies structures like 'scalability' or 'efficiency' that can be applied as alternatives to the query 'what would be useful to connect intents or specify some intent'
        - this is actually a new structure bc its an 'interim intent' (between problem structures and solution structures) which would be useful if it was true (but isnt definitely known, as its not an existing/known useful structure, so it has to be generated/derived/found as a specific structure that is useful for a specific problem space, then checked for usefulness), which provides a useful filter to focus work on to identify if inputs to that useful structure are possible, at which point the useful structure can be created and applied to fulfill the target solution structure
        - in the example of the 'find a drug for a condition' problem, the related useful intents that can be derived include intents like 'find a drug that has general system optimization attributes (like improving inflammation or lipid metabolism)' or 'find drugs that fulfill multiple coordinating intents' (and apply them as a default reduced solution set) or 'find drugs that fulfill multiple contradictory intents' (and filter them out as less probable solutions)
            - similarly, 'find a drug that changes system-level variables' (with associated implication regarding inputs like 'bc the input condition is a system-level condition') applies an 'interaction level' filter to find a useful filtered intent that is more relevant bc of the filter
            - similarly, 'find drugs that change most variables' (with the associated intent of 'to find drugs to exclude, as these produces too many changes, more than what is likely required to correct this condition, and is likelier to produce more negative side effects than positive side effects even if the positive side effects are to correct the condition')
            - similarly, 'find drugs with some structural similarity (like similarity in functionality structures, that reduces some attribute/function) in the intent/function/interface structures (like functionality) to fulfill specific bio-system intents like "regulation of errors from over-functionality" (which align with reduction of functionality)' (find 'reducing functions' to fulfill 'regulation intents' bc of the structural similarity producing the alignment between 'reduce/regulate' intents)
        - these intents are useful to derive bc they provide focus in reducing/filtering the space of structures to search for (now the problem is 'find a drug with this attribute' instead of 'find a drug that corrects this condition', which offers value in its specificity and may also be more trivial than the general original problem-solving intent)

    - identifying useful computation intents and the limits of computation which would make those more useful when incompletely implemented in a specific subset of ways
        - for example, in the 'regression' problem space, identifying a '"data set-function" uniqueness mapping causal/filtering diagram' which maps 'sequences of differentiating/determining points to check for in a data set' to select a 'representative function'
        - this type of algorithm network (a network of directions representing probable flows as causes or filters of the sequence toward an output function, and nodes representing data set points (or missing data set points as a useful signal, or similarly data set subset areas, densities, similarities/differences, local averages, or other related data set structures), and leaf nodes representing output functions) would be useful only up to a point, with finite computation available (unless every available computer is forced to constantly compute more complex mappings to infinitely compute it as much as possible)
        - determining that point of limits on its usefulness (only compute the mapping up to a level of difference in the data set represented by some metric like a ratio of randomness applied to common variable interaction functions, and only up to x number of data points in the data sets) can be determined by filters like 'reduced rate of increase (diminishing returns) from reward from computing more fine-grained or complex data set-to-function mappings'
        - this is the 'trial and error' algorithm applied to a 'pre-computed index of problems (data sets)/solutions (regression functions)', which is more useful when applied incompletely (selective trial and error) if implemented a specific subset of ways (applied to maximally different data set/function pairs covering many types of variable interactions in real complex systems, etc)
        - finding all the 'ways a function can be incomplete but still useful/similar (like retaining slopes at determining points which can be used to derive the function to some degree of accuracy)' is an intent that can identify the differences (subsets taken out of the function which allow the function to retain its general structure and determining attributes like inflection points) that should be applied as filters of functions to solve the regression problem for when building a 'unique data set-function index causal/filtering diagram' ('direction reversals (like found at peaks)' are a useful difference to apply when identifying functions to solve the regression problem for) or identifying function types/base functions to check for when filtering the full solution space
        - checking for 'function volatility sets' (extremely different functions that can be generated with trivial changes, which can be used to check the rest of the data set for signs of the other functions in the volatility set which indicates a higher probability that the function is in that set and switches between them given the allowed input variable interactions) and 'sub-function volatility sets' (extremely different subsets of a data set or function with extremely different implications for the rest of the function which are almost contradictory but are part of the same function) are other useful filters to apply to avoid checking the whole data set while still identifying its general probable structure
        - identifying 'subsets of a data set or function that can be used to derive maximally different functions' is similarly useful bc these should be discarded or specified with additional filters, rather than applied as specific filters that have a one-to-one mapping to a solution function
        - similarly, 'identifying data set input ranges to divide it into subsets to select from, then a subset of a data set across many different input ranges using some filter, then identifying the implied solution function (or structures of it like a probable range), then applying checks for differences that would confirm/invalidate it with the most certainty (like combined difference types when the solution function is tested on some subset, such as constant/non-linear slope and high value difference)' is a similarly useful set of sequences to identify, to select a subset of probable solution functions
        - similarly, identifying specific error filters of maximally different functions that could all describe a data set (like how a highly volatile wave could look like randomness) is useful for identifying possible errors like false similarities and tests to filter those similarities (the differences in the similarities, like a 'lack of direct connection function between peaks')
        - similarly, identifying 'areas of the solution space to skip, given the extremity of differences to the probable solution function parameters and variable interaction functions' is useful as a way of filtering the solution space to only include similar functions (by their parameters and variable interaction functions) to the probable solution function like a general average function, to discard extremely complex functions for example, even if they use the same parameters, as extremely complex variable interactions are less likely
        - similarly, identifying similarities/differences between structures of values/functions 
        - identifying similarities between the 'differences produced by unit function interactions (normalized to within a certain range, such as exponent/multiply or add)' and the 'data set structure' can be made obvious without retaining the original values in the unit function interactions (if unit functions produce a peak, that similarity to the data set structure is obvious (by changing angle or surrounding structures so as to make comparisons to determine equivalence of a shape trivial) without scaling it to the exact data set scale) and finding similarities in this 'unit function interaction structure (different functions produced by unit function interactions)' as well as differences to 'non-unit function interaction structures (different functions produced by non-unit function interactions at different scales or otherwise different from unit structures)' is a useful intent, these differences resulting from scale of these unit/non-unit functions being useful to identify and connect as equivalences to values of inputs
            - 'identifying a set of functions of inputs that create the same differences as a particular value of an input (making that input an alternate)', and 'identifying a set of functions that changes according to the scale of the function inputs' are useful intents to determine variable interactions
        - identifying differences (like 'intersection points of upper/lower bound lines, these intersection points indicating a difference of "parallelism between upper/lower bounds, which is useful for indicating a clear average line"') between sets of related interface structures (like 'pairs of upper/lower bound lines indicating useful differences in data sets') which offer an alternative to some computation (identifying an intersection invalidates the computation of determining similarity of slope) is a useful problem-solving intent

    - identifying useful structure sets and connections between them, such as how alternative structures can often be combined with an additive usefulness
        - identifying connections of interface structures like 'causes/inputs of a problem' with structures like 'adjacent/existing resources as invalidating/opposing inputs of those problematic causes/inputs' is useful as a common interface structure set that appears in problem-solving processes in a useful way
        - example: moving co2/methane so they cant create the greenhouse effect is a more adjacent target than pushing them farther away so they disperse in outer space or compressing them and storing them in the ocean, so tech like lasers can be used to move/separate the gases so they dont have that side effect, as the 'compression/adjacency' of these gases/molecules is the problem creating the side effect of 'trapping heat'
        - specifying this set of interface structures to make it more useful can be done by applying filters (like 'are functions available, or likely to be available given other interface structures like common interaction functions, that can move/separate molecules/clouds of these elements')
        - connecting 'adjacent resources' with 'one of the causes of the problem' is useful as a structure to start from, to specify with filters
        - alternate structures are useful to identify such as 'find functions that are more possible (moving horizontally into non-heat trapping structures), if other functions are known to be less possible (moving outward toward outer space)'
        - similarly, other alternate structures like 'core structures with opposite functions of the problematic functions' ('different configurations of molecules that dont trap heat') are useful to identify as alternatives and optimizations of this workflow
        - 'more adjacent/possible functions (moving in a different direction)', 'opposite-function core structures (molecular/relevant/core differences as useful differences), 'connections between problem causes and adjacent resources', 'connections between adjacent structures like available resources and non-adjacent functions', and 'specifying filters' are useful to identify and apply in the same workflow (since the 'connection between problem causes and adjacent resources' is so general, its useful to 'apply specification structures like filters', as workflows should 'connect opposing ends of spectrums (a default foundational structure that allows these opposites to be connected) that act like primary interfaces' to be useful, just like they become more useful when they connect generality/specificity, uncertainties with certainties, constants/variables and similarities/differences)
            - therefore connections between these 'opposites within spectrums' (of primary interface variables like abstraction, information, change, system) can form a default 'workflow component set' to apply changes to (like connecting them to problem/solution structures) in order to generate other workflows
        - multi-function structures, as in structures that are useful alternatives which are also useful optimizations to apply in a combination as well, are by definition more useful, so its useful to identify alternatives for the problem-solving intent 'identify useful optimizations resulting from combinations of alternates'

    - apply variation to useful structures (like problem-solving intents) to find specific structures fulfilling those structures

        - for example, for the 'find highly similar (in a known way, which can be applied as a constant, reduced and ignored) but relevantly different (in an unknown way, which should be expanded and focused on) functions' intent, applying differences to that intent to connect it with problem-solving structures identifies useful structures like useful intents to fulfill

        - the following intents ('identify useful changes to a base function', 'identify variables of a function type', 'identify similar/representative structures as the data set (like subsets)', 'identify useful function similarity types and variables generating maximal differences within that type') are relevant to this 'find similar but different functions' intent and can be generated as specific examples/variants/implementations of it which are relevant to other problem-solving intents/structures

            - identifying errors (like worst case scenarios like 'randomness') and directions of errors and check for change in that direction to avoid changing a function in the direction of errors
                - identifying/deriving/generating the possible errors like 'randomness' in a problem space like 'regression' is useful as a general problem-solving intent
                - another example of a 'worst case scenario' is where very different functions (different in the important ranges, to a significant ratio and degree) appear to be the same in some subset that is coincidentally selected as the subset to check as a representative subset
                - identifying 'similar functions (like by similar sub-sections, integrals, etc)' is useful for applying differences to a suboptimal solution function (to check for errors to revert to a previous solution function or otherwise correct a function), specifically 'functions having locally similar sub-sections/slopes that are maximally different in non-local subsets' are useful to identify as a 'worst case scenario' to misidentify (a false illusion of local accuracy/similarity that is highly inaccurate in other subsets), so identifying these 'worst case scenarios' and 'function similarity spaces' allows these similar but different functions to be checked for, to avoid that 'worst case scenario' for that function similarity type
                - changes applied (to the 'find similar but different functions' intent or other problem-solving intents/structures of the regression problem space): rather than identifying local minima of an error function or a generally representative function by applying some average metric (to minimize differences from that average), identify directions that are useful/suboptimal to apply changes in, by identifying 'worst case scenarios' and identifying directions of 'worst case scenarios'

            - identifying variables (like 'start/end position') to create a standard function space (such as where function positions are overlapping as functions have been normalized to occupy similar start/end positions, to highlight other differences than 'start/end positions') where function structures are standardized to more easily identify the maximal differences supported by a function type definition (like 'polynomials') as well as function structures like specific variables/variable interactions that break/reduce those definitions/differences, is useful to fulfill intents like 'find similar but different functions'
                - changes applied: rather than starting from a data set and identifying connections between points, start from solution function definitions, then identify & apply standards that are useful for intents like 'identify maximal differences within a range (like a function type or within a function area or having some points in common)'

            - identifying useful structures (like 'data set subsets' and 'limits') that represent the data set in some way (meaning, equivalently, 'are similar to the data set in some way'), and can therefore be used as a constant base to apply differences to, such as finding a local subset set and a point in each subset to use as a constant subset to base maximal differences on, like the 'maximal differences in functions that intersect with those points', given that these 'constant base' structures represent the data set to some degree (above a set of randomly selected points in the entire space), so applying changes to them (a solution automation workflow) is likelier than randomness to find useful functions relevant to the data set and solution function
                - changes applied: rather than finding just a 'solution function' structure, the definition of a solution structure is expanded to include other structures like data set subsets and limits (like 'function ranges') as similarly useful in their capacity as a constant base to apply changes to, as a 'base solution function' like a 'cheap average to find'

            - identifying function similarity indexes (like 'integral value similarity' and 'primary/powerful term similarity' and 'intersection point similarity' and 'slope similarity' and 'inflection point similarity') (and structures like useful combinations of these similarity indexes) which would fulfill useful intents like 'reduce differences between functions' to fulfill other useful intents like 'find similar but different functions', which is useful for problem-solving intents like 'find maximally different functions, within some similarity range', and identify the variables that connect these similar functions of that type of similarity (such as 'specification/generalization' variables which create similar function integrals, but inject a high ratio of differences in the function variables) to fulfill intents like 'find similar but different functions'
                - these function similarity indexes are useful to traverse as a way of filtering possible solution functions (after first generating the similarity index as a set of probable solution functions, given some known subset of the data set applied as a constant input)
                - changes applied: 
                    - 'probable function areas/ranges/limits' as a 'solution structure' of the 'regression' problem space has been changed to have a different 'difference/distance definition' (the similarity index)
                    - rather than traversing the whole (or a high ratio of the) data set, a subset of the data set is identified, a similarity index is selected based on that subset, a set of probable similar functions is generated by that index, and that set of similar functions is traversed, checking for known differences within these similarities outside of that subset which is used to identify similar functions

            - identifying function-adjacent structures, given its definition (like 'continuous'), such as 'areas of the data set which are more adjacent to a continuous function (like a dense subset) than other areas of the data set (like a sparse subset)', which can be prioritized when connecting/selecting subsets
                - this is similar to identifying subsets that are easily identified as belonging to the same function (such as 'points on a constant line'), but applies a different similarity type ('continuity' as a 'similarity' through 'connectivity' between adjacent points)
                - changes applied: connects different types of similarity such as 'continuity' as a 'similarity of adjacent points', which is more useful to identify than connections between distant sparse points bc there is more uncertainty in those connections and uncertain connections are a less stable base, allowing for more variation in function variables

    - identifying useful mappings like the mapping between the 'system-math' interface, which is where most of the useful algorithms have yet to be found
        - the 'system' interface can depict concepts like 'power' in a useful way, depicting a compressed variant of reality in the system network diagram (power dynamics are easily visualized in a diagram like this, such as 'some nodes would have more connections and more resources than other nodes, and would use that to cause changes in other nodes')
        - multiple methods to map this into a continuous differentiable function exist, such as:
            - mapping concepts to function attributes
                - mapping 'powerful variables' to 'more impactful variables', meaning 'higher variation-generating variables, like the primary exponent or high volatility-causing variables, or highly limiting variables, like variables that make a function never reach infinity or otherwise restrict a function (a wave-generating limit variable)'
                - mapping 'number networks (like important connections between 1 and other numbers like itself, its square, its opposite, its inverse, its nearest positive even number, etc)' so that 'powerful variables/functions' can be found by 'connecting these numerical networks (which have emergent conceptual/systemic attributes like "similarities/equivalences between variants of a number") by conceptual queries' for intents like 'find the most powerful/simple/adjacent (or other common/required attribute) connections between input/output values'
                    - this directly maps nodes on the math interface to system/conceptual attributes using 'function networks of numerical nodes', which could be done with 'representative (maximally different) numbers'
                - similarly, mapping 'abstract variables' to 'general variables', meaning 'higher variation-generating variables, like the primary exponent'
                    - relatedly, finding 'structures in common' across the various examples of power structures in various systems will identify default inputs (used like an 'abstract base' of the definition) like a 'maximal difference (high-low ratio)' that these structures frequently adjacently depict in common, so that finding 'limiting structures on what is not power despite having a high-low ratio or other related structure to a definition of power' is similarly trivial and useful
                    - why do this at all? 
                        - bc knowing certainties like that a 'specific set of concepts exists in every system' allows a program to iterate through all (or a filtered set) of the possible structures depicting these concepts and testing those
            - other mappings like 'finding/generating/deriving system sub-structures like 'system dynamics' and then applying vector-based changes to these system structures, then applying those vectors as input variables to find a function'
        - this is one of the more important mappings to link other non-math diagrams to existing methods like statistical regression
        - the workflow would be 'find a possible system diagram of a data set, then find a regression function for some subset of variables, then apply the system diagram regularly in a cycle to tune that function, since the system diagram will make useful structures clear, like when errors could occur that would disrupt the function, allowing for better prediction of the function'
        - network diagrams (simple connections between nodes) are useful in that some attributes emerge from the network in specific cases, like when a 'hub node' is identifiable by its high number of connections, and other structures can be implied/derived from that like concepts such as 'power'
        - the system diagram is useful in that it has default objects like 'cost/benefit' (which functions like a 'gauge' in that it is a 'revealing sub-structure that, similar to a filter, reflects other structures/makes other structures clear when applied as a certainty', structures like 'efficiencies') as it allows differences in related values (like positive/negative values) to emerge from various agent angles/trajectories in their pursuit of goals while traversing the system, where these objects are defaults
            - this is useful when the system can be depicted in such a way that the negative values seem negative in that structure (the costs are below/beyond some line/plane, and they visibly aggregate according to their actual variable interactions, from some angle, the opposite for benefits)
        - some of these graphs only differ by attributes like 'whether they allow repetition (which is a structure where symmetries might fail through over-reducing parameters)' or 'whether they isolate or group some structure' or 'whether they show all structures like "node-connection functions" or over-simplify/over-abstract to just show that a connection exists', these attributes making them useful for different intents
        - causal graphs are limited in what structures they visualize, but make some useful structures clear, like 'independence', 'input/output sequences between a start/end variable' and 'requirements (as in the "only connection leading to a particular node")', as opposed to 'order-independent' operations like 'set membership' and other graphs where the operations can be assumed/applied by default, rather than checking for sequential position (what matters is not whether one is independent/dependent, but that one variable/function always occurs in some developed/complete system/set)
        - similarly, a 'graph of similar functions by inputs or outputs' makes it clear 'what other functions could adjacently become or falsely seem similar to another function', which is useful for deriving structures like errors in the possible systems containing a function
        - similarly, finding the set of graphs where some variable interaction would be preserved (whether by some subset of variables, like relative position or distance) is useful for finding alternate graphs that dont violate some constraint as a 'truth' structure
        - similarly, identifying function structures that can be adjacently combined to fulfill problem-solving intents like 'summarize', 'predict', and 'describe' a high ratio of variable interactions
            - for example, finding a set of maximally different base functions, and identifying functions to fulfill interface structures of those functions (like a function to create an overlap/intersection between functions by applying some adjacent changes), identifying similarity scores (how many and what type of changes were necessary to create intersections), and identifying connections between these interface structures and useful solution metrics of functions like generality/similarity (a ratio of intersections at intervals indicating one could be a generalization of the other), so that these base functions can be re-applied across problems as defaults, and problems can be standardized to the 'regression' problem space to make this function set more useful
            - this specific function set applies some interface structures as defaults: a set of base solution functions (to use in a specific workflow 'change a base solution until its more optimal by some solution metric'), a set of concepts like generality, a set of interface structures like 'regular overlaps' that can be used to connect base/other functions with a solution function, given these conceptual solution metrics and those concepts' connections to those interface structures, a set of similarity indexes that can be used to identify similar/different functions for various intents, and a way to make these interface structures useful when applied in coordination (an 'input/output sequence' of the workflow that makes them useful)
            - this is an alternative to other workflows that connect math interface structures with problem-solving and other interface structures
            - this is a useful set of structures bc it offers a filtered set of specific structures to apply (rather than a bigger or more complete set of structures), through applying insights/understanding to filter specific structures that are more useful, rather than treating each variable as an uncertainty (the insights add some certainty and limit/filter the set of useful structures)
            - the reason that applying interface structures like 'overlaps' to the problem space of 'regression' (in a specific structure like 'regular overlaps') is that the interface structure of 'overlap' encodes a specific structure of the concept of 'similarity' and therefore is useful in connecting/differentiating various structures in the problem space, like possible similar solution functions, possible similar base & solution functions, possible similar general/specific functions, etc
            - other structures of similarity like 'intersectivity at inflections' and 'intersectivity at roots' can be used as complementary additive structures of similarity to determine similar functions
            - identifying similar functions is useful for intents like 'identify the maximally different functions out of the set of the most similar functions (most similar to other functions)' for intents like 'find the subset of functions that cover the highest ratio of functions by adjacent changes', which is useful for other intents like 'find variables that can be used to adjacently generate these function subsets', 'function similarity' being a specific implementation of the 'interactivity' structure that can be applied in workflows involving connecting 'input/output sequences' (where first, interactive structures are found, and then these are used as component inputs to other structures like 'connections that can connect problem/solution structures')
            - this type of solution that covers variable interactions that can be solved with regression can be enhanced by other math interface structures like 'space embeddings' to create spaces where other useful function structures can be found, to connect a problem to a type of space where the solution structure is more obvious, which is another useful problem-solving intent

    - identifying useful structures like 'minimal interface structures required to describe most variable interactions'
        - for example, some concepts are useful in isolation of other structures, and some concepts are more useful when structure is applied to them (when combined with the structural interface), so that a minimal structure that is still useful requires both the concept and the structure, and a set of the minimum interface structures required to describe most variable interactions would contain all of the concept, the structure, and the interaction that is useful, where the structure and the interaction would overlap with the usefulness of the concept, and replace some of its usefulness, so rather than describing the concept fully and applying that, including the structure and the interaction as part of the useful structure is more useful than applying the concept in its full in isolation
        - this means there is a set of structures on each interface that should be included in a minimal set that provides complementary value rather than redundant value (which is useful for intents like 'find the minimal set of maximally different structures to build the maximally different structure')
        - similar to how identifying the concept of 'power' is useful in isolation of many other structures, identifying how 'power' interacts with other variables and systems (such as the 'set of functions of how power is frequently misused when over-centralized') is similarly useful in isolation of many other structures, where identifying the 'interaction of the concept & function set, as well as identifying just the concept and the function set' is more useful than identifying either or both, in isolation of their interaction, so the 'minimal useful set' is the concept, the function set, and the interaction function, rather than any subset of these
        - this insight is useful bc it adjacently identifies useful graph structures, such as 'finding the ways that all concepts vary and aligning those ways in a set of aligned network graphs'
            - for example, all concepts vary by structures such as 'multiple' and 'unit', so creating a stack of networks (where each network identifies the variations of the definitions of a concept), then aligning these networks so that 'multiple' occurs in the same position (relative to the vertical stack of concept networks, or relative to other nodes on each concept network), is one way to implement the interface network structure, where queries would be able to use 'vertical position alignments' to find cross-interface interactions such as similarities or mappings between interfaces (where the concepts would include interface-specific concepts like 'functions' rather than only abstract concepts, although just applying abstract concepts in this way would be useful to identify how concepts interact with structure)
            - relatedly, other interface structures can be applied, to identify other similarities, like 'common variable interaction structures' as a structure to align concepts on
        - generating other useful graphs is trivial with other workflows, at which point insights can be reverse-engineered from these graphs bc of their usefulness for some intent, using this graph-insight connection
            - by identifying the usefulness of 'aligning similar structures across graphs', the insight of 'a minimally useful structure set involving overlaps/interacts between structures' can be adjacently reverse-engineered as the 'connections between these graphs' (the 'overlap' structure of the 'multiple' structure which is obvious when viewed after applying some core variables to find new insights like a change in perspective such as 'vertically' is similarly/compoundingly useful as both of the individual graphs, so that these should be grouped as a 'useful minimum' of a structure set)
        - in this way the 'structure' interface (with structures like 'multiple' and 'unit' as some of its default core structures) is embedded as the 'host interface' that connects the others, which can be switched to other interfaces (the 'meaning' interface is the default 'host interface' of interface analysis, which organizes structures by relevance/usefulness), or alternately some interaction function of the interfaces (like the concept-structure interface) can be applied instead, such as applying a 'minimum useful set of interface structures' as the host interface
        - other useful structures to apply can be found by applying variables to definitions of useful concepts and identifying the most useful structures resulting from those variables
            - for example, applying variables (like 'variables') to find interactions with useful concepts like 'simplicity' can identify other useful structures to align networks on (like 'low numbers of variables' which is the structure of 'simplicity' applied to the variable of the concept of 'variables'), so a 'variable count' becomes a possible useful structure to align networks on, which would identify other useful structures for their specificity (like 'linearity' on the 'math' interface)
            - similarly, applying the 'low count' attribute found with this application to other interface structures like 'functions' identifies other structures of 'simplicity', such as 'low count of sub-functions in a function', which is useful to store as a default useful structure in isolation of other structures but also in the same set as other structures, as including the 'low count' in a set of defaults including 'count' is compoundingly rather than redundantly useful, rather than generating 'low count' every time, as if it's not a known useful structure
            - identifying other 'compoundingly' (independently, complementarily) useful structures can be done by applying attributes of other compoundingly useful structures (such as 'structures that can aggregate to create differences, rather than structures which can be simply similarized and therefore made redundant, using simple combinations of structures like concepts such as 'simplicity'), where 'similarity' doesnt completely equate to 'connectibility/dependence' as some similar structures are 'falsely similar' or 'independent and/or indirectly connected' (more reliably or adjacently connectible using some other origin/base/symmetry), but it is related to that concept and can be used in some contexts
            - aligning the networks so that 'low count' of variables on the 'change' interface is aligned with the 'low count' of 'sub-functions' on the 'function' interface embeds the concept of 'simplicity' as a 'positional' attribute of these graphs, so that the position of simple structures on the change/function interfaces are adjacent/overlappping in this structure and therefore trivial to compute
            - finding different structures of an abstract concept like 'adjacency' (which can describe almost anything, depending on the graph's positional attribute, if the positional attribute determining distance between points can describe extreme differences, which is possible) by applying this method (of applying core variables of interfaces and finding interaction functions connecting them to the original concept of 'adjacency') would identify structures like 'similarity', 'simplicity', 'efficiency', 'probability', structures which trivialize/reduce differences (like 'commonness', 'types', 'expansion structures (that capture a high degree of variation to trivialize differences)') and other useful structures, where this network of concepts related to 'adjacency' is useful in isolation of other structures, bc of the specificity it adds to the definition of this abstract concept
            - applying some specific concepts is more useful than other concepts, such as applying 'independence' as an organizing attribute to separate truths/falsehoods on different sides of a network (as 'maximally independent' structures are less likely to be connectible, as in causally connected, as in related/relevant, which is another way of deriving the 'minimum structures to create the maximally different structure')
            - relatedly, some structures should be paired/grouped, such as how the 'system-function' interface is more useful in some contexts than either interface on its own, as the 'system in which a function is used' and the 'function structure' are not possible to completely isolate as they depend on each other for important metrics like 'emergent functionality', as the 'system' can be taken as an 'input of the function' and the 'function' can be taken as an 'input of the system' with variable definitions of 'input' ('context' in one definition variant, 'component' in another, and 'interactive structure' in both variants)
            - similarly, 'empathy' is a useful structure in that 'maps between different systems' is adjacently derivable with a trivial change 'isolating the structure of the process (the system-mapping function) and the emergent result of the process (the map between systems) and related interface structures (the "alignment" required for the map to be useful)', which identifies other structures that make insights trivial to identify (high variation-capturing, high variation-reducing, highly structural functions) as an alternative to useful graphs like 'aligned network stacks which embed a useful concept in the alignment, making other intents trivial such as "mapping across systems"'
            - optimally, the interface network implementation would have 'maps between systems' as a default structure on the interaction level of the host network (a network of 'maps between systems (such as the concept-structure map, the system-function map, the requirement-potential map)' as a default implementation structure, which would add 'high variation-capturing' functionality and 'alignment of high variation' and therefore 'maximal differences' by default)
            - relatedly, similar concepts united by a common definition are useful for other intents like most other useful structures are, such as 'find causal variables (since similar structures are likely to be inputs as previous states)'

    - identifying useful structures like 'error structures of common structure formats (like incentives)'
        - 'incentives' are useful as a structural format as they capture a high ratio of information such as 'adjacent changes' and 'defaults' and 'benefit/cost ratios', similar to how similarities/differences capture a high ratio of information
        - finding error structures of incentives (like 'carefully avoiding switching to another strategy when it becomes optimal, just bc its easier to continue making adjacent/incremental changes in a wrong direction') is useful as a way to solve problems once formatted as incentives and opposing structures of incentives (like limits on benefits and cost-maximizing structures that create disincentives)
        - an opposing (solution) structure to this error structure would be 'generating alternate solution functions and calculating switching points when one becomes suboptimal compared to another one given the switching cost, to navigate between alternate solution functions'
        - a 'solution-switching' function (fulfills a 'mix/change' intent) is similarly valuable as a 'solution-filtering' function (fulfills a 'find' intent), just like an 'alternate solution generation' function is valuable (fulfills a 'build' intent)
        - similarly, useful structures in the 'math' interface have usefulness emerging from the variation of their interactive components (the 'set & its associated functions, and the limit/symmetry the set/functions interact with', or the 'definition of distance/similarity and the emergent similarities (the attributes that are continuous (topological) or preserved (symmetrical)) using that definition', or the 'defining/standardizing functions like the gauge/kernel/norm/basis of a space and the similarities (like continuities/convexities/supersets) related to structures in that space and its defining/standardizing functions'), which offers a default problem-solving structure that can be changed to create other useful problem-solving structures as they represent a variant of the un/certainty or difference/similarity set that describes all problem-solving structures

    - similarly, identifying alternate graph structures and the connections between them and other useful structures like the interfaces they represent and the structures that make them more useful
        - for example, a 'causal network' is useful bc of other useful structures (derivatives) and structures like 'injected nodes' which allow for other variable interactions to be considered, and is useful for finding useful structures like 'alternate causes', 'high-cause (powerful) variables', 'requirements' or 'dead ends'
            - specifically, an example of a 'causal network' (implemented as a neural network) is a graph that allows for multiple alternate paths between inputs/outputs to be considered, additionally having multiple variables per path (where various layers provide nodes on the path), which is formattable as an 'angle sequence (query bundle) set that allows overlaps, where the angle format is relevant bc its directed'
            - the core structure of causal graphs can be enhanced by other structures of cause, like other relevant measures of similarity (such as 'root/inflection intersectivity with and adjacency to other functions'), which is not directly determined by derivatives, which describe differences in change across variable sets
        - similarly, other related graphs are useful such as 'boundary graph (indicating boundaries between sets)', 'threshold graphs (which graph areas between thresholds)', 'query bundle graphs (a graph that connects different queries of another network that are equivalent/similar in some attribute like meaning/structure but differ in structure/endpoints)', 'variable networks (a standard variable connecting graph without causal direction)', 'gravity/hub graphs (which prioritize graphing variables that are powerful, common, or highly connected/interactive as opposed to graphing direct causal connections, as these are the important variables to graph, as other variables gravitate around them or are directly caused by them so they can be left out)', 'heat/energy maps (which graph specific definitions of variation/potential)' and have associated useful structures making them more useful
        - these graphs visualize a primary variable of reality that has a value in all structures, that corresponds to an abstract variable and also appears on other interfaces, where other graphs that are useful can be generated by applying variables of these graphs and applying those requirements (this reverse-engineers the primary interfaces, by identifying which graphs capture the highest ratio of variation and variables of those graphs)
        - for example, 'a graph of tensors of cross-interface/graph change types' are a useful structure, as a 'set of interactive/powerful change types' that occupy different graphs, as an alternate format of query sequences across a network like a language network
        - another useful graph might be a slightly more complex function network having boundaries between function sets which are sub-structures of the function network, like a graph of 'areas of similar functions (like different areas for generative functions and filter functions)' where any line on the graph has to select one function from each area/set, as these are useful functions when grouped together
            - injecting similarities in each area (like organizing each area by complexity, commonness, interactivity or another useful dimension) would make this graph more obviously useful, in identifying the function attributes that create the most effective combinations for some intent
            - graphing 'boundaries of function sets' in function networks may make some other structures obvious, depending on the connection/position function used to create the graph, such as where there are probably missing nodes to fill in that are not already known, to capture variation indicated/implied/defined by adjacent sub-structures of the network

    - identifying alternate interaction functions between core problem formats, workflows, and solution-finding functions
        - for example, the 'mix' function can be applied to other solution structures like 'input/output sequences' which normally you would apply as inputs to a 'connect' or 'combine' function (in a 'connect problem/solution' or 'build solution from solution components after breaking down problem into sub-problems' workflow), but when a difference is injected at the 'connections between useful structures', value is still retained in that these functions are interactive and act like equivalent alternates in many cases, so they can be switched without losing too much information
        - similarly 'combine filters' and 'reduce solutions (to common/simple solution variables)' and 'break solutions into solution components' are similarly useful as the default core workflows but require adjacent differences be applied to generate them and also require some useful structures to make them more useful
        - this function can be optimized by applying other useful structures like 'interactive' input/output sequences, which reduces the uncertainty of the inputs to the 'mix' function
        - this applies a 'difference' to 'core useful structures' like 'core interaction functions of solution automation workflows' which is applying another workflow 'apply changes to useful structures to find adjacent useful structures which can act like approximations/inputs/otherwise useful structures by their adjacency to useful structures'
        - similarly, identifying the full set of functions to adjacently connect useful interface structures like 'core interaction functions' and 'problem/input formats' (such as 'how to change the mix function or the input format so it can be used with the mix function, like applying useful filters')
        - similarly, identifying the 'functions to connect known un/certainty structure sets (as well as adjacent variants of them)' (connect incorrect variants which are likely to occur in reality, so that the input or function needs to be changed before it is useful)

    - identifying useful structures like 'incorrect definitions where the correct definition is definitively in-between them or similarly calculatable from the error definitions'
        - for example, a 'symmetry angle' is where both points to be connected are equidistant from the origin of the angle (the point overlapped by or connecting the two lines), and is in-between the angles which are not symmetric, which is useful to identify when finding symmetric angles, by applying a line and a copy of it until theyre connected and changing those lines until theyre connectible
        - these sets of structures like 'known incorrect angles that do not create symmetries' which allow some useful structure like a symmetry to be calculated trivially from them (like applying some filter like 'in-between' to the 'position symmetry')
        - similarly, some unit structure of symmetric angles like a 'right angle' (which is shorter than other angle types and has equidistance possible at its midpoint) is useful with some change set like 'moving the right angle until one point connects to some structure to be connected with the symmetry and rotating and scaling it until the other point is overlapping with an endpoint'
        - similarly, an angle that is just barely not a straight line is similarly trivially possible to position as a symmetry (identifying the midpoint) that overlaps at its endpoints with the two points to be connected with the symmetry
        - these 'error structures connected with these change sets' add 'specific (and therefore useful) structure' to the solution by their definitions which differ from it
        - these 'in-between solution structures' can be calculated when some error structure is similar to another error structure by being its opposite in some way, at which point, its possible that a solution might exist between them (the errors might occur on either side of the solution in this case)
            - this 'function to determine cases where error structures offer information about a solution' is related to this function set as another function to include in the set

    - identifying useful functions to identify like 'identifying absolute (or relatively absolute) truths and applying them locally'
        - for example, identifying an 'absolute truth' of 'sun position' and 'angle to an object' creating a 'symmetry in shadow angle reflecting sun position' with limits like 'daytime motion' and 'positions where sun isnt blocked or otherwise missing like inside/near a building' and 'time interval like "hour of the day"' as a 'key variable' to determine 'direction of motion' and 'routes or components of routes like sub-routes that could maintain some direction from start to finish or generally' as well as 'differences in route or sub-routes that could change direction', for a problem like 'move in the same direction' which requires solving problems like 'find out the direction of some motion'
        - this requires finding 'absolute truths' by identifying 'what is true in different positions' (such as the similarity of sun angle in different positions at the same time) and 'what structures can identify different positions (a field structure where the same rules would likely apply)'
        - this requires connecting that 'absolute truth' with the 'current position/problem', so a 'function to connect absolute and local truths' is a related useful function to 'find absolute truths by identifying what is true elsewhere'
        - related functions include functions to 'identify what truths will still be true or will become false when some position is local (when change of type "motion" has occurred)'
        - this is useful as a solution automation workflow bc the dichotomy of absolute/local contexts is reflective of reality, so applying it with an 'un/certainty' structure and 'connecting the two extremes of the spectrum' is useful at solving all problems
        - relatedly, connecting some reality-describing dichotomy variable like 'abstract/specific' using variants of the definition of those structures on different interfaces (apply the 'abstract' interface to other interfaces and connect the structures across interface) is another set of functions that can be used to solve all problems (such as how variants of the definition of abstract/specific (such as 'local' and 'example/exception' and 'subset' and 'limited/contextual' which are variants of 'specific', and 'global' and 'type' and 'general' and 'superset' and 'acontextual' which are variants of 'abstract', which are 'structures of specificity/abstraction (on other interfaces than the abstract interface)') can be used to connect the concept's structures across interfaces)

    - identifying useful definitions of useful structures that add value in terms of metrics like the descriptive/generative/connective capacity of the definition
        - for example, a function set that 'identifies variables which are also "maximal differences" to the point of being "independent" (as in providing "complementary information") and adjacently connects those maximal independent differences (since theyre still related despite being independent or there wouldnt be a requirement to connect them)' is a useful variation of the 'interface' definition bc it provides a useful target for filtering the set of all functions and offers a base for other 'solution automation workflows', as a function set like this would be able to solve most or all problems, as most problems can be formatted as a problem of missing information about how to connect very different variables (which cant be connected in some obvious way without knowing interface analysis, or it wouldnt be a problem and could be easily solved by an error)
        - similarly finding the full set of functions that adjacently connect independent maximally different variables is useful as a set of alternative solution automation workflows
        - similarly identifying structures like 'maximal differences' and 'complementary information (as opposed to redundant information that reflects the same truths)' that create other useful structures like the concept of 'independence' is similarly useful
        - this structure applies the same core useful structure of a 'highly differentiating filter' or 'primary interface' or a 'un/certainty pair' that is a default structure of 'problems', as the 'connection between the variables' is certain and the uncertainty is 'how theyre connectible adjacently or otherwise optimally', so applying variations to the connection can be a proxy intent for solving all problems, if the variables are sufficiently independent that their alternate connections could reflect a high ratio of information about reality
            - relatedly, a 'similarizing function' that can make any structure similar to some 'core set of structures' is useful for finding 'equivalences'
            - relatedly, a 'differentiating function' that can make any structure similar to some 'core set of differences (problems)' is useful for finding 'differences'
        - this workflow is also useful for finding the 'level of specificity/generality' at which some definition is maximally useful for problem-solving (similar enough to some useful core definition structure used to create other useful structures that it can change into that core structure, and similar enough to known useful structures that it can easily become those, and different enough from other definitions that it can be independently useful for different tasks but similar enough that it can act like an equivalent alternate to some set of other useful definitions to replace it if necessary)
        - similarly, identifying useful structures of definitions such as 'structures that appear falsely similar/different but are/not really the same structure' is similarly useful (for example, identifying that 'reasonable sustained (real/legitimate) changes' may falsely seem like an error such as 'hypocrisy', if some information is left out)

    - identifying functions that can connect useful structures adjacently across interfaces as a useful approximation of interface analysis
        - for example, finding a function set that can identify structures that fulfill priorities (like 'prevent harm to innocent agents' and 'incentivize harmless/beneficial functions' and 'incentivize intelligence development'), requirements (like 'fulfill common intents like store data for quick retrieval'), finding error structures (like error structures of a rule like 'too good to be true', such as 'the exception to a rule where it would become false' and the 'threshold that this error crosses'), finding variables (like 'finding variables of errors'), which when found & connected across the primary interfaces would approximate interface analysis, as an alternate minimum function set that acts similar enough to interface analysis to approximate it
        - similarly, function sets that can identify useful structures like 'similar inputs, similar outputs, and input/output sequences/sets' are useful structures for 'prediction' intents, just like 'highly differentiating switches' are useful for 'sort' intents, and 'highly differentiating filters' are useful for 'find' intents in that they are 'components of solution structures'
            - relatedly, other structures are useful for other reasons than being 'components of solutions', such as being causative, determinative, differentiating, required (rather than just inputs/components of solution structures which are adjacent to those structures when connected by their default interaction function 'combine' applied to these components)

    - identifying different function networks/graphs that are useful for different intents (as a proxy for implementing interface analysis) is a useful intent to solve for
        - a function network (of function types & other similarities like input/output, attributes like volatility, sub-functions, potential, best/worst cases) organized by similarity of relevant metrics like 'output attributes/structures' is useful for switching to other similar functions when one function seems to be false, starting from a core of common different functions
            - areas on this network where functions have useful similarities like 'similar sub-functions and similar input/output connections (like function shapes)' are useful to identify
            - this would be useful for tasks like 'find the most incorrect that a function can be while seeming correct at some subset of points' and relatedly 'find the way to connect these "incorrect-implication point subsets" with "correcting point subsets"'
            - finding the intents that can be adjacently fulfilled with these function networks is useful, to identify how similar these fulfilled intents are to problem-solving workflows & their interface queries, or whether these fulfilled intents can be composed to fulfill problem-solving workflow interface queries or fulfill the workflows directly
            - the network can be organized to make these areas predictable/calculatable so that different areas (areas of functions that differ in these attributes) are also predictable/identifiable
        - finding function sets that can generate other function sets (covering the highest ratio, the most adjacently, etc), such as the 'function set that can be composed/changed to generate the find/change/derive/build/connect function set'
            - for example, a 'structure function network' might have common function structures (like a peak/wave, line, rotation, layer, overlap, etc) and it would generate functions by composing these with adjacent transforms
                - this structure function network might solve the problem of 'quantum attribute observability' by finding common structures and assembling them in a way that seems to match observed attributes of the quantum functionality, such as a 'fluid or other non-constant state of some component of spin/position that both have in common, which resolves to either spin or position depending on where most of the fluid is concentrated at the time of observation on its rotations' or 'intersecting wave functions that intersect with zero at various points so that one has no value when the other has value' or 'a spectrum with multiple thresholds in different directions, where above either threshold, an attribute like position/spin is measurable' or other common structural dynamics that seem to match some attribute of observed quantum functions
            - similarly, a network of common meanings/intents (given the relative lack of variation in these in common structures) so that any new problem can be solved with adjacent combinations of these common component meanings/intents is similarly useful as a function network built the same way (as in 'common component functions')
                - a useful insight is that primary interface structures capture a high or total ratio of information but also reduce computation requirements drastically (meaning there arent that many abstract concepts/intents/functions that are equivalently useful, despite the ability of these sets of structures to describe a high ratio of information), a metric which can be used to find other primary interfaces or proxies of them (these primary interfaces are highly similarizing structures, despite supporting a high ratio of differences)
                - common meanings include structures such as 'common interaction functions' as well as 'common concepts like balance (which are abstract enough to describe most systems in some way, such as balance structures like justice/symmetry/equivalence/opposites)'
            - just like 'sequences/sets' are different core structure formats which are useful for different intents ('sequences' being useful for intents like 'derive/connect' and 'sets' being useful for intents like 'combine/build'), other structures are useful for different intents ('commutative sequences' can be implemented as 'sets', thereby connecting the two formats with a directly measurable/calculatable attribute)
                - these interaction levels ('isolatable sets of interactive & adjacent structures') of equivalent alternates (sets/sequences) that are connectible using core interaction functions ('change order of objects/functions') are useful to identify as 'units of reality' that can be applied and connected to other similar interaction levels to create a useful function network of these interaction levels, which would similarly act as a proxy of interface analysis (interface analysis being the derivation/generation of interface queries likely to be useful/relevant for a particular problem, using interface definitions and known useful structures like 'core function sets')
        - a function network of 'similarizing differences' and 'differentiating similarities' would be similarly useful, as a way of implementing a 'filter network' that fulfills the 'find function' requirements (find any information from any other information, using differences like relevance/interfaces/variations)
            - these structures are useful for various specific intents, such as how 'similarizing differences' is useful for 'identifying types that different structures have in common' and 'differentiating similarities' is useful for 'identifying variables that could indicate sub-types or new types, which differ across structures having the same type'
            - similarly, 'differentiating differences' is useful for making differences more obvious (differentiating from an average), and 'similarizing similarities' is useful for identifying an abstract type or merging different examples into a representative average (the most similar structure)
            - 'identifying groups' and 'identifying the most different abstract types that similar structures can be reduced to' are related useful intents of 'similarizing similarities', which is useful for the efficiency of the description by using types instead of the full set of attributes/values
        - these function networks have structures in common like organizing the network to make non-trivial calculations adjacent to high variation-capturing structures like 'common known functions/components'
        - these function networks (a function network optimized to 'find areas of similarity across multiple attributes', a function network optimized for 'finding the most adjacently generative function set to use as components of all other functions', and a function network optimized for 'finding similarizing differences and differentiating similarities to filter all information adjacently') are similarly useful at capturing high ratios of information but use different 'core concepts that optimally standardize information' and 'variable/constant sets' (applying some structure as a default and allowing related structures to vary to handle uncaptured information) to implement that & other intents
        - similarly, a 'potential' function network organized by related concepts of potential (like 'adjacency of what a structure can be with few changes' and 'extremes of what a structure can be if scaled enough' and abstract concepts like interactivity/power/variability) would capture and organize information in a similarly useful way, as would applying any core concept as a default constant to organize a function network
        - using queries between these function networks is a useful proxy for interface analysis, since it allows queries to be run like 'find an obvious error or symmetry using the similarity/difference network (which identifies these structures quickly bc of the definnitions of their core default structure), otherwise proceed with finding similar functions on the function similarity network organized by input/output similarity and sub-function difference, to check maximally different functions, given some subset of known inputs/outputs, to find the rest of the function'
        - similarly, finding differences like the 'differences between optimality and acceptability (the bare minimum over a threshold, connecting optimality with relevant threshold structures)' which are likely to describe a high ratio of variable interactions based on other useful structures like 'incentives' are intents which are adjacently fulfilled with a network of function networks organized this way
        - similarly, a network of error structures, where the network acts like a 'selection/filter function to select between errors' is similarly useful to identify, as a way to prioritize some abstract conceptual metric like 'balance' by navigating between specific error structures, as every perspective has 'obvious errors/suboptimalities', but some errors are acceptable/optimal in some contexts (inputs/starting points, query paths/functions) and others are not
           - for example the 'balanced perspective' specifically would have more optimal errors like 'being different from most specific useful structures (but different in a similar way, as in different in an equivalent and adjacent way)'

    - symmetry structures applied as a default variable structure, rather than 'incremental change combinations', so that by default, symmetry combinations are sought, rather than incremental change combinations, to always frame change in terms of symmetries
    - combinations of useful intents (such as those that are relevant/realistic) as a default interface to base changes on, finding the functions to fulfill them at query time, as implementation variables
    - structures of relevant cross-interface structures as a default structure to apply changes to
        - for example, 'patterns of causal structures of structures of structures' (like the patterns of cause of 'a sequence of structures that develops from another structure')

    - identify the useful structures (like 'metrics optimized for') in useless structures (like error structures), as everything has both positive/negative qualities, and if there is an error, its bc there was something initially or somewhat useful about the error or it wouldnt have occurred (the error state is a lower-energy/more maintainable state, for example) as a way to find other useful structures like 'similarities in differences' that will have compounding value across problems
        - for example, the 'useful structure' of an error structure like 'hypocrisy' is that a system can contain contradictions/paradoxes/differences without destroying itself, which is useful for generating differences, a core intent of problem-solving
        - this is a 'similarity in the difference/error' structure that is useful across problems, and relatedly 'finding the oppositely-charged structure of a structure (finding the positive structure of a negative structure)' is another useful intent of problem-solving
        - to find 'similarities in differences' in the 'usefulness of a useless structure' (or to find similarly useful structures like 'structures that can support opposite ends of a spectrum' or 'spectrums' as useful structures), applying interface analysis again makes this trivial, but also identifying 'common' structures (like similarities/differences) or 'common error structures' like 'self-invalidation/neutralization/contradiction' structures is another way or applying useful perspectives like 'optimism' (with related intents of the perspective like 'find a way to make everything useful' or 'create differences from errors to find solutions')
        - as another example, the 'useless error structure' of 'blaming the incorrect source of an error' has a useful structure that adjacently derives the insight that 'some functions can handle blame as they are highly variable' and therefore the usefulness of these functions is their variability which makes them 'independent' of the blamers, 'variability' and 'independence' being another useful structure derived adjacently from this useless error structure
        - deriving the attributes of these 'adjacently useful useless error structures' such as their 'measurability' (which is useful in that it makes them trivial to avoid), is similarly useful as a core intent of problem-solving, as some functions develop new functionality to handle the responsibility of blame, and responsibility is an important useful structure (related to fulfillable intents as 'functionality' and requirements)

    - identifying error structures like 'gaps in existing resources/inputs like neural networks' and matching those to 'possible specific functions to resolve those specific error structures'
        - for example, ai has errors such as:
            - 'not identifying/fixing its own emergent outputs' (emergent outputs like 'inability to improve itself on optimal human metrics like meaning, efficiency optimization using interaction level switching, and intent-selection')
                - ai cant switch to using structures like 'civilizations' on other 'interaction levels' by default bc it uses combinatorial approaches in many cases, applying some core unit as an input to a combination function that will never adjacently derive those structures, so computing a function like 'a competition civilization collided with a meta civilization' isnt something it can do bc those arent clearly defined, have high computation requirements (without interface analysis to adjacently derive understanding), and they are not adjacent combinations of anything, so an 'interim thinking' algorithm that uses interface structures will get the answer before an existing neural network
                - this is bc it applies a 'bottom-up' approach by default (combining some core unit) and cant switch that to something else (applying complexities like 'civilizations/universes/rule sets as defaults to overlap/merge/connect/format', as opposed to 'core units to combine')
                - it also cant identify this error structure in its emergent output and cant design another ai to fix that error structure
            - 'not identifying its most efficient paths to derive all variable connections adjacently'
                - an example solution structure could be 'applying an algorithm to find the variation-maximizing subsets and substructures of a neural network is a useful algorithm to include in the network, so those substructures can be applied by default for high-variation intents'
            - 'not filtering/generating its own usage intents to optimize its own usage, as it has no concept of its own usage intents that isnt directly input by a human'
                - identifying a graph where pro-social intents are computible/determinable is a useful intent to apply with neural networks
            - 'not applying/identifying/deriving meaning structures like "the emergent impact of a structure in some or all computible contexts or from every angle" or "the adjacent alternate definition routes of a structures that determine/describe/generate/compress it efficiently" or the "most useful structures that fulfill all meaningful intents adjacently"'
                - 'an algorithm to identify useful structures and requiring neural networks to identify those first' are useful intents to fulfill with other related intents rather than requiring human users to apply changes to neural networks to fulfill those intents manually
            - 'not identifying other specific useful structures like similarity metrics and regression algorithms and spaces where some computation is reducible to an inner product calculation which are comparatively useful compared to neural networks and making sure the neural network can adjacently derive these and other new useful structures and apply them (injecting nodes to allow itself to work on that during training or new incoming prediction calls, as new types/generative paths of "differences from optimals" are identified)'
                - ideallly, the neural network would inject new nodes and then not propagate those until it finds a way to reduce the new nodes injected to some interaction level where its functionality is adjacently derived/applied/found
        - 'identifying error structures describing this difference between existing functions and understanding functions as well as the solutions opposing those error structures' is a useful intent to fulfill
        - identifying structures to map useful concepts (like 'wideness' and 'deepness') with structures of the neural network (like 'adding weight path sequences and more possible changes in between input/output layers') and the conceptual impact of that ('increasing ability to handle variation') is another useful intent to fulfill

    - applying concepts related to 'truth' like 'absolute/infinite', 'reality', 'consensus', or 'sanity' (as a structure of 'being correct' or 'connected to the truth' as opposed to 'being wrong' or 'disconnected from the truth') to derive 'sanity interface queries' that can be used to fulfill intents like 'filter true statements'
    	- for example, an implementation of a sanity algorithm would implement functions that relate to sanity, some component of it, or some input of it, or fulfill some intent/output of sanity better than or similarly as sanity does, within some degree of change applied to the set of known facts that matches a sane ratio
    	- relevant sttuctures of 'sanity' include 'regularly checking if the self is creating errors' and 'regularly testing solutions for correctness' and 'identifying different types of correctness' and 'using true statements as inputs/defaults/constants' and 'avoiding improbable structures' and 'avoiding immeasurable/untestable structures'
    	- as a counterpoint (to implement the insight that 'every truth statement has a related false statement that is likely to be true is some other possible context'), some insanity (as 'being wrong') is useful for some intents (like 'creativity (to add differences to reality, as in improve the truth)', 'identifying patterns and variables of wrong structures as useful through being different from/opposite to right structures')
    	- so a 'sanity algorithm' could start with 'true statements', and apply 'insanity' structures to 'create differences from these true statements' for intents like 'finding new connections between true statements or new networks where those true statements are false' or 'identify errors that could be adjacently derived by true statements'
    	- finding similarities in common between these various conceptual algorithms implementing the definitions of those concepts is useful to find a general algorithm of truth

    - identifying useful structures like 'connections between useful structures' such as connections between requirements/symmetries and useful tests/changes to identify requirements which can be used in place of other structures bc of these connections like 'using the test to identify a requirement in order to identify a symmetry'
        - bc requirements of a system are often sufficiently equal to be its symmetries, a change that identifies a requirement (such as 'removing a variable to see if the system re-generates it bc its required for the system to continue to exist') can also identify a symmetry or similarly a stability
        - the connection between 'requirements and symmetries' as well as the connection between 'changes to identify a structure and the definition of that structure' are similarly useful in fulfilling problem-solving intents like 'connect useful structures' and 'find variables of useful structures'

    - formatting 'neural networks' (or 'regression') as a 'matching' problem, as in 'finding a subset of the network to start applying changes to inputs at, as some sub-structure of the network seems to fit inputs/outputs of the original data set adjacently, then injecting/removing nodes within or around that sub-structure as needed, or adding connectivity to other sub-sections/sequences of the network that seem adjacently useful'
        - relatedly, deriving the 'probable change types (like some exponential change or directional change) or other interface structures' required to connect inputs/outputs of the original data set, finding a sub-structure of the network with those change types, and applying that sub-section as default initial changes, adding more or connecting more as needed to connect inputs/outputs approximately/completely
        - similarly, finding 'optimizations' (such as 'approximations') of the 'regression' problem space (such as 'only finding the subsets with negative slope' or 'only finding subset filters and approximate local averages of those subsets and connections between those local averages') that can be applied to reduce the computations required and change the target of the solution (an approximate solution rather than the original solution)

    - identifying patterns of 'reasonabilizing functions' that connect some structures in a way that makes sense (a 'common sense' function implementation)
        - for example, to connect the very different words 'sky' and 'encryption', you can use an interface query like:
            - 'what is encryption used in' (communication)
                - 'what supports/enables communication (what is an interface on which communication can develop)' (network)
                    - 'what is a variant of that interface which enables communication that is relevant to the target (sky)' (satellite network)
        - the interface query of this sequence ('what is it used in', 'what is an interface of that', 'what is a relevant variant of that') is relatively simple and captures a high ratio of information about the variable connection
        - this isnt just a useful sequence for solving one problem, but it also connects structures to other structures in way that makes them reasonably connectible ('reasonable' connections involve 'intents', 'usages', 'foundations like interfaces', and 'relevant structures such as adjacent change structures', which are reasonable to use as defaults in determining reasonable/probable and therefore realistic connections)
        - finding a 'reasonable' solution connection/function that has multiple reasons why it could be real/true (as it connects to multiple reasonable structures like relevant/useful/usage structures) is useful as a general problem-solving intent
        - finding interface structures like 'patterns' of interface queries that are reasonable (or otherwise 'make sense', 'sense' being 'similarity/equivalence to the truth/validity/other structures of truth') is similarly useful
        - this is particularly useful when information about possible/true structures is missing
        - relatedly, a structure that adjacently connects very different structures that are not related (like highly variable structures which offer different sub-interfaces such as 'sights/sounds' which are only connectible by core abstract structures like 'frequency') would allow more similar structures (like problems/solutions) which are related to be even more adjacently connected
        - similarly, a way to generate variables that have reasons for the variation (like the variable is similar to an equivalent alternate of the original variable in some way, like how 'acidity' can be derived in 'water' by applying attributes of equivalent alternate 'core elements' like 'heat' which has a 'burn' function) as a problem-solving intent to fulfill
        - similarly, a graph where the direction of errors is known/guaranteed bc of structures like 'reasons why an error occurs (bc of its initial advantages before becoming a measurable error)' is also useful for implementing a 'reasonableness' algorithm

    - identifying a function to connect 'inputs/outputs' with 'solution functions' using 'connection structures (like generalization)' to predict an 'error graph where errors are obviously knowable/identifiable/differentiable' is a useful problem-solving intent
        - the 'error function of (a solution function of) input/output connections' is not immediately adjacent for a complex data set, which is the problem solved by neural networks/regression
        - as an example, this 'conversion function (from a data set to a solution function, or from solution function to an error function)' could apply a mix of differentiating structures ('de-volatilizing/specifying or generalizing a subset of the graph', 'applying local averages to a subset', 'applying maximal differences like change in cardinal directions to another subset') in ways that are similar enough to the data set that they are reasonable to apply (within a 'reasonable range') if there is a more optimal summary function or a summary function that is adjacent with these structures
        - the point of solving this common problem is to find the error (the missing info) without having it clearly/adjacently graphed in the input/output connection function
        - the 'solution function of connections between inputs/outputs' acts like an interface between the data set and the error function, where the target of regression is the error function determining how accurate the solution function is for a given data set, so mapping the data set to the error function on the solution function interface, where predictable (as in similar) solution structures can connect the data set and some subset range of error functions, is a useful intent to solve for
        - this would answer the question 'which error functions are probable/possible for a solution function, given specific data sets' and 'which of those data sets align with (are similar to) the actual data set' or in the other direction, 'which solution functions align with the data set and which error functions are associated with those solution functions'
        - another format of this intent is 'what error functions are possible, given some solution function and some adjacent changes to it using some similarity metric', which is a useful problem-solving intent to solve for
        - given some similarity of 'solution function' error (like those producible by solving for 'maximally different subsets' of the data set or 'applying a function to summarize points at regular intervals of subset selection/sampling' or solving for a 'generalization of some more specific function or a specification of some more general constant function'), where 'predictable similarities' in the error function can be generated by applying these structures as variables to vary within reasonable limits (the solution functions produced by similar interval sampling should be similar), these 'predictable similarities' in the error function can be used to identify similar ranges of the error function like 'lower ranges with probable minima' (the changes that lead to 'generalization of a function' or 'volatilization of a function' can be mapped to a set of ranges in the error function)
        - the point of this is that applying 'generalizing changes' (as opposed to 'adjacent changes') to a solution function has a predictable impact on the error function of a solution function set (the solution function will either be clearly right or clearly wrong almost immediately by applying generalizing changes (which involve a high ratio of assumptions and resulting variation from those assumptions), so the error function producible by generalizing changes will be predictably/reliably volatile)
        - this is related to the insight that 'powers/opposites/additional terms are likelier to produce volatility than other structures', as it involves the connection between highly differentiating functions (like de-randomizing, generalization, or mixes of interface structures within reasonable limits) that could produce a summary function, and the preserved highly differentiating attributes like 'volatility' of the error function set producible by that differentiating function applied to create different solution functions
        - other functions that 'skip directly to other minima' or 'clearly differentiate errors across similar solution functions' are useful to identify as default structures to use/mix in problem-solving workflows
        - finding a 'range of reasonable limits' in which possible changes should be limited is also useful for solving the 'find a regression function' problem
        - similarly, answering the question 'what solution-generating methods produce some useful structure in error functions (like constant vs. curved vs. volatile error functions) given some more measurable subset of data set attributes' is another way to format this intent
        - relatedly, determining measurable errors (like 'generalizing a shifted version of the solution function, where the error is that it was generalized on a different level bc of a mis-subsampling, producing an error that could be corrected by a shift down/up, as the error is constant') of these solution-generating methods like 'generalization'
        - relatedly, finding 'similar/different variables' that are adjacently usable to generate local minima identified in the error functions of a subsample of different 'input/output points' is a useful intent using existing algorithms like gradient descent, as the variables in common are likelier to explain other 'input/output point' sets and the variables which differ are likelier to explain different 'sources of error (interfering functions, emergent phases, switches between alternate functions) at different points/ranges/phases' or other useful relevant structures
        - this means given some subset of adjacent sets of solution/error function connections, the rest of the error function is determinable, by applying these 'differentiating' structures
        - this is relevant for cases like evaluating the 'loss function of a specific solution function, across all input/output sets', like where volatility in the data set should appear in the solution function or the loss function will be volatile (these are 'conceptual math connections' between these functions in the regression problem space)
        - similarly, some 'similar solution-generating functions (or solution-generating functions)' such as 'inversions/rotations of a solution function around some symmetry, which will preserve info in the form of some error value for some input/output set' will have some similarities in their error functions, these solution-generating functions being relevant for 'generating different possible functions to connect an input with an output (or to minimize some input/output-connecting summary metric like the difference in the integrals of all input/output sets)' (the 'error function' as referenced here)
        - another related point is that individual input/output connection functions or output/error difference should not be considered in isolation and should always be calculated with some other input/output set
            - a useful application of this point is to evaluate a generated possible solution function for multiple maximally different input/output pairs at a time, evaluating whether some improving change to the function exceeds some metric like a specific ratio of input/output pairs (x% of the pairs' error function values are improved by the change, as in the error function values are decreased) and only keeping changes above that ratio
        - relatedly, another useful application of these structures is to evaluate input/output sets until the 'determining attributes that reduce the set of possible solutions sufficiently' (such as volatility, randomness, etc) are identified (meaning they stop changing to some degree), as a way to select a subset of points to evaluate, as the 'set of functions having the same volatility, proximity to randomness, etc are determinable from those attributes' and once you know which 'function set having similar attribute values' a function belongs to, you can trivially determine other equivalent alternate functions in this set
        - 'similarities across different error functions' can be predicted/tested with a function to generate solution-generation functions that preserve similarities
        - formatting the problem as 'differences between relevant function structures' such as 'differences between error values of one input/output set compared to average input/output sets' highlights other relevant differences to these loss/error/solution functions/solution-generation functions
        - formatting solution functions as isolated subset lines (at various selected intervals) to indicate a 'high ratio of probable outputs' is another possibly useful solution function format that offers a representation of the data set as definitely crossing or represented by some set of subset lines but not being constantly-defined elsewhere
        - relatedly, finding errors resulting from specific/similar methods, such as a 'specify' function that optimizes for 'number of intersections with original data set', which can have errors like:
            - 'in the worst-case scenario, selecting a suboptimal subset to apply as specific test cases of a solution function, such as a subset that is simpler than the rest of the data set'
            - 'a case where the correct solution function is a wave, but the "specify" function implementation optimizes for "number of intersections" so it finds a straight horizontal line for a solution function, bc there are many points in the middle of the wave and that is the subsample selected to optimize for as test cases'
        - this means finding 'change types' that create other useful structures like 'generalizations', and applying those to predict the structure of the error function given the solution-generating function that generates similarities/differences in adjacent error values for a particular input/output set
            - for example, changing the inputs in some way (such as increasing the subsample ratio or improving the accuracy of the subsample filter like requiring it to apply maximal differences) might have the same effect on accuracy as generalization, without being generalization
            - differently general variants of a function might have similarly low error function values (might be mimima of the error function)
            - finding these alternate structures to generate generalization effects might allow skipping to other minima from one minimum
            - these alternate structures offer similar attributes in terms of preserving/representing relevant info about the data set and removing irrelevant differences

    - identifying connection functions/variables between known useful structures like functions/structures to fulfill intents like 'de-noise or otherwise vary the original data set to change its shape to more probably or definitively correct structures' and 'de-volatilizing functions of a data set, to identify adjacent more probable/predictable/general/otherwise useful functions' and 'probable function ranges' and 'optimal solution-generating functions which frequently produce absolute minima in error values with adjacent changes' and 'differences from randomness' and 'invariance to best/worst cases' and 'function attributes preserved across changes generated by a solution-generating function' and 'functions with equivalent determining attributes like volatility/intersectivity/adjacency/representativity/connectivity' and 'standard statistical structures like the probability density function' and 'local point set merging functions' and 'alternate representation/solution/accuracy metrics' and 'alternate subset function-connecting functions' and 'info-preserving functions' and 'alternate useful data set formats' and 'local subset filters' which are known useful structures in the 'regression' problem space which are also maximally different and have likely other different variants to apply that are similarly useful, and which connection problem/solution structures like 'adjacent structures to problems/solutions' in different ways
    
    - identifying useful structures that can identify useful attributes like 'intersectivity' as a 'useful similarity metric' and the intents these structures are useful for like 'finding maximally different functions with similar output functions'
        - for example, the method of 'solving systems of linear equations' can be extended and applied to find 'intersectivity' to maximize that to fulfill the intent of 'finding maximally different functions that produce similar/equal functions' as a way of generating different solution functions to filter
        - similarly, the attribute of 'commutativity' can identify 'equivalent alternate' routes to a particular value

    - functions that can adjacently derive the concept of an 'interface' (given its usefulness) or other useful structures are useful for their proximity to useful structures
        - for example, formatting the 'division' problem as a problem of finding a function that 'finds similarities (like a unit base) in the differences between the divided number and its divisor (like a similarity of the divided number or the divisor to some base like 10) that make the problem trivial to solve' applies the concepts of similarities/differences in a way that is proximal to the concept of other useful structures (like an interface)
        - identifying functions that generate adjacent useful structures of useful structures is similarly useful as finding the direct functions of those useful structures, even though the variant isnt the exact definition of an interface or another useful structure, as they will approximate the usefulness of the useful structure if applied in that position

	- functions that act like useful/powerful filters of relevant information, such as:
	    - a function that can determine an attribute like 'input change type' that reveals similarity/difference of a relevant structure, like 'output change type (integer)'
	    - more generally, a function that can determine attributes that reveal 'information about a lot of other information'
	    	- such as how an 'average data point' reveals 'a lot of information about other data points (in that most of them will be near the average)' and an 'extreme/limit of a range of data points' reveals 'a lot of information about other data points (in that most of them will be within that range)'
	    - as another example of highly useful information like types & extremely differentiating filters, absolutely/generally/frequently/conditionally/probably true statements about 'truths/falsehoods' also reveal a high degree of information compared to the input information required to determine their applicability
	        - for example, given the truth statement 'truths usually have a counterpoint that is true to some degree in some context', a 'proof of falsehood by contradiction' which rules out a possibility bc one contradictory example is found can be considered a contextual proof, as there is usually a context where every truth is false (there may be some space that corresponds to the space in the proof where the interaction rule holds true rather than being contradicted)
	        - what is the common factor between useful extremely differentiating filters, types, and truth statements
	            - there is a high degree of information embedded in these structures (when you determine a type of an object, you may know many other things about the object such as the type attributes, similarly when you know whether a filter rules out some object, you know a lot about that object such as its attributes differentiated by the filter, and similarly when you know whether a statement fulfills some truth statement, you know a lot of information about its truthhood)
	            - these structures are highly organized and well-defined/rigid, meaning they are useful by their 'certainty' for comparison/differentiation tasks to determine uncertainty structures
	            - these structures are extremely useful/relevant to many useful intents like 'determine truthhood of a statement (test if a solution is correct)', 'filter out sub-optimal/error statements/solutions', 'classify a statement as true/false or another information type'
	            - these structures occur on different interfaces and have the 'certainty' attribute in common, making them corresponding 'certainty' structures on different interfaces
	        - similarly, other structures can be framed as 'high information embedding structures' such as 'function subsets that act as implementation methods of interface analysis', 'the set of efficient compressions of reality', 'the maximally different variable structure where any problem difference is easily connectible with a query on the structure', the 'optimal set of interface queries that solve most problems adjacently or otherwise optimally'
            - similarly, concepts & other primary interface variables can be framed as 'high information-storing/embedding' structures which are powerful through these functions and similarly offer high ratios of information (knowing the concepts related to a structure gives a high ratio of information compared to non-primary interface metrics)
            - similarly, some structures are 'sufficiently different to some specific structures (as in different from simple structures enough to be capable of handling stress/complexity/embedded variables)' and 'sufficiently similar to other structures (like input information, as they preserve info)' that they are more probable, and likelier to be legitimate/real/truth structures than other structures

    - identifying useful structures that are cross-interface (such as 'structure-concept' interface) highly useful through some metric like 'commonness' to fulfill intents like 'prediction' and 'explanations'
        - for example, 'right angles' are useful in providing 'orthogonality/independence', 'angles' are useful in providing a 'central base to form an angle for comparison', 'multiple right angles' are useful in providing more extreme differences, and 'unifying structures (like a tensor)' are useful for providing relevance, which provides a structure that fulfills concepts like 'orthogonality, simplification of comparison intents, and relevance of different structures' (a cross-interface structure on the 'structure-concept' interface)
        - therefore finding structures like 'tensors' (multiple angles having the same central base) across interfaces are more powerful than other structures
        - other cross-interface structures that fulfill these useful intents and concepts can be derived and used as defaults

    - identifying the map of 'structures that are optimally useful in some metric' and the 'specific structures which can implement them adjacently or with high probability of success or some other solution metric'
        - for example, the 'multi-task' structure is useful, where some function fulfills multiple very different intents optimally
        - specific structures which can implement this functional attribute adjacently include 'undifferentiated structures' (similar to how stem cells are useful through being 'sufficiently similar/adjacent to multiple specialized structures that they can easily become those structures' (occupying some position in between other structures which are optimized for some specific intent, as opposed to being generally useful but suboptimal)
        - the 'map structure' connecting a 'multi-task function' to an 'undifferentiated structure adjacent to specialized structures' is useful in that it can connect other useful intents with specific implementation structures
        - finding a 'specialized structure that is optimal for some specific task, which is also useful for tasks in general or multiple other tasks' is rare bc few tasks require enough functions that the inputs to the functions also cover many other function inputs and usually involves interface structures and/or high degrees/amounts of computation/data
           - finding structures which 'adjacently generate the highest ratio of function inputs' is similarly a useful specific intent to fulfill, as a specific implementation of the 'multi-task function' structure
           - relatedly, as mentioned previously, finding structures which are 'sufficiently similar to every useful structure that they can adjacently generate it' is another useful specific intent to fulfill
           - relatedly, finding 'overlapping functions having some functionality in common (as the metric of function similarity)' is another specific intent to fulfill which can derive the 'multi-task function' on its own
           - relatedly, finding the 'maximally different structure' is likely to be able to be useful for multiple intents, which is another way to find multi-task structures
           - relatedly, finding the structures which are useful across problems, like 'formats which standardize other structures, so that one function can be re-used and useful across different inputs once formatted' are similarly likely to be useful in finding 'multi-task' structures, as 'standardizing formats' compound the usefulness of functions that are useful in the output format
        - identifying the concept of a 'adjacent structure to other useful structures which can be parameterized and these parameters changed to produce other useful structures' is adjacent to the structures of 're-using an existing solution' and 'approximate solutions which are adjacent to optimal solutions'
        - the 'map structure' connecting a 'find useful structures' intent with a 'multi-task function' is similarly useful
        - this is a specific example of a 'function-intent map' structure which is highly useful across problems
        - similarly, identifying the map of 'error structures' and 'specific structures implementing that error' is similarly useful
            - for example, the 'invisible information' structure is useful as an example of a 'missing information' error, in comparison to other 'error structure implementation examples' like 'unmeasurable information', 'corrupted information', etc, which are related but different concepts and are useful to differentiate

	- a function set to determine the meaning of an interface query is useful as a 'reverse-engineering' implementation method
	    - for example, the requirements would be a function to generate all possible/legitimate interface queries and a function to determine the meaning of each query
	    - where the meaning of a query is described as the impact of the query on other relevant structures like problems and the connections like similarities/differences to other queries like on metrics like cross-problem solvabiility

    - given the primary core interaction functions of problem-solving (find/filter, build/combine, apply/change, derive/connect, organize/sort, simplify/reduce, define/structure, try/test, match/map, abstract/specify, standardize, etc which resolve some problematic information structure like a seemingly random large set to filter, or a difference between resources and target structure that can be built with those resources, or a difference between a disorganized structure and a structure that would make finding tasks trivial like a sorted structure, or extra variables complicating some structure, or undefined structures that need to be identified), apply these functions to each other on increasing interaction levels to find other functions of problem-solving
        - for example, given core interaction functions of one unit (change, filter), generate other functions on other interaction levels like multiple units ("change filters"), and connect these to problem-solving workflows ('"change filters" used to solve a problem until the filters are different enough or fulfill some other metric that they could optimize some problem')
        - as another alternative, find core structures of problem-solving like useful structures and find new functions to connect them, as an alternate way of generating useful problem-solving functions
        - as another alternative, find the variables of these functions (spectrums like certainty, simplicity, etc) and generate the set of possible interaction functions that way
        - in all of these, some structure is held constant ('units of core functions', 'positions of useful structures') and another structure that interacts with the certain structure is variable ('how those units are combined', 'how those positions are connected') which allows new structures to be found in the variation allowed within the limits framed by those constants
        - so finding the information which reflects the other information (finding that 'core functions as units' reflects information such as 'how those units can be (adjacently/usefully) connected/combined', reflecting the info through the limits imposed by those core functions being applied as 'certain constants', which interacts with that information in some way, like with a core interaction function) around some symmetry (the 'combination/connection' interaction function symmetry, allowing the units to be combined/connected in a useful way without being destroyed) is useful for finding which sets of information allow this 'certainty/uncertainty' structure to be applied to find new information structures like new connection functions

    - identifying alternate formats that problems can be converted to, such as specific problems like 'finding ways information can be preserved/stored (problems formatted as a info storage problem)', 'finding all connections between useful structures (problems formatted as a problem of identifying useful structures and deriving them from each other)', 'finding all the ways information can be hidden (problems formatted as missing information)', is useful as an input to a change workflow to derive other useful structures related to these problem formats
        - these alternate formats highlight different useful structures
            - the problem of 'finding ways information can be preserved' highlights other useful structures like:
                - the fact that info 'requires structure and also specific structures like connections to other information and limits on info storage'
                - function attributes like consistency/predictability (since some functions vary on how predictable/consistent their outputs are)
        - applying adjacent changes to useful structures such as 'applying them to each other' or 'applying them to known problems' adjacently identifies these other useful structures

	- applying specific interface structures as a default constant set
		- applying 'information' interface interaction rules of the 'physical reality' interface such as:
		    - 'structures that break definitions of some concept can be structures of falsehood, unless the structure is more stable/relevant/otherwise true than the definition, in which case it can be a structure of truth, as change patterns/structures of truth arent false by default but rather can be more true than their inputs'
            - 'true' structures will interact with 'true' structures differently than 'false' structures, as the 'true' structure will use the 'true' structure repeatedly across functions, more so than it would use a 'false' structure, which it would likely filter out and stop using, similarly, 'false' structures are likely to use 'false' structures for the same reason and in the same way (as an input), as 'false' structures are useful for creating other 'false' structures
                - this is derivable by first identifying 'iterated structures' (like 'truth-truth') and identifying their interactions as opposed to the interactions of a mixed structure (like 'truth-falsehood')
                - similarly, 'false stable structures' are likely to require falsehood as an input (if theyre false and in a stable state, falsehood is likely to be related to that stability), just like truth structures are likely to require truths as an input if the truth structure is stable, where 'stability of falsehood' can take the form of 'immeasurable falsehoods' or 'falsehoods below a harmful threshold where they are not measured', as truth structures are likelier to interact with other truth structures in a non-destructive way as they are more similar, whereas truth structures are likely to interact with false structures in a destructive way, given their difference (truth structures can coordinate with other truth structures, without contradictions, as they already occupy the same reality and are technically already coordinating just by being true/real/stable)
                - relatedly, given known possible useful structures like 'neutralizations' (canceling each other out) and 'extremifications' (compounding in the same direction) and 'oppositions' (changing direction/sign), a 'falsehood applied to a falsehood' ('incorrect difference applied to an incorrect difference') is a possible truth structure if there is a neutralization involved, as these are two extremes of a dichotomy like 'abstract/specific' and like those dichotomies which act like reality-covering variables, all interface structures (such as neutralizations) are possible in the true/false interface given some interaction function between them (like iteration, embedding, etc)
			- 'truths can become false when over-depended on, beyond their appropriate context or meaning, or beyond their potential to illuminate or support other truths, or in an incorrect structure like a foundation for other truths'
			- 'truths can be so irrelevant to an intent as to be equivalent to false (example: citing the heat death as a reason not to try to do anything)'
			    - 'relatedly, truths are meaningful statements, where meaninglessness makes some statement so different from relevant/useful/integrated/meaningful structures as to be false'
			- 'truths can be so rarely/improbably true as to be equivalent to false (example: an error state that is so rare you basically dont have to plan for it, like where neutrinos would coincidentally flip all the bits on a server at once)'
            - 'truths that are generally true (like something with a high benefit to cost ratio is too good to be true) have an associated truth that makes them false in some way (like their error structure such as "when the better ratio is actually true, this truth will miss that exception"), where the "error structure that could find the structure with the better ratio" could be more true than the generally true statement, depending on the ratio and the usefulness/importance of the structure with the better ratio'
			- 'truths can be so unstable (difficult to maintain) as to be equivalent to false (example: a rare atomic state that degrades into another more stable state more frequently), as truths generally take less energy to maintain or make true, as lower energy-requiring structures are likelier to exist, and similarly scalable truths (which can be repeated or otherwise scaled without or with fewer contradictions) are more true than other truths'
				- 'relatedly, truths, if prevented or destroyed, will re-occur if true, as its unlikely for a truth to occur in absolute isolation, only once, and simlarly can support more embedded variables if true than a falsehood can'
				- 'relatedly, truths must have some structures or they are unlikely to be true, such as how requirements (costs, responsibilities) are required or the structure is unlikely to be true'
			- 'truths are less likely to be extremely surprising/different, based on a comparison to an input set of sufficiently variable mixed facts'
			- 'truths are less likely to be on either extreme of various spectrums like specificity (an extremely specific fact like a "specific behavior of how to be good" is less likely to be or remain true than a general fact like a generally useful priority like "being good")'
			- 'truths are likelier to have truth-associated structures like costs, whereas a lie is likelier to have fewer of these structures (a lie about zero cost)'
			- 'the ratio of truths to falsehoods is likely to be stable to some degree, because when a truth is measured and sustained enough to be measured, some other true structure might decay, although entropy is a powerful process that might act as a counterpoint to this, decreasing the number of truths (stable structures) over time'
			- 'truths can be so difficult to measure/verify/calculate/derive as to be equivalent to false (example: number of atoms in the universe, or some phenomenon that occurs below the synchronized directed information frequency that constitutes 'time' that it cant be measured in isolation and can only be inferred by its emergent effects)'
			- 'truths can be so non-adjacent to other probable/known truths as to be equivalent to false (example: future truth of a reality that is non-adjacent to current reality and is unlikely to occur)'
			- 'truths can be so lacking in reasons as to be equivalent to false (example: there is no reason for a rare anomaly except random coincidence so it may as well be ignorable)'
			- 'for every fact (which acts like a representation of reality) there is another fact that represents reality equally/similarly accurately or represents a similar proportion of reality or otherwise represents reality in a similar way by some metric of representation, so that these other facts in the same position on this representation metric index can act like equivalent alternates'
			    - 'statements at different positions on this representation metric index can act like a cross-section of reality that acts like a determinant of reality (the cross-section is usable to determine other facts, which is itself a representation metric), though statements that tend to be closer to either extreme of absolute truth/falsehood or the center are more valuable for their obviousness of similarity/difference to other facts/falsehoods'
			- 'truths are generally independent in that they have relatively few dependencies in order for them to be true (less conditional and contextual, more absolute and inevitable)'
			    - 'at the same time, truths are likelier to have more reasons why they are true/can exist (existential risks to the truth were prevented by many guarantees), making it more stable, compared to a falsehood (an agent decided to make it seem true, using cheaper structures than truths)'
			    - 'this is related to the fact that truths usually have fewer opposing statements that are true in some way/degree/context (bc paradoxes/symmetries/rotations may be a core structural unit of reality, just like randomness/ambiguities/balance points where very different structures seem equally true are alternate structures of reality related to symmetries, as most structures can be connected to symmetries given that theyre a structure of stability/robustness where some structure can be sustained under change conditions and therefore a structure of existence/reality, just like useful structures can be a structure of reality as theyre more stable), compared to falsehoods, which usually have more opposing statements that contradict them, and related to the fact that truths are more similar to other truths as they are more adjacently connected to other truths (using truths)'
			    - the question of 'what are the limits of symmetries, or why isnt everything a symmetry (in every permutation/interaction and on every level), given their commonness' is adjacent to that, with related insights like:
			        - 'variables are a type of symmetry', 'variables are arguably descriptive of everything (the change interface can capture all variation)', 'variables are embedded on other variables', 'some variables break/create symmetries and that is more important to categorize them than to call them just another symmetry', 'symmetries dont always resemble symmetries in various states of development', 'some symmetries neutralize each other, leaving randomness (which is different in that it is the interactions of unlimited changes, rather than constrained changes of a symmetry)', 'symmetry types are a meta-symmetry', 'meta-symmetries (symmetries of symmetries) are more powerful than standard symmetries', 'not every symmetry is equally interactive with all other symmetries', 'other structures like levels/systems where symmetries act and symmetry interaction functions are equally important if not more'
			    - similar to the question of 'why isnt everything formatted or adjacent to an input/output sequence by default' which is bc information can be adjacently and usefully stored in equivalent alternate formats, not everything is a symmetry bc there are variables of symmetries (interface structures like 'state' and 'self' can be applied to symmetries) as theyre not absolutely required, and there are many variants of the definition of a symmetry as theyre related to 'limited change' (similar differences) which is a fundamental structure reflecting reality (true/false spectrum pair), so just like input/output sequences arent enforced in all conditions/contexts, neither are symmetries
			    - similarly, the 'limits of definitions' are equivalently useful to the definitions themselves
			- structures of meaning like 'the comprehensive impact of its interactions/functions at various scales, in isolation and in various system contexts, and with other common variables applied' and 'useful structures' are useful approximations of the definition to use in place of the full definition, similarly compressions of reality are more useful than reality itself, these compressions acting like interfaces between the user and the system being compressed
			- 'truths can be so simple that they will essentially be false in that they will be more useful when changed in some way and will be an input used by other more powerful/complex/high-variation functions which could independently generate/alter these simple truths as needed due to their simplicity, and therefore are less real/true, as reality allows complexity and therefore requires some complexity for survival, survival/stability being related to truth, and similarly can be so complex that they are essentially not usable by other powerful/complex/high-variation functions and therefore are less real/true'
			    - 'relatedly, there is always a simpler and more complex variant of a statement within the bounds of simplicity/complexity required for reality, and the same goes for other dichotomies of reality like truth/falsehood, similarity/difference, balance/imbalance, variable/constant, meaning there is a network of statements related to one statement that act like different variations of the same statement, so that the statement acting like the symmetry of this network of statements is more true than any of the variants and will reflect the interface network to some degree/type/structure'
			    - 'truths will be more adjacently connectible than falsehoods to interface structures (such as the primary abstract concepts like balance/power/work which correspond to the dichotomies of reality)'
			- 'almost every fact has another fact that is more relatively true and a falsehood that is more relatively false, except the most absolutely false/true statements, which are relatively rare'
			- 'relative truth/falsehood is determined by proportion of reality represented, accuracy of the representation, power/usefulness/relevance for enabling/determining other representations of reality, logical validity, adjacency to absolute reality, im/possibility, inevitability/requirement, consistency of the fact/falsehood across contexts, consistency of truths/falsehoods across different representation formats, potential of all other metrics to remain the same or change, & other metrics of truth'
			- 'truths often come with an opposing counterpoint as most truths are not absolutely true but are wrong in some way like in a particular context/usage, so that a truth without a counterpoint as how it might not be true is unlikely to be true'
			    - 'relatedly, truths are generally somewhat variable, acting like a symmetry/manifold in that they can vary in variables like position/shape/relationship to other structures without being destroyed, in their robustness to change, whereas lies by comparison are relatively fragile'
			    - 'similarly, truths are often seen to have a "balancing offsetting truth", similar to a "rise and fall" structure pair (which may differ in side lengths, as a statement may be more true than its counterpoint(s)), which is a fundamental structure related to parabolas, angles, waves, etc'
			    - 'however, given that other fundamental structures exist, the "rise and fall" of a point and its counterpoint is not the entire description of reality, despite being extremely useful and common in the form of the concept of balance'
			    - 'similarly, other concepts than balance are known to be relevant, as not everything is balanced, such as the ratio between matter/antimatter, and therefore other concepts are required to describe reality'
			    - 'similarly, other structures than one interface are important to describe reality, such as how a cross-interface structure (such as the structures that a concept takes in various systems) is more useful than either structure in isolation on one interface'
			    - 'for example, given that the interim and combined concept, respectively, of a circle, parabola, curve, a fractal, efficient embedded change (using the previous value as an input as an alternate to a constant multiplier), a sequence describing the variable interactions rather than a set of interacting independent variables, a stability in the change rate, an origin/symmetry, and a concentric circle set is a spiral, how does this interact with the "rise and fall" structure of a point and its offsetting counterpoint?'
			        - 'during the standard process (which may be the "rise and fall" of a new change as it first generates change and then is offset by opposing forces to settle in to its stable form), other changes are allowed to occur, such as embedded changes, curved changes, etc, which can disrupt the stabilization of the new change or even invalidate it or isolate the initial change from its counteracting change forces to prevent their interaction'
			        - 'given that the spiral with a stable change rate (that is maximally different from other structures like a full circle and a cornered shape) has stable structures, it can inject stability in other changes, such as by making a rise or fall more stable than its opposing structure'
			        - 'given the changes enabled by the spiral and its stability and therefore its disruptive power, spirals are a better structure of randomness than some other structures, as an offsetting uncertainty structure to more default certainty structures like the "rise and fall" which are more certain bc they are more balanced'
			- 'truths that are in between simple priorities & rule sets (like "opposites attract") and complex priorities & rule sets (like those generating randomness) are likelier to be true, and truths that connect simple and complex rules are likelier to be true, as there are both simple and complex rules in reality, and neither has absolute priority, as the interim truths between extremes are likelier to be stable and therefore true, and similarly the interim balance point between all dichotomies at which theyre intersecting is likelier to be the most stable perspective, from which the others are adjacently generated'
			- 'truths are rarely the only truth explaining a variable interaction, as there is rarely a variable requirement requiring that specific combination and preventing any other combination from succeeding as there are more often many routes between two points than one route, and there are many alternate equivalent preceding/succeeding variable interactions that are similarly explanatory, bc errors are default in most systems and therefore differences applied to sequences/routes/components are likely and are less likely to be ruled out by some requirement if they still generate movement in the original direction, and similarly bc components are frequently unitary and can be combined in different ways to generate the same structures, and similarly bc real systems are often complicated and subject to errors from their high degree of interactivity with other systems in contrast to a theoretical high degree of isolation'
			- 'truths usually are not obvious (such as the wishes of agents as their default, and therefore also obvious/simple to them, so these wishes can be assumed to be not true) but if they are, a beginner without expert knowledge of rules is likelier to see them than an expert bound by their knowledge of rules'
			- 'truths that are not measured or otherwise useful are likelier to change than truths being measured bc the act of measurement requires finding a way to keep some fact true enough to show up on a measurement scale, while other variables are allowed to remain variable to cause other variables to change, which are not being forced to remain true bc attention is directed at other truths to keep them constant and randomness indicates that anything not forced to remain constant is subject to variation injections (a function to generate change, whose change-generation speed is faster than measurement speed, is likelier to be a more useful intent, to make all measurements obvious or unnecessary, as the changes that are true will sustain themselves and the changes which arent true wont, so no measurement/testing is required, measurement being useful for "changing direction of focus/motion/work/etc")'
			    - 'relatedly, truths in general are likely to change at some point due to allowed variable interactivity/dependence, so any set of facts that was true is unlikely to be completely or equivalently true at some point in the future, as variation isnt equivalent to falsehood but a fact of truth-generation & adaptation & interactivity & dependence'
			    - 'relatedly, truths that are independent from another set of truths (such as processes on another planet) are less likely to remain true, unless another truth intervenes (something needs it to remain true as its useful for some intent)'
			    - 'similarly, truths that dont require ignoring other information (having no contradictions) are likelier to be absolute truths'
			- 'truths above a ratio of already identified truths may be so useless as to be false, as they may be irrelevant, and also preserving the ratio of truths/falsehoods is useful for agents with intents to incentivize additional change/uncertainty (some intent like a wish/dream must be false for them to be able to justify doing work to change any variables)'
			    - 'similarly, future truths can be predicted using the intents/incentives of agents (agents who are capable of fulfilling all of their intents, who dont always select the incentivized option, as "always selecting the incentivized option" contradicts "capacity to fulfill all of their intents"), once their intent & fulfillment capacity (like 'to increase uncertainty', 'to seek freedom', 'to control time', 'to understand physical reality') and incentive selection (ratio of selecting incentivized option) variables are known'
			    - 'similarly, some possible truth can be so different from incentivized truths or intended truths as to be false'
			- 'truths are generally those structures which fulfill multiple solution/optimization metrics as opposed to just one (its rare for something to be true which is only useful in one way, such as ability to change or interactivity with other useful structures, generally structures which survive fulfill multiple optimization metrics as structures generally interact on multiple interaction levels)'
			    - for example, a structure is not usually just 'efficient' in some metric (efficiency meaning low-cost or adjacent in some other way, which means its near to something else, as not every point is adjacently useable as a starting point to find adjacent structures to), it also needs to be 'similar enough to existing starting points/interaction levels' to be useable (in order for that efficiency to be efficient)
			- 'truths can be less true than a falsehood, if the falsehood is more probable, relevant, useful, is about to be true and if the truth is about to be false, is important to be made true, is approximately/generally true, reflects more interface structures like potential, etc'
			- 'truths are generally more useful at generating other truths (used as inputs to other truths), as opposed to generating falsehoods or lies being more useful at generating truths'
			    - 'similarly, truths are useful at specifically generating power, in the sense that truths enable other functions to be more powerful, so statements (about a variable relationship) that seem to control/determine/cause other variables are likelier to be true'
			    	- 'for example, specific structures act like truths (such as certainties/guarantees) on specific interfaces through their determination of other structures, such as how gravity/speed acts like certainty in the physical information interface, and structure acts like certainty in the math interface, which can be used to find different certainty formats in other interfaces (what type of similarity/change acts like gravity/light in the math interface)'
			    - 'similarly, truths are useful at connecting true/real structures, additionally connecting cross-interface structures, such as by "specifying a concept" by connecting the concept to a structure'
			    - 'similarly, truths such as interfaces are useful at supporting variation, so high variation sources indicate a truth supporting that variation, as if variation is false in some way (such as by contradicting truths) it will likely be stopped by agents who benefit from truths, so if variation continues, it is likely to be based on a truth that has created enough stability to support additional changes'
			- 'truths generally coexist with other truths, rather than more frequently/generally contradicting them'
			- 'truths generally are more similar to other truths, rather than being extremely different from them, partly bc of the fact that once a structure is found that is stable/otherwise useful, it tends to be repeated rather than changed'
			- 'truths that seem "counterintuitive/random/complex/asymmetric" are adjacent to some interaction level, despite seeming false (through dissimilarity/disorganization/complexity/asymmetry) at first, as truths are usually an adjacent combination of some components (though neural networks dont usually switch perspectives to start from a different angle/starting point and identify these interaction levels from which everything is adjacent) and by comparison lies will fulfill some usually-checked metric of truth (similarity/organization/simplicity/symmetry) while containing a contradiction (false similarity, etc)'
			- 'truths are often trivially different from lies, in that in order to be believable, a false statement has to be sufficiently similar to truths (plausible, reasonable, logical, sensical, meaningful, relevant) and cant be obviously false (silly/nonsensical/inappropriate for the context/meaningless) at which point it becomes an obvious wrong (a joke), therefore a lie (a difference from the truth, or an error) can be used to generate a truth by applying it to another lie (a difference from the truth) or by basing it on or connecting it to the truth (making the lie obviously false so its not really a lie), so lies should be connected to an offsetting lie/truth rather than given in isolation, as they wont be robust to testing/changes on their own, unlike the truth'
			    - the types of difference that a truth is robust to are fragile bc of this triviality in their difference from lies (applying an 'opposite' transform could make any true statement false, so thats not a useful change type to determine robustness of a statement to change and therefore its truthhood), however given that the truth often has multiple alternate forms, applying extreme differences to a statement could also produce another truth
			- 'lies take meaning from the truth, to the point that above a ratio of lies, truths begin to degrade/decay, and fuzzy approximations/representations of the truth & tools to automate that become more true (in the sense of being more relevant/useful/usable than most absolute facts), to the point that computers/algorithms become "relative/approximate truth" factories, based on some ratio/structure of falsehoods they can compute filters for (and how well they can integrate new info with the few details that are important to hold constant rather than allowing them to decay)'
			- 'truths that invalidate other truths are generally more foundational (identifying a previously unknown interface that invalidates other truths by reframing/explaining/decomposing them to such a degree that they become irrelevant, such as a statement that "everything is a function of cause", so that everything seems invalid/irrelevant except cause)'
			- 'structures that have more functions such as "organize information more effectively" are likelier to exist/be true than other structures as they increase the stability/inputs of other structures and allow other structures to exist (are more useful and coexisting), and also increase their own stability (organizing information effectively allows the structure to handle stress better and solve more problems better), leading to a win-win situation that is valuable for life to occur and stabilize through coexistence'
			- 'structures of truth (like derivation methods) that describe/generate/derive/find/apply/compress/store/retrieve info better than other structures are likelier to be true as theyre more effective/optimal in some way than existing structures (the universe is likely to move toward a state that more efficiently compresses reality than other sets of rules, if a state becomes adjacently possible relative to the existing state), meaning truths that use other truths the best/most effectively are likelier to remain/become true'
        - finding the 'information traps (at which point no further information is derivable, as in a dead end) and the sequences to get out of information traps' such as:
            - trajectories across different graphs (like usage graphs, as opposed to attribute or variable or function graphs, to find paths back to a high-variation/high-potential position)
            - applications of complementary (as in independent) insights, such as applying how 'every fact has an opposing statement that is true in some way/degree/context' to find the errors in the generator of a trap and fix those errors which are inputs to the trap (errors like 'inability to derive information' and 'inability to change' and 'inability to be independent'), or similarly applying variables to generate independent directions of change like the primary interfaces, or similarly applying variables to the generator/trap to make it improved to capture more information more efficiently, so as to become an input to the trap/generator, or similarly applying variables to the trap/generator to make it independent, or similarly applying variables to the generator/trap to make it trap itself, or similarly applying limits to the generator/trap so it is the dead-end
                - a good metaphor for the primary interfaces is the 'cardinal directions' (as dimensions of reality) each being capable of reaching the center and the edge of the universe, in different ways (using different combinations generating different structures), where the other directions are derivable from each direction if the center or edge is known and each direction is enough to understand the universe on its own, having a sufficient cross-section as an input, each providing equivalent information in independent ways
                - the cross-interface connection/change sequences between these 'primary directions of change (on one interface)' (which themselves support connection/change sequences) are important as a foundation of reality (the ways that information can change into other information being a source of freedom/potential where change can occur)
            - applications of limits to trap the generator of a trap or the trap itself and applying variables to the information that is trapped to reverse positions, or applying variables to the trap/generator after applying a variable to change its direction/intent to set it free in another direction
		- applying interim connecting structures of useful structures representing the truth like 'rules databases', 'attribute networks', etc, such as 'common structures to both systems' such as 'constant attribute rules' (variable interaction rules)
		- every true statement should connect to other sources of truth (high-variation variables like interfaces, powerful variables like cause/energy, important sources of potential like 'black holes' as potential energy sources)
		    - 'humans are working on climate change (because we havent tapped the extreme potential energy of black holes) and (because we havent identified all the interfaces) and (because we havent optimized energy efficiency)'
		    - true statements should take important/relevant true statements into account
		    - structures like black holes are a source of truth bc they store information/energy/reality efficiently, just like the interface network does, and just like some formulas do (euler/gravity equations, etc)
		
		- definitions/limits/requirements create a metaphorical corrollary of 'gravity' in that they make some structures more similar/probable/coordinating than others
		    - where does variation go which cant be stored/have structure in the universe, or is this not possible and all impossible variation is approximated/echoed in brains to conceptualize impossible structures (like how you can know some things about an infinite set, such as that one side has no limit, and the connection function of adjacent items, and the impossibility of it stopping, and its starting point, which is like glimpsing a subset of infinity, as that subset is all that can occur in or occupy reality), which implies that this infinite level of variation would contradict some other real structure which has more energy or which has other rules protecting its stability, meaning its more powerful than that infinity
		    - limits like 'are there functions which are possible to graph in 3-d euclidean space but which are impossible to form with real structure variables' are useful to find, to determine the differences between math and physical reality and the limits of this connection
		        - questions like this would also determine structures like 'whether math stores more possibilities than reality' or 'whether reality stores more possibilities than math' (math definitions allow for fewer variables than reality), or does reality offer a useful alternate format of math (like a 'cohesive usage network of repeatable math functions/structures'), and which system controls the other (does math determine reality, or do agents in reality control math in the sense that they can use it for their intents), and 'what is the limit on the variation producible with known math definitions, and can that variation explain reality'
		        - if math offers more possibilities than reality could ever support, reality is like a compression of (a subset of) math, and its possible that reality could only ever represent a subset of math structures, rather than all possible math structures, and 'all possible states of reality' is determined by the set of 'all possible subsets of math, below a certain threshold or in some structure limiting the subset', and 'identifying the structures to connect reality rules with these limits on what math structures can take physical form at any time/state (rules like "nothing too chaotic or simple")' is a useful related intent
		        - a compression function could also act like a generative/descriptive/explanatory/causative function
		        - are some structures only impossible bc of other structures that could change reversibly (undoing the change to revert to a lower-variation or more stable state while retaining the new information) 
		        - 'what information/structure is likely to be irreversible, unstoreable/unretainable, incompressible, or create chaos cascades' is a useful related question

		- identifying useful problems and useful problem formats to solve, such as 'identifying how to format problems/variable/interface structures so that they can be formatted as a problem of "solving a system of equations" to find the alternate function sets that can coordinate to determine reality', like 'which set of functions preserves/stores/organizes information the best without contradictions (using the same components and without removing any components from the original function sets which are known as true, or removing any other functions)'

		- identifying connection functions to apply the opposite or other differences to known problem-solving workflows, which are similarly useful in reverse or with other differences applied
		    - for example, given that a default solution automation workflow is to 'generate possibilities and filter them to find a reduced solution set/area/range', an 'opposite-direction' variable applied to this workflow's direction would be to 'connect filtered/reduced sets with the filters applied to possibilities, to reverse-engineer the sets of possibilities that were filtered by reverse-engineering these filters'
		    - an example of these filters would be the 'type' filter, that when a type is identified, a high degree of other information about the structure is known (the 'type' filter reveals a high degree of information about the other structures that are members of the 'type' group, as well as the other 'attributes' of the 'type')
		    - once you know these connection functions, you can apply them by default as 'default filters' to filter generated sets to identify 'subsets with predictable attributes resulting from those filters, given their connection function', or to connect a 'few specific examples' with 'general patterns of possibilities' or 'full sets of possibilities'

		- identifying function sets to map 'generality (in intent) to specificity (in implementation)' is a way to identify the core functions that need to exist (for intents like adjacently generating all other functions), using examples of general intents fulfilled by specific functions, as a way of writing the fewest functions (writing an abstract general function and then varying it when specific valid intents require changing it)
		- identifying ways to integrate alternate core general useful structures like applying 'causal networks' to decompose some subset of data sets, where some points like 'density centers' indicate connections between points such as 'probability of adjacent points' (as a cause of expecting to find other points nearby) and some points like outliers or non-pattern-compliant points representing structures like limits/dead ends in causal networks (as a reason to find those patterns, or as a reason to discard a point as being generated as only an output by one rare process so as to reduce its possible interactions and usefulness), or alternate causal networks of input feature subsets where some causative relationship is probably in a set of alternative relationships, since adjacent/equal points arent necessarily absolutely independent, in the sense that one point may lead to generation of other similar points if successful/useful in some way, or a particular point may be so useful such as simple that it is frequently generated independently, and these related points can be reduced to one representative/general point that can be used to generate all of them, and instead the 'reduced set of representative points' can be altered by applying these 'reasons for point similarity/repetition in data sets' to generate the probable alternate variations of them, while applying regression to just the reduced set, or the 'reduced set + the generated set', to isolate actually or more independent points

		- identify methods of describing useful methods like interface analysis using functions/variables not directly referenced in their definitions as good approximations of it, like describing interface analysis as 'interim thinking' (as 'thinking without definitions/rules to find new definitions/rules to connect known definitions/rules') and 'meta-representation and similarity reductions and integrations' and 'potential maximization', these descriptions being useful for identifying approximations of the logic (similar to how concepts can more effectively describe a structure than the full set of details about it) which can be used as inputs to generate the logic & identify variables of these useful methods to generate the others
		    - similarly, other good descriptions include:
                - example interactions of 'interface analysis' with other structures:
                    - what is new about interface analysis like "creating many new languages such as 'network language'" and 'identifying that there were abstract info structures like concepts/overlaps/types/requirements/intents/perspectives that covered reality and identifying variables of these'
                    - interactions with 'math usage for inventing' such as "changed how math inventing will be done forever" and "identified so many new variation sources and ways to find them that they wont fully apply these methods and identify their limits for decades, even using AI, bc of the complexity of evaluation to handle that generative complexity potential"
                    - uniqueness in connecting different systems like "identified a good standardized basis for connecting different reflections of reality like math and information"
                    - iterated and otherwise applied these structures to an unused degree and across different nodes/angles to identify new structures
                    - identified new ways to connect reality and the potential of structures like 'high variation variables/functions' to cover reality and solve all problems
                    - identified the hidden connective structure of interim layers in between other systems
                    - identified many unnamed/unused variables of structures like 'reasons for variable interactions and formats of variable interactions' that make problems trivial to solve, and structures like concepts such as 'sensitivity' and the variables of 'connections between interfaces that reflect reality'
                    - identified useful graphs like the 'graph of intersections/limits/contradictions of logical rules in a system like math' and useful cross-interface structures like the 'quantum potential of caring'
                    - identified specific useful parallels across differences that can solve problems like mapping extremes and identifying vertexes as 'useful complementary perspectives' and other variable types like 'generally required dependencies like structure/function'
                    - identified specific useful structures like specific 'definition self-applications/core interaction functions/1-to-1 mappings/similarity indexes/iteration limits or intersections/similarities in overlaps/ratios of opposing sequences' that can be combined to solve all problems and more importantly, filtered using the same structures (like 'changes in components of problem inputs' as a combination to create a new problem-solving target)
                    - increased both the stability/certainty and variation/uncertainty of reality (standardizing to interface analysis allows more variation to occur)
                    - identified variables of problem-solving methods like 'maximal differentiation' and 'standardization' for problem-solving intents like 'compare' and 'organize'
                    - identified structures where variation exists like 'interaction levels' and 'interfaces'
                    - identified useful connections between structures like 'similarities' encoded in structures like 'graphs' and 'variation/time/uncertainty' to enable automatic generation of these structures
                    - moved reality to a new interaction level and base formed by understanding and organization of interface structures
		        - 'a semi-populated network of interface structures with empty nodes to be determined in between and external to these certain interface nodes, which when viewed from various perspectives/angles outside the network, offers new obvious connections that offer a different perspective that is still true in some way as some interface structures are still included in the subset made obvious and the connections made obvious in that subset, where outer layers of the network grow as new interaction levels of interface structures and new interaction functions applied to them (where the semi-populated network is different/new every time, to avoid repetition)'
		        - 'starting from no information (a position in between known structures), what minimum of information (what trivial set of structures) can you apply that will derive a connection to information (derive a new "function to derive information", like a solution automation workflow, which can be used to connect any information with any other information)'
		        - 'finding new interactions of useful structures like cross-interface structures as default units of computation, given their usefulness when composed adjacently'
		        - 'finding/deriving connections (and derivation functions of 'connections' or of "systems of connections") between previously unconnected interface structures'
		        - "finding new extreme differences from 'adjacent change combinations', as new useful structures are the least likely to be produced by adjacent change combinations, which will never approximate 'thinking' unless the adjacent change combinations are applied to extremely useful structures like solution automation workflows & other interface structures, which when slightly changed, are usually still useful"
		    - this is in opposition to the default 'rule-set' based thinking, which applies trivial changes to known structures to create 'trivial innovations' (which could be replaced by a 'poor copying' function)
		    - 'interim thinking' starts with a 'lack of rules/information (known structures)', like finding a 'path to a safe planet from an abyss in the universe', not by applying rule sets (like 'use known physics rules and calculations using those rules and a calculator'), but by asking high variation-capturing questions which direct changes, and applying trivial structures (like the 'functions of light' and a 'increasing light-interaction function' like a magnifying glass has and interface structures connecting and differentiating these like 'abstractions of these structures'), until a new connection is found that can be used to reduce/remove/change/otherwise solve the problem, questions which are new every time which can produce new structures, bc if a problem exists, its likely that a new solution is required, which is a very limited rule set, so much that it may as well be nothing, or an empty rule set having a template/boundary that is fillable with new useful structures, which is like 'carefully avoiding rule sets while being similar enough to existing rules to be useful and probably realistic'
		
		- identify new intents that when fulfilled, have increasing value over time, as they are highly interactive with other useful interface structures
		    - such as 'find new extreme differences' or 'find new patterns of info storage structures or other structures of truth' or 'find new statements that are generally/absolutely/usually/required to be true' or 'find new equivalent alternates to useful structures' or 'find new variables such as patterns of useful structures' or 'find new interface structures (types, connections, reductions, incentives, simplifications, interaction levels, variables, etc) of interface structures' and other intents that can be generated as equivalently useful to these, which are generally useful intents for problem-solving

		- identify structures that always describe some variable interaction when in different formats, which can be applied as default variable interaction structures to check for
		    - for example, there is always a 'self-similar unit effect (like a domino effect) on some interaction level where that structure is a unit, that when repeated up to a threshold, explains some other variable', so checking for 'domino effects' is a structure to generate from inputs & look for in outputs, just like there is a 'halting effect' placing limits on repetitions, and there is always an 'interface interaction structure' that can be checked for, even if these patterns/outputs arent always fully-developed/detectable at every point
		    - relatedly, generating the structures between these known required/probable structure is useful to identify interim states

		- identify/generate structures that can act as default/core/component structures of useful structures like 'filters' to apply as default useful structures
		    - for example, a 'filter' might take the form of a 'barrier with openings that allow some subset of possible inputs to pass all the way through', or a 'set of cage-like structures that can trap excluded structures rather than being specifically designed to only allow inputs of some shape' or a 'sequence of tests that remove functionality/attributes of excluded structures so they cant move/change further or so they dissolve' or 'some structure that moves different structures to different positions', which are default structures to look for when fulfilling intents like 'identify filters', as they fulfill sub-intents of filters in various ways

		- applying useful reality representation structures such as 'metaphors (similar/relevant but different/inaccurate structures to format something differently in a useful way to achieve understanding)'
		
		- applying physics rules such as the 'fuzziness of physical reality' as a way to find other true structures ('symmetries', 'alternative definitions', 'patterns')
		
		- applying structures like the functions that can generate the highest variation (such as the 'Conway game of life') as a default structure to generate other high-variation structures like reality
		
		- applying specific mappings across interface structures (like concepts such as 'balance', physics structures like 'symmetry', and physical reality structures such as information agent structures such as 'justice' and 'economic equilibrium', or concepts like 'interface', structures like 'bases', information structures like 'metaphors' being variable implementations of the 'interface' concept) as default structures (such as default inputs of a neural network used to predict other mappings)
		
		- apply useful perspective structures (like combinations of perspectives such as the 'optimization perspective' and the 'religious perspective' to efficiently describe other useful structures like 'what agents want and what could be true using adjacent transforms of reality') as a default constant set
		
		- apply useful structure sets that should go together (like how 'dependence' is useful in the 'causal' interface but negative in the 'physical reality' interface)
		- apply useful structures (like 'direction or line connecting starting/goal points' and 'implementation structures to get to that point') to model useful structures that are useful when applied together, like 'intents/implementations of intents', as a useful input to neural networks to fulfill the task of 'adjacently combining them in a way that connects them'
		    - similarly apply patterns of implementation structures such as 'causal loops referencing nested sub-problems resolved with some structure and output to the host structure once solved' as default components of implementations of intents
		- fulfill a primary function of a particular interface that can solve most problems, such as 'evaluate meaning', and the useful functions to fulfill that function efficiently
		- apply structures iteratively to calculate the global/universal meaning to check if its obviously false at scale (in its extreme form which is more easily determined), such as applying a rule like 'its ok to violate someones rights if theyre a genius and youre fascinated by them' (after iterating to an extreme scale, its obvious that no, that couldnt be right, then geniuses wouldnt want to live if that rule is applied at scale, to guarantee that geniuses' rights will always be violated, and it couldnt be right for many other reasons, such as it is probably 'better/more sustainable/and therefore lower cost' to prioritize turning people into geniuses than to persecute geniuses with rights violations which will reduce our supply of geniuses to zero)
			- structures that make other structures obvious are obviously useful
		    - apply interactions of known structures at scale as default filters of other scaled structures, determining what else can exist at that scale, deriving core structures of truth from these scaled structures that could also exist at that scale
		- apply structures that can be used for multiple intents as default useful structures bc they can handle more variation than other structures and are therefore likelier to be compoundingly useful when applied in structures like combinations
		- find useful spaces including definition spaces where some attribute makes it useful for common problem-solving intents
	    	- maximum that a definition can change without breaking, depending on the relationship between changes supported/required by that definition
	        - differences that create different 'definition routes' of a definition (comparisons to similar/opposite definitions, usages, formats, different subsets of attributes that differentiate it in some space)
	        - space where embeddings (apply), combinations (build), connections (derive), and divisions/filters (find) are adjacently connectible
	        - finding a space between other spaces that is required to exist by definition where some attribute is also required (some attribute like an equivalence required to exist bc its between a greater/less than relationship)
	        - a space that enables finding the right layer to evaluate meaning/impact at, such as the 'layer where any pattern is identifiable' or the 'layer where variables are identifiable' or the 'layer where unique variables are identifiable'
	        - a space that makes equivalent alternates adjacent, equivalent alternates such as the 'maximally different structure' and the 'minimal structure required to describe primary interface concepts like potential/cause' and the 'dichotomies of reality', which are probably equivalent but are not trivial to derive from each other
		- apply structures that can offset neural network inadequacies such as 'limits of a series' being useful for solving the problem of modeling 'longer sequences of input/output connections' as a tool to determine when a sequence might converge, which is obviously useful for prediction intents, as well as other structures that allow the user to 'see far ahead', such as 'highly complex/different realistic/stable/efficient' structures, useful determining functions of extreme attribute similarities/differences (like absolute change type interactions, such as preserved change types when some function is applied) as 'extreme' structures and 'equivalent alternate structures (that can keep each other in check and identify invalid structures)', and other structures that allow you to efficiently calculate a lot from a little information
		    - you can see how useful functions (like calculating an adjacent attribute such as 'constance/addition' that reveals non-adjacent information like information about 'extreme change type preservation') act like a powerful information filter that reveals information that is difficult to calculate (non-adjacent)
		    - finding useful structures to determine attributes of other useful structures (useful structures like 'extremes') can be done by reverse-engineering 'input problem cases' where these revelatory 'filter' structures would be useful:
		        - finding highly different structures like extremes or complex manifolds (high-variation change based on an interface like a connected structure such as a manifold, which is a useful math structure adjacently mapped/corresponding to an interface)
		        - determining what is easily determined about that highly different structure once you know it
		        - determining what cases this information could be useful in
		        - determining whether those cases are useful
		    - similarly apply functions to 'calculate convexity' and 'filter possible functions given maximal differences (structurally similar change types that can look similar at first like exponential change, waves, hyperbolic functions, etc but which have adjacent filters to filter them out) detected by some point' as a way of improving 'gradient descent errors'
		    	- you can see how these 'interface structural similarities' (like an 'exponentially increasing subset') provide a useful constant base to apply changes to, to determine other possible structures and identify filters of those possible structures
		    	- this is an embedded application of the 'interface' concept, which is clear in the usefulness of this 'similarity to base changes on'
		    - similarly, using 'lifetime data with associated errors across a lifetime' to put a statement into perspective, or having 'civilization lifetime/trajectory' data as opposed to phrase/sentence/paragraph/story data, is likelier to connect information to more relevant structures (to answer questions like 'what problems were solved by a mind that regularly produced this thought')
		    - similarly, including 'historical/prediction data of past/future truths' with answers such as 'additionally, next year, this will probably change x% and previously this was y%' to give context and only provide info that is immediately outdated but is likely to still be true the next time they run the query
		- apply interaction levels ('adjacently connectible structures that interact/connect') as 'input information' or an 'information format' to neural networks (which fulfill the task of 'finding adjacent combinations to connect variables')
	- applying a function to find interaction levels where the problem is solvable with adjacent combinations and the function to find those adjacent combinations once transformed to that interaction level
	- iterating through filtered interface structure combinations and checking test cases of input/output pairs
	- create a network of solutions to use as bases to navigate
	- iterate constant/variable pairs to model the highest degree of certainty/uncertainty pairs
	- applying more adjacently determinable structures as an input
	    - attribute networks as an input and function networks as an output implementing those attribute networks
	- finding an 'input-output sequence' that would be useful and would probably be adjacently constructible in that sequence
	    - like a sequence of first building an 'attribute network' to describe all variables and then building a 'function network' describing that network and then a 'function generator network' to compress/generate that function network
	- applying default interface structures as the core functions of a machine learning network so that errors like 'not understanding abstract concepts' can be avoided by injecting abstract concepts as a default input or function or related structure, so that 'combinations of abstract concepts' required to 'solve a problem of understanding some conceptual combination' are adjacent
	    - 'inventing new machine-learning models' doesnt just require 'sequences of change combinations on some input features'
	    - getting machine learning models to actually invent a new optimal machine learning model requires a function to 'identify optimal models which are maximally different & useful across all problems'
	    - this is 'accidentally stumbling on useful functions' by adjacently changing a weight sequence representing a function (as functions have variables such as components)
	    - this may look like intelligence but is only "accidental coincidental effects of making 'variable changes' resulting from the function chosen seemingly at random"
	    - this "discovery" is a re-iteration of the known fact that "ai models change variables that can act like functions, and then those changes are sometimes useful at reducing the loss, so the model keeps those useful changes rather than discarding them"
	    - this "discovery" is an advertisement to the lack of understanding that "some functions can be re-used across problems"
	    - the model didnt magically know this particular function would be useful across problems, it just applied adjacent change combination that are basic/core changes, and which are easy to find, which coincidentally happened to be useful across problems, bc of the power of some core changes
	    - that is a fact of reality that should be an input to an actual intelligence algorithm (core functions are the only type of function youll get with 'adjacent change combinations' unless your 'inputs are adjacent to interface analysis structures' or your 'connective function is maximally differentiating'
	    - https://arxiv.org/pdf/2211.15661.pdf

	- find adjacent structures to useful structures and how to connect the adjacent structures so that useful structures are adjacent using these connections
	    - for example, finding that 'poverty costs more to maintain than to fix' is a result of finding 'similarities' to 'costs', such as 'poverty', and then connecting them (why is it similar to a 'cost'? its related to a lack of money/resources)
	    - as another example, finding that 'mirrors are useful structures to find hidden information visible from another angle (as they offer useful differences to input angles around the symmetry of a right angle)' is a result of finding 'similarities to light' such as 'light symmetries' which are the reason theyre useful for that intent
	    - these have a structure in common like:
	        - 'cross-interface structures (cost-lack and light-symmetry)'
	        - 'similarities to important variables (these important variables being variants like cost/light of primary interface or otherwise useful variables like information, which are a component of inputs to information such as probability, as costly information is less probable to exist or be measured and probability is an input to information, or are variants of information on different interfaces like the physical interface, light and cost being important variables which are related to information)'
	    - the question of 'what is the limit of the usefulness of finding different variants of a definition and replacing the original definition with the variant in its interactions with other structures' and 'what is similar across similarities to important variables' are other related useful queries

	- finding functions to find/derive/generate & avoid errors in a complex system
	    - for example, in the problem space of 'creating a virus to restore original dna without epigenetic mutations', some worst-case scenarios could be:
	        - some reversal of a mutation causes the problem youre solving in a different way so that 'solving it' in that way just shifts the problem to another position/state (constitutes an illness trigger in a particular host state, or causes an illness given other mutations/genes)
	        	- the host has adapted to the mutations, meaning if you reverse them, the host dies bc they now need that mutation, which need youd have to fix first to avoid killing the patient by reversing the mutation
	        	    - epigenetic mutations store some memory or functionality that some process has come to rely on
	        	    - reversing the mutation in a healthy patient would work, but the patient has complications like organ damage that make the gene therapy deadly
	        	- there are several 'state changes' that need to happen in a sequence to reverse direction away from some bad health state, where if you skip a step or get the order wrong, the host dies
	        	    - some mutation is ok to reverse in one state such as when otherwise healthy, but not given the mutations required by this sequence
	        	    - once this sequence of steps to reverse all mutations is calculated and applied, it turns out a state in the sequence was optimal to stop at, but the completed sequence was deadly, as some mutations could be kept without killing the host for some reason (they made the host better at something, which theyre now lacking)
	        	- the mutations have become so different from the original state that making these reversing changes is too stressful as its too many changes at once, and they need to be broken into multiple subsets of changes to avoid over-stressing the system
	        	- some negative environmental or default response to the gene therapy triggers reversal of mutation reversals (stress-handling methods create mutations, such as exercise, stimulant substances, or cell cycle/immune triggers, which might reverse some reversals before the therapy can be completed)
                - some other system variable needs to be synchronized with the therapy, such as to 'deliver it at a certain time to sync with circadian clocks or right before sleep cycles, distributed over several days' or where some other alignment needs to happen to successfully apply the therapy in a non-fatal way ('deliver it in a way that doesnt trigger heat stress proteins or disrupt cell communication or other intrinsic required processes that must be stable for the host to live or for the host to respond successfully to treatment', for example)
                    - the gene therapy has to be used with some other treatments (like immune suppressing treatments) to deactivate existing processes to prevent mutations, or some process will neutralize it (the immune system will attack it)
                    - it turns out that some unforeseen condition is true (like 'if one cell has a mutation, it will re-occur in all/most other cells', or 'if you change dna in a way that the system is not accustomed/adapted to, the mutations will re-occur out of convenience for the system that uses them'), conditions which only occur in some patients/health states/environments or with some mutations
                    - or some other condition like 'if you apply a gene therapy that is sub-optimal in some way like involving a lot of mutations, you also have to speed up the cell cycle temporarily or induce cell communication or apoptosis in some cell type, or it wont take hold' or 'gene therapy in some sub-optimal way like "many simultaneous mutations" is only effective if some gene to control gene editing/immune function is switched on' applies that we dont know about
                - the gene therapy changes how other complex systems interact (how the host microbiome interacts with their brain, given the impact of dna on both systems) in a way that causes illness in other complex systems which are difficult to predict
                - the gene therapy is successful in the cells it reaches, but it doesnt reach many cells bc default dna repair processes are too effective at preventing this many mutations or those specific mutation types and they effectively deactivate or kill the virus
                - the virus is too useful for other pathogens, which eat/use/attack it before it can complete its processing
                - the virus quickly becomes pathogenic and causes sub-optimal mutations/illnesses, as it gets mutations in its own dna, either from the host, from the host's illness state, or from other pathogens/chemicals in the host
                    - similarly, the virus allows existing pathogens to become more pathogenic
                - the set of mutations required to fix it is so volatile & different for each individual and you would have to know every pathogen, every input (environmental, habitual), every distorted variable in their system, and every illness state they have before making an accurate prediction of how to fix it bc of the volatility (meaning the solution can be drastically different even for similar people)
                - the gene therapy is successful in a patient but the therapy spreads to everyone around them by environmental exposure and it can kill other people so they have to be quarantined or quarantined with people having similar genetics for some period, or the mutations would have to be paired with some other mutation/state/treatment so that the virus would die/deactivate after completing its processing
                - the side effects of changing a lot of genes at once is frequently deadly, no matter which genes youre changing, bc of default bio-system responses (similar to the toxic responses like sepsis to mass cell death triggered by the immune system)
                - the therapy is successful, but the patient's original dna was vulnerable to some other pathogen which they will probably encounter
                - the therapy is successful, but it erases information gained over the patient's lifetime (like immune memory cells or neurons tied to some microbiome change/state)
                - the therapy is successful, but the patient had habits that made their original dna sub-optimal as they didnt use their genes optimally, and their pre-therapy genes were actually more usable in an optimal way
                - the therapy is successful, but habitual/environmental changes could have replaced it with less effort or the pre-therapy genes had some unknown benefit or there was a way to trigger original dna copying without artificial therapy but we dont find it bc we focused on the artificial therapy
                - the therapy is successful, but bc of environmental conditions like medicine/pollution that cant be avoided, the patient's original dna is suboptimal compared to some other set of mutations or their pre-therapy illness state
                - the therapy would be successful, if not for other currently poorly understood components like dna fragments and metabolism and their interactions with dna edits
                - each change to the bio-system would have to either not impact required processes (like energy/enzymatic pathways) or change these processes to an alternative, which means an alternative is required, and there are some processes with no alternative in the bio-system, and these processes could easily be impacted negatively by some gene changes (or the patient would need some complementary treatment to handle these impacted pathways like blood cleaning processes to remove toxins from immune/cell cycles)
                - some changes to dna dont impact patients negatively, but these are probably relatively rare as they require some attributes like 'lack of interactivity with other components' and/or 'similarity to components which are already handled to some degree in some way', and most dna changes are unlikely to have these harmless attributes
                - dna processes have some requirement we dont know about, such as requiring 'some junk/repeated data or neurons or pathogens to help handle mutations' or some other dna edit process that we're unaware of, which is only a problem in some patients
                - mutations are inevitable without some extreme behavioral condition like extreme workouts/supplementation with some substance/rest/temperature, so some mutations will probably/inevitably re-occur, requiring re-treatment
                - mutations present in the host make it unclear/ambiguous what the original dna was, so the target optimal state is not determinable by seeking the original dna state, and a 'generally probable healthy state, which is healthy for most/other people or given some known optimal gene configurations' is a better goal to aim for
                - dna changes have impacts at different interaction levels (dna groups, chromosomes, dna fragments, dna-editing dna, jumping genes, etc) which are currently not predictable/known

        - alternate intents to aim for
            - a 'more general solution that helps most patients to some degree' is likely a better short-term target (which undoes a few common harmful mutations without harming most people) than aiming immediately for a general highly customizable solution that works well for each individual (which it may not currently be feasible to retrieve all the variable information required to implement such a solution)
                - similarly, a 'dna damage-reversal solution' that reverses disruptions to important/required processes, like solutions to 'soften arteries', 'lengthen telomeres', 'de-calcify calcium deposits', 'grow new cells to replace cells damaged irreversibly by acid' and other treatments for negative stress responses from dna damage, microbiome damage & other related problems, might be more effective short-term targets, to fix some negative side effects of dna damage rather than fixing the dna damage itself
                - similarly, a technology to 'prevent dna edits of some type or of any type' could be useful to maintain some health state or to maintain health once mutations are reversed or useful mutations are applied
            - finding mutations to protect against the more powerful/invincible viruses with more evolved self-sustaining dna might be a better goal as they have attributes in common like 'hiding mechanisms from the immune system' which can be targeted
            - similarly, other example intents include identifying 'cancer cures' which could fulfill various intents involving specifications of 'targeting different regulatory processes', such as 'adding production of an antigen to target cell masses of different sizes which dont have useful or approved functions (which dont have self-regulatory processes that they use, or not regularly sending inputs to other useful cell masses)' or 'genome modifications to prevent edit/mutation vulnerabilities, to add more regulatory functions' or 'immune optimization, to add other immune factors which can regulate known immune errors' or 'antigens which have a non-binding function with approved/useful cells' or 'pathway optimization, to prevent existing pathways from being exploited by cancerous processes' or 'energy use regulation, to prevent one cell mass from using much more energy/growing much faster than other cell masses or growing faster than any cells that could inhibit it' or 'adding cancer inhibitor-production functionality, such as adding genes to produce anticancer substances like polyphenols or silymarin' or 'adding integration functionality, so checks of "general health of the host" is integrated with lower cell processes, or so that growth/regulatory components are always interacting with each other by integration'

        - implementation methods
            - finding the 'healthy gene sets required for a normal healthy bio-system in most people' and making sure dna edits dont change those required gene sets for health
                - similarly, finding mutations that help optimize existing systems, like 'gene sets that make antibodies more trivial to create and more optimal/effective to help the immune system'
            - finding mutations that mimic or match patterns of or are otherwise similar to 'natural healthy evolution dna edits (like those that improve stress-handling response or immune function)' as opposed to 'illness-causing dna edits' and restricting changes within the found range of dna edits on some graph where these regions of evolutionary or otherwise healthy mutations are adjacent
                - similarly, formatting genetic information on a useful high variation-capturing interface like the stress interface, so that some illness-causing mutation is adjacent to others bc of the bio-system stress they cause or the stress which is required for them to become a problem
            - some genetic changes can be 'tested' in the sense that side effects are probable/known to be 'increasing from zero' and 'initially non-fatal' to a degree that there is enough time to reverse them with an opposing or de-scaling mutation virus after if side effects indicate a negative health state change, or altered to an interim state that is less strong in the metric that caused the state change, to allow for a tuning process of changes during testing
            - finding stressors that reverse dna mutations (on the assumption that there is a stressor that can reverse any mutation) and find a sequence of stressors that is survivable in most patients for the illness-reversing mutations these stressor produce

        - for an example of applying interface analysis to understand genomes, understanding genetic differences is a matter of applying various interface structures (like 'multiple', 'scale', 'concepts', etc):
            - identifying what functions genes fulfill in the 'genetic language' (how to determine functional requirements that genes provide in/directly, by directly coding the instructions or indirectly providing components of other structures like epigenetic changes)
            - identifying how genes are used/interpreted in a 'genetic culture' (which can explain how similar genes are used differently for different effects/outputs)
                - identifying differences in ways genes can be used to identify alternate 'genetic usage states' (like a functional response to a mutation)
                - for example, if a metabolism can use different inputs (energy sources), different genes which encode different energy processing functions can provide the same input, although the genetic sequence will look different
            - identifying different required genes, if a different optional function is to be fulfilled (explaining how different individuals have different functions, as some genes are optional but are required for a particular function)
            - identifying required functions for life, which the full set of genes must include instructions for (the organism has to have a metabolism, so somewhere this variable must either be directly covered in the genetic code, or it must indirectly evolve as a default/adjacent interaction from applying the genetic code, meaning the whole genetic code isnt free for other functions, as some of it must encode these required functions like metabolism)
                - relatedly, identifying the ratio of genes not used for these required functions for life which could be used for other functions
            - identifying how genetic variables (like junk dna, epigenetic changes, jumping genes, gene groups, mutations, etc) can be repurposed for emergency functionality such as 'developing new functions', 'providing backup functionality', 'providing functionality that emerges with iterations/repetitions like resilience to mutations'
            - identifying how different genetic structures (like 'circle dna') can interact with other structures like 'pathogen dna' in the 'interactome' (like 'providing functionality for pathogens to borrow in a more usable format and a more common structure, making that likelier')
            - identifying the similarities/differences that hold across genetic interactions ('different genes can encode the same function', 'similar genes can encode different functions', 'similar genes encode similar functions', 'which similarities/differences are required and which are optional' etc)
            - identifying the functions provided by host pathogens that are harmless or useful to the host, and the dna involved in those pathogen's useful functionality and this dna's complementary or overlapping functionality with host dna functions
            - identifying the allowed genetic changes by the immune system, the changes allowed by the immune system that change pathogen/host genetic interactions, etc
            - identifying connections between environment stressors and genetic changes or gene sets, as well as 'ranges of direct/horizontal possible changes resulting from environment stressors'
            - identifying scaled interactions, like the 'scaled/iterated evolution of a pathogen in a host', interacting with the 'scaled/iterated progression of a condition in a host', interacting with a 'scaled/iterated progression of a carcinogenic sequence in a host'
            - identifying interactions between genes and functions that impact genes like immune functions and metabolite/blood-clearing functions or dna repair functions
            - identifying 'ratios of genetic differences, compared to ratios of functionality differences across individuals'
            - identifying functionality-neutralizing genetic changes/sets
            - identifying networks of and overlaps in functionality determining high variation variables like immunity/genetic changes, such as 'pathway activation' such as 'metabolism' or 'signaling' or 'activation' or 'methylation' or 'oxidation', these pathways impacting both immune and genetic systems

       - as an alternative complex problem system example, similarly, the reason why 'racism' is an inaccurate descriptor is that there are more correct structures like 'self-similarity/approximation/historical/group/power dynamics' as capturing more variation than the specific structure of 'racism'

        - as an alternative complex problem system example, some interface structures (which are already known to be useful across problems and are defined in interface analysis) can be useful for finding errors
            - to find the error of a function like 'fulfill default intents', use an interface query such as:
                - 'at scale, what is the emergent output of a function (a function such as 'fulfill default intents') given some pattern (like 'contradiction of agent-agent intents (meaning death)') in some interaction level (like 'agent-agent interactions') when repeated, and what is the limit of this interaction (societal collapse from decreased population)'
                - when agents fulfill their default intents (some of which will inevitably contradict each other), and apply no other strategies, anarchy occurs and society collapses from the population decrease
            - this interface query would find an 'intent-contradiction error' in 'emergent output in agent-agent interactions' by applying the 'context' concept of the 'system' interface which checks for impact of a function in the system it is applied in ('context' here means measurable structures such as 'emergent output at scale when repeated to its limit')

        - as an alternative complex problem system example, 'competition' is sometimes an 'error' structure, such as in complex systems like those with rules designed to prevent the errors of competition such as the free market with regulations applied to protect privileged/predator agent incentives, which is the type of regulation that the free market produces
            - in cases where 'competition' leads to errors like 'false or otherwise trivial displays of virtues instead of actual contributions (like in an information economy such as the internet, where information is abundant and cheap and easily faked)', 'competition' can result in abundant false information where true information is drowned out by comparison
            - this can be derived by applying interface structures like 'incentives' (such as the 'incentive to falsify information'), 'incentives' being something that agents are likely to apply as 'default decision rules' to fulfill common/default intents like 'cut costs', which is a default intent in a 'competitive' context
                - the 'incentivized action' in this context is to 'falsify information', and in the specific context of a 'market that rewards virtues but doesnt test for them or require them', the specific incentive is to 'falsify information about virtue'
            - finding incentives as a default structure is as simple as applying the default interface 'cost-benefit' variable to determine 'common cost-benefit interaction variables (like the common "high benefit/lost cost ratio")'
            - what alternatives exist to correct this error structure?
                - a default 'opposite' structure of this error is: "apply 'any cost by default' to create problems to require thinking by default ('thinking' as 'applying more rounds of analysis') which is the opposite of 'acting on obvious default incentives'"

	- finding a function set that 'converts/connects interface structures to/with other interface structures' is a function set that can generate all other useful interface structures bc the primary interfaces already identify the primary directions of change (similar to cardinal directions) and functions that can generate one from the other can identify the full set as well as identifying the differentiating variables embedded on those interfaces
	    - interfaces can be defined alternately as 'structures which can cover/support/capture the information of reality like a continuous field/fabric'
	- for the intent of 'finding useful spaces'
	    - finding 'structures of related optimals' in a space of sub/optimals (a meaningful space, where every point is useful for some ratio of intents, similar to how 'useful structures' are meaningful in the sense of 'adjacently or directly implementing some intent') where some structure of optimals avoids errors above some threshold is useful to connect useful structures like variations of optimals
		- like how some structures which may seem required or otherwise optimal like the 'reward' in a 'reward function' may seem optimal, but the assignment of the reward may interfere with additional inference processes (a learning function with an understanding of meaning wouldnt need a test/reward function set to guide it)
		- related to how the 'reward isnt the point' but the point is 'developing a learning function or learning function-finding method that can learn anything', as nearly everything is both correct and incorrect in some way (the reward helps the learning function when its constructed a certain way, but it also takes resources and isnt the point of a learning function, and learning without a reward would be better if it aligns more with the point or otherwise is more adjacent to an accurate understanding of meaning, as a reward is an additional requirement that not every learning function would need), as everything can be connected using insights, so finding structures that connect structures of optimals (like a set of differences such as 'variables of different filters applied to sort before or halt a trial and error algorithm' to connect suboptimals or pairs of sub/optimals) is more useful than a function to find any one optimal, and given that anything is connectible, an optimal learning algorithm should be able to derive learning rules from anything (such as learning a generate/change, test, filter/reward function set or equivalently an insight set that is useful for solving problems, etc) to achieve meaningful understanding
		- other useful spaces include the 'space of complex structures' where equivalently complex structures (like complex inventions which are similarly useful or equivalent alternates) are mapped to or otherwise connected to each other, where solving for some complexity space means solving all the spaces more simple than it by default, as the complex components can be re-used to solve simpler problems but not all simpler problem components can be adjacently combined to solve the complex problems, like how time is asymmetric in that it favors a direction toward some structure like a ratio or interim point of organization/entropy or interactivity/variation, prompting questions like 'what asymmetries can offset a foundational symmetry like cpt symmetry' and 'is time a foundational symmetry allowing other symmetries to form, so there is no more efficient foundation to navigate to and no way to countradict/change it, or are there other foundational symmetries like "equivalent interchangeable alternates" which are stronger as a foundation than time'
		- a space of 'maximally different structures' (like independence, interactivity, organization) which can invalidate questions about variables of each maximally different structure is useful for adjacently generating new perspectives
		- similarly a space of 'most-used interface structures' (like a 'function generating primary abstract conceptual interactions (such as power interactions) in a system') is extremely useful
		- a space that organizes optimals/solutions in such a way that each optimal can be calculated with some simple function from another optimal (like a lattice where you can move a set amount to get to another point) is a useful organizing structure to search for (using some navigation function, like 'change each of the maximally different variables that describe/generate/determine the function while still maintaining a cross-interface structure')
		- a space of 'equivalent interchangeable alternates' (such as 'generative/determining/descriptive/representation' functions) where these equivalents are adjacent is useful for finding more adjacent structures from a given starting point
		- a method such as 'standard least squares linear regression' leaves out almost everything relevant about the universe to identify whether some variable set 'changes together' (are there more adjacent variables that can be generated from these variables as interim variables connecting them to y? is there randomness interfering with some variables? are some variables just common structures like obeying incentives, stress, or lies? do all variables 'change together' in some way once connected or formatted differently? what level of directness/independence of connectivity is being tested for by this method?)
		    - some subset in the space of all generated functions (under some number of steps, including 'identifying/generating new spaces allowing differences and making other differences obvious') is actually good at sorting and organizing this information in a useful way (identifying possible alternative relationships, identifying possible ambiguities/unknowns, identifying interference from other systems/randomness, identifying variable attributes like directness/independence, identifying useful insights to connect variables, identifying position in a system state sequence, identifying possible systems where these variables could exist and interact in this way, identifying maximally different variables in the input set)
		- identifying connective spaces to connect non-math structures (like language/system diagrams) with math structures (like sequences/networks) is useful for solving problems with existing resources (such as indicating language using a set of spaces where different sequence angles indicate some difference in a subset of language variables in sentence/paragraph/manifesto sequences, where the set of spaces determines independent complementary information about language)
		- finding a space of 'interaction levels where a function would be adjacently implementable' so that similar points on the space can find alternate interaction levels (of functions/variables to use as components to adjacently combine to find a function implementation)
		    - this considers the investment or 'amount of work/energy' invested in the work of constructing/finding a function, the work done in the final function logic itself, and the work of applying a function, as an alternate function attribute, similar to intent, and related to complexity but different from both, more related to the 'differences in the set of possible functions fulfilling an intent' and the 'differences created in functions by different amounts of work (low-effort vs. high effort functions)'
		- similarly, the 'space of all symmetries' is obviously useful for finding 'symmetries/similarities/differences/patterns/interactions of symmetries'
		    - when these spaces are graphed, queries like 'if you switched the values of all equivalent alternates (or other definitions of symmetries) in some specific case, would you occupy a different configuration of spacetime that is stable and are there reversible trajectories connecting these configurations' are more adjacently/obviously computable
	- finding all high-variation variables/functions and applying that as a base to describe other high-variation variables/functions
	- apply optimization interaction structures (like a 'set of game strategies that can result in a tie') as being useful structures to model other useful structures like 'interchangeable equivalent alternates'
	- apply the 'biggest differences in between known problems/solutions' as a set of useful differences to apply to model other differences which are likely to be more connectible than those differences
	- apply combinations of workflows to find other workflows, to find solution-finding methods
	- apply structures guaranteed to be relevant (like 'changes within x causal degrees') as a default set of structures likely to be useful in solving a problem, then apply other useful structures like 'combinations/input-output sequences' to those structures to find the useful set of structures to solve the problem
	- apply a sequence of these other implementation methods as a way of designing a path from mvp to final product
    - apply useful structures that are more adjacent to solutions than problems are, such as a structure including 'interactive structures' and 'useful descriptions of the problem'
        - the better explained a problem is with the best representation of it, the easier it is to solve it
    - finding structures that represent the most interface structures (like a 'high variation structure that represents various core interface structures like interactions and requirements and generative functions')
    - finding structures that describe most problems/solutions in terms of interface structures such as 'a breaking of an interface' or 'misrouting of randomness' or a 'structure of resolution between ambiguous alternates' and other useful interface descriptions of useful structures, so these can be re-applied or applied as defaults or core structures
    - similar to how solving a maze can involve applying these structures, as 'equivalent alternate' structures that can provide the same or similarly useful information in solving the maze, deriving other useful possible solutions is possible by applying useful concepts like 'incentives' (to derive basic structures likely to occur)
       - applying regular 'direction of motion' checks to make sure the agent is still traveling toward the goal and not repeating routes
       - applying standard maze configurations as possible alternatives to select from
       - solving for standard tricks to check for in a maze that are incentivized and therefore likely to be encountered, and solving for the solutions to those tricks
       - solving for interactive sequences of paths that coordinate/cancel each other and can/cannot exist in the same maze
       - solving for 'maximal filters' that can filter out possible mazes the most efficiently
       - apply reverse-engineering to find indicators of various possible sequences of end paths nearest the exit
       - these structures involve solving other problems than 'apply any route using trial & error as a base solution, and change it as you go' which is a default solution to solving a maze
       - solving problems like mazes is a proxy for solving other problems bc of the high variation captured in a maze, if the maze reflects realistic randomness and other variable interaction patterns enough, similar to solving other games if they reflect realistic structures enough
    - a set of useful descriptions of reality that have reasons why they could be true (like a 'calculator of efficient methods of preserving energy cycles' and a 'structure to support maximal variation/uncertainty (time)' and a 'stable alternate coexisting sequence (time) finder') are useful to apply as defaults and efficient structures and truth structures, as well as the variables creating uncertainties (like 'unknown beneficiaries of calculations' and the 'maximally different structure' and the 'most stable system that supports the most variation') in these descriptions and the variables between them (such as interface structures like different priorities), as well as the differences between the description and related structures like 'problems solved with that description' and 'priorities fulfilled by that description' which make the meaning and reality of each description more calculatable
    - apply useful structures like 'clarifying structures' which make something obvious similar to how certain filters make some solutions obvious such as 'standardizing' and 'difference-maximizing' filters make differences more obvious
        - for example, finding standardizing structures like 'matrixes' which are a useful format that is also useful for other intents like 'mapping sets of sequential operations' and 'reducing some function to a set of adjacent combinations once in that format' such as 'solving linear systems of equations' 
    - applying useful methods such as methods of deriving information about other useful structures such as 'types' (all members of a type have this attribute that defines the type) and connectivity (constant lines are formed by a type of addition so it makes sense that adding them which is applying more addition doesnt change their shape, as their shape is the product of addition, as opposed to an operation that adds a dimension or restricts range, which would be required to create different shapes)
        - finding the 'type' of an object gives almost free (low-cost) information about that object, bc of the information stored in the type, where the type acts like an interface that can support some variation within the definition
        - other low-cost, high information-producing structures (like abstract concepts such as balance, alternate definitions, interaction levels, similarity to some other known structure, useful filters like standards to derive maximal information, useful networks to know a position in which is a high-information structure in useful networks) can be derived in a similar way and prioritized as default structures to find
    - useful formats of structures similar to a standard network but different in a useful way like 'gap networks of connected empty shapes' to represent related structures like intents to fulfill with implementations filling the shapes, or 'state networks to model useful sequences/queries' or 'maps to model useful connections' or 'map networks to model different analysis perspectives' or 'interface networks to model bases that capture high variation'
    - a set of certainty/uncertainty pairs to apply as default problem/solution structures capturing high variation
    - build sets of 'loosely related possible' associations (which arent guaranteed by definitions but which are allowed) using definition-adjacent connections, like how the connection between variables 'constant' and 'constant squared' involves a definition involving 'multiplication', but also has related attributes that are outputs like curvature which are required by the definition and other connections which are not as relevant like 'constant preservation of data type between input/output as a scalar' which is a loosely associated connections rather than tightly bound by the definition, using these loose associations to discover new possible connections not explicitly defined but also not definitely restricted
    - find component functions of data set subsets and iterate applying these until a non-matching point is found outside of the acceptable error range, adding terms or alternate functions to process in parallel to handle new points where found outside the range
    - find subsets of the data set that shouldnt be reduced to a function bc of complexity and randomness and other factors likely to predict insufficient information or variable injection points or other uncertainties that cant be resolved, where other subsets of the function are clearly mappable to functions
    - find more useful structures to describe variable interactions such as 'variables (such as squares) creating requirements (such as required growth, as in positive or nonzero growth)' and 'variables creating other useful structures like embedded change (change on change, like exponential growth leading to the accretion of matter)' and inputs to these structures like 'equivalences in factors creating self-similarity (in multiplication/area inputs) leading to multiple differences in outputs (of multiplication, compared to adjacent inputs)' which are useful in their capturing of high variation (similarities/equivalences creating differences like multiple differences between change rates of adjacent inputs)
      - the core unit of maximized potential of a variable in its interactions in isolation of other variables (self-interactions) is the area that can be created by the unit of core maximally different interaction (multiplication) with itself, representing its interaction space in the 'adjacent sides of a rectangle' operation (multiplication), leading to its potential (in its possible range of impact, as squaring it is maximizing its differentiability) to influence probabilities
      - finding the important alternate sets of functions that lead to these important structures like 'required growth (a relevant structure of reality)' can generate a 'limit scaffold' ('the model must not contradict required growth of some structure that has growth as a requirement or other form of certainty, as in some variable in the model must not grow in a way that contradicts known required growth of some other variable') that represents 'points of impossibility' that should be used as filters to avoid when modeling reality
      - the intersection of 'generative scaffolds' and 'limit scaffolds' is a useful place to start modeling the dichotomy between certainty/uncertainty to explore, format, & filter the space between them that is allowed by reality
    - finding other useful representations of a function (such as a 'stack of squares of increasing side length' as a useful alternate representation of x-squared to represent the value of y in a clearer way that reflects the equivalence in multiplication inputs and the multiple differences created by the equivalence in multiple dimensions) which increase the relevance and meaning of a function representation, as its probable interactions with other functions is more clear given the core relevant differentiating attribute of equivalent factors
      - similarly, the equivalence in factors leading to a line with slope 1 (1,1, 2,2, 3,3 as inputs) is significant and indicates the relevance of x-squared as different from other functions and more relevant as a unit function of change and indicates the relevance to the output by its squared area created by the difference between these input pair points and the origin
    - applying rules to find highly useful structures like interfaces such as by asking questions in a sequence like:
    	- 'what are the maximal difference-capturing variables like change' (such as 'functions vs. constants') or 'what contradictions exist in useful structures like change' such as 'what changes dont change' and 'what are variables of changes that dont change (limited change around a symmetry)' and 'what changes unchanging variables (constants, symmetries)'
    - rules like "'what are not inputs' are also a cause of structures in addition to inputs" (bc resources not invested determine states of alternate functions) which is another reason to derive missing information (not just to determine what a function will likely do bc of some input but also what other functions could stop that function given missing inputs devoted to alternate functions)
    - an alternate implementation of a 'blur' algorithm (using the compression/filters/other interface structures involved in vision) to quickly determine trends in a data set is to sample the data set and evaluate each subset quickly to benefit from the overall impression of the emergent pattern visible across a sequence of alternate subsets, where the impression is formed by easily differentiated structures like 'border angles', 'densities', 'ranges', etc which are common across multiple subsets in the sequence, or to align subset data sets as a sequence or network of subsets (organized by some similarity), to make trends more easily identified
        - similarly iterating through data set subsets quickly (to efficiently store memory by identifying the most obvious repeated patterns) is another way to implement an algorithm similar to 'blur', if the neural network has the ability to derive 'overall impressions of a shape' to compress the data set rather than just simply storing the whole data set
        - similarly, identifying a regression function is possible by finding which functions seem to be in the center/average of a data set (as a proxy for their summarizing capacity for longer when rotations/angle changes are applied (to view it from a lower or higher position), a transform that doesnt change the summarizing capacity of a function immediately if its a good summarizing function, 'summarizing capacity' of a function being easily determined from 'impressions' achieved by low memory (obvious feature retention) or fast processing times ('blur' effect)
        - similarly, identifying a data set or regression function that is 'most similar to the origin data set' is trivial by applying some transforms to some maximally different or standard or randomly selected data sets and positioning them in a way that aligns with the original data set (like by aligning its ranges in a separate graph that is close enough to easily identify differences but obviously separable) so it's obvious whether the added data set/line is similar to the original data set (and therefore the regression line of the added data set can be used as an approximation), which applies the 'comparison' functionality of a symmetry to the concept of 'data sets/regression lines'
        - where the 'summarizing capacity' of a function describes how much info the summary preserves (such as info about limits/averages/densities/vectors describing variation from base functions/patterns/possible interactivity/adjacencies/errors/ratios/probabilities, etc) which are metadata about the data set that can be variables summarized by the data set as well as subsets of data set points can be summarized as components of the data set, these variables of the function being possible input components or output solution metrics or interim sub-intents of a regression algorithm, which can be used to determine all other algorithms
        - as a metaphor, the "'definition routes' that when combined say the same thing (in a different way)" is a good set of nodes on a language network to use as an approximation of the variables of regression algorithms (and problem-solving in general), as a stable interface structure that allows maximal variation (it will be similar to my set of verbs like find/build/derive and structures I have identified as useful and the interfaces Ive identified and so on), as different regression algorithms indicate a general summary of the input, which varies somewhat around the interface of the input data set, while "saying the same thing" in the sense of preserving info about the input and "saying it in a different way" such as by preserving different variables and applying different functions
            - a query to find the 'summaries that are the most similar, where the inputs are highly different' is a good way to find these nodes
        - relatedly, finding out if a function is a straight line or a wave function with very small magnitude (or another type of polynomial with difficult-to-measure incremental changes) is a matter of testing a subset of the data set for variation and extrapolating/expanding that pattern to the rest of the data set (as opposed to checking the whole data set), but this isnt a good way to find out if those wave patterns are errors or if errors or error-similar structures like waves vacillating around an average are a default component of reality, which is related to the problem of determining the level of specificity and discreteness to assess incremental changes of integrals and polynomials (waves being a default difference structure), but this problem can be addressed to some degree by applying scalars to magnify the subset to make differences from constants/waves more obvious, checking it for robustness by applying randomness/errors, and other methods of identifying a stable function
            - just like the 'required component of a curve' is a 'subset with count greater than 2', 'subset with count greater than 2' can be used as an 'adjacent input format' to speed up an algorithm to assess non-linearity of a data set regression function, just like other algorithms have input formats that are more optimal for the algorithm than other formats, and 'subset with count 2' dont have as much info embedded that could relate to non-linearity as a 'subset with count greater than 2'
            - related questions to this problem are 'at what interaction level do you zoom out/in to in order to describe the variable interactions adjacently describing the most variation, or otherwise in a useful way (until there is a clear summary line, until the points are obviously differentiable between point types, until the point connections are obviously differentiable between connection types, etc)', as changes in scale and distance from data set can either erase or magnify differences like non-linearity, just like changes in scale/distance from the data set can make similarities such as patterns like lines more obvious
        - finding out 'non-adjacent similarities that are relevant', such as how finding 'non-adjacent inputs with equivalent outputs' is useful bc it implies a 'horizontal line' structure might be relevant, relevant such as 'being a useful base solution function to apply changes to in order to find variation based on that line, or to use as a simplistic summary function', which can be framed as a 'high density output' with importance bc of the repetition of that output in some pattern like an interval indicating cyclical patterns, or like a ratio indicating commonness, either way indicating usefulness of that 'high density output' as a base to apply changes to or use as a simplistic summary function
            - relatedly, vertical lines (indicating an input that can become any value) are so unlikely in a data set that isnt random that they can be ruled out as improbable in most functions except where required or made probable by another route
        - calculating the function from a set of symmetry structures (local subset averages (most similar point or change from most similar change rates to other change rates), a set of inflection points (a change in change structure like charge of change rates), a set of peaks (change in direction of change rate)) or symmetry-limiting structures (extremes of a probable range representing limits) is trivial once a ratio of these are known given some complexity (& other metadata like input ranges) of the function
            - relatedly, another useful problem-solving intent in regression is 'finding asymmetries in peaks which are useful to know about using adjacent info, bc peak symmetry is a useful assumption when true to determine more/all of the function from knowing a subset and relatedly useful to know about when false to identify when implications/similarities are contradicted and not absolute'
            - formatting the average as being more similar to points or being more similar to point connections (or being different from randomness or extreme points) is a useful set of alternatives when finding inputs that are common across useful structures or finding useful formats for algorithms that use averages
    - given that the complexity in regression is caused by non-linear shapes as opposed to constant lines, finding which variable interactions exist (which input variables are exponents) and which cause these complexity structures (exponents of an input variable, constant coefficients of exponents, addition/multiplication with other exponent terms, etc) would enable removing those complexity structures to reveal a simpler form of the data set that is more easily condensed into a regression line or a base solution function to apply changes to (like how removing parabolas that are symmetrical around a clear or probable average is a trivial task that makes the average more obvious), such as applying inputs of complexity structures to the base solution function to specify the more general simpler function
        - this is related to workflows like 'remove variables until a simpler component function emerges'
        - this is also related to workflows like 'find a simpler combination of inputs preceding the original function/data set' which can be applied in another direction such as 'find the function interaction level where maximal differences begin to emerge' such as how combining functions by type (like step functions, wave or other polynomial functions, sequence functions, discrete functions, closed shape functions) can produce functions that represent a 'combination of these maximally different function types' which may be more useful at filtering a function solution set than other interaction levels, so this interaction level of function types may be a more useful place to start when finding a regression function to summarize points
            - relatedly, finding 'sets of useful interface queries that decompose most variation' can be as simple as finding sequences of interface structures like 'find common/powerful/probable/high-variation variable interactions that match common system interactions like common system errors or common differences from incentives/defaults' and 'apply the function type interaction level to decompose the remaining unknown variable interactions, once these common/powerful/probable/high-variation variables are known', sequences which could be generally useful in 'decomposing most variation across problems', which is a useful problem-solving intent
        - relatedly, finding algorithms to connect simplicity and complexity (and independence/dependence and unstable variable/stable constant), generating one from the other (like simple core functions that when repeated generate surprisingly sustainable/cascading and complex differences) are useful to decompose the likely starting point and simple/complex components of a system
            - like the simplicity in the central limit theorem generated from a complex combination of random variables, and the tendency of complex systems to neutralize each other's differences in some way when combined/repeated, for interactivity (as a complex system is less likely to be sustainable on its own when repeated), and the requirement of a variable like a symmetry in a simple function for it to easily generate complexity
    - given the possible set of interaction functions (direct connection, constant connection, side of an area representing a limit/border, error connection, random connection), use these interaction types as a variable to identify algorithms to classify connections between points based on their probability of being one of these interaction types and the line patterns that emerge given some set of interactions of a particular type pattern in some subset of the data set
        - for example, any two points in a data set are very unlikely to have a direct connection, so that should be used rarely
        - if a connection is confirmed or probable to be a direct connection, adjacent connections are less likely to be direct connections and other connections similar to the direct connection in other subsets are likelier to also be direct connections
        - if connections indicate low volatility, finding a connection with an extreme slope implying 'volatility' is less likely after some ratio of connections are tested/determined/assigned a probability of some interaction type
    - a function to find the simplest (or otherwise effective) polynomial to describe averages of local data set subsets (where angle of lower/upper borders and densities influence the average) is an example of a standard method that can be found with adjacent structures
    - changing the definition of useful structures like 'averages' to find alternate methods
    	- defining an average as a 'line that when changed the most compared to other functions, still fits within the boundaries described by upper/lower limits' points directly to a method to determine the average function fulfilling that definition 'find a subset of possible different functions to change, and changes that can be applied to these functions, and apply boundaries as limits to these changes'
    	- defining an average as a 'difference from extremes' or the 'usefulness of right triangles in finding average functions (and vectors applied to them to generate extremes) of a data set' points directly to methods like 'find angles applied to a possible average line that capture the highest variation in a data set, once possible extremes are known'
    - evaluating a function's 'differences from randomness' is another starting point to base changes on rather than basing them on an average bc its the 'opposite of the intended information' and is therefore similarly useful in that adjacency
        - other function bases include functions that 'connect non-adjacent subset averages', that 'connect adjacent subset averages', 'connect function upper/lower ranges', & other representative/summarizing functions
        - a random data set is not useful and is therefore useful to determine early on in calculations, just like function limits and patterns are useful to determine, and randomness may as well be an indicator of falsehood (as in 'something that needs to be changed in order to determine truth, like requirements/impossibilities') bc of this lack of usefulness
        - other known structures that are not useful are equally likely to use as bases for change, like how known useful structures like core components are, bc of their dichotomy in the certainty of their usefulness
        - applying 'common types/variables of functions that can form randomness' (such as 'contradicting/neutralizing change types that cancel each other out' or 'complementary opposite change types (like triangles which form a square)' or 'randomness-amplifying which doesnt change the randomness of the inputs' or 'symmetries like the equivalent weight of dice sides' or 'a high number of variables' or other functions that are likelier than average to create the requirement of randomness as 'even distribution of probable outputs') is useful as a way to 'determine probable randomness' or similarly to 'determine differences from randomness', 'remove/add randomness' and other intents related to randomness
    - building an interface structure of interacting rules to base changes on, like a 'set of requirements' (like how 'connecting components of a structure' is 'required' to 'form component connections to create that structure' by definition) or similarly a 'set of rules that are definitely impossible or not true' as a foundation for other changes (where possibilities exist between contradicting limits imposed by requirements) is a useful structure to start discovering new rules from
        - identifying rules that identify non-adjacent information required by a definition is similarly useful like definitions that identify adjacent/obvious information required by a definition are, like how numbers in a sequence like the set of integers are required to be one unit away from other integers and required to increase if its the set of positive integers, so knowing that a set is sorted in an order like this gives you information about all the numbers between two items in the set bc of the definition determining the set allowing functions like 'estimate where an item will be found to reduce the search space', or how the type of a number determines some of its known functions
    - a standard method to solve problems is framing them in terms of core structures like 'similarities and differences' (like 'similarities to constant representative/average lines', 'similarities to averages', or differences like 'difference-maximizing functions', 'highest angles connecting adjacent/similar subsets as a function that is almost guaranteed to be incorrect to base changes on'), then applying a function to 'determine which differences to resolve' (like 'differences between base functions like averages and alternate similarities like densities' or 'differences from highest angles connecting adjacent subsets and a base average/density-determining function')
        - this method finds the variables likeliest to be known/similar (or easily derived/predicted as adjacent to known variables) constants like 'averages' and 'local subsets (locality as an indicator of similarity)' and 'densities', and variables likeliest to be unknowns like 'alternate more complex functions with more variables' and applies similarities/differences to model those and find which differences are relevant to resolve (the differences that 'connect related similarities/differences', related by providing complementary information for intents like 'represent a data set', complementary information like 'base functions' and 'specifying differences customizing that base')
        - finding a function that models the 'maximally different local subsets of the data set' is a solution-finding method easily produced by this method
        - this is an implementation of the workflow involving 'finding matching structures based on common attributes' to model 'uncertainties within that structure' and connect these structures to problem/solution definitions
        - other differences to resolve could be the 'difference between the set of possible/probable functions and the set of best representative functions' or the 'difference between a set of probable determining variables and a probable representative function' or the 'difference between probable linear representative adjacent local function sets and the non-linear variants that are better representatives'
        - this is related to a structure like 'applying variables to a structure like a "set of maximally different angles" applied as a symmetry to find the angle set that hits (or alternatively/equivalently approaches) the most data points when the angle set is changed the least (like rotated, shifted, scaled, etc)', since a 'line that approaches the direction of or is adjacent to the data point densities' is similarly useful as a 'line that intersects some ratio of points', as a 'representative function' isnt required to intersect with any data points, so that intersection is a variable that can change in the solution-finding method, as well as other properties not required by the definition
        - this formats structures in a way that makes it adjacent to identify variables and sources of variation (like rules like 'sudden constants/similarities that enable other changes to begin by providing a foundation like a barrier/limit for differences are a good way to identify interface variables in systems'), which is why the primary interfaces are useful in the first place (they allow highlighting uncertain differences by applying certain similarities/differences through standardizing/similarizing to fundamental/core unchanging variables like 'cause' as in embedded variables of that variable like 'causal degree' that support/describe/limit other variation and otherwise fulfill intents related to variation the most completely)
            - relatedly, identifying the most powerful variables as the biggest sources of error in a particular format, such as 'causal position' being a source of error in the causal network format (such as how an input might be an output of the output but it could seem like an input in some false similarity errors) and the 'connection function' in the network format (such as how some changes can seem adjacent/probable in one network format but its a coincidence, where some other network format is more reliable at making those connections adjacent)
            - relatedly, the primary interfaces are also useful for being based on the 'reasons' why a structure may be relevant to another structure (it is caused/changed/allowed/required/intended by other structures, it is useful to or interactive with other structures, it is a variation (as in a definition route) of other structures like concepts, etc) which are united on the 'meaning' interface (determining relevance/usefulness of structures to each other)
            - calculating the structures that are not in a data set but which could be relevant based on other structures that filter structure combinations like probability/similarity/commonness is useful as an intent to predict possible real structures that will be found in future data sets (calculating uncertain differences before theyre a problem/before theyre real)
        - this is related to other workflows like 'find maximum differences (what something is not/find the opposite of something to find limits of what it is)' by asking questions like 'what is not cause' (with answers such as 'an event that follows another event is not necessarily a causal sequence bc they may be so indirectly connectible that they are effectively independent') to find useful structures like the limits of the causal interface and how it interacts with other interfaces like meaning (such as how 'events that follow each other in time may be causally separable and arent required to occur in the same system or detectably influence each other')
            - similarly, finding why structures would not interact (such as how 'some structures cant detect/measure other structures, and therefore cant use them as inputs') is useful as a filter of 'meaning' interface logic
    - a unifying function of the various representations of a function where the representations are variants supported by the unifying function is likelier to represent the function the best
        - similarly, a unifying structure that supports various representations (like a 'set of maximally different directions', a 'set of reflective mirrors as polygon sides capable of producing different variants of the same information', a 'network of foundation structures around which maximal changes are supported which can coordinate', a 'set of filters capable of filtering the highest ratio of solution sets with the highest similar degrees of accuracy', a 'set of overlapping shapes with a common center (of common components) that model reality with similar accuracy', a 'set of connections between common high variance-capturing structures like maps/filters/networks') of the interface network (in various perspectives that filter it) is likelier to best represent the interface network
        - finding a useful 'sequence of filters' is useful as a good way to avoid problems of assuming too much & other basic errors of bias, like by applying "possible, known, required, probable, computable, measurable/testable, usable, & realistic" structures early on in the filter sequence
        - similarly, a unifying function of solution metrics (efficiency, accuracy, generalizability, flexibility) is a useful base to apply changes to in order to determine variables of solution-finding methods
    - finding useful structures to combine as defaults is useful, such as how 'symmetries', 'fractals', 'randomness as a limiting counter-structure', and 'right angles' are useful as core structures to describe a high degree of changes bc of their definitions ('applying fractals to changes in the direction of a right angle based on a symmetry up to the limiting point where additional changes appear random in their accuracy at describing change' can describe probable changes around that symmetry) bc of the relations between their definitions ('symmetries' and 'fractals' both having a 'common base (of a "self") for change, and a limit on changes to that base' in common, so applying these in the same structure benefits from their common symmetry in their definition and applies changes to this symmetry in their definitions, and 'fractals' further fulfills other attributes of symmetries like a 'limited change, as fractals converge')
        - similarly other examples like 'why i is a relevant number to rotations' given its adjacent concepts which make this functionality probable or inevitable (not only its allowance by definitions, but also possibly its difference in ability to produce a difference in sign/direction from the origin, its core operation of 'multiplication' being a definition of the components of a type of n-dimensional change, 'multiplication' as relevant to angles through creation of closed cornered shapes, a unit of change that is relevant to rotation, the relevance of sign changes to wave functions which are relevant to circles, etc, which are useful changes for connecting more directly relevant structures to rotations like pi), in its function connecting two other structures of rotation such as e, a structure related to spirals (a structure with a useful equivalence in its change rate ratios of adjacent compounding) and pi, a structure that acts like an origin/symmetry of external/internal spirals (trending toward polynomials in the external direction and trending toward individual points in the other direction) and would be a spiral but is missing the 'change rate increase' to create a different equivalence than a spiral has (a change rate equivalence, that creates the property of 'closedness' in the circle as opposed to the spiral but is sufficiently different from a 'cornered shape' in either the internal/external direction so as to justify its own definition separate from spirals or cornered shapes, despite being equidistant from these and other and unknown structures in their definitions)
    - mapping problems to more defined fields like highly structural creative industries such as 'music' to find concept mappings that are more easily determined like how finding the rule 'intelligent goodness is more difficult than obvious/complicated wrongness or obvious goodness' by applying the clear definitions in music of "obvious rights/wrongs like compliance with major/minor chords/notes or compliance with patterns" and how finding the intelligent goodness requires knowing the obvious errors it avoids like incentives like 'cheap rewards from any difference, even wrong differences' as its easier to create a new minor song than to create a new complicated but good song, as the range of possible solutions is narrower but can still host complexity/variation that intelligence could survive in, and complicated goodness is more complicated than complicated wrongness bc of the additional problem of the limited range that requires creativity to sustain intelligence in, a rule that would help avoid errors of over-simplification, prioritizing any difference, and avoid obvious errors as well as errors that create changes that violate a solution structure like a range of good solutions, and would incentivize finding high-variation variables sooner (to stay within the limited range) than prioritizing wrongness would
    	- finding a corresponding physics version of a useful structure is a good filter to apply when determining useful structures or other structures to apply as defaults bc these are likelier to be functionally useful if not more plausible, realistic, or possible
    	- example metaphor: gravity between 'equivalent alternates' (such as alternative theories) is weak but enough to keep them in the same definition (allows 'aggregation' to occur)
    	- other example metaphors which are useful in the sense of being evocative or otherwise useful for calculating something: 'supersymmetry' and 'interface network', 'string theory' and 'variables as waves, as reality units (possibly related to twistors/spinors)', 'isomorphisms retaining histories/inputs' as some definition of a 'wormhole', the 'jacobi identity' (and other structures of inequality like asymmetries) as an example of a 'way of determining what structures will accrete in some definition of unidirectional time (like to determine what will become matter)', 'commutativity' and 'equivalences in quantum superposition probabilities', 'invariance attributes like associativity/commutativity being a useful structure as a base to apply other changes to, to base other more speculative variables on', 'non-orientability and CFT violation', 'whether representations of reality form reality, to the extent that it can collapse/expand into or be based on other representations as those become more energy efficient, like in a cycle of different representations', etc
    		- related questions: 
    			- are all measurable invariance attributes components of reality or are some of them the basis of some reality and others the basis of another like points on a lattice that can be a foundation of reality, where points in between are not, so travel between them could only involve motion in the transform producing either from the other
    			- which symmetries are foundational and absolute in the math interface? which symmetries (like abstraction as filter or map, connecting similar concepts like energy/variation) connect these absolute math symmetries to physics symmetries? which symmetries allow maximal differences to develop? what symmetries exist in reality (such as symmetries across space-time states preserving potential energy or entropy, so that we only have access to functions within a probable range area governed by that metric)?
    		- the usefulness of these metaphors depends on the variability between 'definitely possible' and 'definitely not possible' (plausible, logical, not definitely impossible, evocative, suggestive/implicative, conditional, etc), so that a tool to 'traverse similarities and apply maximal differences to find structures like requirements, symmetries, concepts, and limits' can benefit from embedded but not articulated useful variables/structures such as connection in language (superset of definitions containing a subset of math definitions)
    	- similarly, other metaphors include how the number-based spaces can be extended to apply to number types (like a 'prime-based space'), how a 'complex number-based space as a possible structure of reality' has a corrollary in the insight that "almost every fact has a counter-fact (similar to paradoxes with local contradictions across statements) where it is related to its equally legitimate opposite (where it's not true), where opposites are allowed by definitions (similar to manifolds that can seem like extremely different objects that contradict each other absolutely but are actually related consistently by some common structure)", where alternate possible spaces may represent/embed other possible insights as their generative/limiting/determining functions, and which may intersect/overlap in a space of these insights (which has a corrollary with quantum field theory), etc
    	- 'time travel' could be possible in the sense that realizing things faster and being able to store/determine/compress more information (which increases the potential time available for the realizer, by slowing down time for them, as they have more functions/variation than other people) can put everyone else in the past relatively by decreasing their relative rate of change, but only while the realizer continues to do so and if they can reverse that change
    	- 'increasing similarity in the sense of synchronicity' by giving everyone the same perspective (like by giving them a maximally different structure to handle problems with) is another function of putting agents in the same space-time or on the same timeline (converging timelines and integrating/connecting spacetimes)
    - finding solution functions for possible known errors to existing methods, like how 'standard neural networks' can have an error of 'finding a different function of different components for each input/output pair' or 'finding overly simple functions that re-use the same (or otherwise simple) components the most but dont handle extreme/new contradictory cases' or 'finding all possible incremental components of some size that could differentiate some outputs and removing some of the less useful/adjacent/common incremental components'
    - identifying interface query intents fulfilled by known useful structures (like how 'cellular automata' and similarly 'standard neural networks' are useful for interface queries such as 'finding maximal differences generatable by one logical/change unit on its own, to identify where a seemingly complex phenomenon can be identified by one variable (the logical unit), thereby reducing the complexity of high variation data sets or the complexity of finding variable interactions between variables of high variation data sets')
    - neural networks may have an error created by the 'sequence of training data', in which earlier training data influences the final solution function disproportionately to its utility value
        - finding the worst case scenario where each training algorithm could miss the most obvious (or otherwise useful) alternate/absolute optimal (such as with gradient descent) bc of the order of training data and how to correct these problems (such as 'start training multiple models at multiple different points and attempt to integrate/converge to an absolute optimal given their change types, like pursuing only those descents where lower values are clearly identified or otherwise where lower values continue to be possible')
        - abstracting this workflow to 'find common variables like "robustness to order changes" that are highly differentiating in math fields/functions, then apply these common variables to neural networks/regression algorithm to check for variation resulting from these variables'
        - allowing variable interactions identified in later training data (like more foundational base symmetries) to replace/change variable interaction structures (like an emergent lesser symmetry already identified in earlier training data) may be more complicated than just applying PDEs to assign changes to specific weights (like by integrating the 'weights of the lesser symmetry' with the 'structures like limits/invalidations of the lesser symmetry allowed/required by the base symmetry' on which it depends, such as by consolidating weights indicating the lesser symmetry variables into one more powerful variable on the base symmetry)
        - having an 'update function' that handles updating the weights in each of these worst-case variable interaction cases (or the most erroneous or most common cases), such as where 'the final training input indicates the base symmetry that is extremely different from the lesser symmetry already identified'
        - the reason interface analysis is so powerful is that it acts like the 'base symmetry (or hyper/metagraph) of all base symmetries (or hyper/metagraphs)', so having a neural network that tests interface analysis variables as a prioritized structure and integrates them into update functions is more powerful than other neural networks
    - 'minimize extreme errors of a solution function' and 'maximize data set coverage of a solution function' are both 'alternative contradictory' intents to solve the 'find a regression function' problem, which offer some degree of 'complementary info' rather than 'definitely overlapping or equivalent info', as the 'extreme errors like missing an entire variable or missing an outlier' and the 'maximum data set coverage' have no guaranteed overlap, as a 'maximum data set coverage' could easily exclude extreme values or other sources of extreme error like variables which are easily missed if some subset is selected
    - an example of specific simplifications to solving prediction problems can involve avoiding known suboptimals/errors/violations of requirements by identifying & applying differences to those and allowing all other variation to develop, such as predicting a subset of just the worst case scenarios by applying extremes to increasingly high variation variables (technological development in some direction like electricity/automation/chemical printing/speed/compression/computation) and identifying opposing variables (particle accelerators/generators which can modify components of or interactors with electricity, software likely to be used with electricity, & quantum technological development which can modify components of electricity can oppose a possible error possible with electricity technology in an extreme such as in a concentration of one position, 'if energy technology or energy technology variation (innovation) was concentrated in one position by some entity, what could oppose/change it'), and otherwise modifying high variation variables ('what are all the worst case combinations of high variation variables with other high variation variables like quantum computers and neural networks'), as if variables dont cause high variation, they can sometimes be ignored in some cases until they indicate change in the direction of causing high variation, as knowing the 'structures like patterns of variable development such as patterns of large-scale errors' is more useful than solving for every possible variation interaction in cases like with computation limits, similar to how 'limit change patterns' are also useful in decomposing all variation
        - this can generate possible innovation intents like 'encrypt physical molecules so they cant be read', 'check if enough particle changes occur in a sufficiently continuous area, whether different types/configurations of gravity can emerge', 'inject variables in reality in a way that creates gravity, if uncertainties attract gravity to require interactivity to develop to handle the increase in variation without destroying the foundational structure (create a structure like a black hole, an isolated uncertainty cascade)', 'check if there are different units of spacetime like an opposing fact/falsehood pair or a pair of space-time states connected in a sequence or the interim structure of other suggested units like a wave/particle', 'speed up information travel/acceleration technology so farther information can be read and used in predictions more quickly (such as by directing radiation/randomness wherever its not visibly/directly reflected back at us by hitting a structure, so that it hits something we cannot measure and therefore is likelier to encounter maximal differences if they exist, which are likely to respond back as intelligence sources and improve our rate of technological advancement, or similarly direct radiation at inputs to these sources which is likelier to be findable and is similarly likely to get a response as well as likelier to incentivize organization/connection and lead to an increase in the power of radiation to cover distances as those will be reduced by this connection and therefore increase its speed), or similarly direct radiation in known stable ways that create maximal differences here to attract other maximal differences as intelligence sources who require differences to solve their problems' and therefore the direction that changes (innovation) may be applied to fulfill those intents, as measurable intents are likelier to be focused on & fulfilled, and relatedly 'horizontal innovation' can also be incentivized to connect these innovations in different directions on a new interaction layer
        - conceptual math is particularly useful here (not just 'add a concept to another in a simple combination/set structure') but entailing all of the possible useful interactions between concepts
            - if you can run queries like 'build a new (not already known) possible interaction type between these concepts which could be true' and 'what are the probable error and limits of the interactions between these concepts' and 'which concepts are more interactive with these concepts' and 'which concepts reduce the interactions of these concepts', you are applying conceptual math, as opposed to just stringing words together in a simple combination/set
            - 'what are the possible interactions of interface variables, such as some areas of powerful developing technologies (like batteries, quantum computers, encryption, and 3-d printers)'
    
    - identifying the few 'specific problems to solve, which can be determining of all other structures' is useful for reducing the required computations to apply problem-solving methods to, such as how identifying some variable interactions is more useful (like eigenvectors/eigenvalues, or energy/entropy/potential) than other variable interactions, which can be optimized in their usefulness by identifying how problems are related so once these variable interactions are found, the other problems are adjacently solved (identifying the 'problem network' that will fall to these variable interaction functions)
    
    - identifying useful structures (like 'math structures' such as 'independence') which act as proxies of other useful structures (like the 'maximally different structure')
        - for example, finding the 'most independent variables' is useful for intents like 'creating randomness', 'identifying organization', 'identifying interactive variables', 'identifying non/correlating variables', as 'independent variables' are useful as possible interactive components of systems (as opposed to the self-interactions of dependent variables which are by definition connected), as this intent is more mathematical and measurable given the more well-defined definition of 'independence' compared to other structures (like the 'most complex structure' or the 'most different structure' which may not specify how to determine differences, but can be more usefully framed as being the most consistent structure creatable with independent variables), as 'independent variables' by definition come from 'different systems' where they can be isolated, and finding the most independent structures which involve the most variables from the most different systems is likely to be useful in generating related structure like the maximally different structure
        - this is another example of why 'cross-interface structures' are more useful than other structures, such as when a specific math structure is more useful for calculations so knowing concept-math maps is useful for that intent
    
    - identifying interim structures between defined structures is useful where existing definitions dont fulfill intents optimally
        - identifying 'complementary filter sets' is useful as they are an information format that can be adjacently connected to the interface definition, as a set of filters that captures complementary information can offer a more complete ratio of information capture than other structures, and filters are adjacently connectible to interfaces in that they highlight specific differences within a similarity limit
        - similarly the 'system layer diagram' is useful in the same way, as being adjacently connectible to the interface definition (the set of component interaction layers describe a system with sufficient isolation and interactivity to capture high variation and fulfill problem-solving intents like 'build/break' such as when applied to problem structures like 'break a problem into sub-problems' or 'build a solution')
        - both of these structures are useful for very different intents
        	- for example, the system layer diagram is also useful for indicating overlapping set memberships and can adjacently generated other useful diagrams like venn diagrams, and occurs in math contexts like 'lie group & quaternion projections'
        	- 'complementary filter sets' can also identify 'complementary variables that frequently co-occur in systems'

    - identifying functions in between error functions and solution functions has some known structures to help determine the interim space, which can be applied as default bases to change to determine interim functions which are more useful than either extreme
        - for example, the functions that produce errors every time except in some specific known/derivable condition such as 'when the solution is given to these functions directly as an input' (identity functions, simple functions) are known to be sub-optimal for most intents, and the functions that are optimal in some way similarly have structures which differentiate them from these known sub-optimal functions (though they are high-cost in some way as well, such as 'check every possibility'), and in between are functions similar/different to both which can be derived
        - specific problem format (benefit/cost problem format) difference-resolution rules like 'differences between cost-handling demand and cost-handler supply' like 'dont centralize costs/dependencies on one cost-handler/independent function, which is known to destroy the value/structure of that independent structure' are similarly known sub-optimal functions which can be altered to produce more probably optimal functions ('instead apply a different structure such as, distribute costs with a non-centralized method such as distributing them evenly')
            
    - identifying structures that are useful in the absence of some information that is probable to be missing across problems
        - when 'maps/fields of similarly sub/optimal solutions' arent available, which simplifies the problem of finding new/optimal solutions, bc simple position changes in some known optimal (or known different-from-suboptimal) direction can produce a better solution, what other variables can be applied instead of variables in that structure
            - apply difference/diversification structures (try multiple maximally different solutions and apply adjacent changes to those until some subsets are filtered out)
        - when a data set isnt graphable bc of its complexity/size, which variables can be applied to determine/visualize it
            - apply standardization, to identify highly differentiating variables on highly similarizing variables to determine/visualize the differences from how these 'differences in similarities' differ from other known structures of 'differences in similarities')
            - identify truth structures, as in some subset/variable of it that probably retains/stores a high degree of variation and determining/visualizing that instead
        - relatedly, what does it mean to have a condition like 'not having a map of similarly sub/optimal solutions' (meaning 'how does it relate to problem/solution structures like available resources/inputs or errors & the solutions to those errors')
            - it means either one of the following
                - the input variables are incomplete/not the correct variables/otherwise missing (the solution involves finding/deriving/generating new info, formatting variables, etc)
                - that the solution field is volatile, so traversing it is not optimally useful (the solution involves de-volatilizing a function, so that adjacent input changes create adjacent output changes)

    - identifying specific maps to connect known useful interface structures
        - for example, knowing which variables (like an extreme difference in outputs of adjacent input) produce which attributes (descriptive attributes like volatility) is useful for mapping these structures/intents
        - similarly, knowing functions which oppose/offset this attribute (like a function to make a graph un-volatile) can be checked for in the data set to predict that attribute in the output

    - identifying common variables in the most useful structures, as a useful intent that can fulfill other problem-solving intents adjacently
        - for example, the most valuable structures are 'cross-interface structures', 'simple connections between complex structures', 'maximally similarity-differentiating or difference-similarizing structures', and other structures which are unique in that they connect highly different/complex structures using adjacent functions
        - another common factor between these is that they reduce dichotomies to an interim structure (the 'most complex simplicity', the 'most uncertain certainty', the 'most variable constant or most constant variable', the 'most different similarity', and other extremes testing the limits of the core definition at the root of these dichotomies) that is more powerful through involving these extreme differences in the same structure, and offering an interim point at which to base changes on to generate the other points
        - identifying the 'points between other useful structures' that can act like averages/symmetries to generate the other useful structures is another useful intent
            - this is related to finding an interim/average/origin that is useful in generating other points trivially, such as with applying a rotation or applying a vector of the same length
        - identifying the connections between useful structures (like 'different intents fulfilled by the same function' or 'all the different ways to use a function' and 'different functions that can fulfill the same intent' or 'all the different ways to connect some structures of the intent') that make some useful intent trivial, connections such as 'similar functions' and 'similar intents' and 'similar functions/intents', where these similarities (yet to be identified, such as similarity in 'information-preservation' intent fulfillment of a function, or other useful metrics like 'specificity of the intent resulting in a highly filtered possible solution function space') offer alternate connection components to form routes to fulfill some intent, as opposed to finding a route in some default/standard space like connecting input/output points
    - identifying the core variables of physics/math can be used to identify other primary interfaces
        - for example, the 'lagrangian' (to measure potential, such as 'adjacent possibilities', or 'all possibilities'), jacobian or relatedly commutativity (to identify sequences/order), lie algebra (to identify symmetries), and other default math/physics structures can be used to identify other primary interfaces and useful interface structures
        - similarly, the concept of 'gauge-fixing' (as a 'disambiguation' structure through 'difference from some embedded symmetry, like a zero/non-zero sloped line connecting rotational states') is related to useful structures in the 'regression' problem space, such as insights/rules like that 'any line crossing a general data set area can be useful in determining the actual average line (reducing the set of functions that could be equally valid, acting like a filter of possible functions)'
        - 'non-local causality' and 'quantum entanglement' (connected non-local nodes, connected by some function like equivalence, to account for interface structures like 'false structural similarities' that occur in real systems but arent semantically relevant (the reason for the similarity is different and independent)) are examples of physics structures that can be injected into neural network algorithms
        - similarly, the concept of 'diffusion/dispersion' comes with the concept of a 'source point' which leads to the other points, which is useful as an alternative to an 'average/interim/hub' point to apply the concept of 'state' to a data set
            - relatedly, the concept of 'heat' corresponds to the concepts of 'potential energy', 'entropy', and 'interactivity'
        - similarly, the math structure of an 'affinity' which 'preserves some similarity metric (like parallelism but not necessarily distances/angles)', or an 'isometry' which 'preserves some similarity metric (like distance between structures)' is useful as a 'variation' of the 'structure of an interface', an interface being that which reduces (rather than preserves) the distance between some similar structures to connect, to make some intents trivial (such as 'connect differences' and 'differentiate similarities'), and relatedly differentiates some other structures to differ (by identifying structures like symmetries and applying those as default interaction objects)
        - similarly, the math structure of a 'contraction mapping' that reduces distance between all points is related to interface analysis, in that its default structures reduce the work a query has to do to connect problem inputs and solution outputs, as it makes all useful structures adjacent to connect, so that trivial queries on the interface network solve problems formatted as differences on the interface
        - similarly, the concept of a 'topology that can be changed to make any of its structures adjacent without altering its properties (like storing maximal differences)' is a useful math concept that can provide structure to filter the searched set of possible solution structures when searching for the 'maximally different structure'
        - the 'stress-energy tensor' of physics is analogous to the intent/function cross-interface structure (the problem/intent/usage of a function and its functionality, as in its ability to respond to the problem represented by the intent)
            - relatedly, the derivation of the 'stress-energy tensor' indicates a sequence of multiple expansions, substitutions with equivalences, groupings, change rates, and orderings, as well as standardization by a similarity in common (coefficient) to reveal an irreducibility/isolatability/component (energy density of the system) or a target defined structure (the 'divergence'), which is related to the core functions which are re-used across problems, changing core variables like order/set/terms/complexity with a set of common operations to produce most results adjacently, and which follows a general 'expand/generate and filter' solution automation workflow
            - relatedly, the mathematical 'group' is analogous to the 'structure-function cross-interface structure' as useful for forming other function/structure pairs or the 'function (as a unit of change, analogous to a variable)-constant dichotomy' that is so useful across problems to produce other un/certainty pairs
                - relatedly, the idea of finding the 'interim' (or other equivalence like an 'average') point between two structures that makes both of them trivially generatable with the same operation applied to the interim point as a useful compression structure of both points
                - relatedly, the 'algebraic structure' that is a 'set/operation/axioms', as a variant of the highly useful default 'generate/filter' problem-solving workflow
                - relatedly, 'lattices' as a structure to identify paths between points using the same vector as a unit of change
        - the 'frame of reference' as an 'observer and a coordinate system' in physics is related to the interface analysis concept of a 'perspective' (as a filter with priorities that highlight/clarify different structures)
        - similarly, the 'momentum' variable shows up in physics frequently and indicates the 'incentive' variable that determines many interactions
        - similarly, the 'frequency' variable shows up in physics frequently and indicates the 'commonness' attribute that can lead to 'probable structures'
            - relatedly, the reason for using the 'wave function' as a unit of polynomial functions and using it as a unit of spacetime, as a high variation structure with a symmetry and a high variation interim interactive structure, respectively
        - similarly, the 'p-adic numbers' offer a mathematical variant of the useful structure of a 'similarizing difference' by applying a similarity to the difference between numbers, changing which numbers are adjacent (highly different numbers rather than the standard of highly similar numbers, by applying a different similarity 'extremity of the high value of the power')
        - similarly, the 'imaginary numbers' of math reflect truths of reality like 'every true statement has an opposing statement that is true to some degree or in some way or in some context' and physics structures like matter/anti-matter as well as the importance & commonness of dichotomies and spectrums as important/powerful variables, which is an adjacent transform, once that insight is known & applied and those structures like dichotomies are known, just like a 'ghost of a definition' which retains the bare minimum 'skeleton' required for the definition to still be true, but is still so different bc of the variation allowed within the definition that it seems to contradict the definition, these overlapping/integrated definition 'ghosts' determining the potential and 'probabilities of resolution (into a structure/state)' of a definition
            - relevantly, applying 'alignments' to preserve connections (like an 'opposite definitive' space of 2-d euclidean space to graph imaginary values, or a 'system' space where both values are definable/graphable but the 'opposite' structure of their connection is maintained, such as that they move in opposite directions, such as an 'opposite of space-time' where the definitions still hold so that 2-d imaginary structures can be applied to physical structures described with the opposite of imaginary numbers, as opposed to using dotted lines to indicate imaginary variables, like an 'inverse space with a negative component indicating the imaginary component, in the position of an exponent coefficient, or a standard vector space indicating change direction components, or a space preserving the positions/connections between roots of real-valued variant transforms like even exponents, with the odd exponents positioned by definition in between, while mapping the powered spaces into the unit space')
                - an 'inverse' originally framed as a possibly useful opposite structure (an opposite as in a orthogonal root or an inverse) is particularly interesting given the 'inverse of x' whose integral area equal to 1 identifies e, which together with x, acts similar to e^i x pi in euler's equation as a factor of -1, as if an 'inverse/e taken to a power of (root-type/i x rotation-type/pi)', given that when multiplied by itself i x pi times it generates -1 (i squared), which isnt quite the case (these arent directly mappable to the concepts of inverse/root/rotation)
                    - but this inverse structure is still related to i (however its only when e is in the position of the base multiplied by itself i x pi times, that i and pi act similarly as i squared, and i is the cofactor of the power rather than the base, and the function is not a square of equivalent factors, but the factors are alternate factors of -1 than i)
                - why look for the 'area equal to 1' (in relation to 1, as in above 1) in a function like the inverse function at all?
                    - finding equivalences, especially in units like 'unit areas (the unit square having side length 1)', is useful as a comparison metric between functions, to allow other intents like 'converting/mapping between functions'
                    - the 'unit area of side length 1' mapped to other functions is by definition related to the unit exponent (the square of 1 and -1, produced by i and 1 respectively)
                    - so you might look for the 'unit area equal to 1' in a function (like the inverse function) bc you know that the unit exponent (one squared, negative one squared) is related to i, and you know units are important, and the 'unit exponent' is a core/important unit
                - why look at the 'inverse function 1/x' at all when examining i
                    - x and y represent different sets of 'factors of 1' in that function, which are related to i's opposite unit (1)
                - what other differences should be examined
                    - the mapping to unit areas of a four-sided shape with all curved edges could be relevant as well, as an interim structure between a square, the area under a function like the inverse, and a circle, and how these structures are related to other interim structures like spirals (which could be useful for related intents like 'identifying change on the outer loop according to a base change of the inner loop')
                    - finding the non-obvious sums that generate i * pi which could be equal and interesting (meaning adjacent/relevant to other intents) as alternate square roots of i when positioned as powers of e
                        - what sets of values of e^x create a unit area of 1 when multiplied and other related constants, and how do these values and the powers of e that created them interact with other relevant structures to euler's equation
                    - finding alternate operation sets that generate useful constants (what else other than roots/opposites/rotations/inverses can be combined with simple operations like exponents/coefficients to generate useful constants like i/pi/e, such as waves vacillating between i/1 or the unit circle)
                - the question is 'what combinations of e and pi can act like i' which can be translated to 'what combinations of alternate unit areas in the inverse function (e) and the rotation constant (pi) can produce an opposite type such as the opposite of the unit root 1 (i)' (why do these 'inverse/rotation' difference types from e and pi produce the 'opposite difference type' that i adds) and 'in what structure/position do these constants act like i (deriving eulers equation)'
                - this is another example of how an 'evocative' structure such as an 'inverse' that is evocative of i (that can not be defined as directly equivalent to some relevant term) can still be useful (can be related another way, like an alternate function/format that identifies the same connection function/variable), similar to how 'circles' are evocative of 'primes' given their false appearance of randomness
                - what does it mean to say that e has i dimensions of itself (e is multiplied by itself i times) and e has pi dimensions of itself (e is multiplied by itself pi times)
                    - related question: is there an interim value between i and pi definitively that makes the equation an alternate root of -1? (e multiplied by itself x times where x is definitely in between pi and i)
            - relatedly, questions like 'what are the types of time in between imaginary and real-valued time' and 'what are the limits of imaginary time in supporting a higher proportion of structures within the time definition than is implied by current understanding of definitions' and 'what connections are required/possible between different types of time and what interaction levels use these types of time as defaults' and 'what variables/functions determine/maintain/require the interface between reality (stable/constant structures emerging from quantum physics interactions) and potential (quantum physics components), is reality similar to seeing a cross-section of a wave that is at a interval where perception can occur and the interval points can be connected and change can be synchronized across those point connections, where other realities are not on the detectable spectrum at that cross-section without making inferences by calculating gradients to adjacent detectable points' are useful as evocative thought experiments and interface queries to find useful structures
        - similarly, the use of 'light' as a metaphor for 'varition' is useful for determining core variables of change (like angles of possible motion in a sequence) to determine valid and realistic methods of inference (like 'scan an area created by some angle of change from this sequential pattern, according to how light reflects information at angles')
            - relatedly, the concept of 'color' is created by 'differences in configurations of points/components in a set' as opposed to just 'simple structures like counts of points/components in a set' which allow different wavelengths to pass through the set without hitting a point and different wavelengths to hit the points of a set, which can be applied to statistical regression in structures such as 'finding which simpler wave functions pass through a subset and hit a subset, indicating information about the wave functions (or variants of them) that reflect info about the whole set', which can be applied to 'preceding sets of former versions of inputs' rather than just 'subsets of inputs', like a reverse fourier analysis
            - similarly, finding other variables like inputs/outputs of light (that reflect information such as 'input/component patterns/connections', such as shadows/angles/reflectivity) is useful for finding information about a data set in statistical regression
            - similarly, finding info-reflecting variables (as in 'info outputs/signals' like 'shadows') to look for when some other useful structure has been identified (like a 'compounding sequence' or a 'contradictory/negating/neutralizing sequence') is useful for identifying the probable identity of the final solution function using shadows of preceding/input functions in the sequence (as in, 'given some output/shadow earlier in the input function sequence, what info is derivable about the shadow of the final solution function, and what does the shadow indicate about the final solution function')
            - relatedly, finding 'similarities/commonalities' in possible solutions/outputs such as the 'shadow shapes that can be generated by extremely different inputs' as well as finding the 'outputs/signals of differences in inputs' like 'changes in the shadow shape when some trivial function is applied' and finding those functions which highlight these important differences to identify, given the extremity of the difference in their inputs, are related useful structures
            	- for example, as mentioned elsewhere, its important to differentiate a parabola and a wave as they can look locally similar, and similarly its important to find other volatile similarities as well as functions to differentiate them (produce the changes that make the difference obvious/detectable)
                - as another example, a 'superconductor' is a metaphor for useful structures like interfaces, which add high value output (electricity) at slight/cheap inputs (adjacent changes), as the structure of the superconductor itself reflects a high variation model of reality that is already useful without inputs at connecting variables
        - similarly, the rules of brain structure interactions, which reflect interface structures such as truths (like 'differences in similarities' as 'random electrical activity in hyper-connected component brain regions')
        	- https://science.slashdot.org/story/23/03/21/2219254/psychedelic-brew-ayahuascas-profound-impact-revealed-in-brain-scans
        - similarly, the related concepts of 'independence/orthogonality/non-intersectiveness' in math is analogous to useful interface structures like 'maximal differences' (such as differences in 'maximal difference-uniting symmetries' like cross-interface structures like 'cross-system similarities')
        - similarly, the structure of 'quaternions' is a useful analogy to a 'unit interface structure' in that it encapsulates a rotation and an axis of rotation, and also applies a 'maximal difference possible within a definition (of a real value) without breaking the definition' as a useful structure to fulfill intents like 'create opposites/differences'
        - similarly, the manifold is a useful corrollary to a 'type' or 'definition', in the sense that items belonging to the type or qualifying as the definition can vary within a set of limits maintaining the manifold structure
        - similarly, the concept of an 'exponent' (self-similarity) is related to 'randomness' (in that a square allows more randomness if its the shape of a data set as any connection between low/high x-values is equally possible) and both are also related to 'symmetries' (in that the square has four symmetries where a straight line has one, and that randomness aligns with symmetries in that symmetries act like equivalences under some change), which you could predict from the 'self-similarity' attribute of the exponent
            - relatedly infinities could act like a symmetry in that if there was an infinity in physical reality, it would be invariant to change (taking one item away from the infinite set wouldnt change its 'infinity' attribute)
        - similarly, the 'density' of a black hole is a corrollary to a 'density' of a data set in that both are 'powerful' (the black hole is powerful through being a source of energy, and the density storing information is powerful to the extent that it influences the prediction function)
            - this is an adjacent sequence once both structures are standardized to the 'information interface' (the 'density' is a significant attribute of a black hole to focus on, bc of how the black hole interacts with 'information', similar to how the density of a data set interacts with information by storing/representing information through its center/average)
            - this indicates if you had an equation like the following, you could derive x using interface analysis to derive 'density of information' and 'power of information' as a useful connecting structure allowing the equivalence/similarity indicated by '=' and identify 'some energy type (like dark energy)' as the x-value
                black hole/x = density of a data set/power (in relevant problems like prediction or determining the rest of the data set)
            - this is a task that transformers should be very good at, given their ability to map systems to other systems, if applied to interfaces as the systems to map
        - similarly, variables such as 'heat' connect different interaction levels (as a 'primary change-determining and change-generating variable' of chemicals, connected to a 'primary exchange unit and primary input' of cells) so these variables connecting different interaction levels are valuable to identify as connections to new interaction levels
        - a universe that requires components to exist also requires a combination function and combinable structures, and one that doesnt require 'uniqueness' allows for 'repetition' and therefore 'quantity' to be measured and 'a number set to differentiate quantities'
        - the idea of a 'particle/frequency of cause' could be the wrong structure to look for, as 'cause' emerges from constants that allow the following, meaning that a coordination between many particles and input/system conditions is required for the concept of 'cause' to be allowed and emerge
        	- change to occur in a 'input/output sequence' that can co-exist with other sequences (occupying and validating the same timeline, and also creating it as a component of the timeline)
        	- interactions between structures, as opposed to requiring isolation of structures
        	- interactions/changes that dont invalidate a high ratio of other structures by default
        	- inefficiency of structures in handling variation injections, needing to move or disintegrate in response to variation like collisions, rather than having stronger forces maintaining their structure, which is an output of energy transfer
        	- however cause could be said to occur at a 'frequency' in that it requires two states to be connectible by some distance across which information/structure can be preserved, and similarly in the sense that a structure is a cause of other structures if its maintained long enough to impact other structures, a structure could be said to be 'causative'
        	- 'causal erasure' relatedly occurs under conditions where many equivalent alternate common structures lead to the same effect, as opposed to a direct identifiable unique causal connection, and similarly erasure of other attributes of cause like 'inevitability' can remove the concept of cause
        	- relatedly, is there a structure such as a 'default network of entanglements (default synchronized structures)' that ensures primary interface concepts like 'information', 'cause', 'change', and 'potential' can continue existing, which creates new entanglement connections if some are disrupted, and is reality the network or set of overlaps connecting these default networks of entanglements, and can these entanglement networks be used to distribute variation/energy evenly (using a randomness entanglement network) so that spacetime curvature doesnt occur and spacetime is experienced similarly for all observers, or would the universe move too fast in some direction once observers are moving at constant speeds for that to be useful or would it distribute change across the universe so that there is no motion of the universe as its neutralized by the evenness of the distribution
        	- more likely than 'absolute retrocausality' is the idea of a 'mass of energy (acting like a large body of fluid) that may overlap/flow into some future state in some condition temporarily (like an occasional errant wave flowing farther out than the others), but will reverse course to be centered in its prior primary stable state, so that this specific future flow seems like an orphan leaf rather than a new timeline, as the return to the primary body of fluid is the stable state, and the future state wasnt connectible/interactive with other states, so time didnt continue to flow in that direction, even though that future state may regularly happen, but not often enough or probably enough to be really real in the sense of stable/connectible time flows, and this future state may or may not impact the primary body of fluid (it may cause itself to re-occur by temporarily flowing back into the primary body of fluid rather than farther away from it, or it may not have a measurable impact on the primary body of fluid)'
        - similarly, 'potential' emerges from constants that 'allow some changes/interactions to occur within a limit, allowing organization to occur' rather than 'preventing any change/interaction' and also 'prevent all changes/interactions from occurring in a way that leads to chaos' and similarly 'allow change types or change inputs to be stored rather than used'
        - why is it possible to make predictions using something other than 'input/output sequences'? bc there are alternative equivalent structures (requirements, networks, probability/potential fields) which are non-sequential and which are useful structures (stable structures of reality)
            - "is there always a 'mirror/symmetry' to reflect across every symmetry (is there always a 'mirror' to reflect past/future states of a system, given some distance to reflect information across)" is another useful question
    - identifying function sets to identify similar structures that can generate useful structures in an alternative way, such as how some sequences can generate a 'change rate that changes every time' like the fibonacci sequence, which can look like and function similar to other types of change like simple exponential change, using a different input structure, as a way to fulfill useful problem-solving intents like identify different possible inputs
        - fulfilling a limited set of problem-solving intents is possible with specific functions which are alternately called 'interim functions'
    - identifying the possible usefulness of alternate function formats (like a 'unifying parameter' of 'maximally different sequences (different for each set of x-values such as 'adjacent x-value pairs') that when summed, converge to the y-value' which would be useful to connect systems of variables that could make every set of x-values justified in having its own function, and in applying a different function format of a sum of a sequence as the sequence of change combinations that are adjacently computed which can be used to calculate the output sum that is the y-value), formatting the problem of regression as a 'solving a system of equations' problem, where the parameter to solve for generates the sequences, and where maximally different sequences indicate different possible systems that the variables could interact with, which could all be equivalent alternates in the sense of generating the same prediction function, which mostly only makes sense when using a set of x-values with y-values to connect in a particular function so that the sequences are relevant by default
        - neural networks should apply changes as 'variables/functions that are actually encountered in the real world', as a variable is applied within a system, as opposed to the approximated variant or the over-simplified or over-deconstructed variant
    - identifying 'points of relevance' where functions can be injected in a useful way to optimize a structure, such as 'during any given training iteration, applying a function like "checking whether changes are moving in the direction of the target output to filter less probably successful change sequences earlier in the training iterations" is useful to avoid computations that are probably less useful to invest in' using rules like 'where are processes repeated or otherwise inefficient which can be reduced in some way to fulfill some relevant solution metric like "reducing required steps" (as in not maximally different structures)'
    - identifying the 'path from math to physical reality' will probably depend on identifying all of the 'maximally useful structure' (on the 'meaning' interface), as physical reality indicates the 'structures which are more computable/adjacent/efficient/stable/sustainable/measurable/independent', and useful structures (useful across problems and interfaces) are the most stable/efficient/adjacent structures
        - I imagine the 'maximally different structure connecting problem/difference types' is going to become necessary to connect math with physical reality, whether as a unit of reality/time or in adjacently connecting other structures or some other fundamental structure
        - a useful depiction of 'time' occurring on this 'maximally different problem/difference type connection structure' is changes to the queries run on that structure, queries which connect increasingly more variables of it, give it additional structures describing it like symmetries/rotations, or create new differences to integrate with the structure (if possible once the maximally different structure is identified)
    - 'solution automation workflows' can follow rules governing selection of structures like 'spaces' to position them in structures like 'sequences', so for instance by the time a query starting on the 'interface network' and moving to the 'math interface network' to a 'causal/neural network' gets to 'finding polynomials in euclidean space', it should be heavily filtered so that the extremely high ratio of 'possible functions allowed' relative to 'probable solution function variation' in that final space are mostly filtered by the point the workflow arrives there in the sequence (as in, a 'probable function range area' or 'probable primary function components' are identified by that point), or otherwise progresses from a space with more possibilities to fewer possibilities if no such filters are applied, so that the space itself can act like a filter on the possible solution functions
    - a function to 'find all different known variable interactions' and a function to 'generate & check new variable interactions (generating new possible interactions and finding a system that seems to be modeled by those new interactions) to fill in the gaps left by known interactions' can be a useful function set on its own as most variable interactions follow common patterns like 'interval interactions', 'cyclical interactions', etc
    	- as another example, functions to derive variable interactions based on insights (like insights interfaces/symmetries, which could be applied as a function to identify the rules of 'embedding variables' as that is a required variable interaction which can be used to frame all variable interactions (how many variables can be embedded in another interface variable, of what type and how can they interact without violating an interface variable theyre based on and depend on to exist and change, in what real systems))
    	- applying symmetries as a 'magnet for change' can help model a system's handling of events like 'interface overload', 'variation injections', 'definition violations', etc (when one symmetry cant handle a change type, meaning it violates the symmetry, what happens in the system having that symmetry, does that variable always obey another symmetry or form its own or decay)
    	- a set of 'known physics functions' is another useful function set, which can be used as a core function set to connect all variable interactions to (a function to connect all variable interactions to physics laws would be useful and independent of other function sets in solving problems, for example 'genetic variables' would be related to 'collision physics and cell pressure/charge rules')
    - identifying useful structures like 'mirrors' that act like metaphors to evoke other useful structures as a way of representing info structures like 'filters', such as how mirrors offer information without having access to all information (like if positioned at an angle), similar to how some info filters reveal info about an entire number type, allowing computations to be skipped, or allowing a program to 'look ahead'
        - these cross-interface structures can be maximized in differences like angles (similar to a staircase or helix shape across stacked interfaces), creating maximally different structures to use as a base for other algorithms requiring changes
        - relatedly, maximally different structures are useful for identifying extremely different ways to frame/format the same variables, such as how 'differences from randomness', 'interim points in between dichotomies like complexity/simplicity', 'adjacencies', 'simplicities', 'embeddings', 'densities', 'curvature', 'probable function area ranges', 'filters of equivalently accurate/possible functions', and 'linear/exponential change type filters' are very different structures which are equivalent alternates (or possibly complementary in providing different info when applied with other different formats) in their usefulness for representing a data set as a function, these structures being non-obvious/trivial to generate the full set of, and optimally useful once generated
    - identifying useful connections and other structures between useful structures
        - for example, the connection between 'filters' and 'interfaces' is useful bc filters are extremely useful when they 'preserve some variation of the input' and when they 'magnify (similarize/differentiate) some variable' to make some similarity/difference more obvious, as this is an implementation of the certainty/uncertainty interface that is so powerful in problem-solving
        - similarly, from this it is adjacently derivable that the concept of an 'interface' has an opposite in that the 'interface' applies a standard format to similarize some variable, to make differences obvious (to identify/differentiate structures), where an opposite of an interface would differentiate some variable to make similarities obvious (to equate/connect structures), which is a new connection between these structures that is useful and can be included in the definition of each interface so it can handle these alternate intents (like to 'make some structures similar to structures on some other interface' to fulfill intents like 'connect interfaces')
    - finding resolution functions for commonly useful connections/transforms, like connecting the 'densities to sparsities' or 'density patterns across densities' or 'densities to extremes' or the 'edge points to an edge line' or 'upper/lower/average edge lines' or the 'densities to regression lines' or the 'shapes (like graphs) formed by densities to regression lines' or the 'local subsets represented/connected and the subsets skipped/unconnected' and other useful connections in the data set regression problem space
        - other useful structure examples to apply as defaults given their higher probability for various reasons (adjacency to requirement, commonness, etc)
            - identifying non-useful structures to filter the set of useful structures
                - for example, identifying what slopes are unlikely to describe a function (making extremely distant points falsely adjacent and vice versa, to determine the slopes that are unlikely to describe the function change rates)
                - 'connections between a sparse subset of maximally different points (in their original positions)' is also a useful structure for intents like 'determining general maxima/minima of a function'
            - identifying points in between extreme errors/suboptimalities as the interim point that is more useful for more general intents while being generally sub-optimal as it doesnt specialize in optimization metrics
                - for example, identifying algorithms with variables that allow 'complementary' best-case input scenarios that are optimized by various algorithms to maximize coverage of input cases in the combined algorithm using these algorithms as components/alternates given some input range
                - for example, gradient descent is optimal in cases where local minima are good approximation of absolute minima and where the whole function is infeasible or inefficient to check
                - in cases where they are not good approximations of absolute minima, or where there are no minima, other algorithms would be better to find other sub-optimals that are less sub-optimal in those cases (like 'maximally different input-finding functions')
            - identifying compressed input info (like the 'output variables') which are more useful to identify than 'adjacent variables to input variables' for determining the outputs
                - the output info contains patterns and other interface structures which are useful independently of inputs in some cases, which can allow skipping connecting them to inputs if the outputs are compliant enough with patterns to be approximated by other functions like probability distributions (such as where the value is usually within some trivial difference from an average output, so this average can be used as a probable output, or where the outputs stay within a range and are relatively equally distributed in a range, which indicates a wave among other possible function shapes)
            - identifying useful structures like 'reasons why some interface structure is useful across multiple useful solution metrics, making it likelier to be robust to changes' such as how 'curvature' relates to 'self-interactivity' which is a core efficiency structure (that generates different change types with one input and one operation) as well as offering 'one cohesive unifying function of disparate sets of linear functions (such as local subset linear functions)'
                - similarly identifying other useful structures (like the 'e ratio' which resembles a core interface structure when formatted as a maximally different angle with a common base, as in a 'right angle', having a smaller change based on a larger change) as a highly explanatory indicator of symmetries which also relate to stability of systems (as systems that vary change types using minimal inputs are useful, small changes within some limit are useful, and are likely to reoccur as a result) and can act like a 'base solution to test first' in the absence of other base values to test as parameters (meaning 'apply e as a base parameter to check if the system has reached some local extreme of optimization/stability'), as the 'e ratio' is likely to sustain itself when applied repeatedly (a spiral), where other ratios are likely to intersect with themselves (circular ratios) which is useful for different intents and other ratios are likely to never intersect with or relate to themselves (leading to infinite change, which is not as commonly useful)
                - similarly other structures may be useful when applied as certain/constant/default inputs to a system, such as the 'exploitative ratio'
                - given that these specific math structures are highly connected to important/useful variables like stability/symmetry/balance, they are useful as specific math structures to start from/use as a base in some problems (like in stable/simple systems) where more complex systems that are not stable are likelier to benefit from differences from this parameter
                - finding the reason why a structure occurs is important bc different reasons create different types of change
                    - for example, if the reason why there is a negative correlation is bc some agent had an incentive to falsify the positive relationship, that would change in the next more accurate batch of data as being opposite to the correlation found, so knowing that reason is useful to predict how the data set might change
                    - similarly, if one agent is involved and if there is an incentive to falsify information, the information is likelier to be simple (follow a simple pattern) than it is to be complex, as simple-minded agents are likelier to need to fake some information
                - finding other constants (or other specific structures) of stable/optimal systems is similarly useful as these constants (or other specific structures)
        	
        	- identifying useful structures like 'changes which preserve info about the data set (such as symmetries in the data set) given the intent to find a representation, rather than an intent to remove info' to identify possible solution functions (composed of those changes) which might be useful for representing the data set (like finding average lines)
                - any function which changes the data set in some way (like removing outliers or removing sets of points that dont change the average) in a reductive manner (reducing computations/inputs/redundancies/noise), while also preserving a relevant solution metric (such as a type of 'average' like a 'local average' which is nearer to the final solution function, or 'probability density' or other 'moment' of a function) could be useful in finding the final prediction function, which is the ultimate average or other representation that is useful for minimizing prediction errors
        		- relatedly, 'mirrors' (or 'look-ahead tools to skip computation') depend on finding symmetries around which changes vacillate within a known range defined by the symmetry, so that once a symmetry is found, all changes around it are determined and dont have to be checked (just like a 'type' acts like a symmetry around which change develops, staying within the range defined by the type definition), which means symmetry structures like 'averages/extremes/inflection points/densities' are useful for determining info with minimal computation
        		- relatedly, finding the 'sets/sequences of symmetries which capture most relevant info about the data set' is useful for finding which sets of symmetries should be applied for which intents, symmetries being useful for predicting change as they indicate change bases & limits
        		- other symmetries (which are highly explanatory of changes in the data set) include higher powers, as the higher powers of a function are good at forming a base function to apply specific changes to in order to find the specific customization of that base which applies to a greater ratio of the data set
        		- finding the opposite of these symmetries (lower powers) is possible to do efficiently if done locally, to find change rates that could be adjacently produced with lower powers, but finding the higher powers is still required and these cant be used to skip a high degree of computations
        		- checking 'exponents of identified local change types' is useful to find possible simpler/alternate adjacent functions (once x^3 is found to be true locally, x^4 and x^5 should be checked as well in adjacent or maximally different subsets)
        		- as another example, the following equivalent alternate sets of insights make interface analysis trivial to identify:
        			- insight about variables of problem-solving
        				- 'there are solution automation workflows (like trial and error)'
        				- 'there are multiple workflows (there are others, like break a problem into sub-problems)'
        				- 'this means there are variables of solution automation workflows'
        				- 'these workflows interact with some objects like problems and have variables in common, like information requirements/interaction types/errors'
        		    - insight about multiple interfaces and automatability
	        		    - 'concepts have structure'
	        		    - 'information has structure'
	        		    - 'problems have structure'
	        		    - 'rules/functions have structure, and are therefore automatable'
	        		    - 'objects with structure are automatable'
	        		- insight about the interface concept itself
	        		    - 'problems are a matter of identifying similarities/differences'
	        		    - 'most problems are resolvable with standards that make comparison tasks trivial'
	        		- insight about the usefulness of each primary interface
	        		    - 'concepts can independently be used to solve a problem, without logic, information, or other primary interfaces, and without functions like "test" which would normally be involved in problem-solving'
	        		    - 'primary interfaces like concepts/logic/information are equivalent alternates in that they can be used to solve a problem independently of each other in best cases'
	        		- insight about the conceptual relevance of default/core structures
	        		    - 'direction corresponds to the intent interface'
	        		    - 'distance corresponds to the change interface and the similarity/difference interface'
	        		    - 'surrounding structures correspond to the system context interface'
	        		    - 'alternate possible structures correspond to the potential interface'
	        		    - 'angles correspond to the perspective interface'
	        		    - 'chainable functions correspond to the logic and function interface'
	        		- insight about how cross-interface structures are more useful
	        		    - 'concepts are only clear when you have an example (such as the variant of the concept in physical reality or in a particular problem) if you dont know the whole definition'
	        		    - 'concept-structure structures are more powerful than either on their own'
	        		- insight about how some variables are more powerful at more useful tasks (like explaining/describing/generating/determining) than others
	        		    - 'there are variables that are more powerful than others, like the general variable of cause, which is highly explanatory'
	        		    - 'there are equivalent alternates that are useful to know (explaining/describing/generating/determining intents, or find/build/derive intents)'
	        		    - 'cause is also good at these equivalent alternate intents'
	        		    - 'cause has an equivalent alternate in that logic can replace its value in these intents'
	        		- insight about how some structures are more generally useful across problems
	        		    - 'structures like rotations, similarities, and limits keep re-occurring across problem-solving methods'
	        		    - 'these structures are different types of objects like math objects, standard objects, and system objects'
	        		- insight about finding the 'most reduced set of useful/important structures'
	        		    - 'if you try to reduce language to the most useful structures, youll find structures like inconsistencies, perspectives, requirements, implications, overlaps, etc'
	        		    - 'the most variation-capturing variables of these useful structures are the bases where the others can exist, like differences/errors, structures, or functions'
	        		- insight about multiple perspectives and simlarities/differences being related to formats (which are like interfaces)
	        		    - 'different formats make different intents trivial'
	        		    - 'everything can be differentiated by changing perspective'
	        		    - 'a perspective is like a filter/standard/format'
	        		    - 'everything is similar and different to everything else in some way to some degree'
	        		    - 'variables/functions are related to interfaces as they change within a defined limit'
	        		    - 'differences in similarities and similarities in differences make problems trivial (similar to the comparison insight above)'
	        		    - 'standardizing to the same format makes some problems trivial to solve, as it highlights meaningful differences'
	        		    - 'some standards/formats (logic/concepts) are useful structures (interfaces)'
	        		- insight about useful graph structures
	        		    - 'some graph structures are more powerful than others, such as networks, maps, trees, sequences, etc'
	        		    - 'the differences between these useful structures involve objects (variables/functions) in the graphs, connection functions (like similarity or interaction function), and the structure variables like direction (in causal networks, for example)'
	        		    - 'a graph of graphs (like the interface interface, or the meaning interface) is a useful structure to organize these variables'
	        		- insight about different problem formats
	        		    - 'different problem formats exist, such as filtering problems and sorting problems and building problems and simplification problems'
	        		    - 'different optimal solutions and solution-finding methods are trivially derivable given the format of the problem'
	        		- insight about equivalent alternates
	        		    - 'there are some structures which are equivalently useful, such as common high-variation functions, common formats, common errors, common causal variable structures, common problem-solving sequences/workflows, etc'
	        		    - 'there are some problem-solving structures which are equivalently useful, like alternate function sets or alternate useful structures like interaction levels or problem metadata like problem formats'
	        		- insight about common problem-solving differences/functions
	        		    - 'different problem-solving functions exist which are common across problems, such as "find a solution" and "derive a solution" and "build a solution" and "change an existing solution"'
	        		    - 'these functions have structure, and there are other common functions to problem-solving processes, such as common problem-solving intents'
	        		    - 'other function sets common across problem-solving processes exist, such as core interaction functions of problem-solving processes and cross-interface functions and connection functions of problems/solutions and formatting/standardizing functions'
                    - insight about how primary concepts are powerful 
                        - 'simplicity/complexity' corresponds to a primary 'difference-resolution or connection' function, so it corresponds to a primary 'problem-solving' function, as problems' default format is a 'difference to resolve', bc a problem can be solved by making it simpler or finding functions that in general simplify other variable interaction functions
                        - other attributes which can describe any structure also correspond to primary difference-resolution and problem-solving functions (like 'work', 'intent', 'potential', 'change', etc)
                        - for example, 'changing an existing solution to a similar problem' is a default problem-solving and difference-resolution function
                        - what other primary abstract concepts can be used to resolve differences (solve problems)?
                            - 'balance' can be used as a primary problem-solving function in the form of 'balancing extremes, as extremes often lead to errors like over-prioritization errors or scaled errors', which translates to a problem-solving function like 'balance the maximum differences in a problem to find a more general/stable format of the inputs which is likelier to be correct (a solution)' or 'find functions that are adjacent/interim to a high ratio of information'
                            - 'power' can be used as a primary problem-solving function in the form of 'finding functions that can reduce the work of all other functions, making these functions more powerful' or 'find functions that store high ratios of information'
                            - 'potential' can be used as a primary problem-solving function in the form of 'finding functions that can interact with the highest ratio of other functions, making these functions higher potential in the higher variability of their interactions' or 'find functions that have a high ratio of usage functions (can find/build/derive a high ratio of information)'
                            - 'intent' can be used as a primary problem-solving function in the form of 'finding functions that determine what other functions are used for (intents) and what theyre useful for (adjacently/optimally fulfilled intents)' or 'find functions that are more useful for a high ratio of information-related intents (like information storage)'
                            - 'cause' can be used as a primary problem-solving function in the form of 'finding functions that identify input/output (causal) sequences of connected variables (such as why a function is useful, as in the reasons the optimally fulfilled function intents develop to be common, such as the efficiency of using the functions and the commonness of the requirement of their outputs)' or 'find functions that cause a high ratio of information'
                            - 'certainty' can be used as a primary problem-solving function in the form of 'finding functions that change certainties slightly, as these slight changes are likely to solve new problems unsolved by existing/known certainties' or 'find functions that require/determine/generate/describe a high ratio of certain information'
                            - 'abstraction' can be used as a primary problem-solving function in the form of 'finding functions that can store/embed most variables adjacently' or 'find functions that can support a high ratio of information'
                            - 'perspective' (as a filter formatted as a set of priorities, or structurally as an angle that makes a subset of information obvious/certain) can be used as a primary problem-solving function in the form of 'finding functions that can change angles/filters to make connecting/differentiating any variables trivial' or 'find functions that can identify perspectives that are useful for a task like embedding information or identify perspectives that can make all information trivially stored'
                        - given how these concepts interact with information (can store a high ratio of information or otherwise are useful for information-related intents), the structure of an interface emerges
                        - the primary abstract concepts (power, balance, complexity, stability) offer good candidates for interfaces bc of the fact that they are high information/variation-storing and every structure has these attributes in some way, so they are a way to access different fields of connections (differences/similarities) to other structures

                - most of these have common variables, such as including 'known common useful structures', applying common useful intents like 'reduce/connect/compare', identifying structures that are relevant to 'problems', applying interface structures as a useful compression/explanation/other intent fulfillment structure, etc
                    - they differ in their method structure variables, such as how 'identifying the type of object that a useful structure is' differs from other insight sets in that it applies a bottom-up direction (generalize from specific examples) from a 'specific' starting point

            - identifying interface structures like 'efficiencies' that align with regression problem space structures such as 'symmetries' or 'densities' by relevant structures like 'causes' ('this point is a hub' meaning 'a lot of inputs created this output bc it was particularly efficient for something') and identifying functions to connect those interface structures with causes that can be mapped to other variable sets (other efficiencies can be generated and checked for, once the reason of 'efficiency' is known as a cause of some 'variable interaction structure'), meaning other relevant useful 'low-cost, high-reward uses of inputs' for known intents can be hypothesized and checked for in the data set to determine the prediction function, as well as 'other causes of efficiency such as energy limits, which could cause other structures related to energy like power imbalances, power concentrations/compoundings, power takeovers, power dispersions, power vacillations, etc'
                - this is useful bc structures like 'efficiencies' are related to 'symmetries' and other specific useful structures in the regression problem space like 'densities' in their usefulness for predicting/explaining/limiting change
            - identifying useful structures like 'questions to answer (like "where does change change") to solve different sub-problems of the problem (like "how to divide the data set into subsets which are likely to contain different change types/rates/degrees/etc")' which make the rest of the problem trivial (makes solving the other sub-problems like "find the most different subsets (which could possibly contradict a function for another subset)" trivial) and identify the sub-problems they make trivial, and filter these by which subsets of sub-problems can replace other subsets or the whole set for some metric like 'finding an approximation of the solution'
            - identifying useful structures like 'adjacencies' as a way to determine if alternate variants of the data set are more obviously compliant with a pattern, as 'adjacent transforms' of a data set are likelier to be valid than other transforms, and some of the adjacent transforms may have more obvious patterns/averages/densities/other useful structures than other types of transforms
                - similarly, identifying useful structures like 'repetitions' to apply to relevant structures like 'subsets' (specific sets) to generate structures (like 'repeated change types across different local subsets') which are more relevant to a prediction function (the complete general set), or 'differences' to relevant error structures like 'non-local subset change types' (which are likelier to be less relevant to the prediction function, as in inaccurate, than local subset change types and therefore applying changes to these errors structures is useful to generate the actual prediction function)
                - relatedly, a 'mix of simple/complex transforms' applied to a base function to balance the extremes of the dichotomies that are useful/determining of the problem space and allow resolution of them by testing small differences favoring either priority may be useful as a testing/diversification structure to hedge bets in the prediction function, as well as embedding other variables reflecting other dichotomies (certainty/uncertainty, specific/general, discrete/continuous, adjacent/extreme), which is similar to retaining multiple different equivalent alternate functions which are ambiguously correct as they fulfill solution metrics similarly
                    - organizing these differences by 'which are likely to be adjacent' & related metrics is a useful way to filter out functions that are exactly obviously incorrect (by placing a constant in exactly the maximally wrong subset, for example)
                - similarly, if there is a subset of input variables that can be used to create the exact opposite (or similarly different variants) of the found prediction function indicating that the variables can be easily made to contradict each other, the symmetry uniting them is likelier to be more true than either the found or the opposite function
            - identifying useful structures like thresholds to filter possible alternate interface variables of a function, like determining the function up to a local subset size that its possible to determine its change types, like whether its exponential or linear, as the meaning of a subset emerges and is more obvious, the greater the number of points in the subset (subset ratio), where the meaning of a point is difficult to determine in isolation of other points
            - identifying the primary interaction functions between subset structures, such as how a subset might represent a fraction such as a quarter of a data set shape, so it should be reflected across two symmetries to create the rest of the data set, or a subset might be an 'orbit' or 'boundary' of the rest of the data set, so checking if these interaction functions apply to the rest of the data set is useful and identifying these interaction functions is useful
        	- identifying useful structures to filter out possible errors like 'randomness in the data set' by checking for 'associated structures of randomness (when defined in that problem space)' like plus/x shapes of lines with equivalent/similar numbers of intersecting points with the data set, or ambiguity shapes like squares (which could represent any line with positive/negative or constant/zero slope with equal probability) or similar areas reflecting randomness when found to describe the whole/most of the data set, or look for indicators of usefully biased shapes like rectangles/circles which dont represent completely equal/ambiguous change
        	    - similarly, identifying the set of 'lines with equivalent/similar intersecting points, optionally using lines as connections between adjacent points in the set (to add simplicity as a proxy of generality)' as indicators of equally probable solution functions or 'lines which skip the fewest points to form a straight or otherwise simple line' are useful structures to use as generators of possible solution functions, similar to other generative strategies like applying variable changes like 'embeddings on other variables' to generate possible maximally different functions, just like how 'any line crossing the data set' is similarly useful as a 'probable line-of-best-fit' to check errors and determine shape/slope of the correct function by assembling 'adjacent errors (to form shapes like lines/curves/boundaries)' and 'error changes like sign changes or phase shifts' into the solution function shape
        	    - randomness is particularly important to identify bc structures that exist are unlikely to be random (equivalent alternates that seem ambiguously similar usually resolve into favoring one or merging or differentiate further and become obviously different in equivalently useful ways, if the host system continues to exist bc multiple equivalent alternates are redundant, high-cost to maintain, and therefore less likely to occur & continue to exist), so determining inputs/components of non-random structures (such as biases, or simple rules) produces a set of 'probable structures to exist'
            - identifying useful dichotomies like 'parabola or line' is particularly useful as the core question to answer in the regression problem space, to generate algorithms such as 'connect the outputs at the lowest/highest x-values, and check midpoints in between the lowest/highest x-values to check for an error indicating a parabola', as its more important to identify when there is a parabola/wave vs. a straight line than to identify any other structure except more core unit structures like averages/extremes/inflection points and errors like 'gaps in data' and other structures resulting from core structures like extremes (such as limits/infinities)
               - the inputs to this algorithm are the x-range extremes (lowest/highest x-values) as well as the midpoints in between which would be useful to check for parabola-type errors at (the selection function of subsets)
               - identifying trends in error structures (like if the error is always 1, the function should probably be shifted up/down by 1, or the program is only checking values at a wave peak/valley where the magnitude is 1 while identifying the horizontal line crossing the wave at its midpont) is useful for identifying corrections to try early on, and identifying how to correct them such as by 'evaluating values at different intervals rather than at the same interval' and 'identifying the simplest line that intersects with the data set the most' to avoid this error, are similarly useful to reduce the probability of repeating that error
               - as another example, if you have a horizontal s-curve (one up and one down peak), identifying that is as simple as identifying that three other pieces of information are required, and identifying those three of the y-values at regular x-intervals (or the two/three determining points of the peaks/inflections), at which point the s-curve will be obvious if curvature is used to connect these points and your interval function identified the extremes of the peaks, if the program specifically checks for that type of curve as a common curve type (the skew/concavity or the squareness vs. linearity of the curves is another useful variable to determine as an important specifiying variable, the squareness indicating less likely change types and the curvature/linearity indicating more probable change types), so an algorithm to identify the variable set of 'curve peaks, x-ranges, and squareness/skewness/volatility at some subset of the function' is likely to produce a generally accurate function, on its own regardless of other variable sets, which can be enhanced by other known variables capable of producing extreme errors such as 'threshold phase change' variables which can make a function act like a totally different function in some continuous subset
            - identifying useful sequences of interface structures like a 'maximal difference connecting line indicating the connection between the most different points' and a 'inflection-point intersecting line indicating a smaller direction of change' which can provide a simple set of linear functions to base changes on to find the regression line quicker, which in a curve with exponent 3 (which starts lower, has an upward parabola, then a downward parabola, then increase indefinitely) would indicate the primary deviation from the primary summary of the change patterns (a line with positive slope connecting the low initial points and the high later points as the primary summarizing line, specified by an interim line with negative slope crossing the inflection point to indicate the primary deviation from that primary summarizing line, which can be further specified by tangents indicating extremes or alternately limits of vertical change)
            - identifying useful sequences of filters to reduce regression solution sets by useful trivially calculatable structures like change type variables, such as a sequence like 'check for a parabola, which implies an exponent, then given that an exponent implies other powers could exist like even/odd powers which determine maximal differences in the resulting function, check if powers are even/odd, then proceed to other variables to determine the rest of the function' which starts with an easily found structure and proceeds to other structures made possible/probable by that structure
                - this is related to other workflows like 'check for a change type, then infer other change types based on that, given change type interactions' but involves finding alternate sequences which are maximally differentiating/filtering or otherwise useful
                - these sequences' usefulness is maximized in cases like 'when the first item in the sequence is an interface variable that can support embeddings of other variables (like a unit exponent variable or other variable structure to differentiate function types) and the following variables are embedded in that interface, so the filters begin with the maximal differentiating filter and then decompose the remaining variation on that interface'
        	- identifying optimal algorithms involves finding useful (such as 'measurable') variables with 'obvious errors/optimals', such as how an algorithm in between simplicity/complexity is likelier to be optimal than an algorithm that is at either extreme (this is useful for filtering all possible algorithms to reduce the search space)
        	    - similarly, algorithms that involve some common useful interface structures (such as symmetries, which are like information wormholes, as well as limits and maximal differences) are likelier to be useful (and true and relevant) than other algorithms
        	- identifying variable interaction functions that could not be true given some system of reality where those functions could exist that is not true, like identifying that a 'wishing reality system' is not an accurate model of reality by identifying that 'agents wish for problems to be easy to solve (they wish for types of freedom)' and 'problems agents have are not usually easy to solve' and 'some common wishes of agents contradict other wishes of their own and of other agents, and would also contradict a wishing reality system for other agents' and therefore a 'wishing' variable interaction to connect problems/solutions is not a valid problem-solving function applied absolutely, which is useful to rule out variable interaction functions to find variable interaction functions that are possible/legitimate as a way of finding a reality system model, though finding states that move in the direction of that system is possible using realistic rules (increasing agent intelligence makes problems easier to solve and reduces agents' contradictory wishes against other agents, which is a solution of removing the intents that make the 'wishing reality system' impossible to logically sustain)
            - similarly, identifying high-variation explanatory variables like 'incentives' and 'interaction levels' from a typical data set where structures like 'defaults' and 'input/output similarities, differences, and requirements enabling interactions' (as more probable than other variable values) and 'adjacencies' (in casual degree) and 'efficiencies' (in benefit/cost ratio and stability) and 'interactivities' (as default interactions between variables) and 'types' (as efficient captors of information) appear more common than others, 'incentives' being a common factor in these common variables, as these structures are incentivized compared to other structures that may be more complex/difficult in some way, so simple queries like 'find common high-variation variables in common high-variation variables to identify other common high-variation variables' are useful in typical problems like regression, as if a common variable seems high cost its likely that we just havent identified the incentive yet, as the incentive is the determining variable more often than not, just like randomness is not usually real equivalence of probability in outcomes, but is likelier to just be lack of information about variable interactions that makes a variable interaction falsely seem random
                - for example, the dynamic between 'filters (as measurement/input-selection tools), as a core structure of differentiating functions' and 'incentives to distort the definition/structure of the filter to exploitatively avoid/subvert it or otherwise exploit it' is a highly explanatory interaction, where once a filter is applied, the incentive is to apply differences to game the filter (either to become a false/illegitimate input, and/or to expand its input range, or to use the functionality guarded by it without having the filter applied to it), and once the filter function has changed enough to handle these predictable adjacent incentivized differences, the opportunity to exploit differences to the filter is closed and differences are applied elsewhere, where these 'differences applied to the filter to handle adjacent differences/errors used to exploit it' is a 'common useful change sequence' to be able to re-use across filters, as the filter structure usually is applied too simplistically initially and must usually be changed to handle obvious exploits, 'filters' being a variant of 'standards' and are therefore useful in core intents like 'finding bases/limits of adjacent similarities/maximal differences'
                    - usually the filter follows patterns of errors, such as being too simple, too specific, too rigid, too structural, etc - so that it benefits from common useful optimizations like 'generalization' (as well as fulfilling useful 'core optimization intents' like 'increasing interactivity with other structures that dont adjacently cause errors', etc), an 'optimization change sequence' that can be matched to an initial filter by its probable error metadata (simplicity, specificity, rigidity, structurality, etc)
                - this can be applied to algorithms in general, such as for example, 'abstracting an input filter' in the 'find a regression function' problem space, to "find points belonging to the same type and connecting those points in a 'type function' to differentiate them from other points" or 'find maximally different points that should be connected in the same function, as they are legitimate and/or representative points'
                - these common distortions of a filter definition can be anticipated in advance so the solutions to these exploits are built-in to the filter definition, rather than applying the 'optimization change sequence' after errors are exploited
                    - a useful question to identify these change sequences: 'what is normally found implemented as a filter in typical systems?' for example, a 'domain/topic/type sub-type filter' such as a 'specific sub-type of object within a type with different rules that should be applied to it', so that a 'filter to find items of this type' is applied, often with a hard-coded function/dict to identify those items, and given the value of differences created by those different rules, other items will try to seem like those items, in predictable adjacent incentivized ways
                    - what other useful functions are commonly implemented? 'functions to correct distortions/errors beyond some threshold used as a filter of acceptable differences', 'functions to batch or aggregate items having some attribute', 'functions to connect some different objects using maps/functions/filters', etc - all of which can be implemented using some filter function, which is why this structure maps to a core interaction function 'find'
                    - a useful application of this would be to 'find new differences to apply filters of in a system, where these filters would be more useful to differentiate some structures and arent already used in a system, to optimize the system'
                - as some algorithms/queries are applied, other interface structure become obviously useful, such as 'similarities which are not adjacent' and 'changes that are neither obviously adjacent similarities or maximal differences' or 'interim differences between maximal differences' (useful as variants of maximal differences to test more different possibilities that are more similar to some interim base) or 'similarities between maximal differences', as 'adjacent similarities' and 'maximal differences' are obviously useful, so connecting these structures using related structures (related by the same base structure of 'similarities/differences') and finding useful variants of these structures and useful intents these structures are likely to adjacently fulfill (such as 'more complex changes which are less obvious and likelier to describe complex systems as well as the differences between various standards/similarities/symmetries that describe a high ratio of variable interactions') and standards/symmetries within these structures ('similarities between maximal differences' to identify input/generator/descriptor/limit variables of these structures) is useful while applying differences to create new structures (which should be abstracted/fit into other interfaces to check for new types of useful structures)
            - identifying the structures like 'shapes that when overlapped and rotated and viewed from a distance look like more common shapes like circles' as useful for describing reality in that they cover and explain more variation than other structures (this structure has a structure embedded in it that indicates 'equivalent alternates (having an equivalence in their center & a similarity in their rotation)', the 'incompleteness of any one alternate in this set when viewed in isolation', the 'usefulness of general trends once a process is repeated/scaled', the 'balance inherent to multiple equivalent alternate perspectives', the 'ever-changing nature of foundational structures leading to changes in interaction levels', and other fundamental structures that are descriptive if not generative of reality)
                - relatedly, viewing reality as a 'set of repeating processes applied to partial overlapping/embedded structures at varying intervals/magnitudes/scopes' may be more useful than some 'networks with unique nodes or just functions as nodes/queries or just individual usages as nodes/queries' bc the repeatability models useful interactions like 'aggregations at scale', 'net/emergent effects', 'in/stabilities', and other important structures created by the interactions of real systems, so this model of reality may simulate a more useful interaction level than abstractions like function networks tend to capture
                    - similarly, 'similarities (like patterns) in "differences from incentives"' and 'similarities in entropy/uncertainty/potential reduction structures' and 'structures of differences (such as attributes like unfulfilled/impossible/required intents) between differences/variables and difference-connection/solution structures' are related useful structures that captures high variation and formats differences in a minimally complex structure
                    - a network/field of these sets of useful structures that describe high variation in reality is useful as an alternative to a set of useful interface structures which can generate these adjacently
                - relatedly, viewing reality as a set of 'differences from required limits (impossibilities)' or alternately 'commonalities (similarities to probabilities)' (produced by adjacent/input/causative structures of commonalities like scale/aggregation/repetition/isolation/efficiency/incentives/investments/adjacencies/examples/usages as ways to produce commonalities) and differences from commonalities (like those that occur when "previously isolated commonalities interact after repeating enough to reach each other's position") can be more useful in its specificity as different from other useful formats like just any high ratio of all combinations of 'similarities/differences', this specificity being useful in the certainty it provides to base changes on
            - identifying the structures like combinations/ratios/networks/interaction levels of optimals, stabilities, requirements, errors, and other interface structures that occur in reality is important for solving the 'find a regression function' problem, as there will inevitably be something that a real system 'optimizes for incompletely', something that a real system 'should do but doesnt given this priority/requirement/input/opportunity', something that 'stabilized after a particular error type', and other combinations of interface structures like 'conditions' and 'causes' and 'optimals' (such as rules like 'apply high variation or high potential-variation variables first to decompose high variation'), which can explain most variable interactions but are not default/core structures already identified/required (these structures are more complex variants resulting from more interactions), which are obviously useful despite their complexity, as more useful to retain and use as defaults that re-generating them from core components every time, similar to how solution automation workflows can be more useful to retain than re-generate
                - these are useful for determining when a data set (or the system it reflects or the parameters of the analysis process like definitions) is incomplete/incorrect or otherwise suboptimal/erroneous in some way
                - relatedly, structures like 'monodromies' can be useful to apply existing math structures to indicate which points/lines/other structures that could represent 'limits which should not or need not be crossed' (which differences should not be resolved, such as where an ai program might 'resolve a difference' of a medical problem which is a 'state of difference from health' by allowing patients to die to create a 'difference from the requirement for health/health resolution to invalidate the problem' which is the difference to avoid except in extreme circumstances like where a 'health generator is adjacent, invaliding the requirement for health' and otherwise resolving unnecessary differences like 'resolving a difference that is already a known constant' or 'resolving a simple difference that is obvious' and 'resolving a difference that leads to an extreme such as hyperbolic change which is unhandled')
                - relatedly, retaining structures that are more useful to make constant (such as where its more useful to retain a map of inputs/outputs than to find a connecting function, like where extremely high variation is observed/possible and not reducible or where reduction is contradictory to other intents) and use as input defaults than to re-generate is a way of identifying the stable structures of reality that should not usually be changed/tested but rather applied as default inputs, combined in increasingly complex ways to describe more variation, rather than replaced with more descriptive variables, as these defaults are already the best at description
        	- identifying patterns/limits/useful representations (like areas)/other interface structures of useful 'false similarities' of functions (at some point, subset, in relation to some threshold, or range), such as when '(x^2) + 1' depicted as an area will look like a rectangle (when x is near 1) and when it will look like a square (as x approaches infinity), and identifying the point where these 'false similarities of areas' change (when x is sufficiently large that 1 looks trivial by comparison), and whether other changes are possible/defined/likely (whether those 'trivializing' changes will apply again at some point in that direction) or whether the pattern will continue, to identify 'different change types possible with a function', which is useful for determining the remaining sets of a function given a determined set of input/output relationships and in filtering functions that are equivalent or seem equivalent in some subsets but are not
        	    - similarly, finding 'standardizing' functions like to remove 'obviously non-impactful variables' such as the 'shift to move intercept value at y-axis to y = 0' that make the change type patterns of x^2 more obvious (the inevitability of the 'increase in area created by x' being exponential being more obvious once the 1 constant additive is removed as the impact of adding 1 at higher x values is more trivial which obscures the otherwise obviously exponential area increase, and the addition of 1 being increasingly trivial in changing this exponential increase in an invalidating direction)
        	    - similarly, other 'variable combinations with obvious/predictable impacts' exists like how a 'sum of constantly increasing and constantly decreasing variables' could easily be a horizontal line depending on their ratio
        	    - depicting these 'possible variable interactions' in a way that their commonalities are depictable as common points/overlaps or other obviously similar structures (such as combinations of inputs having an attribute in common crossing the same line/point or having the same shape or area) is useful for filtering probable interactions that are relevant to some other interaction (like an input/output interaction)
        	- similarly, finding a subset of points that represents an average/midpoint of some other set of points can replace the calculations required to connect the set of endpoints and instead just using the midpoint is acceptable as an approximation algorithm of the others, if enough different points are preserved to retain the general shape of the data set, just like 'removing extremely similar points' as 'redundancies' (but not redundant in all cases, as in the case of points around a density center, which preserve the weight of the density given its surrounding adjacent points) and removing 'removing non-adjacent (but non-maximally different) points connectible with local constant functions' as 'improbabilities' is another way to reduce the set of points required to connect in the same function
        	- finding useful formats like the 'set of angles/areas creating the components/features/products created by an input' which are summed to generate the output and which can be easily checked as un/applicable to other inputs by changing the angle of perspective in viewing these angles/areas so the impact of component areas/coefficients is obvious (viewing the interim products of operations in between x and y as a stacked set of areas, angles referring to the differences between area upper/lower limits compared to input x), rather than repeating the multiplication/sum operations on the other input, which is related to the format of a 'network of areas (representing products of variable pairs/sets) that is separated and aligned to make the output (sum) more obvious to avoid re-computing it', these formats being useful for approximating/prediction/avoiding computations by identifying similarities in component metadata like ratios of areas, and identifying obviously wrong 'product component sums', similar to filters like types that determine 'information about what else is also true' from a 'set of input facts'
        	    - this format is maximally useful when the interim components are few & large or otherwise obviously different, 'either very large or small' and 'very few' (meaning more adjacent to the final output)
        	    - once probable 'large components' (or otherwise simplified components) are identified as probable, filtering the sets of possible smaller input components of these components to resolve the ambiguity of 'which smaller inputs created these larger outputs' is a reduced problem compared to 'filter all possible solution functions' and possibly also 'apply incremental changes in a direction that reduces error' in some cases, which can be repeated up to the point where additional component-resolution is trivial (differences added by further component identification are trivial, or equivalent to other differences created by some other component set or simple layer like some function of randomness)
            - similarly, the 'equivalences in differences' from a particular 'possible solution function' are useful to identify 'probably useful summary functions', as functions that produce 'equivalent differences' are likelier to be more adjacent to the actual solution function (transformable to the solution function by some trivial transform to produce these equivalent differences, like a shift or scale change)
                - relatedly, finding these useful standardization functions that produce an 'equivalence in differences' from one function to another is useful to find these functions which have equivalent differences
                - relatedly, the '(patterns and other interface structures of) differences in functions that have equivalences at various subsets' are useful to identify, as having a common interface defined by their equivalent subsets which vary adjacently by some variables that generate the different functions united by those subsets, which make the problem of 'selecting between equivalent alternates such as ambiguous solution functions' more trivial
        	- identifying the 'maximally different subsets' that will stay under an error threshold for the same given general solution function is useful to solve for and apply once the 'maximally different subsets' of a data set are identified to filter the possible solution functions by taking a subset
        		- similarly finding the probability of a particular solution function by comparing the ratios of coverage of these 'different subsets, which are equivalent in their error range of a general solution function' (which function corresponds to a higher proportion of maximally different subsets under a minimum error range, this proportion representing an approximation of the probability of that function being the solution function)
        	- checking for set of 'points and directions' within a horizontal slice of a probable range (where the densities are, indicating where the solution function probably is within that range) is more trivial than checking for inputs corresponding to any outputs, allowing a method to skip points corresponding to outputs outside of that slice, as a subset of 'points with adjacent directions of change' is an alternate format of the solution function that can be used to determine the rest of the function, as finding the actual points in a horizontal slice of the data set and adjacent directions of change moving away from those points captures the value of a subset of determining points (in a high-variation slice of a probable range area), as an alternative to the usual determining points like extremes/inflections/averages, which are further determining of the rest of the function when paired with adjacent change directions, if the slice is in a high-variation section so that most variation of the function is captured in that slice (the same can be said for any horizontal slice of the data set but moving it to within the probable area range of the solution function and specifically to a high-variation subset makes it more useful, and pairing points with adjacent change directions increases their determinability of the rest of the solution function), so 'finding the most useful position of the slice' is the problem to solve, in addition to 'finding the point/adjacent direction pairs that determine a function, instead of the usual determining points/metadata'
            - identifying useful structures like different formats to handle cases where it's not possible to select between alternate solution functions, such as where a range of probable area is too large and functions within that range are equally or similarly possible, in which case every possible function in that range is more useful to format as a node on a function network, as every connection can be distorted to be every other connection type by applying some variable or error, these variables or errors being possible to apply generally across systems, so any data set could be the product of an error applied to some correct function and could therefore require some distortion to correct, so a set of weights indicating which function is likelier in a function network is more useful than a single function, or alternately formatted as a function generating a set of 'certainties' like angles (indicating a probably correct point, after which there is a divergence creating a probable area), and 'randomnesses' acting like 'ambiguities' where many possible functions exist
                - this is related to other methods involving handling alternate functions but applies the insight that 'every possible connection function between points could be valid and could be altered by excluded/hidden variables its likely to encounter until it becomes every other function, and having a function network where identifying the node where a function is gives useful information about adjacent/probable alternate functions, which is useful for identifying the probability of the function becoming other functions and the probability its an incorrect function, given some known correct input variables, this function network organized by probability of becoming adjacent functions'
            - identifying patterns of error feedback indicating obvious solutions to correct the errors is useful, such as where the errors of a function that is incorrect bc its shifted to another position follow obvious error patterns that can be easily identified and corrected
            - identifying points that, when some simple connection function is applied, can generate the most other points is useful to identify as simpler connections are likelier to be true (if the whole data set can be reduced to a set of center points and circular border points around these centers, these patterns are useful to identify as being significant in their repetition, structure, simplicity, and difference from simpler/core functions like lines, even if the effect of these overlapping circles seems like randomness or a linear function in some subset)
        	- identifying useful components to describe data set subsets like adjacent point sets such as 'filters/bottlenecks/convergences (connect only different surrounding directions) and random/circles (connect points in all surrounding directions) and extremes/limits/peaks (connect similar but opposite surrounding structures) as default structures of adjacent point connections to use as components'
        	- 'filtering structures similar to outputs to find the outputs' is a useful function such as how its useful to know a set of the most unique (maximally different) functions to filter maximal differences, and its useful to know the 'most similar function of the set of similar functions' to filter similar functions bc the representative/average of these functions is more similar to each item in the set and is likely to be relevant to other items in the set
        	- identifying new densities from a known density is often trivial bc identifying the midpoint (or other representation like density) of a one-dimensional set of points in some direction to discover densities in (from the perspective of the known density looking in that direction) is trivial compared to the task of graphing all points and finding all densities, where finding 'trivial to identify' densities is more efficient and can benefit from some transforms like angle of evaluation changing the problem space of 'filtering all points past this x-value' into a one-dimensional set (comprised of a subset of points in that direction) to identify a 'direction to move in', finding local averages in many subsets, densities being likelier to be less common and therefore more trivial to identify than the local subsets used to calculate local averages to select/create points to connect, and structures like 'equally distributed points in some direction which would make the task of identifying a density or average trivial' is less likely to occur in a realistic data set and calculations can be skipped in those cases
        	- identifying what point interactions (like high volatility, extremes in ranges, high slopes) can indicate possible sources of high variation, which are the most important points to identify, to identify whether an average line (or base solution regression line) needs to be adjusted, so that identifying these 'high variation-causing points' can be an approximation or alternate of a regression line-finding method
        	    - similarly, identifying which regression lines correspond to 'what types/sets/ratio/other metadata of points indicate the points that must be ignored in order to make the regression lines seem accurate', like how 'change rate changes' and 'difference from base function slope sign changes (positive/negative difference from base function slope)' have to be ignored to make a constant line seem accurate, etc, so that checking for these types of points once a base solution function is found which might contradict it is trivial
            - identify what variable interactions look like with various error type structures like combinations and filters of these possibilities to apply tests to input variables or gather more input info (expanding the workflow loop to include changes to test/data gathering variables)
            - identify at what point inferring that a 'more complex function with more peaks exists after how many negative indications indicating the opposite' is the wrong inference and can be contradicted 
            - identify filters to select/switch between function formats like 'probable function range areas' (useful when an area seems more random) or 'average functions with surrounding error vectors' (useful when an error or legitimate function range seems too ambiguous to resolve with current info) based on different complexity structures and cases in data sets
            - applying variables to create randomness from a non-random data set (or similarly linearity from a non-linear data set) and identifying whether some randomness/linearity-resolution functions also use those connections required to create it (if some system dynamic like 'system collisions or overlaps' is a randomness-resolution function and a collision/overlap variable created randomness, do the systems explain the original data set before the collision/overlap)
            - identifying error/legitimacy resolution structures (at what point does an outlier seem like a trend-change predicter rather than an error)
            - identifying gaps in uncertainty/certainty, randomness/linearity, difference/similarity, complexity/simplicity, and error/legitimacy spectrum variables that cant decompose some change type adjacently and the spectrums which can resolve them using some resolution structure (or the other structures using these spectrums which havent been identified yet)
            - identifying the most useful tests to apply (like 'a method that can identify new inventions as well as non-adjacent high-variation variables or sources of randomness in systems like "high-distance high-randomness variables like neutrinos" and "powerful/change-triggering variables like incentives as explaining most variable interactions"' as well as reversing the function to 'find system structures created with incentives/randomness' or 'a method that adjacently identifies useful interface structures from highly different as in non-interface structures') to check if a solution-finding method is successful at which point different solution-finding methods can be generated combinatorially and tested iteratively
            - identifying similarity structures between subsets of the data set (similarities such as 'similarity in intersection with some line type having some attribute', "similarity in a point's difference types from neighbors") which explain the data set the best, as points which are similarizable by these attributes are likelier to be related in that way in the system producing the data set, these similarities serving as inputs to probability of relatedness and therefore can be usable as a filter of points to incorporate in some algorithm to determine a representative line, compared to some difference set to generate a composite ratio of relevant similarity types compared to difference types
            - identifying inevitabilities of structures and the related structures like points/ratios/distances required to identify them (such as two slopes on either side of a peak making the peak inevitable, given the input variable interactions implied by the slopes and given the distance between them and the existence of other peaks already identified)
                - identifying evocative structures (which are similar enough to other structures to be useful in deriving them)
            - identifying relevant truths like how the 'extremes/borders/upper/lower limits of the probable area range' might be a better representative format of the function than a line bc different behavior at a higher vs. a lower value is a realistic possibility that occurs in real systems, or the 'extremes with another structure' (like an average or a probability distribution of a point being in a particular sub-area, as in a different probability distribution for each 'x-value' or 'local x-value subset')
            - identifying useful structures to add/remove such as 'areas of randomness' such as a cube of evenly distributed points, which its possible to filter into possible solution functions to connect it with more certain lines around it but is also possible to remove as a structure of randomness that indicates neither priority so can be removed as possible noise and added back in when new information might help filter the possible solution functions, in cases such as where the more certain lines around the random structure are equivalent and indicate none of the solutions in the random area as more probable
            - identifying useful alternatives (like whether to 'apply multiple average methods and merge the average outputs (like an ensemble of networks)', or 'whether there is room for improvement in the average methods and finding new variants of them given their variables is worth pursuing (like a function of method errors where a more optimal point is implied on the error curve)') in existing methods by applying interface structures like 'input-output sequences' applied to 'average methods' as an important component of the regression problem space
        	- identifying unit structures to apply as components of a data set range (such as a data set density or probable function range) include units like 'overlapping circles which are likelier to describe a probable function range of a typical data set which is usually an area rather than a clear line', 'semi-circles shifted around a linear average line', 'shapes that indicate exponential change and also some non-trivial area like curved rectangles or circles as tiles of the probable function range' to indicate variable interaction structures that could represent the legitimate variable interactions mixed with some error likely in a system of some probable complexity (local change tiles which are likely to be simple shapes distorted by errors in some degree/way), where a 'probable function area as opposed to a probable function line' indicates either a complex system, a system of maximally different or locally representative alternates indicated by edges/corners or local averages of these shapes where the interim points are errors, a lack of complete input variables, an error like randomness injected in the input data set, where a better (simpler, clearer & more accurate) representation of the data set would be a previous variable set on a prior causal node where the differences creating the area are graphed as vectors and the variable interaction can be graphed with a line
        	- just like some structures are useful in their simplifying and explanatory effect (like 'core components' leading to outer 'interaction levels' supporting 'maximal differences (like a function network but in every direction)' where 'interactions are adjacent between nodes'), other structures that combine important useful interface structures like 'core components' and 'interaction levels' to achieve useful intents like 'adjacent/linear interactions' are similarly useful, and can enhance the usefulness of these structures, such as 'cross interaction level errors' like what errors can happen when an output layer interacts with the core input layer, or a way to organize the structure so that frequently interactive functions are adjacent even across interaction levels
        	- identifying 'sequences of change sets' that are commonly seen across systems, to identify common useful functions found in sequences like a 'creativity/generation' step, a 'maximally different/uniqueness' step, a 'standardization/grouping' step, an 'abstraction' step, a 'contradiction/neutralization' step, a 'incentives/efficiency' step, a 'stress/competition' step (like to see which variables stay constant and which can maximize their variation), an 'explanatory/decomposition' step, a 'understanding/organization' step, a 'ambiguity or other error generation/identification' step, a 'randomization' step, in a way that reflects real change patterns, and applied to connect known certainties (probably certain trends in the data set) and uncertainties (the remaining variation), these 'sequences of change sets' being more useful than other function formats to identify commonly repeated useful functions, as well as other patterns in change sets like cycles and equivalent alternate change sets, some of which are simpler/more useful than the original connection between original inputs/outputs, and to identify different useful function formats like 'sequences of vector structures & maps' applied to inputs (which move in the direction of function networks and allow queries like 'which are the most commonly useful function networks' and 'which function network compresses all other function networks adjacently'), as structures of 'useful incompleteness' created by mixing cross-interface structures that leave irrelevant variation unhandled and are adjacently transformed into useful variation-capturing functions
        	- condensing variables as 'some or any change in this variable set of the same type (rather than specific changes in each variant of the type)' is also useful to quickly identify sources of variation on a different interaction level
        	- identifying structures like areas/slopes of maximum volatility (adjacent input-extreme output connections) and connecting them to areas of constance as a way of identifying the maximum volatility allowed by a filtered set of functions in a more constant subset of the data set, as connecting the filtered set of a function in a more determinable subset and the volatility allowed by that filtered set can determine other local subsets of the data set
        	- identifying useful similar structures like overlaps/convergences as indicators of similar but different types of change (overlaps being an indicator of robustness or probability, and convergence being an indicator of some limit structure or an average structure)
        	- given that embedded variables (meaning 'embedded on interface variables') are likelier to create 'maximal differences', testing those as a default filter to describe complex data sets that can easily look random in some subset is useful in this problem space
        	- given that there may be a transform of the data set (like a subset of input variables) may be a more efficient way to generate the probable data sets found in real life (as in, there is a subset of input variables around which most of the variation is clear and easily explained by more common functions, indicating the original input variables include random noise), its useful to identify subsets of variables with simpler variation as complex systems are less likely to be stable and are likelier to contain random noise from variance injections, similar to how identifying simple structures like 'evenly spaced repeated data set subsets' and 'densities' can identify common patterns that are simpler to model as a pattern or a component of the solution function if they represent a sufficient ratio of the data set
        	    - relatedly to the workflow involving removing simple structures, removing some subset of the 'points' that are connectible in 'straight lines' in a data set is useful to remove structures too simple to be useful (in the sense of relevance for the intent of capturing information beyond a general simple summary base function to use in finding the actual solution) in describing a more complex data set as is more commonly found than a simple linear data set
        	- some structures are more probable than others 'a range of data points around a pattern (as in an area of probable range of a function) which describes likelier structures like errors' is likelier than a 'highly variable specific function (like a function with many peaks)', so these likelier structures can be applied as more default than other structures
        	- function sets which are adjacently transformed into each other which have some base of similarity in common (like accuracy, variation, input/output patterns, or input variable subset ratio) are likelier to be equivalent alternates and probable solution function sets than other function sets
        	- 'maximally different data sets with known error types applied, mapped to solution functions' are another useful starting point for reducing the regression problem
        	- variables like 'dependence/connections between inputs' (such as a 'sequence using the previous adjacent input as an input', as opposed to a 'function of independent variables') that create dependent/independent variables are also useful to identify, as these variable differences are important to filter out, where independence of variables is not obvious but a sequence is easily detected
        	- identifying structures (like isolatable structures, including obvious components, such as 'anomalies') which are likely to explain some variation interaction patterns (like an 'occasional extra peak disrupting a more probable pattern') as common error structures to remove to find simpler functions and add to find error-handling functions or erroneous versions of data sets, similar to how modeling a 'system cascading to destruction' is useful to identify signals of these error structures in data sets, just like identifying 'probable new more stable states' of a data set is also useful
        	- increasing the 'directness' or 'adjacency' of variable interactions as being more explanatory, more likely to be linear/simple, and less likely to be subject to undetectable noise/errors is a useful intent rather than trying to identify distant connections between indirectly causally linked variables, as indirect connections are likelier to change (but also likelier to re-occur when removed so extremely distant effects are still useful to model)
        	- similarly, given how the problem of 'finding a function to summarize a data set' is a problem of 'finding missing variables (coefficients, powers, etc) that are not in the input', other formats of a problem are similarly useful to apply as defaults (what are the patterns of missing variables, are they more difficult to detect, are they less visible using data gathering techniques, do they have similar complexity like 'host dna' & 'pathogen dna' which allows inferring the 'existence of dna and pathogens in a host', and 'default function errors (like immune errors)'), similar formats like 'stable variables' to identify variables that are more stable than other variables so they will become 'high-variation'-causing variables, a useful structure to identify with 'high-variation variables'
        	    - for a more complex specific example, identifying people from their dna involves identifying 'missing variables' (like phenotype-determining genes, as well as 'genetic change variables' such as epigenetic changes, errors in dna-phenotype mappings, mutations, disorders in dna editing, etc and causes of these 'genetic change' variables) and filters of these missing variables (tests of these variables to help filter them or legitimate stable differences to filter/separate these), which are variables in between the inputs/outputs in different interaction levels than the inputs/outputs like the 'gene function' interaction level
        	    - similarly, the machine-learning problem involves finding 'missing in-between variables' (like a 'function to create differences (to find variations of structures like combinations of inputs)' and a 'function to attribute/connect useful differences to filter out (like connecting an error difference to a node or node structure)' and a 'function to calculate differences from correct values (calculate the error/loss from a particular input variation combination)')
        	    - these are highly complex sets of variables (input/output subset filters, high-variation change inputs, 'instruction' functions generating similarities, variable interaction functions, generative functions, difference-filters) which fits with highly complex systems, decomposing them to slightly less complex variables on different interaction levels, which can help with identifying missing variables (such as inferring 'alcohol' as a cause of 'genetic changes' when other complex sub-systems arent sufficient to decompose all variation in a data set, as a source of variation from another interactive complex system, given how complex systems usually are created by interactions between multiple complex sub-systems)
        	    - variable interaction functions like 'add/subtract/neutralize', 'limits/extremes', 'filter/differentiate', 'repeat (like as a default)', and 'interfaces/averages' are common (or even required) across systems so are likelier than other variables/functions to re-occur in unknown systems, which makes them useful to apply as default variable interaction structures
        	- similarly, applying known 'contradictory cases' of 'when known rules are wrong' such as when a more general function is wrong compared to a less general but more accurate function despite a common solution metric like generality
    	- similarly, identifying the 'core shape of a local subset representing a change unit of the more general function' and 'its interaction function (such as overlapping with other core units)' and its change functions (such as how it can 'rotate to some degree or vacillate in some way') as a way to identify the general function from a local subset that is sufficiently representative of the differences in the data set that it can be used to find the 'core change unit shape' that can be repeated/shifted/scaled/rotated/otherwise changed to find the rest of the function using some interaction function
    	    - relatedly, finding the network that filters the possible core change type combinations (rotate, scale, shift, vacillate/cycle, embed variables, repeat, abstract, connect, format, etc) in a maximally efficient way for most functions is a useful intent to fulfill as a default implementation strategy to start with
    	- similarly, identifying useful structures like 'maximally different directions of change such as the cardinal directions (or high/low left/right directions of change)' as useful structures to use as a filter to identify different change types that are common and highly different, as a 'maximally different unit of change' which is useful to find adjacent changes
        - similarly, identifying connections between variable interaction structures (like how a variable structure such as a 'variable upper bound and a constant lower bound' have a useful structure of "implying but not guaranteeing (making them probable and useful to test)" other variable structures like 'more change (either expansive/reductive) happening in the upper range' or 'fewer limits on change in the upper range' or 'more change incoming to the lower bound' or 'interface variables and/or constants relevant to the lower bound') which can be connected to possible filters/limits of those structures, such as whether other variable interaction structures (or specific known problem space system rules) filter/limit/prevent a possible implication
        - similarly, identifying local subset representation structures (like densities or average lines explaining the majority of differences in the subset/densities) for one local subset and then removing those probable components of various possible representations in another subset to find additional possible variables in maximally different subsets to filter the set of possible alternate representation structures (and their components) from the original subset, after identifying the maximally different subsets likeliest to have differences in probable representation structures & their components
        - similarly, identifying connections between 'uncertainties' (like subsets of the data set that are more uncertain/variable where other subsets are more easily determined) and 'uncertainty resolution functions'
            - resolution functions like 'intersecting with the most points in the subset with the simplest line' or 'the simplest line that remains some minimal distance away from the most points in the subset' or some balance of 'averageness' and 'intersection' which are useful variables of filtering/generating these 'alternate regression lines' in 'highly uncertain subsets'
        - similarly, identifying useful structures like the 'variation range' necessary to create a useful structure like an identified 'acceptable error range area' (with a trivially identified area of coverage that is likely to be useful in finding a maximum of points falling within that area when moved across the data set 'probable regression range') that can be moved across a 'probable regression range' and tested in various subsets to cover a 'ratio of the data set in that subset', this 'acceptable error range area' being useful to avoid calculating the error for a possible line at every input value and just checking if it falls within the range area centered in some 'probable regression range', either after calculating some 'probable regression line' or using the 'probable regression range' to fit it inside that range, or finding the trajectory of the 'maximum coverage direction' as the area is moved across subsets
        - similarly, identifying the relative usefulness of structures like 'intersecting/overlapping summarizing lines of data set subsets' as opposed to 'adjacent local subset summarizing lines with strict range limits' to allow for possible overlaps in local subset selections bc the borders of these ranges might not be guaranteed/required by data and to allow for known variable interaction patterns like 'multiple possible interaction types/states involving the same variables, variable interactions which can explain variation in outputs with similar/adjacent inputs
        - similarly, identifying alternates of useful structures like 'randomness' (such as how 'randomly dropping a ratio of data points can reveal robust variables') alternatives such as 'combinations/sequences of common/powerful functions across systems' can identify more probable structures to replace these less likely structures, using probability structures like 'commonness' to replace the less accurate/likely structure like 'just any randomness at all (a random selection of randomness)', which applies 'more probable & less random' randomness created by a higher degree of certainty through alternate structures like 'commonness'
            - similarly applying more relevant/useful structures of randomness like 'obvious structures like simple shapes like densities removed from the data set' which are more relevant to workaround (as simple-minded agents are likelier to intervene with obvious human error and likelier to be required to stop the interference of), as true randomness is unlikely to be easily verified like obvious randomness and is less relevant/useful to account for (above some ratio of expected noise), as more complex randomness could easily be hidden legitimate variables undeterminable from the original data set requiring more data/variables to identify such as 'incoming changes' to the data set
        - similarly, identifying a function to convert non-linear to various probable linear functions (more adjacently computable, or using fewer variables involving equivalent alternate variable subsets of the data set) is a useful intent to fulfill with structures like logarithms, topologies, & mappings to model 'interaction levels where interactions are adjacently computable with objects defined on that level'
    	- similarly, an 'index of pre-computed regression lines to compare with original data set subsets' to fulfill useful intents like 'avoiding computation' is useful for connecting original data set subsets and pre-computed regression lines for alternate subsets
    	- similarly, how clustering relevant structures (like inputs) by differences/similarities (such as organizing by the 'similarity/equivalence of output value') can identify useful structures like input patterns of similar outputs given assumptions like 'non-volatility' and 'non-randomness' and 'non-uniqueness of inputs for each output' (like input subsets that have clear patterns, like how a wave function organizing the inputs by similarity of output would have clear patterns of magnitudes/amplitudes in a few subsets that prevent requiring checking the whole input space for the pattern, as the output y-value would have a few data points associated with it such as 'input value points graphed vertically with the y-value on the x-axis' representing inputs having that output, where this vertical input pattern associated with a y-value would have clear patterns such as obvious differences in distance between points that are clear after a few points rather than many), which reduces the problem to 'find n y-values having the same value m times to check for obvious patterns in x-values for each y-value, if there are multiple x-values'
    	    - preemptively testing for validity of 'assumptions of algorithms' is an example of 'alternate equivalent structures' (like limits/requirements/intents) and an application of the 'input/output sequence' of optimal algorithm filters
    	    - this applies a useful function like 'sort' to the outputs rather than the inputs to get useful information about similar outputs once sorted that way such as how variable/cyclical/maximally different the function might be by comparing inputs of equivalent outputs
    	    - once outputs are sorted, it is possible to find highly different outputs easily, which is useful for intents like 'check for maximally different outputs' or 'find output range or output extremes'
    	- similarly, 'finding average magnitude/amplitude/count of peaks in a data set for some subset' is useful just like 'finding the general average value' and 'finding local subset averages' and 'finding reoccurring subsets' and 'finding highly different local subsets and their connecting functions' are useful
        - similarly, a 'adjacent points merging function (using some representation like an average midpoint, or using a similarity metric like distance from/angle to local densities, and using some ratio selection like merging n points at a time within some distance m of each other)' and a 'adjacent point non-merging function (leaving some points unmerged bc of their representativeness of legitimate differences)'
            - similarly, an algorithm to identify variables that can capture high variation when applied together (representation metrics, similarity metrics, ratio of input metadata like count/difference score) as useful complementary capturing variables of information, acknowledging differences in data sets like that averages are sometimes more useful than similarity metrics and sometimes the opposite is true and there is a useful input type like 'input count/ratio of the total count' that is optimal for some structures like high-variation data sets with some noise level
        - similarly, applying similarities between structures that have a reason why theyre useful for summarization/representation intents (like how 'big/simple' shapes like 'equilateral' shapes are particularly useful to identify to simplify the task of identifying a regression function bc finding their centers/averages/densities/patterns is more trivial and likelier to reflect reality as big/simple shapes are less likely to occur by accident in a data set and therefore likelier to summarize the data set, up to a certain point, like how identifying that a data set as a whole has a generally square shape makes it likely to be equivalent to random)
        - an example resolution function between 'densities and regression lines' is 'divide into subsets, then find one representative density for each local subset, then expand densities until an overlap/equivalence is reached with another expanded density' by applying the 'reason' for why it would be useful to connect 'representations (like density averages) of adjacent local subsets' (bc adjacent local subsets are connected in the original input data set, so the reason to connect them (or a variant of them like a representation of them) later is that connecting them later 'aligns with the original input' in a relevant way, relevant by 'preserving the information of the original data set' which is useful for the 'find a regression function' intent) and applying the structure of 'how' to connect them through 'expanding' them (and the reason why to use that, which is an adjacent transform applied to a density average and is therefore useful, where equivalents are also trivial to determine, and these operations in total can beat other regression algorithms in some solution metrics)
        - similarly, a function that connects the 'points that vary' and the 'points in common' across multiple probable regression lines is a useful function to solve for 
            - finding the sections of the regression line that would be variable in variations of the bias vs. variance tradeoff, to focus on finding functions to connect the 'points in common across probable regression lines' that should be optimized for in the final function, where the 'points that can vary across regression lines' can be averaged or otherwise represented by known probable points at discrete intervals rather than a continuous line, where a point not on those points can be approximated by adjacent points, where the 'points in common' can be approximated by a range that is narrower than the range for the 'points that vary' and the range representing a range of acceptable solutions, so finding a function to resolve the reduced solution set of the 'points in common' connecting functions by connecting these subsets with the functions describing the 'points that vary' is a useful function to solve for
	     - finding the connecting function between different sets of summarizing functions like the 'average' and a 'slope-standardized function (to find the useful standard to compare changes to, to find the core differentiating vectors from a straight/average line) and its scalar to scale it to the original' and the 'lines that describe local subsets to the points of extremes (similar to eigenvectors)' and the 'lines that connect averages of densities' is a useful function that connects these alternates which offer the same representation attribute but also capture different information in the data set, as connecting 'efficient representations' is more trivial than connecting 'every data point'
	         - finding the 'useful core function representing the most standardized (such as de-scaled) function' is useful to find a 'component function' to check against multiple subsets of the data set (do any known variable interactions create change types other than this component function or component function range or do they follow the structure of the component function/range) and look for variables that adjacently create/scale the core component function to check it for realistic probability of representation of variable interactions
	     - finding the most important structures to check for when filtering possible solution functions (such as how its important to check if an amplitude of a polynomial is different across different peaks to determine if a peak pattern can be applied/found, how its important to check multiple local subsets of the function input range, etc) can act like maximally differentiating filters of the solution set
    	- finding the useful ratios & other structures of inputs to an algorithm like 'find the common slopes of connection lines between points in local subsets of the data set', where the algorithm to find the 'useful ratio/count of slopes in common (a ratio compared to some standard, like the number of possible connections)' is the target to solve for, as the other structures that are useful are already known or easily determined and the uncertainty is in finding the threshold values or other values to optimize implementations of those structures
       - connecting alternate formats of the data set/regression functions like 'maximally different connectible shapes (like interfaces) that can be formed by a data set subset of some ratio' which can be used to indicate 'embedded variables' (like variations on that interface) is useful for determining one function format from another which may be more trivial than another method
        - other structures than standard regression structures (averages, connection lines, subsets) like 'maps' can be applied as a useful structure in the regression problem space bc of how mapping one subset to another through substitution can be an efficient way to decompose a more complex set of points into a more standardized or otherwise useful set that is likely to represent the original set and requires less memory to store, which are useful as components of solution-finding methods
        - finding useful metrics like 'degree of erroneous difference to ignore' between obvious average functions of local subsets is useful to find out what information to ignore when an average line of one subset differs to some degree from an average line of an adjacent subset, especially if the next subset confirms the original subset average line, applying the concept of 'data corruption' to describe some degree of error deviating from some implied metric, resolving these 'implication' structures (like the implication of a 'common subset average line') into 'conclusion' structures (like a degree of commonness of that line across subsets above some ratio), and finding useful tests of these differences, to find out when a difference may reflect a common or otherwise probable/implied structure (implied by adjacent inputs, common patterns, similarity to known implications, etc) rather than an erroneous anomaly to ignore
	- in the 'network (fuzzy space) of structures' fulfilling intents, interface structures like 'overlaps' exist between structures adjacent to or otherwise useful for multiple alternate intents, these interface structures indicating their usefulness for other intents like 'deriving alternate intents' and 'building a maximum ratio of structures'
        - in the space of useful structures, concepts like 'balance', 'alignment', 'simplicity', 'probability', 'composability', and 'uniqueness' will be obvious, which can be used as 'conceptual filters' of useful structures that are likelier to be useful than other structures
        - an example of this fuzzy space includes structures (like 'angles, partial closed shapes (like sides and corners), connection functions of partial closed shapes, higher-dimensional closed shapes, shapes that when combined can produce a closed shape in between them') as the set comprising the fuzzy space of a 'closed shape', this space being composed of components adjacent to or otherwise useful for fulfilling intents (forming/describing/differentiating a 'closed shape') related to a 'closed shape'
        - a network of similar/equivalent alternate spaces include a 'non-repeatable/unique component space (which is optimal for storage minimization)', a 'repeatable component space (which is more optimal for displaying usages/queries of components)', a 'usage adjacency component space where frequently co-used components are adjacent (that is useful for finding probably useful structures using a component)', a 'difference as adjacency space (where maximal differences are possible with adjacent queries)', a 'layered space with both intent/structures (where the fuzzy space of an intent contains maximally different structures fulfilling/adjacent to fulfilling that intent)' bc of the adjacency of these structures for these intents related to components
            - relatedly, a 'usage network (apply)' has adjacent corrollaries like a 'filter network (find)', a 'component network (build)', a 'difference-resolution network (derive)' due to the core functions it is an equivalent alternate to
        - graphing an intent by its 'surrounding related structures' such as by 'structures that use it' or 'structures that fulfill/build/create/cause it' or 'its input/output structures' or 'structures that filter out everything in some relevant subset but that intent' (or similarly 'structures that determine that intent') is another way to visualize structures like intents that are more useful when defined as a set of alternate definitions which can represent examples of them in some other system
    - framing common structures with relevant metadata like useful intents in standard terms to maximize the optimal positioning of these structures in queries
        - a network (which depicts uniqueness and similarity) is useful for 'finding new/different unique similarities by the gaps & other structures in the network' as well as 'identifying difference/similarity of two known structures'
        - a map is useful for 'finding unique connections & other metadata about connections' (like the commonness of connections having equivalent/similar connection/input/output) as well as 'translating a structure in one format (of a set that can be described by a network indicating uniqueness/similarity) to a corresponding position in another format (of a set that can be a network)'
        - a filter is useful for 'finding a similar subset of points in a network/set of points' (as in similarity to some attribute, like a solution structure/metric/requirement)
    - finding a good starting point to start applying interface structures is crucial for deriving adjacent solutions, like how a limited subset of 'logical rules such as definitions' or 'physics rules' or 'truth limiting rules (what is definitely not true)' might be useful as a starting constant input to start applying interface structures (like changes) to, to derive other rules that follow logically, are required to be true, are implied, are not contradicted, or have other structures of truth associated with them
        - similarly, finding a 'useful structure to describe common patterns in changes' is an example of a useful isolateable structure that can be a good approximation of a full implementation on its own, answering questions such as 'are most variables an adjacent combination (or other core structure) of some subset of interface structures', which is findable with iteration
        - similarly, finding a 'reason for similarities/differences' (reasons like 'its an efficient/useful combination of few inputs commonly available, so is often repeated across irrelevant systems') is another isolateable rule set that is a good approximation of a full implementation of all logic rules of interfaces
        - the differences between these 'equivalent alternate' isolateable rule sets that are good approximations of interface analysis make them useful to combine in an adjacent combination as offsetting 'ensemble' structures to weight the impact of their outputs against average outputs & other representations of outputs
        - this is like how everything can be framed as a component that can be added/multiplied to other components, but thats not always useful in terms of reducing computation requirements, such as how knowing 'addition' and 'multiplication' are capable of describing all other structures, but that doesnt capture a useful degree of complexity of the potential interactions of those two operations, where knowing concepts like 'self' and 'embedding' is more adjacent to the complex operations/functions possible with addition/multiplication (self-multiplication like 'powers' and embedding as in 'embedding of operations'), these two concepts being adjacently derivable with interface analysis through core structures like 'unit/identity (self)' and 'application/usage (embedding)', similar to how matrixes (aligned multiplication of ordered sets) and inner product spaces (spaces where some product is trivial to compute) and convolutions (complete multiplication product of sets) are not adjacent to just the functions add/multiply
             - this is a useful structure for tasks like 'encryption' as the 'set of concepts that are adjacent to a useful structure' is more difficult to guess (from the set of all possible concept sets) but is easy to verify
    - applying interface structures to optimize with ml, such as by applying 'input/output sequences' to position 'generative feature layers before filter feature layers' and other patterns that make sense to server as a useful contradiction to offset the irrelevant variation introduced by the preceding layer, or applying 'self' to apply neural networks to select preprocessing & algorithm functions/parameters
    - to handle common error structures like 'dead ends', apply structures like patterns to find the solution to (the 'way out of') traps using these errors, such as by applying insights like 'nothing is unconnected to everything' which means 'there are no real dead ends' as 'everything is both true and not true in some way' and find the distortion of the perspective that created the error of the 'dead end' and connect it to the balanced perspective (where interface structures exist or are adjacent), creating a difference that allows other differences to be embedded/connected/supported
        - if there are filters allowing for only one possibility (a 'dead end' error that leads away from a network), find the filters that represent the errors in those filters, reversing the perspective back to the balanced perspective and exiting the perspective creating the error of the lack of variation leading to that error, creating opposition to the incorrect difference
        - inject more variables to connect the 'dead end' position back to the balanced perspective, applying the interface metadata to create new differences (such as random differences through interactivity) where required to offset the incorrect differences of the over-reductive perspective, and use those differences to build differences on them and create change in another direction
        - where one structure seems to capture everything (like an attribute network), apply differences to identify other networks that capture alternate complementary information (usage networks, limit networks, difference networks, function networks, etc) to limit the limits of that over-reductive perspective
            - like how an attribute (such as 'criminal') can obscure info or over-simplify info, despite being an efficient way to store some relevant info (like the 'crime of jay-walking' being equated to all other crimes despite the fact that it is mostly only criminalized to collect fees to fund police stations and other far more harmful behaviors are not punished at all, such as stalking/copying inventors), whereas a network with contextual or functional info can more accurately depict that info to reflect its true variation
            - this is similar to how one infinity can be used to create other infinities (like the Banach-Tarski paradox) bc these graph structures are like topologies or fields in that they are capable of describing reality at every point but offer useful alternate advantages when starting from different points
        - this is like the structure of a 'mobius strip' or an 'isolated system describing the system containing it by identifying variables of observations of subsets of its own structures (regarding the incompleteness theorem)' where one structure seems ambiguously equivalent to different structures that seem to contradict each other but actually can co-exist in the same structure using the perspective interface, or like the structure of a 'rule set' occupying a point on a torus where what determines one error doesnt determine another error bc different 'rule sets' are supported and the 'rotation' and 'injection of new interaction levels (as different concentric circles allowed to be the base/core/stable level)' allowed in the torus shape enables endless (stable) balanced variation
        - a 'balance' of variation is a core structure driving interfaces, as there needs to be some variation-supporting structure like a ratio of change (like 'potential/kinetic energy' and 'momentum') and counter-change (like 'gravity', 'energy preservation/transfer limits'), or a structure like 'caring' ('connection to the most stable foundation, where this connections acts like an equivalence') that is supported and stable, otherwise the interface cant exist or similarly cant support any change, which can be used to find interfaces
        - on the other hand, filters may seem restrictive/limiting/reductive (like a trap that is a 'required error' such as a trap hiding a 'dead end' error) but may enable endless complexity/variation, like how a 'reality' filter may seem boring/reductive/simple until you identify how complex reality is, and that the 'reality' filter is powerful in that it empowers other structures to exist like 'clear descriptions/representations & measurements/experiments' as well as constants/variables and consistencies/contradictions to occur, enabling the pursuit and identification of truth and falsehood and the application of differences to both
        - "filters to identify errors such as common structures of paths leading to 'dead end' errors" are a way out of errors of 'constance' (like 'over-prioritizations' such as 'over-reductions') just like finding 'abstractions' or 'interaction levels' or 'equivalent alternates' or 'embedded perspectives' or 'connecting function of perspectives' are a way out of traps
        - finding a common perspective that hosts both a trap/error and also a solution/way out of it (or generally a 'perspective that can trap any trap' as in 'capture the variation of incorrect differences driving traps') is an example of a useful interface to apply to other problems
        - similarly, 'traps/errors may be more adjacent to a solution than another trap/error', so creating a 'path of differences leading to errors' is not just a way to find errors but also find out what is not a solution and therefore what is a solution, and value can be created from a trap if there is a way to convert it into a solution like by applying it to itself ('trap the trap')
        - the 'maximally different structure' that can support the most difference types is also the structure that can support adjacent structures of differences, like 'ambiguities' (like the ambiguity in an equivalent distance of one error from the center of this structure to the distance to another error, and like 'contradictions/paradoxes' and 'counterintuitions/complexities' and 'alternate representations' as different variations of 'maximally different structures producible with the same inputs, supportable in the same system')
    - finding a solution base that is optimal for different algorithms allows finding different possible solution bases, at which point finding the more optimal adjacent solution to those bases is possible with known/adjacent algorithms (as opposed to finding 'maximally different solution bases' to start from with one particular algorithm)
        - finding the maximally different functions that make an adjacent optimum findable with some algorithm helps 'filter out these more adjacent solutions if theyre incorrect' and 'find counterexamples of alternate possible solutions' or 'divide/filter the solution space' faster, such as by finding 'common overlaps in solution spaces generated by different algorithms' where these overlaps are more trivially calculated in some way than by applying either/both algorithms
        - deriving the 'solutions findable/verifiable with an algorithm' is a useful way to filter the solution space, find common solutions across algorithms, and find variables of algorithms to find other algorithms or match algorithms with metadata like intents/metrics optimized for
        - finding a 'network of algorithms with rules for switching between algorithms in certain cases like with certain input patterns or certain solution metrics' is also adjacent using these structures of connections between 'algorithms' and 'solutions/ranges/sets adjacently found/filtered with those algorithms'
    - finding the set of concepts that builds a solution (such as how 'adjacency' and 'generality' help build a 'regression' solution) which help to offset each other ('adjacency' in the form of 'adjacent/local subsets' or 'adjacent/local optima or adjacent/local density averages' offset by 'generality' in the form of 'representative subsets' or 'representative summaries like averages') can help form a common base set of solution structures to build on top of
        - these are useful specifications of more general spectrums like 'specific local variables, as opposed to general abstract patterns/summaries', 'adjacency' being a non-definite partial format of 'specificity' in the 'regression' problem space, having the additional concept of 'triviality' included in the subset of concepts driving 'specificity' to relevant variables like 'position (point, density, extreme)' in the 'regression' problem space
        - finding regression lines whose differences from data points frequently follow a common component pattern (like 'having the same distance from data points' or alternately 'use the fewest components (one component as in the same value)' which fulfills a 'simplicity' or 'linearity of combination' metric) is another useful intent to fulfill in the 'regression' problem space
        - similarly, finding a set of components that commonly connects extremely different points (like points from different non-adjacent subsets at different limits (upper vs. lower) connected to some representative subset like an average) is a useful intent to fulfill in a regression algorithm, the specificity of this intent making the algorithm trivial to find
        - similarly, applying rules of relevance such as 'removing variables with equivalent information coverage is acceptable if one variable remains to cover that information in an intent related to an information-preservation intent but its better to leave the variables/rules generating those variants in the data set rather than leaving in one variant or all known variants in some cases like when minimizing memory storage is prioritized'
    - apply common structures like filter/reduce/match/add/change/map/sort to format functions to identify the maximally different functions, then find the reason why those functions are useful as a way to identify 'rule sets that can act as limits of usefulness to identify the areas and boundaries of usefulness'
    	- example: 
	    	- bc its useful to have a 'map of a keyword to different variants of it', its possible to identify that "a unique signal has variations because of alternate definitions/formats it can take while still remaining unique (bc of 'definition routes')" (allowing this 'variable interaction' & 'variable interaction-structure rule' regarding usefulness to be derived: 'bc this variable interaction exists, keyword maps/definition routes are useful')
	    	- bc its useful to have filter functions in general, its possible to identify that 'not all differences are adjacently useful for every intent' and 'different variations of structures are not organized by default' and 'different structures can coexist'
	    	- bc maps (connections between 'user-assigned (relatively arbitrary)' rather than 'absolutely-defined' values) are useful, 'arbitrary connections' are also useful for intents related to randomness/uniqueness, like when connections dont matter absolutely but are still useful to assign (like organizing a filesystem to optimize common queries such as using symlinks or naming conventions, even though that organization doesnt reflect absolute truths of the universe, or such as how mapping several terms to one identifying term allows quicker identification of unique term usage while minimizing memory storage, or how an explicit map to identify a category of an attribute value set is useful when identifying the causal variables is non-trivial, to skip that causal analysis, or how substitution maps can create the appearance of randomness bc they are relatively arbitrary)
	    	- bc maximally different functions (filter, map, change) often co-occur indicating their usefulness, its possible to identify the 'common complexity of problems solved more frequently recently (bc of existing solutions to simpler problems)'
	    	- bc similar functions often co-occur (filter, sort) indicating their usefulness, its possible to identify that those functions are cooperative for various intents ('sort' speeds up some filters in some cases)
	- applying rules to identify when a regression function can be incorrect mapped directly to problem space structures
		- example: when a high ratio of incidental/random variables (like variables about a context that any species can exist in which is not relevant to identifying a species) are included in the function, or when an important variable is ignored to fulfill an incorrect priority like simplicity, these errors are mappable to differences between the incorrect function and data set subsets
		- when a more simple function is found but it contradicts the more complex function that would cover more cases bc of the data set subset chosen/available that makes the simpler function seem correct, its violating other known solution priorities like generality which can offset over-simplification errors
		- more importantly, structures of this error can be mapped to a set of example possibly relevant regression structures (data set subsets and regression lines)
		- for example, when there is a 'small pattern in a local subset indicating a more complex function' which is 'within the boundaries of a potential field of variable interactions' and 'not overlapping with probable error areas' but is ignored in favor of a simpler function, thats a structure that could be an error of 'ignoring a general trend bc of a data set subset selection (applied either before receiving available data or after)'
	- applying interface structures to known useful structure (like gradient descent) given their definition can identify relevant structures useful to those useful structures like optimizations to apply to inputs before applying the useful structure
		- the idea of 'gradient descent' is most useful when applied to a 'function that is adjacently connectible to the optimal solution' (meaning the inputs are already adjacent or equal to the actual optimal inputs) bc of the definition of 'gradient descent' which applies 'adjacent change combinations', assuming that 'adjacent change combinations' are capable of finding an optimal (which is best used when 'everything is adjacent' or 'maximally different examples are adjacent', etc)
        - this makes 'maximally different interaction levels' a useful structure to identify, to make most variable combinations or 'the most variable' variable combinations adjacent (which is what interface analysis does by default)
        - for example, to identify the structure of 'imaginary numbers (like i)' as a possible useful structure, some intents (like 'create a circle' or 'create common structures (like a circle)') make it obvious to try as a component of formulas and its not adjacent change combinations as theyre typically implemented ('addition/multiplication') but rather as an adjacent combination of interface structures (like identifying how addition/multiplication arent easily describing all structures and solving all problems, so try applying the 'opposite' of 'components of those core operations' to try to generate differences to use to describe different structures), for example to solve the problem of finding 'equivalent alternate maximally different descriptions/generators of a circle as a useful structure to describe/generate/apply', otherwise the idea of 'square root of negative one' is not adjacent to most intents but is obviously useful to identify, so identifying 'useful intents that would make such structures obviously useful' are useful to identify by applying interface analysis to fulfill probably useful intents like 'create common core components in different ways'
        - as another example, the way I discovered that circles are related to primes is by picturing multiplication in my head, in which I could easily generate squares/rectangles but I noticed there were gaps between these shapes' corners which were easily generated by integer factors, and these gap points were likely to be more describable as on a curve rather than as a product of integer factors
        	- https://twitter.com/alienbot123/status/997394393471516672
        	- it also reminded me of the fibonacci sequence bc of its embedded growth between numbers in the sequence rather than clearly exponential or clearly linear growth, as a different type of default growth that was neither of the two simpler types
        	- the idea of 'gaps between integer non-1 factors (when graphed as pairs of factors)' is not actually definitive as indicating non-linearity but is still evocative and therefore still useful in adjacently deriving that non-linearity is relevant, these 'evocative' rather than 'implicative' ideas are still useful in deriving new probable structures to test
        	- this insight path would be easily identified with interface analysis 'which asks questions like "what something is not" and "what is not easily generated by these variables" and "what is not adjacent" and "what is different" and "where would similarities be expected (like with other integers) but differences are found instead (like with primes) and what interface unites these (like the dichotomy of curves vs. rectangles)" whereas machine-learning dumbly applies a few insights at a time such as 'adjacent change combinations eventually can find some useful functions if you have enough compute',
        	    - while applying other relevant structures, like filters to avoid irrelevant exceptions/rules like pieces of the definition that most closely overlap with other more relevant functions like the 'identity function (using multiplication)' being more relevant than 'addition/multiplication' as the identity function is less meaningful than non-1 factors)
        	    - identifying the alignment between 'numbers between non-1 integer products in 2-d space' and 'numbers between non-1 integer products on the number line (primes)'
        	    - other useful structures to apply are:
        	    	- 'equivalent alternate formats' of primes and their factors (the relevant default variables) such as 'sets of alternate factors and lines connecting sets'
        	    	- the basic 'interface' structure which connects the 'equivalent alternate formats (number sequence of primes and factor sets of primes)' using a common standard they both adhere to in their similarity of patterns/gaps
            - relatedly, deriving other useful structures like 'e' can be formed by maximizing output change types (differences between adjacent values) while minimizing input variables (the 'previous value' and the 'previous previous value' and the 'addition' operation) without using exponents and using a simple function (minimized differences necessary to create the extreme differences)
        - similarly, 'adjacent change combinations' like addition/multiplication are not frequently useful for useful intents like 'differentiate a wave from a circle' which a machine-learning algorithm typically would not adjacently achieve but interface analysis would by applying structures like circles by default as a common core structure on the structure interface
    - applying interface structures like 'causal sequences' to identify useful structures like 'alternate points where variation is useful to inject'
    	- for example, injecting variables at the point of data-creation/gathering rather than data-processing (pre-pre-processing) could influence the value of data (its reflectiveness of reality) such as how 'smiling' before taking pictures can make some variables more obvious/detectable, which is a 'reality change rule' that is useful for making variables less ambiguous/more obvious so a classification algorithm can be simpler, so an algorithm to identify these points/rules is useful to improve data quality, or to identify more useful problems to solve like 'if the subject was smiling, what would these hidden variable values probably be, based on similar expressions as smiling that are generatable with existing data' (similarly, 'identifying a dye to inject (input side solution) or a treatment to try (output side solution) that would differentiate cells more easily' is a useful structure for a classification algorithm to be able to identify to improve data quality and delay decision-making of the model structure until a data quality ratio/info minimum is reached), which applies the insight 'bc there are some differences between items in different classes, there will likely be other differences that are detectable'
