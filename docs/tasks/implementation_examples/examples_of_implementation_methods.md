- examples of other implementation methods than documented default methods (like the 'apply/find/build/derive' function set is an 'implementation method' of interface analysis as its an independent set of structures that can be applied as defaults to implement it to fulfill some solution metric like comprehensiveness, minimal constants, variability, etc)
    - this document contains 'function sets which can act as implementations of workflows' (function sets which can implement a solution automation workflow) as well as 'solution automation workflows' (useful sequences of steps to solve a problem)

- how to implement the interface analysis framework as a set of simple functions for an initial version, involving alternate default constant simple structures to combine, such as:
    
    - identifying 'concepts driving errors' is useful for identifying the 'various solutions to those conceptual errors' (like how randomness and volatility often indicate lack of understanding or lack of information, and identifying the associated variables having that info is useful to correct these errors)
        - for example, 'specificity' is a structure that leads to 'volatility', which is an important variable to avoid where possible, such as how a neural network or context-similarity identification tool may identify that two substances are used in the same context (like two examples of a type), but the type doesnt explain the reason for using the substance in that way, so identifying the type isnt a context that works to identify similar substances that can be used the same way
            - specifically, identifying 'lime juice' and 'water' as both examples of a type of 'liquid' is irrelevant to why theyre used in similar ways (such as used with the word 'drink' and 'cup' or 'ounce'), so identifying their type in common isnt the reason why theyre used similarly, so other usages like 'drink 8 glasses of 8 oz per day' isnt applicable to both, as the reason for their similarity in one context ('both can be and are drunk in non-trivial quantities like an ounce without deadly effects') is different from the reason for the similarity in the other context ('only one of them is essential for life and in high amounts'), but the problem space of 'identifying structure-function connections between substances' is so volatile that even one change to a substance can totally oppose its previous function, and the reason for this volatility is 'lack of understanding of the reason why different structures have different functions' and the 'specificity of some functionality requiring that exact structure to have that function'
            - so avoiding high volatility functions is useful in regression, as it usually indicates lack of understanding or specificity that is not fully identified by those input variables (the specificity is stored in receptors/surface components, not the substances interacting with them, so that info isnt directly stored in the substance structure but is indirectly inferenceable)

    - identifying useful structures like truth interactions is useful for intents like 'filtering truth structures'
        - for example, given that specific/local facts about variable interactions are not true until they connect to other facts without every possible contradiction being true or more true (preventing a fact from ever being true), how many facts do they need to be connected to in order to become real (such as a real system acting as a base, where the fact can connect to other facts), and what other metadata needs to be fulfilled before a fact is clearly true (its still true or becomes true, even when some barriers to it are placed in positions to prevent the fact, bc the falsehoods are irrelevant in some way, such as they occur in a different irrelevant system or they only remove one input to the fact which is replaceable or not required)
        - for example, a statement is only true, if the structures in that statement are 'possible/defined', which identifies at least one other required fact before the fact can be true
        - similarly, being 'robust to extremes' and 'robust to having requirements removed (reoccurring across different systems with different bases)' and 'causing higher degrees of relevant variation (variation source)' are also truth structures, determining the 'reality-building/intersecting potential' of a 'fact'
        - similarly, how can the fact become an absolute truth, where does it stop on the way to absolute truth, and what sequences can convert facts into absolute truths or otherwise make them real
            - relatedly, what other spectrums are particularly useful to apply in identifying truth types/sequences other than absolute/local (such as iterated/repeated vs. uniquely occurring facts and non/adjacently creatable facts)
        - similarly, 'detecting lying' by surrounding it with barriers to lying and clearly differentiating it from truth structures and otherwise removing opportunities for lying is possible, which would be applied in an algorithm like 'placing barriers between variables so that the lie has to create truth on its own or using other lies, which it cant create (itll be usable to create other facts if its a fact)'
        - relatedly, the intent of 'solving a problem permanently vs. temporarily' involves solution structures like 'repeated regular applications of solutions' or 'speeding up computations to enable repeated solutions' or 'storing solutions in a input problem/output solution index' or 'finding interfaces or spaces that collapse the problem to a point', which is a useful intent to identify

    - identifying useful structures like 'function sequences' to identify other useful structures (like components and orthogonal variables) is useful
        - for example, 'squinting' is useful at identifying light dynamics bc it increases interactivity of light and reflective surfaces and bc of the reflectivity of eyelashes which allow the components of light to be identified (it first filters the light into a smaller input range, and then reflects light off a high-variation variable like a shiny surface), and similarly, the differences in wavelengths created by 'adding/removing a barrier to light' (sunset/sunrise) can also identify components of light, as different wavelengths can be combined/overlapped to create different wavelengths, and adding/removing a barrier changes the wavelengths in a way that reveals components of wavelengths/light, where the barrier acts like a filter
        - identifying this sequence is useful for 'identifying components of variation' by applying a filter and a reflection across a high variation variable
        - relatedly, identifying 'rotations' as useful for identifying 'orthogonal variables' is a result of identifying that the 'perpendicular alternative' is identifiable by a 'rotation', as 'rotations' cover 'every possibility in a range likely to contain differences', so rotations are likely to cross various differences types, including a point where difference is maximized (a 'midpoint between extremes' (in angles between vectors) can replace the value of a rotation, once 'angles between vectors' is identified as a new variable to apply a 'midpoint between extremes' to)
        - identifying new variables like 'angles between vectors' is a prerequisite to identifying these structures like 'orthogonal variables', just like 'identifying the range of rotation to apply, to cover all difference types'
        - relatedly, finding the 'rotation function' enabled by every variable (like an imaginary variable) is useful for identifying composing variables of high complexity problems, which are often explained with overlaps/rotations/iterations/other simple mixes of interface structures 
            - relatedly, a 'rotation' of a 'network' could be a function that changes connections of nodes to 'different adjacent/similar nodes'
        - similarly, identifying 'possible interactions between vectors' (like 'forming a closed shape') could also identify 'rotations' or 'perpendicular/orthogonal variables' (in the interactions between sides resulting from the closed shape)
        - relatedly, the 'equivalent alternates' are likely to be connected to other 'equivalent alternates' ('superposition probabilities' and 'entanglement similarities') and constructing reality based on 'alternate paths between these equivalences' is useful and possible (finding structures that map to interface structures which are like 'abstract info variables/structures' is useful to connect them to physical reality and therefore determine and change it)
            - so applying this would take the form of a 'possibility range' created by quantum processes like 'thinking' (which enables considering 'other possibilities than current/past reality'), these 'possibility ranges' acting like 'possible connections' on which reality can be based, which can be connected in many different ways, but given known principles, may prioritize 'high variation-supporting connections' ('variation' will move in the direction of paths formed by equivalences that support the most variation, like 'better understanding of reality' and 'agents acquiring that the quickest', which support more variation/freedom than current reality, so queries of 'abstract info structures (interface structures)' that form the best 'base graph of all other useful graphs')
            - relatedly, identifying a set of variables that reflects equivalent/similar/related info is related to identifying the same structure that can look like either variable from different angles or another structure that can fulfill multiple metrics which are sufficiently isolated or otherwise structured that each can only be measured at once
        - relatedly, applying abstract structures like 'iterations' and 'applications' and 'interfaces' as 'components of a neural network' is likelier to describe realistic variable interactions than just 'sequence coefficient/exponent changes', which only identify the variable interaction when its a polynomial, and lose/avoid/miss the info of 'alternate routes to the same function' and 'non-polynomial connection functions' and 'all the ways to reduce routes to important variables'
            - there are ways to format an interface as a 'sequence of change combinations', but that sequence isnt particularly likely in most neural networks
            - relatedly, identifying 'alternate forms of energy' (like 'dopamine' and 'distractions' and 'similarity to a pattern') which help with intents like 'maintaining a rate' is another example of 'identifying different graphs/connections'

    - identifying useful structures like new interface structure combinations (like 'vertex networks') that identify other useful structures adjacently (like 'new function formats')
        - for example, a 'vertex network' is useful for identifying interface structure sets as useful components of interface queries, a 'vertex map (whether 1-to-1 or 1-to-n)' is useful for finding similar/equivalent/different/variable vertexes to apply, and a 'vertex wave' is useful for identifying how a vertex can vacillate between opposite extremes on a spectrum
            - similarly, a 'concept-structure-reason' set is useful for identifying high variation sources in 'reasons for a variable interaction' (structural reasons like structures in hidden dimensions and conceptual reasons like randomness), which is useful as a node on this vertex network (or relatedly, a cross-interface network)
        - framing polynomials as 'structures such as waves/parabolas which act as resolutions of pairs of opposing forces' is useful as a system-structure explanation of a polynomial, as a 'sequence of factors/exponents' doesnt map 1-to-1 to real system variable interactions
        - this set of opposing forces isnt quite enough to cover all polynomials, as there are other powerful forces, such as 'scalar forces, which increase/decrease the changes created by the opposing force pair', but the network of these together is more highly covering of all possible polynomials

    - identifying useful structures like 'different optimization network variables' is useful to identify different useful structures like 'specific optimization variant networks' which can be applied as a constant in some other algorithm once identified and which are possible to identify using other workflows
        - for example, a 'network that optimizes for no metrics (is suboptimal in every way to some degree, as it fulfills no metrics)' is useful for different intents like 'identifying interim spaces in between other graphs' and 'applying changes to a known incorrect structure like a base suboptimal solution', and is similarly different from other useful networks like 'networks that specialize to only optimize for one metric'
        - identifying these variants of optimization networks is useful to identify different types of optimization and different useful structures for different types of optimization, 'types' referring to 'different solution metric sets'
        - relatedly, identifying 'networks of variation sources' to fulfill problem-solving intents like 'identify sources of variation' are useful for identifying new possible variable interactions from new variation sources, which are particularly difficult/useful to find, such as high variation variables, powerful variables, interaction levels, randomness, etc, which are difficult to find because theyre usually already known
            - identifying 'reasons for difficulty of determining/finding info' is similarly useful as a problem-solving intent
        - relatedly, 'removing variables' is a known useful way to identify if there are other variables present that matter (like 'removing waves makes it clear if a shark is present') but more optimal changes would apply trivial changes to existing similarities/patterns (a type of light reflection that when bounced off of waves of similar magnitude/position, makes a shark clearly different, despite the similar structure of fins/waves), where the corresponding math structure is 'assuming there are variables in surface patterns'
        - the 'equivalent alternate graph of similarities' is useful to identify parallel connections which can be equated with a core function like a shift and which dont have an overlap/intersection or a convergence point where they both halt their variation, and similarly, a 'graph of intersecting functions at some regularity of interval' and a 'graph of converging functions by additive/sequential variation, or by angle/limit' is useful to identify as an alternate similarity type, where overlaps/connections between similarity graphs are similarly useful to identify
            - there should be a graph where 'checking for an opposing statement of a statement and whether it will intersect/interfere with the statement' and 'checking for convergence of a set of statements to the same statement' and 'checking whether statements halt at some theorizable limit' is possible using vectors, so these graphs are a useful starting point to begin identifying that final version of the graph with all of these computible functions, where the overlaps in these graphs will be a useful secondary intent to fulfill toward that final version of the graph
        - the similarity in usefulness is in 'connecting high variation variables like optimization graph variants or similarity graphs in different ways'
        - relatedly, an 'iteration network' is similarly useful as a 'unit network', which can be used to determine whether truth statements hold, such as 'any statement can seem true in isolation (in a vacuum)', where there are structures that can never generate another structure, no matter how often its iterated, which are useful to know about, such as where an 'opposing structure to iteration' is optimal like a 'component' as opposed to a repetition
            - structures can so often be formatted as iterations once a unit is found (such as fractals where the unit of change is the scale) that its useful to identify structures that cant (randomness sources)
        - relatedly, the 'rarity of compounds with only one function' is a probability ratio that can be applied to find 'missing nodes' in a network of structure-functionality connections
        - the 'way to game/falsify a solution metric' is similarly useful metadata to identify about solution metrics, where there are some optimal sets of solution metrics which cant be falsified (the function is a solution at regularly tested intervals, fulfills multiple accuracy metrics, is robust to different valid inputs, etc)
        - similarly, a spectrum graph like the 'graph of positive/negative structures' is useful for identifying how positive structures are created by negative structures and other interactions between extremes of the spectrum variable, and is similarly useful to connect to other spectrum graphs so that other spectrum graphs have clearly similarities such as overlaps/maps to other spectrum variables
            - the variation across 1-to-1 mappings which dont map directly to each other is similarly useful to identify, and is related to the variation allowed between 'equivalent alternates' which are similarly useful, or 'ambiguous alternates' which seem similar or are useful in different ways, or other types of structures like 'allowed possibilities by requirements' which have less similarities to each other and more similarities to requirements
            - relatedly, the structure of 'equivalent alternates' is useful for constructing other structures like similarities and interaction levels (so 'equivalent structures' can be used to build structures like 'info', 'connections', 'paths', 'bases/foundations/standards')
            - relatedly, finding the cases where 'assumptions are correct' is useful to identify interim spaces where theyre incorrect, just like finding the way that a structure can be true/false is useful
                - for example, 'dont look directly at the sun' is a 'first principle' that would be applied as an absolute fact that can be assumed, but in reality, looking at the sun can be useful in some cases and in some ways

    - identifying useful structures like new graphs which would be useful to identify (even if only identified incompletely) for useful intents like 'identify equivalent alternate variable/function sets on different complexity levels'
        - for example, 'solving all problems in one step' would require 'computing every problem/possibility/connection/etc', which is suboptimal and probably not possible in some circumstances, but 'solving all problems in two steps' would require slightly less, such as 'computing all core structural interface interactions (like all structure interactions such as overlap/specification interactions)' and 'converting to structural interface' (or some similarly reality-covering set of functions and a function to convert a problem to/back from that format, such as 'compute all variation sources' or 'compute all solutions in a high variation specific problem space')
        - this 'network of n networks of intents (and functions fulfilling them) that are usable to solve problems in n steps' is useful to identify as it can guide the design of interface queries and also future queries to fit problems into a solution in a n-step network
        - to identify this, I was thinking about vertexes and variants of them which could be used to solve problems using one variable (like one interface, or one node of a network) or similarly one function (like a core function such as connect), then connected this to graphs, then noticed that a graph of n-step solutions/intents would be useful to identify, then I connected this to problem/solution structures like problem-solving intents
        - identifying 1-variable/function sets that map to n-variable/function sets is useful to identify (same for other metadata than variable count, such as added/constant/embedded variables) bc they are equivalent alternates on some useful metric like 'complexity'
        - relatedly, identifying highly 'absorptive/attaching/moving/using substances (with high requirements)' is potentially useful for intents like 'neutralizing a substance/pathogen' bc of the high variation of these variables (such as identifying 'heavy metal chelators' as probably useful against other harmful substances/pathogens bc of the variability they can apply)
        - relatedly, identifying 'functions with one point of intersection with a data set' (or 'functions with one point of intersection with a data set and minimal angle similarity with the data set') is similar to solving the problem of regression (identifying a perpendicular line of the data set is related to identifying a parallel line of the data set bc maximal differences can be applied to the incorrect perpendicular to create the parallel)
        - relatedly, identifying the 'minimum nodes in a network' required to create functions like 'generalization/specification' (and other interface functions) is useful to identify networks that can likely solve other complex problems (once a base solution is found to either generalize or specify, or similarize/differentiate, or stabilize/vary, etc)

    - identifying useful structures by identifying variation sources ('high variation subsets') that are relevant in their reflection of other relevant differences ('sufficient variation to differentiate functions') by applying differences to structures that are useful in other intents ('high variation subsets' are useful in intents like 'identify functions indicated by a subset') such as a 'specification' like a 'specific useful variant' (a 'high variation set that is useful for filtering possible functions to very few possible functions'), once a related intent is identified that could use that specific useful structure in a new way such as 'filtering functions, once a high variation threshold of a subset is reached', then identifying problem-solving structures to implement/fit that structure into a set of workflows (like an 'index' or 'identifying function')
        - for example, 'identifying subsets with enough variation to differentiate a function uniquely from other functions (filtering out other possible functions to explain it)' is a useful intent to solve the regression problem
        - how I thought of this was first thinking about maximally different functions in a range, then identifying cases where two maximally different functions would have similar error values for a particular function and what case would explain it, such as where a more correct solution function was at their midpoint, then identifying the similarity of that to other workflows, then identifying a related intent of regression as 'resolving an interval between certain data points to find their connection function across that interval', and how having at least two local subset functions in 'maximally different subsets of the function' would be useful, if there was an index to identify 'possible functions indicated by a specific pair/set of local subset functions', and then identifying the similarity of that to other workflows, then identifying a related intent of identifying the highest variation subset of the function as particularly useful for filtering out other possible functions, then identifying a new structure (a 'function to identify subsets with sufficient variation to differentiate/select the function from a set of possible functions indicated by that subset') as a useful function/index to identify (indexes like 'variation required to identify this range of functions in this function parameter space', and a function to identify data points with that variation in what structures like a particular value of locality/adjacency of that subset)
        - this is useful bc it applies differences in relevant positions until a new useful structure is found, which is one way to identify new solutions ('change a base solution', where 'maximally different functions' are a base solution structure in this workflow application)
        - relatedly, the idea of 'immune-boosting (with substances like turmeric/withaferin)' or 'inflammation decreasing' is useful as a complementary structure of 'low-dose exposure' as a possible source of immunity to pathogens, similar to how 'applying vaccines in the morning' applies an optimization structure in the 'immunity' problem space
        - relatedly, the incorrect assumption of 'absolute usefulness' given a structure that is useful such as 'input/output sequences' (with known errors, like 'misidentified type' which can be corrected by a common function like 'change position') can be offset and made more correct by applying interface structures (embeddable variables in the sequences such as applying 'direction changes' and 'position changes'), similar to how a 'area of missing data' in a regression function that uses sequential/adjacent info can be offset by a general function to identify non-local connections to make a solution more generally applicable
        - relatedly, the common structure of 'several different changes' that need to be applied to 'default assumptions' (defaults like an 'over-focus on recency/extremity/locality') in order to find a correct variant frequently includes errors possible with interface structures, like 'localized info' or 'isolated info' (similar to how any connection can be created by some connection type, but some connection types/other connection metadata are more common, useful, or stable than others), such as how 'absolute lack of meaning' can seem correct after a 'recent/extreme/local meaningless event', but the correct variants include:
            - 'meaning can be created', 'meaning results from integrating/connecting localities', and 'meaning is not required, given structures like randomness', and 'there is always a meaning of some structure (even if its a copy or a lack of structure)', and 'randomness is not absolute, just like meaning isnt', which are several steps away from the 'default connection' resulting from applying various biases
        - similarly, identifying structures like 'probable incompleteness' to identify related structures like 'variation possible in alternate solution functions by applying different completion structures' is useful
            - for example, identifying structures like "'adjacent linear points' (incomplete structure) as well as 'non-adjacent linear points' (incomplete structure) indicating the same linear slope (complete structure)" are an example of a 'in/completeness structure set' that doesnt indicate 'other solution functions that should be considered with equal/similar probability' (assuming no other sets contradict the 'equivalent linear slope')
            - a network of these in/completeness structure interactions is useful as an alternate to 'subset contradiction resolution structure networks' (networks to resolve local subsets (or other structures that should reflect each other like sequential patterns and densities in a data set) that contradict each other or are otherwise difficult to integrate into one function in a data set)

    - identifying useful structures like 'position' which allow other intents like 'identifying interim sets (between positions)' which act like bases for different change types and therefore different workflows ('position' is a useful base to start applying workflows at, which 'change position' or 'connect positions' once the positions are known)
        - for example, regarding the problem of 'partial spaces (such as a partial dimension)', the attributes that differ between different dimension types (impossible/undefined dimensions, lack of dimensions, imaginary dimensions, topological dimensions, set dimensions, integer dimensions, prime dimensions, etc) likely contain the variation necessary to find the set of differences possible in partial dimensions, such as a space of graphs created by attribute differences (like how 'prime', 'area', 'differences in angles' are allowed in integer dimensions but not necessarily allowed/defined/required in all dimension types), as creating a space of graphs with different sets of these attribute values is useful to identify sets of variables that can be represented in partial dimensions, where these 'interim graph spaces' between dimensions known to be limiting/surrounding/adjacent are useful as ways to find a space where a definition (like a definition of a partial dimension) can be allowed/required/defined/valid
            - 'finding spaces where all known definitions hold (continue to be valid and differentiable from different defined structures)' and 'finding position of spaces in relation to each other by similarity metrics (like conversion steps)' is a useful intent to find new spaces where problems can be solved
        - similarly, identifying a dimension where some attribute is reversed is useful to identify new possible variables, such as a dimension where irrationality is mapped directly to some other opposing variable, so that the filter separating irrational and rational numbers can be reversed completely (so the types are exactly indistinct/ambiguous, rather than exactly differentiable by that rationality evaluation function), or conversely, reflected/preserved in some way
        - relatedly, given that 'number types' map to useful attribute sets (abstract concepts), such as how 'primes' map to 'uniqueness', workflows can be constructed based on these number types, such as 'primatizing a problem space' by 'identifying all the unique differences which have the least in common except some unit', similar to how 'matrixes/sequences/progressions' can map to 'sequences' as a useful structure to base a workflow on and 'convergences/ranges' map to 'limits' and 'averages/logs' map to 'bases' and 'derivatives' map to 'changes'
        - similarly, identifying 'equivalent alternates' allows reducing that set to a point on a graph ('any value in the set, randomly selected' is indicated by that point on the graph) so connecting these 'equivalent alternate sets' with core functions like 'change/connect/differ/reduce/format/map' reduces most variation to "queries on a graph where nodes indicate 'equivalent alternate sets'" (queries that will likely reflect the connections between equivalent alternates themselves, so the graph or variants/subsets of it will likely be found in the set of equivalent alternates as well), and finding some function like a rotation that maps these equivalent alternate sets to each other is similarly useful, given their likelihood of being mappable if not equatable (there is still a set of multiple alternatives that cant be reduced, but there may be some graph where they can be represented as a point with different position related to other equivalent alternate sets, related by different angle sequences (or other structures which indicate the difference in the sets) applied to connect the sets or find some angle where the sets seem 1-to-1 mappable or otherwise equivalent, similarly identifying a graph with a 'common center' between equivalent alternates is useful as they can be represented by the center/radius rather than the sets)
        - relatedly, identifying a 'set of equivalent alternates (or similar alternatives or ambiguously similar alternatives or a set of structures that are different in some way that is not yet understood, such as differences in output of a test of each structure, thereby requiring the test)' is an intent that, when fulfilled, enables specific workflows like 'trial and error' to be applied, indicating that the set has not been filtered before and contains a new difference, therefore the test has to be different as well (solution metrics should be changed for each actually different/new problem, so the 'ways that solution metrics can change' indicate the 'ways that their mapped structures (new problems) can change/be solved')

    - identifying useful structures like insufficient structures for some problem-solving process like 'fulfilling a solution metric' and identifying differences from this insufficiency which is a type of suboptimality
        - identifying what is not sufficient for general problem-solving, like a set of structures like 'an adjacent function, an opposite function, and a reason to avoid adding other functions like "everything can be traversed with enough adjacent or opposite changes"' which has a known/identifiable error of 'requiring many iterations to find all relevant structures' and 'missing all the changes that are in between adjacent/opposite changes without iterating every possibility (and other errors like having patterns of movement that miss other patterns of structures without iterating every possibility)', is useful as these changes are useful for solving problems without testing every possibility, so its clearly insufficient for solving problems in general, even if adjacent/opposite structures can compose other structures eventually
            - to offset these errors, a 'structure of relevance like meaning/usefulness/intent' is useful (or an alternative structure similarly useful in deriving this, like 'interfaces' or 'vertexes' or 'concepts')
            - relatedly, making this problem specific, such as 'can these change types like vectors, rotated at these possible angles, traverse every point on a structure (at their endpoints), points of a certain size making it a finite number of points' leads to identifying more specific interim structures, like:
                - 'identifying a variant of the problem that is more provable and applies to/covers the original problem as the original problem is a subset or variation with irrelevant differences that dont determine the angle/missed point difference'
                    - 'identifying "areas/centers/patterns/types/simplicities/units/difference ratios/other similarity structures" of problem variants that are guaranteed to be traversible at all points, and navigate in the interim spaces of problem variants to prove at least one uncertain example'
                        - 'identify the "first problem where the answer becomes unclear" or the "first set of multiple unclear problems with different solutions sufficient to find variables of the solutions" or "identify a solved unit problem and a solved more complex but more solvable problem and apply variables to their solutions to identify an interim complexity solution"'
                    - 'identifying applicable problems to solve other problems (when this problem is solved, this other problem will also be solved or more solvable)'
                        - 'identifying a simpler but equivalent variant which reflects the same variable connections at a smaller scale or otherwise simplified'
                - 'identifying patterns in angle sequences and the points where they begin/stop differing or increase/decrease in difference from repetitions of known sequences'
                - 'identifying what is not required for solution success (traversing a new point every angle) and what is required (traversing the full set of points, traversing with enough regularity that it can be completed before the differences run out)'
                    - the important variable is the 'comparison' of the difference in 'regularity/interval of the new points' and 'differences allowed/required by remaining possible sequences', which need to be in a specific ratio to succeed (one needs to be greater than the other)
                    - once a 'ratio between relevant differences, such as opposing differences, where one of the differences would be useful if it exceeds the other' is identified, the problem is often solved (a 'ratio representing a difference in a similarity of the similar opposing forces')
                    - a related intent that can generate this same result is:
                        - 'identifying the determining variables and their related intents ('find the first missed point', or alternately 'traverse new points every iteration, or regularly enough that all points will definitely be traversed, without a decrease in this interval that would converge at less than the number of points') and the determining variables of those intents (where the possible angles can produce enough differences to differ from repeating the same pattern, thereby covering new points every time, but also enough similarity to stay within the structure'
                        - similarly, a comparison in the 'interval of new points' and a 'decrease in the interval that violates some convergence' is another set of useful structures to compare
                        - this is bc a useful structure (a 'sequence with a seemingly useful interval of new points') has been identified, so identifying whether that structure is invalidated by some opposing structure (like limits on next possible sequences that make it required to not traverse every point) is useful to identify if that structure can be iterated until the full set is traversed
                        - relatedly, identifying structures/variables that create 'requirements to miss points' are useful to identify, similarly identifying 'sets of opposing structures in the problem space' are useful to identify, to check as a default set of 'possible solution components'
                - 'identifying structures of additive progress toward the solution, such as different sequences that traverse a lot of new points at an increasing interval with some sequence' is similarly useful
                    - similarly, 'generators of differences between sequences that vary in their traversal ratio' are similarly useful to identify
                    - comparing 'sequences with different ratios of new points traversed' is another useful structure set to compare
                - 'identifying a "trial and error" iteration number of angle sequences where the answer would likely be clear, aiming below that range, and finding sets of steps that fulfill the lower ranges'
                - 'identifying sequences of angles (generatable with the possible angles or not) that miss a point on the structure and checking for repetition of these point-missing structures and repetition across the points that are missed and difference from those generatable with possible angles, or alternately identifying points that are regularly missed across sequences generatable with the possible angles'
                    - 'identifying examples of possible angles (and angle sequences generatable with those) that would and would not produce a missed point and determining relevant differences to those examples'
                    - 'identifying possible sequences, then out of those, identifying required sequences of angles to traverse a point missed by previous sequences'
                        - 'identifying whether there is always an opposing point-traversing sequence possible for every point-missing sequence, as the variation generating both sets is equal, and whether these sequences are possible to connect (once the point-missing sequence occurs, can the point-traversing sequence that opposes it also occur)' ('is there a solution structure to oppose every problem structure')
                - 'identifying self-interactivity/similarity of various probable sequences (how often it intersects with its previous points) which is a determining variable of how it can differ from its previous points'
                - 'identifying relevance of other variables like the traversed structure variables and the starting point (does it change probable sequence attributes or missed points)'
                - 'identifying thresholds in adding variables where the problem becomes unclear, and solving for sets of variables, and the function to merge solutions in these sets'
                - 'identifying co-occurring and mutually exclusive sequences' as a useful intent for other intents
                - 'identifying the point where, if a sequence can miss a point infinite number of times, indefinitely, without any identifiable correcting structures to oppose it, what point does that infinite capacity for missing a point become clear'
                - this example problem is just specific to direct focus but general enough to identify/allow variables
        - relatedly, identifying problems like 'determining whats inside/outside a box' that reflect enough reality and enough variation to be useful to derive other functions (like identifying possible signs of change in higher dimensions by determining signs of change in available dimensions like 3-d signs in 2-d)
        - relatedly, identifying 'reversibilities' as a useful base function of a network to test other structures in like 'assumption sets leading to errors that are dead-ends (irreversibilities) by first identifying those errors so they can be avoided' and 'similarity set that is reversible' (like moving in specific ways to avoid known dead-ends like assumptions that lead to removing info that cant be retrieved or deadly errors that invalidate all functions or avoiding over-similarizing with low-info functions)
        - relatedly, identifying a 'standard-standardizer' to identify 'standards that should be similar/different or which should fulfill some standard' are useful to identify, such as 'double standards (that should be equal)' 
        - relatedly, identifying 'missing connections, which probably exist but are not known (like the full set of interactions of a structure with other structures in its interaction level)' as useful targets for explanatory variables of processes that are similarly incomplete or not understood

    - identify useful structures like variables of 'filters which reduce computation requirements' are useful to identify as optimizations of a solution-finding method
        - similarly, other filters include 'compounds that interact with all examples of the condition (or compounds that are required)', as such a connection is unlikely to be coincidental/random, and 'compounds which are inhibitable' which are more useful targets to identify, as there is already a function that can be applied to make that a solution, so they have reduced computation requirements
            - these types of variables may be embedded in or missing from the data set, such as how 'interactive compounds with every data point (in a data set of compound structures and some useful attribute associated with them)' may be reflected in some interim variable of the changes in between the input/output, or it may occur after the output is determined
            - relatedly, identifying '1-to-1 mappings' as structures unlikely to be random is useful to identify probably useful structures to apply when decomposing a non-random data set
            - relatedly, identifying the function types/differences (like how a random function is usually useful in some position in a function network to generate that function given the noise usually present in data or in real systems or in changing variable interactions) that normally interact to build 'functions in between the set of all simple/pattern-compliant and random functions'
        - similarly, identifying 'subsets of the data set that generate similar functions, which are shifted transforms of each other' is likely when the probable solution range is trivial to identify and reflects the shape of the average/solution function, meaning the upper/lower bound shapes are similar (a stack of similarly shaped functions, which reduces the problem to finding the limits and average of that range, as the solution function is just the average of the other similar functions when the average is the definition of the regression line)
            - this is possible by identifying one example of a function connecting points in local subsets and then looking for similar functions following that pattern after applying a transform like scale/shift
            - by contrast, when the upper/lower bounds are different, or when the solution range isnt an area at every point but has some subset where it has only one point (every function in that area must cross that point), there wont be as many possible similarly shifted functions connecting subsets of the data set, and fewer points will be required to find those connections if they exist
            - these represent cases where the solution is identifiable by checking for these specific cases encoding some similarity/difference and once identified, can be computed with a specific algorithm to identify the limit of a similarity/difference (can it be iterated or does it have a limit within the probable solution range)
            - 'overlaps between functions to identify/filter different cases' are useful to identify to reduce overall computations
            - these cases have features like 'reduced computation requirements' where a similarity/difference is applied locally and globally, or in some pattern, or on another vertex that reduces computation requirements bc of the similarity/difference encoded which applies across those vertex or interface variables
                - relatedly, identifying other case types, such as 'when a solution function doesnt intersect with the data set', is useful, as there are often indications of this case that involve less computation, such as the 'variation of points around but not intersecting with some line like an average'
                - identifying the function to identify info required to identify most different cases is also useful to identify
        - identifying other cross-interface cross-sections of a problem set is useful, such as how 'function similarities' are useful to identify as '1-to-1 mappings' (such as how a function whose points are more distributed is mappable to that function), which are likelier to be quantum entangle-able and therefore likelier to be quantum entangled (at what point in the mappings would an entanglement break down, with which error types or similarity/difference types/sequences/other structures), and how strong are these connections across similarities (are there stronger 1-to-1 mappings than other 1-to-1 mappings, do they exert any force like 'changing probable adjacent functions of a function' that differs such as a 'hub function that is repeated more by being more connective/interactive', is there a useful 'map sequence to default to' in order to retain the most info when some connections are broken), what impact will agent-driven entanglements have on known variable interactions (how variables can be combined), what connections should be returned to for a function with missing info in order to be changed by variables so that the function can be differentiated again
        - relatedly, identifying the 'most reduced set of solution components' vs. the 'most reduced set of base/interim solutions' vs. the 'most reduced set of solution variables' vs. the 'most reduced set of solution filters' are different intents that are equivalent alternates but still useful to identify the full set of and variables of and other interface structures of, bc these are unlikely to remain in their 'most reduced state' as interaction levels change, as theyre abstract types of structures which will still be relevant regardless of the interaction level but adding to them will still be useful
            - similarly, identifying future versions of a solution component will likely be adjacent/computible from known solution components and interface structures of them, so 'applying changes within these types to generate newer higher variation examples of the type' is a solution automation workflow (rather than 'only identifying new workflows by connecting these variables across types, when required or adjacent') just like 'connecting these new variables to other types to identify new variables in other types' is a workflow, to apply a new interaction level of changes across types
            - this is bc it will likely be useful to identify new solution components by identifying/applying change patterns and other structures of solution components (like 'difference ratios encoded in a solution component'), and is possible bc these types are likely to be unlimited in the variation they can contain given their abstraction and permanent relevance
        - relatedly, identifying all the interactions which are likely/unlikely to be useful, such as 'embedding find queries' rather than 'sequencing find queries' and 'avoiding simple patterns like alternating sequences except where high variation structures are being varied/alternated' and 'identifying the few useful structure interactions that are useless, which is less likely and more filterable' and 'most methods of minimizing cost are likely to create future additional costs' and other probabilities is useful as a 'probably/approximately/commonly correct rule network' to apply as a 'base network for future solutions'
            - relatedly, as mentioned elsewhere, identifying 'probabilities as related to commonalities/ratios' is useful for 'filtering' intents bc identifying 'improbable (as in less common) subsets' (such as 'useless combinations of useful structures') is useful as a reduced set to filter
        - similarly, identifying 'info that would be identified anyway (such as adjacent or required info), if another function was identified/implemented/applied' is useful to identify in order to avoid identifying all info or re-identifying info
        - relatedly, the workflow 'change a base solution' is more useful with specific variable types (like 'trivial changes', or 'embedded variables' which are 'required' and also which 'require the base in order to be applied')
            - why are 'adjacent/required variables' so often alternates? bc they both have an element of 'forced interaction'

    - identifying useful structures with useful functions (like 'find a way to increase usefulness of an error/suboptimal structure') applied to useful structures (like 'right angles') is useful to identify
        - for example, the 'right angle' is a specific useful structure on the plane created by 'all angles intersecting at a common endpoint', just like '1-to-1 mappings' are sets of 'linear/simple/useful connections between different structures' and other specific useful structures on other networks/planes are identifiable
        - the 'right angle' can be identified as an error structure in some positions, where it connects overly similar variables (and is therefore not useful or an error, such as by being overly simple), but otherwise is useful for identifying obvious differences, independence intersections, etc
        - similarly, the right angle can be made more useful, such as by connecting two planes or high variation variables like concepts, in which case it becomes a very useful structure despite its simplicity (a vertex between two perspectives which connects them)
        - identifying changes that make these specific useful structures interact, such as by overlapping at a common center, makes it possible to connect them (such as identifying all the specific useful structures in a plane and then connecting them to useful structures on other planes, by aligning or orthogonalizing the planes)
        - the 'interactivity' of the structures is what makes them more useful (as in more comparable or more connectible as in sequence-able)
        - 'making a problem/solution interact (such as by overlapping or intersecting them)' is similarly a useful solution automation workflow, that is derived from this thought process about 'error structures like 2-d variable sets', 'variables of 2-d variable sets like over-simplifications to generate all the 2-d variable sets', 'positions in the set of all 2-d variable sets that could be non-errors', 'patterns/similarities in this set of non-error positions', 'specific 2-d variables that can be useful', 'right angles', 'ways that right angles could be errors (they are 2-d and can be over-simplifications)', 'ways that right angles could be non-errors (they connect to other variables like their common endpoints, magnitudes, etc, and specifically when used to connect planes/networks/other high variation variables, which is complex enough that it might be adjacently usable to solve problems)', 'maximally different variants of right angles (such as 1-to-1 mappings, a similarly useful structure in the map interface)', 'useful variants of right angles', 'useful connections between useful variants of right angles (alignments/overlaps)', and 'useful functions creating those connections and what positions those functions would otherwise be useful in (intersect)' (similar to how any line can be useful in the regression problem space if it intersects with the data set range bc it reveals useful info about the optimal function even if its very incorrect)
        - relatedly, a 'network of simple changes' should be positioned near a solution-determining function, to catch all the approximate/adjacent solutions that might have been reached by some suboptimal algorithm (rather than missing them), as its useful to determine structures around a solution like 'solution metrics', 'solution generators', 'adjacent solutions', 'solution descriptions', and 'simple changes that can convert a suboptimality into a solution function', a network of structures which may be high variation enough that a corresponding network around a problem might not be necessary
        - similarly, identifying/applying 'errors of perception' created by 'neuron limits' (such as how the 'vision' function optimizes for some metrics such as 'summaries, which merge adjacent structures') and related structures like 'adjacencies/overlaps', such as how a structure can seem 'merged (like a line) when distant' but 'close up it is clearly separable/distinct', are useful to identify (similarly other variants of this are useful to generate, such as when very different structures are adjacent/distant, do they seem different/similar, and when very similar structures are adjacent/distant, do they seem different/similar, and whats the distance where all structures seem the same, and what difference in distance is often useful to differentiate these cases, etc and the same for other variables that can create 'errors of perception' like angles/blur, as mentioned elsewhere)

    - identifying variations of core structures (re-examining core structures to identify alternate variations of them which are useful, as is likely) such as 'variations of vertexes' or 'variations of factors/products' or 'variations of averages' and the 'overlap across these variations' as a similarity which leads to relevance of these structures (the usefulness of finding variants of averages like vertexes, and variations of those variants of averages) is useful for identifying their interactions with other structures, given the variation they encode (vertexes encode adjacent transforms from a common point, so they can represent a simpler variant/format of a function)
        - for example, 'identifying vertexes' in a data set is 'identifying the common points between point sets' which is similar to the intent of 'identifying averages'
        - what does it mean that a set of right-angle vertexes (where two vectors are joined by a right angle) can explain most data set interactions, if such a pattern is found? 
            - it means that those two dimensions of the vector magnitudes explain the variation from the vertex common points (the point that the two vectors have in common), where the 'vertex common points' form an alternate similar function as the solution function, with two parameters of variation away from the solution function
            - its useful to know a simpler function that can be converted into the more complex function with a similar change, as the simpler function seems like a useful base function given its simplicity, where other resulting complex functions can be generated from it which are probable variants of variable interactions
            - this intent of 'finding vertexes connecting points' removes an 'independence' assumption from structures like 'sets of input/output sets' but that is reflected by reality as well (adjacent sets can sometimes influence each other in reality, especially in non-volatile systems where both input/output are adjacent, and it is likely to find systems where adjacent points are used to generate other points around it, similar to how identifying averages is useful in that points might tend to gravitate around the average to some radius, so finding 'similar angles connecting points' is not a false relevance structure in all cases)
        - 'variants of vertexes' like 'a force, an opposing force, and a smaller opposing force of the opposing force' are similarly useful to identify, as larger components of interface queries
        - similarly, 'vertexes' act like 'filters', in that the selection of the 'common point' and the 'angle' and the 'magnitude of the second vector' describe all possible interactions between two vectors with a point in common (and thereby identify all possible opposing forces and possible connections between those opposing forces and the original forces, converting it back into the original force), and so these selections determine the selectivity of the vertex in what interactions it can describe
            - a 'right-angle reality (where all connections are orthogonal)' is over-simplistic and doesnt adjacently allow for other descriptions of connections, but applying similarities (like right angles) to find other similarities (like angle patterns such as 'repeated right angles' or variations in angle patterns such as 'variations around a right angle') can be useful
            - vertexes are like filters in that they allow all possible connections/similarities in some angle connecting similar/comparable objects, and so do filters (applying some change to reduce a structure, like an opposing vector in a vertex of two vectors)
        - a 'vertex between simpler/complex function spaces which connects them' is likely to be a useful vertex in identifying functions in the uncertainty space between known limits (like 'avoiding randomness/linearity')
        - a 'line/function/vector set across these function spaces' might be a more useful format of a function than other formats
        - a 'sequence of these function spaces' might be possible/useful in determining ways to reduce the interim space of uncertainty between randomness/linearity, though 'selecting/identifying the simplicity space' is not trivial (which simpler functions are generated and connected to which complex functions)
        - identifying 'reasonable vertexes' (such as 'reasons for a lie and reasons to oppose the lie' as indicated in 'vertex_variations.drawio.svg') are useful to identify as structures of falsehood to check for in variable interactions and similarly 'similarity vertexes to truths' (such as the similarity of a lie and a truth, such as an adjacency or intersection) are useful to identify, and similarly, 'similarity vertexes to concepts' (like how a lie might be similar to 'multiple/variation' or other conceptual structures associated with truth) are useful to identify
        - these 'useful vertex sequences' are similar to how a 'cross-section of a structure that intersects with most of its variables' is a useful structure to describe/determine the structure, so finding 'sequences of vertexes' that are 'likely to intersect with variables' or 'likely to intersect with cross-sections of a complex structure' is useful, just like 'identifying a non-one dimensional slice (of height greater than zero) of a function is determining of that function' and how 'identifying sequences of vertexes that lead to false structures (like irrelevant connections between small subsets that are too simple to reflect real complex structures which leave out the bigger, truer variables)'
        - similarly, its possible to identify useful filters of similar structures, like how the 'non-overlapping sections of equivalent alternates' are useful to identify their differences (as those are the primary difference in overlapping sets), so finding non-overlapping structures is a possible difference to look for when 'finding differences, to find similarities supporting or connected to those differences' or 'find new differences in a structure that is not clearly equivalent alternates, to check for that'
        - similarly, identifying the differences possible with different selected cross-sections is useful to identify like identifying differences possible with different selected subsets is useful

    - identify useful structures like overlaps between useful structures like workflows and connect them to workflows (such as the variants of variables of these workflows that have useful connections to identify (insights)) and connect them to 'new graphs' so the full impact/meaning of the new useful structure is identified
        - for example, given that 'anti-fungals frequently also have anti-cancer activity or activity against other negative compounds', it can be inferred that compounds with 'broad activity against many compounds' are useful to identify as possible solutions to other problems
            - this is inferrable using a workflow like 'identify structures similar to problems/solutions and connect those instead (find fuzzy/wobbly connections, which is useful bc of the similarities at either endpoint of problems/solutions)'
            - similarly, as mentioned elsewhere, other compounds can be inferred using this workflow, such as 'poisonous substances which kill host cells can be useful against negative cells like cancer cells' and 'tumor markers as tumor inhibitor treatments' (given their adjacence to negative/problem structures like tumors) and 'compound that have complex regulatory functions (like fixing inflammation or circulation or demyelination or calcification or metabolism) are likely to have other useful complex regulatory functions' and similarly 'compounds that treat side effects of cancer as possible alternate treatments' or 'compounds that interact with known important variables like inflammation variables such as NF-κB' or 'compounds like anxiolytics that treat inputs like stress of other conditions' or 'side effects of cancer treatments as alternate treatment targets', and 'compounds which treat conditions like demyelination/high cholesterol involving the same processes like phosphorylation or pathways like Wnt pathway' (bc 'fixing errors like deregulation in overlapping processes' is likely to be useful for fixing errors in other processes, so finding 'overlapping problems/solutions or their connections/components/variables' is useful)
            - its also inferrable using a workflow like 'change a base solution (such as a known medicine like an antifungal and see if it works against other problems)'
        - identifying the 'overlap' of workflows like 'find fuzzy connections between problems/solutions' and 'change a base solution' is possible bc they have a similarity in that they both involve changing problem/solution structures in different ways for different intents
        - this is also bc compounds with activity against one type of problem are also likely to have activity against another type of problem bc the original host of the compound is clearly capable of changing to solve problems as it contains one solution and is likely to have encountered other problems, which is the 'probability of a similarity' in 'containing other examples of a type (like counteractive compounds)' and 'interacting with other examples of a type (like problems)', so applying 'other examples of a type' is a useful similarity to apply across structures
        - similarly, compounds with 'specificity/selectivity of activity' are useful for targeting one negative factor to remove/reduce, when such a condition occurs, which is more rare but occasional is very useful, like where a condition has just been triggered and hasnt reached broad/systemic activity yet
        - this indicates that there is a 'useful mechanism' at either end of a 'high variation spectrum variable' (extremes typically result in specificity/specialization/differentiation, and extremes in a high variation variable typically result in equivalent possible functionality)
        - the connection between specificity and differentiation is useful to identify, just like identifying the input of extremes as useful to create other differences like specificity/differentiation is useful
        - this is also useful for identifying interim spaces where other useful compounds could be, such as the space between extremely broad-acting substances and extremely specific substances and further filtering this space by navigating the space between compounds that bind in known ways which invalidate them, like being required for another useful process so they cant be candidate treatments or possible substances to avoid, and similarly finding the overlaps in these spaces as more probable spaces that contain solutions
        - graphs where compounds with an attribute are positioned adjacently or in an identifiable similarity like a pattern are useful for finding 'types' and '1-to-1 mappings with other variables aligning/overlapping with those attribute values' (where another attribute also occurs in that adjacent subset)

    - identifying common structures of queries that identify useful structures is useful, such as applying a different info structure set (like 'barriers' and 'false') to identify what is not real (such as 'false connections')
        - for example, the structure of a 'incentivized filter' is attainable by identifying 'info barriers that benefit some agent', such as if there is an 'info network' which agents might find useful to know so they can query it and blocking access so that connections cant be made on this network without paying a fee is an info barrier that would benefit an agent, acting like an info filter that is incentivized
        - similarly, thinking about alternate ways to connect data set points is relevant to connecting the reasons for a point with a structure (such as how road would be a reason for a distribution of points like lights, which is adding another dimension), and this is not particularly different from other methods of identifying new connections between related points, so what is different is identifying structures of false light, such as how far away light needs to be measured differently, taking distance into account, bc its 'received as being in a position after its in that position', so by the time its measured to be in a position, that position is false info, which is useful to identify, and similarly, light can reflect in a way that is false, so a 'reflection of light' might be being measured rather than light itself and similarly a data point might only be in that position bc it encountered a barrier elsewhere, which is useful to identify
        - by applying structures like 'barriers' and 'false' and 'incentives' to standard variables like 'data point connections' and 'reasons for those connections' and 'differences from other connections', alternate methods of regression can be identified (filtering out false info, identifying highly causative variables like barriers, identifying incentivized info and info structures, etc)
        - relatedly, identifying the connection between core structures like 'barriers' and 'filters' is useful, such as how a 'barrier is a component of a filter' and 'can act as a filter'

    - identifying useful structures like 'common structures across solutions' is more trivial when applying 'self-contradicting structures in a high-variation spectrum variable like an interface' (like 'simplifications' of 'complex' variables) at which point identifying other useful structures like 'the commmon usefulness of constant indexes in simplifying complex problems' is trivial and similarly 'identifying variations and variables' of these structures ('variations of constant indexes which might be useful for some intent') are similarly trivial
        - for example, the common structure of a 'solution metric' (used as a standard to filter solutions) and a 'common pattern' (like the difference between 'easy/difficult paths and high input-high output and low-input high output') are paired as 'filtering standards', once 'differences from constant indexes' such as 'a virtue network (where different directions indicate different virtues)' or a 'difference of a reality variant from known reality' or a 'base solution', which are a specific vertex structure ('differences from an index' and 'filtering standards') that is useful across problems
        - I thought of this by focusing on connecting simple structure with complex structures, after thinking about 'variable count' (a low-info but still determining variable some of the time) recently, and wondering 'what was the minimum/simplest number of variables needed to format complex structures, like "most solutions to most problems", arriving at a default brain limit of three variables as a useful example value to test with, wondering if I could format most solutions with three variables, then wondering what 'complexity of structures' was necessary to format most solutions (like a reduced set like a 'specification, a modification, and an exception' which are common sentence types, or a 'story (a query on a reality network), civilization (a reality-variant network where queries take place), and a concept (high variation structure explaining/relating to many/most variable interactions'), and noticing that 'constant indexes' are useful across problems and most solutions involve 'computing a difference from these indexes' (such as a 'base solution' or a 'specific network' like a 'virtue network') and then applying a standard to find a difference/similarity in some related structure (like a 'simple/complex' step on a 'virtue network'), as a useful pair of structures to apply together, similar to generate/filter (bc standards act like filters)
        - then after wondering what the connection was between 'simple/complex paths' (such as a 'right angle with sides of different lengths') and the difference between 'low-high/high-high input/output connections' (such as 'incentives'), if any, as they seem related by a common structure which is an 'angle between structures having a common endpoint' which was a structurality similarity identified by coincidence after thinking about a useful 'index/standard pair', identifying 'high degree-angles (differences) between related structures (similarities)' (having a common endpoint like a common type or common starting point) seems to be the structure relevant to problem-solving that is useful to identify, as a specific structure implementing a 'difference in a similarity', and indicating a possible '1-to-1 mapping' in between 'simple/complex' and 'low-high/high-high input/output connections' that is validated by the vector-formatted definition of those concepts (meaning when 'simple/complex' are formatted using vectors, they retain their difference angle, which is useful to identify 'sequences of formats' that would maintain some useful difference structure like an angle)
        - "finding an index/map and a 'common similarity/difference' from/in that map and its approximate 1-to-1 mapping forming a 'connection to a related high variation concept (like false/true)' or then finding an 'embedded similarity/difference' in those similarities/differences that determine the map" is a useful problem-solving intent to specify with vertexes (like specific interaction levels or systems relevant to the problem)
            - for example, identifying the virtue network as a useful index to run comparisons on, then identifying the difference in the virtue network between 'simple/complex values of the same virtue' (which map approximately 1-to-1 to a related concept applicable to the concept of the network, like 'false/true virtue'), then identifying the common standard embedded structure of those differences, such as the right angle between simple/complex values of the same virtue (incentives)
                - this is useful bc once the differences within/from an index are found/determined, finding 'mappings to high variation interface structures like concepts/structures/standards/differences' are useful and more trivial at that point, which is an example of a way to convert 'info queries to numbers' (using reference indexes like 'constant concept networks' and numerical structures like 'approximate 1-to-1 mappings' between concepts like 'simplicity/truth' or 'angles' between concepts like 'simple/complex')
                - similarly, applying indexes/spectrums/vertexes as 'constant nodes' in a neural network is useful where the nodes around them can vary by functions like 'embedding additional nodes' is useful as a way of representing structures like 'vacillations between concepts like organization/entropy', variables which are useful to model vacillations of because some variables relate to all variables, so that the borders of the network represent absolute limits of these reality-covering variables and queries of the network represent sequences of changes in these conceptual variables
                - similarly, other similar indexes could be useful, such as a vector set of high variation variables (where no mapping is preserved, so any vector moving outward from the origin indicates any high variation variable that is different from the others) and embedded variables indicating the differences across high variation variables, which could be connectible in similar patterns that would be made obvious by similarizing the high variation variables
            - similarly, identifying a base solution as a useful index to compute/apply differences from, then finding solution metrics within that computed difference to apply as a standard to further filter a solution, is a simpler application of this structure
            - similarly, 'differences from the concept network' are similarly useful to identify, and similarly applying variants of these networks, like 'sequences/stacks/sets of concepts and their embedded variables' (as opposed to 'maximally different vector angles from the same origin') and 'variables of their interactions and usefulness for determining useful interface queries and other useful structures' is similarly useful to identify/apply
        - as to the question of whether vertexes can be used in a sequence to optimal degree, such as whether 'finding a commonality, applying high angle differences based on that commonality, and iterating' is useful on its own or requires specifications like 'avoiding intersecting with its original commonality' or 'maintaining a 1-to-1 mapping' or 'intersecting with some constant index regularly to stay relevant to some specific system/problem'
        - relatedly, identifying all the light interactions that can make some truth seem false are another example of a useful constant index to apply, such as how light can dissipate light to make it seem like it doesnt exist, light can seem like the source of light by applying filters to obscure/hide the source of light, so all of these interactions should be checked for
        - relatedly, taking out obvious info (like removing averages/densities from a data set) is useful for various intents like identifying low-variation differences like alternate functions, as 'making something obvious by focusing on it, to make other structures seem relatively false by comparison' is a way to hide other types of light (the less obvious structures), so its useful to apply as an interaction in 'directly mappable problem spaces' like 'regression' as well (answering the question 'when obvious strctures (which could be false, as a way of hiding other info) are removed, what is obvious then?')

    - identifying the ways the core concept determining the 'usefulness of a particular workflow' can also indicate the 'usefulness of different workflows' is useful to optimize workflow filtering/generation functions (identifying ways that it could be false/true, useful/useless, etc)
        - for example, some problem spaces are so random (which usually means 'incompletely understood') which usually implies that 'trial and error' would be useful, that 'brute-forcing a problem' by pouring enough computation/resources to  generate/identify/test every possibility (trial and error) is not likely to succeed or even be possible, bc the generative variables of these possibilities are incorrect, as the problem space is not understood so it falsely seems random (real randomness rarely occurring in real systems, as 'equivalent alternates' dont often exist for long, as there is usually a filter selecting some alternative that is more useful in some metric and equivalents are usually differentiated/specialized at some point), which sounds like its impossible until the implications of 'randomness' are understood
        - therefore even though the concept of 'randomness' is determining of the usefulness of 'trial and error', it can also be a reason not to use that workflow
        - similarly, even though the 'adjacency' of an existing solution to a probable optimal solution (given an identifiable solution metric allowing a probable solution range to be identified) is a reason to select 'change a base solution' as a workflow, the 'adjacency' is also a reason not to use it, in cases where the similarity of the adjacency is false or can be changed
        - this is possible bc every sufficiently abstract concept contains enough variables that it can contradict/limit/invalidate/filter itself, so finding these ways and applying them where that concept is used like in "workflow filtering/generation functions" is useful

    - identifying a connection between 'similarities' and 'reduced sets' is useful to identify different structures that can connect them, such as structures like 'averages' of related concepts like 'probability' which are useful at reducing sets given their coverage
        - for example, why is a 'probable solution range' possible to identify and why is it useful? bc it represents a 'similarity' which can create a 'reduced set that still has multiple items (rather than one)' that fulfills the concept of probability (a filtered subset that is more probable bc the obvious errors have been removed but there are still multiple solutions in the set, making it a probable set rather than a totally filtered set leaving one possible solution, so its possible but not guaranteed, making it probable)
            - other implementations of 'probability' applied to identify solutions could be similarities between subsets based on 'frequency' or 'adjacency' or 'average' or some other structure (rather than applying the structure of 'area' as a structure of probability indicating a 'reduced set of multiple possibilities')
            - similarly, its useful to identify a range bc solutions often dont vary so much that a range would be useless
            - relatedly, combining these is useful to identify interactions of structures that implement/determine probability
        - this means "because probability can take the form of an 'area'", its useful to identify 'probable reduced sets in the form of areas' (identifying a 'probable solution range/area' as a solution format)

    - identifying useful structures such as 'overlapping structures with a common base' which are useful for identifying 'variables' of these structures (like 'variables' of 'similarities') and apply these structures to useful structures like 'high variation variables like interfaces' (and applications of interfaces, like 'causal meaning of a similarity')
        - for example, the 'causal meaning of a similarity' could be that one structure causes the other adjacently/directly, therefore they are similar
        - other 'meanings of a similarity' could be that one of the structures is 'default/common/adjacent/required/useful and therefore there will be similar structures to that structure
        - the 'potential meaning of a similarity' is the set of possible interactions of that similarity with other structures, up to its limits such as its 'uniqueness limit' (how unique its interactions are compared to other interactions)
        - the 'ultimate meaning of a similarity' is the set of overlapping meanings, where their common center/limits can be identified, and therefore the variables of this set of meanings can be identified, since these meanings will overlap on some attributes but will have a common base, similar to how 'reality-covering concepts' overlap but have a common base, and therefore cover most if not all of reality
        - these 'iterations of interface structures' are particularly useful to identify similarities/differences in bc of their composable functionality where they can be combined to create interface queries connecting maximally different structures like problems/solutions

    - identifying useful structures like 'alternates' (such as how 'overlapping different types' can be an alternate of 'inputs that can be outputs and vice versa, causally') by identifying useful insights and then identifying useful structures to apply those insights
        - for example, identifying that an output could be an input could be identified by other functions/intents, such as 'check for symmetries (by rotating outputs around a variable to see if theyre also inputs)' or 'check for multiple type sets that a structure fulfills (rather than one type)'
        - identifying insights like 'inputs/outputs can often be switched causally and otherwise' is useful to identify alternate structures that can replace that info by applying other structures like 'symmetries/multiple/type'
        - inputs/outputs have an 'overlap' in some variables like 'cause' (bc of multiple possible configurations where inputs/outputs can be equal/unchanged/preserved in a function or where outputs can also inputs by some other process creating those outputs which are used by the original function, and different input/output structures like sequences/cycles), even though they are extreme opposites in the variable of 'adjacently associated structures of a function'
        - so by avoiding assuming 'one type' vs. 'multiple types', an error of 'assuming outputs cant be useful as inputs and vice versa' can be avoided even without identifying the insight
        - similarly, identifying absolute truth structures is sometimes possible, just by identifying ratios or thresholds, such as where if 'reasons having a count above a threshold n' can be identified to indicate some statement is true, that usually is enough to infer if its true (as 'reasons to think a statement is true' can increase in a simple variable like the 'count of reasons', as the 'truth' becomes more stable and therefore more depended on and more necessary to maintain), though its useful to identify cases where 'false statements have more reasons to indicate theyre true than a true statement, such as where falsehoods are useful for a larger group'
            - similarly, identifying structures which require/guarantee/imply truth/falsehood are useful to identify, such as how if a statement is locally true (like a one-degree connection), then fitting it into a system of other local truths makes it more true
                - similarly, identifying whether a statement aligns with iterated inputs/outputs of that truth (its implications or requirements) makes it more true
                - similarly, identifying whether a statement aligns with a core abstract connection makes it more true
                - similarly, a statement without filters/limits/specifications/conditions/requirements is unlikely to be true, as almost every statement has a falsifiability (a way that it can be made false)
                - similarly, a statement that fulfills multiple of these structural truth metrics is less likely to be a false or adjacently falsifiable structure
                    - falsifying it, meaning 'making it so rare as to be nonexistent' or 'finding a far more determining variable like an interface (such as how mutations seem like a determining true variable, until the far more determining variables of "DNA repair" or "neutralizing/compounding genes" are identified, so that the original variable seems relatively false, given this far more determining possibly orthogonal variable which can contain/embed it'
                - however, identifying 'false structures of truth (ways that an apparently true statement can be false)' is similarly useful, such as how identifying 'variables in between an apparent similarity/connection' is useful and can contradict the similarity/connection, indicating other possible causes in between that are causally separable from the original connection
        - relatedly, 'identify' is related to 'determine', and therefore 'determining variables' are related to 'filter' intents, such as how 'more determining' variables like 'powerful variables' make better filters in the sense of filtering a set more reductively, leaving fewer options
            - identifying 'types' from the 'overlaps/similarities of different filters' is also useful for identifying what 'type' of structure (such as more 'common' vs. 'rare' structures, where useful structures like ratios hold that can benefit from some highly reductive filter) can be identified by what filter sets, which filter sets are likely to overlap and why (theyre at a stage of divergence/convergence or the filter is similarly required across systems), and related intents

    - identifying 'structures that align/connect across interfaces' is useful to find 'variants of useful structures' such as workflows (like the conceptual variant of a workflow, by applying the cross-interface structure between info/concepts)
        - for example, a possible solution to the problem of 'cancer' could be 'improving organ function, so that organs can more easily handle cancer drugs or fulfill more immune functions, rather than being damaged and disabled by drugs/immune functions' which is a solution of 'extending/increasing/optimizing/overlapping the functionality of every node so that it can connect more nodes independently without changing other nodes' which is similar to the workflow of 'changing position of nodes to make more useful paths accessible', both of which can be created as implementations of a more conceptual workflow like 'create luck' (luck being 'adjacent useful structures')
        - similarly, other workflows have similar 'conceptual variants' like how 'trial and error' is an implementation of the conceptual workflow 'create possibilities (and thereby creating an input to creating probabilities)', where 'creating probabilities' is implemented by worfklows like 'increase adjacence', 'increase connectivity', 'increase filtering', etc (so 'create possibilities (generate)' and 'create probabilities (filter)' are useful when applied in that sequence, which reflects the conceptual sequence as well, 'possibilities' being an input to 'probabilities')

    - identifying useful solution functions (like 'reduce/oppose (inequality in functionality) to similarize probability of winning (or reverse the odds by reversing the inequality to favor the host)' or 'reflect (in an opposing structure of the variation source, like the immune system) to similarize') to handle different problematic 'variation sources', which are a useful problem format to apply various problem-solving intents to (such as 'block the source of variation, if variation is the problem')
        - for example, a condition like 'cancer' is such a systemic illness that probably every variable/sequence/system (such as 'metabolism', 'filtration', etc) is connectible to it with trivial changes, therefore its useful to identify how other variables are connected in order to solve cancer, even if 'cancer-causing pathways' are not directly or ever identified, the 'other connections' are so important that solving the problems with other connections (such as how a system disruption can impact a related/adjacent system, where both systems are not directly related to cancer) may approximately/indirectly solve 'cancer' as well, bc problems cant exist in a vacuum and must be related to other problems, so solving 'all the problems around cancer (like identifying/fixing all sources of dysregulation, or solving the stacking/sequencing of dysregulations which are likelier to cause systemic illnesses or solving cross-system interaction problems to solve illnesses which can cross systems or solving "alignments across errors which favor dysregulation/other errors that can lead to systemic problems" or solving the "host/parasitic/foreign-substance identification problem" or solving the "functionality/resource ratio/distribution/sharing/equalizing problem so cancerous cells/pathogens cant borrow functionality in excess")' is likely to be approximately equal to solving the problem of 'cancer', as 'cancer' is unlikely to be such an extremely different/independent problem that solving it requires drastically different structures unrelated to other problems
        - similarly, identifying the variable interactions between 'high-high variation variables' (often the most important variables to identify connections between, like identifying how metabolism/mutations or changing/regulation are connected) and similarly identifying the connections between 'high-low variation variables' (how important variables are distributed across high variation variables and constants and how variation is connected to constants) can provide the other info required to connect other variables like variable combinations
        - these intents focus on different 'sources of variation' and different solutions to these 'variation sources' (such as how 'variation between functionality of cancer cells and functionality of host cells, should be reduced to solve that variation cause to make them more equal so the host can fight the cancer better (if the cancer has more functionality its likelier to win)' and 'variation between host/foreign substances should be similar to/reflected in immune memory or generality of antigen applicability') in systems so these are likely to be useful alternates

    - identify structures like 'equivalence structures like limits' that are useful for useful intents like 'identify alternates'
        - 'equivalent alternate structures in different structures like different systems or different input/output sequences' are useful to identify, such as where an input in one sequence can be equivalent to an input in a sequence with the equivalent function as the other sequence (the inputs are only equivalent as a possible treatment target, bc the sequences are equivalent, such as by having some resulting function and similar inputs/outputs and similar position), which are useful structures to identify as 'alternate sources of functionality'
        - similarly, 'inputs in the same sequence' are also useful 'equivalent alternates' but identifying all the 'structures of equivalence' between structures like sequences is useful to identify the other structures which can be used instead (such as 'treat dysregulation in adjacent pathways rather than inputs or inputs to equivalent sequences')
        - these 'similarity/difference' structures are useful to identify 'structures likelier to be equivalent' and 'structures like thresholds where the equivalence starts to become different', which are useful to identify
        - useful differences to identify involve cases where an 'adjacent' change (gene mutation) to a 'determining' variable (like a regulatory function using that gene) with a 'volatile' distribution especially with cases that extremify the situation, such as a 'function or structure with no alternates' (there is no alternate regulatory function with that specific usage/functionality/structure/impact), which is a difference structure that could easily cause negative differences (errors) in other structures, given the similarities/differences of these structures (given the 'broad activity' of the 'regulatory function', its especially important not to dysregulate it, but even more important if its volatile and if it has no alternatives that can be activated in its place and other extremifying variables)
            - similarly, some difference structures can be more probably relevant than not (such as how 'adjacent sequences' are likely to be relevant though not required to be), so 'applying differences to adjacent structures as the important regulatory function' could also disrupt the regulatory function
        - this is useful to identify the 'similarity/difference' structures that cause problems, as opposed to being neutral or useful/optimal in some way, which is sometimes possible with some info structures like these variables and their connective sequences

    - similarly, 'volatility' can be irrelevant in some cases (such as where the volatile reaction is not ever triggered/used or its at extreme values that are unlikely to be useful, or its at an interval that is not likely to be used as it overlaps with the opposite of the useful interval, or some other usage structure that invalidates the volatility's impact), which is useful to identify
    - identify possible 'opportunities for optimization' in useful structures (like 'vertexes') given other structures like 'probabilities of concepts like simplicity in those useful structures' (given that solutions arent usually so simple to find, that a simple set of two functions can cover all the complexity of real systems) and identify implementations of those structures which are still useful (such as simple) but reflect reality more completely (such as having an 'embedded' structure and a 'dimension change' and other interface structures)
        - if the assumptions that limit/determine a generator (like its 'assumed variables') are incorrect, the filters can only be so useful in identifying the solution function, since they cant update the generator, so ideally there would be filters applied to the generators (generators would be generated and the maximal negative impact of the errors of their assumptions identified, as well as the maximal corrective impact that a filter can have to correct these errors, so that generator/filter sets that would avoid these errors with the highest probability can be filtered and applied, rather than some adjacent generator to some assumption set)
           - this means the 'generate/filter' vertex has a more optimal variant, the 'filter-generator -> generator-error/filter-limit connection -> change-generator' set, which is more optimal to apply than just a generate/filter workflow
        - this is possible to optimize bc of the 'improbability that a given set of assumptions is correct' (there is a 'probability' generating this 'opportunity')
        - the 'improbability of a default selected set (like a set of assumptions) being correct' is trivial to identify, when identifying 'im/probabilities' of 'correctness/errors', 'errors' such as 'incompleteness' or 'unnecessities' being particularly useful to identify when identifying 'optimization opportunities'
        - the likelihood of a visualization being reflective of reality is similarly improbable, and also the likelihood of a visualization that differs from reality being better than reality at reflecting reality is similarly likely, given that some graphs are better at reflecting perspectives of reality than reality, so a mix of 'un/realistic structures (such as im/probabilities)' is likelier to reflect the 'mixes' (such as solutions/errors) often seen in reality, such as how a variable network doesnt reflect reality but is often better than reality at describing a subset of reality (as reality takes more info to describe than the graph), and similarly there is likely a 'set of these graphs' which are better than reality at reflecting all of reality, which is another 'opportunity for optimization' given the 'improbability of any one graph being complete/correct'

    - identifying structures that reflect useful interface structures in a problem space (usually structures applicable with core useful structures, like 'graphs of different variable interactions' and 'input/output sequence graphs') are useful to identify as reality-reflecting structures which can be trivially changed by interface structures to identify solutions/optimizations
        - for example, identifying 'fragilities of regulators (observed with frequent disruption/breakage of regulators or their inputs/requirements)', 'alternates that create a requirement/certainty of growth', 'variable differences that create opposite functions (between growth/regulate) and available inputs to these functions', 'overall growth-favoring ratio given the factors that can create growth', 'determining sets, where if any item in the set is present, regulation is disrupted or growth is favored', 'adjacent changes favoring growth (if methylation occurs, growth is favored)', 'higher availability of inputs to growth than to regulation', 'fundamental/core/required/default attribute of growth pathways like energy', 'lack of connectivity/communication between regulators', 'lack of filters inhibiting/filtering usage of growth outputs like energy' are all useful interface structures to identify with 'input/output sequences' (like 'growth/regulation pathways'), and 'similar alternates to regulators (like repair) and growth (like DNA copying errors)', and 'high variation variables whose interactions are not fully known like DNA structures like ecDNA which are likelier than other structures to cause variation in other structures like genes', and 'unrequired inputs of growth, such as dead ends/isolated growth structures or growth structures which have sufficient alternates, which are often more useful to inhibit than other structures as they have fewer side effects/functions/interactions', and 'equivalent alternates to growth inputs that can be used during treatment of over-growth', 'exceptions to growth functionality', 'common associated structures (like mRNA and tumor markers)', 'difference combinations (like compounding or adjacent mutations in a region), which when present, indicate a probable aligned difference reflected in related functions, like growth/regulation functionality', and 'difference/similarity structures that frequently indicate pathology (such as false similarity in identify to seem like a host cell and difference in energy routing target)'
        - connecting these structures on a normal input/output graph is useful but similarly, identifying the inputs/outputs to connect is useful as well, just like 'changing position of inputs/outputs' is useful
        - relatedly, structures like 'filters' are abstract in that they can be mapped to 'variables' and also other interface structures like 'patterns' (a filter using a variable is a more generative filter like 'filter odd numbers (by first identifying/generating them or identifying/generating what isnt odd)', where a filter using a pattern like 'filter every alternating item' is a more specific filter depending on the previous value)
             - filters are flexible in that they can be similarly useful if they filter one structure in a set of opposite structures (filter odd or even numbers, without having to identify both)
             - its useful to identify these connections between equivalent alternate functions like filter/generate bc the connections likely apply to other equivalent alternates and the connections are useful to find differences from these alternates
        - similarly, some graphs are useful to identify regarding the growth/regulation interface variable, such as the 'network of growth factors' and the 'network of regulation factors', where 'adjacency to growth functionality' is indicated by angle to indicate how frequently/adjacently its favored, and similarly a 'network of variable interactions where growth is indicated by some obvious structure when it occurs is useful to identify to indicate how often growth (or regulation/repair) occurs in the actual variable network
        - relatedly, the 'standards that allow maximal differences to exist and be compared' derive from 'references (numbers) of structural interactions (numerical interactions)' where numbers are abstract variants of 'structures like quantity' (the number 1 doesnt have a 1-to-1 mapping requiring it to be connected to structuures on other interfaces/concepts, but it can mean a 'measurable unit', 'something in between zero and infinity' or 'the first non-zero whole' or a number of alternate definitions, which is why each number can act like an abstraction or an abstract intersection of other definitions, some numbers being more adjacent to concepts and other structures more than other numbers, where if a program didnt identify 1 as a number to focus on, these and other interactions wouldnt necessarily be identified, so 1 would seem meaningless and equal to all other numbers rather than being significant to e and primes), such as 'euclidean dimension sets' (where a similarity between dimensions, such as a common center/base that can be an intersection, allows them to be compared in a maximally differentiating/clarifying way and enables other differences to be embedded on this standard in a similarly clear way, such as multiplication/area, rotations, opposites of visual default defined quantities like the root of 1, etc)
            - relatedly, 'different sets of generative functions' acts like different 'centers/bases of reality'
            - the question of 'whats the clearest way to differentiate some structure set' is a useful question to answer (use which space to base changes on in order to make other changes clear, such as standardized/normalized space, base space, gauge space, graph/network space, set space, sequence space, topological space, vector space, euclidean space, etc)

    - identifying useful structures like 'new interface structure interactions' (such as 'opposing' structures like 'exceptions' to an 'insight') is useful to identify other useful structures like 'connections between independence and interface structures (like unique filter sequences and info barriers)' and the 'connection between independence and complexity' to fulfill intents like 'identifying more complex connections' to 'resolve more complex problems (using independence-related structures)'
        - for example, identifying the insight that 'everything is connectible' is useful to identify 'structures of this insight' (like 'exceptions' and 'opposing structures') such as 'independence structures' (an 'opposing structure of an exception' to 'connectible structures'), which are not trivially connectible bc of info structures like 'info barriers/boundaries', 'non-adjacencies', 'discretizations', 'cycles/interfaces/other complexity structures', 'over-restrictive filters like unique filters', and 'over-similarities creating ambiguities', etc, all of which can make it more difficult to connect variables (or differentiate and then connect variables which is sometimes necessary), all of which are useful to identify as 'error structures to resolve' (to solve the problem of 'connecting all variables')
            - as a specific example, a 'unit of reality like a molecule' could be called a 'universe-generating machine' in that it has some default functions which would occur without prompting so it could be called a 'machine' and in that it is a component of universes so it could be said to 'generate universes', but its less true than other structures like a 'realistic model of reality such as interface analysis which incorporates understanding' in that it requires very 'specific filters to occur in a specific sequence' before the 'molecule' can be considered a 'real universe-generating machine', and one of the reasons why its more true is that "not every 'molecule' can be applied in reality to generate universes simultaneously, so theyre not all equally capable and would have to select one and then iterate/apply it in the right ways to avoid invalidating its ability to generate reality, such as by slowing down relative to other structures", whereas "only one copy of interface analysis can determine the universe", so these structures like insights such as 'not every unit can be applied in the same way, which means theyre not equivalent' are useful to identify as useful differentiators of 'relative truth', and similarly opposing exceptions to these are useful to identify, such as 'every molecule could be equally powerful, in cases such as where there is a way to specialize them all, or otherwise vary them all such as by integrating a high variation structure like "interface analysis (or a unit of it like vertexes)" into them all'
            - identifying 'sequence-independent variables' (variables that occur regardless of sequence, such as 'filters that are useful regardless of the sequence theyre applied in') are similarly useful to identify, as possible 'requirements/defaults' or alternate structures like 'sets' or 'powerful/useful structures that would occur regardless bc they enable other structures'
            - similarly, identifying structures which are connectible (like 'iterations of units' where 'units' and 'iterations' are the more 'connectible' structures as theyre more 'common/simple/available')
                - this is useful for intents like 'identifying the ultimate limit of iterating a unit' which is like 'time travel' or 'out-calculating time' in that it determines a possible universal limit, which are useful to identify in order to identify paths avoiding these limits, as 'useful simulations' can be useful as 'units of a combination' or 'units of reality' in that applying many different simulators can be useful in predicting 'optimal time state sequences' to guarantee 'high-variation support' across these sequences, as 'predictors/simulators' can connect more variation than other structures, so theyre useful for identifying probable/possible/required time sequences (so intents like 'identify a connective sequence of time ranges by applying a simulator/predictor at different probable/known points in a probable/known time sequence, to find all their connections' is useful to 'host variation in the time range from the universe starting point to the current point, thereby reversing/fast-forwarding time through this higher time/variation-hosting/supporting', as 'sequential time' may be replaceable with 'co-occuring sets' as an alternate time structure, so 'identifying a state in a sequence using connections between these simulators where more variation can coexist' is useful to 'direct variation to that state' using more optimal simulator networks than reality, which will become the new reality once variation is better supported there)
                - so given some optimal distribution of simulators/predictors, the universe could host more variation than it would otherwise support, which is possible by identifying 'real paradoxes (real contradictions allowed/required by requirements such as definitions)' so that these can be used as 'units of reality invalidation' which can be combined to invalidate reality rules and construct new reality rules, basing a new reality on this 'path/network of paradoxes', as 'paradoxes' are useful in identifying/building other contradictions like 'reality invalidations', such as paradoxes of 'how the universe could have absolute limits on time/variation but also host more time/variation that goes above those limits (using structures like simulators to simulate embedded variables or to host more variation/time in the simulation than reality can otherwise host, these simulators acting like portals to infinite variation if built optimally, where connecting them is another high variation structure, and similarly other interactions can increase their variation like if they all coordinate, and so on to add other embedded variables to support infinite variation)'
            - relatedly, identifying structures which often exist in 'specific ratios like a high-low ratio' are useful to identify 'different variants of a variable (where one variant is more common)'
            - more difficult variables to connect often involve a 'sequence of edge/extreme cases' (such as a 'sequence created by unique filters that leave one possible solution') which when aligned in that sequence, make it almost impossible to connect variables (like a set of rotating cycles that have one possible option that can interact with other cycles in a way that allows the input/output variables outside the sequence to be connected)
            - so stacking 'extreme/edge cases' or 'unique filters' is useful to identify more complex connection structures which can be used to solve more complex problems
            - its useful to identify 'equivalent alternates' and stack/sequence them bc they have a 'similarity' in that their particular rotation state doesnt matter, so any direction/state of rotation is equivalent within each set
            - for example, 'identifying equivalent alternates (such as alternate ambiguities, complexities, functionality, etc)' identifies 'rings of structures' which can be stacked to decompose complex systems into a set of possible sequences of the rings, at which point, 'useful sequences/alignments of the rings' should be possible to identify (the way to align/sequence a set of rings that makes most variables trivial to connect), as an alternate way to connect 'concept networks' and 'numbers' (other than mapping differences to vectors in a vector space and losing most of the concept-identifying info)
                - another alternative to 'concept vector angle differences in a vector space' is identifying vector networks for each concept (which removes the original info of what words are used in the definitions, but when positioned in relation to all other concepts similarly formatted, would regain this info through the relative network structures across vector networks), then mapping vector networks in a network indicating 'usage' vectors (or some other interface variable that can connect all the vector networks for each concept)
                - similarly, useful 'orthogonal rings acting as cross-sections of the ring sequence' or 'host rings where the others are embedded' should be possible to identify, after identifying the 'equivalent alternates' to apply in this way, such as 'connecting rings using known useful vertexes to organize the sequence/network of rings'
                - similarly, identifying 'ambiguous alternates', 'useful combinations of alternates like vertexes', 'maximally different equivalent alternates', 'maximally different resolutions of ambiguities' are useful to identify/position/mix in these sequences/networks, to identify 'maximal difference-supporting/similarizing sequences/networks', where important/useful interface structures or problem-solving structures like 'determining variables' or 'phase shift thresholds' are trivial to identify (meaning 'similar to each other in some way in this network, such as being possible to identify by some similar similarity/difference query of the network in relation to some supporting system/interface' such as how 'this type of phase shift is possible, when this change set is allowed in this system, and when this change set has occurred in this system')
                - more likely, some 'sequence-generating function' applied to these rings/vertexes will be more useful to 'organize the sequence for specific intents' (so rather than building a one-dimensional stack that acts like a wormhole in that it preserves some important info about the input to connect it with some complex/independent target output, a 'orthogonally-rotating, self-similar, maximally different wormhole similar to some manifolds' is built which would likely act like an 'info vortex' and 'capture all info', therefore creating a possible cascade that would end reality/time, as it would fulfill every function and connect everything in a very condensed structure so nothing could out-vary/out-pace it and it would therefore be all-powerful, and once that occurred, no external variation would be able to resist its gravity)
        - similarly, 'sequence-generating/determining variables' like 'gravity/attraction' are useful to identify and apply as an alternate to 'sequences', 'sets', 'probabilities', 'adjacencies', 'interactivities', 'info barriers', 'independences', and other useful alternate structures

    - identifying useful similarities/differences in problem-solving structures, such as 'interface queries (connecting intents to structures to fulfill it) which can be changed with some vertex (probability vertex) to be relevant to resolving problems in that vertex format (probability resolution problems)'
        - for example, a problem is often formattable as 'what is a new similarity/difference, which differs from known similarities/differences' like 'does this apply to all examples of this type, or just this known example' (is it the type that determines the functionality or something else), where the uncertainty/ambiguity lies in 'whether there is an alternate route between attributes (attributes whose connections are not fully determined)', such as whether there is a way to generate the same structure that is 'normally generated by defaults/common inputs/requirements' by instead using 'non-default, uncommon, non-required but possible differences', at which point finding a 'unitary solution/connection structure to iterate/differ' (like a unit of a possible connection type) is a useful next step to determine the uncertain space of 'non-default, non-required, uncommon allowed interactions', at which point finding 'iterated structure variants forming larger subsets' from that connection type unit (such as 'different shape subsets that the unit can create with trivial/probable/common changes') is a useful next step
            - 'determining an alternate route' -> 'finding a unit connection to iterate/vary' -> 'iterating/varying this unit connection to form subsets of possible solutions' -> 'determining subset interactions to find/build possible solutions' is therefore a useful route between problem/solution structures, which can be changed by 'probability-resolution structures' (such as 'ways that a rare structure can fulfill the same intent as a common structure', etc), such as by identifying 'rare unit connections which can build common subsets'
        - these problems are problems of 'decomposing variation resulting from various structures of probability and their interactions' (common/default/powerful/probable/required vs. opposing structures which are possible but less known and more variable so more likely to be useful in resolving an uncertainty and likelier to be difficult to filter)
        - similarly, solving 'unknown connection functions' is a useful problem-solving intent to fulfill, as resolving unknown connections creates additional connection functions that can be applied to connect other structures

    - identifying new connections between useful structures like '1-to-1 mappings' and 'interfaces', which are connectible in that a vertex formed by a 'lack of a 1-to-1 mapping' (like growth/regulatory-sweetness/bitterness vertex, which are more useful to apply in combination rather than in isolation) can indicate an interface variable on either side of the mapping (similar to how generate/filter or grow/regulate are alternate function sets where either item in the set can be used to solve all problems, other variables that are not directly mappable to these vertexes can also act like interfaces)
        - for example, 'exceptions to 1-to-1 mappings between generative/limiting forces' are useful to identify new possible interfaces, such as how the 'sweet/bitter' variable isnt 1-to-1 mapped to 'growth/regulation', such as how some growth-favoring types like sugars have some regulatory exceptions in the type (such as glycosides or regulatory compounds found in honey), which have inhibitory/regulatory impact despite being sweet or in a sweet compound, an attribute which usually favors 'growth' functionality as it is paired with energy inputs (such as how 'stevia' has anti-growth functionality and doesnt provide energy inputs like other sweet compounds tend to), which is possibly a result of other variables associated with the sweetness, such as 'lack of energy inputs associated' or 'high interactivity' ('bee pollen' is supposed to help and interact with many different species, as an input to their survival, so it makes sense that generally useful compounds are found in it), which implies that either variable value can act like an interface where all functionality can develop (sugars can be both growth-favoring and growth-regulatory and can possibly contain other functions which arent 1-to-1 mapped to sweetness or other attributes of sugar)
        - this means that 'sweetness' can be applied as an interface (in its mapping to an interface like 'grow/build') in that it has enough variation that it can interact with 'regulatory' variables or act as a 'regulatory' variable ('growth' can be 'regulatory', such as by 'helping regulatory molecules grow')
        - relatedly, 'high variation/powerful/determining inputs' is similarly useful as 'input/output sequences' in that 'high variation inputs or filters' can create all 'input/output sequences', just like 'co-occurring sets' and 'probable sets' act like equivalent alternates of 'input/output sequences' (the info of 'all sequences' isnt necessary if the info to 'generate/filter all the sequences (generative input variables or regulatory filters)' is available or the info of 'variables describing/connecting useful sequences' like adjacency/interactivity/probability is available)

    - identify useful structures like 'connections between high variation structures like vertexes' which make identifying other useful structures like 'useful intents' (such as the intent of 'matching filters/generators' as other 'useful opposing structures to connect') trivial
        - for example, identifying how vertexes are related is useful, such as how 'matching resources/requirements' and 'generate/filter' and 'abstract/specify' and 'reduce/expand' are related by their common structures, where the differences of 'generate/filter' applies a similar variation type as matching 'resources/requirements' (generating possible resources, then filtering them with requirements), so they could be seen as variants of each other, and similarly, 'abstracting' a structure is 'filtering its attributes to its common/general structures (which form an abstract type)', where 'specifying a structure' is 'generating an example of it' (by combining a set of its possible values)
            - this is useful to identify so that 'vertexes which are extremely different can be identified'
            - similarly, its useful for 'identifying missing structures of a vertex which would be useful to include' (such as how a 'change' function is always useful, such as 'changing generators/filters or changing position of structures like changing position of filters to integrate them with generators'
            - similarly, some structures are often useful for specific vertexes, like how an 'abstract-specific combination structure' like 'causal position' is useful to integrate with 'abstract/specific' structures to make their usefulness/relevance more obvious by applying them to a 'high variation structure like a cross-interface structure that is both abstract/specific'
            - similarly, identifying the ways that one structure in a vertex can be similar to the other is useful, such as identifying how 'if enough filters are applied to a generator', it would seem like a specific function structure, such as a 'filter sequence', if it only generates one solution per input bc of the filters applied (as in 'one possible implementation, as in a specific function, rather than a set of possible function implementations/solutions')
                - similarly, if enough abstractions are applied, they can indicate a specific structure by adding increasing information
        - relatedly, the task of 'matching' these vertex structures (matching a generator with a filter) is the unit of the task of 'integrating' them ('filtering generators' to be optimally useful, to avoid generating irrelevant solutions as much as possible)
            - 'generating a solution set that cant be filtered by a particular filter' is an interesting intent to fulfill or find an example of (such as where the emerging structures of solutions like solution sets should be evaluated in a combination/network, but a filter only checks the same variables used to generate the solution or the differences between solutions generated by those variables)
            - similarly, 'generating only maximally different or otherwise useful solutions' is a matter of 'identifying the variables that cause maximal differences' as in 'filtering possible variables used to only include those variables'
            - at some point, if the filters are specific enough, the generator is not required, as the filters contain enough info that solutions dont need to be generated for filtering, as all the requirements of the solution are known and implemented in the filters
            - "avoiding filters that clearly optimize for/map to known requirements" is a useful intent to identify new filter sets for a different/new problem than those solved by known filter sets which optimize for/map to known requirements

    - identifying useful structures like different sets of 'offsetting function networks' is useful to identify the 'sets of function networks that can act like useful boundaries/limits on variation' and the 'connection/switching/prioritizing functions between these networks to avoid known errors'
        - for example, identifying function networks with 'complementary error sets' is useful to identify 'systems/networks to differentiate from' when fulfilling intents like 'creating an optimized system', which systems/networks are definitely suboptimal but connect with each other in a useful way, such as by offsetting the others' errors
            - these are useful to identify just like limits/errors/optimals are useful to identify bc once the difference/distance from errors/limits/optimals is known, other useful structures can be computed, such as the maximally different points in between them or their various averages
            - relatedly, a 'flow state' is a related concept in which solutions are easily accessible with available functions/variables, within a limited range of required changes that is handled by those available resources, where some variation is allowed but minimal variation is required and is well within functional capacity limits, bc it is allowed to avoid known errors and errors are known (acting like 'guardrails on a highway')
            - this means identifying function networks with overlapping errors like 'attention and bias networks' as well as identifying networks with exclusive and complementary errors
        - relatedly, a 'minimumm speed of analysis' is useful to identify, as a 'maximum speed' is unlikely to be able to fulfill all required analysis functions, so there is an upper limit on optimal speed, which doesnt sound correct until its identified that 'identifying multiple equivalent alternates or other types of alternatives' is almost always useful among other useful functions that create a 'limited set of required analysis functions to apply in combination', which requires fulfilling functions that require non-trivial work (to generate equivalent alternates, like by 'applying differences in simmilarities until an equivalent alternate is found'), this non-trivial work being likely to occupy more than a point on a network, no matter how optimized the network
             - as a counterpoint, a network where these optimizations/function usages are points on the network may offer a useful alternative, wherever these can be identified (such as a 'network of useful interface queries')
        - relatedly, identifying 'all the ways a suboptimal structure could be correct' is a related function network that is useful for its specificity and should therefore be included in this 'error-avoiding function network set' to handle known error structures like 'perspective-switching/prioritizing/connecting errors' such as 'missing the case where simplicity is correct (such as when identifying a simple structure like a unit is possible and optimal), bc simplicity is more often a known error like over-reduction or over-prioritization' or 'missing the counterintuitive structure bc a sufficient difference-containing insight set that can create counterintuitive structures wasnt identified and applied'
            - for example, an algorithm that involves 'avoiding trusting any structure with 100% responsibility/resources' is likelier to avoid known errors like 'over-burdened structures' and 'false optimal metrics' given structures like 'incentives' and given the 'improbability' that 'any given function can handle all use cases optimally or that such a function is required', which can be combined with other algorithms that avoid other known errors like 'simplicities that dont qualify for exceptions in some case'
        - matching 'counterintuitive structures' with a 'difference ratio/type/set/structure' is useful to identify 'functions/workflows/structures' that can identify counterintuitive structures (its whichever insight sets contain enough differences to create counterintuitive structures such as structures which self-invalidate and self-optimize at different phases/amounts/degrees/positions/structures)
        - relatedly, identifying 'degree/distance/type of difference required' is useful not just for intents like 'avoiding crossing some threshold distance to a known error' but also for intents like 'finding useful opposing structures to include (like use cases/counterintuitivities/contradictions/exceptions, or generate/filter, or similarize/differ, or abstract/specify, or change/limit)' which can correct the errors theyre likely to cause
            - similarly, other opposing function sets exist (such as 'change position' or 'change surrounding variables' or 'change connection functions') which are useful to identify as alternates which have similar functionality, these 'equivalent alternates' being useful for identifying new workflows as they involve variants of known useful structures
            - similarly, identifying 'reasons why a structure needs to be specified or otherwise acted on by interface structures' is useful, such as cases where it would be considered optimal/suboptimal, like how 'input/output sequences' are useful in specific cases, such as where "known structures are possible to connect with available/adjacent changes, and where known structures are sufficiently complete to identify variation in all possible path variants of the variable connections, and where there are no opposing cases such as where the structures in between known structures are not definitely structures that can be dismissed/ignored, like 'irrelevant structures', where they are instead relevant like alternate reasons why another variable can exist, so identifying these alternates is required, rather than dismissing them as equal or non-changing of other variables", where these cases are useful to specify so that "switching between similar/equivalent alternates (like 'required/available differences', 'probabilities', 'sets', etc) to create useful differences" is possible
            - why is it important to identify and connect alternates? bc there are 'alternate routes to errors' just like there are 'alternate routes to solutions', and similarly 'multiple alternates can co-occur bc not all alternate variable sets are exclusive and can have structures like overlaps' and 'routes to solutions/errors can also have overlaps' and these structures are all useful to identify, rather than identifying a subset
            - similarly, a 'network that avoids all known structures (patterns, limits, types, other certainties) by embedding more variables in an interim space' is useful for 'identifying new structures', just like a 'network that applies maximally different structures like 'generate/filter' at the center and applies changes in an outward direction or across iteration levels' is useful for that same intent
            - 'similarly useful' structures can be applied (such as reality-covering variable sets) but also 'mixed-usefulness' structures can be useful in that they capture different info ('randomness may not always be useful in a case, but in a new problem it could become useful to add/handle more differences'), given that differences are required in a useful structure
            - similarly, generating new similarity indexes using interface structures is useful, such as new similarities like 'overlaps but no intersections' or 'waves but no similarities/patterns across magnitudes', or other sets of similar functions (a similarity index which can be decomposed into a set of structures like 'core components that can generate the set', 'useful variants within the set' and 'maximal different examples in the set' and 'limits of similarities/differences in the set' which fully describe the 'similarity of the items in the set' and 'identify variables of similarity indexes', and 'connections between these items creating the similarity index across similarity indexes' are useful to identify, to identify other structures like 'alternate routes between and variables of indexes')
            - similarly, networks having some similarity like 'similar connection structures' and 'overlapping nodes/centers' are useful to identify, as sets of 'pre-standardized/similarized networks of differences' which can be used by default to find new networks that are different such as by being non-similarizable or non-overlapping with these network sets

    - identify common structures across useful workflows to fulfill problem-solving intents like 'identify counterintuitive truths' by 'identifying useful filters' (such as 'optimize true structures, then differentiate and extremify to find other true structures', or 'extremify a default, then differentiate and extremify, and compare extremes')
        - for example, the problem of 'identifying a counterintuitive solution' is often a problem of focus/filtering
            - such as how when focusing on fact sets, such as:
                - identify constants that could optimally be variables like 'added variable size' (as in 'increasingly small' changes), then identify important differences like initial/added changes and maximize their difference to test the impact of the required changes (added changes) in different input cases (like with a large initial size and increasingly small added changes)
                    - this involves identifying more optimal structures of the problem structures first, then identifying and applying useful changes (like maximize difference between known relevant differences, within definition limits) to identify other useful changes (like scaled/extreme impact), once the more optimal variants are identified
                        - 'all structures can be composed of infinite infinitely small components which are infinite in that they can continue to get smaller, especially with numbers, as numbers have no requirement to be reflected in a real system'
                        - 'distorting the scale of different changes, such as adding very small changes to a very large structure repeatedly, makes it more obvious that repeating that addition wouldnt change the large structure very much, and would taper off into an undetectable amount if the changes kept getting smaller'
                            - this fact set involves a 'true variant' of a 'false connection' such as 'iterating any change enough times can create any other change eventually', the true variant involving the 'possibility of a variable in the change size, such as an increasingly small change' ('applying a variable to a constant')
                            - 'changing embedded variables (as in variable variables, like size) can usefully differentiate structures' to fulfill intents like 'make differences more obvious/extreme', first by identifying a possible difference between variables like 'initial size' and 'added variable size' and applying a difference-maximization function to this variable set to 'further differentiate variables, within the limits of their definitions, to test for a difference in the impact of extreme changes' and fulfill intents like 'try every possible variation (trial and error)' or 'test if trivial differences in known different structures (known different structures, like initial and added variable size, which are required to connect in the solution) change other variables, like output size after all expected/required changes (the iteration sequence)'
                            - given that 'identifying true variants of known errors' by 'applying a constant as a variable' and 'differentiating true differences to the extreme allowed by their definitions (within a true range)' are useful to apply in a combination, what other structures become clear?
                                - identifying an insight (to base changes on, to connect with other insights, to find false variants of, to find useful structures like 'increasingly small changes') is an important first step
                                - applying changes to that insight is a useful next step (applying 'fuzzy connections between input/output (such as problem/solution) structures'), not just to see if it holds when changed, but to see if an extreme difference is useful as it often tends to be for other problem-solving intents, such as to identify the impact of 'scale/size' on a variable connection ('increasingly small' changes are the 'change type' to identify as important in this problem), changes which are useful for 'finding the impact of the useful structures identified in the insight, when applied in variants such as in extremes'
                - identifying the extreme of a default change (constant change) then identifying the extreme of variants of it (like less/more than default change) is similarly useful to focus on to derive the counterintuitive structure
                    - 'adding an infinite number of constant changes (such as the same type of atom) would obviously eventually create a non-trivial/large structure'
                    - 'adding an infinite number of structures smaller than an atom would not definitely create a non-trivial/large structure'
            - the differences between the correct solution structure and other structures (possibilities that seem equally probable at first, before focusing on these fact sets) become obvious
            - other possibilities that seem equally valid at first include false connections such as 'iterating any change enough times can create any other change eventually' which seems true until a 'decreasing change size' is identified and compared to the obvious impact of a 'increasing or constant change size'

    - identifying the info structures like 'rule/function networks' between structures not already determined (like 'networks mapping between interface structures of function attribute sets'), using some standardized structures (like some constant value of a change rate that can exist in a standardized function to a 0-1 range that implies other change structures should be checked for), as an alternative to a 'network of function similarity indexes', given that with enough input info about the function, the rest of the function can be determined by applying some interface structures like implications in the correct structure like a network, where the input info available about the function helps to 'select the position of the network to start applying interface structures at', which is like a network of 'determining structure sub-sequences/networks' (where these sub-sequences/networks act like 'dead-ends' confirming some structure, resolving some uncertainty)
        - for example, identifying the 'set of contradictions/differences/opposing structures' that can coexist in a particular function type/shape/other interface structures (such as function attributes like '1-to-1 mapping') is useful to identify, to filter out possible structures (such as how 'opposite slopes' can exist in the same function, but not in the same input range and not in the overlapping input range for a 1-to-1 function, so if opposite slopes overlap, either one of them is wrong or both of them are wrong, and given some established change rate of some subset, there may be a distance requirement between the slopes, to allow a maximum or other structure implied or required by the established change rate)
            - this set of rules between interface structures like requirements/alternatives and function structures like constants, such as 'the options which are likely or guaranteed to be incorrect or could be equally correct as some alternate, requiring testing for both alternates, given some established function attributes like an identified change rate in a subset or a attribute like 1-to-1 mapping' is useful to identify in the 'function filtering (given a local or regular interval subset)' problem space, which is related to the 'regression' problem space
            - identifying the 'contradictions that can coexist' (differences that can be united by a similarity of the same function) and the 'structures that allow/require other structures to enable that coexistence' is useful to filter out 'possible remaining subsets of the function'
            - given these structures to find 'maximally different' and 'maximally similar' functions (given an identified input, like a verified subset of the function, to identify the most different/similar functions having that subset), computing the 'interim' functions at the midpoints of these alternates (the 'interim' function between maximally similar/different functions) is possible/trivial/useful, as is computing the 'reasons to vacillate between functions' such as interface structures like 'requirements/implications/probabilities' which can be used in a function network to determine variable interactions
            - finding the unit of a useful difference to apply to connect variables (rather than just pure numerical operations like 'multiply') which are 'definitely similar but uncertainly different or vice versa' is unlikely to be solely numerical and likelier to be some interface structure (like a 'maximally different vertex' such as 'changing the angle and position and extremity of some input variable set', as mentioned elsewhere as the 'rotation'-analogous operation that explains variation between similar but different structures by some common similarity/difference associated with problems/uncertainties, although a rotation is a good place to start searching bc it does apply 'interface changes' as opposed to other change types)
                - similarly a 'derivative' is another good place to start, as both operations involve 'vertex changes' of uniting differences with a common structure, it could also technically be said about 'multiplication' but that is still lower-variation than rotation/derivatives, similarly combining these is likelier to support more variation and be more useful in connecting different structures (solving problems), similar to how 'rotating functions' can generate other functions on the similarity index (similar by shape and other metrics)
                - similarly, a 'matrix' and a 'vertex' have some attributes in common, as a 'matrix' unites a 'set of different sequences' and a 'vertex' is a 'common structure across maximal differences', and relatedly, its useful to apply interface structures (to find the 'similarization' of a matrix with some structure such as itself, the 'standardization' of a matrix, the 'exclusion/exclusivity' of a matrix as in 'what does it obviously leave out', the 'specification' of a matrix as in 'what type(s) does it encode and what is a more specific variant', the 'intersection of a matrix' with some structure such as itself as in 'what intersections/overlaps can be trivially created with trivial changes allowed by matrix operations', 'perpendicularities/parallelisms' of a matrix as in 'what changes would make the values align (parallel) and what is it already aligned with', etc with other structures like 'unique' and 'required/default' and so on, just like the 'diagonalization' of a matrix or the 'inverse' of a matrix or the 'symmetry' of a matrix are known to be useful to identify)
                - relatedly, 'similarities between different function subsets of the same function' are more useful to identify than 'similarities between functions' (such as functions with similar area/shape/slope)
                - similarly, its useful to identify structures of vertexes that are useful in intents like 'determining other structures' such as how 'two opposing vertexes' could form structures like 'boundaries/limits' if their endpoints also are vertexes, which would create a determining structure like a boundary/limit
            - finding all the centers/cycles/networks in between all the 'dead-ends' to connect the 'dead-ends' is similarly useful
                - this structure keeps re-occurring without resolution bc its a filter to get to the next higher variation state, but its not the only filter to get there
            - the interface analysis program would avoid these 'dead-ends' itself so it can identify them (meaning it would regularly 'optimize for nothing', so as to avoid being determined (by allowing some randomness), at which point it would be predictable by a higher variation structure, rather it should always support the most variation to be effectively optimal at identifying other variation)
                - this is another reason to call interface analysis 'interim thinking' (staying 'in between' structures like 'limits' by applying structures like 'embedded variables' is a way to maximize support of 'high variation')
                - relatedly, staying in between known errors such as 'one-priority/perspective at a time' and an unresolved structure of 'all priorities/perspectives at a time' involves structures like 'independent variable sets (like the set of both abstract/specific and uncertain/certain)' that connect these two opposing error structures (the known error of over-prioritization and the known problem of 'resolving the real structure connecting all these variables'), where interim structures are likely to be useful toward resolving that known problem of 'connecting all reality-covering variables'
                - 'moving toward or away from interface structures' in this graph of interface structures (designed to be navigated around/avoided) is a problem-solving function that can be used to generate interface queries using that graph, such as 'avoiding certainty resolutions or approaching reality-covering variables or approaching higher-variable intersections (when more variation is needed)' and 'approaching certainty resolutions or avoiding reality-covering variables or approaching fewer-variable intersections (when less variation is needed)', which is useful to know for designing the graph (positioning these opposing structures in different positions, but also creating differences within each type, so that reality-covering variables arent all in the same position)
            - similarly, identifying uncertainties to resolve that would not be dead-ends even though they identify certainties is useful, such as where the certainty will 'support so much additional variation, that its worth sacrificing the uncertainty maintained by not resolving it' (which is 'allocating cost' as in 'how to spend your uncertainty, to preserve some balance as in both certainty and uncertainty, without a cascade in either direction, either toward certainty/determination or uncertainty/chaos'), which are ways around just 'avoiding dead-ends', as these new variables allow new spaces for new connections to be found
        - similarly, identifying the 'worst case' of 'how could this variable interaction be completely incorrect, if extrapolated to the extreme' (what probably incorrect functions can be generated/tested for, by applying a probable error, such as by assuming this one variable interaction, like one pair connection, is over-prioritized and applied as the most important variable connection in the data set) is useful to filter out functions based on some identified variable, such as a slope that is repeated or iterated/scaled to an extreme, which is both unlikely and likely to be trivial to filter out if incorrect, as a 'simplified version of a function created from an implication of a subset' that is likely over-simplified by applying this extreme to such a small subset, and relatedly in the opposite direction, what functions are more probably correct, given these known error structures such as over-prioritization
        - alternately, 'overlapping subsets' may be a better function format in some cases, such as where variable interactions in the original system have yet to stabilize into one function or a clear set of multiple alternate functions and clarifying the functions by resolving them isnt optimal yet (similar to how averages/types/limits/ranges/function networks can be used instead of a single solution function, as they store a high ratio of info using few info structures)

    - identifying useful structures like 'required symmetries' is useful as well as structures that can help fulfill/implement them like 'organization' structures
        - for example, a 'required symmetry' exists between problem/solution structures such as generate/filter or generate/evaluate, where a generated solution set is useless if it cant be filtered/evaluated, so 'organization' structures are required to be matched to every 'problem' structure, otherwise the problem will not be possible to solve, as 'organization' structures help fulfill the opposing intents like 'filter/evaluate' to problem structures like 'generate'
        - this is bc if a generated set is 'organized sufficiently', such as being sorted, then its more trivial to filter/evaluate it, because of the symmetry in usefulness that exists across generate/evaluation structures
        - identifying these required symmetries is useful for identifying structures that need to be applied together to optimize for usefulness/relevance such as measurability/differentiability/filterability
        - this forms an interface structure based on 'organization', where generate/filter are just a pair of opposing intents adjacently fulfilled by the 'organization' interface, similar to 'group/subset'

    - identifying structures such as 'similar number of points connecting some relevant difference' which seem truer/falser when some structures are applied (local context, specification, connection to other truths, etc) is useful for determining false signals of truth, which are similarly valuable to identify as truth structures
        - for example, if enough irrelevant similarities exist between some subset, they will seem locally true and similarly true as more relevant similarities in some subset, bc of the comparable count of the similarities
            - its possible to say that some irrelevant similarities exist, such as 'all structures have functions' or 'all structures have problems, and solve those problems in different ways', so saying 'all structures are equally smart' seems falsely true if only that fact is applied, in isolation of other relevant facts, bc its technically true but irrelevant (as in not differentiating, as it applies to all structures), where whats relevant (differentiating) for determining intelligence is whether some structure supports such an extremely high degree of variation compared to the other that they can easily beat the other in all relevant metrics, or to an extent that the other may as well have zero functionality, such as if a supercomputer has so much relative computational potential that other computers become irrelevant
            - in that example, some group of computers may have similar computational capacity, but they have to coordinate which causes additional costs for their computation, in order to reach the supercomputer's capacity, and similarly the supercomputer can compute aggregated/iterated calculations such as computing differences between sub-networks in a way that a group of computers has to communicate and coordinate to compute, and they also have additional costs like selecting a host/lead computer in the group to aggregate/evaluate the others' computation results
        - similarly, opposing sides of an issue seem right when their points are considered in isolation, such as 'everyone has potential to be president' which is technically true, but irrelevant in the sense that 'only one in the group' can be 'president', so not everyone has potential to be 'president' bc thats not how the 'president' structure is usable in reality and similarly an opposing point like 'qualified people have the potential to be president' seems true, and bc they both contain 'one similarity that connects differences' (similarity in 'potential to be' connecting everyone/president and similarity in the 'adjacency connecting people who are qualified and people with potential to be president'), they both seem 'equally' true bc of the equality in the number of technically/irrelevantly true points, but both also seem false in various ways, as the statement can become more true when interface structures like 'specifications' are applied ('everyone has potential to run for president, but not be president' is more specific and more true and similarly 'everyone has potential to be president, if nobody else is president at the time' is another more true specification), and similarly 'everyone has potential to be qualified to be president' is more true in that it connects true statements and resolves their overlapping uncertainty (who has potential, who is qualified -> everyone has potential to be qualified)
        - similarly, identifying that 'justice' has a symmetry with its input in that it reflects its input ('truth'), which is a way to connect justice with reality (reality/truth -> justice), as reality is a variant of truth (which allows for some structures of falsehood)
            - another way to connect them is by their similar structures in common (like balance)
            - so 'structures in common' and 'input/output sequences' and 'adjacent similarities' are identifiable as alternate structures to derive truth
            - similarly, structures like 'constant (like causal direction) could be a variable' or that 'cause can be multidirectional' or that 'correlation doesnt equal causation' or 'co-occurring structures can be causally associated' or that 'input-output sequences can have an overlap and be cyclical as well' and 'there are cases where sequence is irrelevant bc "either sequence could apply" or "it doesnt matter either way"' are all structures which can identify the same insight
            - the last structure is particularly important, as an algorithm that only identifies 'variables which change other variables' would miss a 'pair of variables whose sequence doesnt matter bc they both create the same change' so their difference may be undetectable, but identifying the 'reasons why and the structures of how they have the same impact' is useful, or only identify one item in the pair and stop evaluating at that point
            - this type of interaction is useful to identify as it changes the relevance of other important structures like 'sequences'
                - 'co-occurrence/coexistence' is like 'sideways time' (a different dimension of time) in that higher variation (time) is possible when more differences can coexist
                - similarly, identifying/embedding more variables in between certainties like infinities/limits (to add variation between current position and the certainty) is another type of variation (time) in that it increases variation supported, which is like 'stacked time'
                - these are useful alternatives/compounding structures to 'sequences' as a structure of time (time sequences)
                - similarly, 'difference from not-time' (difference from anything that reduces time, like cascades leading to infinities/limits/other dead-ends like uncertainty/certainty) is another type of time
                - this means its probably possible to 'support all variation' in one spacetime, if enough horizontal/stacked time is applied (to make one spacetime higher variation) instead of sequential time (across space-times)
        - relatedly, as mentioned elsewhere, the structure of 'perspectives' is useful when identifying 'truth' connections (such as how there is a 'simple' way to connect all variables, a 'balanced' way, a 'positive' way, a 'causative' way, etc), and a truer perspective is likelier to have structures of truth such as 'more/common/multiple/alternate' (such as 'more of these perspective structures encoded in it, in a way that reflects the conceptual variable interactions' such as a 'vertex' structure involves a common point between multiple perspectives), so the 'simple way some function could be a solution/error' (and a 'simple way some function could be true/false' and a 'complex way some function could be true/false' and a 'balanced way some function could be true/false' and so on, iterating and embedding structures like variables/functions) can be identified as new indexes (connecting some vertex like truth/simplicity with some specific structure they have in common, like a specific function), which if sufficient examples of these are identified, an approximation of the interface network could emerge from applying this to one specific function
            - similarly, a 'simplified' way to connect all variables will have errors such as 'missing info' and 'inability to adjacently derive more complex solutions' and 'mismatches between over-simplified and real structures' so once these errors are found, the perspective should be switched to another perspective without an error in that position, or to an opposing perspective that can correct that error
        - similarly, the 'number of points made' is irrelevant (unless compared to the other number of opposing points or otherwise connected to interface structures) bc it doesnt map directly (like 1-to-1) to truth ('more points' doesnt mean 'more true') but there is a point where it will seem true (if 'more false points are made than true points, above a certain ratio', the 'false points will start to seem true, just by being the only thing that is being observed/measured', so there is a 'limit' structure of the '"illusory truth" attribute of many false points', though it doesnt 'add truth' or otherwise 'increase the truthhood', but it may change the 'measurements being made, from false to true')
        - similarly, identifying 'false filters of truth' is useful, such as how some filters are definitely false to apply too prematurely but may seem true as in correctly applied at that point, and result in a 'premature filtering' error, where continuing in that direction would have been useful (such as where a variable can cause another variable at different amounts/degrees of the input so it might be filtered out bc the 'active amount/range' was not present, and similarly how other variables can cause the same output so it may seem like the absence of the original input variable means it couldnt cause the output, but this is false bc it has an alternative causal variable that was present)
        - similarly, 'obvious differences' (related to 'extreme differences' and 'absolute differences') are a useful problem-solving structure just like 'required/default differences/similarities' are useful across problem-solving structures, so that when a measurement/metric is insufficient at measuring truth/fit, it can still be used to tell the correct variable interaction, bc the difference was made obvious
        - other structures are similarly useful, based on what they are different from or interact with:
            - 'truth structures' which are too strong to the point where they destroy other 'truth structures' (where the destroyed structures were real but were just under the strength threshold to be measured, which is an alternate cause of a 'missing info' error)
            - 'switch' as a useful problem-solving structure similar to 'swap' as it involves applying some opposite structure, like 'switching contexts around a very different structure pair' or 'switching position of opposites' which is useful across various problems, as it creates a high ratio of difference which can be useful for intents like 'identify maximal differences' (which is similar to asking 'what if some assumption was extremely true/false')
            - 'stretch' as a specific variant of 'iterate' that can act like an alternate to 'iterate', being useful to identify as iteration is a problem-solving structure (how many times can you iterate before identifying a new useful structure vs. how much can you stretch one structure to cover/solve other problems)
            - similarly, 'usage structures' can increase the usefulness of a structure, such as how a usage structure like 'repeatedly using a function (like to identify high variation)' leads to other functions/structures developing around it (as the variation of a problem-solving network/function has nowhere to go but in the surrounding structures now that the function usage is certainly useful for a particular problem/intent/case, and the input direction is a useful direction to apply variation in, such as in input filters), and these developed functions can increase the usefulness of the function, such as if a function to identify high variation develops a set of patterns that are associated with high variation and starts using these structures to make the function more useful, which is only possible with repeated usages of the function to identify those patterns

    - identifying useful structures from mapping new problem/solution structures like new errors in a different problem space and identifying optimal mappings (like 'mapping to certainties/similarities in the original or regression problem space') which are the most useful in standardizing a problem to identify new optimization structures/insights that apply across problems
        - for example, a 'navigation' error might be an error of 'missing a right turn at an uncertain destination (not sure where the destination is or how far away it is, but there is a sign pointing to it that youre checking for regularly at interval x)' which is caused by 'solving a different problem which takes resources like attention away from checking for the sign to turn at' where a different problem might be 'deciding to switch lanes', where there is an optimization structure in solving the other problem, which is the 'checking whether to switch lanes' directs attention to local structures, so a turn sign is less likely to be missed given that attention is being routed to local structures including signs, however it can have an error structure of 'only paying attention to car position/speed rather than signs or other adjacent structures'
        - when converted into an optimization/error/function set in the 'regression' problem space, this error would take the form of 'missing a local minimum bc the interval checked at was skipped bc a different metric was being evaluated, such as a function to check adjacent points for some superior local path between points, where this function to check local points might find the missed local minimum if its missed bc of solving the other problem at the time or missed bc the interval x which the original function is checking for minima at coincidentally misses the minimum'
            - the shape of this algorithm would take the form of a "straight line connecting a set of 'regular interval points' with an occasional embedded cycle (occasional alternate problem solved, like 'evaluating for an optimal algorithm-switching point')"
        - therefore 'having multiple functions with overlapping functionality' and 'avoiding solving a different problem (applying changes to solve the problem of deciding when to switch lanes) while an uncertainty (finding the minima or finding the sign) is still being resolved' has a structure of 'keeping other variables constant while some uncertain variable set is being resolved' (keep a similarity like "lane adjacent to a required turn" while a difference based on or related to that similarity is being resolved, as in the uncertainty of "where to turn"), which are both optimizations across problems derived from mapping this 'navigation' case to problem/solution structures in the 'regression' problem space
        - identifying this problem/error/optimization set also identifies an important core problem format, which is the problem of switching ('switching between algorithms') and changing algorithms ('applying algorithms in structures such as combinations/sequences/mixes'), similar to the 'batching' and 'selecting' and 'scaling' problem formats
        - relatedly, the general solution of 'improving connectivity so that all points in a network are trivial' is possible to implement with structures like 'specific connectivity ("communication") optimization structures' (related to how 'improving cell communication in the brain' with substances like 'citicoline' or 'heat' or 'blood flow' or 'synchronized brain waves at a useful frequency like a default frequency' could solve brain problems like 'fungal infections' or even possibly 'tumors' or 'brain lesions')
            - this makes it trivial to identify other optimization/solution structures like 'building new connections between non-adjacent structures', 'rotating/switching structures so theyre automatically connected regularly with other structures', 'adding varying receptors so each component can interact with a higher ratio of structures', and 'ensuring inputs to connections/communication are present at all times', given how there are many ways to build connectivity, rather than just one way ('connecting existing/known input/output components') using common core functions like 'build', 'switch', 'add', 'vary', 'increase (applied to useful structures like inputs)', etc

    - identifying the interface structures of a known useful structure is useful (like the 'reasons for the usefulness of a structure' and the 'adjacent structures that make it useful like the surrounding system, available functions, inputs, etc') bc the surrounding interface structures are likely to also be useful structures and are likely to make some similarity/difference (such as a useful alternate problem format) obvious/trivial and usable for solving some other problem
        - for example, identifying the 'reason why' a known useful interface structure such as a 'constant' is useful (as in, useful through providing an absolute reference point, and useful in applying a structure as a constant can make "errors" obvious and can make "tests of truth" obvious), so that it can be applied in new useful ways, such as 'determining if something is true by making it constant' and "finding alternatives to 'constance' which are similarly useful at determining truth/falsehood of a statement" (such as identifying other interface variables like generality, simplicity, etc), and similarly identifying constants and the variables of constants is useful for identifying other constants from the 'variables that determine useful constants' (such as that they connect useful interface variables like non/linearity), and similarly identifying the systems/functions where constants are useful is useful for making other constants useful
            - relatedly, as mentioned elsewhere, this 'constant' structure is useful as a limit/filter of reality, as the 'set of constants' acts like a 'bounded area limiting possible solution sets'
            - similarly, its useful to identify other insights by making a structure more similar to other interface structures (make a structure a variable, make a structure a format, make a structure a certainty "such as a base", make a structure a concept, make a structure a requirement, and check if its more useful or obviously false in that role/position or fulfills other intents like 'being connective across useful concepts')
        - for example, 'identifying certainties' is a useful intent, such as how identifying specific certainty structures like constants is useful, specifically identifying constants that can connect useful differences (like how the constant of pi can connect differences such as linearity/nonlinearity or simplicity/complexity which are useful to connect), so identifying other sets of structures that are useful to connect and identifying constants that can convert one into the other is similarly useful, and these constants are useful to apply as default maximally different variables that can create useful differences (like converting between non/linear functions or constructing a network of numerical constants that reflects conceptual networks with nodes like 'simplicity' and 'complexity' and 'generality' and 'specificity', similar to how some functions can add simplicity such as derivatives, although constants are more interesting as 'unitary absolute reference points for a concept', acting as an 'interim point in a vertex between simplicity/complexity')
            - relatedly, identifying differences is more possible when a function is standardized, such as being standardized to intersect with (1,1), or standardized to fall in the y-range of (0,1), at which point its more trivial to detect differences
                - relatedly, identifying similarities is possible by connecting related numbers on the number line or other graphs with functions that have interval shapes such as waves, to see where various waves intersect (such as how many different interesting functions intersect at 1), or by identifying a new combination of attributes like 'curved inward and infinitely high in either direction toward 0 or infinity' or 'curved inward with negative slope' and making it intersect with (1,1) to standardize it
                - relatedly, identifying different core functions is a matter of applying changes in interface variables like 'variable position' (putting the exponent on the other side to create a logarithm, or making x a constant like e and putting x in the exponent position)
            - identifying these connections makes other graphs obvious, such as a 'number line with interface structures (such as "connections to other important numbers like the reduced set of numbers to solve most problems", graphed in the z dimension as a network or graph) and a common interface structure like "patterns/related numbers (indicated by waves or other interval functions in the y direction)"'
        - similarly, identifying certainties that are clearly wrong is useful, such as how 'capitalism' is 'forced selfishness' and 'socialism' is 'forced generosity' and both are clearly wrong in that theyre both forced so neither makes people free, and instead what would make people free is making resources abundant enough to require nothing to be forced of people, which requires (regularly) distributing intelligence, which is real freedom
            - applying almost anything as a certainty (by forcing it, such as by applying it as a constant/requirement) will make it obvious how that is wrong to apply as an absolute certainty, and if it is an absolute certainty, it will quickly become clear by resisting attempts to make it seem false, for example an absolute certainty like 'power is an important variable' will always seem true, no matter how much you change a system to try to make it seem false, bc of the definition of 'power' as semantically similar to 'important'
            - similarly, applying a variable as a 'generative variable' is useful to identify whether it can trivially generate other variables
            - this rule to 'find out if something is incorrect' (by applying it as a constant/requirement) is obvious once an incorrect structure is identified (a simplistic system like capitalism/socialism) and the reason for its incorrectness is identified (the 'forcing' and the 'simplicity' which doesnt reflect the real simplicity of reality, 'constance' and 'simplicity' being related errors)
            - then its possible to design a test ('force another variable to be constant/required' and 'force another variable to be overly simple' and check if obvious errors occur or if there is always a way that it still seems more true than false, no matter how much you force it to be constant/required or overly simple)
            - this is another useful truth/falsehood structure, in that 'obviously true/false variables tend to make it obvious whether other variables are also obviously true/false', as identifying the 'obviousness' of a statement is similarly useful as identifying the truth/falsehood of a statement ('obviousness' being related to 'simplicity' in some similarity or difference), similar to how identifying the generality/absoluteness or requirement/optionality or variability/constance of a statement is similarly useful as identifying the truth/falsehood of a statement, similar to how useful structures make other useful structures obvious through their adjacence/similarity
        - similarly, identifying 'how to make a constant useful' is similarly useful to identify, such as how applying pi as a coefficient "once other operations have taken place" makes it useful for generating randomness or nonlinearity (this property can be replicated by some subset of all possible number pairs when the pair is applied as a coefficient or exponent, where, by contrast, some number pairs generate lines that are nonlinear but seem linear in some subsets or significant ratios)
        - identifying alternate constants that can 'convert a structure into a base/unit/map/field/grid/graph', just like how (pi, 2) (and some subset of all number pairs when applied as a coefficient/exponent set) can convert a function into a nonlinear function
        - identifying these 'subset pair filters (which may be formatted as area filters)' to find alternate numerical pairs/sets (useful pairs like equivalent alternate 'factor sets' or 'coefficient/exponent pairs') is similarly useful, to find 'subsets of pairs that dont create other constants or other effects' (the subset of coefficient/exponent pairs that re-create some of the effects of (pi, 2) do not create all other effects, as some effects are distant rather than adjacent), these 'subset pair filters' being usable as a 'similarity index filter'
            - what is more useful than 'similarity indexes of all numbers' is a set of interaction levels where 'properties of number sets are trivially comparable (similarizable and differentiable)' where this interaction level set can store all useful variables and different interaction levels are created by some 'set of operations' that creates 'differences that are sufficiently useful to compare', so that the differentiating/generative variables of differences on those interaction levels are identifiable
            - similarly, more useful than a 'subset pair filter' is a 'generative variable/function of all pairs in a subset (such as a series/progression function)'
            - this answers the core question of 'what variable sets can be generated trivially by other variable sets, using which operations'
        - the problem of 'identifying useful constants' is not just a matter of solving the problem of 'connecting opposing interface variables like non/linearity or simplicity/complexity' but also a matter of solving the problem of 'identifying common factors across useful structures to connect', which is useful to identify as an alternate problem format of the original problem, which is useful in that it provides a more measurable alternate way to solve these conceptual problems, through formatting the problem in a more structural (and therefore measurable) way, which is another reason for the usefulness of a constant (it fulfills a structure, which is stable enough to act as a certainty/base/limit to support other variables, is measurable, etc)
        - connecting 'constants' to other 'certainty' structures on other interfaces is a useful 'cross-interface query' that can connect problems so that solutions/useful structures are reusable, which is like a 'stack of subset networks crossing each interface', where this 'constant' stack is useful to connect to its opposing stack, the 'variable' stack, so that it fulfills more interface metrics/structures (like 'multiple', 'equivalent alternate', 'difference-connecting', etc)
            - relatedly, finding 'vertex structures like vertex sets/networks/queries' across vertexes (like local/global and certain/uncertain) are useful to identify across problem-solving workflows that use multiple vertexes and spectrums and interfaces, so that 'finding common variables of vertexes and their useful sequences/connections' is more trivial, so that a 'graph uniting all useful vertexes to optimize for useful vertex structures like vertex queries' is similarly trivial to identify
            - for example, the 'function-intent' vertex and the 'function-structure' vertex are useful to apply in the same interface query (find 'structures matching some function', then 'find functions matching some intent', to find 'structures fulfilling some intent', which is a 'simple sequence of vertexes'), these 'structures of vertexes' being more trivial to apply to solve problems (with trivial operations like 'rotate' or otherwise 'trivially vary')
            - similarly, the 'constant vertex stack across interfaces' is useful to connect to the 'variable vertex' (or more completely/realistically, the 'variable vertex stack across interfaces') bc of the differences/information encoded in their connections
        - similarly, identifying other opposing structures is useful, such as identifying 'different' equivalent alternates (a 'similarity' structure) to identify other vertexes through these 'different' equivalent alternates which have an 'overlap' at some significant point/area but are useful in different directions/intents and useful in combination (united at that point/area)
            - for example, identifying 'different' equivalent alternates such as how a 'human brain' is an equivalent alternate of another similarly but differently equipped 'human brain', but a 'computer' is similarly an equivalent alternate to a 'human brain', and both are useful to retain rather than selecting one or the other (the 'different human brains' and the 'different computer as an alternate human brain' as an equivalent alternate of a human brain but in a different way than a different human brain is different, these 'different' equivalent alternates being useful to identify and store, so ideally a question would be evaluated by both an AGI model and a differently-thinking but similarly equipped human brain, rather than either one or the other, as theyre 'different' equivalent alternates that form a useful vertex where all/most variation can be captured effectively)
            - relatedly, an intersecting vertex set could form a structure like an 'info vortex' (such as in a conical shape) which would capture and standardize so much info that it eventually effectively 'closes to new info' or 'is destroyed bc it holds too much info' or 'destroys other variables not captured in the vortex'
            - relatedly, identifying connection structures like functions between these vertex structures (such as the core unit function like 'rotate' that can be iterated on some unit structure like a 'vertex' or a 'paradox' to construct all variation) is useful to identify other connection structures (like 'trivial ways to connect all info')
        - similarly, identifying how a 'requirement' could invalidate a 'requirement' (like if its more true than the other requirement) is useful, similar to to how identifying the limit of filterability and requirability is possible bc of invalidating structures (like how a requirement for a less causal variable can be overridden by a requirement for a more causative/determining/powerful variable, such as how it 'doesnt matter if criminals cant or wont be contributing members of society bc either way, they still need to go to jail')
        - similarly, identifying 'opposing structures' of useful structures is useful, such as identifying attributes that are not descriptive/determining/otherwise useful in regard to problems, such as how 'colorful' doesnt describe problems unless mapped to other known useful attributes like 'noisiness', and similarly, 'opacity' isnt useful as a descriptor of problems unless mapped to other structures like 'complexity' or 'obscurity', however all variables are mappable to problems but the 'degree/type/ratio/structure of change' required to make them useful is a useful attribute to identify

    - identify 'causes of requirement of a specific structure like a specific workflow' is useful to avoid those causes and those requirements, to allow other workflows to be useful, as a structure of truth (if a specific workflow is required to solve it, the facts that are known about the problem are likely to be false in some way, such as by being incomplete)
        - for example, identifying that 'unnecessary variation' or 'suboptimal variation (which helps errors evolve rather than more useful structures)' is useful to identify, in order to avoid suboptimal workflows like 'trial and error', which are usually only useful when a system has been allowed to become so complex that its not understandable, which doesnt describe any known system, as systems by definition have organization built-in to them, as often there are insights that reduce the set of solutions to some degree, so really 'trial and error' should never be necessary, as identifying 'causes of variation and errors' is almost always possible and where its not possible, that system is not understandable with current computation tools, which means the tools need to improve if 'trial and error' is necessary to solve some problem
        - relatedly, iterating through a subset of the list of filters of a solution set is useful as a way to filter out less useful filters
            - for example, some useful variable will be able to filter a solution set to reduce it by some useful ratio like by x%, and some other variable will also be able to reduce it by x%, so identifying these are possible adjacent/similar/equivalent variables is useful to identify in order to find their common input variables and generate other filters that would also reduce it by x% and therefore avoid those filters and find different filters that are differently useful
            - similarly, identifying the 'causes of usefulness' of a filter is useful, such as that it 'crosses a diagonalization' of a solution set (differentiates on more dimensions rather than just one) or that it filters out a 'significant ratio of a solution set' (such as irrelevant points like some outliers, redundant or otherwise non-determining points which create or dont change the same averages across averaging functions, randomly distributed points, etc) or 'filters different subsets than other filters' or 'filters more quickly or uses fewer inputs than other filters to identify the same subset as the other filters
                - similarly, filters like 'removing the midpoint of a point pair' (which is redundant in that it represents the average which can be constructed by some averaging/regression functions by the point pair) can be useful to reduce 'points to connect' in a distribution
                - similarly, identifying 'maximally different slopes' that can be created with 'relevant point pairs' like 'point pairs within some degree of difference from an identified general average slope from some subset (like a subset at regular intervals)' is useful as a reduced solution space to filter than the set of all point pair connections and which can create different functions trivially which can be considered equivalent alternates if they occupy a 'probable solution function range'
        - similarly, if a function like "reduce" is more useful when combined with an opposing function like 'reverse' (or 'change direction', so that it can also have other functions like opposing functions like 'expand' and orthogonal functions like 'equate'), its likelier to be a useful vertex for problem-solving or a useful cross-interface structure or a useful spectrum variable, as 'opposing/orthogonal' functions of a function are useful to include to generate self-awareness and self-regulation functionality to avoid known errors like over-prioritization, and similarly 'self-copy' is similarly useful for 'self-awareness' functionality (as in 'create a self-copy to evaluate the self-copy')

    - identify useful structures like 'areas of unsolvability in the required interim states between a problem/solution when applying a specific workflow' as well as 'variables (like maximally different equivalent alternate functions) to change a solution to improve it for a particular requirement/problem'
        - for example, given how common problem-solving structures like 'problem-solving intents' exist (which make them abstract problem-solving structures given that theyre common across different problems), these 'problem-solving intents' often have required solution structures (like how the 'intents' of 'merge two structures' or 'separate two structures' have a 'required structure' in common, which is the 'overlap' that is likely to be or is required in a default problem-solving workflow to fulfill these merge/separate intents, although there are other ways around it)
        - given that this 'overlap' structure describes the problem-solving process of multiple problem-solving intents (specific to a problem space that involves intents like 'merging/separating two structures'), in that it describes the 'interim connections' between a problem/input state and a solution/output state, it can be applied across problems to describe complex variable interactions using one structure, and determines the problem/solution in this case, so it should be applied as a determining/powerful variable by default when generating solutions
            - identifying all the functions like 'blur' and 'missing info (remove)' and other functions that can complicate the problem by changing how the structures appear, thereby causing error structures, are similarly useful to identify and oppose with 'counteracting functions that dont cause other errors'
                - abstracting this, 'identifying all the differences that can cause errors and opposing functions to those differences that avoid other errors and other suboptimal interface structures like over-prioritizations such as over-simplifications which could or will likely become errors' is a useful problem-solving intent
        - similarly, the workaround to avoid having to use an 'overlap' involves methods like 'take a unit component from the structure to copy and iterate it to create the copy (rather than creating an overlapping copy and separating it by changing its position)', or similarly 'identify input variables of a structure to copy and copy those instead of copying the structure to copy', which are alernative ways to solve the problem because they fulfill a different intent ('build from components' or 'generate from input variables' rather than 'copy and separate copies') identified using maximally different equivalent alternates
        - similarly, the 'interim space of possible states between complete overlap and complete isolation' is a useful space to identify 'areas of unsolvability' in, such as where its not computible to determine whether two structures are overlapping or not
            - relatedly, identifying the 'areas of noncomputability' in a unit of a more complex variant (such as 'whether two units of function networks have an overlap') is useful to determine the problem when complicated in a useful way (identifying 'overlaps' across function networks is likelier to be useful than just an overlap between two simple shapes)
            - identifying these 'areas of noncomputability' is useful to identify 'required changes to make a problem more solvable' such as 'changing the position/other attributes of a network to differentiate it in a way that makes the differences/similarities like overlaps clear'
        - similarly, identifying 'opposing intents' such as 'merge/separate' is useful to identify 'core problem-solving structures' that can connect the 'states required to make those intents useful' (states which could be either the problem state or the solution state given that changing the direction of these intents reverses the direction of the required solution and doesnt change the other variables of the states)
            - identifying more complex problem-solving structures can be done by applying more variables to oppose another function (rather than just one variable, such as 'direction' which is what differentiates merge/separate) and then finding problem-solving structures that are required in the interim space of connections between problem/solution states, where the solution state is identifiable by metrics
            - similarly, identifying 'solution metrics' of 'problems that are far more definable than the solution' is another useful problem-solving intent to fulfill for newer or more complex problems, for which the solution metrics add clarity to optimal filters of workflows to solve those problems
            - similarly, identifying the interface structures like 'limits' and 'thresholds' in this space are possible to compute by some workflow like 'trial and error' and, once found, can be connected to other structures that cause those structures (or other workflows can be applied like 'apply probable causes of interface structures generatively by default and check iteratively for a interface structure like a threshold until at least one example of each interface structure is found'), and then applied to other problem spaces to automatically identify those structures in other problem spaces, now that their position in a completely computed problem space is known & its connections to other interface structures is similarly known, assuming the 'usefulness' of the computations/complexity/other attributes of the problem make it similar or optimal enough to be relevant to other problems

    - identifying error structures that could result from otherwise optimal structures like 'generally applicable functions' is useful as an opposing/limiting/filtering structure of 'when not to apply optimal metrics like generality'
        - similarly, an error of 'over-application/over-prioritization' is likely when applying a function too often outside of its range of applicability/specificity, as a function may be generally applicable so it should only be used in specific contexts where it wont have negative side effects, such as where a function like 'absorb' is generally useful, but if applied outside of 'absorbing negative compounds' (if applied in such a way that it creates negative errors, such as 'absorbing positive compounds so they cant be used by the host') and if over-applied/re-applied in a situation where its already absorbed negative compounds and now contains negative compounds (so two errors are created, including 'ingesting the substance wont absorb negative compounds as its already absorbed its maximum', and also 'it may expose the host to those negative compounds its already absorbed and is bringing in'), it creates error structures
        - relatedly, another example would be an overly general antigen/antibody which confers resistance to some deadly pathogen, but also confers resistance to antibiotics as it forces pathogens to change and one of those changes might be antibiotic resistance
            - this is a 'type preserving operation', so that 'resistance to pathogens' also creates another form of resistance, which is an error structure if its resistance to a currently/frequently positive structure like antibiotics

    - identifying useful structures like 'insights' that can be used to generate alternate useful graphs to organize different info and fulfill different intents trivially
        - for example, a 'network of concepts with vectors indicating probabilistic connections between them' is a 'top-down' approach compared to a 'bottom-up' approach applied by a standard neural network that applies combinations of components to build more complex variable systems
        - similarly, a 'network of concepts where all adjacent sequences are true and more distant sequences are possibly true' or a 'network where connections indicate a proxy/alternate of probability (such as "ratio of each input component" which is related to a weight/probability coefficient, or "ratio of similar/equal attributes in common" or "probability of non-adjacent/eventual/emergent usefulness of a variable" or "probably useful difference/similarity at that position, given surrounding connections" or "overlapping ratio" across input components, or similarity/difference of the sequence, where commonly useful similarity/difference queries could be easily defined)' or a 'network where all interfaces are applied as possible connections, so that a "logical probability" (like a requirement or implication) and a "potential probability" (such as an adjacent possibility) and a "causal probability" (of causation in that sequence/direction) and a "conceptual probability" (given conceptual similarity or common conceptual connections/sequences) and so on all have probabilistic/similarity connections between each node pair' where alignments across interface probabilities/similarities would indicate a higher probability of truth, or a 'network of concept definition networks where multiple concept repetitions are allowed and aligned in a stacked network' or a 'network where probabilities are indicated by a ratio of connections (so a probability of 60% is indicated by 6 out of 10 connections leading to copies of that node, for specification of differences in routes to alternate nodes in a set with that probability)'
        - these and other networks are derivable by applying insights such as 'an adjacent concept to a solution can be more useful than probabilistic connections', and the idea of creating 'adjacent concepts to a solution' by applying changes to vectors building a concept (represented as a network of definition routes) and what variables would be useful to change such as the meaning of a connection between nodes (like 'probability' vs. 'similarity')
        - these networks would vary metrics/errors like 'only finding the simplest connections between nodes' which current neural networks might find
        - similarly, other variables are trivial to identify from applying insights and variables to generate new graphs, such as the variable of 'computation/memory/resource limits', which can vary by increasingly restricting computation by re-training iteratively, where a model can only use previous versions of itself and an increasingly limited ratio of computational capacity, requiring it to learn faster and retain only powerful variables and the variables generating their variation

    - identifying reasons why a particular solution like a function network (neural network) might fulfill structures by default/adjacently is useful to identify why/how to optimize those reasons/the structures they cause
        - for example, similar to how 'neural networks' are prone to 'using more variables than required' and similarly 'storing more info than required', they are also prone to other specific errors, such as 'favoring indexes too much, just bc they are given computation/storage', resources which they use suboptimally rather than optimizing that storage for patterns or other structures of understanding than simply 'mapping inputs/outputs', which is like 'storing all the different ways to spell a name rather than storing the root and the reasons a name might vary and the structures related to these reasons'
        - similarly, it might learn simple connections between words like 'structural patterns (such as grammar and usage patterns)' bc these are trivial to find with enough computation and example input/output indexes and simple combination functions, but these will be suboptimal for extreme use cases like 'understanding/meaning' such as 'prediction new priorities', and these simple connections arent adjacent enough to more complex functions like 'understanding' to adjacently create that
        - identifying the structures a particular function will select/create/use by default is useful when the implementation of the function can vary but doesnt, in order to identify why it doesnt vary (one reason being that 'simple connections between most linguistic structures are possible' while more relevant 'complex connections between linguistic structures are useful') and identify optimizations to make it vary as needed, so that it doesnt learn the simple version of a connection but the correct versions
        - similarly, identifying all the suboptimal usages and use cases and implementations of a structure like an index/map is useful, to avoid these known/derivable suboptimal use cases/usages/implementations

    - identifying common variables of known connections between extremely different structures is useful to identify new connections between extremely different structures (connecting 'maximally different functions' to 'symmetries' and 'solution-similarity and problem-difference' structures and 'potential fields' and 'fuzzy connections') and paths to those new connections (like 'iterate through various example structures from different interfaces, checking for connectivity')
        - for example, identifying similarity indexes in real systems is useful to apply as more probable variable interactions, to guide navigation across similarity indexes when filtering functions, such as how a 'two-way correlation can easily become a cycle' is a useful connection between similar structures that can identify different similar solutions than a purely numerical similarity index such as 'points of intersection' or 'similar slope sequences'
            - these connections between useful function types in real systems are useful bc they indicate 'adjacence' which is a proxy for 'probability'
            - relatedly, identifying numerical structures associated with 'missing info', such as how if a function has extremely high magnitude (high peaks), its possible that info in that peak is missing bc of how measurement tools often work, which is useful to identify as a possible cause of 'missing info', similar to how 'adjacent info in the real system' is likelier to be missing than 'adjacent input info in the data set' (except in cases such as where it was sorted and stored by input range), which is useful bc it connects the 'info' ('missing info' error), 'intent' (common intents, such as 'measure', given errors like 'missing info'), 'usage' (as in 'how are tools/functions used, given common intents'), and 'math' ('related function types likelier to have this error, given function usage') interfaces, which is a useful 'interface-crossing sequence' composed of 'cross-interface connection units'
            - mapping numerical structures to other interfaces may be the most useful vertex, which become more useful the more theyre connected to other vertexes
            - 'interface query vertexes' are useful to identify as 'reality-covering interface query sets', as a new way to apply vertexes (defined as 'sets of reality-covering variables'), where the vertexes are implemented by more than just variables, and in an organized way (like a sequence in a query)
        - I realized this by thinking about how 'independent/dependent variables' could be connected using structures like 'sequences from adjacence to distance' and vice versa, which indicates the structural sequence of creating 'independence' from a 'direct causal dependence connection', then wondering what other structures were clearly mappable like structures-concepts & other vertexes (like connecting independence to very different concepts like entropy, given that independent variables are likelier to cause the illusion of randomness, which is connected to entropy), then wondering what was possible to identify from combining vertexes and identifying all the connections within a vertex (from independence to dependence) and between vertexes (from independence/dependence to certainty/uncertainty), then realized a 'graph of overlapping potential fields created by iterated interface structures' would be useful
            - then I started thinking about 'similarity indexes' created from this 'graph of overlapping interface structures from potential fields' and then back to a useful structure, the 'set of maximally different functions' and how they are also 'maximally similar functions' (maximally different functions are the most similar function in a very different range from other maximally different functions, so they have a core similarity of being average in some range, where the ranges are the core difference), and ways to identify more useful 'similarity indexes', which led me to think about the 'most similar function to other functions' (a function that has a unit of other function types embedded in it, like hyperbolic, parabola, linear, step, non-1-to-1 mapping, etc, which is the most trivial to convert to all other functions), and how this function wasnt particularly useful for filtering functions, but was useful as a store of possible variables to apply
        - given that 'maximally different functions' are very useful, and that they are a specific implementation of 'a set of a similarity and a difference' (core similarity of 'being average in some function set' and core difference of 'that function set being very different in some metric like range/type'), what are variables of these implementations and what other specific implementations of this similarity/different structure are similarly useful:
            - 'symmetries' are a known useful 'similarity/difference set' as detailed before ('limit on differences applied to a core similarity')
            - 'similarities to solutions overlapping with differences from problems' are similarly useful (like 'concepts adjacent to solving a problem')
            - 'potential fields' are a useful 'similarity/difference set' in that they apply trivial changes (differences) to identify adjacencies (similarities) of a core structure
            - 'fuzzy/wobbly connections' are a useful 'similarity/difference set' in that they maintain some core connection 'angle range' between two structures (like a problem/solution) but allow some differences in the endpoints (structures similar to problems/solutions)
            - these all apply some workflow (like 'change a base solution') bc similarity/difference sets/networks form the basis of workflows, which seek to apply those sets
        - relatedly, a graph of overlapping 'potential fields' created by applying interface structures iteratively is a useful graph to identify, where these paths/ranges of overlapping interface structures in potential fields are useful to identify as possible interfaces/concepts, given their similarity through the overlap with other interface structures
        - given that in a 'high entropy system', all possibilities could be equally likely and more possibilities exist, 'trial and error' is somewhat useful (in that it accounts for 'equally likely possibilities' by trying them all or until it finds an acceptable solution) and also somewhat useless (in that a higher entropy system will have more possibilities to try), so 'entropy' is a useful workflow-filtering solution metric, so some 'modified variant of trial and error' is likelier to be useful in this situation, given the difference from an optimal case for that workflow, such as 'identifying maximally different possibilities, after identifying probable function volatility' (volatility increasing computation requirements for finding maximally different functions in a set of possible solution functions, and therefore being generative of useful differences like embedded differences and 'determining requirement' differences, in its extreme ability to determine other variables like computation requirements)

    - identify useful structures like new ways to apply a useful structure (applying 'system/case' as a generative variable of the usefulness of a function), and all the interface structures derivable from it (like problem-solving intents related to it ('generating new systems/functions'), abstractions/workflows that make it more useful (applying a workflow to a vertex to fulfill intents like 'identify all connections'), etc)
        - for example, a function's usefulness (whether its a solution or not) is often determined by the case where it's used, which makes it useful (its useful to connect cases with functions that are optimal in those cases), so identifying a new case is an input to 'identifying whether that case is an error/problem' and also to 'identifying functions that are optimal/useful in that case' and therefore 'identifying a new problem-solving function'
            - 'generating new cases' is therefore a useful problem-solving intent to fulfill other related intents like 'identify new problem-solving functions' (an overlap of the system/function and problem/case-solution/function vertexes)
                - 'generating new functions' is similarly useful in the opposite direction, but 'cases that are possible/probable' might be more trivial to identify than 'possible functions'
            - this is similar to how 'identifying a new error' is an input to 'identifying a new variable'
        - this is a result of 'identifying all the inputs/causes of the usefulness' of a particular structure (like a function) which includes structures such as the 'system where its being applied, or what other structures it could interact with, or the specific function inputs', applying the 'system where the function is applied' and its 'connections to usefulness of a function' as a generative variable of a useful function
        - similarly, the 'potential impact' of a function on a 'specific system in a specific state in a specific position' is useful to identify, to identify 'how a function will change a system, once applied in a specific position/state of that system'
        - the variations of these vertexes (all the structures where a connection can 'wobble' to create a 'fuzzy' connection) are useful to identify, to fulfill the workflow 'change problem/solution structures and connect those changed versions instead', applied to structures that are useful to identify all the possible connections of, such as vertexes
        - I realized this by thinking about 'differences in usefulness given an input case', and thought about the word 'determine' recently, so it was trivial to wonder 'what determines usefulness' and fit 'input case' to that
        - relatedly, the word 'determine' is similarly useful as 'identify', as 'determining structures' are useful to identify 'limits of computation requirements' (once its determined, no more computation is required)
            - relatedly, 'determining structures' are like 'adjacent concepts to solve a problem trivially' in that a 'determining variable' of the problem of 'filling a box' would be a structure that is adjacent to the size of the box, like a cube in a cube that almost fills the cube, in which case the smaller cube is a 'determining variable' of the other box, in ways that the 'simple set of dimensions of the box' are not (changing the specific smaller cube trivially solves the problem in ways that trivially changing dimensions of the cube dont, without an additional function of 'changing dimensions of the cube to create another cube that fits in it or contains it' which is not as trivial), so 'determining variables' of a problem are also 'adjacent/approximate solutions' of it
        - relatedly, the 'possible/required scaling to solve a problem' is useful to solve for as a problem-solving intent, so that iteration and iteration-optimizing functions can be used to solve it, which is more trivial with 'adjacent structures' to the solution (such as a similarly sized box as the solution), similar to how a unit case is useful for determining computation requirements

    - identifying trivial but important differences between similar structures is useful to identify variables of the most important trivial differences/variables
        - for example, 'filters' and 'limits' have a similarity in how they change a structure like a set, however, a filter would 'reduce solution count' of a set by 'removing some solutions' (after first generating a list of possible solutions), and a limit would be likelier to be positioned to reduce changes during the generation process or limit an area where solutions are possible to a sub-area (to find/filter or generate solutions in), as limits relate more to thresholds (limit above/below a threshold, as in a barrier to change) and filters relate more to attribute sets (variable values), as in 'find solutions with this pattern or variable', though the distinction seems trivial bc they can both be modeled as lines limiting/reducing an area, and similarly limits relate more to maximums/extremes and filters can identify interim/regular values following a pattern which is extremely different from extremes, so limits can pre-filter a set so that filters are more effective (limits help make filters more powerful/useful, however its not always possible to identify useful limits of generative functions and comparing/analyzing generated solutions is often necessary to identify these useful limits)
        - similarly, 'generate solutions' is made more possible by applying other structures, such as 'types' which reduce the computational requirement of the 'generate' function
        - the triviality of the difference between these structures becomes important in how they can vary and be used and otherwise interacted with regarding interface structures like problems/solutions
        - 'filter' and 'limit' have similar impacts on a solution set but are used in different ways, to filter out different differences, at different positions in the workflow (such as at 'generate' time or 'filter' time), which has an impact on possible solutions generated/selected so it impacts solution metrics, as well as performance metrics, in that a 'limit' positioned in a 'generate' function could fail to identify a possible extreme solution in a way that a filter might not be similarly biased
        - similarly, 'combining/grouping' functions (the opposite of 'filter' functions) are useful to connect, as they can replace 'filters' as a way to 'group relevant sets' for example, which could make 'filtering a set' unnecessary
        - relatedly, these functions are not always useful but are frequently useful when applied to different/opposing structures (for example, 'grouping all sets in all possible ways' isnt particularly useful bc its not computable but 'grouping filters', so they are pre-filtered bc of their group identified, could be useful)
        - identifying important differences between similar core structures is useful bc those similarities/differences will compound the more theyre used, which is likely as theyre common/useful structures, so once these are identified, their scaled impact can also be identified/derived
        - 'differences between core structures' will scale more and explain/determine more than other structures, so they will become more useful through their scaled interactions, so theyre more useful to identify

    - identify useful structures (like iterations of interface structures) which are useful for being adjacent and/or also useful for fulfilling some intent like 'connecting more interface structures' (which is useful for intents that involve 'connect' functions and 'input/output sequences' and 'interactive structures') and "'de-arbitrating/apply meaning to' a variable" (by connecting it to known certainties like known similarities/differences which are also relevant/useful/meaningful)
        - for example, 'filters of filters' are useful just like 'generators of generators' are useful bc every useful structure has problems, even if they are useful for solving some problem and theyre specifically useful at solving that exact problem ('filters' has a problem of 'filtering which filters to use with which interface structures like which cases or solution metrics', and 'generate' has a problem of 'generating generative variables', to extend the input/output sequence another degree)
        - by extending this input/output sequence another degree, another connection is made and some structures can be pre-computed as useful (whether generally or specifically useful, in simple/complex cases, etc)
        - 'useful filters' for example are 'maximally different' variables, so 'filtering the list of filters to include maximally different filters only' is often useful for differentiating intents, whereas numerical mappings tend to be more arbitrary unless heavily standardized (such as standardized to a reduced set of numbers that is adjacently connectible with a high ratio of functions, making that set a useful set for modeling other structures), so 'filters that identify only one solution' (such as 'identify the first solution above a threshold' or 'identify the best solution') are often less optimal for being less general, unless the trajectory to that filter is capable of identifying generality (such as 'simplifying filters' or 'regular interval filters'), and 'filter/identify filters that leave one solution possible' isnt clearly mappable to a relevant solution metric (like its implementations such as 'identify the first solution above a solution metric threshold' are), yes even filters have problems too

    - identify useful structures like structures which create/identify other useful structures, such as how 'power dynamics' impact 'info structures like certainty ratios/cascades' and the variables of interface structures of them (like variables of why they cause other structures to be created/identified)
        - for example, its useful to pre-compute various power dynamics like 'one ruler over all others' and how they interact with info structures like 'certainty', such as where a 'optimal or all-powerful AI' is allowed to exist, built from successive lower layers of inferior AI, which is suboptimal (just like having one government form or one government or one priority or one simple rule is suboptimal), as it adds certainty/info and decreases possibilities, to the point where the other possibilities lose their potential, and energy is directed to the all-powerful structure, which is worse for most other structures including agents
            - therefore, structures like 'one structure thats more powerful' are useful for creating certainties but its not optimal to create all certainties, as there are many ways to 'determine the universe' (blowing up as much as possible with chain reactions is one way), but what is more useful is a balance between uncertainty/certainty, so that alternative possibilities can exist, in case the 'determining method' was incorrect which can only be detected if its not selected/invested in, but rather detached from
            - identifying this balance as useful is possible using power dynamics
        - similarly, the structure of a 'predetermined fate' is that of a 'set of routes that all lead to the same path/point', which is only possible if all other 'predetermined fates' allow such a path/point to be traversed or to exist
            - this structure identifies other useful structures, such as how 'changing the variables like magnitudes/positions/overlaps/sequences/directions of cycles in a system' is often a useful structure to solve problems in complex systems (such as 'justice distribution' and 'supply chain optimization' problems)
        - similarly, the structure of a 'rotation of a unit' often means a symmetry is determined/understood, so abstracting that to identify an 'angle change' (or 'switch between dimension sets') and a 'unit to apply it to' (iteratable structure that can be used to compose/change useful variation around a symmetry, such as 'all rotations/angles' determining its differences) is a useful set of intents to find this structure to determine the symmetries in a system, just like determining the 'center/base and the change limit' are useful to understand the symmetry
            - relatedly, finding the 'symmetries of symmetries' is useful, to identify other symmetries that 'change symmetries around a base' and 'symmetries which act like limits of other symmetries' and other iterated interface interactions
        - similarly, 'other beneficiaries' such as 'adjacent structures' of a function that creates/uses some benefit is identifiable once 'cost/benefit' and 'incomplete' are applied (the function is an 'incomplete set' of the structures that could benefit)
            - I realized this by thinking about how an agent might not notice something for various alternative reasons, such as if theyre 'hoping for the opposite/busy implementing it to notice it/benefitting enough to intentionally avoid noticing it' and noticed that given this variability, similarity variability could apply to 'allocation of benefits' to other agents by proximity/adjacence or some other similarity (like parasites benefit from a host's resources, by their proximal position), bc an agent cant see some group dynamics easily if they sympathize with groups too much or if they obey incentives too much, so they have to be sufficiently different from groups ('relying on the group' too much) and incentives ('obeying incentives' too much) to see the errors of the group interactions or scaled interactions such as scaled incentive-obedience
        - relatedly, identifying 'interface structures' of the structures used in a workflow is useful for identifying structures of workflows like 'areas where workflows overlap' bc these interface structures were applied to workflows and iterated until overlaps and other structures were created, so these structures were identifiable bc of these iterated applications of interface structures
        - similarly identifying 'mutually exclusive' (they cannot occur together) and 'mutually required' (they have to occur together) structures as trivially convertible is useful, is an example of other useful structures that are trivial to derive from each other by applying interface structures (applying interface structures like 'opposite/extreme' to 'high variation variables' like 'co-occurrence/set membership')

    - identify useful functions that create useful structures like workflows from other useful structures (like an info structure + a system/logic structure)
        - identify structures that, when adjacently changed such as adjacently combined, create other useful structures, such as how the system/logic structure of a 'requirement (of a problem to solve)' and the info structure of a 'lack of information' creates the workflow 'trial and error' adjacently, as in 'when info is lacking, all possibilities need to be tried' (more specifically, until an acceptable solution (solution metric threshold as a halting condition) or all solutions are found (no halting condition))
        - this is derivable using the following changes (applying similarities with trivial differences to generate similar structures as the previous input in the sequence applied to 'one of the structures', then integrating it with the other structure 'lack of info' in the next sequence)
        - 'problem to solve' -> 'requirement (of a problem to solve)' -> 'general solution metrics' -> 'possible solutions'
            - 'lack of info to filter possible solutions' -> 'try all solutions' ('trial and error')
        - relatedly, a 'symmetry' is a specific type of 'difference in a similarity', as it is a 'specific 'ratio/type/combination/set/other interface structure of differences' that fulfills a metric of 'not invalidating the core similarity forming the symmetry base', where all the other similarity/difference interactions are similarly useful to identify and apply as common/core/default components of systems

    - identify useful structures like 'useful queries that lead to useful thoughts like identifying other useful structures'
        - for example, thinking about the following is more often useful than not:
            - previous/recent/useful workflows/other structures identified
            - problems with variation that seems to not be or is definitively not completely described yet
            - new connections between high variation structures like concepts, where the new connection might also have a new way to find the new connection (new workflow), and where the new connection might have the highest value given the variation of the structures being connected
            - vertex structures like 'common variables' of useful structures
            - identifying the limits of usefulness and its causes like where useful structures arent useful, to identify usefulness areas/ranges
            - random/extreme/scaled structures/variables and how they interact and how to connect/format them in new ways, given the high variation often required to connect them
            - new formats/graphs of problems which often have new solution methods in those formats/graphs
            - identifying structures that are not easily or relevantly described with change/component combinations (like a network of recursive/overlapping paths rather than a two-dimensional network), or finding the components/units that can be combined to identify structures not easily/relevant described with change/component combinations (units having some value in all of the available dimensions of the network)
            - identifying abstractions of other structures and how theyre connectible to the structures in a useful way (like overlapping types connecting to examples)
            - thinking about useful specific interface structures like 'similar but different' to identify useful structures like 'equivalent alternates'
            - newly identified core functions or other function types and how to apply them in a new way, which might be a new variable or workflow
            - new differences from identified useful structures which could lead to other useful structures (like differences from useful structures like common variables that would be useful to identify equivalent alternates of useful structures)
            - new structures/variables like 'variables that capture more complexity or some other metric/concept'
        - these structures focus on connecting useful structures, identifying new sources of variation and the interface structures surrounding variation, given its usefulness for determining new structures like new workflows, and other problem-solving intents, applied to specific structures like 'understood problems/concepts', 'recent solutions/structures identified', and other useful structures that are useful for their specificity and recent usage/storage in memory, meaning navigating them is more trivial

    - identify useful structures like the 'meaning of a similarity/standard/useful structure' to pre-compute the meaning of that structure rather than calculating it later when integrated into the meaning/interface interface
        - for example, the 'best' in a field could be identified by many standards (the best in the past century for 'technological access' similarity, the best alive for 'testability/provability', the best at using AI/other technologies to fulfill their intents, the best at fulfilling intents without AI/other technologies, the best at explaining/automating/understanding their field, the best given the variation they identified/added to the field, the best given the degree to which they made other tasks in the field trivial)
        - these various standards of 'best' have different meanings, which is useful to identify before selecting/applying them, such as how the 'best in the past century' is high variation and doesnt account for differences in decisions to use technologies they had access to, and the 'best alive' have a meaning of 'functionality like testing them on other problems'
        - pre-computing the meaning of structures like similarities/standards is useful to identify so that the ultimate meaning/relevance/impact (at scale in its interactions with other structures) is more probably predicted, which simplifies selecting structures

    - identify useful structures like 'non-1-to-1 mappings' that are useful to combine with '1-to-1 mappings' (or approximate mappings like extremes/limits) to identify structures that offer a mix of certainty/uncertainty and similarity/difference to fulfill a useful ratio of those opposing structures to cover high-variation differences common in problems
        - rather than asking what is associated, what is not associated (what variables dont seem to be particularly associated with other structures)
            - for example, just thinking about numerical values is often too isolated from other structures to be useful (theyre only useful when integrated with other structures like causal sequences of values, position of values, etc)
            - similarly, some variables are similar enough to be associated (extremes are associated with limits, as extremes are useful for identifying limits, just like how solving a unit case and then iterating it can identify limits)
            - other variables are unassociated and general enough to be difficult to connect (such as physical ratios like low/average/high which are commonly significant in determining other variables but arent clearly mapped to all useful interface structures - 'whats the certainty of low vs. high' - the 'certainty occurs if a low/high value adds more info, which isnt guaranteed by a low/high value, as opposed to a more clearly useful structure like a specific/abstract structure'), unlike how some variables are definitely connected though not equivalent in every way (abstract/specific, certain/uncertain, simple/complex), where their 'common structures' tend to be interface structures
                - this is useful to identify these 'low-info and less associated and more general' variables that are nevertheless very powerful structures ('commonly determining ratios' are useful to identify and apply), bc they offer a counterpoint to the definitiveness/certainty of interface variables
                - this means "when a problem doesnt fall to interface structures on one interface like similarities/differences, apply less certain/connected/specific variables like 'commonly determining ratios' which dont clearly map to any other structure in a 1-to-1 mapping)"
                - these general structures like 'commonly determining ratios' offer alternate sources of complexity than 1-to-1 maps like adjacencies or approximate adjacencies (such as extremes/limits)
            - the interactions of very different structures within an interface like similar/different arent always contradictions (an 'adjacency/similarity' between 'extremes/limits' doesnt invalidate the extremity/difference just bc a similarity is applied, as difference structures are connectible just like similar structures are), but rather similarities/differences are highly interactive/coordinating structures that can contradict each other but arent required to
        - this is bc some structures are not always clearly optimal/useful and not always clearly mappable (like abstract/specific mapped to certain/uncertain or simple/complex), so that variation in the lack of clarity is useful for adding differences to a solution where certainties like known useful structures arent sufficient to solve it
        - this is useful in the sense of identifying 'useful cross-sections' of a complex system that explain/capture a lot of its variation (cross-sections being another structure to format primary interfaces, which cover reality)
        - similarly, 'cost/benefit' and 'input/output' are not 1-to-1 mapped, but these structures would seem to be 1-to-1 mapped if incomplete similarities are examined ('inputs' are related to 'requirements' which can include 'costs', 'benefits' are often seen as the 'goal/intent' which is seen as the 'output' in some cases)
            - identifying these high-variation variables which seem but are not 1-to-1 mapped is useful to identify new variables and identify structures that cant be directly equated, so they should both be included rather than standardized to one structure

    - identifying useful structures like 'units/subsets/maximally different examples' of a useful structure like a 'solution automation workflow' (having functions like 'change scale' to solve/indicate a scaling problem or "test a subset of a workflow's functions for progress toward a solution" to test a workflow for usefulness) is useful to identify and apply to avoid workflows that only show progress if applied completely, filtering by workflows that are volatile in between the steps of the selected subset or which have other functions that could contradict the impact of the selected steps, as a way of checking which workflows/solution functions would be useful to invest more steps in by testing a subset or a unit
        - relatedly, identifying sub-problems or causative problems of the original problem can be a matter of testing associated solution functions of a set of problems, such as 'test if a sub-problem/causative problem is the scaling problem' (by "applying a 'change scale' function to variable sets" and 'checking if progress is made toward a solution')
            - similarly, 'units' of workflows can be applied to check for initial progress of a workflow, and similarly, 'maximal differences' of a workflow can be applied to check for possible predictable progress later in the workflow sequence, so identifying 'units' and 'maximal differences' created by workflows is useful to identify and store and apply to check a sub-set of a workflow's functionality for indications of progress toward a solution

    - identify useful structures like 'reasons for the usefulness of a structure' which can be applied as 'sufficiently specific filters' of solutions which have a useful effect of 'transferring the variation to other problems' like 'organizing useful structures to maximize usefulness' (such as only applying a learning function to a point below 100% accuracy, since a function that doesnt change at all isnt optimal in that it cant improve, and there are always new variables to identify in a given problem space, problem spaces being essentially equivalent)
        - for example, identifying 'reasons why a structure is useful' is a structure that provides enough info to 'act on' (as in 'select the function of') 'including that structure in an optimal solution', reasons such as 'this function is still changing and improving, so its probably still learning, and still resolving some uncertainty, while being correct enough that it seems capable of resolving that uncertainty, even though it does make errors at various points which are not predictable enough to be impossible for it to learn optimizations/solutions of, as its displayed the functionality to improve on past errors through not repeating them, and its new errors are not simply compliant with any identifiable pattern', which is a useful definition of an optimal structure that is specific enough to be usable, as 'specifications of definitions' are particularly useful for finding implementation functions of those definitions
        - similarly, having 'multiple alternate structures (equal in some way) to provide evaluation functionality' such as sub-networks in the brain which are equal in some way so they can evaluate each other's errors, or a host network that contains the other networks, which can evaluate its component networks as it is not a sum of its component networks but rather is its own network
        - these reasons are also solution metrics, such as how identifying 'functions whose errors are infrequent, insignificant or quickly fixed, not repeated (indicating the function fixed them), and also not compliant with any pattern (indicating its capable of learning)' (general functions) and similarly (but oppositely) 'functions with very predictable errors' (which is a useful function in that it specializes/optimizes for some specific task and gets some other specific task wrong, which is useful in that its errors will be easily identified and fixed with opposing structures), are useful functions to apply as filters of useful machine learning networks 
        - relatedly, identifying 'unimportant/irrelevant errors' is useful as a way of determining which errors a function should be optimized to avoid, if a subset of errors has to be selected for prioritization
        - so 'improving/changing functions that make new errors' and 'multiple alternates of useful structures' and 'opposing structures on some interface which are differently useful (like abstract/specific structures)' are useful structures to include in a neural/function network, but only in specific positions in the network structure, 'positioning/organizing useful structures to maximize their usefulness' being the problem to solve once the useful structures to include are identified
        - 'being optimal' is also a suboptimal structure in that it is low-variation (it wont vary once its optimal, in the universe where variation is time and preserving time is optimal), unless it optimizes for everything, which is unlikely
            - identifying these 'opposing intents/priorities/structures' like 'being optimal' and 'preserving time/variation' are useful to identify, as they act like dichotomies (to expand into more variables) or tradeoffs (to integrate with some more complex interaction like embedding) or ambiguities to resolve (by identifying some defining difference that is relevant and makes one more clearly optimal in a way that the alternate cant adjacently fulfill or provide an alternate to)
        - similarly, an algorithm that applies useful opposing structures within dichotomies (such as thinking and deciding/acting, which both take so many resources that one often has to be chosen, which creates error like 'inability to evaluate actions simuultaneously as acting, as the agent is too busy working to fulfill that action and thinking about adjacent structures like immediate outputs of the action and benefitting from or focusing on benefitting from the action to think about the action with scaled relevant interface structures that can identify the ultimate meaning of the action') like 'think more, act/decide less' is useful for developing good analytical functionality (functionality that can help avoid decisions and avoid having to make decisions as it can avoid dead-ends or other limited options), as an algorithm that can avoid most decisions (avoid having to choose one option bc of resource constraints) is more optimal, as it means the algorithm can identify alternatives, however never acting is also suboptimal but there is a ratio that is likely to optimally balance these two alternates

    - identify useful variables that act like 'dichotomies' such as 'universes' and 'computers' through their common interaction functions ('embedding', 'optimization')
        - the universe that allows the most/best computers is one that can contain all other universes, bc computers offer the possibility of a multiverse of many different 'maximally/relevantly different' timelines (simulating the past and other components of reality better than reality can, as past info was lost for the most part, with better storage/representations/retrieval/etc, allowing different possibilities to become real in the simulations, thereby applying more variation to past info than was preserved in reality and therefore making the past more real, similarly when simulating the future the future becomes more real if the simulation is good enough), where if a computer has a good reality simulation but cant compute something, it either needs to improve its reality simulation or that something is false, which can be checked against other computers that simulate different realities to check for the same result, an indicator of falsehood of the something if repeated across simulations, simulations which have errors like 'attracting variation from other universes, so that they live in this universe in a simulation that is better than their reality instead' which requires identifying 'what type of life form would be attracted to a particular reality and what are their possible rewards/intents', and therefore 'more optimal paths' between different simulations of the past/future can be connected than the paths that occurred in this timeline (so 'computers enable constructing different timelines and traveling across timelines'), and if one simulation/computer or simulation/computer set is drastically more optimal than the others it could very well contain the others, including this universe
        - I realized this by thinking about how 'historical figures could be brought to life by a really good simulation (that is better at storing info than reality, which lost that info of their life)', which made me think about the iterated effects of many optimal computers given their simulation power, how real a simulation could be, how it relates to existing structures like multiverses, and how these computers/universes could interact and what side effects would occur
        - what seems likelier than a QFT/multiverse model is a set of overlapping universes, as assuming they have isolateable effects may be assuming too much, and given that they vary by dimension, some are likely to have dimensions in common with other universes and may reflect these dimensions across the universes which have those dimensions, so traveling by these 'common dimensions' may be possible given their possible interactivity or overlap, and rather than repeating a dimension independently/in isolation, its likelier that common dimensions overlap/are re-used, unless so many variables have been enabled that divergence of the dimension into multiple alternates is possible, but initially its likelier that they are overlapping/equivalent
        - relatedly, computers that allow higher dimensional modeling may be useful for modeling universes with the most dimensions (able to contain the other universes)
        - relatedly, 'sending a computation to another universe (simulated in a computer) where its more computable' takes time away from that universe (as it involves 'required work' which takes away time/resources from whatever it was computing), and a 'universe death' occurs where its computational capacity is reached and no time/resources/possible changes are possible in that universe
        - a 'universe grid/network' of hypercomputers that sustain a reality that is more optimal than default reality could be a useful structure in future, where 'killing a universe (in its default purpose of an uncertainty/potential-maintenance device) by overloading it with computation requirements' can be avoided for required components of the grid/network, which would form the new base of reality (the highest variation life would occur on this grid of computers)
        - the grid that requires the least work from other/component universes and allows them to compute whatever differences they compute by default (which are useful in some way like computing new differences, or improves their computation so they can compute more of what they tend to by default and have the option of computing other things) and stabilizes any possible universe simulated on a computer (integrates it with the other universes so they can coordinate) will be more successful
        - this grid is a more optimal way to 'travel the universe' as it makes everything trivial to compute (including different ways to achieve space travel, different routes of space travel, etc)
        - when universe simulations are optimized in this way, every universe simulation will likely converge to the same universe model, once an optimal one is found, at which point keeping the computers distributed or merging them is a question to answer, a merging which would mean there is one final universe and once it is optimal, a question of 'why would it continue to exist' is default at that point (is there variation that is only obvious at that point, such as the variation of destroying nonoptimal structures, in an optimal universe that supports the most/useful variation, which it would continue to compute stabilities and other optimizations of rather than ceasing to exist), or would it be better to keep the distributed computers so that they could evaluate/replace/fix the others and identify when they were making an error of some computation, and is that possible if theyre all optimal, and if the variation of destroying suboptimal structures is the only variation at that point, the optimal computers would all have to stay the same or be destroyed by the others, which would likely leave zero computers, as an optimal computer might be subject to some new variable it hadnt computed before, such as if its optimal for a previous definition of optimal but not a new one, given the new variable, and is an unpredicted variable possible in an optimal computer (could variation develop quickly enough or at all, so that an optimal computer, which can compute everything that exists or is likely to exist, could still fail to identify the new variable and be found to be less optimal by the other computers which didnt encounter that variable yet, which implies that its useful to keep these computers distributed so they can learn from another computer's failure to account for a new variable such as this, assuming its possible, and the 'optimal structure of a universe' is actually 'multiple copies of an optimal computer simulating reality equally well' rather than the 'merged and then nonexistent computer', as if the multiple optimal computers fail to encounter any unpredicted variable at any point, they will just remain stable infinitely, which is also the end of time, just like the merged computer is the end of time as they are the end of variation)
        - the core structures of the universe (like 'multiple' and 'network' and 'alternate') also seem to describe its final state, where first these structures were used to generate variation, and then these generated possibilities were filtered to identify optimals, which resulted in a game of 'distribute vs. merge' that has the same outcome (the end of time), and this final state was described by these core structures again
        - relatedly, once its known how the universe will end, the various paths there become less interesting (less high variation), so the moment its known, the universe has basically started to end (it reached its peak variation and will start to decline in variation), and that is similar if not equivalent to ending it, so the variation becomes 'how to change the path to avoid the final state, or apply/embed variables so that state is never reached, like always finding new opposing variables of variables that cause progress toward that state so that it is never reached', though there are limits on when this 'variable finding' can happen where it could still change the path sufficiently to continue to delay the final state forever (identifying confusion structures like high variation structures and applying them enough to avoid a straight path to the final state), as one sequence of time is unlikely to be a stable structure, even if it fulfills the definition of stability and optimality in all known ways, and relatedly given the value of alternates, as what is currently known/computed may be proven false or changed at some point
        - 'finding alternate paths' would have a solution structure of 'identifying different ways the universe could end than the path its trying to diverge from (merge or distribute, ending time either way)', so identifying 'alternate end sequences' (using different workflow functions than merge/distribute) would be useful to identify the 'paths to those end sequences'
            - for example, identifying a 'end sequence' that uses 'generate/filter' (by generating the variation required to sustain time, then filtering that set of variables for usefulness as needed) is an alternative end sequence than the 'merge or distribute' dichotomy, as a 'end sequence' with only two options, where only one can be selected, is not an optimal structure (where the definition optimal involves 'maintaining variation/time' as top priority), so in reality, an optimal computer would not produce such an end sequence, which contradicts the progression outlined above involving a grid of optimal computers
            - however an optimal computer may identify different priorities than 'maintaining time/variation', so that definition of optimal may not be stable/valid

    - identify useful variable interactions (like truth structures, like truth patterns) that make useful structures like different possible solutions clear ('ambiguous alternates' as 'possible ways a statement could be true or false', which once identified, can be clearly solved with some trivial similarity/difference check - given how variables are related, how true/false are related, how similarities/differences are related, etc)
        - for example, a frequent truth structure is 'x is related to y, but y can result from x-distorted as well as well as x-opposite as in x-negator', as in 'x true y, and x-false true y and x-opposite true y', which is a 'realistic/truth structure, including contradictions/differences/neutralizations and alternatives'
            - identifying truth structures is useful bc truth structures are likelier to be related to other truth structures, like other useful structures (a truth structure is likelier to be stable, so its likelier to have more interactions, such as more interactions with interface structures such as limits/requirements/alternates)
                - this truth structure makes it possible to identify related useful structures, such as 'reasons/ways it could be true or false' 
                    - identify possible different solutions (ways it could be true/false, positive/negative, abstract/specific)
                        - this could be true bc of different effects that happen at different amounts, among other possible causes
                           - or it could bc false, in the sense that it could mean that x does not cause y at all ('x true y' is false), since changing x in these ways doesnt change the causation of y
                    - identify similarities/differences to resolve the ambiguity between these alternate solutions
                        - 'different effects at different amounts' (true structure) is an ambiguous alternate of 'no causal relationship' (false structure), which is useful to identify
                        - limiting the number of these alternate ambiguities is a useful truth-creating structure (the fewer ambiguities there are, the likelier each possibility is)
                        - identifying ambiguities between true/false structures is useful to identify alternates to filter out, by applying the 'true/false dichotomy' or applying similar/different to 'true' (the opposite of true is false, so finding a structure fulfilling a 'false' version is useful)
                        - similarly, finding 'ways to connect/differentiate these alternates (different effects at different amounts and no causal relationship) and their common variables (x, y)' and 'adjacent interface structures (like limits and implications) of the alternates' will be useful to resolve the ambiguity
                        - connecting 'different effects at different amounts' and 'no causal relationship' (independent, indirectly connected, incompletely connected, random, etc) can be done by changing the 'different effects at different amounts' structure to be more 'random'
                            - then checking if these functions creating randomness are available in that system, and checking how trivial it is to convert it into a random structure, is useful to identify the significance/truthhood of 'different effects at different amounts', where randomness can be a likelier structure than any other specific variable relationship
                            - similarly, checking if 'different effects at different amounts' is a variant of 'no causal relationship' is useful (checking for a similarity between these alternates)
                            - similarly, checking for a similarity can also take the form of checking if 'random/opposite structures can also reliably create the same variable interaction' is also useful to identify if x is really significant as a cause of y
                            - similarly, checking if 'only specific values' of x cause y (if only a small number of possible x values cause y, or if trivial changes to x causes it to stop causing y, indicating fragility of this causal connection, it might not be a significant causative variable of y compared to other variables, which might cause y more) and if there is an identifiable 'reason why those values might cause y' (a 'causal' similarity between x and y) is useful as an additional filter
                        
    - identifying useful structures to identify like 'variables in common among important structures like problems/solutions' and high variation variables like 'selectivity' (a 'filter' structure encoded in other structures by default, as structures can filter out various structures like interactions and functionality) which are useful in identifying new insights (like 'logical input/output sequences between previously unconnected variables (commonness/problems and commonness/selectivity and structure/filters) used in those structure sets')
        - for example, filters like 'substances that are not active in human bio systems (such as pathogens that dont kill humans)' might be active against error cells (like cancer cells)
        - similarly, 'selectively active/interactive compounds (which dont interact with much or arent activated by many inputs)' are a good solution space to search by default, bc a 'highly selective compound' is likelier to be useful in killing 'one specific cell type' (such as 'cancer cells') than an 'unselective compound' (which is likely to have many interactions which could easily cascade or cancel each other and which is likelier to kill many cell types, including healthy/helpful cells)
            - deriving this is a matter of identifying the 'priority/intent' of 'only killing harmful cells' as important to fulfill, which is a common problem in the 'medicine' problem space, and identifying a 'matching similarity' in the 'selectivity of compound activity' with the 'selectivity required to only kill harmful cells' (identifying this similarity in 'selectivity available' and 'selectivity required' by that problem-solving intent is the important function)
            - this means 'highly differentiating filters' are useful, as compounds with high selectivity and low activity/interactivity (which might be useful against one type of cell) are likely to be rare and different from many other compounds, as its often more useful to have higher values of these metrics so compounds that only bind with one other compounds are likely to be rare, and also theyre likely to be rare for another reason which is that the 'set of all possible chemical sub-structures of a molecule' is very limited, as there are only so many structures, which means they will be repeated often, and therefore there are likely to be multiple similar structures to a particular structure and also given that every structure has multiple adjacent variants, they are also likely to have interactive (opposing) structures with multiple adjacent variants, which means theyre likely to be interactive with multiple structures than one or a limited amount
            - antibodies are often highly selective/specific by default which is why theyre useful, but there are other selective structures which could be possible solutions for functions like selectively killing only error cells
            - relatedly, given the common occurrence of anti-cancer compounds that exist within a large type (such as antifungals or antibodies), every structure type with a large number of structures in that type set is a possible solution set to identify 'the anti-cancer compounds in that type', given that a 'large number of compounds in a type' implies 'high variation in the type' and 'differentiation of functinoality resulting from this high variation', 'anti-cancer' activity being a sufficiently common function that it can be expected to be found somewhere in a function set above a certain threshold of complexity/variation/size
            - given that the opposing variable of a useful structure is often useful as well, the 'abstract' version of this 'specific' useful structure is, for example, identifying the types of compounds with high selectivity, which allows testing other structures in that type set
        - this speaks to the usefulness of useful structures in identifying other useful structures (which is likely bc theyre likely to be similar in some way through having a common important type of 'useful', variables in common like 'high variation' and 'interactivity' and so on)
            - identifying a useful 'selective compound' (like a virus that only kills cancer cells) makes it trivial to identify 'identifying high selectivity compounds' as an important intent, at which point its trivial to identify core interface structures of selectivity like 'variables of selectivity' and 'extreme cases of selectivity like extremely low/high selectivity compounds'
                - identifying the 'selective compound' could be done by identifying 'rare functions' (like 'only killing error cells') which are likelier than common functions to be useful given that common functions, which are usually available/used/active, are not sufficient to act as anti-cancer structures
                    - this fulfills the intent 'what is different from known useless compounds like common compounds'
                - similarly, identifying that 'unselective compounds' are a core and common problem of existing medicines is useful to trivially identify 'highly selective compounds' as useful
                    - this fulfills the intent 'what is different from common/core/sub-problems'
                - similarly, identifying other mechanisms than 'killing error cells' as equivalent alternates is useful, such as selective/specific functions that bind to error compounds, increase metabolism of error compounds, increase proximity of error compounds to the skin, making it likelier to be excreted in sweat, increase isolation of error compounds, increase usage of error compounds in existing processes as inputs to those processes, etc
                    - this fulfills the intent 'what is different (in structure) but similar (in functionality) from known useful functions (like "killing error cells")' (finding 'equivalent alternates'), or similarly, 'what are the variables of useful functions like "killing error cells" and how to use those variables to generate the other variants (such as "try all combinations of variable values, then filter by functionality")'
                - identifying the 'commonness' of problem/solution structures is therefore useful to identify and apply as a way to guide the interface query that finds useful structures like similarities/differences (find similar solutions to common problems, or find differences from common useless/error structures), which adjacently identifies other insights, such as that 'problem structures are likelier to be common'
                - the core insight at work here is that when a variable connection is identified (unless its a very simple linear direct connection), its likely to have other relevant variables of cause such as 'interim nodes' in the 'causal sequence/network' (useful to identify as 'more direct causes given the increased proximity of causation') and 'more linear connections' in the causal sequence/network (such as how connecting A to B and B to C can also easily involve a more linear connection between A and C in some systems/cases), which means when a problem is related to a variable like commonness and problems are related to solutions by structures like differences, its possible to identify other connections such as "applying differences to a problem to create a solution", which is simple logic applying interface structures
                    - relatedly, identifying the other 'causal structures (like direct connections) associated with specific cases (like where more linear/direct connections might be or definitely are possible)' is a useful problem-solving function to fulfill
                - relatedly, identifying the 'cases' possible in a 'system' is useful to identify 'possible input cases' for a specific function in that system, such as identifying cases that allow a linear connection between A and C as referenced above, by identifying the system in which those cases can occur and checking if that system structure applies
        - relatedly, 'unique/maximally different' structures are likely to be useful in identifying 'new directions of change', as structures that are unique compared to other structures and different from other unique structures are likely to align with primary directions of change
        - relatedly, the dichotomy of 'adjacent vs. emergent functionality' is useful to identify, as a way of connecting functions that trivially fulfill an intent vs functions which require iteration or other non-trivial structures to fulfill that intent, where connecting these equivalent functionality structures creates a similarity index that identifies 'possible variable interactions between adjacence/emergence', which can be used to 'generate alternate functions that vary by adjacence/emergence' once those connections between adjacence/emergence are known (connections which will often involve known interface structures)
        - interestingly, important interface structures like 'filters' are also important variables of structures, such as how a structure can filter out various variable interactions or functions by default, so a filter is a built-in/default variable in structures (identifying the 'filters' applied by a particular structure is to identify a similarly valuable structure as the 'intents' fulfilled or 'functions' used by a structure), where the 'filter' format of reality was already known but identifying the specific usefulness of 'filters fulfilled/used by a structure' is useful to identify 'selectivity of a structure' as useful/relevant in 'medical' problem spaces
        - relatedly, identifying 'differences' in problem-solving structures like problem-solving intents is useful to identify and apply, bc different problem-solving intents are likelier to produce different solutions (bc of the input difference), and producing different solutions is useful to identify 'complementary' problem-solving intents (if you solve this intent, youll get this info, and if you solve a different intent, youll get additional/different rather than repeated/similar info of the original info), bc identifying 'differently useful' info is useful, so identifying 'differently useful problem/solution structures' is useful by default but also bc of the applicability to relevant structures like problem/solution structures
            - relatedly, identifying 'multiple sets of useful variables' (like structures that fulfill both 'relevance/usefulness/similarity to useful structures' and 'differences in usefulness (differently useful structures)') identifies a solution space of structures to check/filter, that could be useful for fulfilling multiple useful variables, which is a useful intent to fulfill ('identify all the structures that fulfill each combination of useful intents')
            - relatedly, identifying all of the 'interactions between similarities/differences' is useful to solve for 'volatility', such as how identifying all the similarities/differences that produce very similar (trivial) and very different (extreme) differences in functionality are useful to identify
        - relatedly, 'function usages' are a reflective structure of 'functions available', since a function often only exists if its used, so checking 'functions that are used' is a useful structure to find 'functions that exist', and similarly, 'extreme function usages' can identify emergent functionality when existing functions are used in a scaled way
        - relatedly, finding 'randomness filters', such as 'filters to identify evenly distributed sets, similar sets, and sets outside of these known patterns/attributes as in sets with no discernable attribute' are useful in decomposing randomness, and similarly, 'common structures' and 'continuous structures' are more probable by definition, so adjacent structures are likelier to occur than extremely different structures, as stable structures are likelier to occur than volatile structures or other extremely different structures, as useful structures often have variables in common, one of which is stability, so a completely unorganized randomness-generating function would have to contradict this organizational certainty structure of reality, as a pure randomness-generating function would generate these structures reflecting reality at some point (adjacent structures), so preventing a function from generating them would not be random, even though it would be less organized (which is so unorganized as to have an over-prioritization error of 'too much disorganization' to be random, as 'disorganization' would be required in that less random function and therefore it would be predictable by whatever structures were disorganized, generating strings that are highly different across strings and characters every time rather than highly random), as pure randomness would reflect reality, and non-pure randomness would be incorrect in some way by some bias (favoring one structure artificially), so non-pure randomness functions are a way to derive what isnt real just like pure randomness functions are a way to derive what is real (if there are favored options in reality like stability, they will be selected more by a purely random function, which reflects its input data fairly/randomly, so pure randomness functions are a way to detect probability of a variable in reality, which may not be equal to all other possibility probabilities)
        - relatedly, a way to improve the success/reward of a structure like a 'requirement to guess a randomly selected integer of length n only once (one-shot learning)' can be optimized with structures like 'making other guesses less accurate so that rewards dont need to be shared', and scaling this structure to make 'many guesses inaccurate', which is likely to not only increase coverage of the incorrect/error set (incentivizing large groups to select a set of less likely answers, given some less-than-random algorithm such as one that doesnt pick structures that have repeated digits, or less likely answers such as answers complying with rarer patterns, making selection of one of these less likely to be randomly selected, and similarly decreasing the number of multiple tickets that need to be bought to cover the remaining solution space), but also to distribute any possible rewards of these inaccurate guesses so much that they approach zero (creating large groups that guess the same thing), which involves making a strategy like 'buy multiple lottery tickets' likelier to be successful, these optimizations that improve existing strategies being useful to identify
            - relatedly, given that 'resources reflect luck rather than value', 're-assigning resources regularly, so that they reflect value such as potential, is likelier to be optimal, to distribute and increase/create luck through the higher probability of success created by that strategy' and relatedly, a structure like a 'debt to repay' if an agent with resources hasnt created a similar degree of value than that reflected by their resources (misaligned value and resources) is a useful structure to identify, to avoid errors of misaligned resources/value, which should be similar rather than differing, this alignment being important for various other intents
            - similarly, a 'debt forgiveness threshold of wealth' where extremely wealthy lenders wouldnt be paid back by debtors, if their wealth is sufficient to sustain their income with interest and if their wealth was not reflective of their contribution and if their wealth is being invested suboptimally (such as in fossil fuels or soda)
        - relatedly, identifying a 'problem type' is a matter of identifying the determining structures of the solution (does 'iteration' or 'equivalent alternates' or 'alternate definitions' or 'limit' likely determine the solution, meaning if you iterated enough or determined the alternate definitions or identified the limits, would that solve the problem? then its an 'iteration/equivalent alternate/alternate definition/limit' problem, after which the problem is then identifying/applying those structures in the correct implementation structure, such as the correct sequence - which is similar to 'identifying high-level intents that would improve a problem', after which the problem is then 'implementing the intent with functions that are similar to that intent')
            - an 'iteration' solution is identifiable as determining in cases where its clear that 'some available/identifiable calculation is likely required' and 'there is a clear limit on the iteration of that calculation' and 'info increases with each iteration so that progress toward the solution with each iteration is required/defined/otherwise guaranteed', then its an 'iteration-solvable' problem

    - identify useful structures like 'opposites' of workflows which can often be similarly useful as workflows (just like how identifying specific vs. abstract structures can be similarly useful, despite their difference)
        - for example, 'filter solutions' has an opposite of 'avoiding filtering solutions', which could be implemented by taking the form of 'identifying how all solutions are similar', which uses an adjacent similarity (the similarity of 'connections between solutions (similarities)' with 'filters of solutions (differences)') to identify info retained which is usefully different (in the case where you have functions to derive connections between variables such as solution variables to identify connections between solutions)
        - similarly, 'change a base solution' has an opposite of 'change a base error' (applying opposites to negative structures like 'errors' until theyre not an error), just like how 'identifying cause to identify generative variables' has an opposite of 'identifying outputs/scaled impacts' which is related by the 'similarity between input/outpuut of a function', where some info is retained in either input or output, so applying this similarity to identify other workflows is possible
            - given that an existing solution and an existing error are both unlikely to be the solution for a new problem, finding the useful 'interim balance point' between these is a useful problem-solving intent to fulfill
        - other structures than opposites are useful in this workflow, such as 'vertexes' (such as 'maximal differences' or 'cross-interface structures'), which are useful in identifying similarities between high-variation variables like interface structures and are always connectible using interface structures, so they can be used to identify other workflows (a workflow using one structure in a vertex can be converted to using the other structure in a vertex, given the similarity encoded in the vertex)
        - similarly, the 'level/type/structure of difference/similarity required for a workflow to be useful' (like generate differences and filter differences until similarly maximally different solutions or similar variables of solutions are found, or change a base solution until similar to solution metrics or apply extreme differences until other differences (relevant to solution) are obvious) is useful to identify, in order to identify other variables to create other variants of workflows

    - identify useful structures like 'required differences' (such as tradeoffs) and related structures like 'dichotomies' (which often dont have viable options to mix both extremes) which are useful in optimizing queries related to 'solution filtering' (identifying tradeoffs is useful to guide design of solution-filtering queries)
        - for example, when paying attention, there are often fewer errors bc of the planning functionality added by paying attention, so success is likelier and more trivial when paying attention and planning some number of steps ahead that accounts for required changes/sequences/structures, however when not paying attention/planning, randomness is a default variable and can produce 'useful errors' in that, up to some trivial degree, a difference from an optimal path ('error') requires adding additional stored functionality, when the error is still reversible and fixable, so functions like 'predicting fixable thresholds of errors' and 'creating/enabling errors up to a fixable threshold' can be useful for general functionality, which is a way to use randomness to optimize a path (a path that fulfills multiple functions rather than the original intended function)
        - the cost of paying attention is 'lack of error info' and the cost of trivial fixable errors is itself 'trivial lack of attention (as its required to fix the errors)'
        - query-planning around 'fixing these trivial errors to add more functionality' can be optimized, such as where 'original intent paths are obvious/trivial to fulfill (like a "section of road with no cars ahead" so that temporarily handling some other error is possible while trivially successfully fulfilling the original intent)'
        - an algorithm should be allowed to be 'distracted' by solving some other problem temporarily, such as when 'testing an alternative function that diverges from a known average metric to resolve whether some new variable is useful', which is useful for 'finding new variables or similarity indexes (to use in future navigations of similarity indexes)'
        - this tradeoff between 'planning/organization' and 'allowing a degree of randomness, below some fixable threshold (meaning where the function isnt so general/specific as to be useless as its had its info removed/conflated with randomness or proxies of randomness like complexity), that will be useful in adding functionality (such as by identifying new variables or resolving new error types)' is the balance between 'assuming an optimization strategy is generally useful and sufficiently complete to solve all problems or a whole specific problem' and 'assuming this problem space could be new in some way', which is useful to identify as a 'tradeoff' structure, where 'planning for randomness' is a useful balance between alternatives
            - relatedly, identifying the 'paths to randomness' is useful to identify possible inputs of randomness (like 'how many times would this function have to be applied to generate a high probability of containing a random sequence', and similarly identifying structures of randomness, such as overlapping shapes with a common center and a high ratio of overlapping areas, which would seem like a random distribution), similar to how 'confusing problems' often involve the same structures (such as ambiguities, self-references, iterated abstractions that are more similar to the solution than to existing abstractions which need to be derived, and other structures of complexity and false similarity), so these structures should be looked for by default given a high 'confusion' metric score of a problem
            - similarly, identifying the 'paths to convert useful structures (like extreme differences) into other useful structures (like extreme similarities/equivalences)' is useful to completely identify, such as how defining a 'intersection' as having 'one point of similarity' is useful to identify changes that could convert it into a complete equivalence (like 'rotating one line to be parallel, and basing their midpoints at the same point', once its derived from the definition that 'one point of similarity' indicates a 'perpendicular' interaction type)
                - for example, it would be useful to identify all the 'structure sets' which could adjacently derive how a particular system is organized, which are equivalent alternates and therefore are useful to connect by identifying their variables
                - similarly, if there is a 'ratio of similarity/difference to other structures' that any real structure has, that is another useful structure to identify (some structures are so similar that they only vary by position, but that is an important and high variation variable, rather than a trivial difference, which is useful to identify, as 'changing position of nodes in a network' is a useful function for problem-solving workflows involving 're-organizing existing components of systems to optimize the system')
            - similarly, its useful to identify structures that reflect reality better, such as how 'continuous' functions are likelier to describe real variable interactions than 'discrete', as 'discrete structures' still have components that can be incomplete or combined to create non-discrete values, so function-finding filters should be slightly biased in favor of finding continuous functions or function networks that give the same effect as a continuous function, except where there is too little info to infer a continuous function, or where discrete points are maximally different and could be increasing in difference rather than similarizing
                - relatedly, its useful to apply symmetries to errors, such as where an outlier higher than other points exists, rather than identifying it as an error, applying a symmetry to identify a data set where that outlier is an average or another extreme (lowest point in another distribution), rather than grouping it into the original data set, as overlaps across systems could easily produce outliers if mistakenly grouped with the wrong data set
        - the usefulness of this is that the 'planning vs. error-handling' tradeoff is useful to maintain, even require, so that new variables are always being identified, as where existing planning functions are sufficient, no new unhandled variables are being identified, which is suboptimal
            - similarly, the 'checking one solution vs. checking multiple alternative solutions' tradeoff is useful to maintain (sometimes one solution stands out as obviously useful to check so checking the obvious optimal shouldnt be prevented, such as in workflows like "change a base solution" where checking the base solution on its own is a useful function to apply, whereas checking multiple alternatives is useful where resources are available or where required like in a problem space with some non-extreme degree of randomness, these extremes being simplifying, and checking unit/reduced structures of multiple alternative solutions, such as checking alternatives 'at intervals' and checking 'trivial/unit' changes in the direction of other solutions, is similarly a useful resolution of this tradeoff between investing in checking one obvious optimal vs. multiple alternate solutions to account for new variables and incorrect assumptions)
                - given that 'extremes are simplifying' (similar to how similarities/limits are simplifying), navigating the space between extremes and other simplifying structures is useful to identify new variables/problems/complexity sources
            - relatedly, structures such as 'extremes' and 'requirements' and 'powerful variables' are useful as 'info triggers', as they cause changes, which creates info
            - these tradeoff resolutions often involve using one structure to negate the other to some degree without completely negating it, as a balance between choosing either absolutely (where choosing one absolutely would imply theyre absolutely mutually exclusive and cant be integrated in the same structure)
        - relatedly, identifying 'functions that exist within a system' is useful to identify cases where a 'true function' (an insight that is stable, useful, absolute, etc) is identified, at which point, a system-function map would be useful to identify 'other possible systems that apply, given that the other system also allows that function', as a way of identifying alternate systems such as alternate realities (which comply with absolute truths)
            - similarly, scaling various structures is useful to identify structures that exist in systems created by known errors (like creating a system by scaling an attribute until it has a known over-prioritization error), at which point other structures in that system could also be incorrect, as a way of generating/identifying errors
        - identifying useful structures in the 'interim space' (between extremes of simplicity and randomness, where both complexity and maximum relevant variation occur) is useful, such as applying reality-covering variables like positive/negative to decompose the interim space, like how 'negative complexity (like an error of over-complexity)' describes part of that interim space, which is useful to map/describe bc it also contains the options that enable the highest variation (which are likelier to be a set of multiple alternatives rather than one solution), given that 'balance between extremes of spectrums (like complexity/abstraction spectrums)' is a structure reflecting reality
            - relatedly, identifying whether variables are spectrums (like simplicity/complexity), vertexes (useful differences to connect), networks (like the network of abstract concepts) is useful, as checking for these structures in combinations of concepts is trivial, and useful for workflows that require a high ratio of realistic structures (where other workflows might require opposing structures of reality, such as to check for new errors)

    - identifying useful structures like how vertexes such as the way a statement could be true/false and cycles have an overlap bc they can both contain an opposing structure such as how a cycle can seem like a constant point if you measure it at its completion interval and a vertex structure contains opposites such as the opposite of a statement/variable like abstraction
        - relatedly, structure and content are a vertex structure where the content can oppose the structure, such as how a 'structural format like a style/pattern' can be used that is contradicted by the content, so structure and content are important to include together rather than separating them
            - relatedly, finding the 'structure of content' is a useful structure to identify, such as how 'high differentiation within a sentence (such as containing a contradiction/vertex)' is a common structure, similar to how 'sentences are different from standard sentences', although its often still useful to maintain lists/maps of specific content such as connections between abstract concepts, but identifying all the interface structure connections in existing sentences is a way to derive rules such as 'truth rules' (such as how 'true statements' will not be too different from physics rules, will be slightly different from previously known truths, will contain multiple ways it can be true, more than it contains ways it can be false, etc)
        - these interactions between abstract structures are useful to identify and especially useful to find variation they can contain, such as identifying the most different structures that can have an overlap and identifying causal networks composed of other abstract structures that create those differences
        - relatedly, identifying all the ways that one vertex point can be converted into the other is important as they are interactive and connected by default, so identifying these connections is useful, as these connections are the 'navigation/interaction rules' of the interfaces formed by these vertexes
        - relatedly, identifying 'connections between vertexes' like the connection between 'abstract/specific and uncertain/certain and generate/limit and global/local' is useful as another vertex structure (applying those vertexes as vectors of another vertex, to identify the common interface where those two vertexes are standardizable and convertible to each other)
        - relatedly, identifying the 'inaccuracy midpoint/threshold' between similar but different average/regression lines is useful to identify, which is the point where, after changing each average function, the returns to further changes would seem to diminish, indicating the most accurate function was already found, until further changes were applied (crossing the inaccuracy midpoint/threshold), after which the further changes would increase accuracy until the other average function was reached, which is a useful concept to apply in gradient descent and other problems involving predicting determining points of a function from a subset, as identifying functions to generate 'similar but different' average functions (by some similarity metric) is useful to identify areas/patterns/structures of equivalence/similarity (which are useful for identifying areas/patterns/structures of difference) and identifying connections between these 'similarly good average functions' and 'interim functions that maximize some inaccuracy, an inaccuracy made possible by the degree/type/structure of similarity between the similarly good average functions' are useful as an alternative to identifying 'outer limits of in/accuracy' such as those found by adding more specificity/generality
            - this means 'identifying possible structures like symmetries of in/accuracy (like symmetries in error ranges, where errors reach a peak and start decreasing, which is the opposite of a "solution range symmetry", where errors reach a minimum and start rising)' and applying those to determine other useful structures like 'maximums of accuracy'
            - relatedly, identifying the functions that often preserve accuracy or often violate accuracy of the original function is useful (functions which preserve info about useful differences related to workflows like "change a base solution", like "inverses/rotations" as "maximal differences through sign differences while preserving function shape", logs as "bases", roots as "similarity through intersectivity to a standard constant like zero", waves as "similarity to an iterated pattern", tangents as "unit change rates", the adjacent even/odd-powered variant of a function as "adjacent change types", etc)
            - similarly, identifying changes which identify alternate formats of a function are useful to identify, such as how 'extremifying' a function's differentiating variables makes it obvious that a 'vector format' is an alternate useful format of a function
                - similarly, 'extremifying' a function in this way (such as by increasing the magnitude of peaks of a function so much that the peaked areas become so different as to be irrelevant to a point that is not part of the peak, which makes the average/inflection points that are not part of the peaks obvious, and the changes required to make a peak irrelevant such as by decreasing areas of peaks by scaling them down are useful to identify in case such a change can happen in a variable system, so as to make the peak easy to miss)

    - identifying useful structures like 'variables that involve ambiguities which can benefit from differentiation' by identifying the 'variables of their connections' is useful (structures generally having a similarity in that theyre adjacently changeable into a useful/useless structure, so differentiating the paths to either attribute value is more useful than identifying one/over-simplified attribute value, characterizing a structure as either positive/negative)
        - for example, the set of 'connections between useful/useless structures' is useful for other intents, such as 'connecting/mapping these connections with other relevant/useful connection structures, such as the set of ways that an error can be changed into a solution'
            - a specific example in medicine is the 'set of ways a useful structure like a medicine can be converted into a useless structure such as a condition-causing structure', where knowing 'all the ways a structure can become positive/negative or useful/useless' (such as being 'useful at extremely high scale and useless in trivial amounts') is useful to identify, since most structures can have either effect or neutral effects, as most structures are complex enough in their interactivity/potential to be adjacently usable for maximally different intents (such as positive/negative intents), as many structures in medicine are useful/useless depending on a simple variable like the 'amount', which is adjacently changeable and acts as an adjacent changer of usefulness/uselessness
        - similarly, resolving ambiguities between related but unequivalent conceptual variable sets is useful (like relating useful/uselessness to positive/negative structures, such as how 'useful for negative structures' is a 'negative' attribute)
        - this applies an insight of 'every error can be a solution if its changed enough', with the related insight that 'some changes are more trivial/probable/realistic/powerful/common than others' and the related usefulness of 'identifying the changes that are most often useful in changing an error into a solution' and the insight that 'every variable connection could appear to be different functions at various points/states/changes' and therefore the obvious resulting usefulness of identifying the useful changes ('zooming in an extreme degree to a data set' is often a change that produces 'inaccuracy', where other changes such as 'regular input interval sampling above a ratio of representativeness' likely would not)
            - relatedly, identifying insights (true new important connections such as 'there are changes that are more frequently useful than others in preserving a similarity in input/output info') is a useful input for trivially identifying useful structures made more identifiable/important/useful by identifying that insight ('it would be useful to identify "adjacent structures of these frequently useful changes", such as "variables of changes that preserve info, as opposed to destroying info"')
            - similarly, identifying differences between 'standardizing functions' (like functions that standardize coefficients/powers or resulting slope to values adjacent to one so as to make relevant differences more clear, as an input to useful structures like 'subtle differences between similar structures') vs. 'equalizing functions' (such as 'functions that make slope appear the same value to be basically equivalent to a constant value like one, regardless of input function' as an input to randomness) is useful since these are core useful intents and variables of these differences in functions are identifiable
            - relatedly, mapping concepts like 'simplicity' to specific math functions is useful as a way of applying a useful standardizing format to solve problems conceptually, while skipping other interfaces, or apply other interfaces as later filters
                - applying a 'simplifying set of functions' and applying a 'complex set of functions' and checking if either set made progress to a solution that is scalable (the progress is not false progress but rather indicative of a true connection) is a useful way to check what the problem is conceptually (if its a problem related to over-simplicity or over-complexity), at which point other variables like abstract/specific can be applied with the mapped math function sets in a similar way, and similarly, this initial filter can be used to select more relevant workflows to handle the problem attribute identified (the simplicity of the problem identified in this way)
        - relatedly, identifying structures that differentiate structures such as variables are a default structure relevant to the problem of resolving ambiguities, as 'differences' are obviously useful for 'creating differences (differentiation)' which is useful for 'resolving ambiguities' (any structure like a 'variable' that can create a difference could be useful for resolving a specific ambiguity)
            - relatedly, 'ambiguities' are problems of 'lack of specificity/clarity/difference' so resolving them can be a matter of opposing those attributes
        - relatedly, identifying how 'similarities can become differences' indicates useful input structures to apply as 'possible ways a solution can become an error' and a 'positive structure can become a negative structure' and vice versa
        - relatedly, identifying 'similar but different' structures as possible relevance structures given the usefulness of workflows that can use them like 'change a base solution' are useful to identify, such as how derivatives are 'similar but different' in that they retain some relevant info while being different in that they are in a different format through the differences in the structure of the derivation function (and once in that format can apply to other functions through a new similarity in the coefficient), and identifying other 'similar but different' structures is likely to identify other useful structures
            - relatedly, theyre 'similar but different' in other ways, such as that they have an eventual similarity in power that creates a coefficient of 1 (x ^ 0), where the coefficient other than 1 at that point still retains info about the 'original function' and still differentiates the function from other functions which would have different results at the point it gets to x^0, which is a similarity between all the functions that could produce that if a derivative is applied (varying on the number of usages of that function)
            - relatedly, a specific 'simplifiying change' such as a 'derivative' can be applied as a default strategy to address the conceptual change of the 'opposite of complexity' ('simplicity') being probably useful, in problems where 'over-complexity' is the problem, which is frequently the case (and the opposite for the opposite case)
        - relatedly, identifying how a 'structure that can seem falsely like a useful structure but is not equal to a useful structure' is a useful specific ambiguity to resolve, such as how 'selecting a path' can falsely seem like inventing but a selection may be required (such as a driving force requiring motion, like a predator), simple/adjacent (such as where there are only two choices), and irrelevant/random (such as where the selection is not useful or otherwise directed by some useful force like intelligence as in an ability to identify a useful path but rather is meaningless)
            - relatedly, definitions of concepts like 'randomness' can be identified by structures like where a 'sequence of selections doesnt resolve some ambiguity but rather maintains an equivalence in some random distribution'
        - identifying useful differences in graphs is useful, such as how a 'graph that is different from a network in that it depicts the rate of info connections/changes (in addition to direction and connectivity of info connections/changes)' such as a 'set of barriers' (similar to a maze used to race slime molds) is useful to identify in that it can depict more info bc of the 'differences in number of info components/instances that can occupy a connection', which can be used to identify error structures like 'info bottlenecks', and adding 'state sequences' to this graph adds more useful info in that it depicts differences in sequences of states, which identify other info like 'causes/thresholds/cascades of error states'
            - relatedly, identifying alternate definitions of randomness is useful, such as how it can result in various states such as where 'default/adjacent changes' seem random in the sense that they could take any form, depending on whatever was default/adjacent at the time, and similarly, the 'randomness' of 'lack of meaning/organization (as in "what doesnt matter enough to be organized/examined/identified, or what is unknown so it cant be used and therefore doesnt matter in that way")' is another definition, or the opposite of 'meaning' (which has definitions like 'relevant/useful structures like differences'), although there is a way that randomness 'means' something which is when known variables interact with it, in which case it can mean 'removing the info/certainties/constant values/probabilities of those variables', as its easier to know 'randomness' by what it is not

    - identifying useful structures like 'opposing vector sets' that offer a useful structural target for algorithms to aim for when identifying robust solutions that account for identifiable errors
        - for example, building AI that has requirements such as a 'world-model sub-network' (to allow a network to check that its selected solutions wont favor criminals in their criminal intents or that it has identified a solution to possible misuses) and a 'concept sub-network' (to prevent concepts from being defined differently to favor a result, like defining good as evil, to make sure the network can derive/identify when definitions have been distorted for an evil intent by allowing it to access concepts that enable it to derive real connections/balances between concepts, thereby deriving definitions like good/evil by defining concepts like power/balance/equality), as well as a 'sub-network specifically to identify info indicating misuse and identify solutions to possible mis-uses of the solution-finding sub-network for evil intents' (solutions to misuse such as 'only allow n number of people to use this more than the people around them' or info indicating misuse such as 'when these transaction types start occuring between these groups after using the network, the network is likely being misused')
        - similarly, including info about usage of a set of variables and semantic info about variables such as 'what numbers represent' makes it more possible to prevent evil intents, and similarly, including a 'few maximally different alternate solution functions having different reasons, connected to an adjacent sub-network of variables that would cause that reason to be relevant', so that when new info or additional info is found, the differences in these 'reason-based sub-networks' makes it obvious which function is more correct
        - the minimum info to detect evil intents is usually not clearly present in the numerical variables created by current tools, although some algorithms such as the algorithm that 'identifies the simplest or most adjacent solution' are clearly evil in the missing info they leave out or fail to identify, having an obvious error of over-simplification likely to perpetuate biases, so testing out a selected solution in a world-model is a useful countermeasure that could be required of algorithms, where the solutions resulting from this hybrid network would benefit society more and therefore produce solutions that are preferable to criminals as well, as an algorithm capable of complexity will also be able to identify solutions that will improve criminals' quality of life in harmless ways, like identifying 'less harmful scams and the right scam targets' as well as identifying 'other paths out of crime', which will make their lives less stressful and have a similar effect on their victims
        - this has a structure of 'limiting change/chaos created by a solution function' by creating change vectors in a direction away from evil intents like over-simplification, but also creating vectors in the opposite direction, to limit the changes created by the solution function when they become error structures (like when scaled to an extreme/over-prioritized degree), similar to how 'filter' limits the changes created by the 'generate' function when applied with it, a structure of a 'generally useful change vector' and a 'specific opposing error-correction vector set' that should be fulfilled by most useful algorithms as it accounts for 'identifiable errors and solutions to them (opposing vectors to offset them)', as 'change in only one direction' is rarely the right solution function and should more often be accompanied by 'extreme change offsetting/reversing functions'
            - relatedly, the difference from a similar structure (such as an ambiguously similar structure that is still useful to differentiate, as the alternates are unlikely to merge despite having some ratio of simple connections) and the difference from an extreme/scaled structure (such as 'obvious outputs of a process, when scaled') are useful to include as a vertex in identifying the useful relative variable values of a structure being compared (local vs. global, and ambiguously similar vs. extremely different)
            - relatedly, identifying a 'workflow/query' and 'examples of the errors it could have (such as how change a base solution wont immediately find non-adjacent values, adjacent given change functions applied)' is another example of useful 'opposing vectors' involving problem/solution structures
        - this structure of 'opposing vector sets' is useful for many other different intents, like how the 'ultimate standard of info' (as in 'how do you really know something') requires that no incentives/biases exist to fake the info and if there are incentives/biases, they have been removed/isolated/otherwise neutralized from impacting the test of info or that the only incentives/biases allowed to impact the info are only biases in favor of info/other similar priorities like fairness
            - this is similar to the 'standard of meaning' which is where some function/structure occurs despite all attempts to prevent it (as in 'its more meaningful to be kind despite no rewards for it'), or despite having no possible benefit, which makes it more meaningful if it still occurs in those conditions, as it indicates power and therefore meaning since power influences other variables
            - relatedly, other structures are useful to identify, such as how 'understatements' are errors bc they 'leave out info (create missing info)' similar to 'riddles', which involving resolving the 'missing info' error in often very different and therefore useful ways
            - similarly, identifying other useful connections like the connection between 'justice' and 'reality' through the concept of 'balance' is useful (since 'just agents' are more real, as they interact with and thrive in reality, rather than ignoring truths, they apply a more complex structure than just an 'info barrier', where both opposing vectors can exist)
        - relatedly, structures like simple structures can trivially produce the opposite of a true statement, as they can be so different from reality in their over-simplifications that they can be trivially changed to produce other false statements
        - relatedly, its useful to identify reasons why useful structures can be derived from each other, bc there are workflows that can be applied to adjacently identify other useful structures once a useful structure is known (like how 'change a base solution' can identify trivial variants of a structure more adjacently than other workflows might, such as 'specialized variants'), and similarly, intents like 'identify variables of a structure (to identify variants of it)' are possible to fulfill once a useful structure is known, so that other useful structures can be derived from it based on 'variables of usefulness'
        - relatedly, its useful to identify 'areas of ambiguity' in a function space, so that where there are 'areas of missing information or ambiguity' such as 'what other filters exist that can be applied across problems', info can be added to solve the 'lack of info' problem, such as by applying a specific problem to identify new filters, at which point the filters found can be abstracted to make them applicable to problems in general again
            - this is another position where an 'opposing vector set' (specify, then abstract) can be applied, to create a useful change ('add info') that enables a useful target structure ('general info'), as spectrum interface variables like abstract/specific contain maximal differences similar to cross-interface structures so variations within these interface variables can be applied as 'opposing vectors'

    - identifying differences that add value once an insight about errors (suboptimal differences) is identified, to focus an interface query to apply those differences where they are useful (applying differences to the errors, as in 'create a solution')
        - for example, its possible to 'connect any two points in a data set' to give a totally opposite conclusion of the solution regression function compared to the actual regression function, so methods of applying this insight would involve identifying counteractive measures to contradict the use of these 'opposing point lines' (opposing the real solution function), similar to how its possible to make any true statement seem false by changing it in various ways (such as removing the general info or removing most info and only focusing on one point or a similarly low-info subset or zooming in) and vice versa, counteractive measures such as 'including more points in the line', 'including points that follow different patterns in the line (points on a straight line, points on curved lines, etc)', 'including maximally different points in the line (points in different directions, at different distances away from the previous or "previous average" point)'
        - relatedly, combinations of interface structures like 'previous average' (as a program iterates through a data set in a consecutive sequence of inputs) are useful to identify in case they are relevant to some workflow, as it is usually possible to build a workflow using these combined or cross-interface structures given the variation they contain and the general reality of these structures which makes them relevant and also given the interactivity of these structures with other structures in the problem space
        - relatedly, identifying the limit of a workflow such as 'trial and error' is useful, such as how a chip designed to apply 'trial and error' will create problems at scale that cant be solved by 'trial and error' (identifying the activities of a trillion chips all using 'trial and error' on some problem cant be solved by 'trial and error', bc the chip army will beat the chip trying to predict them with 'trial and error', unless the chip trying to predict them is exponentially better than the chip army units, which is a problem that will occur at some point, as inevitably criminals will gather a lot of chips and use them to out-compute the enemy, and if chips hit a computation limit, algorithms have to improve, so 'trial and error' has an expiration date either way, whether bc its necessary to predict and out-compute an army of chips using one chip or bc a computation limit was reached)
        - relatedly, 'iterating/extrapolating n times' is useful to identify if a statement is true (by iterating the process of evalating what the statement implies, n times), as the idea of a 'count of useful iteration applications' represents a 'ratio/degree of differences' that could possibly change the statement into its opposite (whether true or false), as frequently an incorrect conclusion is incorrect bc its incompletely analyzed and its n-th implication isnt identified, which is a problem of 'scaling (to the nth implication)'
        - relatedly, identifying a 'way that an error structure can be useful, if implemented in that specific way' is useful to identify, such as how tech like neural networks should generally only be used for very specific intents like 'fixing supply chain/delivery optimization errors that cause areas with insufficient resources' to 'reduce war' and should not generally be used for intents like 'winning a war' to 'reduce war', but there is an optimal way to apply the error structure of 'applying neural networks to win a war', which is that if used correctly, the neural networks can help win a war very quickly, and thereby effectively 'reduce war', as there is always a way to use an error to solve some problem, but some errors have adjacent ways to be used to solve a problem
        - relatedly, identifying alternates like 'evaporating sea water' vs. 'moving undersea land onto land' or 'moving mountains into the ocean to create more islands' as alternative ways to 'lower the sea level' is useful for identifying 'variable sets' which can be altered without harming other useful structures, which act like alternate inputs to the sea level
            - similarly, 'diffusing heat' is possible by 'altering light with lasers to produce different types of light than sunlight' or 'planting trees or creating artificial covers like clouds to cover more land to reduce heat on land' or 'creating highly light-absorbing structures' without harming other useful structures, all of which implement some form of a common function such as 'distribute' (applied to the problem inputs) to have an impact of 'reducing' the problem, where 'distribute' (applied to problem inputs) is a useful variant of 'isolate/separate/filter' and 'reduce'
                - this applies a function of reversing/opposing a negative structure (using a structure like 'light (from lasers)' against other types of 'light (like sunlight)'), which can be done in many ways like creating reflective surfaces on the earth's surface, or creating 'light absorbing structures (like darker materials)' to cool the surface
                    - this function isnt always applied correctly, such as where 'two wrongs dont make a right' bc the next wrong exacerbates the previous wrong as the wrong is not applied to other wrongs but rather is applied in the same direction, so it doesnt create a useful opposite such as where other useful opposites occur, when a negative cancels out a negative
                - given that obscure materials would prevent light from passing through, and there are layers in between the sun and the surface such as cloud layers, these clouds can be used to prevent light from passing to the surface (which can be done in many ways, such as finding less harmful chemicals than pollutants which are nevertheless obscuring and also light/interactive enough to form clouds by default, and only done in some places like less inhabited places or over the ocean if there are positions where its more trivial to maintain a cloud position)
                - relatedly, a more complete workflow would have an opposing function to 'distribute', such as a function to re-combine the outputs of the 'distribute' function in a more useful way, which has at least some opposing function to offset errors of extremity that are likely in an overly simple workflow
            - similarly, identifying useful structures like 'plants' which take up a resource to reduce 'seawater' as input is useful to identify, which re-directs the problematic structure to a useful structure that can use it or reduce it or 'reduce it by using it'
            - similarly, identifying other intents fulfilled by functions like 'terraforming' such as 'creating a planet with more equally distributed resources (such as a planet with a water source every n miles)' is useful to identify future benefits of developing a function
            - human brains dont have the capacity to compute variables like 'ultimate impact on climate/biological life/ecosystem/magnetic field/ozone/weather/chemical ratios of a climate change solution' so 'training a bunch of different AI models to try to get them to agree on some solution' is not just a good alternative that is available for methods with unknown scaled impacts like 'spraying chemicals on clouds' but is required
                - a 'doomsaying/error-avoidance AI' to identify 'worst-case scenario side effects' like 'biological life forms developing poisonous toxins to handle the climate change' or 'unexpected extreme impact on evaporability/other important properties of seawater types under clouds with chemicals sprayed on them' need to be very low-risk for a given solution identified by an 'ensemble of maximally different AI models with high computational resources' for it to be even worth considering, and to predict these worst case scenario side effects, an extremely good simulation of reality (bio systems, ecosystems, climate/weather systems, chemical interactions, light interactions and interactions of all combinations of these) is required (a 'doomsaying AI' with access to high computational resources, that can model reality accurately, and quickly identify the worst things that can happen if a solution is selected/applied at scale, at which point if the doomsaying AI thinks a solution might not trigger the worst or even negative side effects, and all the other doomsaying AI models agree, then it might be ok to try in small amounts)
                    - relatedly, a 'best case scenario AI' is useful to build for identifying benefits that can happen, but these generate so much 'blinding hope' that they are not as useful as worst-case scenario AI's
                - similarly, the opposite of an 'error-avoidance AI' as in a 'solution-progress AI' to evaluate all the applications of solutions and whether theyre making progress (in the real world or in simulations)
                - a 'risk-evaluation AI' would be useful to indicate whether the "doomsaying AI's" worst-case scenarios are probable if a solution is applied 'at scale' or 'simultaneously with other solutions/problems'
                - a 'cost-benefit AI' to evaluate if the risk is worth it and identify costs of a solution, given probabilities of positive outcomes and other non-worst case scenario outcomes
                - an 'ensemble AI' to evaluate all the solutions selected by the solution-filtering/generating/changing AIs and identify solutions that have consensus
                - an 'info-finding AI' to identify missing information to solve the other problems (which AI models need more info to make progress and how to find that info)
                    - similarly, an 'info-barrier solving AI' to identify barriers to information, such as difficulties/complexities/required iterations which hide info and make it non-trivial to identify, or identify methods to workaround info barriers (such as finding 'equivalent alternates' or 'approximations' of the info being searched for) to help the 'info-finding AI'
                - an 'integration/interaction AI' to evaluate possible interactions and all the other methods being tested around the world ('what other experiments are being applied, simultaneously or around the time that this method is being applied') and the impact of variables like 'current pollution levels' on a solution method (can this solution succeed, even if clear skies or ozone holes or pollution or a solar flare is happening in that area at the time its applied), where this integration AI would be useful for most other AI's to use by default, as it would check if their output was coordinating/contradicting with other solutions being applied
                - an 'organization AI' to evaluate the 'best possible amounts/positions/interactions/connections' between alternate solutions being applied and other variables like pollution, which would have the most useful perspective on how resources should be organized, which would help and be helped by the 'state sequence-identifying AI' to identify paths to achieve these optimal resource organizations once identified
                - a 'solution state-identifying AI' to identify optimal solution states (what are the important solution metrics and what states would fulfill those - as in 'everyone having some ratio/amount of resources' and other useful specifications of solution metrics)
                - a 'state sequence-identifying AI' to identify 'possible useful state/step sequences that would lead to optimal solution states', such as 'how to change the supply-chain, pollution, economies, regulations, technologies' so that optimal states are more adjacent/possible
                - an 'implementation AI' to help workaround political/social/technological/scaling/distribution/manufacturing/supply-chain/other barriers to applying the suggestions of the AI
                - a 'balancing AI' to help avoid extremes, which are likelier to be error than optimal states, given the usefulness of balance
                - a 'distributing AI' to simulate testing a solution in various distributions and various systems
                - an 'optimization AI' to evaluate the other AI models and suggest changes to improve them
                - a 'scaling AI' to scale solutions and identify extremes and limits
                - an 'inventing AI' to identify useful possible implementations of intents to help the other AI's such as the implementation AI, but specifically to solve the problem of 'inventing new technologies' which is particularly useful in the energy/climate/medicine problem spaces
                - a 'connecting/combining and separating/deconstructing AI' to connect possible variables and connection structures like 'combinations' that would be useful to connect, such as variables that could cause extreme negative side effects (like materials that when adjacent form explosions), which are useful to identify so they can be separated or deconstructed, which helps the organization AI
                - a 'interface variable AI' to identify the most determining/powerful/causative/common/standardizing/simplifying/interactive/supporting/interface variables which explain the most variation, and identify all those variable interactions, as theyre the most important to identify, as it reduces the 'not understood variation' in a system, and help simplify models of even complex systems
                - a 'simulation AI' to help generate and improve simulations of reality, with help from the 'interface variable AI' to help simplify/standardize/improve its simulation models
                - an 'alternative-finding AI' to help find alternatives to a solution, such as 'temporary solutions that would be useful to apply now while waiting on simulations/implementations of more complex/risky solutions'
                - this is basically an optimized version of 'checks and balances' in govt - identifying how all these functions should be positioned and interact is useful as a way to identify an optimal 'problem-solving function network'
                - worst-case scenarios arent likely to be identified by most experiments/AI models bc these scenarios might only happen at extreme scales, except those experiments/models specifically designed to cause/model significant harm (you cant cause doomsday scenarios in your experiments just to find out if they would happen, youd have to cause units of doomsday scenarios in an experiment and then stop and identify that at scale, this would continue, or just simulate the causation in a computer)
                - as a temporary interim solution while waiting on AI models to agree on a risky method like spraying chemicals, 'reflective foil tarps (that dont bleed their surface chemicals very much or at all) covering a small ratio of the ocean or land to test out its side effects' might be a better alternative that might cause less pollution/side effects while we wait for AI models to simulate things like chemical sprays, the best long-term solution is probably something like a 'tool to evenly distribute ozone, or push ozone up from the ground layer, so there are no holes in that layer'
                - if 'hoping it will work' is heavily involved in a solution application, it is extremely likely to fail or cause negative side effects that hope blinded agents to, hope often blinds people to negative things like negative side effects that would have been useful to identify, hope is a bad investment, 'reliability/consistency of results across many different AI models' is a better alternative to 'blind dumb hope'

    - identify useful similarities in different problems like a 'network' format where other strategies in problems with that similarity can be applied trivially to solve similar intents across problems (like 'find a path that maintains some direction or reduces distance to a destination node' which is a generally applicable intent across problems formattable using a 'network')
        - for example, 'simple rules to solve a maze' exist (such as 'aim for this many left/right turn pairs or take a maximum of n left turns followed by n right turns, followed by other patterns of turns, to avoid turn error structures that result in circles or back/side directions') which can be applied to other problems with some similarity in common such as 'finding optimal paths' which can benefit from 'functions to maintain a direction', similarly solving other problems like 'decrease the distance between all nodes in a network' or 'organize a network so all its nodes are trivially connectible' can be applied to other problems having a network format such as 'finding optimal paths' or 'connect all variables (in a neural network)'
        - relatedly, the reason that many different terms align with 'interfaces' is that many different useful structures encode a 'core similarity (around which trivial differences are applied)' such as 'perspectives' (similarity in priority), 'indexes/maps' (similarity in input/output, similarity in usefulness of the same structures which are useful to store a list of, similarity in approximation of the index's original list), 'formats' (similarity in structure), etc
            - other useful structures involve the opposite structure like a 'set of differences that creates a similarity' such as 'equivalent alternates' or 'complementary info sets that construct the same resulting set'
            - for example, identifying that an index can solve a problem of 'unnecessary or uncoordinated variation, where there should be coordinated variation (like how component updates should reflect the values in a "coordinating versions" index', rather than being applied without coordination, as the components coordinate so the versions need to also coordinate, as the versions reflect differences in component functionality, so the versions are also an index of the component functionality) is useful, as it reduces the problem to 'finding the correct coordinating version index and applying it as a standardizing structure creating a similarity in component versions, allowed changes/updates, and the version index'
                - relatedly, indexes can solve other problems if the index is identified first and is relatively trivial to compute, such as where its useful to identify whether a subset of a function indicates its one of various maximally different function examples/types, using a 'subset/maximal different function' index, and similarly an index of 'interface structures' can be used to solve many problems trivially
        - knowing this is useful bc it means that a useful problem-solving workflow will involve different pairs of 'similarities in differences' and 'differences in similarities' and related useful opposing structures, similar to how 'generate and filter' and 'standardize and differentiate' are similar opposing structures that are useful in workflows (the 'vertex' structure uniting different perspectives that keeps re-occurring in many useful workflows)
            - relatedly, the 'vertex' of architecture and algorithms is useful to identify (such as where 'trial and error' is only useful if there are multiple processors available to compute them simultaneously, creating a useful similarity between 'processes that can occur simultaneously' and the 'number of possibilities tested', which is still suboptimal in that it doesnt identify understanding of how to filter solutions, so a 'maximal filter processor (which identifies important variables and applies those first to predict functions, then others of less importance)' would be more useful as an architectural resource, similar to how some algorithms have specific memory limits/requirements)
            - relatedly, these structures are useful when mapped to similarities/differences, such as how 'generate and filter' creates differences (generating 'multiple specific examples, which are different enough to be filterable') which are made more useful by further differentiating them (applying filters to find different solutions like unique solutions), and how 'standardize and differentiate' involves identifying a core similarity which is modified with trivial differences, and how 'specify and abstract' involves identifying 'specific examples which are then more possible to similarize by identifying abstract types which apply to multiple examples' (similar to generate and filter)

    - applying useful or 'problem-solving' intents (like 'find variables' or 'find maximal differences') on each interface is useful to pre-compute useful structures like 'maximal differences' that can be used in interface queries
        - for example, 'certain uncertainties' and 'specific abstractions' are 'maximal differences' within a primary interface
        - relatedly, identifying cross-interface function structures is useful, like how 'logic' and 'information' create 'pressure/power/control' or 'heat/energy/change'
            - bc of how numbers are defined/identified, specific structures such as specific 'patterns of interactions' are possible, so identifying this cross-interface structure of the connection between 'definitions' and other interface structures like 'interaction potential' and 'interaction patterns' is useful (which enables other intents like 'find the most reduced number set that reflects reality in various metrics like potential, variation, conceptual network/balance/variables/priorities, etc' and 'find generative variables/requirements/implications of this number set as a proxy for the same structures of reality')
        - similarly, applying 'extremes' like 'infinity' to find other related structures like 'limits/thresholds' and applying variables of a 'spectrum' like 'less/more/average' to spectrum variables like 'abstract/specific' is useful for identifying all the interface structures in that variable, to identify other useful structures like insights such as how other variable formats like network formats or interface formats are more useful than spectrum formats of complex variables like 'abstract/specific'
            - for example, its useful to have both specific/specialized structures and abstract structures, so that errors like 'over-focus on details' cant occur and the 'general picture' is always integrated regularly, so the 'conflict between abstract/specific' doesnt need to be resolved by choosing one or using other interfaces instead, but rather maintained as both are necessary in different cases that will continue to occur
        - relatedly (to both this workflow of resolving problems within interfaces and the previous workflow of pre-computing specific problems and the workflow of 'designing networks to offset errors like evil usages/intents'), solving the ambiguities between useful structures to differentiate like 'good/evil' is useful, such as how 'not committing a crime' can seem like either good or evil, such as how good agents will not commit a crime but evil agents will also frequently not commit a crime, such as when theyre being monitored, when they are planning other crimes, when they are trying to seem good or evade detection, when they are recovering/resting/gathering new crime resources after a crime, at which points they will falsely seem good, which are useful structures to apply in algorithms designed to avoid errors like 'adjacently fulfilling evil intents' (like by creating barrier to evil intents or creating incentives for good intents or providing identification info for evil intents)
            - the fact that this is 'related to and useful to multiple workflows but cant be equated with any of the workflows' indicates it can act like a workflow on its own, such as to 'resolve other differences by resolving differences on an important interface (which fulfills multiple metrics as the good/evil interface is by default related to meaning and is therefore automatically also relevant/important, so it acts like a useful proxy of the meaning interface)', which is a way to identify alternate workflows (find intents that are related to multiple workflows but not equivalent)
            - the idea of 'resolving evil intents by aligning intents across all agents/groups' indicates a useful structure of 'alignment', which connects to useful questions like 'what differences are useful to maintain, if its so useful to align structures using structures like intents/standards/similarities' (such as 'differences reflecting or possibly reflecting the variation in reality, so that this variation can be handled, once encountered')
            - relatedly, solving the problem of 'what is the causative problem of an important problem, such as the reason for the positive/negative impact of an important variables (like incentives)' is useful and is a proxy for solving other problems, such as how identifying that the reason incentives are negative is that they create 'disorganized cost allocation, allocating cost where default/adjacent/otherwise irrelevant but not allocating it where its best handled/allocated' which if that problem of 'organizing cost allocation' is solved, the errors resulting from incentives are reduced
            - relatedly, another solution to the problem of good vs. evil is conflating the two with complexity structures like extreme/embedding structures, which produce more complicated structures like 'evilly good' and 'purely evil' and other conflations of the concepts and related concepts which are neutral but associated with ethical values, as increasing complexity often reduces the impact of a variable
  
    - identifying useful structures like 'problems to solve, which when solved, can be applied to trivially create other solutions to other problems' which is possible bc core/complex problems can be used in combinations/changes to solve other problems
        - for example, resolving core/simple/composable ambiguities like 'differences between similar concepts' is useful to resolve all ambiguities, as the solvable unsolved ambiguities will often changes/combinations of the solved ambiguities which are combinable to create other ambiguities
        - similarly, solutions to complex problems like 'resolving differences between similar concepts' (such as 'purity' and 'simplicity', which have some overlap but are ultimately quite different) can often be varied trivially to solve other complex problems
        - the core insight here is that some differences between problems involve trivial changes (like 'similarly complex' problems being able to be solved with trivial changes bc of this similarity), whereas other differences between problems would involve 'iterated combinations' (the change required to make a core problem solution useful for complex problems), so knowing attributes of problems like composability/complexity is useful to identify how problems/solutions could interact with other problems/solutions
        - this is bc solving unit cases in a 'problem format' like 'resolving ambiguities' is useful bc other cases in that format are likely to involve combinations of those unit case solutions
        - similarly, 'identify equivalent alternates', 'identify variables that determine a set', 'identifying generally powerful/determining variables' are related intents that can usually be swapped in for each other in interface queries
            - identifying 'similarly useful intents to fulfill' is useful to identify intents that can be combined/changed to create useful structures like 'different interface queries that are similarly useful'
        - these differences in how structures can be used (combined/changed) are useful to identify bc they determine which workflows are optimal to use with them (use 'trivially changeable structures like equivalent alternate or similar concepts/intents' in a workflow involving 'change a base solution')

    - identifying useful structures that avoid multiple errors simultaneously is useful as an alternative of other useful structures (like structures with multiple useful functions) by applying that structure with a variant (applied to 'error' structures)
        - for example, a simple rule like 'no simple rules' is capable of being applied correctly with a higher probability than other rules, especially simple rules, which will miss the rare correct simple rule but that is trivial to identify, as there are fewer functions/structures to test for when identifying simple structures and that can be included in interface queries by default
        - this avoids an 'over-prioritization' error (over-simplification), both by avoiding over-simplifying and over-complicating (having a solution for the error of missing simple rules)
        - it also avoids a 'self-invalidation' error (where simplicity negates itself) by finding the one self-reference structure of simplicity that applies generally (which is a simple rule - specifically, 'no simple rules'), so that more common self-reference errors of self-invalidation (which are more common than correct/useful/relevant self-reference structures) can be avoided once the correct alternative is identified
        - by applying an insight like 'most statements are somewhat true and somewhat false', it can be derived that finding 'correct and incorrect usages' of a rule is possible, such as cases where a 'simple' rule is justified, such as 'where a system is new or newly stable and its interactions are likelier to be simple'
        - this applies a 'bias against bias' which is another correct self-reference structure, as bias means an error of some sort such as 'over-simplification' or 'assuming too much'
        - relatedly, identifying interface structures of bias is useful to identify structures related to bias errors, such as how a 'self' bias has a 'structural reason/cause' such as 'being/having a structure (being embedded in a state, using functions, getting rewards from a function) making it almost impossible to see that structure clearly (see its negatives/costs) without trying to think from the opposing/different perspectives (like the negative perspective)' and relatedly, the 'self' bias is related to other biases like 'local/adjacent' bias bc of various structural reasons/causes, such as 'sight' (which makes local/adjacent structures seem more important than they are, though they can be important such as where a unit case test is possible to extrapolate insights from and local errors can be avoided)
        - similarly, if a structure is to fulfill an intent like 'differentiate' some set of structures, it has to be different in some way, rather than situations like the similarity of the ambiguity of the overlap in 'having a false appearance of some structure, that both a true and a false statement would seem like' (which is how both true and false statements seem when the statement doesnt contain an 'aligning difference reflecting the difference between truth/falsehood', such as how a 'neutral structure can seem false or true with trivial changes' so these neutral statements are likelier to be polarized (as in 'extremified') for various intents, meaning the truth/falsehood differentiating function and the true/false statement have to contain sufficient differences rather than similarities)

    - identify useful structures like core problems which, once solved to some degree of complexity, can add value by making it possible to compose their solutions to make other more complex problems trivial to solve (meaning solvable with trivial structures like 'adjacent change combinations') and 'differences in cases that a solution should work for to be generally applicable such as by being composable to solve other problems'
        - for example, the 'repeat' function is associated with the 'scaling' problem (and relatedly the 'over-prioritization' problem and relatedly the 'threshold/signal/determinant of success/failure' is associated with the 'recursion' problem, as the 'halting/limiting' problem), the 'sequence' structure is associated with the 'sorting' problem, the 'format' structure is associated with a 'interactivity' problem (where not every structure is already formatted to that format so differently formatted structures may not be interactive/mappable to/usable in some format), the 'base (as a reducer of differences)' structure is associated with a general 'difficulty' problem (not having adjacent structures to solve a problem in some base or interaction level), the 'group' structure is associated with the 'batching/dividing' problem, the 'similar/ambiguous' structure is associated with the 'defining/filtering' problem, the 'storage' structure is associated with the 'usage' problem, the 'intent' structure is associated with the 'meaning' problem (what is the meaning of this intent, given what it reflects that the user doesnt already have and wants) which are core structural functions/structures and core problems, which if solved to some degree of complexity (in a unit case and a complex case with scaled and random and other variable structures applied), can likely offer composition of their solutions to solve other problems
        - similarly, other functions exist which have general applicability to problem-solving, such as 'risk' (as in 'select a function set with incomplete info, that has a sufficient probability of being sufficiently complete or adaptable or otherwise useful') and 'trust' (the ever-popular, 'wait for someone else to fix it')
            - these are functions that can be used in low-info cases but understanding (implemented with connections between high variation variables) is more useful, as 'risking it' works if youre lucky which is rare unless youve created it through various structures which is a problem-solving strategy (creating luck such as by distributing power/opportunities/incentives/info/other valuable resources so that 'any position or any path can become lucky (which is a sequence of low-cost rewards by creating those reward sequences in a distributed structure)', increasing organization/scaling/problem-solving functions, creating value that is useful for many positions, increasing relevance/interactivity of various useful structures like 'aligning incentives and opportunities to solve problems', increasing ratio of understanding relative to the now less fortunate, improving functionality of existing functions, etc), and should only be used as a last-resort method, especially in problems like 'positioning mirrors/lenses around the universe to direct light to implement various functions like gravity/heat/magnetic manipulations', which is already risky enough and is important to get right, so you wouldnt want to risk anything if possible
                - relatedly, identifying 'functions which perform similarly across worst/best case scenarios (and different scenarios like default scenarios)' is possibly by identifying 'functions with similar performance across variable inputs', this function set being a default function set to try to minimize the impact of luck in the optimal attribute of a particular algorithm (some algorithms only work bc luckily a solution is adjacent, so finding algorithms that perform similarly is possible by 'creating luck' through applying reality structures like 'multiple alternatives' and 'networks of variables rather than one optimal variable' and 'multiple perspectives rather than one over-prioritized attribute', as an algorithm to solve problems is supposed to be better than reality, as reality doesnt automatically solve all problems, but it needs to reflect reality to some degree while also changing it enough to improve it, meaning it should contain some reality structures, and also apply some insights such as 'dont always rely on known useful structures but instead apply randomness to identify new variables/errors' which aligns with a algorithm having some non-zero and non-absolute ratio of reality structures)
            - relatedly, risky strategies are an 'improbability of success' where the 'reason' for the improbability is that there is 'incomplete info, such as info about more optimal strategies', where the 'reason for probabilities' is useful bc probabilities are useful, in that certainties about im/possibilities/requirements are less complicated and problems are likelier to occur with uncertain possibilities rather than certain possibilities like 'guaranteed impossibilities or required possibilities')
        - the interactions of unit/complex/other highly different cases of these problems are likely to be usable to solve most other medium-complexity problems
        - 'generating/selecting the differences in cases that a solution should work for' (like best/worst case, simple/complex case, random case, scaled case, etc) is a useful problem-solving intent related to 'defining requirements' and 'defining solution metrics' that makes it more obvious to determine the solution
        - a structure formed by 'common functions usable to solve these problems', organized in sequences that adjacently solve these problems, is likely to be a generally useful 'maximally different' structure to identify, which can interact with other 'maximally different function set networks' to create a network of 'maximally different function set networks' identify gaps in problem-solving functionality
        - similarly, the set of 'surrounding interface structures of a structure like a high variation structure (like a concept)' fulfills various interface structures (the system context surrounding a structure, the adjacent structures created by applying interface structures which are relevant to the structure, the high variation implemented and achieved by applying interface structures, etc) and is also useful to find interface structures of the interactions between these 'structures with interface structures applied', such as 'overlaps' between the 'similarities of a structure with concepts' and the 'similarities of another structure with concepts' being useful to determine different variables of concepts, as another useful graph to identify with interface analysis (which builds 'understanding' by applying these surrounding/extreme/limiting structural interactions with interface structures)
            - relatedly, formatting 'existing useful structures like high variation functions (such as organize)' using interface structures is a way to determine the other useful functions that could be implemented with these structures which are similarly useful or otherwise equivalent in some way
            - this is useful for avoiding errors of 'incomplete thinking' where an assumption isnt followed through (iterated, or iterated in its probable/identifiable input/output sequences) to its ultimate conclusion regarding its interaction with other structures, such as where some iterated structure would hit a limit at some point but its assumed to be infinite, and the limit structure is not identified as relevant (in its path of iteration/interactivity), and the iterated structure is not iterated until that limit is hit, so the contradiction of the infiniteness of the iterated structure is not identified
            - similarly, its useful to identify 'directions of change' so that a structure which limits another in this way can be predicted
        - similarly, identifying useful structures like 'interaction levels' is possible by finding structures that can be created using the same function/variables (like a radius rotation anchored at an origin), so checking various functions/variable sets for potential to create many different structures is a way of identifying these variables and the resulting interaction levels
            - identifying functions/variables to generate other useful interface structures is similarly useful at finding these structures such as overlaps/interaction levels, which are useful in that they make some useful structure like a 'difference (equivalent alternates) within a similarity (the same interaction level)' or a 'difference (non-overlapping section) based on a similarity (overlapping section)' more obvious
        - similar to how interface analysis is based on the assumption that in order for structure to exist, it would have to have some supporting structure to base changes on, other structures can exist which are less like standard supporting structures than a platform/plane/pole/lattice (like limits, light, or a field that supports free molecules not associated with a planet which can form stars/planets as they are attracted to a planet/star center/base) which add variability allowing interfaces to change
            - relatedly, the symmetries present in systems reflect symmetries present in others, such as the symmetry between a shadow/structure creating the shadow/light shined on the object, which can be applied to identify other useful structures (like the limits of the info reflected in the shadow of the structure light is shined on, how the shadow can be changed to appear similar to other shadows thereby removing info, etc) bc differences tend to follow similar patterns (like adjacent differences, embedded differences, etc and other structures defined in interface analysis), and therefore it might be useful to create a similarity index based on this and other symmetries, such as a 'shadow index' of common shadows of functions (like parabolas) that can be resolved with trivial differences to de-shadow the shadow of the function and expand it into the set of possible functions associated with it

    - identifying what set of priorities (like 'niceness') can be maintained with some decisions/functions/structures is a way to identify what reflects reality (interactivity, potential, stability, variation)
        - for example, its not possible to experience real high status (in a community of nice agents) if you are also nice in some way that is real (legitimate, justified, valid), so avoiding violence is one way to maintain interactivity with that community and possibility of that position, which is a guide to reality as well - niceness is useful as a priority of structural variables like stability, and de-prioritizing niceness is a way to avoid missing out on that potential
        - the reality of the variable (real niceness) is less likely and also more difficult once violence occurs, so this variable is not as possible in reality once that occurs, which it frequently does, and arguably those locked in violent conflict and the requirements of a violent life arent part of reality, as a result of their forced work/conflict and lower potential/interactivity
        - meaning, 'real niceness' is possible in reality, so a decision that prevents future niceness (by making it more difficult or even impossible) isnt real
            - relatedly, 'real niceness' requires being the opposite of nice in some cases, such as where being 'nice to predators' is the opposite of 'nice to victims', so being 'cruel to predators' is the ultimately 'nice' decision, where being 'nice to predators' is an over-prioritization error that invalidates 'niceness'
        - similarly, 'real caring' is indicated by solving problems, which is a structure of stability, power distribution, variation-increasing, and other priorities of reality
        - similarly, 'real abstraction' is indicated by being undefinable, except in relation to other real abstractions (covering reality)
        - similarly, 'real similarity' is where one structure can be used to derive/predict the other, because they are relevantly/really similar
        - reality often involves the space between these real variables/concepts as they degrade through their less real interactions, as 'real' structures are less common than 'decaying/previously/almost real' structures (there are very few stable structures in reality), as finding a real similarity (that is resistant to change) is tough to maintain given the chaos of reality, and real abstractions are tough to maintain given their requirement for formatting all of reality, and real niceness is tough to maintain bc of the required abundance of intelligence or other resources to be capable of niceness, and real caring is tough to maintain bc of the unreality and meaningless of most structures, same for the other priorities, and similarly finding one structure that represents all real variables is tough to find as a result of these improbabilities and difficulties
        - these real structures avoid an error of over-prioritization bc they enable other real structures (such as enabling more variation), so theyre not over-prioritized in the sense of causing failure/instability, but rather are balanced in a way that allows other structures to be real (allow more variation to occur)
        - a function set that doesnt involve real structures (like the spectrum of 'niceness' which includes cruelty and therefore allows for the concept of 'justice' as the spectrum reflects a similar degree of variation as reality as it allows opposites) is therefore less real and less likely to be useful than a function set which includes a higher ratio of real structures, as there is a reason why real structures exist (or are known) and that is that they are useful for some intent such as 'justice', as there is a legitimate place for cruelty so any algorithm that over-simplifies, over-prioritizes, or otherwise results in over-niceness or absolute niceness is similarly unlikely to be realistic/useful in/relevant to reality

    - identifying useful queries that find useful structures like differences/similarities is useful (as these queries identify a structure reflecting/associated with the cause of those structures)
        - identifying differences with questions like 'what is not an input/output sequence or simple adjacent change combination' (everything can be formatted as a 'sequence of inputs/outputs' but what is least like an input/output sequence is structures/functions like changing position to make input/output sequences adjacent or unnecessary, connecting inputs/outputs with different inputs/outputs and then connecting the different inputs/outputs, change the base so that input/output sequences are unnecessary/adjacent, or combining abstractions or other causative variables which are powerful change-causing variables meaning their membership in a set is more important to know than their sequence, equivalent alternates which identify all the variable input/output sequences, variables like probability/interactivity/adjacency which determine input/output sequences)
        - identifying similarities with questions like "what systems overlap on the 'combine' function" (as in what can be combined, bc it exists in a system that has the simplest function which is combine, making it sufficiently similar to reflect other systems - such as languages, interfaces, systems, states, etc)
        - relatedly, identifying 'differences in similarities' applied to cross-interface connections (like connections between improbability/difficulty) is useful to identify when a similarity doesnt hold, which is significant as a predictor of other variables given its causative power of variation
            - for example, identifying how two related/similar concepts (like improbability and difficulty/complexity) are different is useful, such as how a real structure may be difficult but not improbable, if it is the 'first path found' to achieve a common intent, so its used more often despite its difficulty
        - the reason that concepts enable workflows is that the concept allows a difference (covers enough variation to explain reality) and the workflow solves a problem (connects a difference within that concept, or optionally wihtin sets of concepts)

    - identify useful structures like how useful structures like 'abstractions' can be applied as connected to other useful structures like 'bases' that are used in workflows like 'change a base solution'
        - for example, structures are more useful when obvious, when embedded in or connected to existing structures (like a new 'difference' possible in a known similarity), and when abstracted to some degree that removes some details but preserves other info like important similarities/differences
        - this is bc of their connection to core workflows, such as how a relevant abstraction can be applied as a useful base to be applied as an input to the 'change a base solution' workflow
        - at that point, the problem becomes 'find the relevant attributes to abstract which are uncertain/variable/unknown/irrelevant, and the relevant attributes to preserve as a core standard of useful certainties' to find the relevant abstractions, which are often new structures that capture different info than existing structures

    - identify useful connections between interface structures that are useful to apply in combination (like network/concept, component/system which is across the 'scope' interface, function/usage and difference/standard), where these connections can be used to identify other useful structures like useful filters
        - for example, identifying whether a 'criminal society' or a 'non-criminal society' is likely to apply concepts like 'freedom' is possible by applying a different scaled unit (non/criminal), where 'freedom' is only possible in a non-criminal society, since a criminal society requires work like 'identifying/separating criminals', where this required work contradicts the concept of 'freedom'
            - this is possible bc concepts like freedom are complex enough to be best defined/represented by network interactions (such as societal interactions in the social network), and agent interactions are a complex/high-variation variable that contains a lot of information, making other high variation variables like abstract concepts more trivial to identify/derive
        - similarly, other scaled units are often trivial to filter out, such as functions created by 'simple repeating patterns of scaled units (like waves)' which are often uncommon in real systems, as its trivial to identify if a solution function is in this set of 'scaled unit/repeating pattern' functions, as a trivial variant of simpler functions
        - this sort of 'connection between components (like units) & systems' is possible bc of the 'existing connection between components & the systems they occur in', and is useful bc components/systems often reflect info about each other as theyre often specialized to be optimal in some connection/interaction, so the component reflects the system and vice versa
        - connections between these combination structures can form the basis of core workflows ('define a concept in a network, to account for its network of possible alternate definition routes and its complexity' can be used to fulfill problem-solving intents like 'identify adjacent concepts to make some problem more trivial to solve') bc these structures are different enough to contain enough variation to solve most/all problems when connected
        - similarly, there are variable sets which dont contain enough info or dont handle enough errors/edge cases to be an optimal solution, so a 'network of functions' is more useful to switch between functions when one is more optimal in some case (such as a switching function between cooperative/competition when its impossible to find a reward in one system, such as how a 'win' is not always possible in the 'competition' system, as its an incomplete system without enough symmetries reflecting reality to be useful to solve most/all problems)

    - identifying useful structures like reasons for usefulness of a structure in fulfilling some useful intent like 'finding errors' (such as by connecting structures like 'scale' with possible errors like 'over-prioritization')
        - for example, applying useful structures like 'differences in a similarity' across interfaces is useful, such as how finding the 'contradictions in a system' (the 'differences in a similarity (the same system/context)') is always possible (such as how 'capitalism' involves 'socialism, for some but not all' in a position), and is useful for identifying possible initial errors that could invalidate the system if allowed to repeat
            - relatedly, finding structures that invalidate a system when applied as 'scaled units' (scaled initial errors) is useful to identify important error structures to prioritize handling with solution functions
            - 'contradictions in a system' are also useful in that theyre a 'unit complexity/embedding structure' that is useful to identify
            - also structures which can be defined with many different definitions are often more useful in that they will fulfill more intents and interact with more structures
        - scaling/iteration is one of many functions that is useful for finding errors bc it often creates 'interactions', 'known errors like imbalances like over-prioritizations', and 'more obvious structures' through the iterations applied, which is why its useful to iterate some structures
            - applying 'similarities/differences' and 'simplifications/complexities' and other reality-covering spectrum/conceptual variables are similarly useful for finding errors

    - identifying useful combinations of interface structures which make it more possible/trivial to identify useful structures like 'probable structures', as an alternative to 'interactive/connection/group' structures
        - for example, identifying 'false useful' structures, which are falsified versions of useful structures, which are easier to fake and therefore likelier to exist than 'true useful' structues, are an implementation of the concept of 'incentives' applied to fulfill the intent of 'identifying probable structures', which is a useful problem-solving intent, just like identifying interactive structures and input/output sequences and connection structures are similarly useful in filtering probably useful/true/relevant structures, by applying the concept of 'truth' to 'useful' structures and applying its opposite to differentiate true/false useful structures, given the relevance of false structures bc of incentives, which makes them more probable
        - similarly, identifying meaningful vs. meaningless structures is useful to differentiate useful filters of real/probable structures, such as outdated/nonsensical/false/trivial/non-causal/indirect/independent/incentivized similarities (such as arbitrary coincidences, like the requirement to use one of a set of values, where any selection in the set could have a false similarity but it wouldnt be causal/relevant/real and doesnt change/determine any other variable) which are useful in determining falsehoods vs. new/specific/obvious/reasonable/true/determining/powerful/causal/direct/dependent/dis-incentivized similarities, which are more useful in determining the truth and related structures like possible/probable structures
            - for example, the statement of 'justice hurts everyone' ('an eye for an eye makes the whole world blind') has an incentive for predators and doesnt successfully apply the concept of a symmetry (whats in it for victims, rather than whats in it only for predators, since there is a symmetry between individuals where both should be treated like equals if they are equal and otherwise fairly treated, rather than favoring and sheltering predators, as there are often different info sets that should be connected at all times, such as a 'global/local perspective' and a 'victim/predator perspective' where a symmetry of fairness is applied, such as in the golden rule, where these differences from a 'vertex' structure aligning with a cross-interface structure)
            - similarly, there is a 'test' (like 'see if an agent responds to this incentive to see if they would develop or have this function') for every 'filter' (like a 'similarity' filter, as in 'is the agent similar in other relevant ways to the agent who developed/had that function') bc they obtain info in different ways
            - relatedly, these symmetries and vertexes can be applied to derive the optimal positions of these concepts like balance/justice/power
            - similarly, the 'self-awareness' symmetry (between the perspectives of self-reference and how the self is referenced by others) and the abstract structure/context symmetry forming a vertex (the context that makes an abstract structure like an abstract overlap/frame obvious or otherwise useful)
            - relatedly, identifying the 'function usage' structure of 'regularly applying the golden rule (to consider other valid perspectives, like other equivalent alternate priority sets that are similar to other useful perspectives)' is a useful structure to identify and apply as a solution to the common error of 'only focusing on one perspective at a time', to regularly correct for this error as it will tend to re-occur regularly or by default, related to function usage structures like 'regular intervals/alternating sequences of applications of a rule', where this structure is necessary to make the rule have any permanent impact or make the rule valuable at all (the golden rule is only useful if applied regularly to offset the 'one perspective/over-prioritization' error) 

    - identifying structures that connect problem-solving structures like functions/workflows so that other interface structures like the 'reasons why problems exist' (such as non-linearity/complexity, resulting from 'lack of methods to find shorter routes given that there always is a short route possible in some graph') are trivial to identify, at which point once identified, they can be used to understand problem-solving better and optimize workflows
        - the problem of finding all ways to solve problems involves solutions like 'reduce difference/distance between problems/solutions' (by implementation methods like 'by reducing differences between all structures') and 'connect problems/solutions' (with implementation methods like 'by finding all connections'), which amounts to the problem of 'making all problems linearly solvable' by identifying connections that are linear or connections which linearize other connections, such as how high variation variables are useful in making many complex problems linearly solvable, since all structures can be connected and there is always a short route vs. a longer route to connection any pair of structures, so connections to find these shorter routes (involving variables like 'high variation variables' and 'powerful variables', both of which cause a higher ratio of changes than other variable types) are useful to identify to fulfill the intent of 'linearizing all problems' (by finding all linear connections between variables, all graphs which make connections linear in a useful way such as repeatedly useful across problems of connecting different structures, etc)
        - for example of linearizing the problem of regression, identifying insights such as 'balance is a common structure' (and related insights like 'extremes are rare structures' and 'alternating' is another way to apply an 'average' function) to identify useful filters such as 'alternating between extremes' or 'balance between extremes' applied to reality-covering variables (like abstract/specific, clear/ambiguous, volatile/stable, simple/complex, similar (average)/different (outlier), etc) is useful as a selection algorithm of the 'next point in a sequence of inputs', applied when fulfilling intents like 'selecting points to use to create a regression function intersecting with these points, when iterating through input points'
            - similarly, identifying reality-reflecting variables such as 'randomness' (which reflects the real proportions in a data set when applied to a 'point subset selection' function) is useful to identify variables that are useful to identify real structures like real patterns/averages
                - relatedly, identifying the various reasons why a data set might reflect randomness/balance/other conceptual structures is similarly useful (these variable interactions allow random variables to compound, creating some distortion of the data set), and identifying various signals that these errors have occurred (some ratio of data set points reflects a similarity/average function that is stable across subset selections, indicating that any distortion of this similarity/average function is an error structure)
            - why does reality reflect these structures such as 'balance'? 
                - 'balance' between extremes like simplicity/complexity enables a higher ratio of variation to occur/be real and continue to occur/be real (such as how over-simplifications lead to static structures like 'avoidance/lack of problems' which leads to 'lack of variation' and 'lack of variation-handling functions', and over-complexities lead to static structures like a 'required higher ratio of time spent to resolve the over-complexity, so that other problems cant be focused on'), but between these extremes, there is freedom, in the variation supported by that balance
                - similarly, standards/limits offer freedom to vary a 'more trivial amount, within those standards/limits', until a better standard is found that has more certainties which are more relevant to the new higher variation that is enabled by the previous standards/limits/certainties
                - finding 'structures that enable freedom' (like balance/standards) is likely to find real/new structures such as 'probable next structures in a sequence including current/past known structures', as reality can only be real if there is variation/freedom supported/enabled within real structures
            - this makes other useful structures trivial to identify, such as identifying that these variables determine most useful variation in useful functions, such as the most 'complex independent' function (supporting complexity such as by implementing scaling/iteration, while requiring the fewest inputs, such as a function that produces volatility) or the most 'abstract simple' function, etc, which are useful inputs of workflows as default solution-finding methods, default difference-connecting methods, etc
                - similarly, rather than just the 'most' ranking/standard of a structure, other rankings/standards using solution metrics, such as the 'simplest extreme differentiating' function, the 'unitary complex function' and the sets of these functions, which are useful in that they make structures like 'function limits/extremes' clear so that finding functions around/in between limits more possible, and also they are not only cross-interface structures but they are also components of results of interface queries and interface queries themselves, which can be applied as default inputs to workflows, given that they embody various standards which are useful for workflows like 'change a standard/base solution' and also embody various extremes which are useful for workflows involving 'reducing differences or applying maximal differences like extremes'
            - similarly, identifying other 'reflection symmetries' which preserve info, like how the 'system' complexity where a problem occurs is likely to be reflected in the 'problem' complexity bc of the symmetry in the context scope reflecting info of different but related scopes (system/component info)
        - relatedly, the intent of 'deriving the implementation method (reduce all variable interactions) from the intent (reduce problems)' can be a matter of fulfilling the definitions/requirements of 'implement', such as 'vary the intent enough for it to be specific, since the implementation method will have to be similar to the intent to a sufficient degree but also sufficiently different that it is useful in some way such as being more specific than the intent'
            - similarly, related implementation methods like 'find all "equivalent alternate" simple variable sets and apply those as default reduced variable sets to connect, since one of the equivalent alternate sets should be enough to derive the "most reduced set of connections" that is the intended solution structure of the "reduce problems" workflow' can be found by applying changes like 'generate "interim" structures like "reduced variable sets" which are a useful specific structure to make finding the solution structure trivial'
            - similarly, 'stacking reductions' (to find the biggest reducing variables and inputs of them, such as 'compounding reductions') and 'reducing randomness (the most complex structure) to linear functions (simple variable interactions)' are alternate variants of the 'reduce problems' workflow that can act like implementation methods for their specificity
            - relatedly, identifying variables that are reality-covering identifies variables in which 'all things are possible', but more specific/filtered variables are useful for finding 'biased interactions' (with a tendency to favor an imbalance or have an unambiguous ratio), which are often more adjacently useful in determining important variables like 'net/scaled effects'
        - 'identifying new connection functions between high variation variables (like abstract variables)' such as by 'scaling interactions of abstract variables (like scale) to identify connections between high variation variables (like extreme/unit structures) to approximate "thinking deeply about a concept like risk/scale"' is useful to identify new workflows, as connections between high variation variables are likely to capture enough variation by default to store solutions to most/all problems, and are likely to be adjacently connectible to problem/solution structures through their tendency to be abstract and therefore adjacently connectible to most if not all structures
            - similarly, identifying new connection functions between 'maximally different structures' is similarly likely to contain enough variation to store solutions to most/all problems

    - identifying structures that seem disconnected but have enough variation to be generally useful and have enough interactivity/similarity to be connectible is useful to fulfill intents like 'identify new variables to apply in algorithms'
        - identifying uncorrelated vs. correlated variables (based on 'info loss/adjacence/similarity') is useful through implementing the concept of 'independence', which is useful to identify through its interactions with other variables like 'complexity' and 'cause'
            - relatedly, identifying how to convert one into the other through connections between un/correlated variables, and identifying variables that are uncorrelated vs. how to differentiate variables from uncorrelated variables to identify variables that are correlated, are all useful intents to fulfill to identify new variables to apply as inputs to regression algorithms, such as how 'area under curve' and 'average change rate' are not correlated with functions bc they encode the concept of averages (area removes encoding of function shape, but not average output height), whereas 'maxima/minima points', 'slope sign change points', 'local averages', and 'regularly-intersecting functions like summary/average functions' are correlated with functions bc they encode 'differences from function similarities (like intersections/averages)', so identifying differences between uncorrelated and correlated variables and similarities within correlated variables and within uncorrelated variables is useful for identifying all useful variables in determining a summary/regression line of a data set
            - identifying 'equivalent alternate descriptors' like 'slopes vs. maxima' is similarly useful, as they can often be used to determine each other bc they store similar info, vs. 'variables that store complementary info' such as info describing different extremes of a reality metric like abstract/specific (storing abstract info and storing specific info often involves storing different sets of info), as 'complementary' encodes the concept of 'independent variables' in a different and therefore differently useful way
            - given that variable interactions depicted in a function form are micro-descriptions of reality, they need to reflect reality (they should fulfill basic metrics of reality like some degree of complexity in general, except for cases such as where a variable interaction is causally adjacent and therefore likely to be linear/simple, so identifying this network of cases where a reality metric doesnt apply such as with specific causal structures is useful)
                - relatedly, identifying how 'independent' variables correlate with other conceptual variables like 'complexity' is useful to identify other useful structures, since more independent variable interactions are likelier to be more complex (but are not required to be complex)
            - relatedly, identifying structures that are meaningful to agents is likely to be a source of meaningful structures in algorithms - for example, identifying a life/decision that is meaningful, such as one that doesnt only follow incentives but also fulfills other solution metrics (such as 'high variation' and 'under pressure, became reflective of reality' and 'allows for and involves progress/learning'), as 'differences from incentives' is a highly differentiating variable
            - this 'set of fields of functions connected by concept metadata (like concept combinations, concept values, concept intersections, concept spectrums, etc)' is generally useful for identifying useful function similarity indexes (to connect/identify similar functions), identifying useful function variables, and identifying useful connections across function sequences like 'variation across "similarity indexes (like plateaus/powers/slope sign changes/wave similarity/volatility/specificity)" and variation across "differentiating filters within similar function sets (like limits/extremes)" used to generate the sequence' (sequences such as "sequences created by a function change/generation function" to find functions to check, or "sequences from partial to complete functions", or other function sequences resolving ambiguities between similar functions), so that questions like 'what attributes should a function sequence or function sequence-generating function have, to maximize chances of finding a useful summary function in that sequence' can be answered ('it should apply certainties/uncertainties in the data set as constants/variables', 'it should increase/decrease a variable if it increases/decreases an optimal solution metric', "it should apply a 'conceptual similarity score across many similarities' rather than a "one-metric/simple similarity score" when calculating errors", etc)
                - finding a 'most reduced set of numerical values' that encodes enough reality-covering variables and represents enough variation to graph all relevant different functions in a problem space (like all maximally different polynomials for the regression problem space) and connecting them with every similarity type, in every possible function sequence having a sequence size that could be useful, is similarly a useful set to connect to other structures like similarity index networks and data sets/functions, to find useful queries to connect similar functions
                - for example, a set of numbers that has 'overlaps in unit operation outputs' (where various unitary polynomials overlap) is required in this set of useful numbers to describe all different polynomials
                    - similarly, other interface structures should also be present in this set, in order to fulfill the requirement of maximal differences allowed by the set, similarly it should include a set of numbers that allows 'polynomials with varying magnitude', etc
            - similarly, the 'structure of data set values checked/used directly in a regression algorithm' (a structure such as a 'continuous subset line' or a 'set of points or continuous subset lines at regular intervals' or a 'ratio with a selection method like random') is a useful variable to apply that determines differences between algorithms that are useful, so given this differentiability, it can be applied as a variable to generate new algorithms

    - identifying problem/solution structures on interfaces that are likely to apply to other problems and therefore likely to be useful for identifying other useful structures like insights is a useful intent to fulfill
        - for example, identifying 'causal problem/solution structures' such as how nodes with common inputs (like how 'emotions' and 'actions' both have 'information' as an input) and other nodes with common outputs (such as how 'thoughts' and 'emotions' may have 'actions' as an output) may have an intersection ('thoughts may change emotions' and 'info may change emotions') rather than 'emotions' being an irrelevant output of 'thoughts/information', it includes the output of 'emotions' as a possible input interim node in a sequence
        - identifying this causal problem ('which nodes are connectible in a sequence') and a possible solution to that problem in the form of the accurate causal structure of the actual causal sequence that applies ('outputs like emotions of an input like information may be an input to the other outputs like actions') is useful for identifying insights like 'thoughts/info can change emotions'
        - this can be resolved by applying the 'sequence' structure as a possible solution, to check if its possible to compress the irrelevant/unconnected nodes into the same sequence
        - an alternative solution exists, which is that the output node (emotions) would occur after the other output (action) but a 'time' constraint filters this out, similarly, 'emotions' could reflect a similar degree of variation of 'information' inputs that it could act like a proxy or equivalent of 'thoughts', which is another possible insight to identify by switching/overlapping/grouping node positions in the sequence, adding a 'node layer' variable where multiple nodes are possible at one position in the sequence
        - resolving other causal ambiguity problems with similar solution structures would be similarly useful for identifying insights (causal connections between nodes)
        - these example problems/solutions on various interfaces like 'cause' are useful to identify in case they apply to a given problem, which is likely given that they are core/unit problems that can be composed or otherwise operated on using core interface functions, as pre-solved problem/solution unit pairs

    - identifying useful structures to combine with other workflows in structures like a sequence or variation (like a specification) such as how following workflows like 'pressure/stress/change known certainties until uncertainties are resolved by identifying new certainties' (or apply changes to uncertainties like 'ambiguous/trivial differences' until theyre connectible to certainties like 'obvious differences') is made more specific when followed with or specified with structures like to apply changes 'in the direction of uncertainties, once certainties (like limits of computation) are identified and structured in a network or other structure that correctly positions uncertainties in between known certainties/limits'
        - relatedly, specifying the 'limits of computation' is useful in general and similarly solutions to solve the problem of 'increasing computational capacity', such as to identify complexity and number of problems that can be solved with current computation capacity simultaneously and how these problems are likely to increase computation capacity, and identifying problems that help solve other problems to increase distance from limits of computation, and similarly, physics limits such as the distribution/usage of computers in the universe that can exist without interfering with variation of life forms that would benefit from their computations (its possible to convert most positions into a component of a computer, but is that detracting from its ability to compute something else, and what else might it be computing)
        - similarly, identifying the 'sets of certainties' that are useful in identifying these uncertainty directions/structures is useful, such as identifying the 'high variation cross-interface certainty sets' that add the most value in determining uncertainty structures
        - relatedly, identifying a structure where the uncertainties have some similarity in common like a 'pattern of directions' is useful to identify variables determining uncertainties

    - identifying useful structures like 'alternatives to definite negatives (like war)' such as fake wars with fake weapons like joke weapons, being higher variation/potential than the enemy, increasing distance from the enemy by leaving the enemy behind with more space innovations, using different methods that are less toxic like insults, creating so many resources that the enemy doesnt need to fight over resources, different positions of war like different battefields such as online or in entertainment media or in energy innovations, making other people fight a war, being better at defending the enemy than they are, etc
        - these are useful bc they incorporate more solution/optimization metrics than just war on its own and support higher variation bc they involve less irreversible decisions like killing, which even if similar to war in other ways at least avoid its worst errors
        - the problem of 'war' applies to other problems that involve negative/harmful processes or agents like in the bio-system, where pathogens often use similar tactics such as false info
        - similarly, the problem of 'war' often occurs bc of a scaled/combination function applied to errors that can easily cause instability (like 'lack of logic' and an 'overabundance of emotions' and 'lack of ideas about alternate govt methods' and 'large groups wanting the same resource which are positioned adjacently and are similar enough for the war to have an uncertain outcome')
        - identifying alternate problem formats which capture enough variation to be applied as an interface is a useful intent to fulfill for other problem-solving intents, just like solving 'scaling' or 'batching' or 'delegation' or 'circumventing info barriers' is useful for solving other problems
        - relatedly, identifying structures that are obvious in some problem formats as generally useful structures, such as how 'war' makes structures like 'conflicts between opposing forces' obvious, so that this structure can be applied across problems, such as how in the 'regression' problem space, opposing forces like generative functions and limiting/filtering functions exist, which can be resolved similar to how war conflicts can be resolved (by aligning the opposing forces to have a similar or equal direction, such as in 'war' by helping solve the enemy's problems for them, and in 'regression' by integrating the filter in the generative function, such as by applying filters to inputs to the generative function which reduce the post-generation filtering problem requirement)
        - relatedly, problem structures like 'tradeoffs' can be resolved with similar direction changes/alignments in the opposing forces of the tradeoff, so that both forces can change in a similar angle or other similarity

    - identifying useful structures like 'truth structures' from identifying structures that when applied to the opposite/any/all statements seem equally possible/valid/true, like structures containing 'multiple perspectives', which may falsely seem true bc they contain an interface structure ('multiple' and 'perspectives') and support other truth/interface structures like 'high variation' and truths like 'the higher probability of validity of more perspectives than one (except interface analysis itself)', so these 'truth structures' which make a statement seem true can be applied to identify other truth structures
      
    - identifying useful structures (like 'positive opposites' and 'connections to truths') that can be applied to find useful differences from error structures like 'known conceptual errors' to find useful rules to apply in intents like 'filtering workflows' or 'filtering truths'
        - for example, known conceptual errors like 'over-simplification/reduction' offer a way to identify opposing conceptual solution metrics like 'fairness' (found by applying the positive opposite, as opposed to a negative opposite, such as another error like 'over-expansions'), such as 'allocating work to functions/systems that can handle it' which derives other solution structures like 'delegation', which can be optimized with other workflows to identify other optimizations (like 'allocate work to functions/systems that can almost handle it and identify resulting errors as a way to determine where organization can be applied to enhance their capacity' which could fulfill other functions like 'learning' as the functions are likely to be almost optimal but will need to change slightly to achieve optimality)
        - similarly, identifying related structures of conceptual errors/optimizations is useful, such as how identifying that 'most statements could be called over-reductions, as the truth is often more complicated than most statements, and most statements are also false, as there are often more distortions that could be applied to make a true statement false than distortions that could make it true' is useful to make identifying over-reductions more trivial

    - identifying useful different formats is useful, bc they are often useful for some specific intent like describing a relevant attribute of a problem, but are also useful for generally useful intents like 'identify conceptual differences (like the difference between correlation and causation)'
        - for example, different graphs like a graph of 'state changes of a variable (such as the state sequences that describe its real interactions in real systems)', vectors indicating simple attributes (like 'slope of one variable') which allows comparing slopes between different variables trivial, or a euclidean graph of input/output variables which has false implications that need to be corrected with rules like 'correlation is not causation' as it adds a directional difference as a standard compared to 'simple slope vectors', all fulfill different useful intents and also imply different adjacent conclusions which may be false, such as where 'simple slope vectors' might not cause this confusion between 'cause' and 'parallel slopes', but the euclidean graph fulfills other useful intents like trivially identifying 'what changes in the output variable when the input variable increases', where 'difference from the original format' or 'difference from standard formats' may be an indication that more false implications are likely, and possibly also an indication that these formats should be graphed as a set rather than in isolation

    - identify useful structures like useful filters for workflows/interface queries like 'cases that make a workflow more useful/optimal' (for reasons such as 'bc of the functionality of the structures involved' and the 'problems identified which would benefit from that functionality')
        - for example, workflows using concepts are useful when various cases apply in the problem space, such as where 'adjacent conceptual combinations havent all been applied in the problem space, indicating that applying these would be useful to check' and when 'a field/problem has stagnated for lack of new concepts, indicate a missing concept which would be useful to find', in which case workflows applying conceptual structures are most useful, indicating that those workflows should be used first and are likelier to succeed, as a way of filtering workflows to use for a particular problem
        - similarly, 'abstract (reality-covering) concepts' are high-variation structures which explain many other structures with trivial changes, so theyre also better to apply in cases like where complex understanding or complex changes are required, as high variation structures like concepts can often simplify complex problems

    - identify useful structures like info structures common across many or all problems, such as 'function usage barriers (where using the function is easier with usage/practice)' or 'info barriers' (that require coordinating/differentiating effects to overcome or structures adjacent to other functions like 'adjacent concepts to make some function easier')
        - for example, a 'barrier followed by a cascade' is a common info structure in problem-solving, which is associated with some other structures, such as 'complementary/coordinating/compounding and differentiating function sets/sequences/networks' where these function sets have some structures in common that give them this side effect and often have structures in common with other 'barrier-cascading' function sets such as the number of functions and function iterations in the set, as most problems are problems of a lack of functions specifically organized to overcome that barrier, but which often already exist and just need to be paired with enough existing functions in the right structure to have these complementary effects to achieve scaled outputs that could workaround the barrier, so finding coordinating structures that can achieve scaling is useful to find possible 'cascade-producing' functions, which are a solution set in the problem space of 'overcoming scaling barriers'
            - for example, 'thinking' is a problem space with barriers such as biases, which are easier once these barriers are overcome (same with other processes that become easier with usage/practice, like 'default functions' or 'default/cached function usages' or 'pre-computed function inputs/output maps' or 'indexed functions'), with function sets like 'focus/filter, structure, change' or 'start with unit case' or 'find interaction level with highest variation'
            - relatedly, some functions that use similar functions as workflows ('start with unit case, then expand/complicate it' being related to 'start with base solution, then change it') which can indicate that they fulfill those workflows to some degree or in some structure
                - these functions are useful to identify in connection to these workflows as theyre more specific and therefore specialized/optimized for some problems, and identify other useful structures such as 'find the intersections of scaled unit cases of different problems, which create the more complex problem'
                - relatedly, another reason to start with a 'network of unit cases' is to have a solution for every initial direction that leads to a problem
                - another reason is to avoid the over-simplification of 'avoiding barriers' which may solve a problem temporarily but would not solve the problem of 'overcoming the barrier within the parameters of the unit case, which if it can represent all problems, should have the variation necessary to overcome the barrier', although 'avoiding barriers' is one way to determine where the barriers are
        - relatedly, 'info bottlenecks' preventing info arriving as expected, and being described/explained by structures like 'batches after reaching a threshold' are related structures of specific errors that can describe some problems
        - similarly, 'delegation' is another problem-solving structure which involves 'allocating problems where they can be handled best' which requires 'organizing/sorting these problems or related problem structures like problem causes'
        - similarly, 'rules that a bias would create' such as over-simplified rules such as 'just add variables until something works' is an over-simplified biased rule that can often be avoided

    - identify useful structures like 'useful positions of workflows in the workflow network' which can be determined by structures like 'probable ranges/requirements' of useful metrics like 'creativity'
        - for example, 'change a base solution' is a useful workflow when an average 'degree of difference from existing solutions' ('creativity') is useful, so determining metrics like 'probable range of required creativity' in order to apply rules to select workflows accordingly is useful as a way of generating alternate 'intent sets' to fulfill in the initial interface query that selects workflows and applies them in various positions in the network represented by the query, otherwise other workflows can be selected for other ranges of required creativity, such as low complexity problems which can likely be solved by 'adjacent combinations of existing solutions' or high complexity problems which can likely only be solved by 'finding "new abstract concepts in a field" or "a new field and default concepts in it" is required to make a solution adjacent'
        - therefore determining the position of 'change a base solution' in the network of workflows is possible by applying metrics like 'probable required range' of 'degree of difference required to solve the problem' (representing 'creativity') and associated metrics like 'complexity' which can be determining in the identification of creativity ranges
        - metrics that temper 'creativity' such as 'relevance' are useful to apply to determine 'relevant creativity' (a more useful form of creativity as well as most other metrics) by keeping creativity within a degree of difference that is useful, usable, testable, measurable, determinable, differentiable, describable with existing/adjacent concepts, possible to implement, different from simple/default structures that could falsely seem similar to creativity like 'iterations without a reason' which can be applied for more 'exploratory creativity' intents (for when resources are abundant like when the problem-solving program isnt being used) where real relevant conceptual creativity by comparison 'implements multiple conceptual variables in a useful way (such as by identifying a new structure based on new combinations of similarities applied as certainties/standards (such as requirements/insights) to base changes around, which fulfills multiple conceptual attributes like "complexity" and "abstraction" and can be described in many ways, which can be generated by default to apply as new solution sets to filter when more creativity is required)', etc
        - these conceptual workflow metrics can form a network that can be applied to select workflows/implementations of them, just like a workflow network is useful to determine known useful routes between and positions of workflows
        - similarly, a 'concept set' iteration algorithm can be applied to identify the structure that implements each concept set in a problem space, since abstract concepts describe reality in the most orthogonal ways which are likeliest to decrypt complex structures and also likeliest to be creative, other than algorithms that 'apply a simpler type of difference like "more differences" such as "iterated differences"'
        - similarly, 'trial and error' has a similarity to the definition of 'random' (in a way to generate it) just like 'change a base solution' has a similarity to the definition of 'certainty' (in its interactions with certainty changes, such as new information), which means that other concepts can be defined with variable definition routes that, through their interactions with other structures, will likely correspond to workflows

    - identify useful structures like optimization functions/metrics to apply to improve/differentiate solutions (like usage/implementation/interpretation/understanding/existing information before applying a workflow, etc)
        - for example, 'change a base solution' is a generally useful workflow, but it could be useless depending on the implementation (the interface query implementing the workflow) and on the usage
        - realization query: how did I come to this conclusion? by comparing how a smart mind (mind with better scaling functions which can hold/identify more structures and think more steps ahead and has more functions/structures like more memory and more sub-networks) would implement a workflow like that, compared to how another mind might implement a workflow like that, which could have very different results despite using the same logic, as implementation and usage and important factors in the ultimate usefulness of a function
        - for example, a smart mind might apply 'change a base solution' by first learning why a solution is useful/optimal, learning the problem space system to some degree such as knowing common problems/solutions in that system, as well as what variables determine optimality, how to test solutions for improvement in some optimization/solution metric, etc, and therefore bc of these other structures, the solution they select to apply as a base solution would be far more optimal in many cases, whereas another mind might apply it by randomly changing or adjacently changing the base solution without thinking about whether the changes could produce an error
            - I could have also realized this by iterating through 'high variation variables' (one of which is intelligence) and applying it to interface structures iteratively

    - identify useful solution structures like 'generality' of solutions and 'specificity' of information about how to enable/trigger the more general solution (like enabling immunity by clearing inflammation) that tends to be useful in the problem space system
        - for example, proxy target solution structures in the 'find a medicine' problem space include medicines that 'avoid some causative factor of medical conditions (like whether a medicine is pro-inflammatory or anti-inflammatory, as its rare for a pro-inflammatory compound to be useful)' or 'increase/decrease some metric outside of its optimal balanced range (where if the metric is extreme, different groups of cancers are caused, but if its in the balanced range, cancer is rare)'
        - similarly, compounds that have a wide variety of similar structures to pathogens but arent pathogens (molecules designed to trigger antibodies) are a good target for building immunity which is useful for avoiding cancer
        - similarly, compounds that have useful functionality related to important functions like cell communication, lowering stress on the system, repairing DNA, clearing useless metabolites (waste) from the system (increasing antioxidants to reduce reactive oxygen species, increasing flavonoids to trigger useful metabolic processes similar to diuretics/enzymes, increasing preserving membrane integrity with modulators like caffeine/cbd, increasing highly variable negative compound-binding compounds, increasing cell devision inhibitors, increasing anti-inflammatory substances like salicylates to reduce inflammation metabolites in blood, increasing antimicrobials like sitosterol to avoid excess microbes & their metabolites, increasing metabolic regulators like berberine to avoid excess metabolites, increasing functionality of filtering organs with substances like chicory, increasing processing of plaques/calcifications, increasing paraoxonases to protect existing cells, increasing DNA repair compounds like silymarin to protect existing structures rather than needing to break them down, increasing immune regulators like sitosterol to decrease immunity metabolites, increasing molecular weight of compounds like hyaluronic acid to avoid absorption/processing to prevent excess metabolites, etc, all in small enough amounts to increase 'clearance of the blood'), downregulating cell growth, etc are useful targets for anti-cancer compounds, bc they represent components/causes of carcinogenicity of mutations/pathogens/states
        - so identifying whether the compounds is likely to be anti-inflammatory is useful to filter possible medicines, which is likelier to be a more effective and generally useful target for finding medicines than specific activity against some state/pathogen, which is hard to find as most medicines with specific activity against pathogens are also toxic to host systems, and also the immune system can often handle most disease states on its own if its work is minimized by clearing inflammation, reducing oxidation, managing epigenetic activations, repairing DNA, etc
        - relatedly, finding useful interface structures like 'optimal ranges between extremes' and 'extremes which cause disease states (like how extremes of some variables cause different cancer groups, as there are groups like hormonal cancers and immunity cancers and so on)' allows specifying target solution structures further
        - similarly, identifying useful interim structures like 'pathway activation sets' is another useful structure to identify and apply as a default input, as its more trivial to identify an optimal set of pathways to activate to manage a condition, and then find specific medicines for that set of pathway activations, than to just filter the whole set of possibly useful medicines, as a useful interim/proxy intent to fulfill than the original problem-solving intent

    - identifying useful structures like graphs that organize useful structures (like 'filter sets with common outputs') which are useful to filter by finding their organizing patterns, since there always are some patterns of filter variables that determine other useful filters
        - for example, identifying a graph of a 'filter circle' where patterns between results of different filters can be organized and easily seen by applying different filter sets/combinations/sequences in different directions, is a useful structure to identify to see 'patterns in outputs' of filter combinations without having to compute all the filter combinations
        - this is useful bc (using a structure like an 'undirected' network), it applies 'trial and error' to resolve an uncertainty in a particular variable (an uncertainty between what are falsely similar to 'equivalent alternates', where an ambiguity exists that looks like an equivalence), and the routes that produce equivalent/similar changes will become clear so that alternate 'causal paths' of the same outputs can be identified and filtered further once identified, and applying every variation of 'filter variables' is particularly useful in finding maximal differences in filters to apply
        - for example, applying filters like 'functions with average values of useful differentiating attributes (or function similarity indexes) like volatility, to find useful structures like "determining directions" away from this average value toward other function types' and 'functions with extreme/trivial change rates (to filter falsely similar function sets, like linear functions and wave functions with trivial magnitudes)' are useful to filter function attribute values/ranges that can be applied to resolve uncertainties between different function types, where these types are useful to identify bc they are determining of other info about the function, as 'slope extremity' and 'volatility' are useful maximally differentiating filters to apply
        - this indicates the value of related useful functions like 'find the points of these attribute values that are determining, in that trivial changes are required to determine the actual attribute value, like changes in opposite directions from that determining point'
        - this is also useful for finding the 'rings/overlaps/filters/directions (and related structures) of function attributes' that create function similarities/differences
        - applying patterns in differences (like a pattern of alternating values, applied in every direction) is also useful in that it could make identifying a "certainty/structure (applied in the pattern) in the change sets reflecting real new/unidentified similarities" trivial, since patterns are a type of similarity
        - relatedly, applying a filter to exclude 'equivalent alternates' as default changes to apply since they are likelier than other structures to represent real ambiguities (except for the 'equivalent alternates' of maximally differentiating variables), to resolve these ambiguities by applying different structures than 'equivalent alternates', could make identifying the resolutions trivial
        - this solves the problem of 'identifying the variable to add that makes connections/similarities/differences between other variables clear' (like identifying a causal structure of a circle that when rotated, produces the original variable, like 'height of a point on the circle')
        - this identifies useful info like that workflows like 'trial and error' and 'change a base solution' are connected to abstract concepts like 'randomness/uncertainty and ambiguity' and 'similarity and certainty'
        - similarly, given the spectrum of 'equivalence, similarity, difference, opposite' that exists (and other possible structures like cycles where these would be in surprising positions) in many problem spaces, determining this field of equivalence & related concepts by identifying 'functions that connect these concepts in that problem space' is useful as a filtering structure for function sets to apply as a default solution space
        - similarly, a graph of 'cross-interface' structures or 'perspective vertexes' is useful bc of the usefulness of these structures (including the 'reasons/intents' associated with a variable interaction since theyre highly determining of variable interaction structures such as changes)

    - identifying useful structures (like 'optimization rules/metrics') that are useful in that they can be applied in other systems (like 'filtering interface queries' which is a problem-solving intent) to a similarly useful degree
        - similarly, just like identifying simpler vs. more random functions is useful to identify the interim set of functions in the regression problem space where more complex functions will be found, identifying the patterns of interactions in possible interface queries that have known effects or random effects is useful to identify the set of interface queries with interim effects (uncertain effects, between known/simple and random), like how identifying that 'function/structure' is a pattern that can be applied in a sequence in a valid form, and similarly, some functions can be applied in an 'iteration' pattern to solve a problem (problem-solving structures like 'filter' can be iterated in a sequence forming the interface query), and some structures have 'interchangeable equivalent alternates' which can be used to vary the interface query, similarly, other useful interface queries which are likelier to be able to handle complex problems are likely to be in the interim space between these pattern-compliant queries and random queries
        - identifying query metadata, such as the 'degree of the sequence of side effects of a possible solution that is derived and filtered for some solution/optimality metric' (like applying '10 steps ahead' as a solution metric for filtering a solution, and determining that 10 is the number where most solutions are more permanently optimal or reusable, if their side effects are analyzed 10 steps away and they still pass solution metrics at 10 steps ahead) is similarly useful for filtering this set of queries

    - identifying useful alternate structures of useful structures is useful to identify variables of useful structures that can identify other alternatives
        - for example, structures like a 'language', 'memory', 'observer', and 'computer' can be applied as functions to use as proxies for other functions (like map/find/check/scale), which indicate their usefulness in algorithms, so an algorithm using these useful info/solution structures as components will likely have overlaps with algorithms based on the functions optimized by those structures, to a point where sets of these structures can be used as an alternate set of problem-solving structures
        - similarly, applying useful intents to these structures is useful, like how 'differentiating useful structures' like dictionary/language/memory (a dictionary is a set of unique definitions, a language is a dictionary that can be more easily used than the original dictionary, a memory is set of usages of the items in the dictionary in connecting/organizing/storing inputs, such as sensory inputs) identifies the usefulness of identifying variables of these structures and the other structures created with those variables
        - similarly, a 'state with rank/priority' (best/worst cases) is useful to identify as a useful info cross-interface structure (where 'relevance/optimality' and 'contexts/systems' intersect) and 'info structure with intent' (like a list, which is likely to be used for creating other similar structures like maps, for filtering info related to the listed info, for identifying members of a set, for aggregating useful structures like 'instructions to be copied & applied elsewhere', etc), which can be 'described to identify variables' and 'varied' to find other useful info structures
        - why is it useful to identify new ways to implement the same or similar workflows (like 'identify useful structures')? bc specific implementations can be different enough to be useful to identify and apply as separate workflows useful in different situations
        - governance/market structures are similarly usable as an alternate set of structures to these core problem-solving functions, as they resolve resource distribution problems and encode power distribution insights like 'group dynamics should be applied in governance structures, such as majority rule or representation of groups' to derive other insights like a 'true democracy handles group dynamics the best, and improves itself over time to better serve different groups (such as with incentived paths out of crime like incentives for drug dealers to find medicines or narcotics with neutral effects, or more specialized incarceration structures to handle group dynamics, which allows earning privileges like switching between groups to avoid incentivizing more serious crimes)', and similarly 'communism' encodes structures like 'sharing information' and 'social networks' as a structure to fulfill problem-solving intents like 'find info'

    - identify connections between useful structures like 'similarities' and 'connections to requirements' and 'optimizations' to identify specific useful structures like 'limited ranges of optimality' that determine other useful structures like 'probability' (structures will likely be in the 'limited range of optimality' if there is one so its useful to identify that as a useful determinant of probability and similarly useful to check for it)
        - for example, identifying that a 'limited range of optimality that is not required (allowed to vary)' leads to functions like the 'normal distribution' is useful to connect variable interactions with interface structures like common structures and probabilities and useful specific concepts like 'degrees of freedom', this 'range of optimality' being possible to predict using other relevant variable interactions such as 'common weight of required components requiring certain height/strength ranges to be narrow/limited (as opposed to unlimited)' (given weight/strength, usage/interaction functions of components of bio-system, its possible to predict the 'limited range of height optimality' leading to a normal distribution)

    - identifying useful structures like 'graph intersections' which identify structures that fulfill problem-solving intents like 'identify structures useful for multiple intents'
        - for example, the grid formed by 'repeating a unit interaction' is a useful graph to implement a graph of an interface structure (a 'unit') and to identify interface structures of that interface structure such as "intersections/overlaps between the 'unit grid' and other graphs", to identify structures that 'fulfill multiple metrics or functions', which is a problem-solving intent, as structures which are formed by multiple graphs are likelier to exist than structures formed by one graph, as 'commonness' is a structure of reality/truth, similarly a 'default', 'adjacent change', and 'iterated' graph are other interface structure graphs that are likely to create overlaps/intersections between interface structure graphs, as interface structures are highly connectible and interactive and many subsets of interface structures act like equivalent alternates
        - similarly identifying graphs that form interface structures (such as similar concepts or other useful structures like randomness) is useful for identifying probable/adjacent routes to randomness (as well as the 'scaled unit' route to randomness and other useful routes), as its unlikely that a problem will be solved by one graph and likelier that it will be solved by multiple graphs with common points/structures between them, where these common points are similarities that can act as the connections between the graphs, and similarly connections that create other interface structures are likelier to exist and reflect reality than other structures
        - 'finding the similarity indexes between graphs' is more useful to identify useful graphs or realistic graphs than any specific graph on its own, although some graphs are known to be more useful (such as a 'high variation, realistic, relevant graph' which captures a lot of information/variation but is still realistic and therefore relevant, such as by applying adjacent changes to real facts or combining known real concepts) so these can be used as default input graphs to a function to find similarity indexes between graphs
        - similarly, finding related useful graphs like the 'network of the highest variation variables (such as waviness, moments, repetition, symmetries, adjacencies, non-obvious pattern compliance, complexity, etc) that determine differences across functions (and have the fewest overlapping functions associated with these differences)' as a default network to traverse when finding maximally different functions and the 'network of optimizations (such as generally useful structures, like function optimization structures such as where functions are capable of producing the opposite/different effect of their explicit defined intents so they can self-correct or reverse when they determine theyve found an error, and similarly can evaluate/alter their own functionality to self-optimize/adapt, and similarly can evaluate the systems at various levels in which theyre applied for finding additional optimization opportunities)' which offer value in their differences of intent/structure associated with these functions as well as the variation captured in these networks and the connectedness with other useful networks, which are also useful as alternate perspectives of reality

    - identifying useful structures like 'filters which apply changes within a known range of optimality' (like changes that 'maintain functionality of a system')
        - for example, when finding medicines for a particular condition, 'compounds that deactivate a function with an interchangeable alternate function (acting like a backup)' is possible to use as a filter with changes 'within a range of acceptable changes in the system that likely or definitely wouldnt destroy the system (safety range)'
        - similarly, identifying 'compounds that dont have extreme requirements' (such as 'compounds that require 100% organ function' or 'compounds that require ketosis or create ketosis') is useful as another filter
        - similarly, identifying 'compounds that are not deactivated by common inputs' and 'compounds already created by other functions of the same bio-system (existing resources, which are less likely to be absolutely harmful given that there is already a function to process these compounds)' are similarly useful to identify
            - relatedly, identifying these useful compound filters also identifies useful optimizations of the bio-system (functions/requirements/variables that would be useful to scale up or add)
                - adding processing functions so the bio-system uses cancer cells as an input (energy source) to some existing requirement would be useful
                - adding processing functions so that more compounds can be handled would be useful (is the reason for many deaths a result of failed organs which couldnt process errors at scale, like excess inflammation or metabolites of immune functions? if so adding processing functions like blood filtering machines could make some health conditions trivial, so scaling up processing power would be a trivial optimization to fix these conditions)
        - 'deactivating one alternate, to require using other alternates, when there is a set of alternates of non-trivial size' and 'not requiring extremes' and 'not deactivated by common inputs' are structures that can construct 'optimality' or apply changes that stay within a 'range of optimality'
        - similarly, there are 'conditional filters' (as opposed to the mostly 'absolutely useful' filters above), such as variables with an imbalance often/generally favoring one variable value, like how 'compounds that can be processed by the bio-system' are generally preferred, but its possible that a 'compound which cant be processed would have useful impacts (such as various forms of sugar)'

    - identify problems with sufficient interface structures (like 'variable/limit sets') that solutions to those problems can be altered with other interface structures (like 'scale') to solve most problems
        - for example, the system associated with some problems (like the packing problem or the traveling salesman problem) can be isolated, re-applied, scaled, and trivially varied in such a way that it reflects more real systems
            - where the isolation is useful for reducing less relevant variables from changing the system, and where systems can be isolated in this way, solutions in this simplified problem space can be applied
        - relatedly, the packing problem has enough variables like 'uniformity of shapes' and cases like 'all uniform shapes' and components like sub-problems such as 'shapes created with unit combinations, where these shapes limit or allow other combinations' to make it high variation enough to reflect some ratio of 'complexity' (and other forms of reality) of real systems, and also this problem is complex and otherwise realistic enough to derive enough interface structures to derive all the others, where insights like 'identify all the variables that can change the problem or an important structure in the problem space' (and related workflows, like 'try combinations of variables that can change the problem or important variables of the problem') is trivial to identify from this specific problem space system, as an alternative problem-solving structure as other structures (like causal structures such as input/output sequences or interactive functions or sequence patterns)
        - similarly, the difference between the 'unit/extreme cases' and the difference between 'function definition/use cases/usages/intent/structure/implementation' are useful to include in a combination rather than in isolation, as they contain a useful degree of variation

    - identify specific structures (like 'specific difference functions' like 'isolate') which are useful to apply in the specific variant, through the usefulness of specific structures in identifying other specific alternatives and specific related or interactive structures to the specific structure which are similarly useful
        - identify useful variables of implementation functions of problem-solving intents like 'isolate negative/problematic structures (such as problem causes/triggers/inputs/users/usefulness)', where variants of the 'isolate' function include 'separate the problem structure from other structures, by adding more structures in between' (like for example, adding a non-plastic layer inside a plastic layer, to separate plastic from ingested substances, as these linings are interactive in a 'stackable' way, and some of them are non-toxic or can deactivate the toxicity of others such as by binding to toxic structures on the inner surface, as 'surface' is variable and has multiple implementation structures such as 'external/internal', as the original package has two important surface variables, how it interacts with the outside world to protect the substance and how it interacts internally with the substance, and similarly, adding another layer in between has corresponding external/internal interactions, to provide a different external interaction with the original package and a different internal interaction with the substance)
            - similarly, other variants of 'isolate' include 'wrapping the problem structure', 'prevent the problem structure from interacting with other structures by binding to its interactive structures', 'deactivating the problem structure', 'dispersing/decaying the problem structure', 'change the problem structure', 'limit the problem structure', and other known problem-solving functions which are useful variants of the 'isolate the problem' intent
            - relatedly, various core structures can be applied to solve this problem of 'isolating the problem structure', such as stack/interim/connection structures, as they'll have similar impact in solving the problem to find this specific solution of applying another structure in between the original structures, so applying one structure in each of these similar/equivalent alternate sets (like the set of 'stack/interim/connection structures' as one set in a set of sets that contain similar structures in the set and different structures across sets) is a useful way to test multiple maximally different structures without checking all structures
        - similarly, as 'usage' is a known useful structure associated with functions, interface variants of it (like 'users' and 'usefulness') are similarly useful in core functions like describe/determine/generate, so that these alternate structures like 'usefulness' can be applied to identify other problem-solving intents/structures
            - for example, 'making a problem less useful' (for whatever benefits from the problem state/structure) is similarly another problem-solving intent, as problematic structures often exist bc theyre useful for some intent/agent, so 'finding alternate similarly optimal structures for the agents benefitting from the problem structures' or 'making the problem higher cost for the agent, so they find another structure' is a related variant of that intent (like 'finding alternate ways to help make money for criminals, so they dont have a reason to commit crimes')

    - identifying useful structures that can be connected to derive other structures, like rules about structures like probabilities/defaults/biases which can be extrapolated to identify other rules connecting other info structures, where these structures can be used to derive a high ratio of other structures if found (perspectives/biases can be used to identify most other variables trivially, so connecting them to probabilities and other structures like defaults is useful)
        - for example, rules like 'the first guess is rarely correct' is related to insights like reasons such as 'bc most positions apply incorrect biases by default' and 'biased positions are more common than correct/more optimal positions', which can be derived from the rules about structures (like sequences/positions) of info structures (like guesses/hypothesis/predictions) by extrapolating the rule to interface structures (like agent perspective and related defaults/biases of that perspective and the relative position of that perspective to other perspectives and variables of perspectives like optimality) by applying defined changes of the structures in the rules, to identify useful structures like 'probability of optimality of a perspective/bias set' and identify 'perspectives to check, as equivalently probable'

    - identifying useful structures like 'spectrum variable configurations of reality (which can be formatted as intersection or adjacent/interactive points of conceptual variables)' and other useful structures like 'cross-interface structures' which are useful to connect to identify other useful structures
        - for example, the fact that 'reality tends to be complicated' (which is a point leaning toward complexity on the simplicity vs. complexity spectrum variable) can explain why other structures are useful, such as that a 'cross-interface structure' is useful bc it covers more complex cases and covers more information than one variable, as its difficult to explain all of reality with one variable (for example, its useful to know function/intent/usage and its useful to know 'if a person has brain damage, while evaluating their performance on a test' and its useful to know 'if someone read about communism or came up with it independently' rather than just one of those structures, as reality is not simple enough to describe with one variable most of the time, so this simplicity/complexity configuration reflects the reason why other structures are useful)
        - the point of this and why its useful is that one (the 'imbalance toward complexity of the simplicity/complexity variable') can be identified from the other (the usefulness of 'cross-interface' structures) as they reflect the information contained in the other
        - similarly, matching an insight like the 'certainty/uncertainty' structure as a tool for problem-solving ('find a certainty to use as a base, then apply changes to that base to find a more optimal structure') with a graph is a useful set of structures to find bc applying various insights as certainties determines possible graph structures, which can then be usefully varied to resolve the remaining uncertainties, such as how applying certainties like 'interactions are best modeled with graphs' and 'some optimal interactions are known in graph format, such as surrounding every mad person with people theyre unlikely to be mad at' is more possible once a graph is found that allows these interactions to be accurately modeled and applied, such as a state sequence of a 'graph of interactions' to switch the position of every mad person to be surrounded with non-mad people (you know that you want to find that specific more optimal interaction type, you know that interactions can be modeled with graphs, such as where every node is an individual agent in a system, and you know some interactions increase the madness of people directed at specific individuals, so you can find a graph format/type within the limits created by those applied certainties, that fulfills various useful functions and allows useful variables to vary rather than applying them as certainties of the graph), as a way of organizing groups to avoid/create various interaction types, such as justice/peace graphs to determine who should be held accountable in order to achieve a peaceful state, team-selecting graphs to avoid or maximize arguments, etc
        - this means the 'certainty/uncertainty' variable is useful to apply with the 'standard/base-change' problem-solving structure (associated with the 'change a base solution' workflow) to find useful structures (like common structures such as graphs/networks) associated with those certainties and the uncertainties resolved with those structures (graphs)
        - similarly, the other interface variables ('abstract/specific', 'similar/different') can be applied in a similar way, to resolve core problem-solving workflow functions/structures (such as how 'similarities/differences' are useful for the 'connect problem/solution structures' workflow, and the 'abstract/specific' structures are particularly useful for the 'generate/filter' workflow)
        - relatedly, one item in these spectrum variables is often usable to solve problems (using only abstractions vs. using both abstractions/specifications) but using both sides of the spectrum is more optimal in many cases, like where a spectrum like 'unit/extreme' (or relatedly 'finite/infinite') is useful to identify interface structures like 'limits' of a particular function observed in a unit/simple/definitive case by applying both the unit/extreme rather than either variable
        - similarly, identifying errors of thinking such as 'the most recently heard point often convinces people' (the recency and simplicity bias) reveals other info, like that its important to be able to consider multiple points at once to resolve a complex problem, and thinking about one point at a time will just make it seem true, so multiple structures (a network of many nodes and many networks and many connections between networks, etc) are better than unit/simpler structures to handle this error
        - similarly, the 'first loud/confident point heard' often convinces people more than other points (bias toward certainty structures like confident, bias toward simple structures like 'first'), which reveals other info, like that an algorithm that identifies possible uncertainties or ways that some point could be/become incorrect is more valuable than an algorithm that states it has certainty about some point, and other info such as that 'irrelevant structures to meaning' (such as position in a sequence, like the 'first' point in an argument) may be used as proxies of thought, but can be disregarded/filtered out as possible structures of truth bc of their irrelevance (the 'firstness' of a point doesnt create 'correctness' of the point, so structures like 'firstness' can be disregarded as structures of relevance/meaning, even if theyre often misused as relevance/meaning through errors of bias like the simplicty bias)
        - relatedly, the 'determining' variables are maximally differentiating variables, which are useful in that they can act like filters, or alternates of sequential structures like input/output sequences or other interactive structures, since 'determining' variables have an impact of differentiating related structures (such as making other structures required or associated), so these can be used to determine subsets of the space in between problem/solution structures (where rather than finding connections or input/output sequences, find highly determining variables and see which of them apply or are useful or see how to connect those)

    - identifying useful structures like optimization structures which are adjacent to other useful structures like optimization structures so they are useful for identifying new useful structures
        - for example, an optimization structure in the 'driving/navigation' problem space could take the form of 'taking advantage of the first green light at a required turn direction (like the first green left turn, if a left turn is required at some point and left turns will have a similar effect on removing the left turn requirement, no matter at what position theyre applied)', which is an optimization bc of the improbability that any other left turn will have a similarly zero wait-time associated with it, which is a cost structure of most types of turn, and bc the effects are the same as another left turn and it fulfills a requirement for a left turn
        - so identifying 'equivalent alternates' (equivalent left turns that will not change the route too much or which have similar effects on the route and/or requirements) and 'improbabilities of finding lower cost required structures' are useful structures to identify in this and other problem spaces, the 'improbability of a lower cost required structure' being a new optimization structure that is trivially identified from this optimization structure of a 'more optimal route' (especially useful when connected to the case of the more optimal route, as its improbable bc of the 'case of a few more possible left turns, in which a better/lower/equivalent cost structure is not likely, except at prohibited high speeds', and these cases with more optimal routes can be identified automatically, and improbabilities of more optimal routes than a specific optimal route in that case can be determined with these cases, and that is useful bc these determinable cases offer structure around which its useful to identify interface structures related to those cases and then its possible to connect cases with other cases as well as other useful structures like probabilities/optimizations in certain cases as well as general case-invariant optimizations)
        - similarly, 'useful structures' are often 'adjacent combinations of existing structures' bc existing structures reflect the information that if it exists, it must be useful to some agent and it must be similar to what agents have intent to build/find, otherwise it likely wouldnt exist, and therefore many useful structures are adjacent to these existing (and therefore preferred) structures

    - identify useful symmetries of information that can be applied to find 'rules to find information' such as 'find recent information, as its likely to still be true, to some degree or in some ways/cases/structures, bc of the "temporary time symmetry of information"', where 'recent information' is a proxy for 'current information', although this is an imperfect approximation of current information as theyre not required to be equivalent but rather often have similarities/symmetries maintaining some of the information in some way/degree/structure/case
        - for example, there is a 'temporary time symmetry of information', where past information is often still 'measurable in and connectible to the present', bc the original past state of the information is maintained to some degree or in some way, as the information may not have completely decayed yet, so this symmetry can be applied to identify present/future truths to some degree or in some ways or in some cases, where past information is reflected across this time symmetry to the present, even if incompletely or approximately rather than in its original form

    - identify proof structures (and related structures like opposites of it, such as non-provable structures) as useful structures to apply in functions that determine facts or probabilities, which are adjacent to problem-solving structures in general, as facts can be used to fulfill problem-solving intents like 'connect problem/solution structures'
        - for example, structures that determine non-provable structures include differences from provable/existing structures, such as differences from replicable/measurable/existing/scalable structures (non-provable structures are structures beyond current scalable limits of computation)
        - similarly, 'differences from the fact to be proved' determine non-provable structures, such as differences from its inputs, barriers of non-computability surrounding it, differences from the fact (contradictions, incentives to contradict it, relative commonness/measurability of contradictions of the fact as opposed to the fact, triviality of faking contradictions of the fact, etc), differences from existing concepts/structures to understand/determine it, etc, where if too many of these differences from the fact exist, the fact can never be proved

    - identify useful structures like 'inputs to useful structures such as filters' like the examples/problems that trigger identifying useful filters and the resulting realizations of applying those filters in those problems and the useful connections like maximal differences made trivial by those filters and the reasons for the usefulness of those filters (like that 'combinations of some structures can create reality-covering variables', so these combinations are useful to identify) to identify other useful structures (like the 'combinations of structures which create reality-covering variables')
        - for example, the function 'realize' is a matter of identifying a structure related to the newly identified structure, created from adjacent changes applied to existing known structures (or more rarely, adjacent changes applied at scale to approach the realization from a new direction, or non-adjacent changes like interface structures), so first 'finding the structures that can adjacently be connected to the new structure and then applying the right changes to connect it' are both functions that form a realization, which is nontrivial in that it requires focusing on some relevant subset, then forming relevant connections to some new structure, so 'realize' is like 'applying structure to an adjacent degree from some starting point on the consolidated function diagram or other interface diagram'
        - just like 'knowing that people lie and focusing on that will make it trivial to realize facts like when someone is lying' and 'knowing that its possible to be good will make it trivial to realize facts like when someone is good' and generating both of these by applying opposites is required to realize either of these facts
        - also relatedly just like how knowing that graphs have different similarity/position metrics makes it trivial to identify whether a new graph is actually new, as this fits the definition of a 'new example graph' with the definitions of existing/known graphs in a trivial way, making it trivial to realize whether a graph is new)
        - these mental functions of intelligence/thinking are useful as outer layers of the consolidated function diagram (with inner layers like core functions such as find/build and outer layers like useful high variation functions like organize/implement), as functions that can be built with the inner layers
        - connecting these mental functions of intelligence/thinking to 'structure' using the interface structures defined in that structure interface and often some other interface structure like a function diagram is required in order to identify useful implementations of the function ('realize' is usefully implemented when formatted with interface structures)
        - similar to how 'realizing the idea of capitalism (without already knowing it) while working as a checkout clerk' is not trivial, 'realizing interface analysis while working in software engineering' is not trivial, but some perspective make these realizations trivial, especially 'formatting new structures in the structure interface', which is a useful step in the 'realize' function (just like 'identifying maximally different filters (like good/evil) to generate different starting points to focus on' is a useful first step)
        - similarly, 'sets of graphs/functions like cross-interface or cross-perspective structures that cover reality when combined/changed/otherwise interactive' are useful alternates to reality-covering variables (such as power/balance/truth) which are maximally useful on their own, so these 'combinable/interactive sets' are useful to identify as 'positions to apply variables' or 'useful filters of solutions to check or solution-finding methods to apply'
        - reality-covering attributes are useful to identify (and build with other attributes) bc you technically only need one to solve all problems (these reality-covering attributes are the bases of the primary interfaces), similarly its useful to identify approximations of these ('incentives' could be an approximation of a reality-covering attribute even if it doesnt cover every last point of reality, it covers most points, as there are agents in or adjacent to/within accessible distance of most points and incentives exist when there are agents)

    - identify common variables of useful structures that can be used to identify useful interface structure interactions, which can be used with higher priority in interface queries (to filter probably useful interface queries)
        - for example, how 'standardize, then compare' and 'sort, then filter' and 'generate, then filter' and 'understand, then explain' and 'organize, then understand' and 'identify example solution, then change/optimize it' and 'define, then connect/differentiate' and 'classify/sort/separate, then combine/mix/vary' are examples of 'coordinating useful interface structure sets' that have a 'optimal sequence' structure and where these sub-structures have specific useful interactions, like how some specific sorts are useful for some filters, to apply as a filter of interface queries that are likelier to be useful, so identifying structures with 'identifiable optimal structures like sequences' is useful for solving the 'filtering' problem in the 'input/output sequence' format to fulfill the 'connect problem/solution' workflow
        - similarly, specific variants of these functions are useful, such as 'identify the unit/component "case/variable/error" that scales to create the problem and solve the unit case/variable/error' and 'identify common combinations of problems/workflows/formats and identify useful structures that solve for that combination', which are general useful problem-solving intents
        - similarly, identify structures that are not useful to differentiate from when filtering interface queries and functions which often decrease the value of an interface query when applied, such as tautologies ('falsely framing an equivalence as a difference') or 'randomly removing a sub-function or randomly adding variables/structures outside of the defined variables/structures'
            - similarly, a simple function like 'list examples of a type' is not likely to be useful in isolation, usually a mix of structures like this is required bc usually one function is not high-variation enough to solve a problem on its own, with rare exceptions like organize/understand, which are not adjacently coded and which dont completely solve most problems in their probably incomplete probable implementations given the 'adjacent combination-prioritizing' mind likely to implement it
            - this is bc a simple function is different from the general requirements of a solution structure (often requiring complexity or high variation in some other format)
        - to identify this insight, apply the workflow 'identify useful structures of useful structures' (identify 'common variables like sequences of useful functions in a set of opposing useful functions and the reasons for the usefulness such as that they apply the 'differentiate a similarity' workflow or capture high variation or are required to be useful when applied in that structure, given their interaction and input/output requirements) and then apply useful functions that frequently solve problems like 'identify maximally different examples of a type (to identify the extremes/limits of variation within a type and identify variables of variants within a type)'
        - identifying function limits is similarly useful as identifying the unit case that scales to explain a variable interaction (if there is one, which there usually is), such as how identifying that 'combining the same 10 integers (to create integer sequences, as opposed to combining sequences of digits of size greater than one, such as by randomly varying between randomly distributed sequences)' is useful for identifying a high ratio of random sequences up to a finite sequence length, but not infinitely, as there is a limit on the number of infinite random sequences that a particular infinite random sequence can contain, but sequences of lower length are likelier to be covered by any given infinite random sequence, and identifying this finite limit is useful, just like identifying that the set of 10 digits can be re-combined to generate most other random sequences is useful (but not infinitely bc once an 'infinite random sequence' starts to be generated, other infinite random sequences are not possible, down to the finite limit), similarly identifying the 'unit components that scale to create extremes' and the 'interim structures of extremes' is useful in other problems (interim structures such as the 'finite limit on a specific sequence length that is required/possible to be contained in a particular infinite sequence' and the 'variation in probability distributions of sub-sequences of unit components/digits that are found across sub-sequences, these variable probability distributions being equally probable'), as extreme differences are likelier to be a problem that needs to be resolved, so 'identifying interim structures between extremes' is generally useful

    - identify useful structures like system contexts (like cases where the info structure is more useful) that coordinate with info structures (like unused variable interactions) to optimize a neural network
        - for example, some variable interaction that deviates from the standard neural network structure of an 'input/output sequence applying an adjacent change combination' is where an output could be useful as an input earlier in the sequence, but that info isnt applied in a standard neural network architecture/algorithm, so 'allowing earlier computations to skip ahead and use later outputs as inputs' is a possibly useful function in systems with enforced boundaries that are likely to force a high ratio of possible variable interactions to occur bc of these extreme limits (this is useful during later iterations of training, after likelier weights are likely to already be found, so that these likely true later weights can be used as inputs, similar to how the 'system state' where a variable interaction occurs is also an input to every function while being changed by that function)
        - similarly, keeping variants of a neural network that were 'almost correct' or 'nodes that were just barely deactivated' could be useful bc these are likelier than not to be useful for some other intent as they are probably correct in some other way than for predicting this particular variable interaction (as in a probable variable interaction that could explain some other connection in the host system) and are likelier than not to be variants of the solution function

    - identify insights which indicate useful structures like simplicities/opposites/errors/differences which are easily converted into or connected with some other useful insight
        - for example, an insight that indicates an opposite or error like 'correlation does not mean causation', which is easily converted into another insight ('but then what numbers do build or map to structures of cause or cause itself, bc there is a connection between cause and numbers, even if there is high variation within that connection?')
            - numbers that indicate precedence/sequence (like numbers in a pattern), numbers that indicate differences from other useful structures like concepts (different from random, unit, etc), numbers that indicate organization/randomness/units/types/other interface structures like other concepts (also indicating patterns) numbers that are unique (building a requirement, which is one input/component of causation), numbers that are useful (such as 0, 1, pi, e, 6, etc which are more causative as they are more useful than other numbers), numbers that indicate other useful structures like embeddings such as embedded variables like variable exponents, etc
            - similarly, numbers which are continuously connectible, adjacent in position, etc are likelier to be causative of numbers on that continuity or adjacent to that number in some space or set/network of spaces, just like numbers of the same type are likelier than other numbers to cause numbers of that type, given the frequency/usefulness of type-preservation functions
        - in general all numbers are causative (and otherwise also fulfill the other primary interface and reality-covering concepts), but some are more powerful, required, unique, useful, organizing, or otherwise causative than others, as some numbers are more easily used to implement a concept or make a useful function adjacent or otherwise more easily connected to some interface structure (in general and also specifically to causal structures like causal components/inputs), and interface structures in general are also adjacently connectible to cause
        - numbers are 'structures that can describe/measure/track some example/application of some interface structure, and can be re-used across examples/applications of other interface structures, to act as placeholders for interface structures to indicate the relative position between interface structure examples/applications as well as the structures themselves, as an alternate set of structures that can describe all changes/structures/concepts, and indicate spaces between interface structures, when no interface structures can be used to describe some number bc its not completely understood yet, bc some number represents a new difference/variable not fully connected/fit to interface structures yet'
            - interface structures, by comparison, are often stacks/networks of numbers, which become so variable through supporting high variation that they seem to not be easily mapped to numbers at all
        - neural networks apply concepts like 'causal input/component' through embedding that concept in the structure implicitly, rather than explicitly giving these interface structure definitions to a neural network as inputs or functions it can use in its own processing
        - therefore neural networks rarely create any interface structures that are not trivially composable with adjacent/incremental iterated change combinations (such as maps/indexes/networks, rather than efficiently and optimally building concepts like balance/truth/certainty/cause)
        - similarly, the fact that 'correlation is not causation' can be applied to generate other useful structures, such as generating possible solution sets by generating undirected variable interaction structures rather than keeping tha variable value constant

    - identify useful structures like common inputs to useful structures, such as info that (once known) is useful to identify other useful structures in multiple ways, such as knowing 'negative states/processes' which is useful for adjacently identifying multiple other useful structures (changes to negative states, causes of negative states, opposites of causes of negative states, opposites of negative states, etc)
        - for example, useful structures in the 'find a medicine' problem space include 'opposites of causes of negative states (errors)', which requires knowing 'what the negative states are (inflammation, oxidation)' and 'causes of those negative states (reactive oxygen species)' and 'opposites of causes of those negative states (anti-inflammatory substances, antioxidants)'
            - without the info about the negative states, causes, and opposites, its not trivial to identify antioxidants as useful, but with that info, it is trivial
            - identifying oxidation as an important variable/function to negate is a matter of identifying variables which can cause high variation (such as causing damage/change to a high ratio of compounds or highly binding compounds)
        - similarly, identifying that 'sweetness/bitterness' as a useful variable is trivial once the info about the 'connection between sweetness and cell growth' is known and once its known that 'unrestricted cell growth is an error', so that the 'opposite of the error-cause' (bitterness) can be applied as a possibly useful variable of solution structures (useful compounds are often bitter, indicating a possible difference from growth-promoting compounds like sugar)
        - without this info about the 'opposite of the error cause', what else could have identified 'bitter compounds' or 'antioxidants' as possibly useful?
            - structures like defaults and overlaps and limits on types are useful in determining some structures in this problem space 
                - overlap: useful and harmful compounds overlap in some attributes like sweetness/bitterness, even though the ratio of bitter compounds compared to all useful compounds favors bitterness over sweetness, and similarly other simple interface structures (like extremes) are often harmful in general
                - type limits: there are often clearly extreme effects of most compounds, as its rare to have a neutral compound with no noticeable effects on a system, so its usually possible to sort compounds into either useful/harmful, as theyre usually clearly useful or harmful, and the boundaries of these types should be possible to find (boundaries which are often dose-dependent as there are often multiple modalities and phases and conditions of usefulness in various contexts, rather than one simple type per compound)
                    - given that there are multiple ways for a compound to be both useful/harmful, finding the 'networks of usefulness/harmfulness of a compound' (including the 'paths to usefulness' and the 'paths to harmfulness' and the 'connections between harmful and useful paths/positions' and the 'default/common/final/static position on these paths, given common processes/usages/interactions/states') is likelier to be a useful way to describe a compound than by its structures in isolation of other variables
                - opposites/inputs/causes of common/default/high variation attributes/processes: oxidation is like methylation in that its a core process in bio systems and is therefore important by default, in addition to also creating high variation in the system, so finding substances that trigger it or counter it is useful by default
                - 'causes of the opposite of the error/negative state' (causes of 'negation/destruction/neutralization' of a negative compound/state like 'compounds that destroy alcohol/aldehyde') are similarly useful to identify as 'opposites of the error cause'
                - static-triggering/over-limiting states which regulate/prevent other types of changes or prevent changes in a direction of usefulness or limit variability that is possibly useful or at least not definitely negative in other cases (like strong bonds that prevent breaking down a harmful substance, calcifications, fibrosis, tumors, blockages, clots, ph/electrolyte disruption cascades/triggers/compoundings, etc) are similarly generally useful to identify and differentiate from useful regulating/limiting structures/processes (like anti-inflammatory structures), as its unlikely to be useful to be trapped in any given particular state, as optimal states are rare in bio systems, so cascades/traps/triggers leading to static states are likelier to be harmful than useful
                - structures that change a harmful structure at all (but also in a specifically useful direction, like providing an electron to common negative compounds to correct blood ph or change a compound to a less harmful compound, like antioxidants do) are similarly useful to identify, once harmful structures are known
                - common structures like benzene rings are often found in other compounds in a harmless position (when surrounded by other structures, these are not always as harmful as they have the potential to be in other contexts), so these common structures arent necessarily a clear indication of usefulness/harmfulness and the interactive sub-structures on the surface can be more useful/harmful than the base structures in the center, so much that these can sometimes be ignored, unless the outer structures are often removed/destroyed/changed to make those base structures possibly interactive
                - useful structures like anti-inflammatory substances are often high variation in their sub-structures (and often have other attributes/compounds in common like forming structures which could be highly interactive with harmful structures, like structures that can standardize a surface of a compound by binding with/trapping structures likely to be on its surface, which could be useful for anti-inflammatory substances) but are also extremely high variation within the class of useful compounds, which is predictable given the variability of the set of all compounds of a certain size
                    - similarly, the volatility of the useful/harmful types is similarly predictable with its structure in isolation of other variables, as trivial changes can change a neutral substance into a harmful or useful substance, so structure on its own (in absence of insights about harmful processes like oxidation) is not as useful as structure with information about interaction/harmful/other types of functions
                - similarly, co-occurring processes/attributes ('bitterness' occurring in 'surviving/successful' systems to protect itself from being over-eaten out of existence) is an indication of a bio-system that can adapt to the environment, this variation being likely to be useful in other ways given the priority of 'efficiency' that can be implemented with structures like 'multi-functional attributes' (such as having antimicrobial attributes or regulating/limiting effects associated with the bitterness)
                - input/output sequences or interactivities can also be useful, such as by knowing whether a compound is similar to existing bio-system structures (like whether a compound mimics a bio-system receptor), bc if a compound is replacing, preventing, or over-using functionality of the bio-system, that would likely already be known bc it would have extremely noticeable or negative effects
                - similarly, 'bitterness' is an attribute of common/core bio-system compounds (like amino acids), so its not an indication of an error structure to be avoided at all costs in all cases

    - identify useful structures like 'interface structures that can be applied as similarity indexes' which can connect other structures that are useful to connect (like concepts and problem formats) and variables of them (like 'information retained in the similarity structure', such as proximity/position/distance/etc)
        - for example, structures like sets (to identify co-occurring structures), intersections (to identify functions with structures like points in common and other useful structures like ratios of intersections), adjacencies (to identify structures that will probably change into each other bc of their proximity in some similarity metric), alternates (to identify ambiguities to resolve, and equivalent alternates, and useful selection functions), overlaps (to identify structures like points that can be used to switch to other paths), identifiers (like 'moments' which are unique across similar functions as a way of determining function differences), similarities (to form similarity indexes to find similar functions and skip computations), & concepts (to easily decompose a complex system into a few structures) which form an alternative to more structural structures (like 'input/output sequences' or 'patterns') to determine 'interactivities' as useful structures of 'probability' (all having the same structure of similarity/difference uniting them which determines some probabilities), as alternates to apply as expansions of false conflicts like 'frequentist vs. bayesian', as a way of 'determining the next node in the sequence/network' without using simple standard input/output sequences to fulfill core interaction functions like 'connect problem/solution' to identify other useful structures that can be applied as alternates of sequences
        - applying interface structures to the problem of 'connect some problem/solution structure' to fulfill a sub-intent like 'determine the next node in the sequence/network (of possible connecting paths)' is particularly useful bc of the standardized simple format and is likely to find other useful structures adjacently, which makes it a good 'interface problem' to host other problems to solve other problems in that problem, similar to how 'find a regression function' is a useful simple standard problem format that can host most other problems, as both apply the fewest biases requiring non-standard assumptions/conditions that might make it too specific to apply to other problems
        - finding the connection between 'determine the next node in the sequence/network' and 'probability' is useful to identify, as these 'similarity' structures (like intersections or adjacencies) are related to both the problem (of determining the next node) and the concept of probability
        - finding these cross-interface connections (these 'similarity indexes') between these primary interface 'concepts' and 'problems which solve for or fulfill that concept when other problems are standardized to that problem' is useful to identify other problem formats that can solve for another concept, and how to connect those with different similarity indexes
        - similarly, identifying the ways that other interface structures like 'information' and 'change' can act like 'probability' (since probability is a reality-covering concept), as information acts like a 'temporary determinant of probability, since information is often temporary' and 'change structures like degrees of change and change rate are directly related to probability such as by determining other structures that determine probability such as adjacencies/potential change', and how probability can be converted into these interface structures to form a useful queryable network with these structures
        - similarly, identifying all the independent sets of structures on a particular interface like the math interface that can determine/describe probability, such as the structure of 'variance' such as applying interface structures like 'change' and 'group' in a new way to identify 'changes across groups vs. changes within groups' or 'range/distribution of groups of changes'
        - this set of a 'false similarity' error ('correlation is not causation'), a 'dichotomy' (specification vs. generalization), and an 'equivalent alternate' set ('frequentist vs bayesian') form an almost complete set of variables that can describe many statistics variables/problems, where other problems can be adjacently derived from this set, such as how 'correlation is not causation' is an error of the false similarity of an input and a similarity where there is an implied requirement that is not reflected in real systems (a similar change structure may occur even if one input is not an input to the other change, which means other interface structures with possible false similarities through these implied but not required rules can be found as alternative statistics problems to find and resolve) - whether such a set (an error, a dichotomy, and a set of equivalent alternates) is useful for describing and deriving most complex systems is useful to identify
            - similar to how a 'change component limit' (activation), 'change unit' structure (weight indicating relative importance/usefulness of a change, given other surrounding changes), 'change change identification' structure (partial derivative), 'useful change identification' structure (error function), 'useful change limit' structure (change halting function), and 'core combination' structures ('multiple' and 'sequence' and 'overlaps' and 'direction' applied to 'probability units' to form a 'network of probability units') form a good description of a neural network, which make other useful network configurations clear, such as 'changing weight position' rather than 'changing weight value', to identify 'useful sets and sequences of weights that are repeatedly useful across intents or which can be identified as useful specializations to fulfill a specific intent, justifying its separation and also its generalization by connecting it to other specialized intents'
            - for example, 'core centralized/starting nodes are more powerful through adjacently generating higher variation of functions such as by including maximally different functions as core/starting nodes (or adjacent components or other high ratio-covering inputs of maximally different functions)' is a useful network organization function to 'quickly find higher-variation variables determining most variation in a solution function' and 'sort changes to attribute to a subsection of the network' (as well as "allocating more 'trivial change-producing' variables to the later low-variation/specification remainder of the network to stay within a pre-determined error range"), with built-in assumptions such as that any high variation function could exist and all useful high variation variables are known and can generate all possible high variation functions (like different polynomial sub-types and different function types having different configurations of discreteness/linearity and other function attributes)
            - similarly, a network organized to identify a 'function similarity index that a solution function is likely/required to exist on' first, and then sorting that index to be most useful for iterating in order/some other pattern (such as a sort that makes any iteration of the sorted index useful for some likely/common intent), and then navigating that index with that navigation function in the later nodes or later training/update iterations is a useful way to organize/apply a network
            - this is like how 'generate possible solutions (being similar in that they fulfill some more obvious solution metric)/filter (to identify solutions fulfilling some more complex solution metric)' and 'identify similarities/differences to connect useful structures (identified as similar in their usefulness so they must have other similarities like variables in common)' and 'connect structures adjacent to problem/solution (similar structures to different structures, having a similar difference that is more solvable/connectible/similar) rather than the original problem/solution (different structures)' and 'standardize/similarize then differentiate within that standard' and 'identify similarity, then sort to identify useful differences for optimal navigation' and 'organize similarities/differences so all similarities in differences and differences in similarities are adjacent on the structure' and 'change a base solution (already similar to the optimal solution)' are similar in that they apply a similarity such as a standard/interface, then identify similarities/differences in or connected to that similarity, bc its common to be able to identify a primary/powerful similarity and then need to identify specifications/variants of that similarity when solving problems, just like its common to need to 'resolve multiple ambiguously different alternates into one most optimal solution' or 'obviating differences' once similar alternates are identified as possible solutions, as these structures are all variants of each other, as similarities are useful in that they fulfill some base structure that is known to be useful (they solve some base problem of identification/filtering, in identifying that similarity, so that the structure can be re-used for other/future intents), which can be changed trivially to identify other useful structures that are variants of it and also identify structures that are different, which are core problem-solving intents
                - similarly, its common to be able to identify a 'difference required to connect in order to solve some problem', as the difference between the problem/solution is usually clear
            - pairing these 'assumptions/errors' and 'network organizations' (implementing various known useful structures to fulfill known useful problem-solving intents in the 'regression' problem space) is useful as a way of identifying useful network structures for unresolved problems in a particular workflow and similarly for identifying generally useful network organizations that fulfill a high ratio of problem-solving intents
        - similarly, identifying alternates to 'similarity' such as 'importance' (such as how a structure is more important if its a requirement or an input to multiple functions or a different structure or a stable structure or a common structure), which overlaps with similarity in its component/input/variant structures, as structures which are similar are important/relevant to each other in multiple ways (as similar structures may be alternates of each other, may be errors of each other, may be connected to each other, etc), as 'importance' structures also indicate probability in various ways (such as 'difference from random' and through 'frequency/commonness'), and similar rules apply for 'usefulness/interactivity/variation'
        - similarly, other examples of alternate structures that can determine probability or can be applied in its place include other structures of certainty, other reality-covering attributes, other interface structures like requirements and other structures that occur often for a reason like 'incentives' (such as biases which are incentivized structures)
            - 'stability' as a determining variable of the 'next node in the sequence/network' (and components of stability/independence/certainty/requirement, like self-sustaining processes, iterative/scaling processes that cause other determined processes like cascades)
            - 'variation/information requirements' as determining attributes of variation/information, such as determining where the remaining variation/information is stored (in a different/new path or node than those already determined, or in errors like noise), such as 'incremental changes required in a sequence'
                - otherwise applying specific solution format requirements like 'making sure each weight set can have noticeably different results (being sensitive to inputs)' to the network parameters like weights applied as a whole
            - 'biases/incentives' as determining variables of the 'next node in the sequence/network'
            - 'interface structures' such as 'false similarity errors', which indicate some weight subset of the network should be falsely similar to some other subset if the network is going to reflect reality and real variable interactions (making sure the network has each interface structure, whether embedded on some interaction level or represented in a default structure like a node or node subset)
            - applying 'components' of connections like maps/sequences/patterns to fulfill the 'connect' function for some input/output points to connect
            - interface-connection structures like probability-information interactions such as how probability becomes information when a difference is clear (when some structure is clearly not random, bc its difference from randomness is clear) and how an ambiguous structure becomes information (when a selection is made, differentiating it from another alternate, resolving multiple alternates into one structure) which can be used to connect multiple different structures (like probability structures such as weights and info structures like indexes/maps and logic structures like requirements) used to 'predict the next node in the sequence/network' to cover gaps that other structures cant fulfill
        - connecting all core/primary interface structures to one reality-covering concept (such as probability) is useful bc then all problems can be formatted to be problems in that problem space (a problem of determining probability of success of a solution, probability of various alternates, etc)
        - why is 'probability' useful to find all the interface structures of (such as components/variants of probability)? 
            - bc it is a useful structure for solving the core 'filter' problem, to resolve the ambiguity of multiple alternates, by weighting one path as more probable than another, and therefore 'finding probable paths to connect structures (like connecting a problem/solution)' is solved once these probabilities/weights are identified
            - similarly, 'connection' structures of 'important' structures (like how differences/variables are important, which can also represent filters/errors/connections/similarities) so that structures like a 'network of particularly important (as in maximal) structures of importance (like differences)' is particularly useful for solving problems with queries on that network
                - relatedly, core interface structures like 'networks' with other core interface structures like 'sequences' applied (such as a structure like a 'network' sorted in a sequence of a reality-covering concept like 'importance', like a 'decision tree') are useful for implementing other structures that can be used to solve all problems bc of the fundamental aspect of the structures applied

    - identify useful structures like 'unit/core problem formats' to fulfill intents like 'find variables of problem formats' and 'find connections between problem formats' (to make these formats more useful in a network, at which point interface queries can use that network)
        - identify core problem formats like 'ambiguity resolutions' (filter a set of alternates) and 'scaling' (applied in its 'opposite' structure, to find the 'most compressing units/inputs (such as a useful high variation interface/standards) of an iterated or otherwise complex structure' as opposed to the 'most efficient iterated structures') as alternate problems which are useful to solve in order to solve other problems, as they involve core formats of problems and are useful when they are applied with other interface structures, like in their reverse, with other workflows like reverse-engineering or to fulfill problem-solving intents like 'connect opposites/extremes (so that connecting trivial differences is already solved or easier)'
        - applying variables to these core problem formats is useful, such as how another useful problem format is the problem of 'finding a way for alternates to all be useful or all coordinate (to avoid solving the filtering problem)'
        - identify the connections between problem formats is useful to find useful interface queries for intents like 'filter which problem to solve' and 'find a useful problem format given these inputs'
        - similarly, 'making some structure like "problem formats" useful by finding the network connecting them so that network can be queried' is like identifying 'interaction levels' of 'equivalent alternates' so that these can be applied as 'random enhancers of algorithms' (as any structure in the interaction level is likely to be useful, even if selected and applied at random, similar to how applying any function out of the set of 'find/build/derive/apply' is likely to be useful)

    - identify structures in common across useful structures such as 'neural networks' which can identify 'adjacent combinations/variations' (like interactions between workflows) that are similarly useful as the first example found
        - for example, the 'neural network' structure is a result of applying 'change a base solution' with 'trial and error', to feed the base solution as an input to 'trial and error' to find the next solution to try, based on an update function applied to the previous solution tried which was suboptimal, using some function to determine a halting point (such as the first found local minima), after applying the 'reverse-engineer' workflow to identify the changes responsible for a change in the error
        - 'trial and error' may be a higher-variation workflow (and therefore usable as a base for other workflows, more so than another workflow) in that it tries every possible solution in its default form, and in that it allows various points for variables to be applied, such as to 'generate solutions' (build), 'filter solutions' (filter), 'sort solutions to try more probable solutions first' (sequence), 'halt testing solutions' (limit), and therefore interacts with most if not all core interface structures in its most complete/optimal form
        - 'trial and error' is useful in problems bc it allows for a set to be reduced to a subset, which is useful when multiple possible solutions are known and the uncertainty to resolve is 'finding a selection of a subset of those alternates'
        - the 'neural network' applies this set-reducing workflow ('set-reducing/filtering' being the primary function of many workflows in their default format) of 'trial and error' in multiple places to resolve multiple uncertainties - one to assign default weights, one to try many different change combinations (either by applying many nodes or using another structure like a variable interaction function like 'adjacent input convolutions') & one to test whether a final weight result should be changed in some way
        - 'trial and error' also has clear optimizations in the 'filter' function variable position of the workflow (which is optional) to filter the solution set, like finding solutions with overlaps, finding generative functions of solutions with overlaps, finding solutions with similar results or similar components/inputs, finding areas/other patterns of solution similarity, solutions optimized for some input case attribute, etc
        - from this it can be inferred that most useful inventions apply some 'interaction between workflows', where the workflows solve different problems to solve the primary problem motivating the invention, and its usually different workflows that are combined, rather than iterations of the same workflow, but iterations of the same workflow can also be useful (like when alternates are ambiguously useful and should all be tried or when theyve been filtered as much as known certainties allow), similar to how 'trial and error' is applied in multiple positions in the standard neural network bc of its focus on computation rather than efficiency
        - then after this initial version which solves multiple problems is found, this initial solution version formed by these workflow interactions is often tuned to account for a new structure that can be applied (such as an insight) to solve some problem found during testing (such as 'random dropout' helping with 'generalization' to solve the 'over-specification' problem ("randomness" again being related to "trial and error"), which adds structure to the network to format that certainty (that an 'average of subset lines, with subsets created by all adjacent combinations of size, as in one less than the full set' is a useful representation of the 'full set line' in the 'regression' problem space) and integrate it in the information input/output sequence) in a structure like 'create subsets (like with "random dropout") to fulfill intents like "generalize"', which can be predicted by connecting that initial solution version with reality-covering attributes (like generalization, efficiency, possibility, etc) by applying functions such as 'random filters' to the input/output sequences of the network, so that certainties/insights about 'variable/weight-update limits' from related problem spaces (like 'regression') can be applied to the 'network' structures to change the possibilities/variables of the 'output function' structures, when the info certainties have already been applied to pre-process the data or pre-set the weights

    - identify useful sets of problem spaces like the 'regression' and 'optimize' and 'differentiate' and 'scale' problem spaces, which can produce useful structures/functions to apply in other problem spaces, bc of the generality of the problem space and the intersection that it occupies (like at an 'intersection' of a core structure like 'iterate/repeat' and a 'trade-off with a cost at each iteration, such as the opportunity to invest in a test of a different change at scale or a different scaling method or the inputs required to iterate', which creates the 'scaling' problem, as in its a problem bc there is some cost/input that isnt optimized yet, so its too expensive to repeat enough times to be usable for some intent)
        - for example, alternate problem spaces requiring functions like 'differentiate', 'scale', 'optimize', etc require sub-functions that fulfill those intents and are useful for related intents (a function that solves the 'scaling' problem for some specific problem space is good at finding interface structures like limits, thresholds, specifications, new variables, new errors, new interaction levels, etc) so solving the 'scaling' problem can produce structures useful for other related intents or examples of these interface structures, where solving the 'scaling' problem also solves for attributes like specificity which are useful in other problem spaces like regression
        - solving general problems involving differences between interface structures (like 'scale' or 'optimize' or 'connect/differentiate') tend to be useful for solving other problems, similar to how 'organize' and 'store' and 'obfuscate' and 'scale' are similar alternative problems, whereas solutions in the 'scaling' problem space might not be useful for finding more optimal unit/default structures to apply at scale, more adjacent optimal structures in a different direction than the scaled directions of change, or predicting the output (like a limit or interaction level) of a scaled structure, and the certainty produced by a scaling optimization algorithm might be over-used which would create errors, if applied in a sufficiently different system
        - finding these connections between problem spaces like scaling -> specificity -> regression can be useful for finding opposites (finding the specific/general dichotomy is adjacent to connecting those problem spaces), where finding 'reasons not to specify' is adjacent to identifying the structure of 'alternate' applied to 'inputs' (as in 'alternate data sets having some ratio/other structure of similarities in common')
        - solving the 'scaling' problem tends to have more similar solution structures than other problem spaces (the problem of 'finding cost-minimization structures like shorter paths to remove costs that act as barriers to scaling'), like the problem of 'organizing information' (the problem of 'finding a network/distance/position/node mapping to allow all info to be found with queries on the network'), so 'scaling' solutions are likelier to cover a subset than the entire set of all problems when those solutions are mapped to other systems, but they are clearly related (the problem of 'finding the shortest path on a network' is related to the problem of 'finding a network that allows short queries to be run to solve all info problems', where one is a subset/specification of the other, which is made clear by standardizing the problems to a network format)
        - similarly, 'regression' can involve structures as different as 'similarity indexes', 'filters as differentiators of functions having some other similarity', 'maximally different base functions', 'summaries/representations', 'local average/subset selection functions', 'intersections', 'primary exponents', and 'densities/limits/inflection points', which indicates its a high-variation problem space whose solutions can cover a higher ratio of all problems when those solution structures are mapped to other problem spaces, and can identify interface structures by being connected to other high variation problem spaces
        - similarly, its useful to identify maximal differences to skip scaling structures, such as maximally differentiating functions like 'oppose the angle, such as change the sign of the angle' (to test for a corrective angle difference or an angle change, when some angle is above a ratio of incorrect), 'differentiate' (to apply complexity), 'parallelize' (so that both adjacent/non-adjacent errors are constant as in equidistant vertically/horizontally/diagonally/similarly, as once errors are constant, youve basically solved for the regression function, similar to how once errors are small enough, youve basically solved for the 'regression' function and how once youve solved the unit variant of a function youve basically solved the function, as small and constant and similar in shape (the unit variant) are variants of 'trivial to similarize, as in adjacent', and similarly 'rotations' are basically solutions of the regression function such as how a rotated wave, similar to a shifted wave, has error patterns like having exactly wrong solutions at adjacent subsets, and similarly once youve solved for the primary exponents or densities, youve basically solved for the regression function, as the highest variation variables are similar to the actual solution function), and other functions which are useful for finding different solution functions, as a way of finding alternate directions of change or solution base functions to apply as inputs to a neural network/regression algorithm, after first finding a base function to check first to base these changes on, as possible useful different functions to split into different networks as default parameters, such as by randomly positioning parameters of the base function in the network, to increase the likelihood that adjacent changes in one of these different networks will find an optimal (first finding a set of inputs/outputs for the function and the error for some subset of the inputs, then using that info to filter the remaining possible error functions), and as an alternative to an incremental parameter change to test for a 'direction of optimality', when in reality, 'areas of optimality' are likelier to occur in both directions (and similarly, its useful to find similarity indexes to filter the solution space of error functions by which functions can have overlapping subsets for which ratios of the range)
        - relatedly, 'scaling' is a problem of 'how to make a variable value extreme, without disrupting the system in which it occurs (while other variables are being scaled in all probability)' and 'scaling' solutions often involve other related problem space structures like 'organization' (such as 'distributing cost among many sub-functions to lower cost through specialization, such as mass production')
        - identify other useful problems/solutions is a matter of finding ranges where these problems exist, such as how 'scaling' (applied at its most extreme scale) can compute every function similarity, so AI is unnecessary at that point, since 'mapping any set of input variables to any set of output variables' and 'mapping that input/output function to an error function' will both be computable in a reasonable time, so intents like 'finding the variables that are adjacently connectible or constantly connectible to the outputs' will be trivial and similarly 'finding the input/output function-changing function that creates the most traversible/optimizable error function' will also be trivial, so in between that extremely scaled range of scaling solution structures, there are algorithms to make these intents trivial with less info (such as 'average' or 'type' or 'limit' info, which is different from the 'full set of input/output connections or probable solution functions within some error range' but which is still relevant info to the data set in some way, so its likely to be useful as an input to an 'interim' algorithm that doesnt compute every possibility)
        - the connection between 'scaling' and 'trial and error' is obvious ('scaling' solutions enable 'trial and error'), and similarly other problem spaces have solution structures that make other workflows trivial or otherwise align with other workflows ('quantum computers' are useful for 'scaling' as well as 'parallelism', so they are even more useful for 'investing in multiple directions of change to some ratio of the full possible range in that direction, as in the ratio up to the point where one investment pays off to find some optimal structure')
        - connecting a problem space like 'scaling' to workflows (like trial and error), interface structures (like variables/networks/concepts), and other problem spaces (like regression or cost/benefit/incentive analysis) is useful, just like 'connecting other interface structures' is useful
        - future computation will allow 'scaled thinking' to reach understanding, so that all variable interactions, variants, alternate definitions, limits, and other interface structures of a particular concept will be understood (meaning 'in working memory while conversing') as opposed to just using a reference/shortcut to the understanding (like calling 'scaling', 'iteration' or 'efficiency' instead, as its clearer/more structural than using 'scaling', rather than thinking through all the variable interactions of scaling and its alternate definitions and other interface structures, to find the emergent meaning or understanding of the concept in that context on outer interaction levels)

    - apply workflows to fulfill problem-solving intents like 'find variables in common across different structures to find other variants of the structure' to other workflows that are particularly useful such as being 'maximally different' which means connecting them is a particularly useful intent to fulfill and may fulfill other problem-solving intents like 'identify all items in the set of high variation/complex variable connections'
        - for example, workflows like 'identify workflows used to create existing structures (like how trial and error is used to create a useful structure of a convolution, which is an application/integration structure of applying one structure to another to find all possible interactions)' and 'identify truth/falsehood structures like interaction variables such as how a statement is often both true/false in some way to some degree' are very different but similarly useful in this difference (its useful to think about truth/falsehood structures and workflows that create useful functions/attributes, rather than either workflow's structures in isolation)
        - identifying 'workflows successfully applied to create useful structures' and 'reality-covering attributes like true/false and their interactions' are connections between useful (high-variation) structures (workflows, reality-covering attributes) on different interfaces (the 'usage/function' interface and the 'concept/info' interface), which is all it takes to fulfill some subset of problem-solving intents (problems are often a matter of 'finding a connection between different structures')
        - similarly, 'iterate through all problem-solving intents, finding structures that fulfill multiple intent sets that have some other useful attribute like being maximally different intents' is similarly useful as a workflow applied on problem/solution structures
        - similarly, applying workflows to some specific problems like 'neural architecture' creates useful structures adjacently even where the workflow isnt completed or successful bc of the usefulness of sub-intents or intent subsets of the workflow (such as 'multiple-intent neurons' that support different change sets, either of which is useful across problems and which can be rotated) which can be found with any workflow involving 'finding cross-interface structures or their possible new applications'
        - this is related to a workflow like 'find useful structures, then find all the problem-solving intents or useful structures nearby that could have led to those useful structures' by using workflows with useful structures (like highly differentiating attributes or reality-covering attributes, which are not unrelated, as an attribute has to be maximally different in some way like the variation it supports in order to describe reality) to connect workflows in new ways

    - identifying intents like 'identify specific useful structures like alternates' for a problem space structure such as 'universes' with associated specific structures (like 'alternate uncertainty resolutions') that can be resolved with alternate function sets ('identify variables of the current position to determine other possible positions', or 'connect it to known info', or 'specify a more testable variant' then 'find a measurable metric to test for' then 'find a test using that measurable metric')
        - for example, identifying a possible universe structure (such as a 'reason why a specific universe structure might be real') from the structure 'alternate' by identifying 'reasons why a specific structure (applied to "universes") like an "alternate" might exist', such as that an alternate universe exists to allow exploring/testing/resolving an uncertainty, just like how 'copies/equivalents allowing some degree of change' exist to allow 'resolving uncertainties between relatively similar alternatives', so now the questions are:
            - 'what uncertainty is resolvable with this universe and what uncertainty might be resolvable with other universes (what variables might differ that allow different questions to be answered)' (if true, what specific variable values are relevant to this universe)
                - 'specify it so its testable'
            - 'can any of those "possible universe structures" (or related structures like "uncertainties resolvable with universe structures") be mapped to known quantities/formulas' (if true, is it relevant in the sense of being adjacently connectible to known certainties like known information, like is there a formula that could explain some information which could indicate that this specific universe structure or uncertainty is a legitimate possibility)
                - 'connect it with known information'
            - 'can a test be identified to gather info about a structure like that, if it happens to be definitely true/false' (if true, can it be relevant in the sense of being proved/tested in some way)
                - 'identify a test that differentiates the input (of the specific universe structure or related structures) into a relevant output (a measurable quantity)'
        - you can see how common simple functions are applied to solve the problem which would apply to other problems, and see the connection between these common functions (like 'specify an abstract structure so that its testable, then apply tests', which connects common functions in an input/output sequence)
        - identifying useful specifications and variants of these simple/general function sets is similarly useful, like how identifying related sub-intent of an intent is useful
        - how is this different from other workflows that use an 'input/output sequence' to connect structures like 'function sets' to some intent? 
            - its connecting the common core structure that is an 'alternate' structure with other useful structures like 'reasons for a specific structure, like an alternate' (to resolve different questions, a function that a universe might have, as other structures have that metadata as well, structures tend to be 'useful to resolve some problem') and then connecting those reasons/causes with other interface structures like 'function sets to implement a particular reason why a structure might exist', since 'reasons' are so highly determining/predictive of function/structure, since the structure/reason stores so much possible variation in how they could be related, so solving the problem of connecting them is useful and contains a high probability of finding a new connection given that high variation
            - this is useful to apply to universes bc knowing whether there is a 'variable that can be varied to find the other possible universes' is useful to 'find the other possible universes', 'find out if theyre likely to still be accessible or if the core uncertainty is resolved', 'find out if another universe can be triggered by an uncertainty in a different universe' and 'find out if there is an optimal sequence/network/combination of uncertainty resolutions that should be applied in that sequence/network/combination', all derived adjacently just from applying a 'reason' to the structure of an 'alternate', and finding related structures like 'specific variants of structures' and 'tests' connected to that reason
            - applying simple/core functions defined in interface analysis can be used to connect the 'specific universe alternate' and the 'possible reason for it' with other relevant info, such as by identifying info structures that coordinate (like a 'specification of some structure' and a 'test to differentiate some determining variable', where both the specification and the determining variable act like filters of the set of structures that are possibly relevant) to filter the set of possible connections between the 'specific universe alternate' and the 'possible reason for it' and connect the 'possible reason for it' with 'reality (known certainties/information)', applying the 'possible reason for it' as a 'connection' structure (to apply interface analysis to fulfill and connect with other structures), as once the 'specific universe alternate' is connected to at least one other interface structure like a reason, that connection can be used as a basis for other connections to be found (applying a workflow of 'find structures related to the problem/solution and connect those instead')

    - identify useful structures like the 'useful structures found/almost found, but not used/connected/understood, interface structures' or 'useful structures not found' by a particular algorithm, and how it could be altered to find those interface structures and use them to improve its own solution metric values (like whether a particular algorithm has enough 'variation potential' to simulate itself and its errors, and enough built-in self-evaluation requirements like 'identify useful structures and abstractions of them found during training', that it can find those structures)
        - for example, the fact that an AI algorithm might frequently find a common solution structure like 'create an index of some answers rather than leaving those variables uncertain or re-calculating them, as they are unlikely to change (info interface) and pair it with adjacent change combinations of existing known solution functions (change/core interface) using some reward mechanism to incentivize additional changes of some type/direction' doesnt mean the AI will find other relevant useful structures, like the following, but it does mean that this structure is useful and that existing algorithms can find some similar/equivalent solution structures and that this solution structure is trivial to find for some algorithms
            - the 'cause of the solution success' as in 'why its useful', or identify that its another useful structure such as a 'cross-interface structure' (a info-change/core cross interface structure) as well as a 'cross-certainty structure' (variable + constant), or otherwise identify other useful structures like 'cause/logic/other interfaces' or other reality-covering variables which could replace the 'info index and the adjacent changes with a reward function'
            - relatedly, this solution is findable to this algorithm bc the solution is trivial to find with its available functionality (as in 'adjacent to inputs') and is a trivial change of existing solution structures
        - this is partly bc AI algorithms (even those tuned to find solutions) use solutions differently than a human brain does, as a 'possible useful output of some process or set of variables applied as inputs', in isolation of other info like other problems, other queries of the brain/model to solve problems, the solution's impact when applied across interfaces, patterns between solutions found, effective functions to describe all solutions, etc
        - a 'reinforcement learning algorithm with some feedback/reward mechanism' (related to specific loss evaluation methods like gradient descent), applies a core solution automation workflow like 'change a base solution (until its across some threshold of a solution metric)', which is not a required workflow to use, 'finding patterns/structures/variables of solutions or other useful structures and re-combining/varying these patterns/structures/variables is an alternative that doesnt necessarily involve using known solution structures'
        - its feedback evaluation function might not be able to adjacently detect subtle differences or hidden variables or be able to use alternate info as input, such as 'info about the network params and algorithm sub-functions' as input bc those will have to be simulated while using them to evaluate them, and the variation required for that simulation might not be supported by the network while also running the evaluation function
            - if its not adjacent, some algorithms will never find it
        - given that these are known errors, a good algorithm would have to combine algorithms like this in a set with a different algorithm tuned to handle this error, at least to provide a counterpoint or contradictory view (like a 'maximal difference' algorithm applied with an 'adjacent change combination' algorithm) if not the trivial improvement that is more useful than a counterpoint in some cases, where the exact opposite might be too extreme and thereby create errors, as the exact opposite implies there is nothing optimal about the original solution structure and it should be opposed completely to test its opposite
        - a good algorithm should be able to re-identify a missing interface structure, if one is missing, using the others
            - if you took the useful structure of the 'reward mechanism' away, what could replace it?
                - this idea of the 'reward/feedback mechanism' is a useful way to 'connect outputs with inputs' (to incentivize inputs that led to valuable outputs)
                - 'common variables/patterns of solutions' is another useful structure that could replace a 'reward mechanism' (like how a 'test' function to check every solution in a set can replace a 'filter' function to differentiate info likely to be a solution)
                - a 'function to make every input useful (derive info from any input, and connect any input to a useful structure)' (rather than finding useful inputs) is another alternative useful structure
                - what other connections between interface structures arent fully used or optimized yet? cross-interface structures are a good place to start looking, as well as other workflows and variables of them
            - if you didnt identify the 'info index + adjacent change combination' cross-interface structure, what else might you have identified?
        - similarly, connecting all solution-finding methods to 'incentives' and 'usages' is useful as another solution metric (does this workflow integrate the concept of 'incentives' and 'usages', which is required for not just finding solutions, but building them optimally, and making sure theyre used, and making sure theyre used correctly, to apply 'usage' structures that make some solution more likely to be used correctly, which are related functions that cant be replaced by each other - 'independent coordinating/complementary alternates' rather than 'equivalent alternates')
        - why might an algorithm identify an 'info index' adjacently without having that given as an input? bc its format allows/requires identifying 'input/output indexes' and 'interim indexes' in between those are a default structure in some networks, given some algorithms, so 'maps/indexes' might be adjacently identified as useful (implicitly, rather than explicitly) by various 'neural network' algorithms

    - identify useful structures in specific problem spaces and sets of useful info like 'sets of differences/starting points' that can be used to identify those useful structures
        - for example, in the 'find a medicine' problem space, identifying that the 'ocean is a harsher environment than land in many ways/cases' and that 'stress leads to evolution' means that 'compounds to resist pathogens are likely to be found in aquatic life, as they survived this harsh environment' and similarly the 'ocean is a high variation environment' and therefore 'new compounds that are useful in a new way or dangerous in a new way are likelier to occur in the ocean' and otherwise identifying efficient structures (like survival functions/compounds in a harsh environment), high variation structures (like where evolution occurs faster), common structures (like common points/paths of evolution), and other useful structures is possible and useful for other intents, like 'predict which organism might evolve an antibody to this pathogen quickest'
        - the set of information including 'starting points/inputs' and 'variables that are useful when applied from those starting points/inputs' is useful to identify in that set, to apply when identifying 'when an interface query will be most useful'
        - whats even more useful is when the starting point is not adjacent to the solution, but the query makes it adjacent to connect to the solution, across problems
        - the 'starting points' here are the 'insights', which when applied, make it trivial to solve these problems such as 'filter the solution space' (identify that aquatic life are a good solution set to test first as they are likelier to be more resilient bc of the harsh environment, which could be why algae and fungi are useful for many things as theyve developed many functions as theyre resilient to harsh environments, and similarly is why they can be more dangerous as well, theyre over-evolved)
        - finding those "'useful starting points/inputs' for a query (run on a particular problem-solving network) that make the query most useful" is a useful problem-solving intent, just like finding the functions of that problem-solving network is a useful intent, which is like workflows that involve a step to 'solve a different problem, or change the problem into another problem, or solve a sub-problem' but involves changing the inputs until theyre insights, which are useful inputs/starting points to most problem-solving queries, and finding useful connections to match an insight set with a query set that would make that insight most useful
        - similarly, adjacent changes to known insights can produce other insights (new insights which can be used as new inputs to queries which could make them more useful), like how the above insights adjacently produce the insight 'extreme systems/contexts (within survival limits) can produce useful structures like multi-use structures and other powerful survival structures like a high variation in antibodies', similarly an environment producing survival structures like 'high variation in antibodies' could produce other useful structures, like 'common compounds of structures (fish) having those useful structures (high variation in antibodies)' (which is why omega 3s might be useful for survival, bc the fish species evolved to have a high variation of antibodies, which led to the development of other useful compounds, or the omega 3's might be an input to the antibody creation rather than an output, where the omega 3's might be useful as an 'equivalent alternate' as the antibodies if they have such a causal dependency that they can be used as an approximation), similarly insights make most workflows more successful, so theyre useful to apply regardless of the structure of the workflow in many cases (answering questions like 'is the omega 3 compound only useful or only develop to be useful bc of the high variation in antibodies, or only useful bc it occurs in a resilient/surviving structure which indicates other compounds would be useful in the same species, or only useful bc of the harsh environment which would require any surviving structure to develop useful structures, or only bc the survival structure of the host uses other useful structures like "resilient multi-use/function" structures like algae as an input')
        - differences in the possible reasons for the usefulness of a structure indicates the usefulness of identifying differences in reason, as different reasons for usefulness reflect different functions, systems and variable interactions and other interface structures
        - similar to how cross-interface structures tend to be useful by default, 'opposing' structures (like a generative function and a filter function) tend to be useful as well, possibly bc theyre also cross-interface structures

    - identifying specific variants of known useful structures like known errors which arent formatted with sufficiently specific structures to be automatable which have potential for additional specficity and therefore an opportunity for optimizing the usefulness of those structures
        - for example, the 'loophole' structure is a useful general error structure concept, as a target structure to focus on/avoid/close, but its not specifically structured enough to be automatically identifiable/solvable, so adding specificity to this structure is useful (such as identifying a specific definition of it such as 'a gap in coverage of a solution of all possible inputs, where coverage of inputs prevents some error structure from occurring for those inputs'), which is a more useful variant of the error structure through its specificity, which is now possible to find automatically (find the gaps in possible inputs covered by a particular solution, given this function to identify coverage of inputs and this function to find all possible inputs) and also automatically fix (finding a function to increase the coverage of inputs for a particular solution)

    - identify structures that are common across solution-finding methods like solution automation workflows/interface queries that are particularly useful, and identify structures related to these common structures (such as conceptually similar as, or causal structures of these structures)
        - for example, a 'structure common across solution-finding methods' is 'high variation', 'usage of high-variation functions', 'high variation between functions used', and other forms of 'high variation' such as 'independence/orthogonality', where other structures can be used to connect these structures such as 'similarity between input/output (such as a protein/receptor similarity) that makes these high-variation structures interactive' and 'a clear difference/similarity created by the solution-finding method that is useful for some problem-solving intent (similar/different in the way that is specified as useful by that problem-solving intent) and similar/different to the solution/problem structures in some way', among other structures of the solution-finding method like how 'connect a sequence of similarities to create a clear similarity/difference' is a function that can describe many workflows as those workflows will fulfill an intent like 'connect problem/solution' that makes this structure a common implementation structure of a workflow
        - similarly, the solution-finding methods will vary in a new way to connect structures to fulfill some core function interface (like the 'find' or 'build' or 'derive' interface), so the solution-finding method would need to be a 'new/different connection between functions that still fulfills some core function intent (like find/build/derive)', where 'fulfilling some core function intent' might take the form of 'vacillating around the find interface applied as a symmetry, so that its similar enough to the find function to be an approximation/alternate of it'
        - finding 'independence' structures such as 'uncorrelated, indirectly related, distantly related, specifically but not generally related, relatable but not by default related, relatable with either the same unit/iterated function or with new variables, requiring a high degree of work to relate, or otherwise independent variables in the same system' is a way of finding useful variants of independence to apply in solution-finding methods
        - this is similar to how structures common to solution automation workflows are higher interactivity/connectivity/variation than other structures, they also have other attributes in common which are related to these attributes (independence being related to variation)
        - given that there are likely to be multiple reasons why a structure is useful, as useful structures are likely to be useful for multiple intents (an alignment between multi-intent and multi-reason utility), identifying the other reasons why a structure is useful (independence isnt just related to high variation, its also related to a known useful structure like a cross-interface structure, which is useful in that it connects different systems), and the connection between these reasons (high variation structures like 'independence' would likely be related to other high variation structures like 'connections between different systems (as in cross-interface structures)')

    - identifying functions like 'specific repetitions/iterations' that are less likely to create useful info, to identify structures likely to create useful info by applying differences to those structures, such as 'repetition functions with specific variables (like "variable position" and "variable application position (change input or change limit)")' that make it likelier to be useful for problem-solving intents like 'identifying new variables'
        - some repetitions (as in 'iterations') of a structure can create useful structures like additional dimensions (like 'repeating a square by stacking it can create a cube')
        - other repetitions of a structure can create less useful info (like how 'iterating across every possible point of a square' is not the same as 'adding/finding new useful variables')
        - the difference is that while both add new dimensions (a point creates a line when iterated through 'stacking', and a square creates a cube when iterated through 'stacking'), one can contain more variation as its higher dimensional and the iteration function can also contain more variation for the same reason
            - other functions to add a dimension include 'position a copy of the structure so it has a side/surface in common (an overlap on one unit of the border, such as a side) and fill in the implied structure by the endpoints/other sides of these overlapping structures, connecting their endpoints/corners' or 'position a copy of the structure, a distance away equal to one side of the structure with an overlap of the centers of the two structures, meaning a distance in one dimension only, and connect their corresponding endpoints/corners/sides of the two structures'
            - all of these involve 'positioning a copy of the structure in a different position from the original structure, and in a different position from other iteration functions, and then connecting the two structures'
        - to include relevant metadata such as a 'way for this error to be true/useful/a solution' (for the stated intent of 'identify new variables' that its an error for):
            - its possible to create high variation with a function that will definitely cross every point and is not a function like 'iterate every point crossing this constant line, then add an increment to the constant line and repeat the iteration'
        - what is different about a function like 'iterate every point crossing this constant line, then add an increment to the constant line and repeat the iteration' and some other iteration function that crosses every point but is less predictably uninformative and low-variation? 
            - the variables are applied at a 'position' where the variables can have 'compounding' value (such as 'integrating the feedback from one variable directly in another', similar to the fibonacci sequence), as opposed to being applied in a limit of variation
            - in the lower-variation iteration function, the variation is in the limit of variation, where the line intersecting with the square is used to guide (and therefore limit) the variation of the iteration is predictably and linearly changed every time, to produce the same result every iteration
            - there are some 'compounding/exponential/otherwise non-linear' functions that would produce predictable results, but this output is less common in that type of function
        - similarly, where might this iteration function be useless/useful? 
            - it might be useless in creating new variables, when the structure being iterated on (such as a cube) is already higher-dimensional than what the iteration function could create (such as a line), if the structure being iterated on isnt a 'dimension set' but rather a 'structure in a dimension set' (when the input problem structure is already a solution structure, more so than any change created by the solution-finding method could create)
            - this is an example of why its useful to be able to find the 'net/emergent effect of iterated changes' and 'metadata about those effects' (what 'degree of variation' is possible with this function, applied in this way, to these inputs, to this degree of iteration)

    - identifying high-variation structures like 'philosophies/perspectives' that are abstract or otherwise useful and also easily filtered
        - similar to how the golden rule has associated concepts like symmetry, generosity, justice/balance, reciprocity, and empathy, it also has associated errors (its not enforced, so could easily cost someone all their resources, and empathy doesnt always produce a requirement for generosity, and justice/reciprocity arent equal to generosity, and symmetry is different enough from balance to justify its own concept)
        - similarly, perspectives like 'magical thinking' are easily disproved without a test (whether a real-life test or an algorithmic simulation), just with finding info about 'common thoughts' (such as 'go to store', with related info like 'the store doesnt come to people, even though they think about it and visualize it')
        - relatedly, other high variation structures include combinations of existing high-variation structures like 'perspectives' (a 'filter with priorities that covers reality') and 'cultures' (a 'stack of iterated changes to create variable interactions within a perspective'), such as 'culture-perspective combinations that lead to the destruction of the society that applies that culture-perspective combination'
        - relatedly, its useful to connect perspectives with structures like useful structures (such as 'cross-interface structures adjacently created by that perspective', which are a useful output that these philosophies/perspectives can sometimes adjacently derive)
        - how do you find these errors from these philosophical rules?
            - 'intersections of scaled interactions, where one variable/structure arrives first and uses the resource first, or theres an overlap creating some resource conflict, or these required structures otherwise contradict each other when they interact' is a interface format description of a common error
            - 'apply the rule at extreme variable values, like at scale or at its most extreme interpretation or an extreme test like a test with measurable large structures like a building, to determine scale limits of the philosophy'
            - 'finding differences between concepts' of the philosophy/perspective, such as how 'justice is not equal to generosity' and errors resulting from those error structures like 'differences being equated as equal' (or 'equalities being differentiated as different'), although both are related concepts adjacent to the 'golden rule', and similarly connect the concepts to interface structures like 'requirements'
            - 'finding differences in the context that created a philosophy and useful contexts like current/modern problem spaces, where differences/gaps in this mapping across time contexts could indicate the irrelevance/in-absoluteness of that philosophy'
            - finding 'lack of connectivity to reality-covering structures (how does it connect truth/falsehood with abstraction/specificity) or interface structures (does it contain enough variation to be realistic)' and 'lack of handling for common problems (how does it handle "group interaction errors")' are other useful functions to find errors with these perspectives ('if you prioritize the same priorities as this perspective, can you avoid errors in some common problem space?')
        - why are these philosophies/perspectives useful?
            - given that 'philosophies/perspectives' are already extremely filtered rule sets, they themselves are easily filtered, which makes them useful
            - these 'philosophies/perspectives' are an attempt to find 'abstract efficient/limited rule sets that cover all of reality' (given the structure of 'perspectives' as a filter that prioritizes a subset of priorities over another subset, as in a usefully limited set), but are often incorrect, as they tend to over-simplify/over-limit/over-filter, use structures on an incorrect abstraction/interaction level, and also dont connect their rule sets to actual reality-covering variables (truth/falsehood, im/balance, power, un/certainty, simplicity/complexity, abstract/specific, solution/problem, useful/relevance/meaning, requirement/alternate, probable/impossible, etc)

    - identify solution metrics like how a falsehood should be accompanied with the corresponding true version of it and an error should be accompanied by the useful/solution version of it
        - a 'false similarity' (like a 'variable that seems correlated but is independent') can be a 'true similarity' and is useful to connect to a true similarity, even if the connection contains very little information
        - for example, two variable interactions may seem correlated (a 'false similarity'), but in real systems, those variable interactions might occur for the same reason (a 'true similarity', in the position of 'cause'), even if theyre technically independent of each other, they might be dependent on the system that creates the pattern in the same reason for those similarities in structure, and therefore not absolutely independent, a reason such as 'because of common limits and inputs, this structure is often adjacent and optimal for some common intent like "storing info", so it keeps re-occurring, even though the two processes are not directly interactive, or otherwise dependent or similar or connected, causally or otherwise, and the similarity they have in common is that reason of adjacency for why a structure keeps re-occurring, and another similarity in the available limits/inputs that are adjacent to that structure, and the similarity of the adjacency itself'
        - this may not seem like a lot of info, it may not be derivable (as in exactly knowable, having ruled out other possibilities, with just the variable correlation), and it may also not be useful in predicting the structure's functions/variables, as they may vary far more in uncorrelated variables than the correlated variables, but its generally useful to know 'system/input/use cases where a similarity/difference may not only occur, but also may be some other high-variation useful structure as well, such as false/true/causal/independent/similar/connected/adjacent/etc', as these 'cases connected to structures' provide a structure to look for/apply as a base structure, to describe the system in which the variable interactions occur

    - identify useful structures like 'patterns connecting useful structures' such as 'cause (backward in the sequence to find inputs)/predict (forward in the sequence to find outputs)' as different values of the same variable 'direction' applied to a core structure like a 'input/output sequence' to find useful structures like functions to find core functions or primary interfaces or other useful structures by applying this pattern and the reason for its success (involving 'interactivity', 'variability', and 'powerful' structures like 'core functions' or other useful structures like 'cross-interface structures')
        - similarly, 'filter/generate or reduce/combine or compress/expand or find/build' is related to different values of the same variable 'count' applied to a core structure like a 'set' (or a 'limit', as in 'what is not in the set', which acts like an 'equivalent alternate' storing similar information as the 'set')
        - 'embed/inject' is applicable to a core structure like a 'container' through values of the same variable 'position'
        - this is why 'sets' can be used as an alternate structure for 'predict' intents, they capture similar variation/information as 'sequences' and are an 'equivalent alternate' core structure, where the intents enabled by 'sequences' are similar to the intents enabled by 'sets', as 'sets' encode information about other interface structures like 'interaction levels' which are highly predictive of other variables (structures on the same interaction level can be used to predict other structures on the interaction level, more so than variables on other interaction levels)
        - finding these variables (direction) applied to interface structures (sequences) to get primary interfaces or other interface structures (cause as a core function and a primary interface, predict as a core function)
        - finding the 'set of similarities' and 'input/output sequences that determine a structure' can identify the 'variables of structures which can identify it'
            - for example, a set of slopes/starting points creates a 'set of similarities' in the 'set of intersections of these 'slopes/starting points', a 'set of similarities' which can create a 'set of limits' through the 'set of intersections' resulting from those similarities (the slopes are 'similar enough in position (to allow the possibility of an intersection)', and 'different enough to be sides of a polygon (which are 'maximally different' slopes)')
                - this 'set of limits' is a core interface structure (if you know the 'requirements/limits (of what something certainly/definitively is or is not)', you can often solve the problem) that fulfills one of these core function intents like 'filter'
                - 'similarities' (adjacency of position, difference of slope) -> 'intersections' (different type of similarity) -> 'limits' (as 'requirements' of some similarity, or 'limits in the sense of differences from other possible structures')
                - the 'intersections' are adjacently convertible into another useful structure 'set of limits' (if they connect enough to form a bounded shape, where those limits can act like boundaries)
            - what other structures fulfill a different intent, and do those structures capture the same information and do they use different similarities/structures and how can those be connected to these similarities/structures, through identifying variables of those 'equivalent alternate' structures
            - a 'symmetry through an equilateral triangle', the 'length of the symmetry' and the 'radius of change that constructs the remaining side of the triangle' can also construct a triangle if its equilateral (has a symmetry dividing it in equal sections), which reflects the utility value of a vertex between two vectors in creating a triangle (once you have one vertex with two vectors as a corner of the triangle intersecting with the symmetry, which is retrievable with the symmetry and the radius of change which determines the length of the remaining side, the triangle is determined), where the 'length of the symmetry' and the 'presence of the symmetry' determine the other angles which are possible, similar to how the 'hypotenuse of a right angle triangle' encodes info about the other sides (once its known that its a right angle triangle)
                - the 'set of similarities' in this example is the 'set of similar radius lengths from the symmetry, which creates a similarity in the angles possible in the equivalent-angled corners, in their equivalent interactions with the point identified as the remaining corner', and this 'similarity in angles' creates a 'filter' of the 'remaining possible angle value'
                    - 'similarity (in symmetry)' -> 'similarity (in radius length)' -> 'similarity (in two equivalent angles)' -> 'filter (of remaining angle, through the "requirement" of angle sums that acts like a filter)'
                    - these structures are adjacently connectible (such as how a 'similarity across a high ratio of inputs, as in the equivalent angles' leads to a filtered set of 'possible other inputs that could explain a difference, as in the remaining angle')
            - the 'similarity in angle sums of a triangle' contains related information which can be used to filter possible angles of different corners, once one corner is known
                - the 'set of similarities' in the 'similar limits (upper bound) on angles of a triangle' which leads to a 'similarity in related structures (the limit/upper bound of "combined angles of a triangle")' which leads to another similarity in related structures (the similarity between each angle and the other angles, and each angle and its relation to the upper bound of their sum and each angle and its relation to requirements like 'at least one right angle' or 'two equal angles', as the progressing sum of each newly identified angle has to be some degree of similarity/difference from each other, the upper bound, and requirements (given the scarcity of allowed number of angles and the requirement created by the first known angle and the 'requirements about triangle type which are known', with rules such as those regarding 'differences between variables of types of angle connections (like two equal angles or all different angles or one right angle) in different types of triangles (like isosceles triangles or triangles with all different angles or one right angle)') to the other known angle but different enough from the sum to allow a third angle before the upper bound of the sum is reached, depending on their position)
                -  'set of similarities' in the 'similar limits (upper bound) on angles of a triangle (where each angle has to be less than 180)' -> 'similarity in related structures (the limit/upper bound of "combined angles of a triangle")' as their sum has to be less than 180 as well -> another similarity in related structures (the similarity between each angle and the other angles, and each angle and its relation to the upper bound of their sum, as these angles of the triangle corners encode info about the intersections/limits found in another structure, as a 'vertex of two vectors' encoding the angles is another format than the 'set of slopes/starting points' that intersect or the 'set of intersections' defining the endpoints of the triangle
                - this encodes a similarity between upper bounds across structures that are operated on with a symmetry (the upper bound of each angle's maximum possible value 'in certain cases of the other angles', applies to the sum of the angles as well, bc summing the angles preserves this similarity) which creates another similarity in 'first-next/next-last angle' values in the sequence, as each angle is filtered and applied as a filter of the next, as the 'sum' sequence has an 'alignment' with this 'angle' sequence and can be used to filter the 'angle' sequence
                - the 'unit/sum' adjacency is the basis for the similarity (the units are adjacent to and useful in creating the sum), and the angles are adjacent to each other (in required similarities or limited differences) as well as the sum and the requirements of the triangle sub-type
            - similarly, triangles can often be trivially changed such as reflected/rotated to create other triangles
            - what is the common structure across these variables ('set of intersections/limits', 'set of symmetries/radii/lengths', 'set of vertexes (of vectors)', 'set of equivalences in sums/related angles/requirements', 'set of similar shapes like other triangles') which create the same shape type?
                - all of them encode different structures of 'similarity' through applying different sets of core structures, similarities which connect different 'differences' (like different 'input formats') which are variations of the same information (different 'sides/endpoints of a triangle'), where these differences are similar enough to the input and target structure to be 'equivalent alternates' in capturing the same info, which can connect 'maximal differences' (such as different slopes required for an equilateral triangle) within some range determined by a similarity (like an angle sum requirement)
                - all of them differentiate the structure from other shapes ('given these intersection points, it cant be a square'), and some of them differentiate within triangle sub-types
                - all of them fulfill a "sequence connecting interaction levels of the primary different sub-structures of a triangle that couldnt build a different shape", given their possible connection functions (components that can be used to 'build' the triangle or 'limit other possible shapes so that a triangle is required'), starting with input structures that have the right differences to create a triangle with a few changes and applying changes on various interaction levels of similarities/differences so that the final structure is created as a result of this sequence
                - these involve 'conversions across "types of similarities"' and eventually create a useful 'difference' (from other shapes) or 'similarity' (to the target shape), or a 'similarity in a difference' (a 'common connection between multiple maximal differences like different slopes/angles' or a 'similarity between a vertex/intersection structure and a triangle' or a 'different way of connecting/representing/generating/determining/deriving the same info') that can be used to connect the various input sets to the target structure
                - given that a triangle is adjacent to 'core' structures (like 'lines/points/angles'), it should be similar based on these/other adjacencies, such as similar to these core structures to some degree (similar to components like lines/points/angles and structures of them like vertexes), similar across sub-types (similar to other triangles of different types), similar to the sub-type (similar to requirements of a triangle sub-type), similar to itself (similar to requirements of the triangle definition), different from non-triangles, similar to the supertype (other polygons), and possibly also similar within some symmetry defined in a requirement of a sub-type (such as how 'two angles must be equivalent' in some sub-type), similar to adjacent structures of core structures ('intersections' as a structure of a core structure like a 'line', or a 'combination' applied to a core structure like an 'angle', or a 'combination' applied to a core structure like a 'triangle side' to create 'vertexes' as 'combinations of sides'), similar to problem-solving core interaction functions/structures like 'limit/filter/connect/combine' as core structures (or structures of core structures) can interact in ways that are different enough to be similarly variable and therefore interactive with those core difference structures ('limit/filter/connect/combine') before achieving the target difference of a 'triangle' from the 'inputs'
                - similarly, the similarities present in the connectivities/interactivities/adjacencies (like how the corner/intersection is adjacent to the side) of the maximal differences (three sides) determine the 'equivalent alternate' structures which can store the same info, where the 'maximal differences' act like symmetries to base changes around, and the adjacent changes found with those changes determine 'equivalent alternates' around the symmetry of the 'side', where the 'corners' also act like 'connection' structures of the 'sides' (similar to how input/output sequences and interactive structures like 'coordinating output/input sets' can be used as alternates bc of the 'connectivity' of one meaning the 'interactive structures' for the other meaning the 'input/output sequences')
            - from this, it can be derived that a structure common across solution-finding methods is that a solution-finding method results in a clear and stable similarity/difference to the target solution structure or adjacent solution structures or problem structures
            - similar to 'adjacencies', identifying 'asymmetries in probabilities' (as in, identifying structures that make another structure 'likelier', as a more specific form of 'adjacent') is similarly useful in determining structures that co-occur in a set or occur in a causal sequence, as an alternate useful structure to apply as 'connections', 'sequences', 'causes', etc
            - identifying 'functions (such as find/build/change/derive)' that act like interfaces (as a 'find information' function can be used to solve all problems) is useful for identifying other 'sets of equivalent alternates' that can be used to apply variables/similarities to problems to find 'equivalent alternates' in other problem spaces, where these core functions connect interfaces (such as the 'filter' structure with the 'find' intent and 'reduce' functions) and act like a set of structures that connects all primary interfaces and therefore act like alternates to the primary interfaces (like a set of horizontal lines of these core functions like 'find', as opposed to the vertical lines of primary interface structures like 'potential', which cross the same information and are highly interactive and can act like equivalent alternates for some intents, which is a useful metaphor for all information sets that can act like an interface)

    - identify useful structures like 'problem/solution connection structures' such as how 'adjacent info' is connected to the problem input and 'agent incentives is likelier to be closer to the solution as they indicate probabilities which are useful for predicting functions/solutions' so connecting 'adjacent info' and 'agent incentives' is likely to be useful to solve problems as a 'connection structure', which is related to the workflow 'connect structures adjacent to problems/solutions rather than the actual problem/solution'

    - identify useful structures like 'scaled interactions' like 'group interactions' such as how 'the implication of one example is not sufficient to solve most problems' and similarly, other info is required such as the vertex between the 'group-group interaction' (whether one group is suboptimal for other groups) and the 'group-system interaction' (whether one group is suboptimal for the system), as 'group interactions' are high-variation and can be applied to solve problems involving group/scaled interactions

    - identify useful structures like structures that are useful for a particular problem-solving core interaction function like a filter or other core interaction functions, such as how 'maximally different' structures are useful for 'filters/generative functions' and similarly 'randomness' and 'sensory deprivation' and 'extremely independent/disconnected variables' is useful as a 'optimizing/learning input' for 'deriving' functions
        - similarly, these similarities fulfill other solution metrics like 'alignment of incentives between opposing forces' as 're-using a structure across functions' is a useful optimization of function usage

    - identify useful structures such as attributes of useful structures like 'connectivity', as a useful structure is likely to be connectible to other useful structures and is likely to interact with other useful structures, as in likelier to be used by useful structures and other encodings of similarity like 'commonly found as an input or used in a useful function', since theyre simmilar by being useful and are therefore likely to be similar in other ways

    - identify useful structures like 'requirements' such as 'optimism' which is required as an input to continue pursuit of a reward and therefore required for success, but which may be contraindicated by negative inputs, so identifying false statements is likely to connect that negative state with a 'successful/solution' state as it buys time to find a solution

    - identify useful structures like 'concepts' (like 'fairness') and 'structures' (like 'scale') which are useful to apply to find limits/emergent effects, such as how 'scaled fairness' creates a very aligned and organized system (in that 'potential/ability' and 'problems requiring that potential' are aligned) but also a limiting system where there isnt much freedom to vary and therefore finding alternate optimizations to the system is less likely, although such a system isnt likely, which is useful for finding 'new directions of inventions'

    - identify useful structures like structures like 'metrics' to 'determine if structures are equivalent', such as if two structures are more interactive, theyre likelier to be equivalent which is true bc theyre likelier to exist on the same interaction level and are therefore likelier to coordinate than not and bc theyre likely to contain differences (like 'input structures like receptors/spike proteins') that can be changed to interact with each other

    - identify useful structures like useful 'mappings' such as between the 'navigation' problem space and 'networks' to apply navigation solutions to networks to find network format optimizations, such as how when investing work in multiple different solutions (where each agent with a car creates possible 'solution/error' structures in the 'driving/navigation' problem space where the problem is 'generally find a useful solution-finding method or specifically find a route and usage of this route to get to a destination point without errors', where those solutions intersect or pass, the slower solution can temporarily stop and use various structures of terms from the faster solution to test if it is an optimization structure, where each solution-finding method has a network of solutions that it finds or is likely to find or capable of finding with adjacent changes given the network's defaults (like similarity metrics, position change functions, etc), and the overlap of these networks is useful to identify so that these intersections between networks can be identified and the optimal route between these networks (switching between solution-finding methods) is findable on this set of networks, where the network might only represent the adjacent solutions a solution-finding method might find

    - identify useful structures like 'optimization' structures such as a suggested/optional 'slow/fast lane' which optimizes for grouping drivers with different relevant sets of intents, relevant to core functions/metrics like 'speed' to minimize a specific error type like 'lane changes' frequently cause
        - why is identifying a new solution/error in a problem space almost equivalent to a solution automation workflow? bc solution automation workflows are 'unique abstract/other interface connections between problems/solution structures', and each error is a new step added to or changed in an existing connection, creating a new connection between problem/solution structures, which is easily abstracted or otherwise converted to a primary interface, at which point its likely to be a new solution automation workflow
        - identifying a new solution/error is similarly useful for identifying new variables/differences/similarities/other useful interface structures (like how identifying a new error type is also identifying a new solution type bc you can just apply differences to the error type to adjacently derive the solution type, as errors are different from solutions), which is a useful problem-solving intent (identifying a new useful structure or new variable/difference type is often the same as identifying a new solution-finding method)

    - identify useful structures like 'generosity/strictness' (non-adjacent vs. specifically accurate) or 'optimistic/pessimistic' ('implying info about a future change, as in a future change of a data set') or 'experienced/amateur' ('rule-compliant vs. rule-unaware') which can be applied as a solution metric of predictions, similar to how metrics like 'specific/general' can be used to find optimal solutions

    - identify useful structures like 'different information accessible from a different position' and structures to find that info like 'changes/functions applied at that different position by other agents'

    - identify useful structures like insights such as how 'adjacent changes' (like in gradient descent) cant be used as the solution in all cases, bc some changes can be errors but seem like solutions (like the 'low-cost decision') and can occur at scale, so that any agent in that system doesnt see the errors or the net effect but there is still a huge change set required to fix it ('making agents intelligent' is not a solution likely to be found by gradient descent, neither is its implementation)

    - identify useful structures like optimizations such as where a function can be applied with trivial changes like 'in reverse' and still be useful, such as the 'lane change signal', which can be applied 'in front of a car to signal lane changes' or 'behind a car to pass in case they will see it and move' and still be useful, or similarly, speeding up, which can be useful for 'covering more distance faster' or 'to communicate that other cars should switch lanes to allow car to pass'

    - identify useful optimization structures like 'moving horizontally to see info farther away' which should be regularly applied in cases where 'the lane change function might be useful' and 'info about far-away errors like traffic jams or crashes is signalled as a possibility'

    - identify useful structures like 'specific examples of how a statement/solution could be true/false (as in correct/incorrect)' such as 'specific data sets that could make a solution true/false' so that 'determining difference from these true/false example data sets' is useful as a 'test/filter of a specific solution'

    - identify useful structures like similarities between error structures, such as in the 'driving' problem space where a 'sequence of agents' can produce multiple errors such as when 'driving next to the sequence where other errors or error inputs occur, such as the sequence is slow, which makes it easy to speed by it (a dangerous speed if any of the sequence agents uses a common function to change their position), and one of the agents changes lanes and its difficult to see their change lane signal (input block error) bc the sequence is slow as in a traffic jam so theyre close together' and the 'driving sequence will be difficult to pass if the incoming traffic is distributed rather than occasional or grouped together'

    - identify useful structures like 'independent subsets' ('independent' as in 'equivalent alternates' which have similar/overlapping info coverage/storage/functionality), such as 'convergences/divergences', which are useful to identify useful sets of structures which can be used to derive a high ratio of other info (like 'intersections' and 'change directions'), and which therefore are useful to identify 'similarities/differences', just like 'filters/averages/densities/limits' and 'parallels/orthogonals' are useful independent sets of structures to identify that cover a high ratio of info
        - the core insight is that 'equivalent alternates' can act like each other and replace each other, and are therefore independently capable of deriving info, so identifying 'independence' in this sense is a way of identifying 'equivalent alternates' and vice versa

    - identify useful structures like how applying 'proving' function often requires 'holding some information constant that is normally a variable', and therefore requires a 'distortion like a stretch/angle' of reality to achieve, as systems might only falsely (as in 'rarely') actually 'be in that state, or function that way, or even just seem/be represented that way', so much so that its arguable false to distort it in that way by holding those variables constant, as in 'proven true, in a generally false (as in "requiring any distortion at all" or "requiring unusual distortions that are false, in that they are not generally useful for other problems") but specifically true (as in "having some trivial number of conditions/changes to be true" and "possibly true, as in not guaranteed to be impossible") distortion of reality', which is less like 'proving a statement to be true' and more like 'forcing a statement to be true' (where there are usually some contexts where any given statement can be interpreted as true but some statements require more work than other statements to be verified or proven and are therefore less true)
        - similarly, identify useful structures like 'specific applications of insights' (such as that 'most statements are true to some degree and false to some degree', such as by applying 'specific variants of insight components like truth such as useful') where the 'specific application of that insight' using 'useful' instead of 'true' is 'most structures are useful in some way', such as how an almost completely useless structure might be only useful as 'another example of some type which is unnecessary as there are plenty of other examples', or only useful in that it 'creates errors', so it provides info about 'errors to avoid', so identifying extremely useful structures is a matter of identifying common variables of these useless/error/suboptimal structures and differentiating from those structures, as 'useful' structures are often more identifiable than 'true' structures (as 'true' is less testable/verifiable and more variable/general than 'useful'), so its useful to apply that specific variant of the definition

    - identify useful structures like 'connections between useful structures' such as connections between concepts like 'random' and 'nothing', as 'randomness' is associated with a 'lack of info' and also 'equally distributed info', so that 'nothing' might have an alternate definition like 'extremely/maximally distributed info', which could be a default state of the universe that is regularly returned to when info optimizations are found and applied to distribute info again, where 'organization structures' (like patterns that occur in random value sequences) also occur by default given this randomness, and can lead to temporary organizations that often produce these optimizations which then distribute info to return the state to nothing again, and these patterns in random value sequences can identify other possible 'initial conditions' of organization leading to other universes

    - identify useful structures like 'empathy' which are connected to other useful structures, such as how 'empathy' can produce an error like 'bias' if applied with other 'biases' (like 'locality', where 'empathy applied only locally' might produce 'biases toward a local group that is the majority locally') where it can also produce useful structures like 'complete/deep understanding' if applied 'absolutely locally' (as in 'to one system' rather than to 'local systems in general', which could be useful for understanding systems in general, where this 'deep understanding' provides an opposing structure to more efficient structures like 'type' or 'local interactions' as a representation structure and can be created by combinations of these more efficient structures)

    - identify useful structures like 'optimizations' such as 'incentives' applied to specific useful structures like 'attention' (meaning 'attention incentives' like 'distractions') which can optimize for intents like 'routing resources in a direction to centralize/distribute resources', such as 'clearing a lane' in the 'driving' problem space by 'creating distractions that require being in the other lane'

    - identify useful structures like 'requirements' as 'reflections of real structures like real differences' such as how a 'structural pattern' (like a mnemonic device or a structurally similar word) can only be used so many times before it creates errors like 'over-use' where the set of structures that use it is large enough the the structural pattern isnt unique anymore, and another structure must be identified to use as a 'structural pattern' to apply to create useful changes, a set of 'required different structures' that reflects real similarities/differences in real info in real systems

    - identify useful structures like useful solution metrics such as 'sensitivity', which is a structure with high/extreme output from low-cost input, which is useful for 'magnifying/extreme/differentiation' intents such as 'identifying certain/extreme/obvious error structures from an initial/trivial error signal, errors that are possible to identify by magnifying changes like scaling the signal'

    - identify connections between useful structures like 'risk/cost', such as how 'minimizing risk' doesnt necessarily correlate with 'minimizing cost', which is useful to identify as a possible error if 'cost minimization' is a solution metric

    - identify useful structures like 'optimizations made possible by applying variables like "scale" to error structures', such as an 'over-reduction' error can produce optimization structures, like identifying irreducibilities more adjacently than other structures

    - identify useful connections like connections between useful structures like 'neural networks' and other useful structures like 'clocks' and 'input interfaces like senses that store/process overlapping information but process that information in different ways' which can be used to derive different filters which are 'complementary/overlapping/compoundingly useful' as opposed to being 'equivalent alternates', such as how sight/touch/hearing can all detect overlapping/equivalent information about 'position changes' but each of those interfaces stores and uses different info/info formats (sound/light frequency/connectivity/overlaps, heat, etc), and different processing functions ('identify areas of light reflection frequency differences such as color areas', 'identify overlapping audio frequencies like echoes') to derive that same info about 'position changes', with varying info available/adjacent on different interfaces like how 'language is more variable on the sight/sound interface than the touch interface, but each interface adds information compoundingly even if there are overlaps, such as how interpretation of language is more successful, the more interfaces are available', which identifies useful structures like 'frequency' adjacently which can be applied in a useful way in neural networks
        - similarly other adjacent structures on these interfaces like 'focus' in the 'sight' interface can adjacently derive structures like 'filters' and 'functions' (like 'identify type/local interactions') which are commonly used in 'sight' info processing as a specific implementation of a 'focus/filter' function, which is useful in its specificity and approximation potential of the general 'focus' function (when you 'focus on/think about' a structure, you often identify its type/adjacent changes/local interactions as a quick reference to the structure that encapsulates a high degree of variation and functions like a definition, where its type or adjacent changes or local interactions store enough info to act like an alternate to a definition, as they are representations of the structure just like a definition is)
        - 'neural networks' are similar to 'clocks' in that they both implement standards, capture high variation, reflect reality, connect info into sequence structures, have variable difference/distance metrics, are useful for comparison of changes, etc, where 'clocks' act like an alternate info format similar to 'networks', 'sequences', 'interfaces', and 'concepts', as for example in the 'driving' problem space, 'structures moving at similar speeds' are more relevant than other structures (such as by identifying errors like 'sequences of cars that are difficult to pass' and 'inability to pass a car with a simmilar speed'), where both slower/faster cars are easier to interact with ('pass if theyre slower or let them pass if theyre faster') than similar/equal-speed cars, bc of the 'extreme similarity' in cars with similar speed, which makes it more difficult to differentiate and therefore more difficult to interact with, as everything but that car will be easier to differentiate and differentiating the observer and the other car will be more difficult as a result of the comparative ease of differentiating everything else, which will get more attention allocated by default and will be easier to determine by default
        - similarly, the senses are required to reflect the variation of reality in order to be useful, so alternate senses might be useful as well, such as a 'social sense', if there is enough variation in social structures to reflect reality

    - identify useful structures like connections between 'error' structures like 'identifiable certain differences from a solution' and 'commmon/core functions' like 'approximation', such as how 'repeated approximations can generate errors'

    - identify useful structures like connection between common/core interface structures like 'general and specific structures', like how 'general structures can be derived from specific structures as specific structures contain more info, but the reverse is not as useful bc it involves an error of a 'loss of information' and how 'specific structures can offset the over-generality of otherwise useful abstractions like how variability can explain everything but its useful to store specific interactions of variability like how it tends to be preserved across interfaces as they act like equivalent alternates in their information storage/coverage', connections which can be applied to create a 'directed probabilistic graph' where the direction favors change in the direction of specificity

    - identify useful structures like connections between core interface structures (like 'time', 'position' and 'distance' variables) and other useful structures like 'critical points/useful points/ranges/thresholds/starting points' such as how the 'time/distance to a critical point' is a useful structure in the 'driving' problem space, as there are frequent requirements to identify 'approximations of distance and time to move to a structure/point, where some threshold is relevant such as a range of criticality/importance/relevance/variability', such as how its useful to identify the 'optimal distance from a sign to the relevant structure in the rule (like "ice forms on bridge") of the sign (like a bridge)' by identifying the connection between 'distance/time' (as in 'time it takes to slow down, from distance x' and 'distance from relevant rule structure like "bridge" at time of "successful slow down" at a given "slowdown starting point"' and 'optimal distance from relevant rule structure like "bridge" at time of "successful slow down" at a given "slowdown starting point"') and the critical range where the info in the sign is useful (as in "before the critical point, after which its impossible to use the info in an optimal way as in 'slow down to prevent sliding on ice'")

    - identify useful structures like 'connections between formats and useful functions', such as how changing the 'sequence' param (as in 'switching order of a set of items' to differentiate it from 'changing position of one item' by connecting the change to its impact on the 'rest of the set of items') of a structure formatted as a 'sequence' is a common solution structure in that format (as its different from core functions like 'change position of one item' but is still trivial to identify and has higher variation than the core function and also involves higher connectivity than the core function), similar to how 'changing similarity/connectivity metric of a network' when a structure is formatted as a network is a common useful solution structure in that problem space of optimizing networks (a function that is not a core function in that problem space but is still trivial to identify and is high variation and related to connectivity)

    - identify useful structures like 'connections' between 'optimizations' and 'error structures' (or 'errors and other errors') using useful structures like 'scale', such as how 'lying may be optimal in some cases' but its useful to identify errors that can occur if that optimization is applied at an extreme/scale, such as how 'gaming speed traps' can be easier than always obeying the speed limit (which requires functions like 'forcing attention to speed' and 'forcing a limit on a default like speeding'), but if applied at scale or at every possible opportunity, 'gaming speed traps' has errors like 'future costs' such as 'more iterated/recursive speed traps' or 'lost time/resources from traffic violation tickets' and 'cost of ineffective monitoring/enforcement' and 'impact on other regulations and the system of regulations, given the lack of enforcement of a particular regulation', which is likely to create errors like 'suboptimal system states' like 'lack of rule of law'
        - similarly, other errors are likelier when applying the suboptimal strategy, such as 'over-use of brakes' (which tends to occur in common cases like 'at any non-constant speed') which is derivable from the other error structures bc the 'variation in speed' of a '"gaming speed trap" driving pattern' is unlikely to match 'required/optimal variation in speed, given other metrics like normal/optimal brake usage patterns, like only braking at stops/red lights and to slow down enough that a constant speed is maintainable'

    - identify useful structures like 'possible errors' such as 'input blocks' which can be 'connected' to other useful structures like 'optimal/suboptimal cases to apply a function (like the "pass-car" function)' and 'possible similarities' like how an 'input block' (blind spot) could be similar to the 'size of a common structure in the driving problem space like a car, depending on distance from observer' and could be in a 'position that blocks the input of the car', which is a 'suboptimal case to apply the "pass-car" function'

    - identify possible alignments that represent useful structures like 'overlaps between multiple metrics' to fulfill intents like 'check map' and 'dont crash', where specific intents like 'keep eyes on the road' are useful as approximations of those general intents like 'dont crash' to create opportunities for useful 'overlaps' like 'align "phone position" and "input field" (by creating a similarity between phone position and input field and identifying that its not required to take eyes off the road in order to look at the phone bc of large background visual clues which are easy to identify, bc of peripheral and unfocused vision, and the size/color of the objects in the background)'
        - from this example it can be determined that in general 'overlaps' are common useful structures in problem-solving, partly bc of the 'structural similarity' and the 'adjacent difference' between the overlapping structures, which reflects a workflow such as 'change a base solution (using adjacent changes, given the semi-correctness of the base solution)' and which encodes other useful structures like a 'rotation'
        - this connection between the structure 'overlap', the workflow 'change a base solution', and the common useful function 'rotation' is useful to identify, as it reflects that common useful structures are adjacently connectible to solution automation workflows and other useful structures like 'core interaction functions' on other interfaces like the 'math' interface

    - identify useful structures like 'types of vision' and their connection to other problem spaces like 'neural networks', such as how the 'light' problem space has problems that are solved by 'vision structures (like eyes and types of vision and light receptors)' so that 'solving a problem of vision (interpreting information formatted as light)' is related to 'solving a problem of cognition', with related functions like 'focus/group' which map to other useful structures like 'filter/combine'
        - for example, 'night vision' is useful for identifying 'heat' which is a proxy for information such as 'location of useful structures', which is a different info format to detect other than standard visual inputs like 'moving light structures', as the 'temperature' interface reflects similar variation as the 'light' interface, and different types of vision can detect different information (that can have an overlap with information detected on other interfaces), where 'vision types' act like interfaces in this problem space
        - similarly, the 'peripheral vision' interface identifies information like 'background/unfocused information' as a 'bet-hedging investment strategy, to account for the uncertainty of possibility of the case where current focused objects dont provide sufficient information for intents like survival'

    - identify useful problem spaces to apply as problem formats which are useful in standardizing a problem and other interface structures, such as the 'light' problem format, which is useful for finding interface structure interactions such as new variables by applying simple functions and other useful interface structures in that problem space
        - for example, applying 'mirrors' as a 'symmetry' structure to 'find new variables' is possible by applying workflows like 'trial and error' to different 'mirror' structures (like 'angle', 'count', 'position', etc)
        - similarly, connecting structures like 'symmetries' to other useful structures like 'filters' is possible by applying differences to the 'mirror' structures that create a 'filter' effect by allowing some light to pass through
        - similarly, identifying 'extreme differences possible in extreme similarities' is a useful structure that is adjacently derived in this problem space, such as 'creating opposites (like shadows/colors) from light similarities like reflections'

    - a default implementation method is to use existing workflows in this repo as configuration and writing a function to apply a workflow to an 'input problem space'
        - remaining questions are variables like 'which combinations of which variables of reality can be optimally applied as equivalent alternate defaults forming a core interaction level (used to adjacently derive the other interaction levels)', which is semi-defined but still unresolved (as not completely specified) by other workflows/implementation methods

    - identify useful structures like 'accuracy/truth/realism/other solution metrics' that identify other useful structures like an 'error in other metrics'
        - for example, if some statement is 'technically' true, and 'only technically' true, that indicates that other truth metrics (like 'priorities', 'optimality for general intents', etc and other metrics of a statement) are extremely false, so much so that the net effect of these truth metrics is false, as the 'network of truth metrics' is more useful to apply with queries of the network (a structure that answers the question 'which probabilistic sequences/combinations of truth metric network nodes are useful to apply, in what cases')

    - apply structures like 'variables' which can be applied to other useful structures like 'probable filters' (as in 'activate/deactivate' probable alternate filters to resolve the uncertainty of which filter is correct) to fulfill intents such as 'apply an uncertainty as a variable rather than a constant' and 'try every probable variable value to find the correct value and find overlapping/otherwise similar values (once probable variable values are filtered into a solution subset, at which point "trial and error" becomes useful again)', to apply insights like 'some workflows are only applicable/useful in some cases (like "low input count")' and 'optimizations of workflows like "trial and error"  include applying additional functions to connect different solutions identified as non-errors by some similarity metric, to identify solution filters that produce similar/equal solutions'

    - identify structures that its useful to identify, like how its useful to identify 'limits on areas/ranges of usefulness' (like how a 'cloud' can be useful for determining 'direction' up to a point as in during a very temporary time interval, where the cloud moves) just like its useful to identify 'limits on error ranges', as these are useful to filter inputs to specific ranges but also to identify common ranges of usefulness/errors of inputs and other interface structures of these limits, as 'limits' are a problem-solving format just like 'filters' are, though they are often more useful when combined, as interface structures are compoundingly useful

    - identify optimizations such as using 'local info' that is more useful for being more specific or more real/accurate when used vs. expert/technically correct but irrelevant/useless info (such as 'names used in actual local signs' vs. expert/technically correct info like 'the numerical number of a road that is only known to a map database but is not used in a local road sign', or directions involving expert navigation knowledge like 'move in the direction of a neighborhood' rather than knowledge that is accessible to most users like 'move south'), through removing requirements to acquiring info (acquiring info about 'which direction is south' requires less steps than acquiring info about 'which direction is a neighborhood in')

    - identify useful structures like 'false equivalences/constants' between useful structures to identify errors in, like 'errors in reasons for a variable change, which are not reflected in approximating metrics' such as where a reason for an approximating metric change (change in 'estimated route time') is assumed to reflect only and exactly the corresponding reason in the approximated metric ('distance to destination') even though the approximated/approximating metrics are not equivalent as there could be other reasons for a change in the approximating metric, as a variable like a 'estimated route time' can vary for many different reasons (like a 'traffic jam' as opposed to a 'missed turn'), and assuming that the actual relevant metric being approximated (the 'distance to destination') is equivalent to the change in the approximating metric ('estimated route time') is false as these metrics are not equivalent alternates but are useful approximations of each other in some cases which are likely to occur (like where a 'general estimate' is more useful than an 'exactly correct estimate')

    - identify useful structures like 'directions to move in which have no immediate errors', which are useful to apply as default variables (like default changes to a base solution) until errors are identified in those directions, at which point its useful to connect existing error structures to that error point once found, to find other error structures like 'error thresholds' and 'error areas'

    - identify useful structures like variables such as 'scalability' that cause problems like 'inefficiency' such as 'lack of specific efficient identification of a useful item in a set of many items', like how 'ill-defined structures (like overloaded terms)' and 'disorganized structures (like multiple words for the same concept)' and 'non-adjacent definition (conceptually distant) structures (like applying concepts like variability and interactivity and similarity without storing specific useful implementations/variants of these that are known optimal in general or for specific purposes)' are another cause of problems like 'lack of understanding', to fulfill intents like 'find useful problem-solving intents such as "find opposing structures of these problem-causing structures to solve problems"', and similarly other concepts like 'complexity' are frequently causative of problems with known optimizations to resolve those conceptual problem causes

    - identify error structures like 'input blocks' which can be used to identify other error structures (such as other errors that are likely to occur, as they are causally adjacent/relevant to those error structures) or solution structures (like 'structures to avoid errors like input blocks, such as alternate structures that appear to be other structures so when inputs are blocked/removed, these alternate structures can be identified/derived, such as alternate information-producing structures like probabilities (like "there is probably a car within crashing distance") or requirements/certainties (like "there will probably be a car in the opposing lane at some point, with such certainty that its like a requirement"), or such as false similarities that can identify alternate structures, like how a flat/linear-seeming surface can actually be a set of hills/waves which is only possible to confirm nearby or from a different direction'), where combining these error structures with these requirements can identify other error structures (like how a 'false similarity' such as a 'set of hills that looks like a flat surface can hide a required/probable structure like a incoming vehicle in the opposing lane while scaling a hill producing an input block like a blind spot which could produce an error like a crash if another useful function like "pass car" is applied in a case like "two-lane highway"')

    - identify useful problem formats like 'navigation' which can be applied across structures once standardized to a format like a 'directed network (such as a probabilistic network where more connected nodes are more probable to occur from each other, or where ambiguity-resolution functions are organized by probability of usefulness, as ambiguity-resolution structures are alternate structures of filters that can be used to fulfill the decision/selection function)' which have known 'optimization' structures on the structures of that problem format like 'path decisions' (like 'path decisions that enable other path decisions, rather than leading to possible error structures dead-ends, which involve work to correct, where staying in decision-enabling loops is usually more optimal through the interactivity/connectivity enabled by those structures') which can be applied across problems formatted that way (like a network to avoid over-specifying solutions, where over-specific solutions are less interactive/connective with other routes on the network, which is useful to identify in cases like where some problem shouldnt be solved with a specific solution but where solutions should be changed/tested regularly to avoid over-investing in one solution bc the uncertainty of the problem isnt resolvable with available information), a format that is useful for finding structures like 'alternate routes', 'points in a route where errors occur or where errors become inevitable/required', 'common nodes/patterns/structures of error/optimal routes', 'reasons for route suboptimality/success', 'structures like directions that are useful to vary or move in', 'alignments between structures like directions and intents', etc
    - relatedly, its useful to identify specific useful ambiguity-resolutions, like differentiating errors from solutions, error directions from errors, errors from variables/differences, etc

    - identify useful structures like function sets that can be used to solve or improve errors in many problems, such as 'identify when constants are optimal such as when constants provide stability and reliability which are useful for basing changes on' and 'identify structures that can create constants (like how slowing down can create a "constant" in the safety variable in a case like rain)' and 'identify when constants arent optimal or arent required or when its required to change the constant', such as in the 'driving' problem space where a 'constant speed' may avoid errors like 'over-use of brakes' and the 'rarity of a payoff of a speed-up except in cases like where next car is far-away', but which create errors in specific cases (like where the constant speed is zero or where constant speed of the first car in a sequence slows down other cars behind it or cases where speed-ups and slowdowns are more optimal, such as how slowing down is more optimal in cases like 'rain' to replace/re-create the usefulness of the higher constant speed in the 'rain' case)
        - this type of difference required to solve a problem ('change a constant into a variable') occurs in cases like 'where existing constants/functions dont fulfill requirements' (where there is a misalignment between existing and required solution structures), as 'injecting a variable' is useful in workflows like 'change a base solution' (as a base solution is identified and identifiable as suboptimal, as in different from a solution structure), as opposed to other workflows like 'reduce problem complexity (where complexity is the cause of the problem)'

    - identify useful connections like how workflows like 'trial and error' can identify other useful structures like 'convolutions' as in 'try every permutation of combinations/other interactions of attributes across two items' and 'neural network interaction functions' as in 'try every adjacent change combination, until a threshold of error minimization is reached')

    - identify useful inputs to intents like identifying 'interesting (as in high variation, or interactive between high variation structures) questions' that can be used as an input to queries like 'if a question/query is useful' (interesting questions that involve some interaction between a high-variation concept and a useful structure like known/available structures like 'neural network node sets', like 'if neural network structures like synchronicity is required for mass-coordination of neurons and whether this provides a communication function between neurons')
    
    - identify structures that can make other structures useful like optimization structures such as structures that can make 'risk' (uncertainty, ambiguity) useful, such as in cases where there will be immediate feedback indicating a possible payoff, where the change is reversible, etc

    - identify useful structures like variables that, when changed, can change other useful variables (like how 'slowing down' can help find information farther away, through avoiding error structures like 'input blocks' which can block far-away information)

    - identify useful structures like when useful structures (like 'high-variation variables') are not useful/relevant which can occur in cases like 'complex systems where many sub-systems can operate semi-independently of each other and therefore irrelevantly'

    - identify useful functions like 'invest' which have related adjacent useful concepts like 'payoff expectation by some threshold' which are useful for finding useful applications/specifications of analysis functions like 'cost/benefit' analysis
        - similarly, a useful 'investing function' is to 'invest in cases with a high ratio of equity', which is like 'investing in cases with a high information content like a useful prediction model or rule database that is already acquired which can be used as an input' (investing in higher-certainty structures), which is an example of how the rules of investing can be used to solve navigation/filtering problems, as investing involves similar information problems

    - identify useful structures like cases where error structures are not errors
        - such as when irreversibility is not an error but a useful intent to some agent, like an irreversibility (in maintaining some useful structure or another useful state change that applies some absolute truth to avoid applying that truth as an uncertainty by making sure its sustained)

    - identify the 'degree of difference/variation captured' and 'meaning of the difference' and other difference structures necessary to determine/identify/generate useful structures like 'cross-interface structures' and 'equivalent alternate sets of concepts like "spectrums of opposing attributes" (like chaos/organization, truth/falsehood, real/unreal, impossible/required, certainty/uncertainty, stability/instability, power/powerlessness, cost/benefit, constant/variation, embedding/containing, similarity/difference, adjacent/distant) and "similarities between attributes" (like potential/chaos and potential/energy and potential/power)' which are sufficient to solve most problems, since interface structures of a difference can fulfill intents like 'determine other useful variables and their interactions'
        - for example, different sets of concepts can solve most problems, just like cross-interface structures can, which is useful in that they have similar differences captured in the interactions of these high-variation structures
        - this means that most problems for agents involve differences between 'mental functions/attributes (like scalability)' and the 'complexity of systems they interact with' (the problem is that the human brain cant process a lot of information at once, not that the problem is particularly complicated)
        - however more complex methods are necessary bc the adjacent solutions found by existing algorithms are usually suboptimal in ways that are not identified by the algorithm and are not obvious/trivial/default interpretations of the solution (a 'solution found by an algorithm to solve a systemic condition involves a compound that switches off an immune function, which causes another condition not solved/identified by the compound, bc the algorithm doesnt connect all variables adjacently, just the inputs/outputs of the data set')
        - identifying the 'degree of variation required to solve most problems' can filter the set of functions/variables that should be included in a solution-finding method or solution
        - existing simple interface structures solve 'medium-complexity problems' which neural networks are frequently trained to solve, which are mostly a problem caused by 'scalability limits of the brain'
        - more complex interface structures on the outer layers of a generative function set diagram solve more cerebral problems like 'logic puzzles' which require more complex structures kept in working memory to make trivial to solve, like 'organization/optimization functions', which can allow other required mental functions for higher complexity problems like 'identify new concepts/similarities/variables' and 'identify unifying systems/structures which interact in a new way to explain/solve a complex problem'

    - identify useful structures that occur in other systems (like the brain/bio-system) which could be related to optimizations of useful structures (like neural networks)
        - for example, the 'blood flow' in the brain changes how the brain can be and is used, and similarly other neurons can evaluate how other neurons are doing, which implies that there should be extra neurons kept in a neural network for evaluating the progress and connectivity/interactivity and similarities of other neurons just like extra neurons are capable of creating scalable mental functions (associated with intelligence) like 'evaluating a culture' which requires a lot of extra variables so there would need to be free neurons to handle evaluating complex interactions like that
        - connecting the 'neural structures' is useful to identify existing biological mechanisms for brain structures like functionality and errors which can be used for optimization or avoided once mapped directly to neural structures)
            - 'blood flow' (as "a force acting to group neurons' interactions and enable other functions like reward/pain signal delivery and waste removal")
            - 'electrical activity/conductivity/other electricity structures' (as 'modifiers of relative position through modifying speed of connections')
            - 'neuron connectivity/presence/activation' (as 'usage' and 'available resource' structures)
            - 'waste removal and immune functions' (as a 'de-noising' structure, to get rid of 'noisy/irrelevant' information structures like dead-ends, unused neurons, outliers, over-specific parameters, and other error/suboptimal structures)
            - identifying other error structures like in order to 'trigger a useful process that creates rewards', there needs to be a 'lack of rewards, if this process checks for that'
            - other structures with high-level general states like 'emotions' (encoding them as defined in neural structures like defining 'depression/mania' as a 'wide-spread, mass-scale de-activation/over-activation of neurons, especially neurons which connect to other subsets like high-traffic subsets' and mental disorders like 'schizophrenia' (like 'forced separation of neural groups which are activated one at a time, forcing each group to develop all useful mental functions as equivalent alternate brains'), as well as 'solution structures of those mental disorder states' such as 'connection/variable' structures for 'autism/schizophrenia', 'positive/variable/opposite' structures for 'depression', 'boredom/constant' structures for 'mania/bipolar', etc, and similarly connecting 'personalities' (as 'brain usage structures like creative/intelligent personalities that optimize for some metric like novelty/boredom') to these other brain structures like 'brain disorders', 'default brain perspectives', 'brain connection units (as repeated structures in the brain, like common connection functions between neurons like "check for related specific variant nearby")', 'brain optimizations' (like commonly useful solution structures to brain disorders and functions to build multiple different brains to use to evaluate each other to implement default power structures like solution structures like checks/balances which offer 'equivalent alternate powerful structures that are connected by some cyclical dependency'), and 'emotion states' (as 'outputs (as outputs of a reward/cost assignment function) of structures common to various personalities and the resulting/associated perspectives and disorders')
                - similarly, identifying other possible useful built-in functions, such as how depression is like a sedative on excitatory responses, which if maintained by survival (the emotion is handled with variables to produce survival), can lead to more optimal thinking (as an alternative to acting), to stop manic/excitatory responses and prioritize slower/deeper thinking to resolve the emotion by identifying variables & identifying variable interactions, where this less hyperactive/excitatory state is similar to 'brain exercise' in that connections/movements are made more difficult so mental strength (such as through highly useful and distributed sub-networks) is required
            - identifying possible errors with connections/interactions between these mechanisms is valuable to identify mental/logical/other brain usage errors and opportunities/solutions for optimization of brain structures, such as how 'empathizing with people with depression/other brain disorders can produce similarly negative signals, if repeated enough, and these structures could become permanent as they might create a cascade, if repeated above a threshold value', and 'brain rewards are required as inputs for the bio-system to continue to exist, rather than always being a significant signal of some successful mental process'
            - identifying possible optimizations (like 'repeating a particular function consciously to adjust how structures are connected/stored in memory', such as 'only focus on new difference types and how theyre different from known difference types (or new solutions), and ignore other variables/structures', which could create a brain that only sends success signals for new variable types identified (or 'sources of high variation' which are likely to produce these new variable types), which is highly optimized for a problem-solving intent like 'identify new variables') to suboptimal memory storage structures (such as 'always connecting memories back to the same problem or same system', which might not be a sufficiently high-variation system to contain all useful variables or might contain so much unnecessary information that its not useful as a mechanism to organize information)
            - other structures like 'optimal neural states' such as 'mental flow' which is characterized by high connectivity, frequent successful usage of the system so its completely imported into working memory, organization such as applying probabilistic connections to base more trivial changes around core probable connections, and lack of barriers to connectivity such as random noise, while still allowing for variation in inputs, which are optimally routed in this sub-network of working memory enabling states like 'flow'
            - other errors such as 'surprise even when the input is normal, bc its necessary to believe falsehoods (trick the brain) to survive shockingly negative frequent truths, so normal inputs continue to trigger surprise'
            - other errors that could be formatted as 'neural states' like 'loss of executive function' (where the 'outer neural layers and available neurons enabling consciousness and executive control are disrupted or used for other tasks as a result of required functions or neuron loss in other regions, where alternate executive function structures take the form of interim networks that connect and control connections between sub-networks like between available neurons and task-specific grouped neurons like memory storage neurons and working memory neurons')
            - 'memory' (like a 'directed network of neurons with similarities in interface structures like patterns and concepts that efficiently store most useful information', as well as 'working memory' as a 'set of available neurons distributed among neurons used for specific frequently used information' and 'recent memory' as a 'set of neurons storing recently encountered info as a proxy for relevant info', and 'abstract memory' as a 'set of neurons to store commonly used general rules which are easy to remember and frequently used and capture high variation and are abstract enough to be generally applicable' and 'physical memory' as 'neurons spread through the bio system to store repeated sensations and connections between them', and 'neurons which remember feelings in the brain like feelings about blood flow as a signaling system to trigger other useful processes, like immune processes where blood flow is inadequate' and other memory types)
               - similarly, 'memory-finding functions' like 'think about (apply functions in, apply differences in) context when other related info to the memory is known, like where a memory was originally stored, in order to find the memory'
            - 'brain waves' as a 'synchronization process where neurons are required to temporarily coordinate in some different interaction pattern like a different neuron/memory access pattern, with each other and with other neuron groups coordinated by some other wave or interacting with themselves as theyre pulled in a different direction by the opposing waves, for functionality like "locally distributed scaled interaction functions fulfilling some general intent, like regulating/contradicting/limiting some other waved/mass-coordinated/temporary process"'
            - similarly, connecting 'biases' with neural structures like 'dedicated subsets of neurons for a particular intent (like a memory network storing recent information) leading to biases (like recency)' and 'locality/self biases (through sensory inputs about local information being highly prioritized in the brain)' and 'similarity biases (like familiarity (as in self-similarity or recent/experience/memory-similarity) to encode minimal new information)'
            - similarly, memory formats can be useful to design a neural network using a mix of memory formats
                - like a 'concept and a structure indicating a variant of that concept when applied to the concept', such as deriving the concept of 'austerity' through concept of 'restraint' having applied a related structure like 'antidote' which is structurally similar but not semantically related, where the concept encodes the semantic relevance and the structure encodes the structural similarity needed to connect the input query with the output word 'austerity' which is a specific variant of 'restraint' that is useful to store in memory for various intents, as opposed to only remembering the concept of 'restraint' or 'limit'
                - similarly, identifying optimizations to memory storage mechanisms, like 'abstract' (abstracting 'antidote' to a 'solution') or 'connect unrelated structures which are otherwise useful' (like connecting 'austerity' to unrelated but otherwise useful words like 'antidote' by connections like 'austerity being anti-equivalent to a solution like an antidote' which has layers of relevance)
            - similarly, attention-maximization mechanisms such as 'emphasis' are useful through applying known attention insights (such as 'emphasized words are paid more attention to' by applying the variable of 'cadence') to solve attention errors like attention deficits such as might occur in cases like 'lack of risk' (where normally 'risk' is the trigger of attention, but its still useful to pay attention in 'cases with low-risk' as in 'boredom' structures, cases in which attention is difficult to trigger), and similarly other structures can be applied to solve the problem of 'lack of risk' (like how 'games' are by definition 'low-risk' but still incentivize paying attention through various mechanisms like 'accessible rewards' such as 'imagined/otherwise false risk'), and similarly other solutions like 'saying thoughts out loud or writing them on paper' as a way of 'forcing paying attention'
            - similarly, brain states like 'boredom' can be applied as a solution metric for the 'neural network architecture/algorithmm search' problem, as both an optimization structure (a 'bored brain' is a brain that has solved some problems or reached a case where there are no problems to solve, so that case should be applied as a possible optimal input case) and an error structure ('boredom' can lead to other errors like 'lack of creativity' and 'increased capacity to try sub-optimal/error cases/functions to find structures like variation/problems/other complexity structures which may be suboptimal')
            - 'bias' structures are useful to identify, as they optimize for some metric, like how 'bias toward interactive structures which can be interacted with' optimizes for intents like 'adjacently identifying useful variables for local intents' and the reasons why these biases are optimal in some way, such as that the 'bias toward interactive structures is related to the local bias' and the 'interactive bias is useful for local intents' and 'its also more useful to optimize for local intents in some cases like where error interactions are probable such as when opposing-direction vehicles are passing each other, in which case its justified to do more computations regarding local structures like local agent intents/inputs/errors'
                - similarly, bias structures are useful in that they can often be adjacently combined in some network where they offset each other's over-prioritization errors (similar to how combinations of high-variation/abstract/otherwise useful concepts like 'power/balance' can be adjacently combined in a network to offset each other's over-prioritization errors)
                - other structures should be applied to offset different error structures than 'over-prioritization' errors, as this is just an example of a 'solution structure to that error' and the other structures that solve other errors should be applied in combination with this one solution structure
                - relatedly, its useful identify useful structures like 'errors of brain mechanisms like biases' such as how 'errors' could seem like a 'requirement/certainty' if a function is applied to 'identify errors' repeatedly, so that the existing brain mechanisms like 'memory' misinterpret it as a 'certainty' structure through its commonness/familiarity/recency/repetition

        - insights about brain usages/interactions/functions through connecting structures like 'feedback' are similarly useful to apply in an optimized neural network
            - the insight that 'existing reward mechanisms are too easy to game/rig/guarantee to incentivize complex problem-solving' can be applied with a solution to that problem (such as 'connect reward mechanisms to problem-solving processes' and 'inject variables like problems in an organized structure like "sorted by increasing complexity" to require problem-solving processes')
            - similarly, 'existing reward mechanisms' dont account for error structures like 'randomness' (rewards can occur randomly, which can be useful for learning complex variable interactions leading to false randomness, but this is not incentivized, as the reward already occurred prior to a decision to identify the falsely random procesess that resulted in the reward, and the reward may seem too random to examine for possible stable variable interactions to identify) and other 'negative' structures like 'corruption' ('rigging/falsely guaranteeing rewards') resulting from 'existing reward mechanisms'
        
        - in reverse, connecting interface structures to possible neural structures that could fulfill them
            - the 'insight' structure in the brain is related to a structure like 'liquid waves' (which could be related to blood flow or electricity or brain waves), which are evocative of a crossing of a threshold, above which momentum is such that it causes an 'organizational cascade', similar to how once the threshold between function layers is crossed (by identifying/generating new structures on the outer layers), the outer layers provide more complex structures that capture higher variation, which can be used to trivially organize structures on the inner layers (an 'organizational cascade' occurs when the new complex variation-capturing structure is identified, which easily organizes other structures), where this momentum is required to identify some new structure
            - 'derivation' structures like a 'mental simulator as a set of neurons that is regularly emptied, so different intents can be fulfilled by importing different structures like "different available core structures" regularly to connect known structures in a different interaction function to find new structures' (interim thinking)

        - also other variables can be injected in neural networks (like 'constant once identified as useful for success' as opposed to inactive/active, as a neural network node type that doesnt get updated once its identified as a contributor to some successful change during training, which is likelier when variable-sized changes are allowed by the update algorithm as opposed to prioritizing adjacent changes)

    - identifying useful structures like 'solution filters' (as in 'checks') that can be applied to solve specific problems
        - for example, identifying that the earth is not flat (a 'default assumption') is a matter of identifying useful filters like:
            - identifying required connections (if the earth is a different structure like round, there would be people who had traveled in the same direction around the world, or from connective subsets of the path)
            - testing a core point on a unit structure ('if a round ball can support a structure on it once the structure is scaled down to be sufficiently small, the earth theoretically could as well') that disproves a core counterpoint (the 'flatness' would be the 'foundational structure' required for structures on it not to fall over, would be the default argument contradicting a round earth)
        - relatedly, alignments tend to resolve ambiguities, such as how alignments between function intent and usage resolve the ambiguity of how the function is optimally used

    - identifying useful structures (like existing solutions and variables/causes of them) to connect with other useful structures (like new solutions)
        - for example, other useful inventions solved a problem that reached a threshold where it was common and there were tools available to fix it if enough manual labor was done, addressing some problem of reality like 'scaling the functional hours in the day' (light bulb), 'reaching enemies farther away' (gun), 'increasing the speed of information transmission and of vehicles through more explosive but controllable materials' (telephone, engine), which were problems of scalability, which is a core structure of most problems, and which created other problems of scalability like 'removing pollution from lungs' and 'verifying transmitted information faster than false information can be distributed'
        - predicting new inventions is a matter of identifying problems that would be solved with more scalable structures of existing solutions
        - similarly, predicting new inventions (and their suboptimalities/errors/optimizations) can be done with other relevant high-variation variables like 'agent personality', such as how identifying error structures in a personality like a 'false assumption/constant' or an 'over-prioritization' error or a 'self-contradiction' error or an 'over-used core/interaction function' error can be identified from a personality, such as how a 'hypocrite' personality reflects an error of 'local bias' (specified through a structure relevant to neural networks of 'avoidance of applying analysis/optimization functions to the neural network itself that produced this error', so an 'optimization structure' of a 'neural network' involves including a function to handle this error structure), these personalities involving variation around core types (based on core personality interaction functions like 'judge/analyze/criticize', 'test', etc), just like intents vary around core intents (like 'safety', 'risk-minimization', 'scaling', etc), as personalities are 'optimization' structures of 'brain usage' structures (as in a 'style of brain usage' since its a variable like 'driving style' as not every type of interaction with the brain is required/constant) that solve some problem (like personalities related to brain disorders like attention deficits/hyperactivity that account for these brain errors, such as 'saying thoughts out loud to force paying attention to them', where the personality results from some usage of this solution to the brain error, such as being particularly talkative/social through speaking more, or more thoughtful as a result of paying more attention to thoughts with corrective measures like 'speaking out loud or writing thoughts down'

    - identifying useful structures like 'repetitions' and 'scaling' and 'core changes to core structures' that can adjacently identify other useful structures
        - similarly, useful structures are useful in connecting other useful structures, like a function (like a spiral, as a 'repeated change that creates other change structures like change rate changes') that intersects with and therefore connects every useful function set layer in a function layer diagram, a connection from which other functions on that layer can be derived (similar to how a simple straight line that abstracts away some attribute/functionality with each step could intersect it if the layers are known/understood or defined/required to be organized by being increasingly abstract), and similar to how 'core cross-interface structures' have a 'vertex structure (of two vectors with an endpoint in common)' on the system layer diagram of interfaces and may also have a 'similarity in vertex angle', if the graph is organized by a similarity of the core interaction in these core cross-interface structures, which would be an example of a useful organization method for identifying other useful structures using that similarity
            - relatedly, structures that occur across problems like 'spirals' can be a different type of useful structure in that they can form vortexes, which is similar to how interfaces attract change
        - similarly, identifying 'core changes to core structures' (like changing position to top of a stack) can identify other useful differences like 'scaled' structures (like far-away structures, like to 'identify water from far-away by scaling a mountain, or identifying clouds that produce rain and cloud movement direction, or identifying lower elevation areas as likelier to contain water and identifying errors of input like false equivalences to identify lower elevation areas even when blocked by an input block like a physical barrier, or identifying concepts like "diversity" which map to useful structures like "variability" which can direct attention, such as through "searching for differences from the current error state, in variables like color" such as identifying green areas which are likely to have water, or concepts like "connectivity" or structures like "networks" to identify "underground networks of water" as a structure to search for or stay within some range of, or identifying plants likely to have water such as through "functional requirements (like water storage)" which adjacently identifies "thickness" as a useful variable to identify, and similarly identifying the "opposite structures of water" (such as a mirage) which can be identified through moving in a way that is not "moving closer" to filter out default errors such as that 'water evaporates by moving closer to it' where 'motion other than moving closer (like moving position of inputs but not position of agent)' could also disrupt the mirage, but would filter out the error structure)
        -  identifying core differences that can be repeated and can interact like scaled life forms to identify other scaled structures like causally distant (past/future) structures (such as identifying that 'dinosaur fossils' could exist by scaling the core structure of 'dust deposited from air' and 'scaling size of existing animals, which might exist at a different time bc of coexistence impossibilities', which is useful for identifying other useful structures like 'asteroids' and 'evolution')
        - finding the full set of example structures that can create surprising real differences when combined trivially is a useful problem-solving intent

    - find similarities in workflows with similar solutions found so different workflows can be found and checked for different probability of finding diifferent minima of the error function
        - similarly, finding overlapping optimals can identify positions without overlapping optimals
        - relatedly, identifying 'variable sets which can be combined in any way to still produce a solution' is a useful set to find, as is a 'variable set that can still produce a solution when an item from some set of terms is added/removed', just like finding common similarities between solution functions (like similar high-power terms) is a useful intent to fulfill
    
    - identifying useful structures like slowdowns/constants as proxies for minima in the error function, and identifying reasons for these slowdowns/constants so they can be connected and differentiatied as needed for various intents
        - various limits can produce a slowdown in other systems which are useful to identify (variants of a limit, as in a 'limit on change potential')
        - identify 'areas of slowdowns' is more possible with some functions than finding 'minima' and these structures fulfill similar intents
    
    - identifying useful structures like connections like that cross-interface structures can be equivalent alternates or symmetries used to build maps and similarities or build complementary info usable to build full sets of differences, to connect very similar or different info usable to solve a problem, which is why cross-interface structures occur in solution workflows, as these cross-interface connections are highly similarizing/differentiating as they connect differentiating structures
        - for example 'function-intent' and 'position-speed' are useful differences to connect and apply in a set, such as how its useful to regularly apply a function to 'check for a more optimal interaction with a core variable (like position)' to connect a function/variable with a useful structure like an optimal usage intent (such as a function to 'find an opportunity to change relative car position' to fulfill intents like 'pass a slowpoke' which can be a required intent to add to an algorithm that accounts for speed/position/far-away change predictions, as 'changing relative car position' is so useful it may as well be a required function to include with any 'driving' method), similar to how knowing the intent of a question can evaluate whether an answer should be found (whether the function to find it is useful to use)
        - similarly, identifying useful intents is useful, such as 'connecting local error minumums with generally useful functions' and 'connecting useful differences in definitions', like how 'function network' is an inadequate but simple description of ai that is more useful when grouoed with intents to solve for, like 'connect many changes to one change' and 'connect complete changes to composing changes' to fulfill intents like 'identify which changes (of the many used to create a particular solution function) are most useful in a particular successful change of a solution function'
        - similarly, 'problem-solution sets' are high variation in that they by definition 'connect differences in a useful way' and can be applied as a structure to solve other problems as a result of this variation-resolution
        
    - identifying useful structures (like differences from interfaces and causes/components of interfaces) which can be used to identify other useful structures (like interfaces)
        - identifying that symmetries are a form of irrelevance (where one variable cannot change the other), so that 'areas of irrelevance' can be identified, such as 'info that can replace other info, like equivalent alternates, so that the other info is not necessary and is irrelevant' which are possible interfaces, just like 'equivalent alternates' are also useful structures to find other interfaces, to find structures like 'positional irrelevance' (the 'position' doesnt matter, what matters is 'any membership in a set/type at all') and other forms of irrelevance which can identify alternate xtructures (like 'similarity/solution metrics') to be used, similar to how identifying alternates to 'sequences' in the 'predict the next word' problem space is useful in that it can identify structures like 'common sets like common phrases/patterns' which can take priority over or replace sequences
        - similarly, 'causal irrelevance' occurs where the 'causal structures' dont matter, what matters is 'whether some event occurs at all'
        - this form of irrelevance is like the opposite of an interface, where the structure's irrelevance indicates that the problem is solved or more solvable on a different interface

    - identifying useful structures (like 'mixes of workflows') that are other interface structures (like 'optimals' or 'requirements') in some cases
        - for example, its useful to identify position and speed but also mix that with another method like 'check farther ahead than is necessary' where that method is mixed by applying it regularly, similar to how a method to 'remain some distance from other structures' can act like an equivalent alternate of that mix, where these methods are better when mixed together to avoid their known errors like 'avoiding structures by not moving at all' where a network of comtexts could allow identifying the contexts' similarities to quickly check if a direction leads to an error
        - identifying insights like that "a 'mix of algorithms' is optimal in some cases, such as where one algorithm will inevitably be wrong at some point, so mixing it with another algorithm at regular intervals can handle this error" is useful as a set of core interface structures which are inherently useful to connect
        - for example, when guessing a route with no instructions available except a general direction to move in, 'following one road' is likely to be incorrect in some way like inefficiency or at some point like where it diverges from the optimal path or the direction that represents the problem to solve (the problem of 'find a path that fulfills this direction'), so injecting a variable like 'changing to other roads' is likely to improve on the "first guessed road" if extrapolated to the entire path
        - similarly, other insights apply to point to this insight, such as where 'structures rarely self-sustain, so assuming one structure will always be reliable is unlikely to be correct, as structures interact without being instructed to and these interactions create degradations of some variables'
        - similarly, other workflows can identify other variables adjacently through identifying variables of a problem space and the interface structures connecting these variables which could be new workflows (variables of a problem space like 'driving' such as 'interactive structure differences like surface differences and usage differences' to identify other variables like 'road texture' as important for safe driving where the solution is swerving to avoid differences in road texture if swerving is sustainable over time, and other usage variables like 'fatigue' and 'lack of requirement to use both hands when steering at all times' which when combined allows for useful optimization functions to be identified, such as 'switching hand used to steer' in cases like 'where next car is far ahead so swerving to avoid a crash is less likely to be required'), workflows such as identifying useful sets of structures like 'error-causing inputs (fatigue)' paired with 'structures of cases (where "next car is far away, so errors like crashes are less likely and more variables can be used") that include a "lack of requirement" (as in a variable, "steering hand currently used") that allows for an adjacent change (switching steering hand), where this adjacent change solves (reduces) the error-causing input (fatigue)', and similarly, identifying generative/identification functions of these variables (apply useful concepts like 'interactivity' to identify specific variants of it like 'surfaces' and errors like 'crashes' and 'usage structures (like driving style which is a high-variation variable of how the agent interacts with the driving tools)' and other useful structures like 'variability causes' and 'error causes' to find these useful concepts like 'interactivity' as a useful specification of 'variability' which is more useful to store than to derive from 'variability' every time), which is useful for problem-solving intents like 'identify new variables (for intents such as to explain a variable interaction not already explained by known variables)'

    - identify useful structures (like 'scaled interactions' and 'time interactions') which can be used to fulfill problem-solving intents like 'identify more useful workflows'
        - for example, the 'trial and error' workflow is associated with a 'multiverse' universe (as the ultimate scaled version of the 'trial and error' algorithm is like building a time machine which can test/simulate all possibilities as good as or better than reality can, so that any moment (and any sequence or other structure of moments) can be examined using the 'trial and error'-implementing program) and some structures are particularly useful to identify for this algorithm, such as 'equivalent alternates having some pattern that allows them to be derived from each other' and other structures that could reduce the computation requirements of 'trial and error' (to equate some possibilities, so that the set of possibilities considered to be equivalent to 'all possibilities' is reduced)
        - any problem-solving workflow is similar to a time machine in its reflection of the variables of reality (as these workflows connect variables in a way that all or most of reality can be determined/simulated/tested by it), but some are more optimal than others, and identifying all of these workflows which can simulate reality builds a more complete time machine (in the sense of a simulation machine) than other methods, and similarly any position where an observer has access to such an optimal machine that reflects reality accurately and efficiently is likelier to be a higher-variation position that attracts information and variation, which could bend spacetime in unexpected/unknown ways, thereby impacting other positions' ability to simulate time/reality through requiring them to simulate it similarly efficiently as the original observer or be subject to the spacetime changes created by the original observer, where the original observer has a tool to process high variation interacts that they are likely to attract with this efficient handling of variation and other observers do not, which is likely to create inconsistencies in how spacetime stabilizes/bends that are resolvable if the simulator is distributed, inconsistencies which can store other universes' information inadequately/suboptimally (other universes being likelier to be attracted to and stored in this universe, the better the variation handling becomes), so once this optimal simulator is found, it should be as distributed as possible and the distributed versions of the simulator should be differently optimized to handle more variation in information attracted to this universe, so there is a structure in place to capture it once its pulled to this more efficient storage mechanism by forces like gravity, to avoid these 'suboptimal universe storages' from occurring, and other strategies to minimize these errors should be applied (like 'inefficiency signals' should be distributed as well to avoid attracting too much information to this universe before the implications of doing so are fully understood)
        - identifying that this algorithm in its ultimate form is like building a multiverse is non-trivial, unless a program identifies the 'meaning' of a structure (such as its set of high-variation interactions in various important/relevant systems), at which point identifying this interaction is a matter of applying 'scale' and the 'physical information' sub-interface of the 'information' interface and identifying 'alternate usage intents' of the resulting scaled structure which could be similar to some structure in the 'physical information' interface (like some variant implementation of itself such as a 'multiverse')
        - similarly, identifying whether some algorithm more efficiently captures reality than reality is a matter of identifying whether there is some simulation of reality that requires fewer representations (such as fewer repetitions) while still correctly predicting every important variable interaction (by identifying priorities which are important and fulfilled by that algorithm's simulation of reality) and other metrics (like if there was such an efficient simulation of reality, it would likely vacuum information in as it would store/interact with information better than reality, and other realities that existed prior to this computer would likely mimic this efficient reality and thereby standardize and synchronize realities to that more efficient reality)
        - therefore identifying how algorithms interact with universe structures as well as other useful inventions (like blockchains and time machines and neural networks) is useful to identify, just like identifying how algorithms interact with good/evil and truth/falsehood and problems/solutions and power/balance are also useful to connect each algorithm to, as important metadata of the algorithm to compute
        - 'structures (like "sequences") of problems to solve' are similarly useful to identify, to direct how these workflows are optimally used, such as a 'sequence and distribution of databases (of structures like "task lists" and "known variable interactions" and "useful problem-solving workflows") to create, which will push civilization in the direction of some priority like fairness, organization, efficient, etc'
        - the 'database sequence (or database network sequence)' is identifiable as an available structure reflecting a 'state sequence' which is the 'core structure' of the 'physical information' interface (reality), which is usable as a component to build a path to a particular future and connect other spacetimes as well
        - what does it mean that workflows (like trial and error) relate to possible implementations of reality (like the multiverse) through similarities (like the 'computation type required' in both 'trial and error' and the 'multiverse')? it means that some universe configurations can be filtered out as impossible or less possible using these workflows and structures of them like combinations of them
        - given that multiple workflows exist and are optimal for some purpose, and there is no 'general purpose, always optimal, in all cases' algorithm (except interface analysis which produces all of the optimal algorithms), the universe is likely to reflect the structure of 'a set (and an interactive mix) of these optimal algorithms' (for example, a multiverse, but an optimized one, with several 'equivalent alternate' universes which overlap or otherwise intersect on their symmetries/common standards), where some interfaces reflect each other as they capture the same information and can be used to derive each other
        - this is different from saying that an 'algorithm operates best in a particular context' (like a 'worst-case scenario' context as indicated in the 'algorithmica' or 'heuristica' world of the 'five worlds' of complexity), as 'system context' is an input to algorithms, but there are 'absolute contexts' where some algorithms would be optimal in all cases and those absolute contexts can be extrapolated to differentiate those from physical reality
 
    - identify structures which are useful for identifying other useful structures like 'maximal differences' and 'equivalent alternates' which are useful to identify variables and thereby fulfill other problem-solving intents
        - for example, different senses provide complementary or equivalent information, usually related to some 'feedback about position change' (such as a 'reward/pain feedback' sense and a 'gut sense of nearby life forms involving microbe-microbe interactions' and a 'health sense of functionality that is working' and a 'thought sense of triggers of useful thoughts like considering alternatives' and a 'sense of synchronicity/alignment between different internal systems' and a 'logic sense of information such as time passed, which indicates survival, which indicates some useful interaction with the external world', regardless of whether the primary senses are available), where different structures can be adjacently derived from different senses and some can combine to form equivalent alternate information (useful for finding different interfaces for information) or complementary information (useful for building a complete set of cross-interface information)
        - similarly, 'connections between maximally different graphs' are useful to identify 'equivalent alternates', 'interaction levels', 'differently useful information', and other useful structures
        - similarly, the question of "identifying what is not an interface (such as 'other interfaces', 'limits', 'interface components', etc)" is useful to identify useful variables, which are useful for many problem-solving intents
        - similarly, the question of 'what errors produce useful functions in neural networks (such as concussions in an autistic brain, which might add connectivity if a coincidental angular interaction occurs with the concussion, a useful input to consciousness and other functions)' can identify 'structural errors connected to functions' in neural networks

    - identify useful intents to fulfill such as "identifying interactions between core interaction functions or problem-solving functions/intents and cross-interface structures" (like 'identify structures that change functionality of structures' and 'identify causes of different functionality in similar structures and similar functionality in different structures') which are specific variants of problem-solving intents (like 'connect different structures') and therefore are more useful as approximations or implementations of those intents (fulfilling those two identification functions could solve a high ratio of problems)
        - related intents include identifying useful interaction levels of useful structures like common concepts/structures such as 'complexity' ('high count of function usage to fulfill required intents') and 'embedding' ('structure enabled by a base structure') and 'heterogeneity' ('difference embedded in a structure, as in non-uniformity or non-unitary interactions'), which have some structures in common like interaction functions (apply/base) and cross-interface structures (function-intent, usage-intent, usage-function, etc) which indicates that other cross-interface structures interacting with core interaction functions could identify other useful concepts
        - this useful interaction level implies that identifying more advanced/complex interactions (such as iterated embeddings) is useful as a mechanism of evaluating simpler structures (identify 'embedding an embedding in an embedding' to evaluate 'embeddings in an embedding' successfully, as an 'outer layer of consciousness, where the inner layers fulfill requirements and the outer layers are free to evaluate the other layers as they can store the inner layers or queries of them in their own layer, given the ratio of free neurons available to import structures into for evaluation processes, in the outer layers')

    - identify useful structures like high variation variable interactions like specific difference structures or specific variable interactions that co-occur to identify other useful structures like 'probable variable sets'
        - identifying interactions between variables like 'difference structures' (such as 'homogeneity', 'heterogeneity/differentiation' and 'specialization', as 'highly different structures' (like different senses or species) tend to occur in real systems rather than 'highly similar structures (like high ratios of repetitions)') which tend to occur across structures, and similarly variable interactions like 'interactivity' and 'variability' which tend to co-occur (like how liquids tend to be higher variation than solids bc of the interactivity within the structure), to create other useful structures (like 'directed attribute graphs of various difference types like 'heterogeneity' where direction indicates sequence and preceding terms in the sequence indicate probability' which can be used to identify 'probable variable sets')

    - identify useful interface structures (like 'type interaction functions' and 'complementary information') which can be connected to other useful structures (like 'change inputs') which are known to be connectible to other useful structures (like 'differences in solution functions')
        - for example, attributes/functionality interact in a way that changes attributes as theyre not static constants (which is why multiple solution functions are better to identify than one in complex systems), such as how a type may mimic other types to hide its identity to opposing agents and therefore seem like other types that it has encountered or which are less antagonistic to the opposing agent, or the 'type' functionality expectations may be valid for only some subsets of inputs (like 'during specific time periods when the type is checked or when the type is first initialized which is when its identifying attributes are most extreme, after which its likelier to vary from these identifying attributes than to maintain them perfectly'), where functionality (like 'type-mimicking' functionality) can act like complementary information to the attributes (like the 'type') as it can change the attributes so its better to have both attribute/functionality information
        - this cross-interface structure of 'attributes/functionality' can be used to identify other structures like 'change inputs' (like 'causes of a variable interaction change to have a different possible state'), similar to how other relevant interface structure interaction functions like 'type merging' or 'default/backup types' are known structures, and can be applied as constants to check for type interaction changes over time or which are evident in the original data set, as these interface structure interactions are connectible to differences in solution functions for a particular variable interaction
      - this is related to how a 'density' can be mapped to various interface structures (like an efficiency, a type, a certainty, a requirement, etc)
      - similarly, identifying useful connections like 'complementary info of adjacent info' is useful for 'creating other connections' and 'identifying structures that can be applied as a certainty' (as in a 'base for change')

    - identify useful structures like common variables of useful structures like mappings between 'common useful structures', 'functions', and 'solution automation workflows'
        - for example, the 'symmetry' structure has a related workflow like 'change a base solution', similar to how the 'limit' structure has a related workflow ('fulfill limits such as requirements/solution metrics' or 'limit the problem structures/inputs/outputs') and the 'connection' structure has a related workflow ('connect useful structures like interactive structures or problem/solution structures') and the 'shortcut' structure has a related workflow (like 'find efficiencies and maximal differences and other components of optimization structures'), and all of these structures have related functions ('differentiate around a similarity', 'limit/filter/reduce', 'connect/standardize/compare/similarize/differentiate', 'map independent systems/connect maximal differences')
        - these are related bc they are alternate variants of each other (the structures are variants of the functions, and the workflows based on these structures/functions are 'solution automation workflows' bc these structures/functions describe core differences of reality)
        - 'finding variants of a useful structure' makes it likelier to be possible to trivially connect that structure to problem/solution structures like solution automation workflows

    - identify useful function sets that can connect sufficiently different, disconnected/uncorrelated structures (like independent variables) that they would likely contain most important variable interactions in their connections
        - for example, the important variables of positive/negative, true/false, certain/uncertain, finite/infinite, similar/different, useful/useless, and stable/unstable are extremely uncorrelated/independent and different and are therefore useful to connect, so much so that connecting all of these variable values in all their possible useful/powerful connections could act as an alternative to deriving other rule sets to identify the rules/limits of real systems or just the complete rule set of the interface network, as some of those attributes encode a primary interface concept like 'information' (certain/uncertain, true/false) and some are concepts that can describe reality like stability/usefulness or structures which are commonly useful like limits (finite/infinite), and different sets of these structures/variables/concepts can be similarly useful in capturing the variation of reality
        - this is related to how finding a map between very different systems (like biology/physics) is useful as a way to derive a sufficient ratio of real rule sets that it can be used as a proxy for a complete rule set in many cases

    - identify solution structures (like solution metrics, solution set size, interactions with other solutions, and other solution info) by applying useful structures like similarities/differences in the problem system (and across problem systems)
        - for example, in biology there are usually multiple different pathways (the solution functions) for the same mechanism (output), so there are multiple solution functions to find, rather than one, and these solution functions may be trivial to generate from each other, after identifying other variables (other required inputs and the connections between them), as the solution functions often seem falsely very different until one connection between variables like some subset of inputs like 'elements and their adjacent functions' or some pattern in the variable interactions like a 'common threshold all of the solutions cross' is identified
        - similarly, there is often a 'backup' and a 'default' solution in biological systems, indicating that there are usually multiple solution functions to identify in many real systems (like a way for different structures like ratios of elements or different sets of elements to produce the same functionality when one is at a suboptimal ratio, and if you had the 'variable interaction' info between the 'requirements' and 'functions' of the system and these 'element sets' which have some useful similarity, the 'other element sets that act like another element set' would likely be trivial to identify, which is a useful set of cross-interface variables to identify the interactivity between which can explain high variability in suboptimal conditions that vary by input structures like ratios of elements, which is a high ratio of conditions)
            - for example, if you had the 'variable interaction of elements and functionality', youd have info like 'sulfur is a required input for central nervous system functionality' and deriving other info like 'desulvofibrio bacteria (which reduce sulfur) could be related to central nervous system functionality disruption (like parkinsons)' is trivial once you know that element-functionality interaction and the related pathogen-element interaction, as the 'element inputs' are a surprisingly high-variation capturing sub-interface of the biological system (bc of its surprisingly extreme limits in its optimal input ranges) and can explain a high degree of variability in pathogen/condition response, so connecting 'elements' with each other and with 'functionality' is useful for deriving these 'input-condition connections' and 'equivalent alternate element sets'
            - similarly, other common variables of pesticides associated with parkinson's can be found with variables like 'interactivity', 'variability', 'persistence', 'negativity of side effects (known toxicities)', 'spectrum (across species and across systems) of negative side effects', 'connection to required components of health specific to nervous system', 'similarity to other known substances with negative nervous system effects', 'extremity of effects', 'multiplicity of uses/effects', 'uniqueness of functions/effects', etc (while recognizing that they dont have to have much in common to have similar side effects in a specific system, as 'structural similarities' arent the only useful structure to identify)
                - dicofol - has other negative systemic side effects (carcinogenicity, organ damage) but also specifically nervous system damage (related to parkinsons), is known to be toxic to aquatic life
                - trifluralin - is known to be toxic to aquatic life as well as extremely variable interactions (meaning its likely to be harmful in some way) and unusual attributes (meaning its likely to be different from known effects such as negative, neutral/harmless and/or positive/coordinating effects in some way, 'difference from positive' meaning 'negative'), has 'broad spectrum activity' and 'core interactivity (interfering at a core stage of development)', has similarity to other toxic substances (like 'explosives'), deactivated by water (a core bio-system component, indicating an extreme difference from the host bio-system)
                - copper sulfate - has broad spectrum activity (fungicide, molluscicide, algicide), toxic to aquatic life, highly interactive (reacts with many different substances), extreme attributes (colorant, variability in color, multiplicity of uses, many different related forms), identified as a toxin by human immune system, related to 'sulfur' (key nervous system component)
                - folpet - chemically similar to other toxic agents (trichloroethylene), powerful extreme compounding and multiple mechanisms of functionality ('fungicidal')
                - endosulfan - persistence/robustness, known to have negative systemic side effects (endocrine disruption), broad spectrum of activity, unusual mechanism, contains at least one component of other toxic substances (trichloroethylene), produces highly interactive/reactive substances ('reactive oxygen species'), related to 'sulfur' (key nervous system component)
                - naled - broad spectrum of activity (kills birds, pesticide), known to be toxic to aquatic life, chemically similar to other toxic agents (trichloroethylene), deactivated by water, toxic with trivial transforms (acid), highly interactive, has known negative nervous system effects, has known negative systemic effects (inhibits key nervous system enzyme)
                - propargite - broad spectrum of activity (kills fish/amphibians, pesticide), known to be toxic to aquatic life, has other negative systemic side effects (carcinogenicity) 
                - endothall - broad spectrum of activity (herbicide, pesticide), known to be toxic to aquatic life, disrupts intestine health, destroys useful inputs (dessicant to plants), inhibits a useful structure (tumor suppressor enzyme)
                - diquat - broad spectrum of activity (herbicide, pesticide), produces highly interactive/reactive substances ('reactive oxygen species') with trivial changes, persistent/robust, known to be toxic with other mechanisms, destroys useful inputs (dessicant to plants)
            - similarly, identifying other useful structures is trivial like the 'connection between high-variation variables (like different sub-systems like gut/brain in the biological system, to infer connections like "check gut microbiome pathogens by default for brain dysfunctionality", and connections between similarly high variation, high coverage, systemic variables like "input/food processing" and "mutations/conditions" which are similarly high variation and could therefore be adjacently explanatory with a trivial transform)'
            - the 'high variation variables' act like examples of solution metrics (as in 'common structures of solutions, like high-variation variables frequently are') which can be used to derive the others (as all the high variation variables interact so they can be formatted as different nodes on a causal sequence and derived from each other as they capture similar info variation), answering the question 'what is likely to cause this variable change' ('apply a high-variation variable filter (check the high-variation variables first)', as 'high variation variables' are a way to adjacently connect different solutions involving high-variation variables, and thereby derive the others from the first one identified, using a 'high variation variable-connection function')
            - relatedly, other sub-interfaces in the system could also explain the output mechanism, as errors (conditions like mutations) can also create functionality (since other sub-interfaces in biological systems can use inputs like element sets/ratios differently, and these sub-interfaces like pathogens are interactive with other biological systems) as well as intentional inputs (with intent to create that functionality)
            - 'how to connect the functionality of important inputs (elements) and other useful structures to connect (requirements, functionality, response to conditions)' is a useful structure to identify as being capable of capturing enough info and variation in the system that it can act like a proxy for other useful structures like a 'full set of all important mechanism inputs (like element sets or state triggers)'
        - another difference is that some different variable subsets can explain the same mechanism (which occurs when variable subsets act like equivalent alternates), and sometimes different functions in the same variable subset can explain the same mechanism (like when an over/under-supply of some nutrient produces the same output condition bc the area of its optimal value is so limited and fragile, like blood ph), which is useful to identify in the input problem system
        - this info about solutions is useful for identifying other useful info like patterns that occur across solutions (such as equidistance/intersectivity between solutions, similar rank/structure, solution set size, similarities in variation of variable subsets that can act like equivalent alternates) which can be used to identify other solutions once one is found, where finding one solution is trivial in the case of having many different solutions and identifying the connection between solutions is more useful in understanding the problem system than identifying one solution
        - identifying 'different input sets that produce the same solution' is useful to identify other useful structures like 'other different input sets that produce the same solution', the 'causes of similarity in outputs, across differences in input sets', and 'similarities/differences in inputs that would produce different solutions'
        - identifying certainties is useful to apply as info 'filters', similar to how applying a 'known similarity as a symmetry (around which differences can be applied)' is useful for building structures in a graph that reflect other certainties (structures like similarities/differences) of reality and is therefore useful for finding those reflected certainties
            - this is bc 'certainties' by definition/interaction map more adjacently to 'structures' (such as useful 'standards/filters', 'combinations/sequences', 'sets/densities', or 'graph organization structures like partitions/connections/nodes/layer/boundaries/intersections or graph usages like graph queries') than 'uncertainties'
            - finding the 'full set of graphs that apply a certainty in a way that reflects its real structure' (the certainty applied as a filter, as a base, as a density, as a network connection function, etc) could be a useful workflow for finding 'other adjacently related certainties, as reflected in these very different graphs that have the same certainty embedded'
            - an example of a useful filter combination is 'ratio/probability' and 'positive/negative' (or 'probability' and 'cost/benefit'), such as how identifying common variables of positive responses to negative inputs can be done by applying definitions of positive/negative (health structures vs. conditions/mutations/requirements/attributes like fragility) and identifying other 'similar' structures (like how identifying 'rare health structures' can be useful for identifying causes of 'rare positive response to negative inputs', and identifying specific examples of 'rare health structures' can be found by applying variation to the definition of health structures, like 'extreme athletes', 'extreme diets having healthy components like raw vegetables', 'extreme adaptations like high performance', to check those positive/healthy subsets first for the positive/healthy response, the extremity of which can be useful in identifying obvious differences, extremity being a useful truth structure through its connection to commonness/probability/stability), and implicitly applying a 'similarity' in positive/negative ratios/probabilities across variable subsets ('rare good health is associated with other rare good health structures like specific healthy responses' as a default solution structure to check first)

    - identify useful functions which are useful for some useful problem-solving intent like 'identifying other useful structures like specific functions that fulfill some different intent/metric optimally'
        - for example, 'control' and 'replicate' are two general/specific variants of other functions ('reduce/limit' and 'copy') which can be used to change the problem structures in a useful way, where 'control' is useful as a more general alternate function which has many variants that could be useful specifications to derive (like 'limit' or 'reduce' or 'isolate' or 'use (as in trigger)' or 'change' the problem or 'convert the problem structures into a constant or a simpler form, so its more controllable' which are adjacent variants of 'control') and where 'replicate' is a useful variant of 'copy', where 'replicating the problem' can solve the problem (as once the problem is replicated, some of its causes are trivially identified from the variables used to replicate it), where 'copy' is a generally useful function for workflows involving 'testing' such as 'trial and error'

    - identify useful specifications of problem-solving workflows that can still be applied across different problem systems
        - 'change the interactive surface (change the input/output of a function by applying injections of surrounding interim functions, rather than the function itself)' or 'change the host system where the suboptimal variable interaction occurs' is another interface query that can act as approximately equivalent to other function sets like 'change a base solution' or 'find adjacent change combinations of existing structures' or 'find interactive structures and build other useful structures out of interactive structures' or 'change the intent to solve for to a more trivially solvable intent, which is sufficiently similar to the original intent', as 'changing the interactive surface of a function' (such as changing the surface of pathogens or gene structures by applying coatings/encapslations/receptor alternatives, etc) is a useful variant of changing the problematic structure of the function itself (rather than optimizing the internal logic, change the interactive components of the logic until the interactive components change it enough to be more optimal/useful), which is a merging of 'find interactive structures to apply as components' and 'change a base solution until its more optimal' that is more trivial to solve for (and therefore more useful) than other more general intents like 'change a base solution', as it adds usefulness in solving the problem of 'which structures to change' ('interactive' components like inputs/outputs or surfaces) by specifying that variable into a constant
        - this is related to how some math structures are useful through their 'specificity' (like specific values of constants/sequences/limits/functions that are particular powerful/useful in some way) but seeing the generally useful structures like 'abstractions' is still useful as well, to add variation when known specific structures arent sufficient to solve a problem adjacently, like to find new generally useful structures like general solution-finding methods, as there is no known set of constants that adjacently explains everything (if there is, its interface analysis), so keeping some variation structures like general abstractions is still useful to offer an opposite/counterpoint when structures are too specific to be useful for some intent

    - identify useful connecting structures like 'high-variation variables' and 'problem space similarities which are high-variation (but not random)' and 'connections between useful approximately equal intents, where some subset of the intents are more trivial than others' which can be applied as useful target structures to apply as inputs to other workflows
        - for example, the 'transformer' applied 'sequences' in the position of 'word sequences', identifying that the variation of the last/next word in the sentence is a high-variation variable, but not so high-variation that it looked like randomness, and not so simple that trivial/few simple rules were usable to predict it, rather being in between these low/high variation variables, so that there was enough structure to predict the next/last word in a sequence of an isolated unit like a sentence/clause, but not so much structure that it was trivial to predict (so that it wasnt a problem at all) or impossible to predict (so that there is no selectable solution that is clearly better in some way), this structure including sequential patterns like grammar, definitions, etc, and embedded structures in those structures like 'possibilities allowed within definitions' and 'sensibility/reasonableness'
        - however just knowing the 'sequence' structure is common or even relevant to the problem of 'sentence completion/prediction' isnt enough on its own to derive 'transformers (which are a structure of predicting sequence completions)', but knowing that there is a 'similarity' to apply as a base for problem-solving in the sentence/paragraph prediction/completion problem space can make this problem possible/adjacent to solve (the similarity between adjacent sections of the sentence/paragraph/document is sufficient to allow prediction in most cases, as its a similarity based on relevance such as adjacency and connectivity and even cause like causal degree, except extreme nonsense/randomness or extreme creativity which deviates from known rules in a realistic way, so applying this similarity as a base for problem-solving is useful and works, meaning its possible to use this similarity to fulfill the 'completion' intent)
        - relatedly, high-variation variables are useful to identify and apply as inputs in other workflows (like how pathogens are high-variation causing inputs, so theyre a useful cause to examine first when trying to explain other high-variation variables like conditions, before other causes, as the similarity in variability implies a possible adjacent connection between pathogens and conditions)
        - the common structure is 'similarity between (high-variation but not random) structures' which can identify more achievable/trivial but still useful sub-intents to fulfill (like 'predict the next word in a sentence' rather than 'predict all sentences given a general high-variation goal like "write an essay"') is in a useful range of variation to make prediction algorithms useful, and which can be useful as similar structures are more connectible than other structures, so 'finding similarities in the problem space' is useful as a problem-solving intent to fulfill other intents like 'connect similar structures'
        - similarly, knowing 'what structures are useful to build high-variation variables' is useful to apply as default inputs to test (generality, few requirements, types, etc as components/inputs that build high-variation complex systems)
        - relatedly, knowing other high-variation structures like 'embeddings' can identify probable structures like 'errors' in a system (such as 'prompt injections' which are usually embedding structures, like a 'character embedded in a story, an embedding which makes the character seem false, and not a person speaking sincerely' or other structures which can produce 'vacillation' structures between relevant dichotomies like certain/uncertain or true/false, such as 'pretending to (pretending to (pretend))', so that 'emergent' structures can be used to identify the net effect of the embedded structures), as most complex structures will be an adjacent structure (like a 'combination') of these high-variation structures (like embeddings, waves, rotations, similarities, etc)
        - similarly, useful structures like 'completion functions from a partial subset' and interim sub-intent connection structures (like 'clarity functions which connect a partial subset to a probable higher coverage-ratio subset with a clear implication of the full set') are useful to apply
        - the 'set of sequences of set completion functions (like similarity indexes, between each adjacent pair of completion states in the sequence)' is a useful structure to identify as an equivalent alternative to other 'useful interface queries' for 'connect' intents in solution automation workflows (similar to how the 'set of sequences of differences' is useful for 'filter' intents)
        - relatedly, identifying 'similarities/differences, paired with the intents they adjacently fulfill' (similarity-intent structures) is useful to identify ('similarities that make some structure combinable/stackable' or 'differences that make other differences obvious'), and the same goes for other useful structures ('function/cause/opportunity/perspective-intent')
        - similar to how adjacent combinations of high-variation structures make problems trivial to solve, common interaction/connection functions make problems trivial to solve, as they offer different versions of the same information (what is connected/what is similar), just like other info offers an alternative (what are the most common/powerful inputs, which offers a different version of the same info) just like 'maximal differences which are equivalent alternates in some way like having similar degrees of variability within some area of probable solutions' make problems trivial to solve ('similarities across differences' and 'differences based on similarities' being highly explanatory structures), so identifying connecting structures of these 'different versions of the same info' is useful to identify the other versions of the same info (as well as identifying 'differences that change/remove the info being connected' which should be avoided)
        - relatedly, identifying useful structures that are not adjacently connectible/combinable is useful to identify other functions that a neural network should support (structural non-combination functions like 'embed/merge/filter' which indicate connections between different states/intents of information other than just 'building a structure out of combinations of components' as well as more complex information functions like 'switch perspective', 'conceptualize', 'explain', 'organize', etc)

    - identify useful structures like solution metrics such as 'realism metrics' (structures that can be used to determine if some structure is real) which can be added to a solution metric set (realism metrics such as 'iteration' and errors like 'irrationality/illogic/unreasonableness/unintendedness' which occur in real systems, where an artificial system might over-reduce/optimize for some metric like uniqueness or minimum memory storage), where realism is useful for simulating real systems (such as representing systems of variables like in the 'regression' problem space, so adding 'realism' to a function makes it likelier to be a good representation of reality, which representation functions in general are supposed to be as well as being good representations of a data set, although for some intents that might be suboptimal like for 'compression, to optimize minimum memory storage')
        - these can apply a similar impact to a solution function for very different or similar reasons, such as how 'iteration' might produce 'non-linearity through iteration of embedded changes' or 'trivial differences around the copied structure (adding randomness, divergence into different functions if the differences are scaled by further iteration, and cascading/self-sustaining errors in the host system of the variables)', and similarly errors like 'irrationality (or deviation from reasonable intent)' can add either/both randomness and/or non-linearity to a function through the 'variation in differences' this error can adjacently create
        - similarly, 'diversity' is a metric of reality so it should be applied as a solution metric of the solution-finding method (does the solution-finding method handle diverse inputs and diverse input cases)
        - how did I arrive at this conclusion (what is the interface query to generate this)? I was thinking of important differences, like how reality is different from most graphs (related to the problem of creating a 'reality-generating graph') and one 'reason' is that 'repetition' is allowed in 'reality', and relatedly attributes like 'count' and processes like 'iteration' (function application count), which are not abstract concepts but which are useful to determine real vs. artificial structures, and then identified that real structures would show up in a 'representation of reality' or a 'realistic solution-finding method' (similar to how more complex solution functions tend to be more real (occur in real systems) than simple linear functions), 'representation' being related to the 'regression' problem

    - identify useful structures like 'opportunities to use workflows optimally', like the 'opportunity to identify new math structures' by identifying math structure variables and applying a simple iteration
        - this example uses a 'trial and error' workflow in an optimal way to identify useful structures by recombining variables like components of known math structure definitions, which is a huge information gain from a small work investment, which can be enhanced with output filters to gain more useful information like the most different resulting structures from known structures and from each other
        - 'maximal differences from equivalent alternates' and 'maximal differences from known structures' are useful filters to apply in a 'combination' structure in many cases, as they filter a set of different structures in many cases, except in a case like where all items in a set of equivalent alternates are known except one, and then they identify the same structure
        - the structures which can be used to fulfill this intent of 'identifying opportunities' include structures like 'complex problem spaces with few known useful (as in re-usable) structures' which indicates that there are likely other structures which are similarly useful which havent been identified yet, and identifying other structures which are different from other structures without having a reason for that difference (unlike other complex systems which have many useful known structures, the math system didnt have as many, which indicates an opportunity to correct that difference, as it seems like a false difference) and similarly identifying similarities which can be applied in a useful way like 'similarities across systems which make it possible to apply variables from one system in the other to find false differences and other errors to correct, where these false differences are adjacently connectible to useful structures like useful re-usable math structures'

    - identify useful structures like 'similarity sources in real systems' such as 'memorization/memory storage structures/functions' (like 'mnemonics') which are by definition a way to remember/store/retrieve/use structures by some similarity (like a pattern), similar to how 'compression functions' and 'abstractions' and 'usage contexts' (in which a function is used) are a useful way to store information about connected/relevant structures that could be used to derive the structures themselves based on just these connections
        - for example, usage contexts can help differentiate the meaning of a structure, such as 'two-dimensional', which can describe an infinite series or a structure with two dimensions or four sides like a plane or square, which are very different structures that can be described by that attribute, so including the usage context in which this description was selected can help filter the set of structures it could refer to
        - these 'similarity sources' are useful to identify new similarity metrics to use when 'finding similar/different structures'
        - the 'usage context-function' index is useful similar to how the 'insight-function' and 'intent-function' indexes are useful, by providing similar info through surrounding/adjacent info like 'info triggers/inputs' such as the reason to use a function and the context it is optimally used in
        - similarly, more specificity can add value (just like a mix of specificity/generality can add value as a problem-solving, difference-connecting structure), as specific examples of usages/intents/insights are useful, and possibly more useful than a network of insights, depending on the insight, such as how a given 'attribute/variable network' can have limited usefulness compared to one specific (pre-filtered) attribute/variable (such as 'balance of power') which could explain most variable interactions and which can be applied in a way so that it is more useful than the whole 'attribute network' to identify specifically in some cases, where the one useful specific attribute can connect the comparatively static attribute graph to other graphs, as its more abstract/conceptual and connected to other graphs than the whole 'attribute network' might be (the 'balance of power' attribute can reverse the limiting, static impact of identifying the attribute network, which could be just useful enough to make other useful structures falsely seem obsolete and reduce thinking into a set of simple structures/functions of those attributes, and expand the static network into a set of connections to other graphs (which is an example of 'reversing the trap' of specificity, by creating generality (its opposite) from it) - connected by balance, power, balance of power, concepts, opposites, alternates, and other related interface structures to this particularly useful structure of concepts, which can be used to derive the interface network and other useful structures with comparatively few changes), similar to how having 'information including the specific primary interface variables' is more useful than having 'information about some specific system' in some cases

    - identifying useful structures like 'overlaps between very different structures like perspectives/interfaces' which enable intents like 'identify structures that fulfill multiple priorities/solution metrics/intents which connect different systems'
        - for example, a structure like a 'word combination' could be a simple structure like a 'sequence' of 'objects (like words)' but it is also the unit of 'conceptual math (combining a concept with another concept)' if the words are abstract concepts (objects with many different definitions), however to call it 'conceptual math' is misleading if its not applied in a useful/relevant way like to identify insights, and if no other structures are ever applied than simple combinations/sequences of objects (like words)
        - the overlap of those systems in the 'word sequence' is useful to connect those systems (simple structures and conceptual operations), as it can identify other simple structures (like convolutions) that can be applied to the other system (like 'every possible interaction between a set of structures'), as well as being useful to differentiate those systems (some function that only fulfills simple structures like sequences is unlikely to ever be able to complete conceptual math operations except the simplest unit, which is not useful in every problem), which is useful to identify structures that should be applied in neural networks other than adjacent change combinations
        - identifying the structures with these overlaps that can occur in some form in every maximally different system/perspective/interface is likely to identify a useful problem-solving structure (like a structure that can be used as a filter/gauge/standard/ring/interface/system/perspective/concept)

    - identifying the important structures determining a solution in a particular problem space (like 'similarity' in the 'regression' problem space) which can be applied across problems
        - what useful structures (like function similarities/types, higher powers in functions, densities in data sets) have in common is that they store some information about some similarity type, which is useful for finding the solution function (the most similar structure to the data set that is also similar to some solution metrics like generality which have absolute abstract insights stored in them)
        - finding structures that are similar to these solution metrics (like 'finding functions using a solution-finding method, where abstract useful concepts like balance/power are applied in relevant structures like solution function metrics in that problem space') is a useful solution structure (the 'balanced powerful structure, balanced in specificity and powerful in storing information about similarity to the data set with fewer more powerful variables') to aim to connect to the problem input (data set) or variations of the input (like representation functions of the data set, subsets/densities of the data set, etc), as the solution also has to be similar to the problem input enough to retain the important info, just like it should be similar to the solution metrics (whether absolute/abstract/general solution metrics or specific solution metrics)
        - this applies the workflow 'find structures adjacent to the problem/solution (adjacent representation functions/subsets, absolute solution metrics) that can be connected instead of the source problem input/target solution output'

    - apply useful structures like 'function sets' to other structures (like 'interface structures including other function sets') to identify useful structures like 'usefulness thresholds/limits/directions'
        - structures applied to define useful high variation functions like 'describe' (map some 'input' structure to a structure like 'combinations' of structures from a 'set of known structures') and 'determine' (make some structure obviously implied and unique and therefore required, such as filling in the ratio of a circle required to imply the circle, and no other structure as obviously) are maximally different enough to be useful when applied to other interface structures to create useful differences like new variables (which is applying the outer layer of generated functions which includes the highest variation functions, to components of inner layers to create a new outer layer)
        - this is useful to identify bc its possible to make it obvious when more iterations are suboptimal by applying similar changes to existing changes, which is when its more optimal to use existing changes than the new changes created, which is useful for identifying when to stop specifying a function by adding more variables, as there are useful structures like 'thresholds' where re-using known simpler components is more useful than identifying and applying new more complex components, 'thresholds' which can be identified to find other 'thresholds' where iterations start to become less useful and related structures like limits more adjacently

    - identifying useful adjacent structures (like 'polynomial factors' as a combination of different structures like 'differently powered terms which can be multiplied to create solution structures like the complete polynomial') that have a similarity to some interface structure (like a 'component') so they can be applied to relevant workflows using that interface structure
        - identifying useful structures like 'factors of polynomials' as useful components for 'identifying similar functions' and 'changing one function into a similar function', a structure which can be found by applying a 'combination' to a 'variable and a constant' to identify a useful difference in the exponent of a variable, rather than only handling terms in isolation or the whole function at once, to identify useful function formats like 'overlapping areas created by offsetting terms having the same exponent', so that a function network that was a network of 'variable + constant terms (like x - 1)' could be another way to format the network to find useful combination sets/paths as well as finding an optimized network where all combinations/paths are adjacent and finding similar functions by similarities like patterns in their components using a function network that can be applied as a function similarity index

    - identifying useful structures like 'inputs to useful structures' which can be used to find other useful structures
        - for example, an 'error' can be a useful structure to identify variables, as 'errors' are 'differences from optimal states like solution states (as in opposites of/differences from positive states)' and are therefore useful in identifying variables, such as how errors (created from lack of control) in 'filling in a circle with a color' can result in useful structures like 'incompleteness', 'overlaps', 'opacity' (through connecting unfilled/filled sections with a gradient) and other useful variables of colors which are possible once the 'color' interface is applied to a 'structure' interface (like a shape)
        - the error results from a poorly controlled application of a function (like 'filling in a shape with a color'), which leads to a difference that is measurable and therefore makes some variable clearer, so 'removing functions that regulate applications of functions' is an input that can be directly applied to identify variables
        - this can be applied to derive another workflow, such as 'apply adjacent changes of every type/combination/structure to the problem to check if there are clear improvements/errors in some direction or other structure of change' which is a useful specification of the workflow 'apply changes to the problem space, which will be likelier than not to improve it, given that its a problem and changes to a negative structure are likely to produce a positive structure'
        - similarly, other workflows can be derived from this such as 'structures which are uncertain like "unrequired/unguaranteed structures" are likelier to be useful to solve some problem, as certain structures like "requirement structures" are likelier to have already been tried and found to not be solutions' (the solution is likely not extremely similar to requirements, or they would be used as a solution, its likelier than more different structures are required than extremely similar structures to requirements)

    - identifying the reason for useful structures and generating other useful structures based on that reason
        - for example, regarding useful structures like 'change a constant (like only one) into a variable (like some or every)' applied to structures like 'core interaction functions' (like reduce/connect) connecting problems/solutions, the query 'does every core interaction function also connect problems/solutions' results, with answers like 'worsen' the problem (so it gets more attention and other problem-solving resources, as attention is an input to problem-solving processes) and 'contain' the problem (find a structure that can wrap the problematic structure and prevent its problematic inputs/outputs from having an impact, as the inputs/outputs of problematic structures are likely to be related to why its problematic so handling those instead by isolating them is a problem-solving automation workflow)
        - the reason that other core verbs like 'worsen' and 'contain' are useful in problem-solving as core interaction functions of problems/solutions is that they change the problem structure in some way, which is necessarily useful for problem-solving, though some functions are likelier than others, and identifying the differences between these is useful ('worsen' only helps if there are available resources like attention and other inputs, whereas other functions are likelier to be less harmful such as 'connect/reduce')
        - similarly, 'copy' the problem (to another system) is likely to be useful, in cases like 'if the other system is different enough to change the problem in a useful way' or 'if the other system is a simplification of the original problem system so it makes the solution or the source of the problem clearer'
        - similarly, 'expand/amplify' the problem is likely to be useful in that it will make the limits of the problem clearer once its expanded or amplified until these limits are reached or implied
        - these are opposing functions of typical solution automation workflow functions, which usually seek to reduce the problem in some way or directly connect it to solution structures

    - identifying useful structures by whether they adjacently fulfill multiple useful intents (such as abstract structures which are adjacently varied to generate very different structures, or functions that can fulfill the opposite intent if their direction is changed)
        - the structure of 'opposite direction (reversible, which can expand the solution space, rather than just reduce it) filters' is useful for identifying different directions of maps that can be useful, such as a 'function to identify the original data set from a solution function', which can be used as a 'filter of solution functions' bc identifying 'data sets that are possible for different possible solution functions' identifies solution functions that are too general/specific in the set of extremely different (so as to be irrelevant) data sets that they map to, and similarly is useful for identifying offsetting variables and fulfilling different intents like 'reverse-engineer a data set from a representation function, to make sure the function retains enough info about the data set'
            - these filters are useful for filtering the set of possible data sets, which is useful to identify, just like identifying filters to filter the set of possible functions for a data set bc both intents allow identification of useful attributes of filters, like info retention (reversibility, specificity, etc), as well as useful applications of filters, like useful sequences of filters that optimize for some filter attribute like specificity
            - checking if the solution function applies to a 'extremely different system of variables which change/interact with the function in different ways is useful' to identify different reasons, change types, probabilities, and other structures associated with the functions, as well as identifying the probable correctness metrics of the function (like the 'longevity' of the solution function, in being robust to predictable changes in the host system)
            - similarly, once the function and filters and data set and host systems are connected with these maps, other structures are connectible, like 'host systems producing similar variable interaction functions that change in similar ways'
        - the structure of the 'average' function is less optimal bc it is more difficult to derive the original data set from this function, even if it optimizes an error metric like 'distance from the solution function', bc it loses the non-linearity/complexity of the original data set, rather than being curved, which is likelier to retain the original complexity of the data set, which can be used to 'filter out filters that over-reduce or over-expand information'

    - identifying useful structures to be connected in problem-solving, such as 'spectrum variables' that can be applied as useful structures like 'solution filters'
        - identifying useful structures like solution metrics such as 'spectrums like good/evil' which are useful to connect problem/solution structures to, for intents like 'find solutions that are likelier to be useful for good intents than otherwise', so arguably every workflow should be connected not just to problem/solution structures but also other useful structures like the most powerful reality spectrums such as good/evil, uncertainty/certainty, constant/variable, etc
        - solutions that are likelier to be useful for good intents fulfill many alternate solution metrics (to adjacently fulfill intents like 'identifying many ways to be good, to cover the highest ratio of users'), use many different variables/functions which are also interface structures so the solution can be used to teach useful structures and fulfill intents like 'making users smarter', etc

    - identifying powerful differences in workflows like 'abstract an insight and connect it to problem/solution structures' like 'core cross-interface functions'
        - identifying that 'any combination of primary abstract concepts (power, balance, similarity) can be combined to produce useful functions that can connect most structures' is not a result of the workflow 'abstract an insight and connect it to problem/solution structures', but rather other workflows like 'is a constant (like only specific abstract concepts that are useful for some intent) a requirement, or can it be changed to a variable or removed without changing functionality/usefulness of some structures (to change it to some or all)' (answering the question 'is it "only" these concepts, or "all" concepts, or "some structure of concepts" like differently-sized subsets of all abstract concepts, which can be adjacently transformed into a useful structure that can fulfill some problem-solving intents')
        - 'abstract an insight' (concept-information interface) vs. 'change a constant into a variable' (change-function interface) vs. 'connect structures to problem/solution structures' (structure-information interface) are useful core cross-interface functions, which can be applied as components/inputs/alternate examples to find other workflows
        - the reason these are useful is that they 'connect maximally different systems', which is likely to be a useful map for solving other problems as it will connect many different variables
        - identifying useful structures to combine these components with is similarly useful, such as identifying that 'equivalent alternates' are a useful input to the 'change a constant (only one) into a variable (every)', which is more useful when applied to one item in a set of equivalent alternates

    - identifying different useful structures like different input/output formats that can be mapped to different problem/solution structures
        - other function/data set formats can be useful, such as a 'probability of an adjacent point being in a specific adjacent dense area, given some input point' (a format of 'points and related dense areas to those points', applied as a predictor of adjacent points) being arguably more useful than other formats in some cases
        - this is similar to how 'knowing that a particular word is in a sentence at all' or 'knowing whether "words that frequently change meaning of surrounding words/phrases" are in a sentence' may be a more powerful predictor than other structures like 'sequences' ('sets' and 'probabilities of words in sets' are alternate useful structures of 'sequences', similar to how 'word patterns like iambic pentameter' are useful to identify, and 'similarities between word interaction functions (like core verbs such as "like" as a proxy for words like "similarity")' are more useful to identify than sequences), and 'differences within a sentence' and 'differences to other/standard sentences' and 'differences from core similarizing (the most standardized, common) words' and 'inputs to differentiating words/sentences' and 'common components of the most different/useful sentences' and 'common sentence interaction structures like embeddings (like a specification such as an example of a point in a previous sentence)' and representations (the 'summarizing sentence of other prior/subsequent sentences') and 'very different sentences with the same meanings (similarities in differences)' and 'sentences around which other sentences gravitate or vary' and 'over-used sentences that would be better phrased differently but which are convenient like excuses or slogans/idioms' or 'sentences that standardize sentences that follow (uniting differences to equate other sentences that follow)' as well as the opposing 'controversial sentences which polarize sentences that follow' and 'sentences which frequently surround facts'

    - identify structures that are common/compelling with uncertainty in the reason for that useful attribute and connect them to useful structures to resolve that uncertainty
        - why do we find symmetries compelling? bc of the 'variation of their contradiction' ('similarity in a difference') and their 'alignment' (the 'similarity in the changes', present in the similarity of their common base/origin/center and the similarity in the structure of changes in relation to that base), as 'opposing' structures and 'embedded' structures (an 'opposing/different structure in the same structure') and different structures ('similarity in a difference') and paradoxes ('similarity in a difference') are useful structures to identify as they are commonly explanatory of other changes like more complex changes once they are iterated/combined/otherwise structured and useful for intents like 'differentiation/filtering' and 'identifying similarities across different systems to connect systems/differences' and 'identify interfaces (similarities that connect all systems)'
            - what makes symmetries more compelling? structures like 'balance', 'intersectivity', 'orthogonality' make symmetries even more compelling
        - similarly, we find metaphors like 'yin and yang' compelling, as it indicates other useful structures like a pair of monodromies ('structures such as spaces having a point representing one impossibility') fulfilling some useful structure (like 'spirals/fractals') which together adjacently create other useful structures ('circles') and which are similar/complementary/coordinating in some ways (structure, alignment in relation to axes) and different/contradictory/opposing in other ways (color, rotation degree in relation to each other) and contain a set of impossibilities which contradict each other in the same structure (a paradox), and a similarity to other useful structures like symbols such as the 'symbol for infinity' (achievable by a rotation to create an overlap, to connect it to other useful structures like the symbol of infinity represents such as a 'set of loops/angles that would necessarily develop and/or would create momentum in real systems to sustain itself and refer to itself by intersecting with itself, indicating self-sustaining/infinite change around a common base which is the intersection point')
        - similarly a wave ('connection between extreme differences') has interface structures embedded in it (common structures like 'circles' as well as change limits, embedded changes like change rate/type changes, etc)
        - similarly a 'peace' symbol has useful structures like 'connections between similar but different directions into one unified direction' and in reverse, 'isolation of a variable into component variables' and a 'variable connected to a constant' and 'lines from a common origin applied as connectible to a circle through being applied as radii of a circle' among other structures
        - similarly a 'flywheel' symbol is a useful variant of a wheel that can be used in its place
        - these are useful structures that have other useful structures embedded/intersecting/otherwise connected to them, which can be applied as useful base structures that are useful for 'identifying other useful structures', "identifying ways that very different structures can interact without violating definitions (which is useful for intents like 'coexistence')", 'identifying maximally different stable structures', 'identifying structures common to useful structures', etc

    - identify insights by applying other insights/interface structures and identify variables of insights to find new insights to apply as inputs to other workflows involving insights
        - identifying errors in perspectives like 'long-term thinking' (such as the 'lack of short-term/immediate feedback') and 'short-term thinking' (such as the 'lack of a solution for scaled/long-term/emergent effects') is possible to achieve by applying structures like 'generally useful structures' (such as known optimal structures like 'feedback' which would ideally be integrated in every solution) as well as 'advantages of opposite structures' ('what is the benefit of short-term thinking and is that missing from long-term thinking') by applying insights (like 'any one solution is rarely the only solution' and 'its useful to identify/apply balance points in spectrum variables like long/short-term thinking') and known error structures (like 'avoid over-prioritization errors caused by applying few structures such as only one structure')
        - the variables of these insights include interface structures ('insights frequently use interface structures, which are known to capture high variation, since interfaces by definition support other variation and allow variation to develop as they are a base structure, and insights are likelier to evolve from structures of high variation-capturing variables, whose interactions are less likely to already be known but are likely to be powerful interactions and therefore important/useful') such as 'multiple' (math), 'alternate' (change), 'spectrum' (structure), 'balance' (concept), which occur across interfaces (from which you can also derive that 'insights often use/connect interface structures from multiple interfaces, as those are high-variation structures which are powerful/useful to connect')

    - applying insights like 'any one solution is rarely the only solution (apply multiple to useful structures like solutions and useful structures)' to structures where useful (where errors corrected by the insight are possible and relevant and where the insight structures dont contradict solution metrics like accuracy)
        - for example, identifying structures of errors that can be inputs to a loss function in neural networks is useful for identifying error structures like 'overlapping error areas (of multiple error types)', such as how 'adjacent input errors' can be used to identify a 'directional error' (an error of an incorrect direction, either the first error that indicates it or is measurable as indicating it or the error that initiates the error direction by being different from the correct direction), to apply an ensemble loss function of various loss functions or structures of loss functions (like combinations of loss functions), as knowing 'similar minimums across loss functions' is useful for finding 'absolute/compromise errors across loss functions'
        - 'a type of difference that is not an error' is another useful structure to identify, such as an error that probably wont push a function beyond a range that would classify it as a slight generalization/specification of a data set average/regression/representation line
        - it is useful to keep multiple loss functions separate bc adding them removes the information of their extremes/averages and its useful to know which types of errors are optimized for at a certain point
        - another example is how 'over-prioritization errors' are likelier the fewer the priorities are (even generally good metrics like 'independence' can be applied to negative effect, such as how 'apathy' is a result of 'extreme independence (leading to lack of caring)'), where multiple priorities are likelier to offset the possible over-prioritization errors that are likelier with fewer priorities, as more priorities are likelier to be able to check each other's power and errors

    - identify useful structures like useful function/structure sequences to apply for some problem-solving intent like 'identify useful similarities/differences to apply to create useful structures'
        - for example, 'if a condition has an output and another condition has the same output, one condition could cause the other' (or 'alternate inputs (having the same output) could cause each other') is a rule that is possible to infer and apply as a rule to find/describe possible variable interactions, given the useful identification of the uncertainty of the causal connection between the alternate inputs and their relative position (either could be causal of the other or there could be no causal connection), which is useful to identify as this uncertainty allows for a possible connection between possibly previously unknown connections (alternate inputs of the same output), which might have a useful difference in differentiating the position of the alternate inputs (as in positioning one before the other, causally), and given that there is no requirement that either condition not cause the other, thereby allowing that uncertainty to exist and the possibility of a causal connection between conditions to be identified
            - these are variants allowed by the known structures, which is not adjacent, as in its not obvious to position one condition before the other in a straight line before the output, indicating a guaranteed causal relationship, between alternate inputs
            - some graphs will position alternate inputs at the same/different distances from the output, but without differentiating information, they would be at the same distance from the output by default in a correct graph that doesnt add info (such as by differentiating the distances) that isnt available/known
            - the uncertainty develops bc the position of the alternate inputs can change while still being an input to the output (still being 'before' the output) and the alignment of their angles to/distances from the output implies different connections to the output and to the other input, meaning there is a possibly valuable difference to apply by 'aligning the angle of alternate inputs to the output, but keeping position different (rather than overlapping at the same point)', so that in the resulting structure, one alternate input is causative of the other (at which point it could still be an 'alternate' input, as the whole causal trajectory isnt necessarily required to traverse)
            - this is applying a (difference in a) difference/similarity (the changing of the alignment/equivalence of the angle/distance between the alternate inputs and the output) and a (similarity in a) difference (the maintenance of a difference in position, to differentiate the alternate inputs as not equivalent) in a combination structure that identifies a possible equivalent alternate structure of a structure (like 'alternate inputs of the same output, whose causal connection is possible but unknown'), where info that is different and should remain different is held constant (the difference in position of alternate inputs is preserved) and a unrequired constant (the similarity in angle/distance from the output) is varied to be different in distance or similar in angle, in a way that doesnt violate the requirements/definitions of the structures
            - this is applying trivial functions (combinations/applications/embeddings) of similarities/differences applied to connections (equivalences, alignments, similarities, positions) within the limits of requirements/definitions, which is a useful default function to apply

    - identify variables of useful graphs to combine/mix/otherwise apply as components of an integrated graph that makes other useful structures trivial to identify
        - given the different graphs available to graph useful structures like interface queries and workflows, such as the default 'functions/lines connecting structures/points' diagram, identify the usefulness of different graphs
        - different graphs are useful to identify different structures adjacently
        - applying some known structure as a similarity in a graph can help derive missing alternate structures
        - an 'iteration' graph can differentiate 'iterated structures (like embedding an embedding in an embedding or pretending to pretending to pretend)' and possibly also the similarities that these iterated structures have in common (line sequences vacillating between opposing/contradictory structures like container/component or truth/falsehood)
            - this graph is interesting because highly scaled iteration can produce complex structures (similar to how mixes of these structures can), such as a 'nested embedding that contains/stores/compresses the host system in which it is embedded', or an 'application of an application of an application that solves the problem of which structures to apply' (self-referential structures, self-containing structures and inverses of traps, where information is derived from approximately zero information, such as the 'minimal information of a unit structure', which is iterated to create the final structure)
            - this graph could be formatted as a set of line sequences connecting opposites like truth/falsehood, or a set of nodes which are also system layer diagrams to identify patterns in the embedded iterations of each structure
        - for example, identify a trivial graph where interface queries/solution automation workflows or other useful structures have a structure (similarity) in common, which is applied as a constant (the similarity is guaranteed to be a similar structure in the graph), like the following, in order to identify missing workflows or other useful structures by applying a trivial transform (such as 'rotate the structures around some center of the similarity, like a common starting point')
            - a 'line of function steps' or a 'wave/cycle between (sequences/lines representing functions)', if formatted as a set of lines connecting nodes, so that different workflows would be lines in different directions from an origin, so that the other workflows can be calculated by determining lines in the areas in between known workflows
            - a 'query across several different layers (representing different function sets)' in a system layer diagram (a similarity which would reveal that objects from different function sets frequently interact in useful structures like workflows)
            - finding a graph where similar structures are guaranteed to have some similarity (possibly by trial and error or another dumb algorithm) is useful for deriving other structures of that type
            - as another example, identifying an error graph where 'overlapping error points/areas (having multiple errors)' are visible is useful for identifying points/areas with one error (compounding/additive errors and their points/areas of intersection/overlap being useful to identify as another error structure that is complementary in usefulness to a 'minimum of a loss function')
        - these graphs make these similarities trivial to identify, by applying some known/constant original similarity structure (like a layer in a diagram referring to functions from the same function set), around which the input structures graphed in the graph (the workflows which can be represented as line sequences connecting functions/structures across the levels) are allowed to vary (meaning 'a workflow isnt restricted to one function set/layer'), then finding the similarities in those variances from the known/constant original similarity structures

    - identifying useful structures (like 'concepts that can store/connect very different structures') is useful for intents like 'connecting maximally different structures' and 'identify function types', which are useful for problem-solving intents like 'filter a solution space'
       - identifying 'uniqueness' which is connected to 'units' which are useful as 'components' for 'combine' intents as well as being connected to 'useful equivalences' (like the 'equivalence' of a single function that is the best representation of a data set by being equal to it in some way, which may be unique in its equivalence in that way to the data set) and which is powerful in that it identifies structures which may be new stable structures (unique structures may be new, as in 'different from previous existing structures'), which is useful since 'uniqueness' captures very different structures like 'unit components' and 'filters' in one concept
       - similarly, identifying 'similarity' as the important structure connecting a 'regression function' and a 'data set', rather than finding the 'average' (identifying that 'similarity' is important in its connectivity to the concept of 'average') is useful to identify as it allows other formats like similarities/differences to be applied rather than default formats like 'numerical values/addition/multiplication' and connect those other default formats with other useful formats like conceptual formats which are useful to identify which equivalences/similarities/differences should exist in which positions in a problem space
       - similarly, 'randomness' can represent 'equivalence' (in its probabilities) and also 'extreme/maximal difference' (in the set of possibilities allowed), uniting very different structures in the same concept

    - identify useful structures (like 'information-intent connections' or 'intent-output connections') that can be applied as an alternate to solution metrics (like concepts such as 'balance') and the full set of structures that fulfill these to apply as components/base solutions
        - for example, for intents like 'acquire a product', its useful to know information like 'impact of product/purchase on economy', which creates a more optimal 'balanced' structure (of an intent and a counterintent, or 'reasons not to fulfill the intent') that can be applied in place of generally useful conceptual solution metrics like 'balance' bc its an implementation of the concept in another system
        - this can be abstracted to include other solution metrics in every solution, meaning a workflow to first 'find all the structures (like fairness, or a point-counterpoint structure) in specific structures (like a particular position in a system, like 'on either side of a purchase (before/after or purchase/instead of the purchase) in an economic system', answering the question of 'where should the optimal structure (balance between information/intent) be in the economic system (around the purchase)' and 'why' (bc given the insight 'think globally, act locally' and useful structures like 'common' structures, its one of the most common 'acts', and therefore also an important scaling structure that is very powerful as a result of this scalability and commonness, and 'powerful structures should be balanced as they are meaningful and meaning is positively connected to balance (they change in the same direction in most ways)')) that implement useful conceptual solution metrics (like balance and its connection to meaning, as in "balance in the position of purchases between global/local intents can allow meaning in a system")' and then apply them as base solutions to apply changes to or solution components to combine to find solutions for a specific problem
        - here Im once again applying the workflow 'find an insight, abstract it, and find a way to connect it to problem/solution structures, then differentiate it from other workflows if it is equivalent, to find a new solution automation workflow' (youll notice Im applying this with other workflows to first find the insight, in which I was thinking about how to prevent errors, such as 'what information stops people from fulfilling an intent', like a 'negative purchase that harms the world', and how would you frame that structure (as a concept like 'balance' in structures such as a 'fair or good decision' which is related to other important concepts like 'meaning'), and similarly was thinking about other insights like 'think globally, act locally' that Ive heard my whole life which are embedded in my subconscious but which I also frequently think about consciously, as its useful to be able to derive insights in many ways, so thinking about the same powerful structures and how to connect/change them in different ways is useful, as those connections/changes will likely be similarly powerful and might also be reusable in explaining other useful connections)
        - connecting queries (and components such as concepts like 'balance') to 'meaning' (the 'ultimate impact in relation to other meaningful structures') is so important that its why I frame that as the 'host interface' on which other interfaces can exist and where interface queries can be run and where they should start/end, bc interfaces/queries should all be connected to meaning and that should be the focus/intent of the queries (every interface query should end with a connection back to meaning, and every workflow would ideally connect to meaning in some way, if not directly then indirectly by connecting to problem/solution structures)
        - similarly, connecting workflows/functions/other interface structures to 'possible futures' enabled or made likelier by those structures is useful to identify future problems/solutions once that workflow is applied, just like how its useful to connect workflows to 'universes' enabled/probabilized by that workflow and 'graphs/networks/indexes' made more useful by that workflow and other problem/solution structures and conceptual networks and structures of relevance/meaning (given that all workflows connect to relevance/meaning by default, as 'change a base solution' and 'apply changes to certainties to resolve uncertainties' and 'connect different structures' are structures of relevance)
        - similarly, connecting workflows/other interface structures to core structures on every primary interface (such as heat/energy and speed/scaling on the 'physics' interface) since knowing how a workflow or other useful structure (that is likely to be used commonly or at scale) interacts with heat/energy and speed/scaling is useful to identify other useful info about it, like its probability of being used in a way that doesnt disrupt the stability of other systems like the system where its run, and similarly its advantage in speed compared to other structures is useful to identify to determine its usefulness in various contexts

    - applying different problem formats (like 'differentiate/oppose a negative structure') is useful to find useful structures like inputs to other problem format workflows (like 'filter a solution set')
        - for example, the 'type' filter is used to filter 'problem/solution types' ('pathogen types', 'drug types', 'drug component types') which means the errors of other examples of that type can be predicted more accurately once the type is known and an example is tested
            - similarly, other filters are useful to filter possible drugs, like finding 'matching offsetting differences' that should be corrected/differentiated (such as 'differences from pathogens', 'differences from other solutions/drugs', 'differences from existing system components', 'differences from pathogen or condition side effects') which are 'negative' so that finding 'opposing (positive) differences to that negative' is useful (like 'matching opposing structures of a side-effect receptor or pathogenic component')
        - other problems arent by default formatted as a 'find opposing/offsetting differences to correct a negative difference' format, but it is a problem format that problem types can be standardized to, since 'differentiate' is a default interaction function like 'connect' or 'reduce'
        - for example, in the 'regression' problem space, the 'negative' structures to oppose/differentiate are structures like 'randomness in the solution space (a solution space area that is uncertain)', which its useful to find differences from (like 'different solution bases or types to apply as base solutions to apply changes to')
        - finding the connections between problem formats ('differentiate/oppose' and 'filter') is useful to identify so they can be converted to each other ('differences are useful as an input to filters since they segment the solution space', and 'applying differences to a negative structure' is related to filters which 'apply differences to find reduced/unique structures'), which is also useful to identify, as it adjacently identifies structures that are useful in one problem format ('negative structures to differentiate from') which can be applied in other problem formats ('randomness in the solution space (which should be differentiated from, with opposing structures like maximal differences to filter the solution set)')

    - identifying useful structures like variants and variables of workflows as well as interim workflows in between core workflows which are still functional solution automation workflows
        - for example, 'change a base solution' has a variant of 'combine subsets of existing solutions', which is not a trivial subtype variant of the original workflow, but still has structures in common like 'existing/base solutions', and is still a solution automation workflow in that its likelier than randomness to have value in some cases/problem formats, and can be generated by applying a workflow using the 'build' function to the original workflow ('build a solution out of solution components')
            - similarly, the 'find' function can be applied to the original workflow, to generate workflows like 'change a base solution (for the "find a filter" problem, to find more optimal filters to be used in solving the original problem)'
            - the 'change/apply' function can be applied to the original workflow, to generate workflows like 'change a base solution (to find base solutions or changes to base solutions that are more optimal)' or applied to other workflows using the 'change' function like 'change problem/solution structures until they are adjacently connectible' ('change a base solution until its more similar to the problem inputs, and apply it as an input to other workflows')
        - these mixed interim workflows are different from core workflows which are likelier to be adjacent to existing formats and likelier to be useful in some cases
        - given the generality/coverage of these functions, there are many ways they can interact, so they can be combined in many ways to adjacently connect new structures
        - they can be used to generate other workflows with simple structures like embeddings/combinations and matching those embeddings/combinations to problem-solving structures like intents

    - identifying useful structures like specific types of similarities (like 'patterns' or 'internal function input/output or sequential similarities') that are useful for specific useful intents like 'filter the remaining solution set, from a subset'
        - for example, 'internal function input/output similarities' (similarity of a subset with another subset of a function, such as 'rotate the subset to get the other subset') are useful for intents like 'filter the solution set', bc if these similarities exist, trivial changes based on those similarities can be applied to generate the rest of the function rather than checking the full set of solutions, just like 'function-function similarities' are useful for 'finding similar functions that can be applied as alternative solutions to check' and 'finding maximally different functions (within some degree of difference governed by the similarity)'
        - some types of similarities are useful in contexts that are counterintuitive, such as how a 'pattern' (such as a 'sequence of similarities') can be useful in the 'regression' problem space, which is counterintuitive bc the rest of the function cant always be filtered by any given local subset of the function, although if there is a pattern like a wave, that is useful to identify by checking for that similarity type (sequential pattern) bc that type is possible in the definition of a function and does fulfill the intent of 'filter the rest of the function (the remaining solution set), from a subset' if there is a sequential pattern or other detectable pattern in change types/rates/other change structures
        - patterns (like 'repeating sequence') are useful to identify in that the repetition allows them to fill out the rest of the function without checking a large remaining solution set
        - the 'repetition', the 'trivial changes' (like 'rotations'), and other similarity types (like 'similar functions having some intersections/subsets in common') structures are useful for intents like 'filter', in a counterintuitive way (once you identify the intent of 'find the rest of the function from a subset', it is more intuitive/obvious, and once you identify the possibility of functions having other formats than a standard input for a regression problem, like functions with formats like 'sequential patterns', these other structures like repetitions become more obviously useful), whereas structures that are obviously useful for 'filter' intents are structures like 'maximal differences in functions' or 'randomly sampled subsets' that can be applied as filters or function approximators, similar to how 'approximation' is a more useful intent related to 'filtering solution sets' than other intents
        - basically any interface structure in the regression problem space (interface structures of 'data sets/functions') that creates or connects adjacently to other information (making derivation of that information trivial) can be applied in a similar way, to 'filter the solution sets'

    - identifying useful structures like insights such as 'if any abstract concepts can be used to solve a problem, its likely that other subsets of abstract concepts can be used to solve the same problem, given that abstract concepts cover reality in a structure like a field (or a quadrant of euclidean space or wedge of a circle, which can be rotated to cover the other sections)'
        - for example, different subsets of concepts like the following can be used to solve 'regression' problems, which are connectible on the concept interface but are different enough for this similarity in functionality to be an insight rather than obvious
            - 'power' (through more important or differentiating power of variables)
            - 'balance' (through its adjacency to core intents like 'comparison' useful for intents like 'filter solution sets' and relation to the abstraction/specificity dichotomy which is particularly relevant as well as its relation to structures like 'interim' which is useful for finding 'maximally different' structures like 'different base solutions to apply changes to')
            - 'specificity' (as a primary interface variable of 'abstraction')
            - 'certainty' (as a foundation for applying changes to known variable interactions to find the unknowns) 
        - similarly, applying this to other interfaces (if any physics structures can be used to solve a problem, other physics structures can be used to solve that problem, given these structures power/absoluteness/differentiability and connectivity on that interface) identifies physics structures such as 'vortexes' on other interfaces like 'information' such as 'info vortexes' (like a variable that, when added, creates a 'info vortex' that makes all certainties seem uncertain or less certain), which are related to 'interface queries' by an 'opposite' structure, as they add information using trivial changes
        - more powerful/interactive/absolute/core/interface/complex structures on an interface are likelier to be adjacently combinable in this way (as structures that contain/connect to enough differences that they can be used to solve any problem, so they act like alternates)
        - why is it true that equivalent alternates develop? bc powerful/useful structures are likely to be required for many intents which they cant all fulfill in real systems simultaneously, so backups are useful, and similarly most systems have enough variables that differences are stable and can compound in different directions, and exact copies are less likely just like extremely different structures are less likely, so in between there are interim similar but different structures that occur if enough variation is possible, and powerful structures are likely to have alternate states that they can form at rest or in different contexts, so 'variations of an original structure' are likely to create equivalent alternates at some point in their development, and 'trivial differences applied to a base structure' is a useful structure as it can frequently fulfill very different intents with these trivial differences, which are another way that equivalent alternates can develop
        - primary concepts can be described with interface interactions (like 'power' describes important input/input interactions like an 'input that can replace other inputs' or input/output interactions like 'low input/high output', and balance describes a 'specific state of "equivalence" in a connection between important structures like compared structures') that can be used to identify other primary concepts that are useful in some way ('equivalence' is useful for a 'compare' intent, and 'power' is a definition that can be adjacently used to describe 'usefulness' or 'importance' or 'relevance' or 'meaning' or 'efficiency')

    - identifying useful structures like 'false similarities' applied to structures like 'definitions' is useful for intents like 'identify/filter', given the usefulness of 'definitions' for those intents
        - for example, finding all the ways a structure can be false (an embedding that makes some identifying attribute false (such as in a 'false copy' or a 'false context') but still be included as a component, so it will still seem true if only that component is checked, a falsely similar substitution of some identifying attribute, a false subset of identifying attributes that is used in an identifying index) is useful for identifying the real/false structures
        - this is useful bc real systems often have false structures, as false structures (imperfectly complying with some definition) are cheaper than true structures (perfectly complying with some definition), to the point where they can be assumed to be more common or the default

    - identifying useful structures like insights such as 'similarities can be used as filters' by applying useful structures like differences and checking for similarities in those differences, to find useful intents like 'find function similarities (and similarity types)' and specific structures to implement these intents
        - for example, a function can be mapped to a set of its identifying structures (x^2 has an identifying structure of a parabola (so the 'probability of a function being x^2 (or other exponents) given that there is a parabola at all, is higher than not'), bc these structures and the functions have a similarity in common (the set of possible functions can be filtered by knowing a set of structures that are associated with a function)
        - this similarity can be applied as a map (a set of structures to a set of functions having those structures)
        - this similarity between function structures and functions can also be applied as a network of term types (or 'significant example (such as defining example or differentiating example)' terms or term 'units') and their neutralizing/magnifying terms (the set of functions you would need to apply to x^2 to erode its identifying structures, even in functions with an x^2 term, such as offsetting 'fraction coefficients', 'inverses', and 'higher powers')
        - other similarities exist between function sets, such as the mapping that adds useful attributes like non-linearity to a function, which can be used to 'connect function sets' (or convert a linear function into a non-linear function with some similarities like general structure, intersection points, average, etc), which is a useful if indirect problem-solving intent in the 'regression' problem space
        - this is in contrast to other attribute sets like 'generality' and 'linearity', which dont have a 'default similarity (like a higher probability of some similarity) to use as an identifying filter' (there are 'general' functions which are either 'non-linear' or 'linear', with no clear differentiation like a 'more probable attribute value', although 'simplicity' of a data set can connect 'generality' and 'linearity')
        - this means the info is retained across this similarity transform/mapping (it is symmetric in some defining structure)

    - identifying useful attributes like 'intersectivity' can be used to adjacently identify other useful structures
        - identifying that 'systems of linear equations' can help identify 'intersectivity' is useful to identify 'systems of linear equations' as a useful input to finding similarities (such as intersections) between functions
        - identifying structures that fulfill the intent of 'connecting subsets of a function' is a way to identify 'intersectivity' as a useful structure
            - for example, identifying that simple transformations like rotations/shifts/scales of one subset can identify another subset (in some cases), and similarly simple direct connections can be used to connect two subsets, and similarly components of one subset can be used to identify another subset, and similarly intersections of two different subsets can be used to connect the subsets
        - identifying that 'intersectivity' is particularly important in differentiating functions that are locally similar or have similar averages
        - identifying the 'intersectivity' of functions is a similarity metric that can be used to differentiate them
        - identifying 'intersectivity' is useful for finding similar functions (such as functions that are 'similar to averages', 'similar to data sets', 'similar to maximally different base functions', 'similar to local function subsets', etc)
        - identifying the 'intersectivity' of tangents as a way of finding 'function limits'
        - identifying functions that are highly 'intersective' is useful for finding a reduced set of functions to check when some of the function is already identified
        - identifying maximal differences based on 'intersectivity' (like the most different structures that can have a particular line segment in common, like a parabola/circle) is useful to find functions to filter once an intersective subset is known
        - relatedly, identifying the 'reason for similarities/differences' is useful, as in identifying 'coincidental equivalences' (function sets may have a subset in common but are still so different they may as well not have anything in common, such as a 'required equivalence (such as all terms with a coefficient of 1 which are a power of x must cross y = 1) that doesnt summarize the rest of either function or connect adjacently to the reason for the intersectivity or help differentiate the functions having that equivalence') which are useful to differentiate from other equivalences that are more significant (functions that have a subset in common and its bc theyre an adjacent transform of each other)
        - identifying 'interface structures like reasons' of errors like 'insignificant and/or false intersections' (like the common intersections of a straight line with a wave, in which a 'higher power is missing' and the 'intervals of the wave align with the subset tested for equivalence with the straight line') is similarly useful

    - deriving inference rules like 'what is definitely not contained in a function (a set of changes applied to inputs to generate outputs)' (its inputs/outputs are almost definitely not bc of the existence of the function which implies a 'lack of those structures, indicating the existence of a structure to create those structures', unless its the defined exception that requires it to contain its inputs like an identity function) is possible by applying changes to certainty structures (like 'definitions') and checking if the limits of those certainty structures are violated
        - the definition of a function indicates a difference between a 'set of changes' and 'inputs', otherwise they would be connected with equivalence structures such as 'like/is' or the definition would likely use the same term rather than different terms
            - given this identified difference in the definition, a query like 'what is different from a function' is trivial to answer (the inputs/outputs)
            - deriving questions that are also trivial to answer is similarly possible by applying trivial changes 
                - questions like 'what different functions are equal/similar to this functions' with answers like 'functions with similar inputs/outputs'
                - questions like 'what causes function inputs' with answers like 'functions with the inputs as outputs'
                - questions like 'what requirements does a function have' with answers like 'function inputs' and 'a system where function inputs can exist'
            - deriving questions that are not trivial to answer is similarly possible by applying maximally different changes like interface structures (which already capture high variation, as they are defined as being the more useful structures) such as 'changes allowed within a definition but which are maximally different within that definition (like allowed contradictions of sub-types within a type)'
                - 'whats a previously undefined statistics structure related to a function' with answers like 'interaction levels where this function is adjacent/default' or 'variable embedding interaction functions which can predict most variable interactions' or 'different formats of a function like filter sets or indexes/mappings' or 'densities paired with associated tensors whose trivial rotations (within a limit) or intersections cover most of the data set or most of its ensemble averages across algorithms'

    - identify useful sets of implication/confirmation or implication/contradiction structures (like a point that when paired with a corner implies a shape, a tensor, or an angle sequence, and when that set is paired with another corner, a shape is more certain if the corner has an attribute like 'concavity' and implies another corner that can be derived and checked without traversing the entirety of the third line, just like the full set of remaining corners of a square can be derived/checked once two sides are known, paired structures that confirm/determine/contradict the implications of the initial structure)
        - this is related to applying 'maximally different variables within a solution set/possibility space, once a filter is applied'
            - this structure of 'increasingly incrementally more specific filters (diminishing returns of each subsequent filter)' is only optimal in cases where the initial filter is the primary differentiator (like a general type variable) and every filter that follows is more specific (like a sub-type variable of that initial type variable), which is unlikely to occur by default without applying organization structures to force it to have that pattern, as variables would need to be sorted to make that filter pattern relevant
        - similarly, identifying non-adjacent structures (like an 'allowed sub-type definition contradiction (like a straight line segment in an otherwise curved function, which is allowed but violates a function type definition of a non-linear function, which is allowed in the definition of a continuous function but not in the definition of a curved function, which are associated but not required to be equal, and is also improbable)') is trivial just like deriving adjacent structures is trivial (like a 'continuation/repetition/iteration of a local pattern'), and both are useful to identify and apply as structures to check/filter
        - structures of volatility are similarly useful to identify in a problem space (such as 'non-repeating structure inputs' and a 'high number of variables' and a 'degree of difference from randomness')
        - structures (like 'allowed sub-type definition contradiction within a type definition' which are similar to a 'paradox' structure, which is a maximally different interface structure) that differ from simpler structures ('iterations/extensions/rotations/other trivial changes applied to a local pattern') are possible interface structures to derive/apply as defaults for intents involving maximally different structures (which are common in problem-solving)

    - identifying useful structures (like 'similarizing differences') useful structures (like 'network hubs') by applying known useful structures (like the 'commonness' value of the 'frequency' variable to derive important interface structures like 'probability')
        - a 'language network' used to find the 'most common words/nodes' would trivially identify structures like 'network hubs' as useful, the reason for this usefulness being the fact that the difference between 'network hubs' and other structures is that 'number of connections' (which act like a similarity between many different structures)
        - identifying similarly useful structures is a matter of identifying/deriving variables, applying these differences, and identifying new emergent differences in core structures like networks, as the primary differentiating variables of core structures like networks are likely to be useful for other intents (applying a workflow of 'finding useful structures' and 'finding intents to use these structures by some similarity in intent once useful structures are known, given their high probability of usefulness and similarity to some useful set of intents, like how a "network hub" can be useful as an input to an average/density/similarity-finding function')
        - similarly, identifying useful formats of info structures (such as formats of 'filters' like 'twisted light' as 'different paths to connect info on different sides of useful positions/structures to connect') is a way of formatting problems in a way that can make use of other system rules to identify 'maximally differentiating variables' of useful structures like filters (the problem becomes 'find a useful set (like the minimal set) of connections/angles that can connect the two structures on either side (optionally using interaction functions of light, like how its preserved across various filters)')

    - identifying useful structures like 'filters' of problem structures like 'solution spaces'
        - for example, when solving the 'find a drug (input) that corrects/changes a condition' problem, the set of possible solutions can be filtered by removing the 'known common inputs', as those are likely to be similarly distributed in people with the condition as the general population (known common supplements like turmeric can be removed as a possible solution, as its unlikely to be the solution, given that its a common input in general and is also likely to be a common input in the subset of the population with the condition)
        - 'filter out known common inputs' can be applied as a filter to find 'uncommon (uncommon in the sense of being new/rare/unnatural) substances'
        - this is a way of deriving info (which substances to ignore/filter out) without having info (info about the specific mechanism of all substances or the mechanism of the condition), using other more available relevant info ('what inputs are common' and 'whether the substance is likely to be similar/different from the common inputs (different)')
        - the info of 'what inputs are common' can be made relevant to this problem by applying changes ('similar/different' applied to 'common inputs') and checking if those changes are useful (is a 'similar substance to a common input' or a 'difference substance to a common input' likelier to be useful for the condition), and given that workflows/rules such as 'change existing suboptimal base solutions to be more optimal for a problem' and 'if a problem exists, it is unlikely to be solved by adjacent/existing/available inputs' are known, and given that rules of probability/reality are known like 'small input changes are unlikely to produce large output changes (volatility isnt a default structure without other attributes like independence, as very independent non-interactive structures are likelier to be extremely different and therefore likelier to produce attributes of randomness and related attributes like volatility)', and similarly other rules like 'if an adjacent input was the solution, it would likely already be known' are known or easily derived, and given that specifically 'common inputs' have already been tried as a solution indirectly and can be determined to not definitely be solutions, these rules can be connected and applied to determine that the 'common inputs' should be differentiated to become a solution ('differences from common inputs are useful')
        - additional filters can be applied like the 'commonness of the condition' (if its a common condition, then the uselessness of common inputs is even likelier than if its a rare condition)
        - therefore by connecting the available relevant info ('what inputs are common') with problem/solution structures ('what solutions already exist and have already been tried' and workflows like 'change existing solutions to be more suboptimal') by applying changes ('structures that are similar/different than common inputs') and applying interface structures (like 'probabilities', such as 'info that is likely, if other info is true') to check if those changes are useful, the solution space can be filtered to exclude the useless structures ('similar substances as the common inputs')
        - filters like this can be applied as 'alternate filter sets' to identify solutions that 'fulfill multiple filter sets' (to find the 'most probable solutions') or to identify solutions that 'fulfill different filter sets' (to find the 'maximally different solutions')
        - this is related to the insights that 'a solution might only work in one case' (with the associated implication that 'it might be the case that was checked') and 'a solution might only work bc it was changed a lot' (with the associated implications that 'these extreme changes might have occurred', and 'other solutions exist which are more optimal/adjacent', given that 'its possible to change any structure into any other structure')

    - identifying useful positions (like specific interaction levels) where useful structures are already default/specified which makes useful intents adjacent

        - applying/implementing 'embeddings, maps, and other powerful interface structures' to neural networks
            - if a node/subset of nodes/layer can learn/apply a 'space/context-switch' operation (like a function to 'identify possible relevant/useful similarities/differences' and a function to 'generate a change that will probably make some relevant similarity/difference obvious'), these can be used to fulfill more complex intents than 'apply standard algorithms to combine adjacent changes' or 'learn an index mapping one structure set to another', intents like 'find emergent effects that are obvious in another context'
            - these structures can be applied as sub-functions that emerge from a sub-network of functions within the network, or as additional structures that can be applied wherever an opportunity/reason/requirements for additional variation or optimization is identified in a subset of the network

        - 'finding/deriving the system (find the filter applied), from a position in the system (from a filtered position)' (such as by 'applying symmetries to "remove filters/limits used" or "abstract variables specified" to create the filtered position') is a useful problem-solving intent to apply as a 'core structure of understanding' ('reverse the trap')
            - for example, a definition of the 'universe' as a 'description of maximal differences, which can compress reality adjacently (like with a few folds or a repetition of some core unit)' or an equivalent alternate like 'an emergent structure which can make sure it always evolves (can use any input to generate itself)'

        - identifying structures of time/change as 'embedded time/change, relative time/change, time/change base, time symmetries/position, and limits of time/change' being additional relevant variables, rather than just its 'direction/asymmetry'
            - similar to how 'membership in a set' can be more important than 'which member of the set an item is (what its position is relative to other items in the set)'
            - for example, having a 'gravitational boundary' on possible time/change allows faster time/change to bounce backwards against boundaries and interact with slower time/change
            - as another example, a structure of 'fluid time' that changes until it fulfills some structure like the 'interface network' and then becomes a constant, or a structure of time that is constant in that it requires a field of uncertainty surrounding any structure to allow some structure (like a 'ratio') of interactivity/potential or some interaction with other structures
            - from this structure, its adjacent to identify that 'symmetries' act like 'positions' and identify useful queries like 'are there absolute connections between symmetries and asymmetries, such as that every symmetry has some connected asymmetry, leading to change in a direction away from the symmetry position and therefore its invalidation, or are there usually some combination of coordinating complementary asymmetries that create/maintain the symmetry by their net effect'

        - 'time structures', 'filters and filter derivations', and 'maximal differences from standard/adjacent change combinations (embeddings, maps, etc)' are useful starting positions for identifying default/obvious and non-default/counter-intuitive structures that are useful, and identifying the intents/structures that are adjacent once those defaults/non-defaults are found, for fulfilling other intents like 'identify extreme/scaled limits of a structure' (what happens when time is iterated at scale, according to these possible structures of time), which are known relevant/useful intents for problem-solving intents like 'identify requirements' (thereby connecting these interim structures like "useful high variation interaction levels" to useful problem-solving structures, allowing variables of this connection (variables of 'core change structures') to be identified as well to fulfill intents like 'generate other interaction levels that are useful to apply as interface structures')
            - these structures are also interfaces or sub-interfaces (time is a 'useful standard that allows structures to be compared in a relevant way', which is an interface)

        - similarly, in a different interaction level of 'number types (and representative constants/units of those types)', imaginary numbers are identifiable as useful in that they are related to rotations and a 'rotation' is an input structure of another useful structure, 'orthogonality' (and can be used to create/identify other differences like independence, maximal differences, symmetries, etc), and that they connect other relevant numbers of rotations (like pi) with numbers like i which offer a different difference type, so i is useful as a default structure as it adjacently identifies other useful structures and fulfills useful intents adjacently
            - similarly, quaternions offer 'embedded differences in their interactions', like how they are equivalent alternates at generating i (using a square operation) and also have different interactions that can generate i (multiplying them by each other)
            - similarly, imaginary numbers offer independence in the form of non-embeddability, in their connection to a similar but different space (where i is a unit) whose interval-determining output points (the powers of its units, where the corner point unifying some i-sided square intersects with euclidean space, acting like an endpoint of two vectors, like every negative number is a base point of right-angled vectors of alternate i-unit operations, and positive numbers have vectors of 1-unit operations and a power of the power of the i-unit operations as well, as positive numbers cant be used as inputs to a power to generate the negative numbers) are embedded in euclidean space by reference of the power operation
            - relatedly, questions like the following are adjacent from this position
                - 'is the directedness of the negative/negative multiplication operation that can create positive values (in contrast to the positive/positive multiplication operation which cant create negative values) significant in determining the existence of a number type that is relevant in creating differences like opposites and which has embedded reflections of this connection (the imaginary/real roots act similarly, as they apply the same operation and are inputs to the original negative/positive values referenced), and are there limits to this reflection of this connection that dont align with the limits of the definitions of the numbers/structures themselves but rather require different structures to identify those limits'
                - 'does the directedness of that connection align with other structures having direction, like useful causal input/output sequences (should imaginary numbers be used as inputs to real numbers for some purpose like expanding dimensionality, just like neural networks expand dimensionality of inputs to apply multiple alternate change sequences that could connect inputs/outputs) and time (are there continuous/regular/adjacent sequences of synchronized changes in "number definitions" which can describe time)'
            - relatedly, 'intersectivity' is another possible input of 'orthogonality' and intersections which create a difference (in 'direction') in a similarity (same right angle applied every time), when applied in some structure (like an iteration of the right angle), or intersections which create an equivalence (in the area/distance/angle structures identified by the separation created by the intersection) add value in creating orthogonality, and relatedly intersectivity adds value in that it standardizes the area created by unit distances in either axis of the intersection to match (and minimize the work of) standard multiplication
            - these numbers (and structures of numbers like number types and opposites of a number type within the type definition) exist by being useful to connect other number sets, some numbers seeming to exist only to complete a set (like a set of type examples) or fulfill a useful definition like continuity or isolatability (for other intents like traversibility or repeatability or adjacency of a specific number type to another type, or to provide a connecting/isolating number in between some pair of numbers, to avoid making those numbers adjacent or equivalent or otherwise non-isolated)
            - other numbers seem to exist to implement interface structures like 'limits' (infinities) or 'interaction levels' (relative infinities), as if every number is guaranteed to have some related number that acts as each interface structure (like a limit) on it when it occurs in some set or is acted on by some operation, as if numbers are the default/unit system where interface structures can exist or be identified
            - the fact that there are absolutes (like a highest infinity or most unitary unit) indicates that there are different starting points to view number sets from, which make these absolutes obvious and which make these sets obvious (infinities acting like waves which repeat, self-sustaining changes without contradictions, or non-zero continuities which reference an infinite set)
                - building a definition of an object like 'infinity' makes it clear how some structures are more optimal to define it with, as some definitions are non-unique in that they can reference other structures and are therefore less optimal, but which can be applied optimally as a surrounding structure with other surrounding structures which act like a set of limits creating the definition
                - these interim definitions (which describe very different structures non-uniquely/generally) can be useful as a default set of definitions to apply to adjacently connect other structures
            - given that these intents (like traversibility/specificity arising from continuity) are fulfilled, they are likely to be useful in some way for other intents (such as 'make sure that all numbers are connectible' and 'make sure there are numbers that can be inferred with other descriptions/definitions, but which cant describe real structures inside the universe, to provide a path to other structures outside the universe')
            - given that a rotation is a core structure of a symmetry/center, a circle, a change, a base (like a vector set with a common endpoint as a base), and other useful structures, its useful to identify a rotation as a default structure and find equivalent alternate formats of it and routes to it

    - identify structures that are relevant/useful to useful structures (such as relevant in that they 'cause' useful structures) like 'similarities across different equations (different as in "not reducible to the same equation")'
        - for example, functions having 'similar defined input/output ranges or similar high-variation ranges where their variation occurs (as opposed to where their constance occurs which is less likely to produce intersections)' are likelier to have intersections than other functions
            - within that filtered set of functions with similar defined input/output ranges or similar high-variation ranges, functions with 'similar shapes/scales/averages/limits but which are trivially different in that they are shifted in one variable to preserve a similarity in another range, or so their difference types dont align in some variable (like when peaks/inflections are not aligned vertically)' are similarly likelier to have intersections
        - this is related to the workflow 'find interactive structures and then apply those as possible connection structures to connect a problem/solution'

    - identify structures that differentiate useful different but related info formats like 'truths/falsehoods' and 'certainty/uncertainty' and 'similar/different' and 'constant/variable', which are the foundation for different primary interfaces but are adjacently connectible as variants of each other
        - for example, a connection between 'truth/falsehood' and 'constant/variable' is constant structures are likelier to be truth structures and variable structures are likelier to be false structures (variables are likelier to be in a state of change, either changing toward a value change or decay or toward further differentiation or toward resolution into one constant value, where constant structures are likely to change toward a value change or toward decay or differentiation but have fewer states which could be likely to change than variables as constants dont have the potential to change by resolution into a constant value as theyre already a constant, and if theyre already a constant, it is likelier that they will remain one than that a variable will become constant, as structures which are likely to exist are likely to already exist in some way), so 'finding system initial conditions, scaled convergences, decay points and change thresholds/limits (as the system will occupy multiple states between these structures so finding these structures determining the boundaries of those states is more useful than finding an individual state)' is likelier to be a more useful intent than 'finding current variable interactions in a current system state'
        - identifying these connections is useful for switching between these formats, identifying useful formats, and applying multiple formats to fill in complementary information

    - identify structures that are useful for some useful problem-solving intent like 'find the remainder/some of the function, given a subset'
        - for example, structures that derive some abstract variant of that intent like 'derive a part from the complete set' or 'find a common similarity (solution value) that connects a system of equations', structures which can be used as a default structure to apply when searching for specific variants (or combinations or other structures) of the structure which are useful
        - identifying these alternate specific intents which are useful for some problem-solving intent is useful where inputs of those specific intents are already available (like 'partial differential equations'), which are also useful for other intents like 'simplify a function (by identifying which variables determine its highest variation and removing them until its linear)', which is an attribute of useful structures

    - identify structures that resolve ambiguities like 'structures that specify an example of some general description', where the 'example' or the 'example with the description' is more or independently or compoundingly useful in combination with the description, than the 'description on its own'
        - for example, the phrase 'similarly similar' is difficult to capture/define by a structure, until you apply a structure like a 'space where some example similarities are comparable and clearly similar but trivially different', where this structure is more and independently useful for its clarity through its structure than just the term 'similarly similar' on its own, and bc it occupies a more 'interim' position on the space of reality variables (independence, abstraction, certainty, etc), and makes other structures (like 'clarifying structures that make some similarity/difference obvious') more trivial to find as a result of these other attributes (similar to how 'similarly similar' makes other useful structures like 'similarly different' more trivial to find), just like the concept of 'abstraction' is useful on its own but specific useful abstract concepts like 'power' and 'balance' are more useful to apply as a default position
        - bc of the high variation possible in defining an abstract structure like 'similarly similar', specifying an exact example associated with this structure is useful to filter that set of possibilities, 'abstraction-resolution filters' being useful structures to identify
        - finding a space where 'abstract concepts are iterated and allowed to interact and vary enough to fully capture their definitions to a useful degree of specificity' (applying components of reality like 'iteration/repetition' and other interface variables like 'change' and 'interactivity' to fulfill intents like 'expand possibilities' that enable fulfilling intents possible with these structures like 'specify' and 'structure') is another useful intent

    - identify structures which are highly explanatory through interactions with simple/default/core/otherwise useful structures
        - for example, functions like 'pretend' and 'lie' can be iterated as a repetition of a core structure of 'falsehood', as 'pretending to (pretending to (lie))' is an iteration of a common structure of falsehood ('lie') which can explain more complex system dynamics when functions (like 'lie') are not invalidated ('still allowed') but simpler variants of them become less useful over time, as other agents change at similar rates
        - the 'lie' structure is useful in systems as a 'cheap/efficient structure' where 'false signals of value' are more efficient than 'true signals', but become less useful as other agents change, and iteration of these structures becomes more optimal while still being cheap/efficient, as the 'lie' structure is still allowed in a given system
        - identifying the points where 'efficient structures' (falsehood) intersect with 'inefficient structures' (truth) is useful for predicting future behaviors (it can become more efficient to tell the truth at some point, so much so that it is useful as a default strategy, as signals of true value are useful to other agents in a way that signals of false value are not, as truth structures pay dividends when applied as defaults)
        - the 'iteration' structure is useful to identify as a powerful explanatory variable when applied with core variables to generate/describe complex system interactions in a simple way (as an iteration of the same core structures)
        - 'pretending to (lie)' is a cheap way to falsely signal true value ('obviously pretending to self-deprecate, when the deprecation is actually true' to signal 'true value' in a false/cheap way) when 'lies' are applied as a default structure that can be identified by other agents
        - 'iteration' structures are a useful way to identify 'embedded/symmetry' variables
        - similarly, 'iteration and other symmetry-hiding variables' are a way to identify 'false complexity', where 'true complexity' is an interim structure between 'random' and 'symmetrical or otherwise simple' functions
        - these structures are useful to identify for adjacently predicting other future structures, like how an increasingly optimal 'truth identification machine' would capture & alter reality more quickly (given the relative simplicity of default 'simple or otherwise adjacent/default, but false' lies, as well as default lies based on truths), given its usefulness at supporting a high degree of variation, and how supporting more variation attracts more variation
        - similarly, identifying variable structures like 'degrees of truth' and useful cases of differences within those variables (such as 'statements that are minimally true, given the structure of the relevant definitions' and 'statements that are obviously true/false' as a way of finding 'connections between definition-invalidating differences (some structure like a limit making some statement impossible by trivially changing it to cross the limit) to connect structures like the minimally true with the minimally false and maximally false (impossible), which are useful to apply to make other structures like their differences obvious')

    - identify different useful interactions between data set subsets, complete data sets, solution functions, solution function ranges

        - identifying networks of 'data set subsets that could be a subset associated with many highly different complete data sets or solution functions' which can be used to navigate between data set subsets, starting with each of the subsets that could be the most alternate complete data sets or solution functions (having the most in common with the most other subsets) and testing for difference types to navigate to more specific ranges of possible complete data sets/solution functions, up to a point of accuracy on some metric

        - identifying maximal differences in a data set that can be mapped to one function in a set of maximally different functions, as a range of different shapes of a data set that could be described by some solution function that is a function in a set of maximally different functions, as a way of finding the maximally different data sets associated with a solution function that is maximally different, to 'identify possible solution functions to use as a base' and 'identify whether the data set matches any of the maximally different data sets associated with each of the possible maximally different solution functions as being within an error range of some metric'

        - identifying 'implications of symmetries in a possible solution function' as useful for intents like 'determining the remainder/some of the function from a subset of the function', as 'symmetries' can be formatted as a 'core prediction structure' that can be applied to 'predict some subsets of the function, once an "implication of a symmetry (like a similar slope sequence)" is found', and similarly identifying structures of symmetries like 'symmetrical peaks' and 'symmetry breaks (which can indicate a position of a new symmetry)' and 'symmetry embeddings (of layered/nested symmetries)' is useful as a set of structures to identify as an input to a function to 'identify the remainder/some of the function from a subset'
            - relatedly, identifying 'differences from symmetrical functions/data sets (like symmetrical waves)' is a useful intent when fulfilling intents like 'building a function similarity index' and 'predicting the remainder/some of the function from a subset', as 'differences from symmetries' are similarly useful as 'symmetries' to know about as an input to a 'find the remainder/some of the function from a subset' intent
                - relatedly, identifying a space of similarities that are useful through co-occurrence like 'intersection ratio' and 'symmetry type (like recursion or rotation symmetry)' function similarities, which are useful to identify when they co-occur, as this provides alternate functions to determine similarly similar functions, is a related useful intent to 'finding useful similarities to build function similarity indexes on'
            - relatedly, identifying symmetry-breaking changes (like 'sort') that alter a function beyond its symmetries is useful to identify 'functions to avoid, in a space of functions to apply when identifying functions (like "blur" or "compress" or "filter") that can change an input like a data set or subset or function into a more useful format'
            - identifying a 'general symmetry score' across 'maximally different spaces where various structures like combinations of symmetries are used to defined distance/position' (which functions retain their symmetries when indexed differently by some set of determining function similarity metrics) is a useful intent for 'function similarity indexing'
            - identifying 'maximal differences' in a 'function similarity index' is useful for finding useful alternate solution bases to apply as inputs to a workflow like 'apply changes to a base solution function to find a more optimal solution function', as 'navigating the function similarity index to find patterns of maximal differences and therefore finding the maximal differences' is useful for finding patterns of contradictions (determining differences between similar functions) to filter the solution space (like 'identifying when a function only overlaps with another on one point or at inflection points or in general structures or at its roots or at a high ratio of subsets' and 'identifying the useful structures of these similarities/differences to use when navigating the function similarity index to find patterns of maximal differences'), as identifying 'functions with the highest similarity score and the highest difference score' is useful for other intents like 'find common/probable functions that also are maximally different from other functions (unique) in some way'
            - relatedly, identifying the useful 'interim function similarity indexes' which are in between a 'similarity index of all possible functions' (most expanded index) and a 'similarity index of abstract function-determining/describing variables' (most compressed index) is another useful intent related to 'function similarity indexes', similar to how finding a 'similarity index of similarity indexes' (organized by absolute similarity or abstract similarity or otherwise useful similarity such as 'realistic similarity' that mimics their indexes' true connections in reality, like how 'similarities are embedded on foundational similarities, up to a limit') is useful as a way of organizing these indexes in a network that can be queried to find relevant functions/similarities/differences, interim indexes which are useful structures to identify for their useful mix of 'reality dichotomy' variables like 'certainty/uncertainty', 'simplicity/complexity', 'similarity/difference', 'independence/dependence', and 'generality/specificity'
                - relatedly to these 'reality variables', identifying structures that would be useful ('spectrum variables determining reality') which can be used to fulfill other intents like 'identify a continuous space representing reality to traverse, to fulfill other intents like "identify areas/points/structures of optimality"'

    - identifying useful interface structure interactions in a problem space and applying those based on their interactions (like their interactions with requirements)
        - for example, a general and specific function are often different but there are cases where they are the same, so framing their connection as 'more probably different but not definitely different' is useful for determining differences probable/required in different solution functions
        - a 'general' change type is less likely to occur with a high degree of variation, and similarly a 'specific' change type is likelier to occur with a high degree of variation, but these are also not required
        - connecting these conceptual functions with other interface structures like probabilities and requirements is useful for identifying a solution function based on conceptual requirements like 'an interim function in between extremes like general and specific'
        - identifying and filtering out cases that dont apply for being less likely or less useful (like cases where general/specific functions are the same) is useful when identifying structures like 'data set-function indexes'
        - relatedly, identifying when a different function format is optimal, like identifying cases where its optimal to retain a 'mix of different (as in unrelated) specific/general functions' in the same solution function, is useful to avoid suboptimal structures for a problem or input
 
    - identifying interface structures resulting from common queries connecting interface query components like 'what would be useful if it was true' (in known/measurable and/or complex or otherwise useful problem spaces) that can be applied as alternatives to those query components
        - in the case where the solution intent is a 'way to change greenhouse gases to avoid heating' and a 'specific intent' implementing that general intent is a 'way to configure molecules to avoid heating' and specifically a 'way to gather molecules to avoid heating', applying a common query like 'what would be useful' (what structures would make this possible/adjacent/efficient/optimal in some way) identifies useful sub-intents like a 'way to scale existing molecule-interaction tech (like lasers) to cover more area than usual (like in different shapes like cones to have higher output from the same or similar input of a laser)' which filters the solution space and identifies structures like 'scalability' or 'efficiency' that can be applied as alternatives to the query 'what would be useful to connect intents or specify some intent'
        - this is actually a new structure bc its an 'interim intent' (between problem structures and solution structures) which would be useful if it was true (but isnt definitely known, as its not an existing/known useful structure, so it has to be generated/derived/found as a specific structure that is useful for a specific problem space, then checked for usefulness), which provides a useful filter to focus work on to identify if inputs to that useful structure are possible, at which point the useful structure can be created and applied to fulfill the target solution structure
        - in the example of the 'find a drug for a condition' problem, the related useful intents that can be derived include intents like 'find a drug that has general system optimization attributes (like improving inflammation or lipid metabolism)' or 'find drugs that fulfill multiple coordinating intents' (and apply them as a default reduced solution set) or 'find drugs that fulfill multiple contradictory intents' (and filter them out as less probable solutions)
            - similarly, 'find a drug that changes system-level variables' (with associated implication regarding inputs like 'bc the input condition is a system-level condition') applies an 'interaction level' filter to find a useful filtered intent that is more relevant bc of the filter
            - similarly, 'find drugs that change most variables' (with the associated intent of 'to find drugs to exclude, as these produces too many changes, more than what is likely required to correct this condition, and is likelier to produce more negative side effects than positive side effects even if the positive side effects are to correct the condition')
            - similarly, 'find drugs with some structural similarity (like similarity in functionality structures, that reduces some attribute/function) in the intent/function/interface structures (like functionality) to fulfill specific bio-system intents like "regulation of errors from over-functionality" (which align with reduction of functionality)' (find 'reducing functions' to fulfill 'regulation intents' bc of the structural similarity producing the alignment between 'reduce/regulate' intents)
        - these intents are useful to derive bc they provide focus in reducing/filtering the space of structures to search for (now the problem is 'find a drug with this attribute' instead of 'find a drug that corrects this condition', which offers value in its specificity and may also be more trivial than the general original problem-solving intent)

    - identifying useful computation intents and the limits of computation which would make those more useful when incompletely implemented in a specific subset of ways
        - for example, in the 'regression' problem space, identifying a '"data set-function" uniqueness mapping causal/filtering diagram' which maps 'sequences of differentiating/determining points to check for in a data set' to select a 'representative function'
        - this type of algorithm network (a network of directions representing probable flows as causes or filters of the sequence toward an output function, and nodes representing data set points (or missing data set points as a useful signal, or similarly data set subset areas, densities, similarities/differences, local averages, or other related data set structures), and leaf nodes representing output functions) would be useful only up to a point, with finite computation available (unless every available computer is forced to constantly compute more complex mappings to infinitely compute it as much as possible)
        - determining that point of limits on its usefulness (only compute the mapping up to a level of difference in the data set represented by some metric like a ratio of randomness applied to common variable interaction functions, and only up to x number of data points in the data sets) can be determined by filters like 'reduced rate of increase (diminishing returns) from reward from computing more fine-grained or complex data set-to-function mappings'
        - this is the 'trial and error' algorithm applied to a 'pre-computed index of problems (data sets)/solutions (regression functions)', which is more useful when applied incompletely (selective trial and error) if implemented a specific subset of ways (applied to maximally different data set/function pairs covering many types of variable interactions in real complex systems, etc)
        - finding all the 'ways a function can be incomplete but still useful/similar (like retaining slopes at determining points which can be used to derive the function to some degree of accuracy)' is an intent that can identify the differences (subsets taken out of the function which allow the function to retain its general structure and determining attributes like inflection points) that should be applied as filters of functions to solve the regression problem for when building a 'unique data set-function index causal/filtering diagram' ('direction reversals (like found at peaks)' are a useful difference to apply when identifying functions to solve the regression problem for) or identifying function types/base functions to check for when filtering the full solution space
        - checking for 'function volatility sets' (extremely different functions that can be generated with trivial changes, which can be used to check the rest of the data set for signs of the other functions in the volatility set which indicates a higher probability that the function is in that set and switches between them given the allowed input variable interactions) and 'sub-function volatility sets' (extremely different subsets of a data set or function with extremely different implications for the rest of the function which are almost contradictory but are part of the same function) are other useful filters to apply to avoid checking the whole data set while still identifying its general probable structure
        - identifying 'subsets of a data set or function that can be used to derive maximally different functions' is similarly useful bc these should be discarded or specified with additional filters, rather than applied as specific filters that have a one-to-one mapping to a solution function
        - similarly, 'identifying data set input ranges to divide it into subsets to select from, then a subset of a data set across many different input ranges using some filter, then identifying the implied solution function (or structures of it like a probable range), then applying checks for differences that would confirm/invalidate it with the most certainty (like combined difference types when the solution function is tested on some subset, such as constant/non-linear slope and high value difference)' is a similarly useful set of sequences to identify, to select a subset of probable solution functions
        - similarly, identifying specific error filters of maximally different functions that could all describe a data set (like how a highly volatile wave could look like randomness) is useful for identifying possible errors like false similarities and tests to filter those similarities (the differences in the similarities, like a 'lack of direct connection function between peaks')
        - similarly, identifying 'areas of the solution space to skip, given the extremity of differences to the probable solution function parameters and variable interaction functions' is useful as a way of filtering the solution space to only include similar functions (by their parameters and variable interaction functions) to the probable solution function like a general average function, to discard extremely complex functions for example, even if they use the same parameters, as extremely complex variable interactions are less likely
        - similarly, identifying similarities/differences between structures of values/functions 
        - identifying similarities between the 'differences produced by unit function interactions (normalized to within a certain range, such as exponent/multiply or add)' and the 'data set structure' can be made obvious without retaining the original values in the unit function interactions (if unit functions produce a peak, that similarity to the data set structure is obvious (by changing angle or surrounding structures so as to make comparisons to determine equivalence of a shape trivial) without scaling it to the exact data set scale) and finding similarities in this 'unit function interaction structure (different functions produced by unit function interactions)' as well as differences to 'non-unit function interaction structures (different functions produced by non-unit function interactions at different scales or otherwise different from unit structures)' is a useful intent, these differences resulting from scale of these unit/non-unit functions being useful to identify and connect as equivalences to values of inputs
            - 'identifying a set of functions of inputs that create the same differences as a particular value of an input (making that input an alternate)', and 'identifying a set of functions that changes according to the scale of the function inputs' are useful intents to determine variable interactions
        - identifying differences (like 'intersection points of upper/lower bound lines, these intersection points indicating a difference of "parallelism between upper/lower bounds, which is useful for indicating a clear average line"') between sets of related interface structures (like 'pairs of upper/lower bound lines indicating useful differences in data sets') which offer an alternative to some computation (identifying an intersection invalidates the computation of determining similarity of slope) is a useful problem-solving intent

    - identifying useful structure sets and connections between them, such as how alternative structures can often be combined with an additive usefulness
        - identifying connections of interface structures like 'causes/inputs of a problem' with structures like 'adjacent/existing resources as invalidating/opposing inputs of those problematic causes/inputs' is useful as a common interface structure set that appears in problem-solving processes in a useful way
        - example: moving co2/methane so they cant create the greenhouse effect is a more adjacent target than pushing them farther away so they disperse in outer space or compressing them and storing them in the ocean, so tech like lasers can be used to move/separate the gases so they dont have that side effect, as the 'compression/adjacency' of these gases/molecules is the problem creating the side effect of 'trapping heat'
        - specifying this set of interface structures to make it more useful can be done by applying filters (like 'are functions available, or likely to be available given other interface structures like common interaction functions, that can move/separate molecules/clouds of these elements')
        - connecting 'adjacent resources' with 'one of the causes of the problem' is useful as a structure to start from, to specify with filters
        - alternate structures are useful to identify such as 'find functions that are more possible (moving horizontally into non-heat trapping structures), if other functions are known to be less possible (moving outward toward outer space)'
        - similarly, other alternate structures like 'core structures with opposite functions of the problematic functions' ('different configurations of molecules that dont trap heat') are useful to identify as alternatives and optimizations of this workflow
        - 'more adjacent/possible functions (moving in a different direction)', 'opposite-function core structures (molecular/relevant/core differences as useful differences), 'connections between problem causes and adjacent resources', 'connections between adjacent structures like available resources and non-adjacent functions', and 'specifying filters' are useful to identify and apply in the same workflow (since the 'connection between problem causes and adjacent resources' is so general, its useful to 'apply specification structures like filters', as workflows should 'connect opposing ends of spectrums (a default foundational structure that allows these opposites to be connected) that act like primary interfaces' to be useful, just like they become more useful when they connect generality/specificity, uncertainties with certainties, constants/variables and similarities/differences)
            - therefore connections between these 'opposites within spectrums' (of primary interface variables like abstraction, information, change, system) can form a default 'workflow component set' to apply changes to (like connecting them to problem/solution structures) in order to generate other workflows
        - multi-function structures, as in structures that are useful alternatives which are also useful optimizations to apply in a combination as well, are by definition more useful, so its useful to identify alternatives for the problem-solving intent 'identify useful optimizations resulting from combinations of alternates'

    - apply variation to useful structures (like problem-solving intents) to find specific structures fulfilling those structures

        - for example, for the 'find highly similar (in a known way, which can be applied as a constant, reduced and ignored) but relevantly different (in an unknown way, which should be expanded and focused on) functions' intent, applying differences to that intent to connect it with problem-solving structures identifies useful structures like useful intents to fulfill

        - the following intents ('identify useful changes to a base function', 'identify variables of a function type', 'identify similar/representative structures as the data set (like subsets)', 'identify useful function similarity types and variables generating maximal differences within that type') are relevant to this 'find similar but different functions' intent and can be generated as specific examples/variants/implementations of it which are relevant to other problem-solving intents/structures

            - identifying errors (like worst case scenarios like 'randomness') and directions of errors and check for change in that direction to avoid changing a function in the direction of errors
                - identifying/deriving/generating the possible errors like 'randomness' in a problem space like 'regression' is useful as a general problem-solving intent
                - another example of a 'worst case scenario' is where very different functions (different in the important ranges, to a significant ratio and degree) appear to be the same in some subset that is coincidentally selected as the subset to check as a representative subset
                - identifying 'similar functions (like by similar sub-sections, integrals, etc)' is useful for applying differences to a suboptimal solution function (to check for errors to revert to a previous solution function or otherwise correct a function), specifically 'functions having locally similar sub-sections/slopes that are maximally different in non-local subsets' are useful to identify as a 'worst case scenario' to misidentify (a false illusion of local accuracy/similarity that is highly inaccurate in other subsets), so identifying these 'worst case scenarios' and 'function similarity spaces' allows these similar but different functions to be checked for, to avoid that 'worst case scenario' for that function similarity type
                - changes applied (to the 'find similar but different functions' intent or other problem-solving intents/structures of the regression problem space): rather than identifying local minima of an error function or a generally representative function by applying some average metric (to minimize differences from that average), identify directions that are useful/suboptimal to apply changes in, by identifying 'worst case scenarios' and identifying directions of 'worst case scenarios'

            - identifying variables (like 'start/end position') to create a standard function space (such as where function positions are overlapping as functions have been normalized to occupy similar start/end positions, to highlight other differences than 'start/end positions') where function structures are standardized to more easily identify the maximal differences supported by a function type definition (like 'polynomials') as well as function structures like specific variables/variable interactions that break/reduce those definitions/differences, is useful to fulfill intents like 'find similar but different functions'
                - changes applied: rather than starting from a data set and identifying connections between points, start from solution function definitions, then identify & apply standards that are useful for intents like 'identify maximal differences within a range (like a function type or within a function area or having some points in common)'

            - identifying useful structures (like 'data set subsets' and 'limits') that represent the data set in some way (meaning, equivalently, 'are similar to the data set in some way'), and can therefore be used as a constant base to apply differences to, such as finding a local subset set and a point in each subset to use as a constant subset to base maximal differences on, like the 'maximal differences in functions that intersect with those points', given that these 'constant base' structures represent the data set to some degree (above a set of randomly selected points in the entire space), so applying changes to them (a solution automation workflow) is likelier than randomness to find useful functions relevant to the data set and solution function
                - changes applied: rather than finding just a 'solution function' structure, the definition of a solution structure is expanded to include other structures like data set subsets and limits (like 'function ranges') as similarly useful in their capacity as a constant base to apply changes to, as a 'base solution function' like a 'cheap average to find'

            - identifying function similarity indexes (like 'integral value similarity' and 'primary/powerful term similarity' and 'intersection point similarity' and 'slope similarity' and 'inflection point similarity') (and structures like useful combinations of these similarity indexes) which would fulfill useful intents like 'reduce differences between functions' to fulfill other useful intents like 'find similar but different functions', which is useful for problem-solving intents like 'find maximally different functions, within some similarity range', and identify the variables that connect these similar functions of that type of similarity (such as 'specification/generalization' variables which create similar function integrals, but inject a high ratio of differences in the function variables) to fulfill intents like 'find similar but different functions'
                - these function similarity indexes are useful to traverse as a way of filtering possible solution functions (after first generating the similarity index as a set of probable solution functions, given some known subset of the data set applied as a constant input)
                - changes applied: 
                    - 'probable function areas/ranges/limits' as a 'solution structure' of the 'regression' problem space has been changed to have a different 'difference/distance definition' (the similarity index)
                    - rather than traversing the whole (or a high ratio of the) data set, a subset of the data set is identified, a similarity index is selected based on that subset, a set of probable similar functions is generated by that index, and that set of similar functions is traversed, checking for known differences within these similarities outside of that subset which is used to identify similar functions

            - identifying function-adjacent structures, given its definition (like 'continuous'), such as 'areas of the data set which are more adjacent to a continuous function (like a dense subset) than other areas of the data set (like a sparse subset)', which can be prioritized when connecting/selecting subsets
                - this is similar to identifying subsets that are easily identified as belonging to the same function (such as 'points on a constant line'), but applies a different similarity type ('continuity' as a 'similarity' through 'connectivity' between adjacent points)
                - changes applied: connects different types of similarity such as 'continuity' as a 'similarity of adjacent points', which is more useful to identify than connections between distant sparse points bc there is more uncertainty in those connections and uncertain connections are a less stable base, allowing for more variation in function variables

    - identifying useful mappings like the mapping between the 'system-math' interface, which is where most of the useful algorithms have yet to be found
        - the 'system' interface can depict concepts like 'power' in a useful way, depicting a compressed variant of reality in the system network diagram (power dynamics are easily visualized in a diagram like this, such as 'some nodes would have more connections and more resources than other nodes, and would use that to cause changes in other nodes')
        - multiple methods to map this into a continuous differentiable function exist, such as:
            - mapping concepts to function attributes
                - mapping 'powerful variables' to 'more impactful variables', meaning 'higher variation-generating variables, like the primary exponent or high volatility-causing variables, or highly limiting variables, like variables that make a function never reach infinity or otherwise restrict a function (a wave-generating limit variable)'
                - mapping 'number networks (like important connections between 1 and other numbers like itself, its square, its opposite, its inverse, its nearest positive even number, etc)' so that 'powerful variables/functions' can be found by 'connecting these numerical networks (which have emergent conceptual/systemic attributes like "similarities/equivalences between variants of a number") by conceptual queries' for intents like 'find the most powerful/simple/adjacent (or other common/required attribute) connections between input/output values'
                    - this directly maps nodes on the math interface to system/conceptual attributes using 'function networks of numerical nodes', which could be done with 'representative (maximally different) numbers'
                - similarly, mapping 'abstract variables' to 'general variables', meaning 'higher variation-generating variables, like the primary exponent'
                    - relatedly, finding 'structures in common' across the various examples of power structures in various systems will identify default inputs (used like an 'abstract base' of the definition) like a 'maximal difference (high-low ratio)' that these structures frequently adjacently depict in common, so that finding 'limiting structures on what is not power despite having a high-low ratio or other related structure to a definition of power' is similarly trivial and useful
                    - why do this at all? 
                        - bc knowing certainties like that a 'specific set of concepts exists in every system' allows a program to iterate through all (or a filtered set) of the possible structures depicting these concepts and testing those
            - other mappings like 'finding/generating/deriving system sub-structures like 'system dynamics' and then applying vector-based changes to these system structures, then applying those vectors as input variables to find a function'
        - this is one of the more important mappings to link other non-math diagrams to existing methods like statistical regression
        - the workflow would be 'find a possible system diagram of a data set, then find a regression function for some subset of variables, then apply the system diagram regularly in a cycle to tune that function, since the system diagram will make useful structures clear, like when errors could occur that would disrupt the function, allowing for better prediction of the function'
        - network diagrams (simple connections between nodes) are useful in that some attributes emerge from the network in specific cases, like when a 'hub node' is identifiable by its high number of connections, and other structures can be implied/derived from that like concepts such as 'power'
        - the system diagram is useful in that it has default objects like 'cost/benefit' (which functions like a 'gauge' in that it is a 'revealing sub-structure that, similar to a filter, reflects other structures/makes other structures clear when applied as a certainty', structures like 'efficiencies') as it allows differences in related values (like positive/negative values) to emerge from various agent angles/trajectories in their pursuit of goals while traversing the system, where these objects are defaults
            - this is useful when the system can be depicted in such a way that the negative values seem negative in that structure (the costs are below/beyond some line/plane, and they visibly aggregate according to their actual variable interactions, from some angle, the opposite for benefits)
        - some of these graphs only differ by attributes like 'whether they allow repetition (which is a structure where symmetries might fail through over-reducing parameters)' or 'whether they isolate or group some structure' or 'whether they show all structures like "node-connection functions" or over-simplify/over-abstract to just show that a connection exists', these attributes making them useful for different intents
        - causal graphs are limited in what structures they visualize, but make some useful structures clear, like 'independence', 'input/output sequences between a start/end variable' and 'requirements (as in the "only connection leading to a particular node")', as opposed to 'order-independent' operations like 'set membership' and other graphs where the operations can be assumed/applied by default, rather than checking for sequential position (what matters is not whether one is independent/dependent, but that one variable/function always occurs in some developed/complete system/set)
        - similarly, a 'graph of similar functions by inputs or outputs' makes it clear 'what other functions could adjacently become or falsely seem similar to another function', which is useful for deriving structures like errors in the possible systems containing a function
        - similarly, finding the set of graphs where some variable interaction would be preserved (whether by some subset of variables, like relative position or distance) is useful for finding alternate graphs that dont violate some constraint as a 'truth' structure
        - similarly, identifying function structures that can be adjacently combined to fulfill problem-solving intents like 'summarize', 'predict', and 'describe' a high ratio of variable interactions
            - for example, finding a set of maximally different base functions, and identifying functions to fulfill interface structures of those functions (like a function to create an overlap/intersection between functions by applying some adjacent changes), identifying similarity scores (how many and what type of changes were necessary to create intersections), and identifying connections between these interface structures and useful solution metrics of functions like generality/similarity (a ratio of intersections at intervals indicating one could be a generalization of the other), so that these base functions can be re-applied across problems as defaults, and problems can be standardized to the 'regression' problem space to make this function set more useful
            - this specific function set applies some interface structures as defaults: a set of base solution functions (to use in a specific workflow 'change a base solution until its more optimal by some solution metric'), a set of concepts like generality, a set of interface structures like 'regular overlaps' that can be used to connect base/other functions with a solution function, given these conceptual solution metrics and those concepts' connections to those interface structures, a set of similarity indexes that can be used to identify similar/different functions for various intents, and a way to make these interface structures useful when applied in coordination (an 'input/output sequence' of the workflow that makes them useful)
            - this is an alternative to other workflows that connect math interface structures with problem-solving and other interface structures
            - this is a useful set of structures bc it offers a filtered set of specific structures to apply (rather than a bigger or more complete set of structures), through applying insights/understanding to filter specific structures that are more useful, rather than treating each variable as an uncertainty (the insights add some certainty and limit/filter the set of useful structures)
            - the reason that applying interface structures like 'overlaps' to the problem space of 'regression' (in a specific structure like 'regular overlaps') is that the interface structure of 'overlap' encodes a specific structure of the concept of 'similarity' and therefore is useful in connecting/differentiating various structures in the problem space, like possible similar solution functions, possible similar base & solution functions, possible similar general/specific functions, etc
            - other structures of similarity like 'intersectivity at inflections' and 'intersectivity at roots' can be used as complementary additive structures of similarity to determine similar functions
            - identifying similar functions is useful for intents like 'identify the maximally different functions out of the set of the most similar functions (most similar to other functions)' for intents like 'find the subset of functions that cover the highest ratio of functions by adjacent changes', which is useful for other intents like 'find variables that can be used to adjacently generate these function subsets', 'function similarity' being a specific implementation of the 'interactivity' structure that can be applied in workflows involving connecting 'input/output sequences' (where first, interactive structures are found, and then these are used as component inputs to other structures like 'connections that can connect problem/solution structures')
            - this type of solution that covers variable interactions that can be solved with regression can be enhanced by other math interface structures like 'space embeddings' to create spaces where other useful function structures can be found, to connect a problem to a type of space where the solution structure is more obvious, which is another useful problem-solving intent

    - identifying useful structures like 'minimal interface structures required to describe most variable interactions'
        - for example, some concepts are useful in isolation of other structures, and some concepts are more useful when structure is applied to them (when combined with the structural interface), so that a minimal structure that is still useful requires both the concept and the structure, and a set of the minimum interface structures required to describe most variable interactions would contain all of the concept, the structure, and the interaction that is useful, where the structure and the interaction would overlap with the usefulness of the concept, and replace some of its usefulness, so rather than describing the concept fully and applying that, including the structure and the interaction as part of the useful structure is more useful than applying the concept in its full in isolation
        - this means there is a set of structures on each interface that should be included in a minimal set that provides complementary value rather than redundant value (which is useful for intents like 'find the minimal set of maximally different structures to build the maximally different structure')
        - similar to how identifying the concept of 'power' is useful in isolation of many other structures, identifying how 'power' interacts with other variables and systems (such as the 'set of functions of how power is frequently misused when over-centralized') is similarly useful in isolation of many other structures, where identifying the 'interaction of the concept & function set, as well as identifying just the concept and the function set' is more useful than identifying either or both, in isolation of their interaction, so the 'minimal useful set' is the concept, the function set, and the interaction function, rather than any subset of these
        - this insight is useful bc it adjacently identifies useful graph structures, such as 'finding the ways that all concepts vary and aligning those ways in a set of aligned network graphs'
            - for example, all concepts vary by structures such as 'multiple' and 'unit', so creating a stack of networks (where each network identifies the variations of the definitions of a concept), then aligning these networks so that 'multiple' occurs in the same position (relative to the vertical stack of concept networks, or relative to other nodes on each concept network), is one way to implement the interface network structure, where queries would be able to use 'vertical position alignments' to find cross-interface interactions such as similarities or mappings between interfaces (where the concepts would include interface-specific concepts like 'functions' rather than only abstract concepts, although just applying abstract concepts in this way would be useful to identify how concepts interact with structure)
            - relatedly, other interface structures can be applied, to identify other similarities, like 'common variable interaction structures' as a structure to align concepts on
        - generating other useful graphs is trivial with other workflows, at which point insights can be reverse-engineered from these graphs bc of their usefulness for some intent, using this graph-insight connection
            - by identifying the usefulness of 'aligning similar structures across graphs', the insight of 'a minimally useful structure set involving overlaps/interacts between structures' can be adjacently reverse-engineered as the 'connections between these graphs' (the 'overlap' structure of the 'multiple' structure which is obvious when viewed after applying some core variables to find new insights like a change in perspective such as 'vertically' is similarly/compoundingly useful as both of the individual graphs, so that these should be grouped as a 'useful minimum' of a structure set)
        - in this way the 'structure' interface (with structures like 'multiple' and 'unit' as some of its default core structures) is embedded as the 'host interface' that connects the others, which can be switched to other interfaces (the 'meaning' interface is the default 'host interface' of interface analysis, which organizes structures by relevance/usefulness), or alternately some interaction function of the interfaces (like the concept-structure interface) can be applied instead, such as applying a 'minimum useful set of interface structures' as the host interface
        - other useful structures to apply can be found by applying variables to definitions of useful concepts and identifying the most useful structures resulting from those variables
            - for example, applying variables (like 'variables') to find interactions with useful concepts like 'simplicity' can identify other useful structures to align networks on (like 'low numbers of variables' which is the structure of 'simplicity' applied to the variable of the concept of 'variables'), so a 'variable count' becomes a possible useful structure to align networks on, which would identify other useful structures for their specificity (like 'linearity' on the 'math' interface)
            - similarly, applying the 'low count' attribute found with this application to other interface structures like 'functions' identifies other structures of 'simplicity', such as 'low count of sub-functions in a function', which is useful to store as a default useful structure in isolation of other structures but also in the same set as other structures, as including the 'low count' in a set of defaults including 'count' is compoundingly rather than redundantly useful, rather than generating 'low count' every time, as if it's not a known useful structure
            - identifying other 'compoundingly' (independently, complementarily) useful structures can be done by applying attributes of other compoundingly useful structures (such as 'structures that can aggregate to create differences, rather than structures which can be simply similarized and therefore made redundant, using simple combinations of structures like concepts such as 'simplicity'), where 'similarity' doesnt completely equate to 'connectibility/dependence' as some similar structures are 'falsely similar' or 'independent and/or indirectly connected' (more reliably or adjacently connectible using some other origin/base/symmetry), but it is related to that concept and can be used in some contexts
            - aligning the networks so that 'low count' of variables on the 'change' interface is aligned with the 'low count' of 'sub-functions' on the 'function' interface embeds the concept of 'simplicity' as a 'positional' attribute of these graphs, so that the position of simple structures on the change/function interfaces are adjacent/overlappping in this structure and therefore trivial to compute
            - finding different structures of an abstract concept like 'adjacency' (which can describe almost anything, depending on the graph's positional attribute, if the positional attribute determining distance between points can describe extreme differences, which is possible) by applying this method (of applying core variables of interfaces and finding interaction functions connecting them to the original concept of 'adjacency') would identify structures like 'similarity', 'simplicity', 'efficiency', 'probability', structures which trivialize/reduce differences (like 'commonness', 'types', 'expansion structures (that capture a high degree of variation to trivialize differences)') and other useful structures, where this network of concepts related to 'adjacency' is useful in isolation of other structures, bc of the specificity it adds to the definition of this abstract concept
            - applying some specific concepts is more useful than other concepts, such as applying 'independence' as an organizing attribute to separate truths/falsehoods on different sides of a network (as 'maximally independent' structures are less likely to be connectible, as in causally connected, as in related/relevant, which is another way of deriving the 'minimum structures to create the maximally different structure')
            - relatedly, some structures should be paired/grouped, such as how the 'system-function' interface is more useful in some contexts than either interface on its own, as the 'system in which a function is used' and the 'function structure' are not possible to completely isolate as they depend on each other for important metrics like 'emergent functionality', as the 'system' can be taken as an 'input of the function' and the 'function' can be taken as an 'input of the system' with variable definitions of 'input' ('context' in one definition variant, 'component' in another, and 'interactive structure' in both variants)
            - similarly, 'empathy' is a useful structure in that 'maps between different systems' is adjacently derivable with a trivial change 'isolating the structure of the process (the system-mapping function) and the emergent result of the process (the map between systems) and related interface structures (the "alignment" required for the map to be useful)', which identifies other structures that make insights trivial to identify (high variation-capturing, high variation-reducing, highly structural functions) as an alternative to useful graphs like 'aligned network stacks which embed a useful concept in the alignment, making other intents trivial such as "mapping across systems"'
            - optimally, the interface network implementation would have 'maps between systems' as a default structure on the interaction level of the host network (a network of 'maps between systems (such as the concept-structure map, the system-function map, the requirement-potential map)' as a default implementation structure, which would add 'high variation-capturing' functionality and 'alignment of high variation' and therefore 'maximal differences' by default)
            - relatedly, similar concepts united by a common definition are useful for other intents like most other useful structures are, such as 'find causal variables (since similar structures are likely to be inputs as previous states)'

    - identifying useful structures like 'error structures of common structure formats (like incentives)'
        - 'incentives' are useful as a structural format as they capture a high ratio of information such as 'adjacent changes' and 'defaults' and 'benefit/cost ratios', similar to how similarities/differences capture a high ratio of information
        - finding error structures of incentives (like 'carefully avoiding switching to another strategy when it becomes optimal, just bc its easier to continue making adjacent/incremental changes in a wrong direction') is useful as a way to solve problems once formatted as incentives and opposing structures of incentives (like limits on benefits and cost-maximizing structures that create disincentives)
        - an opposing (solution) structure to this error structure would be 'generating alternate solution functions and calculating switching points when one becomes suboptimal compared to another one given the switching cost, to navigate between alternate solution functions'
        - a 'solution-switching' function (fulfills a 'mix/change' intent) is similarly valuable as a 'solution-filtering' function (fulfills a 'find' intent), just like an 'alternate solution generation' function is valuable (fulfills a 'build' intent)
        - similarly, useful structures in the 'math' interface have usefulness emerging from the variation of their interactive components (the 'set & its associated functions, and the limit/symmetry the set/functions interact with', or the 'definition of distance/similarity and the emergent similarities (the attributes that are continuous (topological) or preserved (symmetrical)) using that definition', or the 'defining/standardizing functions like the gauge/kernel/norm/basis of a space and the similarities (like continuities/convexities/supersets) related to structures in that space and its defining/standardizing functions'), which offers a default problem-solving structure that can be changed to create other useful problem-solving structures as they represent a variant of the un/certainty or difference/similarity set that describes all problem-solving structures

    - similarly, identifying alternate graph structures and the connections between them and other useful structures like the interfaces they represent and the structures that make them more useful
        - for example, a 'causal network' is useful bc of other useful structures (derivatives) and structures like 'injected nodes' which allow for other variable interactions to be considered, and is useful for finding useful structures like 'alternate causes', 'high-cause (powerful) variables', 'requirements' or 'dead ends'
            - specifically, an example of a 'causal network' (implemented as a neural network) is a graph that allows for multiple alternate paths between inputs/outputs to be considered, additionally having multiple variables per path (where various layers provide nodes on the path), which is formattable as an 'angle sequence (query bundle) set that allows overlaps, where the angle format is relevant bc its directed'
            - the core structure of causal graphs can be enhanced by other structures of cause, like other relevant measures of similarity (such as 'root/inflection intersectivity with and adjacency to other functions'), which is not directly determined by derivatives, which describe differences in change across variable sets
        - similarly, other related graphs are useful such as 'boundary graph (indicating boundaries between sets)', 'threshold graphs (which graph areas between thresholds)', 'query bundle graphs (a graph that connects different queries of another network that are equivalent/similar in some attribute like meaning/structure but differ in structure/endpoints)', 'variable networks (a standard variable connecting graph without causal direction)', 'gravity/hub graphs (which prioritize graphing variables that are powerful, common, or highly connected/interactive as opposed to graphing direct causal connections, as these are the important variables to graph, as other variables gravitate around them or are directly caused by them so they can be left out)', 'heat/energy maps (which graph specific definitions of variation/potential)' and have associated useful structures making them more useful
        - these graphs visualize a primary variable of reality that has a value in all structures, that corresponds to an abstract variable and also appears on other interfaces, where other graphs that are useful can be generated by applying variables of these graphs and applying those requirements (this reverse-engineers the primary interfaces, by identifying which graphs capture the highest ratio of variation and variables of those graphs)
        - for example, 'a graph of tensors of cross-interface/graph change types' are a useful structure, as a 'set of interactive/powerful change types' that occupy different graphs, as an alternate format of query sequences across a network like a language network
        - another useful graph might be a slightly more complex function network having boundaries between function sets which are sub-structures of the function network, like a graph of 'areas of similar functions (like different areas for generative functions and filter functions)' where any line on the graph has to select one function from each area/set, as these are useful functions when grouped together
            - injecting similarities in each area (like organizing each area by complexity, commonness, interactivity or another useful dimension) would make this graph more obviously useful, in identifying the function attributes that create the most effective combinations for some intent
            - graphing 'boundaries of function sets' in function networks may make some other structures obvious, depending on the connection/position function used to create the graph, such as where there are probably missing nodes to fill in that are not already known, to capture variation indicated/implied/defined by adjacent sub-structures of the network

    - identifying alternate interaction functions between core problem formats, workflows, and solution-finding functions
        - for example, the 'mix' function can be applied to other solution structures like 'input/output sequences' which normally you would apply as inputs to a 'connect' or 'combine' function (in a 'connect problem/solution' or 'build solution from solution components after breaking down problem into sub-problems' workflow), but when a difference is injected at the 'connections between useful structures', value is still retained in that these functions are interactive and act like equivalent alternates in many cases, so they can be switched without losing too much information
        - similarly 'combine filters' and 'reduce solutions (to common/simple solution variables)' and 'break solutions into solution components' are similarly useful as the default core workflows but require adjacent differences be applied to generate them and also require some useful structures to make them more useful
        - this function can be optimized by applying other useful structures like 'interactive' input/output sequences, which reduces the uncertainty of the inputs to the 'mix' function
        - this applies a 'difference' to 'core useful structures' like 'core interaction functions of solution automation workflows' which is applying another workflow 'apply changes to useful structures to find adjacent useful structures which can act like approximations/inputs/otherwise useful structures by their adjacency to useful structures'
        - similarly, identifying the full set of functions to adjacently connect useful interface structures like 'core interaction functions' and 'problem/input formats' (such as 'how to change the mix function or the input format so it can be used with the mix function, like applying useful filters')
        - similarly, identifying the 'functions to connect known un/certainty structure sets (as well as adjacent variants of them)' (connect incorrect variants which are likely to occur in reality, so that the input or function needs to be changed before it is useful)

    - identifying useful structures like 'incorrect definitions where the correct definition is definitively in-between them or similarly calculatable from the error definitions'
        - for example, a 'symmetry angle' is where both points to be connected are equidistant from the origin of the angle (the point overlapped by or connecting the two lines), and is in-between the angles which are not symmetric, which is useful to identify when finding symmetric angles, by applying a line and a copy of it until theyre connected and changing those lines until theyre connectible
        - these sets of structures like 'known incorrect angles that do not create symmetries' which allow some useful structure like a symmetry to be calculated trivially from them (like applying some filter like 'in-between' to the 'position symmetry')
        - similarly, some unit structure of symmetric angles like a 'right angle' (which is shorter than other angle types and has equidistance possible at its midpoint) is useful with some change set like 'moving the right angle until one point connects to some structure to be connected with the symmetry and rotating and scaling it until the other point is overlapping with an endpoint'
        - similarly, an angle that is just barely not a straight line is similarly trivially possible to position as a symmetry (identifying the midpoint) that overlaps at its endpoints with the two points to be connected with the symmetry
        - these 'error structures connected with these change sets' add 'specific (and therefore useful) structure' to the solution by their definitions which differ from it
        - these 'in-between solution structures' can be calculated when some error structure is similar to another error structure by being its opposite in some way, at which point, its possible that a solution might exist between them (the errors might occur on either side of the solution in this case)
            - this 'function to determine cases where error structures offer information about a solution' is related to this function set as another function to include in the set

    - identifying useful functions to identify like 'identifying absolute (or relatively absolute) truths and applying them locally'
        - for example, identifying an 'absolute truth' of 'sun position' and 'angle to an object' creating a 'symmetry in shadow angle reflecting sun position' with limits like 'daytime motion' and 'positions where sun isnt blocked or otherwise missing like inside/near a building' and 'time interval like "hour of the day"' as a 'key variable' to determine 'direction of motion' and 'routes or components of routes like sub-routes that could maintain some direction from start to finish or generally' as well as 'differences in route or sub-routes that could change direction', for a problem like 'move in the same direction' which requires solving problems like 'find out the direction of some motion'
        - this requires finding 'absolute truths' by identifying 'what is true in different positions' (such as the similarity of sun angle in different positions at the same time) and 'what structures can identify different positions (a field structure where the same rules would likely apply)'
        - this requires connecting that 'absolute truth' with the 'current position/problem', so a 'function to connect absolute and local truths' is a related useful function to 'find absolute truths by identifying what is true elsewhere'
        - related functions include functions to 'identify what truths will still be true or will become false when some position is local (when change of type "motion" has occurred)'
        - this is useful as a solution automation workflow bc the dichotomy of absolute/local contexts is reflective of reality, so applying it with an 'un/certainty' structure and 'connecting the two extremes of the spectrum' is useful at solving all problems
        - relatedly, connecting some reality-describing dichotomy variable like 'abstract/specific' using variants of the definition of those structures on different interfaces (apply the 'abstract' interface to other interfaces and connect the structures across interface) is another set of functions that can be used to solve all problems (such as how variants of the definition of abstract/specific (such as 'local' and 'example/exception' and 'subset' and 'limited/contextual' which are variants of 'specific', and 'global' and 'type' and 'general' and 'superset' and 'acontextual' which are variants of 'abstract', which are 'structures of specificity/abstraction (on other interfaces than the abstract interface)') can be used to connect the concept's structures across interfaces)

    - identifying useful definitions of useful structures that add value in terms of metrics like the descriptive/generative/connective capacity of the definition
        - for example, a function set that 'identifies variables which are also "maximal differences" to the point of being "independent" (as in providing "complementary information") and adjacently connects those maximal independent differences (since theyre still related despite being independent or there wouldnt be a requirement to connect them)' is a useful variation of the 'interface' definition bc it provides a useful target for filtering the set of all functions and offers a base for other 'solution automation workflows', as a function set like this would be able to solve most or all problems, as most problems can be formatted as a problem of missing information about how to connect very different variables (which cant be connected in some obvious way without knowing interface analysis, or it wouldnt be a problem and could be easily solved by an error)
        - similarly finding the full set of functions that adjacently connect independent maximally different variables is useful as a set of alternative solution automation workflows
        - similarly identifying structures like 'maximal differences' and 'complementary information (as opposed to redundant information that reflects the same truths)' that create other useful structures like the concept of 'independence' is similarly useful
        - this structure applies the same core useful structure of a 'highly differentiating filter' or 'primary interface' or a 'un/certainty pair' that is a default structure of 'problems', as the 'connection between the variables' is certain and the uncertainty is 'how theyre connectible adjacently or otherwise optimally', so applying variations to the connection can be a proxy intent for solving all problems, if the variables are sufficiently independent that their alternate connections could reflect a high ratio of information about reality
            - relatedly, a 'similarizing function' that can make any structure similar to some 'core set of structures' is useful for finding 'equivalences'
            - relatedly, a 'differentiating function' that can make any structure similar to some 'core set of differences (problems)' is useful for finding 'differences'
        - this workflow is also useful for finding the 'level of specificity/generality' at which some definition is maximally useful for problem-solving (similar enough to some useful core definition structure used to create other useful structures that it can change into that core structure, and similar enough to known useful structures that it can easily become those, and different enough from other definitions that it can be independently useful for different tasks but similar enough that it can act like an equivalent alternate to some set of other useful definitions to replace it if necessary)
        - similarly, identifying useful structures of definitions such as 'structures that appear falsely similar/different but are/not really the same structure' is similarly useful (for example, identifying that 'reasonable sustained (real/legitimate) changes' may falsely seem like an error such as 'hypocrisy', if some information is left out)

    - identifying functions that can connect useful structures adjacently across interfaces as a useful approximation of interface analysis
        - for example, finding a function set that can identify structures that fulfill priorities (like 'prevent harm to innocent agents' and 'incentivize harmless/beneficial functions' and 'incentivize intelligence development'), requirements (like 'fulfill common intents like store data for quick retrieval'), finding error structures (like error structures of a rule like 'too good to be true', such as 'the exception to a rule where it would become false' and the 'threshold that this error crosses'), finding variables (like 'finding variables of errors'), which when found & connected across the primary interfaces would approximate interface analysis, as an alternate minimum function set that acts similar enough to interface analysis to approximate it
        - similarly, function sets that can identify useful structures like 'similar inputs, similar outputs, and input/output sequences/sets' are useful structures for 'prediction' intents, just like 'highly differentiating switches' are useful for 'sort' intents, and 'highly differentiating filters' are useful for 'find' intents in that they are 'components of solution structures'
            - relatedly, other structures are useful for other reasons than being 'components of solutions', such as being causative, determinative, differentiating, required (rather than just inputs/components of solution structures which are adjacent to those structures when connected by their default interaction function 'combine' applied to these components)

    - identifying different function networks/graphs that are useful for different intents (as a proxy for implementing interface analysis) is a useful intent to solve for
        - a function network (of function types & other similarities like input/output, attributes like volatility, sub-functions, potential, best/worst cases) organized by similarity of relevant metrics like 'output attributes/structures' is useful for switching to other similar functions when one function seems to be false, starting from a core of common different functions
            - areas on this network where functions have useful similarities like 'similar sub-functions and similar input/output connections (like function shapes)' are useful to identify
            - this would be useful for tasks like 'find the most incorrect that a function can be while seeming correct at some subset of points' and relatedly 'find the way to connect these "incorrect-implication point subsets" with "correcting point subsets"'
            - finding the intents that can be adjacently fulfilled with these function networks is useful, to identify how similar these fulfilled intents are to problem-solving workflows & their interface queries, or whether these fulfilled intents can be composed to fulfill problem-solving workflow interface queries or fulfill the workflows directly
            - the network can be organized to make these areas predictable/calculatable so that different areas (areas of functions that differ in these attributes) are also predictable/identifiable
        - finding function sets that can generate other function sets (covering the highest ratio, the most adjacently, etc), such as the 'function set that can be composed/changed to generate the find/change/derive/build/connect function set'
            - for example, a 'structure function network' might have common function structures (like a peak/wave, line, rotation, layer, overlap, etc) and it would generate functions by composing these with adjacent transforms
                - this structure function network might solve the problem of 'quantum attribute observability' by finding common structures and assembling them in a way that seems to match observed attributes of the quantum functionality, such as a 'fluid or other non-constant state of some component of spin/position that both have in common, which resolves to either spin or position depending on where most of the fluid is concentrated at the time of observation on its rotations' or 'intersecting wave functions that intersect with zero at various points so that one has no value when the other has value' or 'a spectrum with multiple thresholds in different directions, where above either threshold, an attribute like position/spin is measurable' or other common structural dynamics that seem to match some attribute of observed quantum functions
            - similarly, a network of common meanings/intents (given the relative lack of variation in these in common structures) so that any new problem can be solved with adjacent combinations of these common component meanings/intents is similarly useful as a function network built the same way (as in 'common component functions')
                - a useful insight is that primary interface structures capture a high or total ratio of information but also reduce computation requirements drastically (meaning there arent that many abstract concepts/intents/functions that are equivalently useful, despite the ability of these sets of structures to describe a high ratio of information), a metric which can be used to find other primary interfaces or proxies of them (these primary interfaces are highly similarizing structures, despite supporting a high ratio of differences)
                - common meanings include structures such as 'common interaction functions' as well as 'common concepts like balance (which are abstract enough to describe most systems in some way, such as balance structures like justice/symmetry/equivalence/opposites)'
            - just like 'sequences/sets' are different core structure formats which are useful for different intents ('sequences' being useful for intents like 'derive/connect' and 'sets' being useful for intents like 'combine/build'), other structures are useful for different intents ('commutative sequences' can be implemented as 'sets', thereby connecting the two formats with a directly measurable/calculatable attribute)
                - these interaction levels ('isolatable sets of interactive & adjacent structures') of equivalent alternates (sets/sequences) that are connectible using core interaction functions ('change order of objects/functions') are useful to identify as 'units of reality' that can be applied and connected to other similar interaction levels to create a useful function network of these interaction levels, which would similarly act as a proxy of interface analysis (interface analysis being the derivation/generation of interface queries likely to be useful/relevant for a particular problem, using interface definitions and known useful structures like 'core function sets')
        - a function network of 'similarizing differences' and 'differentiating similarities' would be similarly useful, as a way of implementing a 'filter network' that fulfills the 'find function' requirements (find any information from any other information, using differences like relevance/interfaces/variations)
            - these structures are useful for various specific intents, such as how 'similarizing differences' is useful for 'identifying types that different structures have in common' and 'differentiating similarities' is useful for 'identifying variables that could indicate sub-types or new types, which differ across structures having the same type'
            - similarly, 'differentiating differences' is useful for making differences more obvious (differentiating from an average), and 'similarizing similarities' is useful for identifying an abstract type or merging different examples into a representative average (the most similar structure)
            - 'identifying groups' and 'identifying the most different abstract types that similar structures can be reduced to' are related useful intents of 'similarizing similarities', which is useful for the efficiency of the description by using types instead of the full set of attributes/values
        - these function networks have structures in common like organizing the network to make non-trivial calculations adjacent to high variation-capturing structures like 'common known functions/components'
        - these function networks (a function network optimized to 'find areas of similarity across multiple attributes', a function network optimized for 'finding the most adjacently generative function set to use as components of all other functions', and a function network optimized for 'finding similarizing differences and differentiating similarities to filter all information adjacently') are similarly useful at capturing high ratios of information but use different 'core concepts that optimally standardize information' and 'variable/constant sets' (applying some structure as a default and allowing related structures to vary to handle uncaptured information) to implement that & other intents
        - similarly, a 'potential' function network organized by related concepts of potential (like 'adjacency of what a structure can be with few changes' and 'extremes of what a structure can be if scaled enough' and abstract concepts like interactivity/power/variability) would capture and organize information in a similarly useful way, as would applying any core concept as a default constant to organize a function network
        - using queries between these function networks is a useful proxy for interface analysis, since it allows queries to be run like 'find an obvious error or symmetry using the similarity/difference network (which identifies these structures quickly bc of the definnitions of their core default structure), otherwise proceed with finding similar functions on the function similarity network organized by input/output similarity and sub-function difference, to check maximally different functions, given some subset of known inputs/outputs, to find the rest of the function'
        - similarly, finding differences like the 'differences between optimality and acceptability (the bare minimum over a threshold, connecting optimality with relevant threshold structures)' which are likely to describe a high ratio of variable interactions based on other useful structures like 'incentives' are intents which are adjacently fulfilled with a network of function networks organized this way
        - similarly, a network of error structures, where the network acts like a 'selection/filter function to select between errors' is similarly useful to identify, as a way to prioritize some abstract conceptual metric like 'balance' by navigating between specific error structures, as every perspective has 'obvious errors/suboptimalities', but some errors are acceptable/optimal in some contexts (inputs/starting points, query paths/functions) and others are not
           - for example the 'balanced perspective' specifically would have more optimal errors like 'being different from most specific useful structures (but different in a similar way, as in different in an equivalent and adjacent way)'

    - symmetry structures applied as a default variable structure, rather than 'incremental change combinations', so that by default, symmetry combinations are sought, rather than incremental change combinations, to always frame change in terms of symmetries
    - combinations of useful intents (such as those that are relevant/realistic) as a default interface to base changes on, finding the functions to fulfill them at query time, as implementation variables
    - structures of relevant cross-interface structures as a default structure to apply changes to
        - for example, 'patterns of causal structures of structures of structures' (like the patterns of cause of 'a sequence of structures that develops from another structure')

    - identify the useful structures (like 'metrics optimized for') in useless structures (like error structures), as everything has both positive/negative qualities, and if there is an error, its bc there was something initially or somewhat useful about the error or it wouldnt have occurred (the error state is a lower-energy/more maintainable state, for example) as a way to find other useful structures like 'similarities in differences' that will have compounding value across problems
        - for example, the 'useful structure' of an error structure like 'hypocrisy' is that a system can contain contradictions/paradoxes/differences without destroying itself, which is useful for generating differences, a core intent of problem-solving
        - this is a 'similarity in the difference/error' structure that is useful across problems, and relatedly 'finding the oppositely-charged structure of a structure (finding the positive structure of a negative structure)' is another useful intent of problem-solving
        - to find 'similarities in differences' in the 'usefulness of a useless structure' (or to find similarly useful structures like 'structures that can support opposite ends of a spectrum' or 'spectrums' as useful structures), applying interface analysis again makes this trivial, but also identifying 'common' structures (like similarities/differences) or 'common error structures' like 'self-invalidation/neutralization/contradiction' structures is another way or applying useful perspectives like 'optimism' (with related intents of the perspective like 'find a way to make everything useful' or 'create differences from errors to find solutions')
        - as another example, the 'useless error structure' of 'blaming the incorrect source of an error' has a useful structure that adjacently derives the insight that 'some functions can handle blame as they are highly variable' and therefore the usefulness of these functions is their variability which makes them 'independent' of the blamers, 'variability' and 'independence' being another useful structure derived adjacently from this useless error structure
        - deriving the attributes of these 'adjacently useful useless error structures' such as their 'measurability' (which is useful in that it makes them trivial to avoid), is similarly useful as a core intent of problem-solving, as some functions develop new functionality to handle the responsibility of blame, and responsibility is an important useful structure (related to fulfillable intents as 'functionality' and requirements)

    - identifying error structures like 'gaps in existing resources/inputs like neural networks' and matching those to 'possible specific functions to resolve those specific error structures'
        - for example, ai has errors such as:
            - 'not identifying/fixing its own emergent outputs' (emergent outputs like 'inability to improve itself on optimal human metrics like meaning, efficiency optimization using interaction level switching, and intent-selection')
                - ai cant switch to using structures like 'civilizations' on other 'interaction levels' by default bc it uses combinatorial approaches in many cases, applying some core unit as an input to a combination function that will never adjacently derive those structures, so computing a function like 'a competition civilization collided with a meta civilization' isnt something it can do bc those arent clearly defined, have high computation requirements (without interface analysis to adjacently derive understanding), and they are not adjacent combinations of anything, so an 'interim thinking' algorithm that uses interface structures will get the answer before an existing neural network
                - this is bc it applies a 'bottom-up' approach by default (combining some core unit) and cant switch that to something else (applying complexities like 'civilizations/universes/rule sets as defaults to overlap/merge/connect/format', as opposed to 'core units to combine')
                - it also cant identify this error structure in its emergent output and cant design another ai to fix that error structure
            - 'not identifying its most efficient paths to derive all variable connections adjacently'
                - an example solution structure could be 'applying an algorithm to find the variation-maximizing subsets and substructures of a neural network is a useful algorithm to include in the network, so those substructures can be applied by default for high-variation intents'
            - 'not filtering/generating its own usage intents to optimize its own usage, as it has no concept of its own usage intents that isnt directly input by a human'
                - identifying a graph where pro-social intents are computible/determinable is a useful intent to apply with neural networks
            - 'not applying/identifying/deriving meaning structures like "the emergent impact of a structure in some or all computible contexts or from every angle" or "the adjacent alternate definition routes of a structures that determine/describe/generate/compress it efficiently" or the "most useful structures that fulfill all meaningful intents adjacently"'
                - 'an algorithm to identify useful structures and requiring neural networks to identify those first' are useful intents to fulfill with other related intents rather than requiring human users to apply changes to neural networks to fulfill those intents manually
            - 'not identifying other specific useful structures like similarity metrics and regression algorithms and spaces where some computation is reducible to an inner product calculation which are comparatively useful compared to neural networks and making sure the neural network can adjacently derive these and other new useful structures and apply them (injecting nodes to allow itself to work on that during training or new incoming prediction calls, as new types/generative paths of "differences from optimals" are identified)'
                - ideallly, the neural network would inject new nodes and then not propagate those until it finds a way to reduce the new nodes injected to some interaction level where its functionality is adjacently derived/applied/found
        - 'identifying error structures describing this difference between existing functions and understanding functions as well as the solutions opposing those error structures' is a useful intent to fulfill
        - identifying structures to map useful concepts (like 'wideness' and 'deepness') with structures of the neural network (like 'adding weight path sequences and more possible changes in between input/output layers') and the conceptual impact of that ('increasing ability to handle variation') is another useful intent to fulfill

    - applying concepts related to 'truth' like 'absolute/infinite', 'reality', 'consensus', or 'sanity' (as a structure of 'being correct' or 'connected to the truth' as opposed to 'being wrong' or 'disconnected from the truth') to derive 'sanity interface queries' that can be used to fulfill intents like 'filter true statements'
    	- for example, an implementation of a sanity algorithm would implement functions that relate to sanity, some component of it, or some input of it, or fulfill some intent/output of sanity better than or similarly as sanity does, within some degree of change applied to the set of known facts that matches a sane ratio
    	- relevant sttuctures of 'sanity' include 'regularly checking if the self is creating errors' and 'regularly testing solutions for correctness' and 'identifying different types of correctness' and 'using true statements as inputs/defaults/constants' and 'avoiding improbable structures' and 'avoiding immeasurable/untestable structures'
    	- as a counterpoint (to implement the insight that 'every truth statement has a related false statement that is likely to be true is some other possible context'), some insanity (as 'being wrong') is useful for some intents (like 'creativity (to add differences to reality, as in improve the truth)', 'identifying patterns and variables of wrong structures as useful through being different from/opposite to right structures')
    	- so a 'sanity algorithm' could start with 'true statements', and apply 'insanity' structures to 'create differences from these true statements' for intents like 'finding new connections between true statements or new networks where those true statements are false' or 'identify errors that could be adjacently derived by true statements'
    	- finding similarities in common between these various conceptual algorithms implementing the definitions of those concepts is useful to find a general algorithm of truth

    - identifying useful structures like 'connections between useful structures' such as connections between requirements/symmetries and useful tests/changes to identify requirements which can be used in place of other structures bc of these connections like 'using the test to identify a requirement in order to identify a symmetry'
        - bc requirements of a system are often sufficiently equal to be its symmetries, a change that identifies a requirement (such as 'removing a variable to see if the system re-generates it bc its required for the system to continue to exist') can also identify a symmetry or similarly a stability
        - the connection between 'requirements and symmetries' as well as the connection between 'changes to identify a structure and the definition of that structure' are similarly useful in fulfilling problem-solving intents like 'connect useful structures' and 'find variables of useful structures'

    - formatting 'neural networks' (or 'regression') as a 'matching' problem, as in 'finding a subset of the network to start applying changes to inputs at, as some sub-structure of the network seems to fit inputs/outputs of the original data set adjacently, then injecting/removing nodes within or around that sub-structure as needed, or adding connectivity to other sub-sections/sequences of the network that seem adjacently useful'
        - relatedly, deriving the 'probable change types (like some exponential change or directional change) or other interface structures' required to connect inputs/outputs of the original data set, finding a sub-structure of the network with those change types, and applying that sub-section as default initial changes, adding more or connecting more as needed to connect inputs/outputs approximately/completely
        - similarly, finding 'optimizations' (such as 'approximations') of the 'regression' problem space (such as 'only finding the subsets with negative slope' or 'only finding subset filters and approximate local averages of those subsets and connections between those local averages') that can be applied to reduce the computations required and change the target of the solution (an approximate solution rather than the original solution)

    - identifying patterns of 'reasonabilizing functions' that connect some structures in a way that makes sense (a 'common sense' function implementation)
        - for example, to connect the very different words 'sky' and 'encryption', you can use an interface query like:
            - 'what is encryption used in' (communication)
                - 'what supports/enables communication (what is an interface on which communication can develop)' (network)
                    - 'what is a variant of that interface which enables communication that is relevant to the target (sky)' (satellite network)
        - the interface query of this sequence ('what is it used in', 'what is an interface of that', 'what is a relevant variant of that') is relatively simple and captures a high ratio of information about the variable connection
        - this isnt just a useful sequence for solving one problem, but it also connects structures to other structures in way that makes them reasonably connectible ('reasonable' connections involve 'intents', 'usages', 'foundations like interfaces', and 'relevant structures such as adjacent change structures', which are reasonable to use as defaults in determining reasonable/probable and therefore realistic connections)
        - finding a 'reasonable' solution connection/function that has multiple reasons why it could be real/true (as it connects to multiple reasonable structures like relevant/useful/usage structures) is useful as a general problem-solving intent
        - finding interface structures like 'patterns' of interface queries that are reasonable (or otherwise 'make sense', 'sense' being 'similarity/equivalence to the truth/validity/other structures of truth') is similarly useful
        - this is particularly useful when information about possible/true structures is missing
        - relatedly, a structure that adjacently connects very different structures that are not related (like highly variable structures which offer different sub-interfaces such as 'sights/sounds' which are only connectible by core abstract structures like 'frequency') would allow more similar structures (like problems/solutions) which are related to be even more adjacently connected
        - similarly, a way to generate variables that have reasons for the variation (like the variable is similar to an equivalent alternate of the original variable in some way, like how 'acidity' can be derived in 'water' by applying attributes of equivalent alternate 'core elements' like 'heat' which has a 'burn' function) as a problem-solving intent to fulfill
        - similarly, a graph where the direction of errors is known/guaranteed bc of structures like 'reasons why an error occurs (bc of its initial advantages before becoming a measurable error)' is also useful for implementing a 'reasonableness' algorithm

    - identifying a function to connect 'inputs/outputs' with 'solution functions' using 'connection structures (like generalization)' to predict an 'error graph where errors are obviously knowable/identifiable/differentiable' is a useful problem-solving intent
        - the 'error function of (a solution function of) input/output connections' is not immediately adjacent for a complex data set, which is the problem solved by neural networks/regression
        - as an example, this 'conversion function (from a data set to a solution function, or from solution function to an error function)' could apply a mix of differentiating structures ('de-volatilizing/specifying or generalizing a subset of the graph', 'applying local averages to a subset', 'applying maximal differences like change in cardinal directions to another subset') in ways that are similar enough to the data set that they are reasonable to apply (within a 'reasonable range') if there is a more optimal summary function or a summary function that is adjacent with these structures
        - the point of solving this common problem is to find the error (the missing info) without having it clearly/adjacently graphed in the input/output connection function
        - the 'solution function of connections between inputs/outputs' acts like an interface between the data set and the error function, where the target of regression is the error function determining how accurate the solution function is for a given data set, so mapping the data set to the error function on the solution function interface, where predictable (as in similar) solution structures can connect the data set and some subset range of error functions, is a useful intent to solve for
        - this would answer the question 'which error functions are probable/possible for a solution function, given specific data sets' and 'which of those data sets align with (are similar to) the actual data set' or in the other direction, 'which solution functions align with the data set and which error functions are associated with those solution functions'
        - another format of this intent is 'what error functions are possible, given some solution function and some adjacent changes to it using some similarity metric', which is a useful problem-solving intent to solve for
        - given some similarity of 'solution function' error (like those producible by solving for 'maximally different subsets' of the data set or 'applying a function to summarize points at regular intervals of subset selection/sampling' or solving for a 'generalization of some more specific function or a specification of some more general constant function'), where 'predictable similarities' in the error function can be generated by applying these structures as variables to vary within reasonable limits (the solution functions produced by similar interval sampling should be similar), these 'predictable similarities' in the error function can be used to identify similar ranges of the error function like 'lower ranges with probable minima' (the changes that lead to 'generalization of a function' or 'volatilization of a function' can be mapped to a set of ranges in the error function)
        - the point of this is that applying 'generalizing changes' (as opposed to 'adjacent changes') to a solution function has a predictable impact on the error function of a solution function set (the solution function will either be clearly right or clearly wrong almost immediately by applying generalizing changes (which involve a high ratio of assumptions and resulting variation from those assumptions), so the error function producible by generalizing changes will be predictably/reliably volatile)
        - this is related to the insight that 'powers/opposites/additional terms are likelier to produce volatility than other structures', as it involves the connection between highly differentiating functions (like de-randomizing, generalization, or mixes of interface structures within reasonable limits) that could produce a summary function, and the preserved highly differentiating attributes like 'volatility' of the error function set producible by that differentiating function applied to create different solution functions
        - other functions that 'skip directly to other minima' or 'clearly differentiate errors across similar solution functions' are useful to identify as default structures to use/mix in problem-solving workflows
        - finding a 'range of reasonable limits' in which possible changes should be limited is also useful for solving the 'find a regression function' problem
        - similarly, answering the question 'what solution-generating methods produce some useful structure in error functions (like constant vs. curved vs. volatile error functions) given some more measurable subset of data set attributes' is another way to format this intent
        - relatedly, determining measurable errors (like 'generalizing a shifted version of the solution function, where the error is that it was generalized on a different level bc of a mis-subsampling, producing an error that could be corrected by a shift down/up, as the error is constant') of these solution-generating methods like 'generalization'
        - relatedly, finding 'similar/different variables' that are adjacently usable to generate local minima identified in the error functions of a subsample of different 'input/output points' is a useful intent using existing algorithms like gradient descent, as the variables in common are likelier to explain other 'input/output point' sets and the variables which differ are likelier to explain different 'sources of error (interfering functions, emergent phases, switches between alternate functions) at different points/ranges/phases' or other useful relevant structures
        - this means given some subset of adjacent sets of solution/error function connections, the rest of the error function is determinable, by applying these 'differentiating' structures
        - this is relevant for cases like evaluating the 'loss function of a specific solution function, across all input/output sets', like where volatility in the data set should appear in the solution function or the loss function will be volatile (these are 'conceptual math connections' between these functions in the regression problem space)
        - similarly, some 'similar solution-generating functions (or solution-generating functions)' such as 'inversions/rotations of a solution function around some symmetry, which will preserve info in the form of some error value for some input/output set' will have some similarities in their error functions, these solution-generating functions being relevant for 'generating different possible functions to connect an input with an output (or to minimize some input/output-connecting summary metric like the difference in the integrals of all input/output sets)' (the 'error function' as referenced here)
        - another related point is that individual input/output connection functions or output/error difference should not be considered in isolation and should always be calculated with some other input/output set
            - a useful application of this point is to evaluate a generated possible solution function for multiple maximally different input/output pairs at a time, evaluating whether some improving change to the function exceeds some metric like a specific ratio of input/output pairs (x% of the pairs' error function values are improved by the change, as in the error function values are decreased) and only keeping changes above that ratio
        - relatedly, another useful application of these structures is to evaluate input/output sets until the 'determining attributes that reduce the set of possible solutions sufficiently' (such as volatility, randomness, etc) are identified (meaning they stop changing to some degree), as a way to select a subset of points to evaluate, as the 'set of functions having the same volatility, proximity to randomness, etc are determinable from those attributes' and once you know which 'function set having similar attribute values' a function belongs to, you can trivially determine other equivalent alternate functions in this set
        - 'similarities across different error functions' can be predicted/tested with a function to generate solution-generation functions that preserve similarities
        - formatting the problem as 'differences between relevant function structures' such as 'differences between error values of one input/output set compared to average input/output sets' highlights other relevant differences to these loss/error/solution functions/solution-generation functions
        - formatting solution functions as isolated subset lines (at various selected intervals) to indicate a 'high ratio of probable outputs' is another possibly useful solution function format that offers a representation of the data set as definitely crossing or represented by some set of subset lines but not being constantly-defined elsewhere
        - relatedly, finding errors resulting from specific/similar methods, such as a 'specify' function that optimizes for 'number of intersections with original data set', which can have errors like:
            - 'in the worst-case scenario, selecting a suboptimal subset to apply as specific test cases of a solution function, such as a subset that is simpler than the rest of the data set'
            - 'a case where the correct solution function is a wave, but the "specify" function implementation optimizes for "number of intersections" so it finds a straight horizontal line for a solution function, bc there are many points in the middle of the wave and that is the subsample selected to optimize for as test cases'
        - this means finding 'change types' that create other useful structures like 'generalizations', and applying those to predict the structure of the error function given the solution-generating function that generates similarities/differences in adjacent error values for a particular input/output set
            - for example, changing the inputs in some way (such as increasing the subsample ratio or improving the accuracy of the subsample filter like requiring it to apply maximal differences) might have the same effect on accuracy as generalization, without being generalization
            - differently general variants of a function might have similarly low error function values (might be mimima of the error function)
            - finding these alternate structures to generate generalization effects might allow skipping to other minima from one minimum
            - these alternate structures offer similar attributes in terms of preserving/representing relevant info about the data set and removing irrelevant differences

    - identifying connection functions/variables between known useful structures like functions/structures to fulfill intents like 'de-noise or otherwise vary the original data set to change its shape to more probably or definitively correct structures' and 'de-volatilizing functions of a data set, to identify adjacent more probable/predictable/general/otherwise useful functions' and 'probable function ranges' and 'optimal solution-generating functions which frequently produce absolute minima in error values with adjacent changes' and 'differences from randomness' and 'invariance to best/worst cases' and 'function attributes preserved across changes generated by a solution-generating function' and 'functions with equivalent determining attributes like volatility/intersectivity/adjacency/representativity/connectivity' and 'standard statistical structures like the probability density function' and 'local point set merging functions' and 'alternate representation/solution/accuracy metrics' and 'alternate subset function-connecting functions' and 'info-preserving functions' and 'alternate useful data set formats' and 'local subset filters' which are known useful structures in the 'regression' problem space which are also maximally different and have likely other different variants to apply that are similarly useful, and which connection problem/solution structures like 'adjacent structures to problems/solutions' in different ways
    
    - identifying useful structures that can identify useful attributes like 'intersectivity' as a 'useful similarity metric' and the intents these structures are useful for like 'finding maximally different functions with similar output functions'
        - for example, the method of 'solving systems of linear equations' can be extended and applied to find 'intersectivity' to maximize that to fulfill the intent of 'finding maximally different functions that produce similar/equal functions' as a way of generating different solution functions to filter
        - similarly, the attribute of 'commutativity' can identify 'equivalent alternate' routes to a particular value

    - functions that can adjacently derive the concept of an 'interface' (given its usefulness) or other useful structures are useful for their proximity to useful structures
        - for example, formatting the 'division' problem as a problem of finding a function that 'finds similarities (like a unit base) in the differences between the divided number and its divisor (like a similarity of the divided number or the divisor to some base like 10) that make the problem trivial to solve' applies the concepts of similarities/differences in a way that is proximal to the concept of other useful structures (like an interface)
        - identifying functions that generate adjacent useful structures of useful structures is similarly useful as finding the direct functions of those useful structures, even though the variant isnt the exact definition of an interface or another useful structure, as they will approximate the usefulness of the useful structure if applied in that position

	- functions that act like useful/powerful filters of relevant information, such as:
	    - a function that can determine an attribute like 'input change type' that reveals similarity/difference of a relevant structure, like 'output change type (integer)'
	    - more generally, a function that can determine attributes that reveal 'information about a lot of other information'
	    	- such as how an 'average data point' reveals 'a lot of information about other data points (in that most of them will be near the average)' and an 'extreme/limit of a range of data points' reveals 'a lot of information about other data points (in that most of them will be within that range)'
	    - as another example of highly useful information like types & extremely differentiating filters, absolutely/generally/frequently/conditionally/probably true statements about 'truths/falsehoods' also reveal a high degree of information compared to the input information required to determine their applicability
	        - for example, given the truth statement 'truths usually have a counterpoint that is true to some degree in some context', a 'proof of falsehood by contradiction' which rules out a possibility bc one contradictory example is found can be considered a contextual proof, as there is usually a context where every truth is false (there may be some space that corresponds to the space in the proof where the interaction rule holds true rather than being contradicted)
	        - what is the common factor between useful extremely differentiating filters, types, and truth statements
	            - there is a high degree of information embedded in these structures (when you determine a type of an object, you may know many other things about the object such as the type attributes, similarly when you know whether a filter rules out some object, you know a lot about that object such as its attributes differentiated by the filter, and similarly when you know whether a statement fulfills some truth statement, you know a lot of information about its truthhood)
	            - these structures are highly organized and well-defined/rigid, meaning they are useful by their 'certainty' for comparison/differentiation tasks to determine uncertainty structures
	            - these structures are extremely useful/relevant to many useful intents like 'determine truthhood of a statement (test if a solution is correct)', 'filter out sub-optimal/error statements/solutions', 'classify a statement as true/false or another information type'
	            - these structures occur on different interfaces and have the 'certainty' attribute in common, making them corresponding 'certainty' structures on different interfaces
	        - similarly, other structures can be framed as 'high information embedding structures' such as 'function subsets that act as implementation methods of interface analysis', 'the set of efficient compressions of reality', 'the maximally different variable structure where any problem difference is easily connectible with a query on the structure', the 'optimal set of interface queries that solve most problems adjacently or otherwise optimally'
            - similarly, concepts & other primary interface variables can be framed as 'high information-storing/embedding' structures which are powerful through these functions and similarly offer high ratios of information (knowing the concepts related to a structure gives a high ratio of information compared to non-primary interface metrics)
            - similarly, some structures are 'sufficiently different to some specific structures (as in different from simple structures enough to be capable of handling stress/complexity/embedded variables)' and 'sufficiently similar to other structures (like input information, as they preserve info)' that they are more probable, and likelier to be legitimate/real/truth structures than other structures

    - identifying useful structures that are cross-interface (such as 'structure-concept' interface) highly useful through some metric like 'commonness' to fulfill intents like 'prediction' and 'explanations'
        - for example, 'right angles' are useful in providing 'orthogonality/independence', 'angles' are useful in providing a 'central base to form an angle for comparison', 'multiple right angles' are useful in providing more extreme differences, and 'unifying structures (like a tensor)' are useful for providing relevance, which provides a structure that fulfills concepts like 'orthogonality, simplification of comparison intents, and relevance of different structures' (a cross-interface structure on the 'structure-concept' interface)
        - therefore finding structures like 'tensors' (multiple angles having the same central base) across interfaces are more powerful than other structures
        - other cross-interface structures that fulfill these useful intents and concepts can be derived and used as defaults

    - identifying the map of 'structures that are optimally useful in some metric' and the 'specific structures which can implement them adjacently or with high probability of success or some other solution metric'
        - for example, the 'multi-task' structure is useful, where some function fulfills multiple very different intents optimally
        - specific structures which can implement this functional attribute adjacently include 'undifferentiated structures' (similar to how stem cells are useful through being 'sufficiently similar/adjacent to multiple specialized structures that they can easily become those structures' (occupying some position in between other structures which are optimized for some specific intent, as opposed to being generally useful but suboptimal)
        - the 'map structure' connecting a 'multi-task function' to an 'undifferentiated structure adjacent to specialized structures' is useful in that it can connect other useful intents with specific implementation structures
        - finding a 'specialized structure that is optimal for some specific task, which is also useful for tasks in general or multiple other tasks' is rare bc few tasks require enough functions that the inputs to the functions also cover many other function inputs and usually involves interface structures and/or high degrees/amounts of computation/data
           - finding structures which 'adjacently generate the highest ratio of function inputs' is similarly a useful specific intent to fulfill, as a specific implementation of the 'multi-task function' structure
           - relatedly, as mentioned previously, finding structures which are 'sufficiently similar to every useful structure that they can adjacently generate it' is another useful specific intent to fulfill
           - relatedly, finding 'overlapping functions having some functionality in common (as the metric of function similarity)' is another specific intent to fulfill which can derive the 'multi-task function' on its own
           - relatedly, finding the 'maximally different structure' is likely to be able to be useful for multiple intents, which is another way to find multi-task structures
           - relatedly, finding the structures which are useful across problems, like 'formats which standardize other structures, so that one function can be re-used and useful across different inputs once formatted' are similarly likely to be useful in finding 'multi-task' structures, as 'standardizing formats' compound the usefulness of functions that are useful in the output format
        - identifying the concept of a 'adjacent structure to other useful structures which can be parameterized and these parameters changed to produce other useful structures' is adjacent to the structures of 're-using an existing solution' and 'approximate solutions which are adjacent to optimal solutions'
        - the 'map structure' connecting a 'find useful structures' intent with a 'multi-task function' is similarly useful
        - this is a specific example of a 'function-intent map' structure which is highly useful across problems
        - similarly, identifying the map of 'error structures' and 'specific structures implementing that error' is similarly useful
            - for example, the 'invisible information' structure is useful as an example of a 'missing information' error, in comparison to other 'error structure implementation examples' like 'unmeasurable information', 'corrupted information', etc, which are related but different concepts and are useful to differentiate

	- a function set to determine the meaning of an interface query is useful as a 'reverse-engineering' implementation method
	    - for example, the requirements would be a function to generate all possible/legitimate interface queries and a function to determine the meaning of each query
	    - where the meaning of a query is described as the impact of the query on other relevant structures like problems and the connections like similarities/differences to other queries like on metrics like cross-problem solvabiility

    - given the primary core interaction functions of problem-solving (find/filter, build/combine, apply/change, derive/connect, organize/sort, simplify/reduce, define/structure, try/test, match/map, abstract/specify, standardize, etc which resolve some problematic information structure like a seemingly random large set to filter, or a difference between resources and target structure that can be built with those resources, or a difference between a disorganized structure and a structure that would make finding tasks trivial like a sorted structure, or extra variables complicating some structure, or undefined structures that need to be identified), apply these functions to each other on increasing interaction levels to find other functions of problem-solving
        - for example, given core interaction functions of one unit (change, filter), generate other functions on other interaction levels like multiple units ("change filters"), and connect these to problem-solving workflows ('"change filters" used to solve a problem until the filters are different enough or fulfill some other metric that they could optimize some problem')
        - as another alternative, find core structures of problem-solving like useful structures and find new functions to connect them, as an alternate way of generating useful problem-solving functions
        - as another alternative, find the variables of these functions (spectrums like certainty, simplicity, etc) and generate the set of possible interaction functions that way
        - in all of these, some structure is held constant ('units of core functions', 'positions of useful structures') and another structure that interacts with the certain structure is variable ('how those units are combined', 'how those positions are connected') which allows new structures to be found in the variation allowed within the limits framed by those constants
        - so finding the information which reflects the other information (finding that 'core functions as units' reflects information such as 'how those units can be (adjacently/usefully) connected/combined', reflecting the info through the limits imposed by those core functions being applied as 'certain constants', which interacts with that information in some way, like with a core interaction function) around some symmetry (the 'combination/connection' interaction function symmetry, allowing the units to be combined/connected in a useful way without being destroyed) is useful for finding which sets of information allow this 'certainty/uncertainty' structure to be applied to find new information structures like new connection functions

    - identifying alternate formats that problems can be converted to, such as specific problems like 'finding ways information can be preserved/stored (problems formatted as a info storage problem)', 'finding all connections between useful structures (problems formatted as a problem of identifying useful structures and deriving them from each other)', 'finding all the ways information can be hidden (problems formatted as missing information)', is useful as an input to a change workflow to derive other useful structures related to these problem formats
        - these alternate formats highlight different useful structures
            - the problem of 'finding ways information can be preserved' highlights other useful structures like:
                - the fact that info 'requires structure and also specific structures like connections to other information and limits on info storage'
                - function attributes like consistency/predictability (since some functions vary on how predictable/consistent their outputs are)
        - applying adjacent changes to useful structures such as 'applying them to each other' or 'applying them to known problems' adjacently identifies these other useful structures

	- applying specific interface structures as a default constant set
		- applying 'information' interface interaction rules of the 'physical reality' interface such as:
		    - 'structures that break definitions of some concept can be structures of falsehood, unless the structure is more stable/relevant/otherwise true than the definition, in which case it can be a structure of truth, as change patterns/structures of truth arent false by default but rather can be more true than their inputs'
            - 'true' structures will interact with 'true' structures differently than 'false' structures, as the 'true' structure will use the 'true' structure repeatedly across functions, more so than it would use a 'false' structure, which it would likely filter out and stop using, similarly, 'false' structures are likely to use 'false' structures for the same reason and in the same way (as an input), as 'false' structures are useful for creating other 'false' structures
                - this is derivable by first identifying 'iterated structures' (like 'truth-truth') and identifying their interactions as opposed to the interactions of a mixed structure (like 'truth-falsehood')
                - similarly, 'false stable structures' are likely to require falsehood as an input (if theyre false and in a stable state, falsehood is likely to be related to that stability), just like truth structures are likely to require truths as an input if the truth structure is stable, where 'stability of falsehood' can take the form of 'immeasurable falsehoods' or 'falsehoods below a harmful threshold where they are not measured', as truth structures are likelier to interact with other truth structures in a non-destructive way as they are more similar, whereas truth structures are likely to interact with false structures in a destructive way, given their difference (truth structures can coordinate with other truth structures, without contradictions, as they already occupy the same reality and are technically already coordinating just by being true/real/stable)
                - relatedly, given known possible useful structures like 'neutralizations' (canceling each other out) and 'extremifications' (compounding in the same direction) and 'oppositions' (changing direction/sign), a 'falsehood applied to a falsehood' ('incorrect difference applied to an incorrect difference') is a possible truth structure if there is a neutralization involved, as these are two extremes of a dichotomy like 'abstract/specific' and like those dichotomies which act like reality-covering variables, all interface structures (such as neutralizations) are possible in the true/false interface given some interaction function between them (like iteration, embedding, etc)
			- 'truths can become false when over-depended on, beyond their appropriate context or meaning, or beyond their potential to illuminate or support other truths, or in an incorrect structure like a foundation for other truths'
			- 'truths can be so irrelevant to an intent as to be equivalent to false (example: citing the heat death as a reason not to try to do anything)'
			    - 'relatedly, truths are meaningful statements, where meaninglessness makes some statement so different from relevant/useful/integrated/meaningful structures as to be false'
			- 'truths can be so rarely/improbably true as to be equivalent to false (example: an error state that is so rare you basically dont have to plan for it, like where neutrinos would coincidentally flip all the bits on a server at once)'
            - 'truths that are generally true (like something with a high benefit to cost ratio is too good to be true) have an associated truth that makes them false in some way (like their error structure such as "when the better ratio is actually true, this truth will miss that exception"), where the "error structure that could find the structure with the better ratio" could be more true than the generally true statement, depending on the ratio and the usefulness/importance of the structure with the better ratio'
			- 'truths can be so unstable (difficult to maintain) as to be equivalent to false (example: a rare atomic state that degrades into another more stable state more frequently), as truths generally take less energy to maintain or make true, as lower energy-requiring structures are likelier to exist, and similarly scalable truths (which can be repeated or otherwise scaled without or with fewer contradictions) are more true than other truths'
				- 'relatedly, truths, if prevented or destroyed, will re-occur if true, as its unlikely for a truth to occur in absolute isolation, only once, and simlarly can support more embedded variables if true than a falsehood can'
				- 'relatedly, truths must have some structures or they are unlikely to be true, such as how requirements (costs, responsibilities) are required or the structure is unlikely to be true'
			- 'truths are less likely to be extremely surprising/different, based on a comparison to an input set of sufficiently variable mixed facts'
			- 'truths are less likely to be on either extreme of various spectrums like specificity (an extremely specific fact like a "specific behavior of how to be good" is less likely to be or remain true than a general fact like a generally useful priority like "being good")'
			- 'truths are likelier to have truth-associated structures like costs, whereas a lie is likelier to have fewer of these structures (a lie about zero cost)'
			- 'the ratio of truths to falsehoods is likely to be stable to some degree, because when a truth is measured and sustained enough to be measured, some other true structure might decay, although entropy is a powerful process that might act as a counterpoint to this, decreasing the number of truths (stable structures) over time'
			- 'truths can be so difficult to measure/verify/calculate/derive as to be equivalent to false (example: number of atoms in the universe, or some phenomenon that occurs below the synchronized directed information frequency that constitutes 'time' that it cant be measured in isolation and can only be inferred by its emergent effects)'
			- 'truths can be so non-adjacent to other probable/known truths as to be equivalent to false (example: future truth of a reality that is non-adjacent to current reality and is unlikely to occur)'
			- 'truths can be so lacking in reasons as to be equivalent to false (example: there is no reason for a rare anomaly except random coincidence so it may as well be ignorable)'
			- 'for every fact (which acts like a representation of reality) there is another fact that represents reality equally/similarly accurately or represents a similar proportion of reality or otherwise represents reality in a similar way by some metric of representation, so that these other facts in the same position on this representation metric index can act like equivalent alternates'
			    - 'statements at different positions on this representation metric index can act like a cross-section of reality that acts like a determinant of reality (the cross-section is usable to determine other facts, which is itself a representation metric), though statements that tend to be closer to either extreme of absolute truth/falsehood or the center are more valuable for their obviousness of similarity/difference to other facts/falsehoods'
			- 'truths are generally independent in that they have relatively few dependencies in order for them to be true (less conditional and contextual, more absolute and inevitable)'
			    - 'at the same time, truths are likelier to have more reasons why they are true/can exist (existential risks to the truth were prevented by many guarantees), making it more stable, compared to a falsehood (an agent decided to make it seem true, using cheaper structures than truths)'
			    - 'this is related to the fact that truths usually have fewer opposing statements that are true in some way/degree/context (bc paradoxes/symmetries/rotations may be a core structural unit of reality, just like randomness/ambiguities/balance points where very different structures seem equally true are alternate structures of reality related to symmetries, as most structures can be connected to symmetries given that theyre a structure of stability/robustness where some structure can be sustained under change conditions and therefore a structure of existence/reality, just like useful structures can be a structure of reality as theyre more stable), compared to falsehoods, which usually have more opposing statements that contradict them, and related to the fact that truths are more similar to other truths as they are more adjacently connected to other truths (using truths)'
			    - the question of 'what are the limits of symmetries, or why isnt everything a symmetry (in every permutation/interaction and on every level), given their commonness' is adjacent to that, with related insights like:
			        - 'variables are a type of symmetry', 'variables are arguably descriptive of everything (the change interface can capture all variation)', 'variables are embedded on other variables', 'some variables break/create symmetries and that is more important to categorize them than to call them just another symmetry', 'symmetries dont always resemble symmetries in various states of development', 'some symmetries neutralize each other, leaving randomness (which is different in that it is the interactions of unlimited changes, rather than constrained changes of a symmetry)', 'symmetry types are a meta-symmetry', 'meta-symmetries (symmetries of symmetries) are more powerful than standard symmetries', 'not every symmetry is equally interactive with all other symmetries', 'other structures like levels/systems where symmetries act and symmetry interaction functions are equally important if not more'
			    - similar to the question of 'why isnt everything formatted or adjacent to an input/output sequence by default' which is bc information can be adjacently and usefully stored in equivalent alternate formats, not everything is a symmetry bc there are variables of symmetries (interface structures like 'state' and 'self' can be applied to symmetries) as theyre not absolutely required, and there are many variants of the definition of a symmetry as theyre related to 'limited change' (similar differences) which is a fundamental structure reflecting reality (true/false spectrum pair), so just like input/output sequences arent enforced in all conditions/contexts, neither are symmetries
			    - similarly, the 'limits of definitions' are equivalently useful to the definitions themselves
			- structures of meaning like 'the comprehensive impact of its interactions/functions at various scales, in isolation and in various system contexts, and with other common variables applied' and 'useful structures' are useful approximations of the definition to use in place of the full definition, similarly compressions of reality are more useful than reality itself, these compressions acting like interfaces between the user and the system being compressed
			- 'truths can be so simple that they will essentially be false in that they will be more useful when changed in some way and will be an input used by other more powerful/complex/high-variation functions which could independently generate/alter these simple truths as needed due to their simplicity, and therefore are less real/true, as reality allows complexity and therefore requires some complexity for survival, survival/stability being related to truth, and similarly can be so complex that they are essentially not usable by other powerful/complex/high-variation functions and therefore are less real/true'
			    - 'relatedly, there is always a simpler and more complex variant of a statement within the bounds of simplicity/complexity required for reality, and the same goes for other dichotomies of reality like truth/falsehood, similarity/difference, balance/imbalance, variable/constant, meaning there is a network of statements related to one statement that act like different variations of the same statement, so that the statement acting like the symmetry of this network of statements is more true than any of the variants and will reflect the interface network to some degree/type/structure'
			    - 'truths will be more adjacently connectible than falsehoods to interface structures (such as the primary abstract concepts like balance/power/work which correspond to the dichotomies of reality)'
			- 'almost every fact has another fact that is more relatively true and a falsehood that is more relatively false, except the most absolutely false/true statements, which are relatively rare'
			- 'relative truth/falsehood is determined by proportion of reality represented, accuracy of the representation, power/usefulness/relevance for enabling/determining other representations of reality, logical validity, adjacency to absolute reality, im/possibility, inevitability/requirement, consistency of the fact/falsehood across contexts, consistency of truths/falsehoods across different representation formats, potential of all other metrics to remain the same or change, & other metrics of truth'
			- 'truths often come with an opposing counterpoint as most truths are not absolutely true but are wrong in some way like in a particular context/usage, so that a truth without a counterpoint as how it might not be true is unlikely to be true'
			    - 'relatedly, truths are generally somewhat variable, acting like a symmetry/manifold in that they can vary in variables like position/shape/relationship to other structures without being destroyed, in their robustness to change, whereas lies by comparison are relatively fragile'
			    - 'similarly, truths are often seen to have a "balancing offsetting truth", similar to a "rise and fall" structure pair (which may differ in side lengths, as a statement may be more true than its counterpoint(s)), which is a fundamental structure related to parabolas, angles, waves, etc'
			    - 'however, given that other fundamental structures exist, the "rise and fall" of a point and its counterpoint is not the entire description of reality, despite being extremely useful and common in the form of the concept of balance'
			    - 'similarly, other concepts than balance are known to be relevant, as not everything is balanced, such as the ratio between matter/antimatter, and therefore other concepts are required to describe reality'
			    - 'similarly, other structures than one interface are important to describe reality, such as how a cross-interface structure (such as the structures that a concept takes in various systems) is more useful than either structure in isolation on one interface'
			    - 'for example, given that the interim and combined concept, respectively, of a circle, parabola, curve, a fractal, efficient embedded change (using the previous value as an input as an alternate to a constant multiplier), a sequence describing the variable interactions rather than a set of interacting independent variables, a stability in the change rate, an origin/symmetry, and a concentric circle set is a spiral, how does this interact with the "rise and fall" structure of a point and its offsetting counterpoint?'
			        - 'during the standard process (which may be the "rise and fall" of a new change as it first generates change and then is offset by opposing forces to settle in to its stable form), other changes are allowed to occur, such as embedded changes, curved changes, etc, which can disrupt the stabilization of the new change or even invalidate it or isolate the initial change from its counteracting change forces to prevent their interaction'
			        - 'given that the spiral with a stable change rate (that is maximally different from other structures like a full circle and a cornered shape) has stable structures, it can inject stability in other changes, such as by making a rise or fall more stable than its opposing structure'
			        - 'given the changes enabled by the spiral and its stability and therefore its disruptive power, spirals are a better structure of randomness than some other structures, as an offsetting uncertainty structure to more default certainty structures like the "rise and fall" which are more certain bc they are more balanced'
			- 'truths that are in between simple priorities & rule sets (like "opposites attract") and complex priorities & rule sets (like those generating randomness) are likelier to be true, and truths that connect simple and complex rules are likelier to be true, as there are both simple and complex rules in reality, and neither has absolute priority, as the interim truths between extremes are likelier to be stable and therefore true, and similarly the interim balance point between all dichotomies at which theyre intersecting is likelier to be the most stable perspective, from which the others are adjacently generated'
			- 'truths are rarely the only truth explaining a variable interaction, as there is rarely a variable requirement requiring that specific combination and preventing any other combination from succeeding as there are more often many routes between two points than one route, and there are many alternate equivalent preceding/succeeding variable interactions that are similarly explanatory, bc errors are default in most systems and therefore differences applied to sequences/routes/components are likely and are less likely to be ruled out by some requirement if they still generate movement in the original direction, and similarly bc components are frequently unitary and can be combined in different ways to generate the same structures, and similarly bc real systems are often complicated and subject to errors from their high degree of interactivity with other systems in contrast to a theoretical high degree of isolation'
			- 'truths usually are not obvious (such as the wishes of agents as their default, and therefore also obvious/simple to them, so these wishes can be assumed to be not true) but if they are, a beginner without expert knowledge of rules is likelier to see them than an expert bound by their knowledge of rules'
			- 'truths that are not measured or otherwise useful are likelier to change than truths being measured bc the act of measurement requires finding a way to keep some fact true enough to show up on a measurement scale, while other variables are allowed to remain variable to cause other variables to change, which are not being forced to remain true bc attention is directed at other truths to keep them constant and randomness indicates that anything not forced to remain constant is subject to variation injections (a function to generate change, whose change-generation speed is faster than measurement speed, is likelier to be a more useful intent, to make all measurements obvious or unnecessary, as the changes that are true will sustain themselves and the changes which arent true wont, so no measurement/testing is required, measurement being useful for "changing direction of focus/motion/work/etc")'
			    - 'relatedly, truths in general are likely to change at some point due to allowed variable interactivity/dependence, so any set of facts that was true is unlikely to be completely or equivalently true at some point in the future, as variation isnt equivalent to falsehood but a fact of truth-generation & adaptation & interactivity & dependence'
			    - 'relatedly, truths that are independent from another set of truths (such as processes on another planet) are less likely to remain true, unless another truth intervenes (something needs it to remain true as its useful for some intent)'
			    - 'similarly, truths that dont require ignoring other information (having no contradictions) are likelier to be absolute truths'
			- 'truths above a ratio of already identified truths may be so useless as to be false, as they may be irrelevant, and also preserving the ratio of truths/falsehoods is useful for agents with intents to incentivize additional change/uncertainty (some intent like a wish/dream must be false for them to be able to justify doing work to change any variables)'
			    - 'similarly, future truths can be predicted using the intents/incentives of agents (agents who are capable of fulfilling all of their intents, who dont always select the incentivized option, as "always selecting the incentivized option" contradicts "capacity to fulfill all of their intents"), once their intent & fulfillment capacity (like 'to increase uncertainty', 'to seek freedom', 'to control time', 'to understand physical reality') and incentive selection (ratio of selecting incentivized option) variables are known'
			    - 'similarly, some possible truth can be so different from incentivized truths or intended truths as to be false'
			- 'truths are generally those structures which fulfill multiple solution/optimization metrics as opposed to just one (its rare for something to be true which is only useful in one way, such as ability to change or interactivity with other useful structures, generally structures which survive fulfill multiple optimization metrics as structures generally interact on multiple interaction levels)'
			    - for example, a structure is not usually just 'efficient' in some metric (efficiency meaning low-cost or adjacent in some other way, which means its near to something else, as not every point is adjacently useable as a starting point to find adjacent structures to), it also needs to be 'similar enough to existing starting points/interaction levels' to be useable (in order for that efficiency to be efficient)
			- 'truths can be less true than a falsehood, if the falsehood is more probable, relevant, useful, is about to be true and if the truth is about to be false, is important to be made true, is approximately/generally true, reflects more interface structures like potential, etc'
			- 'truths are generally more useful at generating other truths (used as inputs to other truths), as opposed to generating falsehoods or lies being more useful at generating truths'
			    - 'similarly, truths are useful at specifically generating power, in the sense that truths enable other functions to be more powerful, so statements (about a variable relationship) that seem to control/determine/cause other variables are likelier to be true'
			    	- 'for example, specific structures act like truths (such as certainties/guarantees) on specific interfaces through their determination of other structures, such as how gravity/speed acts like certainty in the physical information interface, and structure acts like certainty in the math interface, which can be used to find different certainty formats in other interfaces (what type of similarity/change acts like gravity/light in the math interface)'
			    - 'similarly, truths are useful at connecting true/real structures, additionally connecting cross-interface structures, such as by "specifying a concept" by connecting the concept to a structure'
			    - 'similarly, truths such as interfaces are useful at supporting variation, so high variation sources indicate a truth supporting that variation, as if variation is false in some way (such as by contradicting truths) it will likely be stopped by agents who benefit from truths, so if variation continues, it is likely to be based on a truth that has created enough stability to support additional changes'
			- 'truths generally coexist with other truths, rather than more frequently/generally contradicting them'
			- 'truths generally are more similar to other truths, rather than being extremely different from them, partly bc of the fact that once a structure is found that is stable/otherwise useful, it tends to be repeated rather than changed'
			- 'truths that seem "counterintuitive/random/complex/asymmetric" are adjacent to some interaction level, despite seeming false (through dissimilarity/disorganization/complexity/asymmetry) at first, as truths are usually an adjacent combination of some components (though neural networks dont usually switch perspectives to start from a different angle/starting point and identify these interaction levels from which everything is adjacent) and by comparison lies will fulfill some usually-checked metric of truth (similarity/organization/simplicity/symmetry) while containing a contradiction (false similarity, etc)'
			- 'truths are often trivially different from lies, in that in order to be believable, a false statement has to be sufficiently similar to truths (plausible, reasonable, logical, sensical, meaningful, relevant) and cant be obviously false (silly/nonsensical/inappropriate for the context/meaningless) at which point it becomes an obvious wrong (a joke), therefore a lie (a difference from the truth, or an error) can be used to generate a truth by applying it to another lie (a difference from the truth) or by basing it on or connecting it to the truth (making the lie obviously false so its not really a lie), so lies should be connected to an offsetting lie/truth rather than given in isolation, as they wont be robust to testing/changes on their own, unlike the truth'
			    - the types of difference that a truth is robust to are fragile bc of this triviality in their difference from lies (applying an 'opposite' transform could make any true statement false, so thats not a useful change type to determine robustness of a statement to change and therefore its truthhood), however given that the truth often has multiple alternate forms, applying extreme differences to a statement could also produce another truth
			- 'lies take meaning from the truth, to the point that above a ratio of lies, truths begin to degrade/decay, and fuzzy approximations/representations of the truth & tools to automate that become more true (in the sense of being more relevant/useful/usable than most absolute facts), to the point that computers/algorithms become "relative/approximate truth" factories, based on some ratio/structure of falsehoods they can compute filters for (and how well they can integrate new info with the few details that are important to hold constant rather than allowing them to decay)'
			- 'truths that invalidate other truths are generally more foundational (identifying a previously unknown interface that invalidates other truths by reframing/explaining/decomposing them to such a degree that they become irrelevant, such as a statement that "everything is a function of cause", so that everything seems invalid/irrelevant except cause)'
			- 'structures that have more functions such as "organize information more effectively" are likelier to exist/be true than other structures as they increase the stability/inputs of other structures and allow other structures to exist (are more useful and coexisting), and also increase their own stability (organizing information effectively allows the structure to handle stress better and solve more problems better), leading to a win-win situation that is valuable for life to occur and stabilize through coexistence'
			- 'structures of truth (like derivation methods) that describe/generate/derive/find/apply/compress/store/retrieve info better than other structures are likelier to be true as theyre more effective/optimal in some way than existing structures (the universe is likely to move toward a state that more efficiently compresses reality than other sets of rules, if a state becomes adjacently possible relative to the existing state), meaning truths that use other truths the best/most effectively are likelier to remain/become true'
        - finding the 'information traps (at which point no further information is derivable, as in a dead end) and the sequences to get out of information traps' such as:
            - trajectories across different graphs (like usage graphs, as opposed to attribute or variable or function graphs, to find paths back to a high-variation/high-potential position)
            - applications of complementary (as in independent) insights, such as applying how 'every fact has an opposing statement that is true in some way/degree/context' to find the errors in the generator of a trap and fix those errors which are inputs to the trap (errors like 'inability to derive information' and 'inability to change' and 'inability to be independent'), or similarly applying variables to generate independent directions of change like the primary interfaces, or similarly applying variables to the generator/trap to make it improved to capture more information more efficiently, so as to become an input to the trap/generator, or similarly applying variables to the trap/generator to make it independent, or similarly applying variables to the generator/trap to make it trap itself, or similarly applying limits to the generator/trap so it is the dead-end
                - a good metaphor for the primary interfaces is the 'cardinal directions' (as dimensions of reality) each being capable of reaching the center and the edge of the universe, in different ways (using different combinations generating different structures), where the other directions are derivable from each direction if the center or edge is known and each direction is enough to understand the universe on its own, having a sufficient cross-section as an input, each providing equivalent information in independent ways
                - the cross-interface connection/change sequences between these 'primary directions of change (on one interface)' (which themselves support connection/change sequences) are important as a foundation of reality (the ways that information can change into other information being a source of freedom/potential where change can occur)
            - applications of limits to trap the generator of a trap or the trap itself and applying variables to the information that is trapped to reverse positions, or applying variables to the trap/generator after applying a variable to change its direction/intent to set it free in another direction
		- applying interim connecting structures of useful structures representing the truth like 'rules databases', 'attribute networks', etc, such as 'common structures to both systems' such as 'constant attribute rules' (variable interaction rules)
		- every true statement should connect to other sources of truth (high-variation variables like interfaces, powerful variables like cause/energy, important sources of potential like 'black holes' as potential energy sources)
		    - 'humans are working on climate change (because we havent tapped the extreme potential energy of black holes) and (because we havent identified all the interfaces) and (because we havent optimized energy efficiency)'
		    - true statements should take important/relevant true statements into account
		    - structures like black holes are a source of truth bc they store information/energy/reality efficiently, just like the interface network does, and just like some formulas do (euler/gravity equations, etc)
		
		- definitions/limits/requirements create a metaphorical corrollary of 'gravity' in that they make some structures more similar/probable/coordinating than others
		    - where does variation go which cant be stored/have structure in the universe, or is this not possible and all impossible variation is approximated/echoed in brains to conceptualize impossible structures (like how you can know some things about an infinite set, such as that one side has no limit, and the connection function of adjacent items, and the impossibility of it stopping, and its starting point, which is like glimpsing a subset of infinity, as that subset is all that can occur in or occupy reality), which implies that this infinite level of variation would contradict some other real structure which has more energy or which has other rules protecting its stability, meaning its more powerful than that infinity
		    - limits like 'are there functions which are possible to graph in 3-d euclidean space but which are impossible to form with real structure variables' are useful to find, to determine the differences between math and physical reality and the limits of this connection
		        - questions like this would also determine structures like 'whether math stores more possibilities than reality' or 'whether reality stores more possibilities than math' (math definitions allow for fewer variables than reality), or does reality offer a useful alternate format of math (like a 'cohesive usage network of repeatable math functions/structures'), and which system controls the other (does math determine reality, or do agents in reality control math in the sense that they can use it for their intents), and 'what is the limit on the variation producible with known math definitions, and can that variation explain reality'
		        - if math offers more possibilities than reality could ever support, reality is like a compression of (a subset of) math, and its possible that reality could only ever represent a subset of math structures, rather than all possible math structures, and 'all possible states of reality' is determined by the set of 'all possible subsets of math, below a certain threshold or in some structure limiting the subset', and 'identifying the structures to connect reality rules with these limits on what math structures can take physical form at any time/state (rules like "nothing too chaotic or simple")' is a useful related intent
		        - a compression function could also act like a generative/descriptive/explanatory/causative function
		        - are some structures only impossible bc of other structures that could change reversibly (undoing the change to revert to a lower-variation or more stable state while retaining the new information) 
		        - 'what information/structure is likely to be irreversible, unstoreable/unretainable, incompressible, or create chaos cascades' is a useful related question

		- identifying useful problems and useful problem formats to solve, such as 'identifying how to format problems/variable/interface structures so that they can be formatted as a problem of "solving a system of equations" to find the alternate function sets that can coordinate to determine reality', like 'which set of functions preserves/stores/organizes information the best without contradictions (using the same components and without removing any components from the original function sets which are known as true, or removing any other functions)'

		- identifying connection functions to apply the opposite or other differences to known problem-solving workflows, which are similarly useful in reverse or with other differences applied
		    - for example, given that a default solution automation workflow is to 'generate possibilities and filter them to find a reduced solution set/area/range', an 'opposite-direction' variable applied to this workflow's direction would be to 'connect filtered/reduced sets with the filters applied to possibilities, to reverse-engineer the sets of possibilities that were filtered by reverse-engineering these filters'
		    - an example of these filters would be the 'type' filter, that when a type is identified, a high degree of other information about the structure is known (the 'type' filter reveals a high degree of information about the other structures that are members of the 'type' group, as well as the other 'attributes' of the 'type')
		    - once you know these connection functions, you can apply them by default as 'default filters' to filter generated sets to identify 'subsets with predictable attributes resulting from those filters, given their connection function', or to connect a 'few specific examples' with 'general patterns of possibilities' or 'full sets of possibilities'

		- identifying function sets to map 'generality (in intent) to specificity (in implementation)' is a way to identify the core functions that need to exist (for intents like adjacently generating all other functions), using examples of general intents fulfilled by specific functions, as a way of writing the fewest functions (writing an abstract general function and then varying it when specific valid intents require changing it)
		- identifying ways to integrate alternate core general useful structures like applying 'causal networks' to decompose some subset of data sets, where some points like 'density centers' indicate connections between points such as 'probability of adjacent points' (as a cause of expecting to find other points nearby) and some points like outliers or non-pattern-compliant points representing structures like limits/dead ends in causal networks (as a reason to find those patterns, or as a reason to discard a point as being generated as only an output by one rare process so as to reduce its possible interactions and usefulness), or alternate causal networks of input feature subsets where some causative relationship is probably in a set of alternative relationships, since adjacent/equal points arent necessarily absolutely independent, in the sense that one point may lead to generation of other similar points if successful/useful in some way, or a particular point may be so useful such as simple that it is frequently generated independently, and these related points can be reduced to one representative/general point that can be used to generate all of them, and instead the 'reduced set of representative points' can be altered by applying these 'reasons for point similarity/repetition in data sets' to generate the probable alternate variations of them, while applying regression to just the reduced set, or the 'reduced set + the generated set', to isolate actually or more independent points

		- identify methods of describing useful methods like interface analysis using functions/variables not directly referenced in their definitions as good approximations of it, like describing interface analysis as 'interim thinking' (as 'thinking without definitions/rules to find new definitions/rules to connect known definitions/rules') and 'meta-representation and similarity reductions and integrations' and 'potential maximization', these descriptions being useful for identifying approximations of the logic (similar to how concepts can more effectively describe a structure than the full set of details about it) which can be used as inputs to generate the logic & identify variables of these useful methods to generate the others
		    - similarly, other good descriptions include: 
		        - 'a semi-populated network of interface structures with empty nodes to be determined in between and external to these certain interface nodes, which when viewed from various perspectives/angles outside the network, offers new obvious connections that offer a different perspective that is still true in some way as some interface structures are still included in the subset made obvious and the connections made obvious in that subset, where outer layers of the network grow as new interaction levels of interface structures and new interaction functions applied to them (where the semi-populated network is different/new every time, to avoid repetition)'
		        - 'starting from no information (a position in between known structures), what minimum of information (what trivial set of structures) can you apply that will derive a connection to information (derive a new "function to derive information", like a solution automation workflow, which can be used to connect any information with any other information)'
		        - 'finding new interactions of useful structures like cross-interface structures as default units of computation, given their usefulness when composed adjacently'
		        - 'finding/deriving connections (and derivation functions of 'connections' or of "systems of connections") between previously unconnected interface structures'
		        - "finding new extreme differences from 'adjacent change combinations', as new useful structures are the least likely to be produced by adjacent change combinations, which will never approximate 'thinking' unless the adjacent change combinations are applied to extremely useful structures like solution automation workflows & other interface structures, which when slightly changed, are usually still useful"
		    - this is in opposition to the default 'rule-set' based thinking, which applies trivial changes to known structures to create 'trivial innovations' (which could be replaced by a 'poor copying' function)
		    - 'interim thinking' starts with a 'lack of rules/information (known structures)', like finding a 'path to a safe planet from an abyss in the universe', not by applying rule sets (like 'use known physics rules and calculations using those rules and a calculator'), but by asking high variation-capturing questions which direct changes, and applying trivial structures (like the 'functions of light' and a 'increasing light-interaction function' like a magnifying glass has and interface structures connecting and differentiating these like 'abstractions of these structures'), until a new connection is found that can be used to reduce/remove/change/otherwise solve the problem, questions which are new every time which can produce new structures, bc if a problem exists, its likely that a new solution is required, which is a very limited rule set, so much that it may as well be nothing, or an empty rule set having a template/boundary that is fillable with new useful structures, which is like 'carefully avoiding rule sets while being similar enough to existing rules to be useful and probably realistic'
		
		- identify new intents that when fulfilled, have increasing value over time, as they are highly interactive with other useful interface structures
		    - such as 'find new extreme differences' or 'find new patterns of info storage structures or other structures of truth' or 'find new statements that are generally/absolutely/usually/required to be true' or 'find new equivalent alternates to useful structures' or 'find new variables such as patterns of useful structures' or 'find new interface structures (types, connections, reductions, incentives, simplifications, interaction levels, variables, etc) of interface structures' and other intents that can be generated as equivalently useful to these, which are generally useful intents for problem-solving

		- identify structures that always describe some variable interaction when in different formats, which can be applied as default variable interaction structures to check for
		    - for example, there is always a 'self-similar unit effect (like a domino effect) on some interaction level where that structure is a unit, that when repeated up to a threshold, explains some other variable', so checking for 'domino effects' is a structure to generate from inputs & look for in outputs, just like there is a 'halting effect' placing limits on repetitions, and there is always an 'interface interaction structure' that can be checked for, even if these patterns/outputs arent always fully-developed/detectable at every point
		    - relatedly, generating the structures between these known required/probable structure is useful to identify interim states

		- identify/generate structures that can act as default/core/component structures of useful structures like 'filters' to apply as default useful structures
		    - for example, a 'filter' might take the form of a 'barrier with openings that allow some subset of possible inputs to pass all the way through', or a 'set of cage-like structures that can trap excluded structures rather than being specifically designed to only allow inputs of some shape' or a 'sequence of tests that remove functionality/attributes of excluded structures so they cant move/change further or so they dissolve' or 'some structure that moves different structures to different positions', which are default structures to look for when fulfilling intents like 'identify filters', as they fulfill sub-intents of filters in various ways

		- applying useful reality representation structures such as 'metaphors (similar/relevant but different/inaccurate structures to format something differently in a useful way to achieve understanding)'
		
		- applying physics rules such as the 'fuzziness of physical reality' as a way to find other true structures ('symmetries', 'alternative definitions', 'patterns')
		
		- applying structures like the functions that can generate the highest variation (such as the 'Conway game of life') as a default structure to generate other high-variation structures like reality
		
		- applying specific mappings across interface structures (like concepts such as 'balance', physics structures like 'symmetry', and physical reality structures such as information agent structures such as 'justice' and 'economic equilibrium', or concepts like 'interface', structures like 'bases', information structures like 'metaphors' being variable implementations of the 'interface' concept) as default structures (such as default inputs of a neural network used to predict other mappings)
		
		- apply useful perspective structures (like combinations of perspectives such as the 'optimization perspective' and the 'religious perspective' to efficiently describe other useful structures like 'what agents want and what could be true using adjacent transforms of reality') as a default constant set
		
		- apply useful structure sets that should go together (like how 'dependence' is useful in the 'causal' interface but negative in the 'physical reality' interface)
		- apply useful structures (like 'direction or line connecting starting/goal points' and 'implementation structures to get to that point') to model useful structures that are useful when applied together, like 'intents/implementations of intents', as a useful input to neural networks to fulfill the task of 'adjacently combining them in a way that connects them'
		    - similarly apply patterns of implementation structures such as 'causal loops referencing nested sub-problems resolved with some structure and output to the host structure once solved' as default components of implementations of intents
		- fulfill a primary function of a particular interface that can solve most problems, such as 'evaluate meaning', and the useful functions to fulfill that function efficiently
		- apply structures iteratively to calculate the global/universal meaning to check if its obviously false at scale (in its extreme form which is more easily determined), such as applying a rule like 'its ok to violate someones rights if theyre a genius and youre fascinated by them' (after iterating to an extreme scale, its obvious that no, that couldnt be right, then geniuses wouldnt want to live if that rule is applied at scale, to guarantee that geniuses' rights will always be violated, and it couldnt be right for many other reasons, such as it is probably 'better/more sustainable/and therefore lower cost' to prioritize turning people into geniuses than to persecute geniuses with rights violations which will reduce our supply of geniuses to zero)
			- structures that make other structures obvious are obviously useful
		    - apply interactions of known structures at scale as default filters of other scaled structures, determining what else can exist at that scale, deriving core structures of truth from these scaled structures that could also exist at that scale
		- apply structures that can be used for multiple intents as default useful structures bc they can handle more variation than other structures and are therefore likelier to be compoundingly useful when applied in structures like combinations
		- find useful spaces including definition spaces where some attribute makes it useful for common problem-solving intents
	    	- maximum that a definition can change without breaking, depending on the relationship between changes supported/required by that definition
	        - differences that create different 'definition routes' of a definition (comparisons to similar/opposite definitions, usages, formats, different subsets of attributes that differentiate it in some space)
	        - space where embeddings (apply), combinations (build), connections (derive), and divisions/filters (find) are adjacently connectible
	        - finding a space between other spaces that is required to exist by definition where some attribute is also required (some attribute like an equivalence required to exist bc its between a greater/less than relationship)
	        - a space that enables finding the right layer to evaluate meaning/impact at, such as the 'layer where any pattern is identifiable' or the 'layer where variables are identifiable' or the 'layer where unique variables are identifiable'
	        - a space that makes equivalent alternates adjacent, equivalent alternates such as the 'maximally different structure' and the 'minimal structure required to describe primary interface concepts like potential/cause' and the 'dichotomies of reality', which are probably equivalent but are not trivial to derive from each other
		- apply structures that can offset neural network inadequacies such as 'limits of a series' being useful for solving the problem of modeling 'longer sequences of input/output connections' as a tool to determine when a sequence might converge, which is obviously useful for prediction intents, as well as other structures that allow the user to 'see far ahead', such as 'highly complex/different realistic/stable/efficient' structures, useful determining functions of extreme attribute similarities/differences (like absolute change type interactions, such as preserved change types when some function is applied) as 'extreme' structures and 'equivalent alternate structures (that can keep each other in check and identify invalid structures)', and other structures that allow you to efficiently calculate a lot from a little information
		    - you can see how useful functions (like calculating an adjacent attribute such as 'constance/addition' that reveals non-adjacent information like information about 'extreme change type preservation') act like a powerful information filter that reveals information that is difficult to calculate (non-adjacent)
		    - finding useful structures to determine attributes of other useful structures (useful structures like 'extremes') can be done by reverse-engineering 'input problem cases' where these revelatory 'filter' structures would be useful:
		        - finding highly different structures like extremes or complex manifolds (high-variation change based on an interface like a connected structure such as a manifold, which is a useful math structure adjacently mapped/corresponding to an interface)
		        - determining what is easily determined about that highly different structure once you know it
		        - determining what cases this information could be useful in
		        - determining whether those cases are useful
		    - similarly apply functions to 'calculate convexity' and 'filter possible functions given maximal differences (structurally similar change types that can look similar at first like exponential change, waves, hyperbolic functions, etc but which have adjacent filters to filter them out) detected by some point' as a way of improving 'gradient descent errors'
		    	- you can see how these 'interface structural similarities' (like an 'exponentially increasing subset') provide a useful constant base to apply changes to, to determine other possible structures and identify filters of those possible structures
		    	- this is an embedded application of the 'interface' concept, which is clear in the usefulness of this 'similarity to base changes on'
		    - similarly, using 'lifetime data with associated errors across a lifetime' to put a statement into perspective, or having 'civilization lifetime/trajectory' data as opposed to phrase/sentence/paragraph/story data, is likelier to connect information to more relevant structures (to answer questions like 'what problems were solved by a mind that regularly produced this thought')
		    - similarly, including 'historical/prediction data of past/future truths' with answers such as 'additionally, next year, this will probably change x% and previously this was y%' to give context and only provide info that is immediately outdated but is likely to still be true the next time they run the query
		- apply interaction levels ('adjacently connectible structures that interact/connect') as 'input information' or an 'information format' to neural networks (which fulfill the task of 'finding adjacent combinations to connect variables')
	- applying a function to find interaction levels where the problem is solvable with adjacent combinations and the function to find those adjacent combinations once transformed to that interaction level
	- iterating through filtered interface structure combinations and checking test cases of input/output pairs
	- create a network of solutions to use as bases to navigate
	- iterate constant/variable pairs to model the highest degree of certainty/uncertainty pairs
	- applying more adjacently determinable structures as an input
	    - attribute networks as an input and function networks as an output implementing those attribute networks
	- finding an 'input-output sequence' that would be useful and would probably be adjacently constructible in that sequence
	    - like a sequence of first building an 'attribute network' to describe all variables and then building a 'function network' describing that network and then a 'function generator network' to compress/generate that function network
	- applying default interface structures as the core functions of a machine learning network so that errors like 'not understanding abstract concepts' can be avoided by injecting abstract concepts as a default input or function or related structure, so that 'combinations of abstract concepts' required to 'solve a problem of understanding some conceptual combination' are adjacent
	    - 'inventing new machine-learning models' doesnt just require 'sequences of change combinations on some input features'
	    - getting machine learning models to actually invent a new optimal machine learning model requires a function to 'identify optimal models which are maximally different & useful across all problems'
	    - this is 'accidentally stumbling on useful functions' by adjacently changing a weight sequence representing a function (as functions have variables such as components)
	    - this may look like intelligence but is only "accidental coincidental effects of making 'variable changes' resulting from the function chosen seemingly at random"
	    - this "discovery" is a re-iteration of the known fact that "ai models change variables that can act like functions, and then those changes are sometimes useful at reducing the loss, so the model keeps those useful changes rather than discarding them"
	    - this "discovery" is an advertisement to the lack of understanding that "some functions can be re-used across problems"
	    - the model didnt magically know this particular function would be useful across problems, it just applied adjacent change combination that are basic/core changes, and which are easy to find, which coincidentally happened to be useful across problems, bc of the power of some core changes
	    - that is a fact of reality that should be an input to an actual intelligence algorithm (core functions are the only type of function youll get with 'adjacent change combinations' unless your 'inputs are adjacent to interface analysis structures' or your 'connective function is maximally differentiating'
	    - https://arxiv.org/pdf/2211.15661.pdf

	- find adjacent structures to useful structures and how to connect the adjacent structures so that useful structures are adjacent using these connections
	    - for example, finding that 'poverty costs more to maintain than to fix' is a result of finding 'similarities' to 'costs', such as 'poverty', and then connecting them (why is it similar to a 'cost'? its related to a lack of money/resources)
	    - as another example, finding that 'mirrors are useful structures to find hidden information visible from another angle (as they offer useful differences to input angles around the symmetry of a right angle)' is a result of finding 'similarities to light' such as 'light symmetries' which are the reason theyre useful for that intent
	    - these have a structure in common like:
	        - 'cross-interface structures (cost-lack and light-symmetry)'
	        - 'similarities to important variables (these important variables being variants like cost/light of primary interface or otherwise useful variables like information, which are a component of inputs to information such as probability, as costly information is less probable to exist or be measured and probability is an input to information, or are variants of information on different interfaces like the physical interface, light and cost being important variables which are related to information)'
	    - the question of 'what is the limit of the usefulness of finding different variants of a definition and replacing the original definition with the variant in its interactions with other structures' and 'what is similar across similarities to important variables' are other related useful queries

	- finding functions to find/derive/generate & avoid errors in a complex system
	    - for example, in the problem space of 'creating a virus to restore original dna without epigenetic mutations', some worst-case scenarios could be:
	        - some reversal of a mutation causes the problem youre solving in a different way so that 'solving it' in that way just shifts the problem to another position/state (constitutes an illness trigger in a particular host state, or causes an illness given other mutations/genes)
	        	- the host has adapted to the mutations, meaning if you reverse them, the host dies bc they now need that mutation, which need youd have to fix first to avoid killing the patient by reversing the mutation
	        	    - epigenetic mutations store some memory or functionality that some process has come to rely on
	        	    - reversing the mutation in a healthy patient would work, but the patient has complications like organ damage that make the gene therapy deadly
	        	- there are several 'state changes' that need to happen in a sequence to reverse direction away from some bad health state, where if you skip a step or get the order wrong, the host dies
	        	    - some mutation is ok to reverse in one state such as when otherwise healthy, but not given the mutations required by this sequence
	        	    - once this sequence of steps to reverse all mutations is calculated and applied, it turns out a state in the sequence was optimal to stop at, but the completed sequence was deadly, as some mutations could be kept without killing the host for some reason (they made the host better at something, which theyre now lacking)
	        	- the mutations have become so different from the original state that making these reversing changes is too stressful as its too many changes at once, and they need to be broken into multiple subsets of changes to avoid over-stressing the system
	        	- some negative environmental or default response to the gene therapy triggers reversal of mutation reversals (stress-handling methods create mutations, such as exercise, stimulant substances, or cell cycle/immune triggers, which might reverse some reversals before the therapy can be completed)
                - some other system variable needs to be synchronized with the therapy, such as to 'deliver it at a certain time to sync with circadian clocks or right before sleep cycles, distributed over several days' or where some other alignment needs to happen to successfully apply the therapy in a non-fatal way ('deliver it in a way that doesnt trigger heat stress proteins or disrupt cell communication or other intrinsic required processes that must be stable for the host to live or for the host to respond successfully to treatment', for example)
                    - the gene therapy has to be used with some other treatments (like immune suppressing treatments) to deactivate existing processes to prevent mutations, or some process will neutralize it (the immune system will attack it)
                    - it turns out that some unforeseen condition is true (like 'if one cell has a mutation, it will re-occur in all/most other cells', or 'if you change dna in a way that the system is not accustomed/adapted to, the mutations will re-occur out of convenience for the system that uses them'), conditions which only occur in some patients/health states/environments or with some mutations
                    - or some other condition like 'if you apply a gene therapy that is sub-optimal in some way like involving a lot of mutations, you also have to speed up the cell cycle temporarily or induce cell communication or apoptosis in some cell type, or it wont take hold' or 'gene therapy in some sub-optimal way like "many simultaneous mutations" is only effective if some gene to control gene editing/immune function is switched on' applies that we dont know about
                - the gene therapy changes how other complex systems interact (how the host microbiome interacts with their brain, given the impact of dna on both systems) in a way that causes illness in other complex systems which are difficult to predict
                - the gene therapy is successful in the cells it reaches, but it doesnt reach many cells bc default dna repair processes are too effective at preventing this many mutations or those specific mutation types and they effectively deactivate or kill the virus
                - the virus is too useful for other pathogens, which eat/use/attack it before it can complete its processing
                - the virus quickly becomes pathogenic and causes sub-optimal mutations/illnesses, as it gets mutations in its own dna, either from the host, from the host's illness state, or from other pathogens/chemicals in the host
                    - similarly, the virus allows existing pathogens to become more pathogenic
                - the set of mutations required to fix it is so volatile & different for each individual and you would have to know every pathogen, every input (environmental, habitual), every distorted variable in their system, and every illness state they have before making an accurate prediction of how to fix it bc of the volatility (meaning the solution can be drastically different even for similar people)
                - the gene therapy is successful in a patient but the therapy spreads to everyone around them by environmental exposure and it can kill other people so they have to be quarantined or quarantined with people having similar genetics for some period, or the mutations would have to be paired with some other mutation/state/treatment so that the virus would die/deactivate after completing its processing
                - the side effects of changing a lot of genes at once is frequently deadly, no matter which genes youre changing, bc of default bio-system responses (similar to the toxic responses like sepsis to mass cell death triggered by the immune system)
                - the therapy is successful, but the patient's original dna was vulnerable to some other pathogen which they will probably encounter
                - the therapy is successful, but it erases information gained over the patient's lifetime (like immune memory cells or neurons tied to some microbiome change/state)
                - the therapy is successful, but the patient had habits that made their original dna sub-optimal as they didnt use their genes optimally, and their pre-therapy genes were actually more usable in an optimal way
                - the therapy is successful, but habitual/environmental changes could have replaced it with less effort or the pre-therapy genes had some unknown benefit or there was a way to trigger original dna copying without artificial therapy but we dont find it bc we focused on the artificial therapy
                - the therapy is successful, but bc of environmental conditions like medicine/pollution that cant be avoided, the patient's original dna is suboptimal compared to some other set of mutations or their pre-therapy illness state
                - the therapy would be successful, if not for other currently poorly understood components like dna fragments and metabolism and their interactions with dna edits
                - each change to the bio-system would have to either not impact required processes (like energy/enzymatic pathways) or change these processes to an alternative, which means an alternative is required, and there are some processes with no alternative in the bio-system, and these processes could easily be impacted negatively by some gene changes (or the patient would need some complementary treatment to handle these impacted pathways like blood cleaning processes to remove toxins from immune/cell cycles)
                - some changes to dna dont impact patients negatively, but these are probably relatively rare as they require some attributes like 'lack of interactivity with other components' and/or 'similarity to components which are already handled to some degree in some way', and most dna changes are unlikely to have these harmless attributes
                - dna processes have some requirement we dont know about, such as requiring 'some junk/repeated data or neurons or pathogens to help handle mutations' or some other dna edit process that we're unaware of, which is only a problem in some patients
                - mutations are inevitable without some extreme behavioral condition like extreme workouts/supplementation with some substance/rest/temperature, so some mutations will probably/inevitably re-occur, requiring re-treatment
                - mutations present in the host make it unclear/ambiguous what the original dna was, so the target optimal state is not determinable by seeking the original dna state, and a 'generally probable healthy state, which is healthy for most/other people or given some known optimal gene configurations' is a better goal to aim for
                - dna changes have impacts at different interaction levels (dna groups, chromosomes, dna fragments, dna-editing dna, jumping genes, etc) which are currently not predictable/known

        - alternate intents to aim for
            - a 'more general solution that helps most patients to some degree' is likely a better short-term target (which undoes a few common harmful mutations without harming most people) than aiming immediately for a general highly customizable solution that works well for each individual (which it may not currently be feasible to retrieve all the variable information required to implement such a solution)
                - similarly, a 'dna damage-reversal solution' that reverses disruptions to important/required processes, like solutions to 'soften arteries', 'lengthen telomeres', 'de-calcify calcium deposits', 'grow new cells to replace cells damaged irreversibly by acid' and other treatments for negative stress responses from dna damage, microbiome damage & other related problems, might be more effective short-term targets, to fix some negative side effects of dna damage rather than fixing the dna damage itself
                - similarly, a technology to 'prevent dna edits of some type or of any type' could be useful to maintain some health state or to maintain health once mutations are reversed or useful mutations are applied
            - finding mutations to protect against the more powerful/invincible viruses with more evolved self-sustaining dna might be a better goal as they have attributes in common like 'hiding mechanisms from the immune system' which can be targeted
            - similarly, other example intents include identifying 'cancer cures' which could fulfill various intents involving specifications of 'targeting different regulatory processes', such as 'adding production of an antigen to target cell masses of different sizes which dont have useful or approved functions (which dont have self-regulatory processes that they use, or not regularly sending inputs to other useful cell masses)' or 'genome modifications to prevent edit/mutation vulnerabilities, to add more regulatory functions' or 'immune optimization, to add other immune factors which can regulate known immune errors' or 'antigens which have a non-binding function with approved/useful cells' or 'pathway optimization, to prevent existing pathways from being exploited by cancerous processes' or 'energy use regulation, to prevent one cell mass from using much more energy/growing much faster than other cell masses or growing faster than any cells that could inhibit it' or 'adding cancer inhibitor-production functionality, such as adding genes to produce anticancer substances like polyphenols or silymarin' or 'adding integration functionality, so checks of "general health of the host" is integrated with lower cell processes, or so that growth/regulatory components are always interacting with each other by integration'

        - implementation methods
            - finding the 'healthy gene sets required for a normal healthy bio-system in most people' and making sure dna edits dont change those required gene sets for health
                - similarly, finding mutations that help optimize existing systems, like 'gene sets that make antibodies more trivial to create and more optimal/effective to help the immune system'
            - finding mutations that mimic or match patterns of or are otherwise similar to 'natural healthy evolution dna edits (like those that improve stress-handling response or immune function)' as opposed to 'illness-causing dna edits' and restricting changes within the found range of dna edits on some graph where these regions of evolutionary or otherwise healthy mutations are adjacent
                - similarly, formatting genetic information on a useful high variation-capturing interface like the stress interface, so that some illness-causing mutation is adjacent to others bc of the bio-system stress they cause or the stress which is required for them to become a problem
            - some genetic changes can be 'tested' in the sense that side effects are probable/known to be 'increasing from zero' and 'initially non-fatal' to a degree that there is enough time to reverse them with an opposing or de-scaling mutation virus after if side effects indicate a negative health state change, or altered to an interim state that is less strong in the metric that caused the state change, to allow for a tuning process of changes during testing
            - finding stressors that reverse dna mutations (on the assumption that there is a stressor that can reverse any mutation) and find a sequence of stressors that is survivable in most patients for the illness-reversing mutations these stressor produce

        - for an example of applying interface analysis to understand genomes, understanding genetic differences is a matter of applying various interface structures (like 'multiple', 'scale', 'concepts', etc):
            - identifying what functions genes fulfill in the 'genetic language' (how to determine functional requirements that genes provide in/directly, by directly coding the instructions or indirectly providing components of other structures like epigenetic changes)
            - identifying how genes are used/interpreted in a 'genetic culture' (which can explain how similar genes are used differently for different effects/outputs)
                - identifying differences in ways genes can be used to identify alternate 'genetic usage states' (like a functional response to a mutation)
                - for example, if a metabolism can use different inputs (energy sources), different genes which encode different energy processing functions can provide the same input, although the genetic sequence will look different
            - identifying different required genes, if a different optional function is to be fulfilled (explaining how different individuals have different functions, as some genes are optional but are required for a particular function)
            - identifying required functions for life, which the full set of genes must include instructions for (the organism has to have a metabolism, so somewhere this variable must either be directly covered in the genetic code, or it must indirectly evolve as a default/adjacent interaction from applying the genetic code, meaning the whole genetic code isnt free for other functions, as some of it must encode these required functions like metabolism)
                - relatedly, identifying the ratio of genes not used for these required functions for life which could be used for other functions
            - identifying how genetic variables (like junk dna, epigenetic changes, jumping genes, gene groups, mutations, etc) can be repurposed for emergency functionality such as 'developing new functions', 'providing backup functionality', 'providing functionality that emerges with iterations/repetitions like resilience to mutations'
            - identifying how different genetic structures (like 'circle dna') can interact with other structures like 'pathogen dna' in the 'interactome' (like 'providing functionality for pathogens to borrow in a more usable format and a more common structure, making that likelier')
            - identifying the similarities/differences that hold across genetic interactions ('different genes can encode the same function', 'similar genes can encode different functions', 'similar genes encode similar functions', 'which similarities/differences are required and which are optional' etc)
            - identifying the functions provided by host pathogens that are harmless or useful to the host, and the dna involved in those pathogen's useful functionality and this dna's complementary or overlapping functionality with host dna functions
            - identifying the allowed genetic changes by the immune system, the changes allowed by the immune system that change pathogen/host genetic interactions, etc
            - identifying connections between environment stressors and genetic changes or gene sets, as well as 'ranges of direct/horizontal possible changes resulting from environment stressors'
            - identifying scaled interactions, like the 'scaled/iterated evolution of a pathogen in a host', interacting with the 'scaled/iterated progression of a condition in a host', interacting with a 'scaled/iterated progression of a carcinogenic sequence in a host'
            - identifying interactions between genes and functions that impact genes like immune functions and metabolite/blood-clearing functions or dna repair functions
            - identifying 'ratios of genetic differences, compared to ratios of functionality differences across individuals'
            - identifying functionality-neutralizing genetic changes/sets
            - identifying networks of and overlaps in functionality determining high variation variables like immunity/genetic changes, such as 'pathway activation' such as 'metabolism' or 'signaling' or 'activation' or 'methylation' or 'oxidation', these pathways impacting both immune and genetic systems

       - as an alternative complex problem system example, similarly, the reason why 'racism' is an inaccurate descriptor is that there are more correct structures like 'self-similarity/approximation/historical/group/power dynamics' as capturing more variation than the specific structure of 'racism'

        - as an alternative complex problem system example, some interface structures (which are already known to be useful across problems and are defined in interface analysis) can be useful for finding errors
            - to find the error of a function like 'fulfill default intents', use an interface query such as:
                - 'at scale, what is the emergent output of a function (a function such as 'fulfill default intents') given some pattern (like 'contradiction of agent-agent intents (meaning death)') in some interaction level (like 'agent-agent interactions') when repeated, and what is the limit of this interaction (societal collapse from decreased population)'
                - when agents fulfill their default intents (some of which will inevitably contradict each other), and apply no other strategies, anarchy occurs and society collapses from the population decrease
            - this interface query would find an 'intent-contradiction error' in 'emergent output in agent-agent interactions' by applying the 'context' concept of the 'system' interface which checks for impact of a function in the system it is applied in ('context' here means measurable structures such as 'emergent output at scale when repeated to its limit')

        - as an alternative complex problem system example, 'competition' is sometimes an 'error' structure, such as in complex systems like those with rules designed to prevent the errors of competition such as the free market with regulations applied to protect privileged/predator agent incentives, which is the type of regulation that the free market produces
            - in cases where 'competition' leads to errors like 'false or otherwise trivial displays of virtues instead of actual contributions (like in an information economy such as the internet, where information is abundant and cheap and easily faked)', 'competition' can result in abundant false information where true information is drowned out by comparison
            - this can be derived by applying interface structures like 'incentives' (such as the 'incentive to falsify information'), 'incentives' being something that agents are likely to apply as 'default decision rules' to fulfill common/default intents like 'cut costs', which is a default intent in a 'competitive' context
                - the 'incentivized action' in this context is to 'falsify information', and in the specific context of a 'market that rewards virtues but doesnt test for them or require them', the specific incentive is to 'falsify information about virtue'
            - finding incentives as a default structure is as simple as applying the default interface 'cost-benefit' variable to determine 'common cost-benefit interaction variables (like the common "high benefit/lost cost ratio")'
            - what alternatives exist to correct this error structure?
                - a default 'opposite' structure of this error is: "apply 'any cost by default' to create problems to require thinking by default ('thinking' as 'applying more rounds of analysis') which is the opposite of 'acting on obvious default incentives'"

	- finding a function set that 'converts/connects interface structures to/with other interface structures' is a function set that can generate all other useful interface structures bc the primary interfaces already identify the primary directions of change (similar to cardinal directions) and functions that can generate one from the other can identify the full set as well as identifying the differentiating variables embedded on those interfaces
	    - interfaces can be defined alternately as 'structures which can cover/support/capture the information of reality like a continuous field/fabric'
	- for the intent of 'finding useful spaces'
	    - finding 'structures of related optimals' in a space of sub/optimals (a meaningful space, where every point is useful for some ratio of intents, similar to how 'useful structures' are meaningful in the sense of 'adjacently or directly implementing some intent') where some structure of optimals avoids errors above some threshold is useful to connect useful structures like variations of optimals
		- like how some structures which may seem required or otherwise optimal like the 'reward' in a 'reward function' may seem optimal, but the assignment of the reward may interfere with additional inference processes (a learning function with an understanding of meaning wouldnt need a test/reward function set to guide it)
		- related to how the 'reward isnt the point' but the point is 'developing a learning function or learning function-finding method that can learn anything', as nearly everything is both correct and incorrect in some way (the reward helps the learning function when its constructed a certain way, but it also takes resources and isnt the point of a learning function, and learning without a reward would be better if it aligns more with the point or otherwise is more adjacent to an accurate understanding of meaning, as a reward is an additional requirement that not every learning function would need), as everything can be connected using insights, so finding structures that connect structures of optimals (like a set of differences such as 'variables of different filters applied to sort before or halt a trial and error algorithm' to connect suboptimals or pairs of sub/optimals) is more useful than a function to find any one optimal, and given that anything is connectible, an optimal learning algorithm should be able to derive learning rules from anything (such as learning a generate/change, test, filter/reward function set or equivalently an insight set that is useful for solving problems, etc) to achieve meaningful understanding
		- other useful spaces include the 'space of complex structures' where equivalently complex structures (like complex inventions which are similarly useful or equivalent alternates) are mapped to or otherwise connected to each other, where solving for some complexity space means solving all the spaces more simple than it by default, as the complex components can be re-used to solve simpler problems but not all simpler problem components can be adjacently combined to solve the complex problems, like how time is asymmetric in that it favors a direction toward some structure like a ratio or interim point of organization/entropy or interactivity/variation, prompting questions like 'what asymmetries can offset a foundational symmetry like cpt symmetry' and 'is time a foundational symmetry allowing other symmetries to form, so there is no more efficient foundation to navigate to and no way to countradict/change it, or are there other foundational symmetries like "equivalent interchangeable alternates" which are stronger as a foundation than time'
		- a space of 'maximally different structures' (like independence, interactivity, organization) which can invalidate questions about variables of each maximally different structure is useful for adjacently generating new perspectives
		- similarly a space of 'most-used interface structures' (like a 'function generating primary abstract conceptual interactions (such as power interactions) in a system') is extremely useful
		- a space that organizes optimals/solutions in such a way that each optimal can be calculated with some simple function from another optimal (like a lattice where you can move a set amount to get to another point) is a useful organizing structure to search for (using some navigation function, like 'change each of the maximally different variables that describe/generate/determine the function while still maintaining a cross-interface structure')
		- a space of 'equivalent interchangeable alternates' (such as 'generative/determining/descriptive/representation' functions) where these equivalents are adjacent is useful for finding more adjacent structures from a given starting point
		- a method such as 'standard least squares linear regression' leaves out almost everything relevant about the universe to identify whether some variable set 'changes together' (are there more adjacent variables that can be generated from these variables as interim variables connecting them to y? is there randomness interfering with some variables? are some variables just common structures like obeying incentives, stress, or lies? do all variables 'change together' in some way once connected or formatted differently? what level of directness/independence of connectivity is being tested for by this method?)
		    - some subset in the space of all generated functions (under some number of steps, including 'identifying/generating new spaces allowing differences and making other differences obvious') is actually good at sorting and organizing this information in a useful way (identifying possible alternative relationships, identifying possible ambiguities/unknowns, identifying interference from other systems/randomness, identifying variable attributes like directness/independence, identifying useful insights to connect variables, identifying position in a system state sequence, identifying possible systems where these variables could exist and interact in this way, identifying maximally different variables in the input set)
		- identifying connective spaces to connect non-math structures (like language/system diagrams) with math structures (like sequences/networks) is useful for solving problems with existing resources (such as indicating language using a set of spaces where different sequence angles indicate some difference in a subset of language variables in sentence/paragraph/manifesto sequences, where the set of spaces determines independent complementary information about language)
		- finding a space of 'interaction levels where a function would be adjacently implementable' so that similar points on the space can find alternate interaction levels (of functions/variables to use as components to adjacently combine to find a function implementation)
		    - this considers the investment or 'amount of work/energy' invested in the work of constructing/finding a function, the work done in the final function logic itself, and the work of applying a function, as an alternate function attribute, similar to intent, and related to complexity but different from both, more related to the 'differences in the set of possible functions fulfilling an intent' and the 'differences created in functions by different amounts of work (low-effort vs. high effort functions)'
		- similarly, the 'space of all symmetries' is obviously useful for finding 'symmetries/similarities/differences/patterns/interactions of symmetries'
		    - when these spaces are graphed, queries like 'if you switched the values of all equivalent alternates (or other definitions of symmetries) in some specific case, would you occupy a different configuration of spacetime that is stable and are there reversible trajectories connecting these configurations' are more adjacently/obviously computable
	- finding all high-variation variables/functions and applying that as a base to describe other high-variation variables/functions
	- apply optimization interaction structures (like a 'set of game strategies that can result in a tie') as being useful structures to model other useful structures like 'interchangeable equivalent alternates'
	- apply the 'biggest differences in between known problems/solutions' as a set of useful differences to apply to model other differences which are likely to be more connectible than those differences
	- apply combinations of workflows to find other workflows, to find solution-finding methods
	- apply structures guaranteed to be relevant (like 'changes within x causal degrees') as a default set of structures likely to be useful in solving a problem, then apply other useful structures like 'combinations/input-output sequences' to those structures to find the useful set of structures to solve the problem
	- apply a sequence of these other implementation methods as a way of designing a path from mvp to final product
    - apply useful structures that are more adjacent to solutions than problems are, such as a structure including 'interactive structures' and 'useful descriptions of the problem'
        - the better explained a problem is with the best representation of it, the easier it is to solve it
    - finding structures that represent the most interface structures (like a 'high variation structure that represents various core interface structures like interactions and requirements and generative functions')
    - finding structures that describe most problems/solutions in terms of interface structures such as 'a breaking of an interface' or 'misrouting of randomness' or a 'structure of resolution between ambiguous alternates' and other useful interface descriptions of useful structures, so these can be re-applied or applied as defaults or core structures
    - similar to how solving a maze can involve applying these structures, as 'equivalent alternate' structures that can provide the same or similarly useful information in solving the maze, deriving other useful possible solutions is possible by applying useful concepts like 'incentives' (to derive basic structures likely to occur)
       - applying regular 'direction of motion' checks to make sure the agent is still traveling toward the goal and not repeating routes
       - applying standard maze configurations as possible alternatives to select from
       - solving for standard tricks to check for in a maze that are incentivized and therefore likely to be encountered, and solving for the solutions to those tricks
       - solving for interactive sequences of paths that coordinate/cancel each other and can/cannot exist in the same maze
       - solving for 'maximal filters' that can filter out possible mazes the most efficiently
       - apply reverse-engineering to find indicators of various possible sequences of end paths nearest the exit
       - these structures involve solving other problems than 'apply any route using trial & error as a base solution, and change it as you go' which is a default solution to solving a maze
       - solving problems like mazes is a proxy for solving other problems bc of the high variation captured in a maze, if the maze reflects realistic randomness and other variable interaction patterns enough, similar to solving other games if they reflect realistic structures enough
    - a set of useful descriptions of reality that have reasons why they could be true (like a 'calculator of efficient methods of preserving energy cycles' and a 'structure to support maximal variation/uncertainty (time)' and a 'stable alternate coexisting sequence (time) finder') are useful to apply as defaults and efficient structures and truth structures, as well as the variables creating uncertainties (like 'unknown beneficiaries of calculations' and the 'maximally different structure' and the 'most stable system that supports the most variation') in these descriptions and the variables between them (such as interface structures like different priorities), as well as the differences between the description and related structures like 'problems solved with that description' and 'priorities fulfilled by that description' which make the meaning and reality of each description more calculatable
    - apply useful structures like 'clarifying structures' which make something obvious similar to how certain filters make some solutions obvious such as 'standardizing' and 'difference-maximizing' filters make differences more obvious
        - for example, finding standardizing structures like 'matrixes' which are a useful format that is also useful for other intents like 'mapping sets of sequential operations' and 'reducing some function to a set of adjacent combinations once in that format' such as 'solving linear systems of equations' 
    - applying useful methods such as methods of deriving information about other useful structures such as 'types' (all members of a type have this attribute that defines the type) and connectivity (constant lines are formed by a type of addition so it makes sense that adding them which is applying more addition doesnt change their shape, as their shape is the product of addition, as opposed to an operation that adds a dimension or restricts range, which would be required to create different shapes)
        - finding the 'type' of an object gives almost free (low-cost) information about that object, bc of the information stored in the type, where the type acts like an interface that can support some variation within the definition
        - other low-cost, high information-producing structures (like abstract concepts such as balance, alternate definitions, interaction levels, similarity to some other known structure, useful filters like standards to derive maximal information, useful networks to know a position in which is a high-information structure in useful networks) can be derived in a similar way and prioritized as default structures to find
    - useful formats of structures similar to a standard network but different in a useful way like 'gap networks of connected empty shapes' to represent related structures like intents to fulfill with implementations filling the shapes, or 'state networks to model useful sequences/queries' or 'maps to model useful connections' or 'map networks to model different analysis perspectives' or 'interface networks to model bases that capture high variation'
    - a set of certainty/uncertainty pairs to apply as default problem/solution structures capturing high variation
    - build sets of 'loosely related possible' associations (which arent guaranteed by definitions but which are allowed) using definition-adjacent connections, like how the connection between variables 'constant' and 'constant squared' involves a definition involving 'multiplication', but also has related attributes that are outputs like curvature which are required by the definition and other connections which are not as relevant like 'constant preservation of data type between input/output as a scalar' which is a loosely associated connections rather than tightly bound by the definition, using these loose associations to discover new possible connections not explicitly defined but also not definitely restricted
    - find component functions of data set subsets and iterate applying these until a non-matching point is found outside of the acceptable error range, adding terms or alternate functions to process in parallel to handle new points where found outside the range
    - find subsets of the data set that shouldnt be reduced to a function bc of complexity and randomness and other factors likely to predict insufficient information or variable injection points or other uncertainties that cant be resolved, where other subsets of the function are clearly mappable to functions
    - find more useful structures to describe variable interactions such as 'variables (such as squares) creating requirements (such as required growth, as in positive or nonzero growth)' and 'variables creating other useful structures like embedded change (change on change, like exponential growth leading to the accretion of matter)' and inputs to these structures like 'equivalences in factors creating self-similarity (in multiplication/area inputs) leading to multiple differences in outputs (of multiplication, compared to adjacent inputs)' which are useful in their capturing of high variation (similarities/equivalences creating differences like multiple differences between change rates of adjacent inputs)
      - the core unit of maximized potential of a variable in its interactions in isolation of other variables (self-interactions) is the area that can be created by the unit of core maximally different interaction (multiplication) with itself, representing its interaction space in the 'adjacent sides of a rectangle' operation (multiplication), leading to its potential (in its possible range of impact, as squaring it is maximizing its differentiability) to influence probabilities
      - finding the important alternate sets of functions that lead to these important structures like 'required growth (a relevant structure of reality)' can generate a 'limit scaffold' ('the model must not contradict required growth of some structure that has growth as a requirement or other form of certainty, as in some variable in the model must not grow in a way that contradicts known required growth of some other variable') that represents 'points of impossibility' that should be used as filters to avoid when modeling reality
      - the intersection of 'generative scaffolds' and 'limit scaffolds' is a useful place to start modeling the dichotomy between certainty/uncertainty to explore, format, & filter the space between them that is allowed by reality
    - finding other useful representations of a function (such as a 'stack of squares of increasing side length' as a useful alternate representation of x-squared to represent the value of y in a clearer way that reflects the equivalence in multiplication inputs and the multiple differences created by the equivalence in multiple dimensions) which increase the relevance and meaning of a function representation, as its probable interactions with other functions is more clear given the core relevant differentiating attribute of equivalent factors
      - similarly, the equivalence in factors leading to a line with slope 1 (1,1, 2,2, 3,3 as inputs) is significant and indicates the relevance of x-squared as different from other functions and more relevant as a unit function of change and indicates the relevance to the output by its squared area created by the difference between these input pair points and the origin
    - applying rules to find highly useful structures like interfaces such as by asking questions in a sequence like:
    	- 'what are the maximal difference-capturing variables like change' (such as 'functions vs. constants') or 'what contradictions exist in useful structures like change' such as 'what changes dont change' and 'what are variables of changes that dont change (limited change around a symmetry)' and 'what changes unchanging variables (constants, symmetries)'
    - rules like "'what are not inputs' are also a cause of structures in addition to inputs" (bc resources not invested determine states of alternate functions) which is another reason to derive missing information (not just to determine what a function will likely do bc of some input but also what other functions could stop that function given missing inputs devoted to alternate functions)
    - an alternate implementation of a 'blur' algorithm (using the compression/filters/other interface structures involved in vision) to quickly determine trends in a data set is to sample the data set and evaluate each subset quickly to benefit from the overall impression of the emergent pattern visible across a sequence of alternate subsets, where the impression is formed by easily differentiated structures like 'border angles', 'densities', 'ranges', etc which are common across multiple subsets in the sequence, or to align subset data sets as a sequence or network of subsets (organized by some similarity), to make trends more easily identified
        - similarly iterating through data set subsets quickly (to efficiently store memory by identifying the most obvious repeated patterns) is another way to implement an algorithm similar to 'blur', if the neural network has the ability to derive 'overall impressions of a shape' to compress the data set rather than just simply storing the whole data set
        - similarly, identifying a regression function is possible by finding which functions seem to be in the center/average of a data set (as a proxy for their summarizing capacity for longer when rotations/angle changes are applied (to view it from a lower or higher position), a transform that doesnt change the summarizing capacity of a function immediately if its a good summarizing function, 'summarizing capacity' of a function being easily determined from 'impressions' achieved by low memory (obvious feature retention) or fast processing times ('blur' effect)
        - similarly, identifying a data set or regression function that is 'most similar to the origin data set' is trivial by applying some transforms to some maximally different or standard or randomly selected data sets and positioning them in a way that aligns with the original data set (like by aligning its ranges in a separate graph that is close enough to easily identify differences but obviously separable) so it's obvious whether the added data set/line is similar to the original data set (and therefore the regression line of the added data set can be used as an approximation), which applies the 'comparison' functionality of a symmetry to the concept of 'data sets/regression lines'
        - where the 'summarizing capacity' of a function describes how much info the summary preserves (such as info about limits/averages/densities/vectors describing variation from base functions/patterns/possible interactivity/adjacencies/errors/ratios/probabilities, etc) which are metadata about the data set that can be variables summarized by the data set as well as subsets of data set points can be summarized as components of the data set, these variables of the function being possible input components or output solution metrics or interim sub-intents of a regression algorithm, which can be used to determine all other algorithms
        - as a metaphor, the "'definition routes' that when combined say the same thing (in a different way)" is a good set of nodes on a language network to use as an approximation of the variables of regression algorithms (and problem-solving in general), as a stable interface structure that allows maximal variation (it will be similar to my set of verbs like find/build/derive and structures I have identified as useful and the interfaces Ive identified and so on), as different regression algorithms indicate a general summary of the input, which varies somewhat around the interface of the input data set, while "saying the same thing" in the sense of preserving info about the input and "saying it in a different way" such as by preserving different variables and applying different functions
            - a query to find the 'summaries that are the most similar, where the inputs are highly different' is a good way to find these nodes
        - relatedly, finding out if a function is a straight line or a wave function with very small magnitude (or another type of polynomial with difficult-to-measure incremental changes) is a matter of testing a subset of the data set for variation and extrapolating/expanding that pattern to the rest of the data set (as opposed to checking the whole data set), but this isnt a good way to find out if those wave patterns are errors or if errors or error-similar structures like waves vacillating around an average are a default component of reality, which is related to the problem of determining the level of specificity and discreteness to assess incremental changes of integrals and polynomials (waves being a default difference structure), but this problem can be addressed to some degree by applying scalars to magnify the subset to make differences from constants/waves more obvious, checking it for robustness by applying randomness/errors, and other methods of identifying a stable function
            - just like the 'required component of a curve' is a 'subset with count greater than 2', 'subset with count greater than 2' can be used as an 'adjacent input format' to speed up an algorithm to assess non-linearity of a data set regression function, just like other algorithms have input formats that are more optimal for the algorithm than other formats, and 'subset with count 2' dont have as much info embedded that could relate to non-linearity as a 'subset with count greater than 2'
            - related questions to this problem are 'at what interaction level do you zoom out/in to in order to describe the variable interactions adjacently describing the most variation, or otherwise in a useful way (until there is a clear summary line, until the points are obviously differentiable between point types, until the point connections are obviously differentiable between connection types, etc)', as changes in scale and distance from data set can either erase or magnify differences like non-linearity, just like changes in scale/distance from the data set can make similarities such as patterns like lines more obvious
        - finding out 'non-adjacent similarities that are relevant', such as how finding 'non-adjacent inputs with equivalent outputs' is useful bc it implies a 'horizontal line' structure might be relevant, relevant such as 'being a useful base solution function to apply changes to in order to find variation based on that line, or to use as a simplistic summary function', which can be framed as a 'high density output' with importance bc of the repetition of that output in some pattern like an interval indicating cyclical patterns, or like a ratio indicating commonness, either way indicating usefulness of that 'high density output' as a base to apply changes to or use as a simplistic summary function
            - relatedly, vertical lines (indicating an input that can become any value) are so unlikely in a data set that isnt random that they can be ruled out as improbable in most functions except where required or made probable by another route
        - calculating the function from a set of symmetry structures (local subset averages (most similar point or change from most similar change rates to other change rates), a set of inflection points (a change in change structure like charge of change rates), a set of peaks (change in direction of change rate)) or symmetry-limiting structures (extremes of a probable range representing limits) is trivial once a ratio of these are known given some complexity (& other metadata like input ranges) of the function
            - relatedly, another useful problem-solving intent in regression is 'finding asymmetries in peaks which are useful to know about using adjacent info, bc peak symmetry is a useful assumption when true to determine more/all of the function from knowing a subset and relatedly useful to know about when false to identify when implications/similarities are contradicted and not absolute'
            - formatting the average as being more similar to points or being more similar to point connections (or being different from randomness or extreme points) is a useful set of alternatives when finding inputs that are common across useful structures or finding useful formats for algorithms that use averages
    - given that the complexity in regression is caused by non-linear shapes as opposed to constant lines, finding which variable interactions exist (which input variables are exponents) and which cause these complexity structures (exponents of an input variable, constant coefficients of exponents, addition/multiplication with other exponent terms, etc) would enable removing those complexity structures to reveal a simpler form of the data set that is more easily condensed into a regression line or a base solution function to apply changes to (like how removing parabolas that are symmetrical around a clear or probable average is a trivial task that makes the average more obvious), such as applying inputs of complexity structures to the base solution function to specify the more general simpler function
        - this is related to workflows like 'remove variables until a simpler component function emerges'
        - this is also related to workflows like 'find a simpler combination of inputs preceding the original function/data set' which can be applied in another direction such as 'find the function interaction level where maximal differences begin to emerge' such as how combining functions by type (like step functions, wave or other polynomial functions, sequence functions, discrete functions, closed shape functions) can produce functions that represent a 'combination of these maximally different function types' which may be more useful at filtering a function solution set than other interaction levels, so this interaction level of function types may be a more useful place to start when finding a regression function to summarize points
            - relatedly, finding 'sets of useful interface queries that decompose most variation' can be as simple as finding sequences of interface structures like 'find common/powerful/probable/high-variation variable interactions that match common system interactions like common system errors or common differences from incentives/defaults' and 'apply the function type interaction level to decompose the remaining unknown variable interactions, once these common/powerful/probable/high-variation variables are known', sequences which could be generally useful in 'decomposing most variation across problems', which is a useful problem-solving intent
        - relatedly, finding algorithms to connect simplicity and complexity (and independence/dependence and unstable variable/stable constant), generating one from the other (like simple core functions that when repeated generate surprisingly sustainable/cascading and complex differences) are useful to decompose the likely starting point and simple/complex components of a system
            - like the simplicity in the central limit theorem generated from a complex combination of random variables, and the tendency of complex systems to neutralize each other's differences in some way when combined/repeated, for interactivity (as a complex system is less likely to be sustainable on its own when repeated), and the requirement of a variable like a symmetry in a simple function for it to easily generate complexity
    - given the possible set of interaction functions (direct connection, constant connection, side of an area representing a limit/border, error connection, random connection), use these interaction types as a variable to identify algorithms to classify connections between points based on their probability of being one of these interaction types and the line patterns that emerge given some set of interactions of a particular type pattern in some subset of the data set
        - for example, any two points in a data set are very unlikely to have a direct connection, so that should be used rarely
        - if a connection is confirmed or probable to be a direct connection, adjacent connections are less likely to be direct connections and other connections similar to the direct connection in other subsets are likelier to also be direct connections
        - if connections indicate low volatility, finding a connection with an extreme slope implying 'volatility' is less likely after some ratio of connections are tested/determined/assigned a probability of some interaction type
    - a function to find the simplest (or otherwise effective) polynomial to describe averages of local data set subsets (where angle of lower/upper borders and densities influence the average) is an example of a standard method that can be found with adjacent structures
    - changing the definition of useful structures like 'averages' to find alternate methods
    	- defining an average as a 'line that when changed the most compared to other functions, still fits within the boundaries described by upper/lower limits' points directly to a method to determine the average function fulfilling that definition 'find a subset of possible different functions to change, and changes that can be applied to these functions, and apply boundaries as limits to these changes'
    	- defining an average as a 'difference from extremes' or the 'usefulness of right triangles in finding average functions (and vectors applied to them to generate extremes) of a data set' points directly to methods like 'find angles applied to a possible average line that capture the highest variation in a data set, once possible extremes are known'
    - evaluating a function's 'differences from randomness' is another starting point to base changes on rather than basing them on an average bc its the 'opposite of the intended information' and is therefore similarly useful in that adjacency
        - other function bases include functions that 'connect non-adjacent subset averages', that 'connect adjacent subset averages', 'connect function upper/lower ranges', & other representative/summarizing functions
        - a random data set is not useful and is therefore useful to determine early on in calculations, just like function limits and patterns are useful to determine, and randomness may as well be an indicator of falsehood (as in 'something that needs to be changed in order to determine truth, like requirements/impossibilities') bc of this lack of usefulness
        - other known structures that are not useful are equally likely to use as bases for change, like how known useful structures like core components are, bc of their dichotomy in the certainty of their usefulness
        - applying 'common types/variables of functions that can form randomness' (such as 'contradicting/neutralizing change types that cancel each other out' or 'complementary opposite change types (like triangles which form a square)' or 'randomness-amplifying which doesnt change the randomness of the inputs' or 'symmetries like the equivalent weight of dice sides' or 'a high number of variables' or other functions that are likelier than average to create the requirement of randomness as 'even distribution of probable outputs') is useful as a way to 'determine probable randomness' or similarly to 'determine differences from randomness', 'remove/add randomness' and other intents related to randomness
    - building an interface structure of interacting rules to base changes on, like a 'set of requirements' (like how 'connecting components of a structure' is 'required' to 'form component connections to create that structure' by definition) or similarly a 'set of rules that are definitely impossible or not true' as a foundation for other changes (where possibilities exist between contradicting limits imposed by requirements) is a useful structure to start discovering new rules from
        - identifying rules that identify non-adjacent information required by a definition is similarly useful like definitions that identify adjacent/obvious information required by a definition are, like how numbers in a sequence like the set of integers are required to be one unit away from other integers and required to increase if its the set of positive integers, so knowing that a set is sorted in an order like this gives you information about all the numbers between two items in the set bc of the definition determining the set allowing functions like 'estimate where an item will be found to reduce the search space', or how the type of a number determines some of its known functions
    - a standard method to solve problems is framing them in terms of core structures like 'similarities and differences' (like 'similarities to constant representative/average lines', 'similarities to averages', or differences like 'difference-maximizing functions', 'highest angles connecting adjacent/similar subsets as a function that is almost guaranteed to be incorrect to base changes on'), then applying a function to 'determine which differences to resolve' (like 'differences between base functions like averages and alternate similarities like densities' or 'differences from highest angles connecting adjacent subsets and a base average/density-determining function')
        - this method finds the variables likeliest to be known/similar (or easily derived/predicted as adjacent to known variables) constants like 'averages' and 'local subsets (locality as an indicator of similarity)' and 'densities', and variables likeliest to be unknowns like 'alternate more complex functions with more variables' and applies similarities/differences to model those and find which differences are relevant to resolve (the differences that 'connect related similarities/differences', related by providing complementary information for intents like 'represent a data set', complementary information like 'base functions' and 'specifying differences customizing that base')
        - finding a function that models the 'maximally different local subsets of the data set' is a solution-finding method easily produced by this method
        - this is an implementation of the workflow involving 'finding matching structures based on common attributes' to model 'uncertainties within that structure' and connect these structures to problem/solution definitions
        - other differences to resolve could be the 'difference between the set of possible/probable functions and the set of best representative functions' or the 'difference between a set of probable determining variables and a probable representative function' or the 'difference between probable linear representative adjacent local function sets and the non-linear variants that are better representatives'
        - this is related to a structure like 'applying variables to a structure like a "set of maximally different angles" applied as a symmetry to find the angle set that hits (or alternatively/equivalently approaches) the most data points when the angle set is changed the least (like rotated, shifted, scaled, etc)', since a 'line that approaches the direction of or is adjacent to the data point densities' is similarly useful as a 'line that intersects some ratio of points', as a 'representative function' isnt required to intersect with any data points, so that intersection is a variable that can change in the solution-finding method, as well as other properties not required by the definition
        - this formats structures in a way that makes it adjacent to identify variables and sources of variation (like rules like 'sudden constants/similarities that enable other changes to begin by providing a foundation like a barrier/limit for differences are a good way to identify interface variables in systems'), which is why the primary interfaces are useful in the first place (they allow highlighting uncertain differences by applying certain similarities/differences through standardizing/similarizing to fundamental/core unchanging variables like 'cause' as in embedded variables of that variable like 'causal degree' that support/describe/limit other variation and otherwise fulfill intents related to variation the most completely)
            - relatedly, identifying the most powerful variables as the biggest sources of error in a particular format, such as 'causal position' being a source of error in the causal network format (such as how an input might be an output of the output but it could seem like an input in some false similarity errors) and the 'connection function' in the network format (such as how some changes can seem adjacent/probable in one network format but its a coincidence, where some other network format is more reliable at making those connections adjacent)
            - relatedly, the primary interfaces are also useful for being based on the 'reasons' why a structure may be relevant to another structure (it is caused/changed/allowed/required/intended by other structures, it is useful to or interactive with other structures, it is a variation (as in a definition route) of other structures like concepts, etc) which are united on the 'meaning' interface (determining relevance/usefulness of structures to each other)
            - calculating the structures that are not in a data set but which could be relevant based on other structures that filter structure combinations like probability/similarity/commonness is useful as an intent to predict possible real structures that will be found in future data sets (calculating uncertain differences before theyre a problem/before theyre real)
        - this is related to other workflows like 'find maximum differences (what something is not/find the opposite of something to find limits of what it is)' by asking questions like 'what is not cause' (with answers such as 'an event that follows another event is not necessarily a causal sequence bc they may be so indirectly connectible that they are effectively independent') to find useful structures like the limits of the causal interface and how it interacts with other interfaces like meaning (such as how 'events that follow each other in time may be causally separable and arent required to occur in the same system or detectably influence each other')
            - similarly, finding why structures would not interact (such as how 'some structures cant detect/measure other structures, and therefore cant use them as inputs') is useful as a filter of 'meaning' interface logic
    - a unifying function of the various representations of a function where the representations are variants supported by the unifying function is likelier to represent the function the best
        - similarly, a unifying structure that supports various representations (like a 'set of maximally different directions', a 'set of reflective mirrors as polygon sides capable of producing different variants of the same information', a 'network of foundation structures around which maximal changes are supported which can coordinate', a 'set of filters capable of filtering the highest ratio of solution sets with the highest similar degrees of accuracy', a 'set of overlapping shapes with a common center (of common components) that model reality with similar accuracy', a 'set of connections between common high variance-capturing structures like maps/filters/networks') of the interface network (in various perspectives that filter it) is likelier to best represent the interface network
        - finding a useful 'sequence of filters' is useful as a good way to avoid problems of assuming too much & other basic errors of bias, like by applying "possible, known, required, probable, computable, measurable/testable, usable, & realistic" structures early on in the filter sequence
        - similarly, a unifying function of solution metrics (efficiency, accuracy, generalizability, flexibility) is a useful base to apply changes to in order to determine variables of solution-finding methods
    - finding useful structures to combine as defaults is useful, such as how 'symmetries', 'fractals', 'randomness as a limiting counter-structure', and 'right angles' are useful as core structures to describe a high degree of changes bc of their definitions ('applying fractals to changes in the direction of a right angle based on a symmetry up to the limiting point where additional changes appear random in their accuracy at describing change' can describe probable changes around that symmetry) bc of the relations between their definitions ('symmetries' and 'fractals' both having a 'common base (of a "self") for change, and a limit on changes to that base' in common, so applying these in the same structure benefits from their common symmetry in their definition and applies changes to this symmetry in their definitions, and 'fractals' further fulfills other attributes of symmetries like a 'limited change, as fractals converge')
        - similarly other examples like 'why i is a relevant number to rotations' given its adjacent concepts which make this functionality probable or inevitable (not only its allowance by definitions, but also possibly its difference in ability to produce a difference in sign/direction from the origin, its core operation of 'multiplication' being a definition of the components of a type of n-dimensional change, 'multiplication' as relevant to angles through creation of closed cornered shapes, a unit of change that is relevant to rotation, the relevance of sign changes to wave functions which are relevant to circles, etc, which are useful changes for connecting more directly relevant structures to rotations like pi), in its function connecting two other structures of rotation such as e, a structure related to spirals (a structure with a useful equivalence in its change rate ratios of adjacent compounding) and pi, a structure that acts like an origin/symmetry of external/internal spirals (trending toward polynomials in the external direction and trending toward individual points in the other direction) and would be a spiral but is missing the 'change rate increase' to create a different equivalence than a spiral has (a change rate equivalence, that creates the property of 'closedness' in the circle as opposed to the spiral but is sufficiently different from a 'cornered shape' in either the internal/external direction so as to justify its own definition separate from spirals or cornered shapes, despite being equidistant from these and other and unknown structures in their definitions)
    - mapping problems to more defined fields like highly structural creative industries such as 'music' to find concept mappings that are more easily determined like how finding the rule 'intelligent goodness is more difficult than obvious/complicated wrongness or obvious goodness' by applying the clear definitions in music of "obvious rights/wrongs like compliance with major/minor chords/notes or compliance with patterns" and how finding the intelligent goodness requires knowing the obvious errors it avoids like incentives like 'cheap rewards from any difference, even wrong differences' as its easier to create a new minor song than to create a new complicated but good song, as the range of possible solutions is narrower but can still host complexity/variation that intelligence could survive in, and complicated goodness is more complicated than complicated wrongness bc of the additional problem of the limited range that requires creativity to sustain intelligence in, a rule that would help avoid errors of over-simplification, prioritizing any difference, and avoid obvious errors as well as errors that create changes that violate a solution structure like a range of good solutions, and would incentivize finding high-variation variables sooner (to stay within the limited range) than prioritizing wrongness would
    	- finding a corresponding physics version of a useful structure is a good filter to apply when determining useful structures or other structures to apply as defaults bc these are likelier to be functionally useful if not more plausible, realistic, or possible
    	- example metaphor: gravity between 'equivalent alternates' (such as alternative theories) is weak but enough to keep them in the same definition (allows 'aggregation' to occur)
    	- other example metaphors which are useful in the sense of being evocative or otherwise useful for calculating something: 'supersymmetry' and 'interface network', 'string theory' and 'variables as waves, as reality units (possibly related to twistors/spinors)', 'isomorphisms retaining histories/inputs' as some definition of a 'wormhole', the 'jacobi identity' (and other structures of inequality like asymmetries) as an example of a 'way of determining what structures will accrete in some definition of unidirectional time (like to determine what will become matter)', 'commutativity' and 'equivalences in quantum superposition probabilities', 'invariance attributes like associativity/commutativity being a useful structure as a base to apply other changes to, to base other more speculative variables on', 'non-orientability and CFT violation', 'whether representations of reality form reality, to the extent that it can collapse/expand into or be based on other representations as those become more energy efficient, like in a cycle of different representations', etc
    		- related questions: 
    			- are all measurable invariance attributes components of reality or are some of them the basis of some reality and others the basis of another like points on a lattice that can be a foundation of reality, where points in between are not, so travel between them could only involve motion in the transform producing either from the other
    			- which symmetries are foundational and absolute in the math interface? which symmetries (like abstraction as filter or map, connecting similar concepts like energy/variation) connect these absolute math symmetries to physics symmetries? which symmetries allow maximal differences to develop? what symmetries exist in reality (such as symmetries across space-time states preserving potential energy or entropy, so that we only have access to functions within a probable range area governed by that metric)?
    		- the usefulness of these metaphors depends on the variability between 'definitely possible' and 'definitely not possible' (plausible, logical, not definitely impossible, evocative, suggestive/implicative, conditional, etc), so that a tool to 'traverse similarities and apply maximal differences to find structures like requirements, symmetries, concepts, and limits' can benefit from embedded but not articulated useful variables/structures such as connection in language (superset of definitions containing a subset of math definitions)
    	- similarly, other metaphors include how the number-based spaces can be extended to apply to number types (like a 'prime-based space'), how a 'complex number-based space as a possible structure of reality' has a corrollary in the insight that "almost every fact has a counter-fact (similar to paradoxes with local contradictions across statements) where it is related to its equally legitimate opposite (where it's not true), where opposites are allowed by definitions (similar to manifolds that can seem like extremely different objects that contradict each other absolutely but are actually related consistently by some common structure)", where alternate possible spaces may represent/embed other possible insights as their generative/limiting/determining functions, and which may intersect/overlap in a space of these insights (which has a corrollary with quantum field theory), etc
    	- 'time travel' could be possible in the sense that realizing things faster and being able to store/determine/compress more information (which increases the potential time available for the realizer, by slowing down time for them, as they have more functions/variation than other people) can put everyone else in the past relatively by decreasing their relative rate of change, but only while the realizer continues to do so and if they can reverse that change
    	- 'increasing similarity in the sense of synchronicity' by giving everyone the same perspective (like by giving them a maximally different structure to handle problems with) is another function of putting agents in the same space-time or on the same timeline (converging timelines and integrating/connecting spacetimes)
    - finding solution functions for possible known errors to existing methods, like how 'standard neural networks' can have an error of 'finding a different function of different components for each input/output pair' or 'finding overly simple functions that re-use the same (or otherwise simple) components the most but dont handle extreme/new contradictory cases' or 'finding all possible incremental components of some size that could differentiate some outputs and removing some of the less useful/adjacent/common incremental components'
    - identifying interface query intents fulfilled by known useful structures (like how 'cellular automata' and similarly 'standard neural networks' are useful for interface queries such as 'finding maximal differences generatable by one logical/change unit on its own, to identify where a seemingly complex phenomenon can be identified by one variable (the logical unit), thereby reducing the complexity of high variation data sets or the complexity of finding variable interactions between variables of high variation data sets')
    - neural networks may have an error created by the 'sequence of training data', in which earlier training data influences the final solution function disproportionately to its utility value
        - finding the worst case scenario where each training algorithm could miss the most obvious (or otherwise useful) alternate/absolute optimal (such as with gradient descent) bc of the order of training data and how to correct these problems (such as 'start training multiple models at multiple different points and attempt to integrate/converge to an absolute optimal given their change types, like pursuing only those descents where lower values are clearly identified or otherwise where lower values continue to be possible')
        - abstracting this workflow to 'find common variables like "robustness to order changes" that are highly differentiating in math fields/functions, then apply these common variables to neural networks/regression algorithm to check for variation resulting from these variables'
        - allowing variable interactions identified in later training data (like more foundational base symmetries) to replace/change variable interaction structures (like an emergent lesser symmetry already identified in earlier training data) may be more complicated than just applying PDEs to assign changes to specific weights (like by integrating the 'weights of the lesser symmetry' with the 'structures like limits/invalidations of the lesser symmetry allowed/required by the base symmetry' on which it depends, such as by consolidating weights indicating the lesser symmetry variables into one more powerful variable on the base symmetry)
        - having an 'update function' that handles updating the weights in each of these worst-case variable interaction cases (or the most erroneous or most common cases), such as where 'the final training input indicates the base symmetry that is extremely different from the lesser symmetry already identified'
        - the reason interface analysis is so powerful is that it acts like the 'base symmetry (or hyper/metagraph) of all base symmetries (or hyper/metagraphs)', so having a neural network that tests interface analysis variables as a prioritized structure and integrates them into update functions is more powerful than other neural networks
    - 'minimize extreme errors of a solution function' and 'maximize data set coverage of a solution function' are both 'alternative contradictory' intents to solve the 'find a regression function' problem, which offer some degree of 'complementary info' rather than 'definitely overlapping or equivalent info', as the 'extreme errors like missing an entire variable or missing an outlier' and the 'maximum data set coverage' have no guaranteed overlap, as a 'maximum data set coverage' could easily exclude extreme values or other sources of extreme error like variables which are easily missed if some subset is selected
    - an example of specific simplifications to solving prediction problems can involve avoiding known suboptimals/errors/violations of requirements by identifying & applying differences to those and allowing all other variation to develop, such as predicting a subset of just the worst case scenarios by applying extremes to increasingly high variation variables (technological development in some direction like electricity/automation/chemical printing/speed/compression/computation) and identifying opposing variables (particle accelerators/generators which can modify components of or interactors with electricity, software likely to be used with electricity, & quantum technological development which can modify components of electricity can oppose a possible error possible with electricity technology in an extreme such as in a concentration of one position, 'if energy technology or energy technology variation (innovation) was concentrated in one position by some entity, what could oppose/change it'), and otherwise modifying high variation variables ('what are all the worst case combinations of high variation variables with other high variation variables like quantum computers and neural networks'), as if variables dont cause high variation, they can sometimes be ignored in some cases until they indicate change in the direction of causing high variation, as knowing the 'structures like patterns of variable development such as patterns of large-scale errors' is more useful than solving for every possible variation interaction in cases like with computation limits, similar to how 'limit change patterns' are also useful in decomposing all variation
        - this can generate possible innovation intents like 'encrypt physical molecules so they cant be read', 'check if enough particle changes occur in a sufficiently continuous area, whether different types/configurations of gravity can emerge', 'inject variables in reality in a way that creates gravity, if uncertainties attract gravity to require interactivity to develop to handle the increase in variation without destroying the foundational structure (create a structure like a black hole, an isolated uncertainty cascade)', 'check if there are different units of spacetime like an opposing fact/falsehood pair or a pair of space-time states connected in a sequence or the interim structure of other suggested units like a wave/particle', 'speed up information travel/acceleration technology so farther information can be read and used in predictions more quickly (such as by directing radiation/randomness wherever its not visibly/directly reflected back at us by hitting a structure, so that it hits something we cannot measure and therefore is likelier to encounter maximal differences if they exist, which are likely to respond back as intelligence sources and improve our rate of technological advancement, or similarly direct radiation at inputs to these sources which is likelier to be findable and is similarly likely to get a response as well as likelier to incentivize organization/connection and lead to an increase in the power of radiation to cover distances as those will be reduced by this connection and therefore increase its speed), or similarly direct radiation in known stable ways that create maximal differences here to attract other maximal differences as intelligence sources who require differences to solve their problems' and therefore the direction that changes (innovation) may be applied to fulfill those intents, as measurable intents are likelier to be focused on & fulfilled, and relatedly 'horizontal innovation' can also be incentivized to connect these innovations in different directions on a new interaction layer
        - conceptual math is particularly useful here (not just 'add a concept to another in a simple combination/set structure') but entailing all of the possible useful interactions between concepts
            - if you can run queries like 'build a new (not already known) possible interaction type between these concepts which could be true' and 'what are the probable error and limits of the interactions between these concepts' and 'which concepts are more interactive with these concepts' and 'which concepts reduce the interactions of these concepts', you are applying conceptual math, as opposed to just stringing words together in a simple combination/set
            - 'what are the possible interactions of interface variables, such as some areas of powerful developing technologies (like batteries, quantum computers, encryption, and 3-d printers)'
    
    - identifying the few 'specific problems to solve, which can be determining of all other structures' is useful for reducing the required computations to apply problem-solving methods to, such as how identifying some variable interactions is more useful (like eigenvectors/eigenvalues, or energy/entropy/potential) than other variable interactions, which can be optimized in their usefulness by identifying how problems are related so once these variable interactions are found, the other problems are adjacently solved (identifying the 'problem network' that will fall to these variable interaction functions)
    
    - identifying useful structures (like 'math structures' such as 'independence') which act as proxies of other useful structures (like the 'maximally different structure')
        - for example, finding the 'most independent variables' is useful for intents like 'creating randomness', 'identifying organization', 'identifying interactive variables', 'identifying non/correlating variables', as 'independent variables' are useful as possible interactive components of systems (as opposed to the self-interactions of dependent variables which are by definition connected), as this intent is more mathematical and measurable given the more well-defined definition of 'independence' compared to other structures (like the 'most complex structure' or the 'most different structure' which may not specify how to determine differences, but can be more usefully framed as being the most consistent structure creatable with independent variables), as 'independent variables' by definition come from 'different systems' where they can be isolated, and finding the most independent structures which involve the most variables from the most different systems is likely to be useful in generating related structure like the maximally different structure
        - this is another example of why 'cross-interface structures' are more useful than other structures, such as when a specific math structure is more useful for calculations so knowing concept-math maps is useful for that intent
    
    - identifying interim structures between defined structures is useful where existing definitions dont fulfill intents optimally
        - identifying 'complementary filter sets' is useful as they are an information format that can be adjacently connected to the interface definition, as a set of filters that captures complementary information can offer a more complete ratio of information capture than other structures, and filters are adjacently connectible to interfaces in that they highlight specific differences within a similarity limit
        - similarly the 'system layer diagram' is useful in the same way, as being adjacently connectible to the interface definition (the set of component interaction layers describe a system with sufficient isolation and interactivity to capture high variation and fulfill problem-solving intents like 'build/break' such as when applied to problem structures like 'break a problem into sub-problems' or 'build a solution')
        - both of these structures are useful for very different intents
        	- for example, the system layer diagram is also useful for indicating overlapping set memberships and can adjacently generated other useful diagrams like venn diagrams, and occurs in math contexts like 'lie group & quaternion projections'
        	- 'complementary filter sets' can also identify 'complementary variables that frequently co-occur in systems'

    - identifying functions in between error functions and solution functions has some known structures to help determine the interim space, which can be applied as default bases to change to determine interim functions which are more useful than either extreme
        - for example, the functions that produce errors every time except in some specific known/derivable condition such as 'when the solution is given to these functions directly as an input' (identity functions, simple functions) are known to be sub-optimal for most intents, and the functions that are optimal in some way similarly have structures which differentiate them from these known sub-optimal functions (though they are high-cost in some way as well, such as 'check every possibility'), and in between are functions similar/different to both which can be derived
        - specific problem format (benefit/cost problem format) difference-resolution rules like 'differences between cost-handling demand and cost-handler supply' like 'dont centralize costs/dependencies on one cost-handler/independent function, which is known to destroy the value/structure of that independent structure' are similarly known sub-optimal functions which can be altered to produce more probably optimal functions ('instead apply a different structure such as, distribute costs with a non-centralized method such as distributing them evenly')
            
    - identifying structures that are useful in the absence of some information that is probable to be missing across problems
        - when 'maps/fields of similarly sub/optimal solutions' arent available, which simplifies the problem of finding new/optimal solutions, bc simple position changes in some known optimal (or known different-from-suboptimal) direction can produce a better solution, what other variables can be applied instead of variables in that structure
            - apply difference/diversification structures (try multiple maximally different solutions and apply adjacent changes to those until some subsets are filtered out)
        - when a data set isnt graphable bc of its complexity/size, which variables can be applied to determine/visualize it
            - apply standardization, to identify highly differentiating variables on highly similarizing variables to determine/visualize the differences from how these 'differences in similarities' differ from other known structures of 'differences in similarities')
            - identify truth structures, as in some subset/variable of it that probably retains/stores a high degree of variation and determining/visualizing that instead
        - relatedly, what does it mean to have a condition like 'not having a map of similarly sub/optimal solutions' (meaning 'how does it relate to problem/solution structures like available resources/inputs or errors & the solutions to those errors')
            - it means either one of the following
                - the input variables are incomplete/not the correct variables/otherwise missing (the solution involves finding/deriving/generating new info, formatting variables, etc)
                - that the solution field is volatile, so traversing it is not optimally useful (the solution involves de-volatilizing a function, so that adjacent input changes create adjacent output changes)

    - identifying specific maps to connect known useful interface structures
        - for example, knowing which variables (like an extreme difference in outputs of adjacent input) produce which attributes (descriptive attributes like volatility) is useful for mapping these structures/intents
        - similarly, knowing functions which oppose/offset this attribute (like a function to make a graph un-volatile) can be checked for in the data set to predict that attribute in the output

    - identifying common variables in the most useful structures, as a useful intent that can fulfill other problem-solving intents adjacently
        - for example, the most valuable structures are 'cross-interface structures', 'simple connections between complex structures', 'maximally similarity-differentiating or difference-similarizing structures', and other structures which are unique in that they connect highly different/complex structures using adjacent functions
        - another common factor between these is that they reduce dichotomies to an interim structure (the 'most complex simplicity', the 'most uncertain certainty', the 'most variable constant or most constant variable', the 'most different similarity', and other extremes testing the limits of the core definition at the root of these dichotomies) that is more powerful through involving these extreme differences in the same structure, and offering an interim point at which to base changes on to generate the other points
        - identifying the 'points between other useful structures' that can act like averages/symmetries to generate the other useful structures is another useful intent
            - this is related to finding an interim/average/origin that is useful in generating other points trivially, such as with applying a rotation or applying a vector of the same length
        - identifying the connections between useful structures (like 'different intents fulfilled by the same function' or 'all the different ways to use a function' and 'different functions that can fulfill the same intent' or 'all the different ways to connect some structures of the intent') that make some useful intent trivial, connections such as 'similar functions' and 'similar intents' and 'similar functions/intents', where these similarities (yet to be identified, such as similarity in 'information-preservation' intent fulfillment of a function, or other useful metrics like 'specificity of the intent resulting in a highly filtered possible solution function space') offer alternate connection components to form routes to fulfill some intent, as opposed to finding a route in some default/standard space like connecting input/output points
    - identifying the core variables of physics/math can be used to identify other primary interfaces
        - for example, the 'lagrangian' (to measure potential, such as 'adjacent possibilities', or 'all possibilities'), jacobian or relatedly commutativity (to identify sequences/order), lie algebra (to identify symmetries), and other default math/physics structures can be used to identify other primary interfaces and useful interface structures
        - similarly, the concept of 'gauge-fixing' (as a 'disambiguation' structure through 'difference from some embedded symmetry, like a zero/non-zero sloped line connecting rotational states') is related to useful structures in the 'regression' problem space, such as insights/rules like that 'any line crossing a general data set area can be useful in determining the actual average line (reducing the set of functions that could be equally valid, acting like a filter of possible functions)'
        - 'non-local causality' and 'quantum entanglement' (connected non-local nodes, connected by some function like equivalence, to account for interface structures like 'false structural similarities' that occur in real systems but arent semantically relevant (the reason for the similarity is different and independent)) are examples of physics structures that can be injected into neural network algorithms
        - similarly, the concept of 'diffusion/dispersion' comes with the concept of a 'source point' which leads to the other points, which is useful as an alternative to an 'average/interim/hub' point to apply the concept of 'state' to a data set
            - relatedly, the concept of 'heat' corresponds to the concepts of 'potential energy', 'entropy', and 'interactivity'
        - similarly, the math structure of an 'affinity' which 'preserves some similarity metric (like parallelism but not necessarily distances/angles)', or an 'isometry' which 'preserves some similarity metric (like distance between structures)' is useful as a 'variation' of the 'structure of an interface', an interface being that which reduces (rather than preserves) the distance between some similar structures to connect, to make some intents trivial (such as 'connect differences' and 'differentiate similarities'), and relatedly differentiates some other structures to differ (by identifying structures like symmetries and applying those as default interaction objects)
        - similarly, the math structure of a 'contraction mapping' that reduces distance between all points is related to interface analysis, in that its default structures reduce the work a query has to do to connect problem inputs and solution outputs, as it makes all useful structures adjacent to connect, so that trivial queries on the interface network solve problems formatted as differences on the interface
        - similarly, the concept of a 'topology that can be changed to make any of its structures adjacent without altering its properties (like storing maximal differences)' is a useful math concept that can provide structure to filter the searched set of possible solution structures when searching for the 'maximally different structure'
        - the 'stress-energy tensor' of physics is analogous to the intent/function cross-interface structure (the problem/intent/usage of a function and its functionality, as in its ability to respond to the problem represented by the intent)
            - relatedly, the derivation of the 'stress-energy tensor' indicates a sequence of multiple expansions, substitutions with equivalences, groupings, change rates, and orderings, as well as standardization by a similarity in common (coefficient) to reveal an irreducibility/isolatability/component (energy density of the system) or a target defined structure (the 'divergence'), which is related to the core functions which are re-used across problems, changing core variables like order/set/terms/complexity with a set of common operations to produce most results adjacently, and which follows a general 'expand/generate and filter' solution automation workflow
            - relatedly, the mathematical 'group' is analogous to the 'structure-function cross-interface structure' as useful for forming other function/structure pairs or the 'function (as a unit of change, analogous to a variable)-constant dichotomy' that is so useful across problems to produce other un/certainty pairs
                - relatedly, the idea of finding the 'interim' (or other equivalence like an 'average') point between two structures that makes both of them trivially generatable with the same operation applied to the interim point as a useful compression structure of both points
                - relatedly, the 'algebraic structure' that is a 'set/operation/axioms', as a variant of the highly useful default 'generate/filter' problem-solving workflow
                - relatedly, 'lattices' as a structure to identify paths between points using the same vector as a unit of change
        - the 'frame of reference' as an 'observer and a coordinate system' in physics is related to the interface analysis concept of a 'perspective' (as a filter with priorities that highlight/clarify different structures)
        - similarly, the 'momentum' variable shows up in physics frequently and indicates the 'incentive' variable that determines many interactions
        - similarly, the 'frequency' variable shows up in physics frequently and indicates the 'commonness' attribute that can lead to 'probable structures'
            - relatedly, the reason for using the 'wave function' as a unit of polynomial functions and using it as a unit of spacetime, as a high variation structure with a symmetry and a high variation interim interactive structure, respectively
        - similarly, the 'p-adic numbers' offer a mathematical variant of the useful structure of a 'similarizing difference' by applying a similarity to the difference between numbers, changing which numbers are adjacent (highly different numbers rather than the standard of highly similar numbers, by applying a different similarity 'extremity of the high value of the power')
        - similarly, the 'imaginary numbers' of math reflect truths of reality like 'every true statement has an opposing statement that is true to some degree or in some way or in some context' and physics structures like matter/anti-matter as well as the importance & commonness of dichotomies and spectrums as important/powerful variables, which is an adjacent transform, once that insight is known & applied and those structures like dichotomies are known, just like a 'ghost of a definition' which retains the bare minimum 'skeleton' required for the definition to still be true, but is still so different bc of the variation allowed within the definition that it seems to contradict the definition, these overlapping/integrated definition 'ghosts' determining the potential and 'probabilities of resolution (into a structure/state)' of a definition
            - relevantly, applying 'alignments' to preserve connections (like an 'opposite definitive' space of 2-d euclidean space to graph imaginary values, or a 'system' space where both values are definable/graphable but the 'opposite' structure of their connection is maintained, such as that they move in opposite directions, such as an 'opposite of space-time' where the definitions still hold so that 2-d imaginary structures can be applied to physical structures described with the opposite of imaginary numbers, as opposed to using dotted lines to indicate imaginary variables, like an 'inverse space with a negative component indicating the imaginary component, in the position of an exponent coefficient, or a standard vector space indicating change direction components, or a space preserving the positions/connections between roots of real-valued variant transforms like even exponents, with the odd exponents positioned by definition in between, while mapping the powered spaces into the unit space')
                - an 'inverse' originally framed as a possibly useful opposite structure (an opposite as in a orthogonal root or an inverse) is particularly interesting given the 'inverse of x' whose integral area equal to 1 identifies e, which together with x, acts similar to e^i x pi in euler's equation as a factor of -1, as if an 'inverse/e taken to a power of (root-type/i x rotation-type/pi)', given that when multiplied by itself i x pi times it generates -1 (i squared), which isnt quite the case (these arent directly mappable to the concepts of inverse/root/rotation)
                    - but this inverse structure is still related to i (however its only when e is in the position of the base multiplied by itself i x pi times, that i and pi act similarly as i squared, and i is the cofactor of the power rather than the base, and the function is not a square of equivalent factors, but the factors are alternate factors of -1 than i)
                - why look for the 'area equal to 1' (in relation to 1, as in above 1) in a function like the inverse function at all?
                    - finding equivalences, especially in units like 'unit areas (the unit square having side length 1)', is useful as a comparison metric between functions, to allow other intents like 'converting/mapping between functions'
                    - the 'unit area of side length 1' mapped to other functions is by definition related to the unit exponent (the square of 1 and -1, produced by i and 1 respectively)
                    - so you might look for the 'unit area equal to 1' in a function (like the inverse function) bc you know that the unit exponent (one squared, negative one squared) is related to i, and you know units are important, and the 'unit exponent' is a core/important unit
                - why look at the 'inverse function 1/x' at all when examining i
                    - x and y represent different sets of 'factors of 1' in that function, which are related to i's opposite unit (1)
                - what other differences should be examined
                    - the mapping to unit areas of a four-sided shape with all curved edges could be relevant as well, as an interim structure between a square, the area under a function like the inverse, and a circle, and how these structures are related to other interim structures like spirals (which could be useful for related intents like 'identifying change on the outer loop according to a base change of the inner loop')
                    - finding the non-obvious sums that generate i * pi which could be equal and interesting (meaning adjacent/relevant to other intents) as alternate square roots of i when positioned as powers of e
                        - what sets of values of e^x create a unit area of 1 when multiplied and other related constants, and how do these values and the powers of e that created them interact with other relevant structures to euler's equation
                    - finding alternate operation sets that generate useful constants (what else other than roots/opposites/rotations/inverses can be combined with simple operations like exponents/coefficients to generate useful constants like i/pi/e, such as waves vacillating between i/1 or the unit circle)
                - the question is 'what combinations of e and pi can act like i' which can be translated to 'what combinations of alternate unit areas in the inverse function (e) and the rotation constant (pi) can produce an opposite type such as the opposite of the unit root 1 (i)' (why do these 'inverse/rotation' difference types from e and pi produce the 'opposite difference type' that i adds) and 'in what structure/position do these constants act like i (deriving eulers equation)'
                - this is another example of how an 'evocative' structure such as an 'inverse' that is evocative of i (that can not be defined as directly equivalent to some relevant term) can still be useful (can be related another way, like an alternate function/format that identifies the same connection function/variable), similar to how 'circles' are evocative of 'primes' given their false appearance of randomness
                - what does it mean to say that e has i dimensions of itself (e is multiplied by itself i times) and e has pi dimensions of itself (e is multiplied by itself pi times)
                    - related question: is there an interim value between i and pi definitively that makes the equation an alternate root of -1? (e multiplied by itself x times where x is definitely in between pi and i)
            - relatedly, questions like 'what are the types of time in between imaginary and real-valued time' and 'what are the limits of imaginary time in supporting a higher proportion of structures within the time definition than is implied by current understanding of definitions' and 'what connections are required/possible between different types of time and what interaction levels use these types of time as defaults' and 'what variables/functions determine/maintain/require the interface between reality (stable/constant structures emerging from quantum physics interactions) and potential (quantum physics components), is reality similar to seeing a cross-section of a wave that is at a interval where perception can occur and the interval points can be connected and change can be synchronized across those point connections, where other realities are not on the detectable spectrum at that cross-section without making inferences by calculating gradients to adjacent detectable points' are useful as evocative thought experiments and interface queries to find useful structures
        - similarly, the use of 'light' as a metaphor for 'varition' is useful for determining core variables of change (like angles of possible motion in a sequence) to determine valid and realistic methods of inference (like 'scan an area created by some angle of change from this sequential pattern, according to how light reflects information at angles')
            - relatedly, the concept of 'color' is created by 'differences in configurations of points/components in a set' as opposed to just 'simple structures like counts of points/components in a set' which allow different wavelengths to pass through the set without hitting a point and different wavelengths to hit the points of a set, which can be applied to statistical regression in structures such as 'finding which simpler wave functions pass through a subset and hit a subset, indicating information about the wave functions (or variants of them) that reflect info about the whole set', which can be applied to 'preceding sets of former versions of inputs' rather than just 'subsets of inputs', like a reverse fourier analysis
            - similarly, finding other variables like inputs/outputs of light (that reflect information such as 'input/component patterns/connections', such as shadows/angles/reflectivity) is useful for finding information about a data set in statistical regression
            - similarly, finding info-reflecting variables (as in 'info outputs/signals' like 'shadows') to look for when some other useful structure has been identified (like a 'compounding sequence' or a 'contradictory/negating/neutralizing sequence') is useful for identifying the probable identity of the final solution function using shadows of preceding/input functions in the sequence (as in, 'given some output/shadow earlier in the input function sequence, what info is derivable about the shadow of the final solution function, and what does the shadow indicate about the final solution function')
            - relatedly, finding 'similarities/commonalities' in possible solutions/outputs such as the 'shadow shapes that can be generated by extremely different inputs' as well as finding the 'outputs/signals of differences in inputs' like 'changes in the shadow shape when some trivial function is applied' and finding those functions which highlight these important differences to identify, given the extremity of the difference in their inputs, are related useful structures
            	- for example, as mentioned elsewhere, its important to differentiate a parabola and a wave as they can look locally similar, and similarly its important to find other volatile similarities as well as functions to differentiate them (produce the changes that make the difference obvious/detectable)
                - as another example, a 'superconductor' is a metaphor for useful structures like interfaces, which add high value output (electricity) at slight/cheap inputs (adjacent changes), as the structure of the superconductor itself reflects a high variation model of reality that is already useful without inputs at connecting variables
        - similarly, the rules of brain structure interactions, which reflect interface structures such as truths (like 'differences in similarities' as 'random electrical activity in hyper-connected component brain regions')
        	- https://science.slashdot.org/story/23/03/21/2219254/psychedelic-brew-ayahuascas-profound-impact-revealed-in-brain-scans
        - similarly, the related concepts of 'independence/orthogonality/non-intersectiveness' in math is analogous to useful interface structures like 'maximal differences' (such as differences in 'maximal difference-uniting symmetries' like cross-interface structures like 'cross-system similarities')
        - similarly, the structure of 'quaternions' is a useful analogy to a 'unit interface structure' in that it encapsulates a rotation and an axis of rotation, and also applies a 'maximal difference possible within a definition (of a real value) without breaking the definition' as a useful structure to fulfill intents like 'create opposites/differences'
        - similarly, the manifold is a useful corrollary to a 'type' or 'definition', in the sense that items belonging to the type or qualifying as the definition can vary within a set of limits maintaining the manifold structure
        - similarly, the concept of an 'exponent' (self-similarity) is related to 'randomness' (in that a square allows more randomness if its the shape of a data set as any connection between low/high x-values is equally possible) and both are also related to 'symmetries' (in that the square has four symmetries where a straight line has one, and that randomness aligns with symmetries in that symmetries act like equivalences under some change), which you could predict from the 'self-similarity' attribute of the exponent
            - relatedly infinities could act like a symmetry in that if there was an infinity in physical reality, it would be invariant to change (taking one item away from the infinite set wouldnt change its 'infinity' attribute)
        - similarly, the 'density' of a black hole is a corrollary to a 'density' of a data set in that both are 'powerful' (the black hole is powerful through being a source of energy, and the density storing information is powerful to the extent that it influences the prediction function)
            - this is an adjacent sequence once both structures are standardized to the 'information interface' (the 'density' is a significant attribute of a black hole to focus on, bc of how the black hole interacts with 'information', similar to how the density of a data set interacts with information by storing/representing information through its center/average)
            - this indicates if you had an equation like the following, you could derive x using interface analysis to derive 'density of information' and 'power of information' as a useful connecting structure allowing the equivalence/similarity indicated by '=' and identify 'some energy type (like dark energy)' as the x-value
                black hole/x = density of a data set/power (in relevant problems like prediction or determining the rest of the data set)
            - this is a task that transformers should be very good at, given their ability to map systems to other systems, if applied to interfaces as the systems to map
        - similarly, variables such as 'heat' connect different interaction levels (as a 'primary change-determining and change-generating variable' of chemicals, connected to a 'primary exchange unit and primary input' of cells) so these variables connecting different interaction levels are valuable to identify as connections to new interaction levels
        - a universe that requires components to exist also requires a combination function and combinable structures, and one that doesnt require 'uniqueness' allows for 'repetition' and therefore 'quantity' to be measured and 'a number set to differentiate quantities'
        - the idea of a 'particle/frequency of cause' could be the wrong structure to look for, as 'cause' emerges from constants that allow the following, meaning that a coordination between many particles and input/system conditions is required for the concept of 'cause' to be allowed and emerge
        	- change to occur in a 'input/output sequence' that can co-exist with other sequences (occupying and validating the same timeline, and also creating it as a component of the timeline)
        	- interactions between structures, as opposed to requiring isolation of structures
        	- interactions/changes that dont invalidate a high ratio of other structures by default
        	- inefficiency of structures in handling variation injections, needing to move or disintegrate in response to variation like collisions, rather than having stronger forces maintaining their structure, which is an output of energy transfer
        	- however cause could be said to occur at a 'frequency' in that it requires two states to be connectible by some distance across which information/structure can be preserved, and similarly in the sense that a structure is a cause of other structures if its maintained long enough to impact other structures, a structure could be said to be 'causative'
        	- 'causal erasure' relatedly occurs under conditions where many equivalent alternate common structures lead to the same effect, as opposed to a direct identifiable unique causal connection, and similarly erasure of other attributes of cause like 'inevitability' can remove the concept of cause
        	- relatedly, is there a structure such as a 'default network of entanglements (default synchronized structures)' that ensures primary interface concepts like 'information', 'cause', 'change', and 'potential' can continue existing, which creates new entanglement connections if some are disrupted, and is reality the network or set of overlaps connecting these default networks of entanglements, and can these entanglement networks be used to distribute variation/energy evenly (using a randomness entanglement network) so that spacetime curvature doesnt occur and spacetime is experienced similarly for all observers, or would the universe move too fast in some direction once observers are moving at constant speeds for that to be useful or would it distribute change across the universe so that there is no motion of the universe as its neutralized by the evenness of the distribution
        	- more likely than 'absolute retrocausality' is the idea of a 'mass of energy (acting like a large body of fluid) that may overlap/flow into some future state in some condition temporarily (like an occasional errant wave flowing farther out than the others), but will reverse course to be centered in its prior primary stable state, so that this specific future flow seems like an orphan leaf rather than a new timeline, as the return to the primary body of fluid is the stable state, and the future state wasnt connectible/interactive with other states, so time didnt continue to flow in that direction, even though that future state may regularly happen, but not often enough or probably enough to be really real in the sense of stable/connectible time flows, and this future state may or may not impact the primary body of fluid (it may cause itself to re-occur by temporarily flowing back into the primary body of fluid rather than farther away from it, or it may not have a measurable impact on the primary body of fluid)'
        - similarly, 'potential' emerges from constants that 'allow some changes/interactions to occur within a limit, allowing organization to occur' rather than 'preventing any change/interaction' and also 'prevent all changes/interactions from occurring in a way that leads to chaos' and similarly 'allow change types or change inputs to be stored rather than used'
        - why is it possible to make predictions using something other than 'input/output sequences'? bc there are alternative equivalent structures (requirements, networks, probability/potential fields) which are non-sequential and which are useful structures (stable structures of reality)
            - "is there always a 'mirror/symmetry' to reflect across every symmetry (is there always a 'mirror' to reflect past/future states of a system, given some distance to reflect information across)" is another useful question
    - identifying function sets to identify similar structures that can generate useful structures in an alternative way, such as how some sequences can generate a 'change rate that changes every time' like the fibonacci sequence, which can look like and function similar to other types of change like simple exponential change, using a different input structure, as a way to fulfill useful problem-solving intents like identify different possible inputs
        - fulfilling a limited set of problem-solving intents is possible with specific functions which are alternately called 'interim functions'
    - identifying the possible usefulness of alternate function formats (like a 'unifying parameter' of 'maximally different sequences (different for each set of x-values such as 'adjacent x-value pairs') that when summed, converge to the y-value' which would be useful to connect systems of variables that could make every set of x-values justified in having its own function, and in applying a different function format of a sum of a sequence as the sequence of change combinations that are adjacently computed which can be used to calculate the output sum that is the y-value), formatting the problem of regression as a 'solving a system of equations' problem, where the parameter to solve for generates the sequences, and where maximally different sequences indicate different possible systems that the variables could interact with, which could all be equivalent alternates in the sense of generating the same prediction function, which mostly only makes sense when using a set of x-values with y-values to connect in a particular function so that the sequences are relevant by default
        - neural networks should apply changes as 'variables/functions that are actually encountered in the real world', as a variable is applied within a system, as opposed to the approximated variant or the over-simplified or over-deconstructed variant
    - identifying 'points of relevance' where functions can be injected in a useful way to optimize a structure, such as 'during any given training iteration, applying a function like "checking whether changes are moving in the direction of the target output to filter less probably successful change sequences earlier in the training iterations" is useful to avoid computations that are probably less useful to invest in' using rules like 'where are processes repeated or otherwise inefficient which can be reduced in some way to fulfill some relevant solution metric like "reducing required steps" (as in not maximally different structures)'
    - identifying the 'path from math to physical reality' will probably depend on identifying all of the 'maximally useful structure' (on the 'meaning' interface), as physical reality indicates the 'structures which are more computable/adjacent/efficient/stable/sustainable/measurable/independent', and useful structures (useful across problems and interfaces) are the most stable/efficient/adjacent structures
        - I imagine the 'maximally different structure connecting problem/difference types' is going to become necessary to connect math with physical reality, whether as a unit of reality/time or in adjacently connecting other structures or some other fundamental structure
        - a useful depiction of 'time' occurring on this 'maximally different problem/difference type connection structure' is changes to the queries run on that structure, queries which connect increasingly more variables of it, give it additional structures describing it like symmetries/rotations, or create new differences to integrate with the structure (if possible once the maximally different structure is identified)
    - 'solution automation workflows' can follow rules governing selection of structures like 'spaces' to position them in structures like 'sequences', so for instance by the time a query starting on the 'interface network' and moving to the 'math interface network' to a 'causal/neural network' gets to 'finding polynomials in euclidean space', it should be heavily filtered so that the extremely high ratio of 'possible functions allowed' relative to 'probable solution function variation' in that final space are mostly filtered by the point the workflow arrives there in the sequence (as in, a 'probable function range area' or 'probable primary function components' are identified by that point), or otherwise progresses from a space with more possibilities to fewer possibilities if no such filters are applied, so that the space itself can act like a filter on the possible solution functions
    - a function to 'find all different known variable interactions' and a function to 'generate & check new variable interactions (generating new possible interactions and finding a system that seems to be modeled by those new interactions) to fill in the gaps left by known interactions' can be a useful function set on its own as most variable interactions follow common patterns like 'interval interactions', 'cyclical interactions', etc
    	- as another example, functions to derive variable interactions based on insights (like insights interfaces/symmetries, which could be applied as a function to identify the rules of 'embedding variables' as that is a required variable interaction which can be used to frame all variable interactions (how many variables can be embedded in another interface variable, of what type and how can they interact without violating an interface variable theyre based on and depend on to exist and change, in what real systems))
    	- applying symmetries as a 'magnet for change' can help model a system's handling of events like 'interface overload', 'variation injections', 'definition violations', etc (when one symmetry cant handle a change type, meaning it violates the symmetry, what happens in the system having that symmetry, does that variable always obey another symmetry or form its own or decay)
    	- a set of 'known physics functions' is another useful function set, which can be used as a core function set to connect all variable interactions to (a function to connect all variable interactions to physics laws would be useful and independent of other function sets in solving problems, for example 'genetic variables' would be related to 'collision physics and cell pressure/charge rules')
    - identifying useful structures like 'mirrors' that act like metaphors to evoke other useful structures as a way of representing info structures like 'filters', such as how mirrors offer information without having access to all information (like if positioned at an angle), similar to how some info filters reveal info about an entire number type, allowing computations to be skipped, or allowing a program to 'look ahead'
        - these cross-interface structures can be maximized in differences like angles (similar to a staircase or helix shape across stacked interfaces), creating maximally different structures to use as a base for other algorithms requiring changes
        - relatedly, maximally different structures are useful for identifying extremely different ways to frame/format the same variables, such as how 'differences from randomness', 'interim points in between dichotomies like complexity/simplicity', 'adjacencies', 'simplicities', 'embeddings', 'densities', 'curvature', 'probable function area ranges', 'filters of equivalently accurate/possible functions', and 'linear/exponential change type filters' are very different structures which are equivalent alternates (or possibly complementary in providing different info when applied with other different formats) in their usefulness for representing a data set as a function, these structures being non-obvious/trivial to generate the full set of, and optimally useful once generated
    - identifying useful connections and other structures between useful structures
        - for example, the connection between 'filters' and 'interfaces' is useful bc filters are extremely useful when they 'preserve some variation of the input' and when they 'magnify (similarize/differentiate) some variable' to make some similarity/difference more obvious, as this is an implementation of the certainty/uncertainty interface that is so powerful in problem-solving
        - similarly, from this it is adjacently derivable that the concept of an 'interface' has an opposite in that the 'interface' applies a standard format to similarize some variable, to make differences obvious (to identify/differentiate structures), where an opposite of an interface would differentiate some variable to make similarities obvious (to equate/connect structures), which is a new connection between these structures that is useful and can be included in the definition of each interface so it can handle these alternate intents (like to 'make some structures similar to structures on some other interface' to fulfill intents like 'connect interfaces')
    - finding resolution functions for commonly useful connections/transforms, like connecting the 'densities to sparsities' or 'density patterns across densities' or 'densities to extremes' or the 'edge points to an edge line' or 'upper/lower/average edge lines' or the 'densities to regression lines' or the 'shapes (like graphs) formed by densities to regression lines' or the 'local subsets represented/connected and the subsets skipped/unconnected' and other useful connections in the data set regression problem space
        - other useful structure examples to apply as defaults given their higher probability for various reasons (adjacency to requirement, commonness, etc)
            - identifying non-useful structures to filter the set of useful structures
                - for example, identifying what slopes are unlikely to describe a function (making extremely distant points falsely adjacent and vice versa, to determine the slopes that are unlikely to describe the function change rates)
                - 'connections between a sparse subset of maximally different points (in their original positions)' is also a useful structure for intents like 'determining general maxima/minima of a function'
            - identifying points in between extreme errors/suboptimalities as the interim point that is more useful for more general intents while being generally sub-optimal as it doesnt specialize in optimization metrics
                - for example, identifying algorithms with variables that allow 'complementary' best-case input scenarios that are optimized by various algorithms to maximize coverage of input cases in the combined algorithm using these algorithms as components/alternates given some input range
                - for example, gradient descent is optimal in cases where local minima are good approximation of absolute minima and where the whole function is infeasible or inefficient to check
                - in cases where they are not good approximations of absolute minima, or where there are no minima, other algorithms would be better to find other sub-optimals that are less sub-optimal in those cases (like 'maximally different input-finding functions')
            - identifying compressed input info (like the 'output variables') which are more useful to identify than 'adjacent variables to input variables' for determining the outputs
                - the output info contains patterns and other interface structures which are useful independently of inputs in some cases, which can allow skipping connecting them to inputs if the outputs are compliant enough with patterns to be approximated by other functions like probability distributions (such as where the value is usually within some trivial difference from an average output, so this average can be used as a probable output, or where the outputs stay within a range and are relatively equally distributed in a range, which indicates a wave among other possible function shapes)
            - identifying useful structures like 'reasons why some interface structure is useful across multiple useful solution metrics, making it likelier to be robust to changes' such as how 'curvature' relates to 'self-interactivity' which is a core efficiency structure (that generates different change types with one input and one operation) as well as offering 'one cohesive unifying function of disparate sets of linear functions (such as local subset linear functions)'
                - similarly identifying other useful structures (like the 'e ratio' which resembles a core interface structure when formatted as a maximally different angle with a common base, as in a 'right angle', having a smaller change based on a larger change) as a highly explanatory indicator of symmetries which also relate to stability of systems (as systems that vary change types using minimal inputs are useful, small changes within some limit are useful, and are likely to reoccur as a result) and can act like a 'base solution to test first' in the absence of other base values to test as parameters (meaning 'apply e as a base parameter to check if the system has reached some local extreme of optimization/stability'), as the 'e ratio' is likely to sustain itself when applied repeatedly (a spiral), where other ratios are likely to intersect with themselves (circular ratios) which is useful for different intents and other ratios are likely to never intersect with or relate to themselves (leading to infinite change, which is not as commonly useful)
                - similarly other structures may be useful when applied as certain/constant/default inputs to a system, such as the 'exploitative ratio'
                - given that these specific math structures are highly connected to important/useful variables like stability/symmetry/balance, they are useful as specific math structures to start from/use as a base in some problems (like in stable/simple systems) where more complex systems that are not stable are likelier to benefit from differences from this parameter
                - finding the reason why a structure occurs is important bc different reasons create different types of change
                    - for example, if the reason why there is a negative correlation is bc some agent had an incentive to falsify the positive relationship, that would change in the next more accurate batch of data as being opposite to the correlation found, so knowing that reason is useful to predict how the data set might change
                    - similarly, if one agent is involved and if there is an incentive to falsify information, the information is likelier to be simple (follow a simple pattern) than it is to be complex, as simple-minded agents are likelier to need to fake some information
                - finding other constants (or other specific structures) of stable/optimal systems is similarly useful as these constants (or other specific structures)
        	
        	- identifying useful structures like 'changes which preserve info about the data set (such as symmetries in the data set) given the intent to find a representation, rather than an intent to remove info' to identify possible solution functions (composed of those changes) which might be useful for representing the data set (like finding average lines)
                - any function which changes the data set in some way (like removing outliers or removing sets of points that dont change the average) in a reductive manner (reducing computations/inputs/redundancies/noise), while also preserving a relevant solution metric (such as a type of 'average' like a 'local average' which is nearer to the final solution function, or 'probability density' or other 'moment' of a function) could be useful in finding the final prediction function, which is the ultimate average or other representation that is useful for minimizing prediction errors
        		- relatedly, 'mirrors' (or 'look-ahead tools to skip computation') depend on finding symmetries around which changes vacillate within a known range defined by the symmetry, so that once a symmetry is found, all changes around it are determined and dont have to be checked (just like a 'type' acts like a symmetry around which change develops, staying within the range defined by the type definition), which means symmetry structures like 'averages/extremes/inflection points/densities' are useful for determining info with minimal computation
        		- relatedly, finding the 'sets/sequences of symmetries which capture most relevant info about the data set' is useful for finding which sets of symmetries should be applied for which intents, symmetries being useful for predicting change as they indicate change bases & limits
        		- other symmetries (which are highly explanatory of changes in the data set) include higher powers, as the higher powers of a function are good at forming a base function to apply specific changes to in order to find the specific customization of that base which applies to a greater ratio of the data set
        		- finding the opposite of these symmetries (lower powers) is possible to do efficiently if done locally, to find change rates that could be adjacently produced with lower powers, but finding the higher powers is still required and these cant be used to skip a high degree of computations
        		- checking 'exponents of identified local change types' is useful to find possible simpler/alternate adjacent functions (once x^3 is found to be true locally, x^4 and x^5 should be checked as well in adjacent or maximally different subsets)
        		- as another example, the following equivalent alternate sets of insights make interface analysis trivial to identify:
        			- insight about variables of problem-solving
        				- 'there are solution automation workflows (like trial and error)'
        				- 'there are multiple workflows (there are others, like break a problem into sub-problems)'
        				- 'this means there are variables of solution automation workflows'
        				- 'these workflows interact with some objects like problems and have variables in common, like information requirements/interaction types/errors'
        		    - insight about multiple interfaces and automatability
	        		    - 'concepts have structure'
	        		    - 'information has structure'
	        		    - 'problems have structure'
	        		    - 'rules/functions have structure, and are therefore automatable'
	        		    - 'objects with structure are automatable'
	        		- insight about the interface concept itself
	        		    - 'problems are a matter of identifying similarities/differences'
	        		    - 'most problems are resolvable with standards that make comparison tasks trivial'
	        		- insight about the usefulness of each primary interface
	        		    - 'concepts can independently be used to solve a problem, without logic, information, or other primary interfaces, and without functions like "test" which would normally be involved in problem-solving'
	        		    - 'primary interfaces like concepts/logic/information are equivalent alternates in that they can be used to solve a problem independently of each other in best cases'
	        		- insight about the conceptual relevance of default/core structures
	        		    - 'direction corresponds to the intent interface'
	        		    - 'distance corresponds to the change interface and the similarity/difference interface'
	        		    - 'surrounding structures correspond to the system context interface'
	        		    - 'alternate possible structures correspond to the potential interface'
	        		    - 'angles correspond to the perspective interface'
	        		    - 'chainable functions correspond to the logic and function interface'
	        		- insight about how cross-interface structures are more useful
	        		    - 'concepts are only clear when you have an example (such as the variant of the concept in physical reality or in a particular problem) if you dont know the whole definition'
	        		    - 'concept-structure structures are more powerful than either on their own'
	        		- insight about how some variables are more powerful at more useful tasks (like explaining/describing/generating/determining) than others
	        		    - 'there are variables that are more powerful than others, like the general variable of cause, which is highly explanatory'
	        		    - 'there are equivalent alternates that are useful to know (explaining/describing/generating/determining intents, or find/build/derive intents)'
	        		    - 'cause is also good at these equivalent alternate intents'
	        		    - 'cause has an equivalent alternate in that logic can replace its value in these intents'
	        		- insight about how some structures are more generally useful across problems
	        		    - 'structures like rotations, similarities, and limits keep re-occurring across problem-solving methods'
	        		    - 'these structures are different types of objects like math objects, standard objects, and system objects'
	        		- insight about finding the 'most reduced set of useful/important structures'
	        		    - 'if you try to reduce language to the most useful structures, youll find structures like inconsistencies, perspectives, requirements, implications, overlaps, etc'
	        		    - 'the most variation-capturing variables of these useful structures are the bases where the others can exist, like differences/errors, structures, or functions'
	        		- insight about multiple perspectives and simlarities/differences being related to formats (which are like interfaces)
	        		    - 'different formats make different intents trivial'
	        		    - 'everything can be differentiated by changing perspective'
	        		    - 'a perspective is like a filter/standard/format'
	        		    - 'everything is similar and different to everything else in some way to some degree'
	        		    - 'variables/functions are related to interfaces as they change within a defined limit'
	        		    - 'differences in similarities and similarities in differences make problems trivial (similar to the comparison insight above)'
	        		    - 'standardizing to the same format makes some problems trivial to solve, as it highlights meaningful differences'
	        		    - 'some standards/formats (logic/concepts) are useful structures (interfaces)'
	        		- insight about useful graph structures
	        		    - 'some graph structures are more powerful than others, such as networks, maps, trees, sequences, etc'
	        		    - 'the differences between these useful structures involve objects (variables/functions) in the graphs, connection functions (like similarity or interaction function), and the structure variables like direction (in causal networks, for example)'
	        		    - 'a graph of graphs (like the interface interface, or the meaning interface) is a useful structure to organize these variables'
	        		- insight about different problem formats
	        		    - 'different problem formats exist, such as filtering problems and sorting problems and building problems and simplification problems'
	        		    - 'different optimal solutions and solution-finding methods are trivially derivable given the format of the problem'
	        		- insight about equivalent alternates
	        		    - 'there are some structures which are equivalently useful, such as common high-variation functions, common formats, common errors, common causal variable structures, common problem-solving sequences/workflows, etc'
	        		    - 'there are some problem-solving structures which are equivalently useful, like alternate function sets or alternate useful structures like interaction levels or problem metadata like problem formats'
	        		- insight about common problem-solving differences/functions
	        		    - 'different problem-solving functions exist which are common across problems, such as "find a solution" and "derive a solution" and "build a solution" and "change an existing solution"'
	        		    - 'these functions have structure, and there are other common functions to problem-solving processes, such as common problem-solving intents'
	        		    - 'other function sets common across problem-solving processes exist, such as core interaction functions of problem-solving processes and cross-interface functions and connection functions of problems/solutions and formatting/standardizing functions'
                    - insight about how primary concepts are powerful 
                        - 'simplicity/complexity' corresponds to a primary 'difference-resolution or connection' function, so it corresponds to a primary 'problem-solving' function, as problems' default format is a 'difference to resolve', bc a problem can be solved by making it simpler or finding functions that in general simplify other variable interaction functions
                        - other attributes which can describe any structure also correspond to primary difference-resolution and problem-solving functions (like 'work', 'intent', 'potential', 'change', etc)
                        - for example, 'changing an existing solution to a similar problem' is a default problem-solving and difference-resolution function
                        - what other primary abstract concepts can be used to resolve differences (solve problems)?
                            - 'balance' can be used as a primary problem-solving function in the form of 'balancing extremes, as extremes often lead to errors like over-prioritization errors or scaled errors', which translates to a problem-solving function like 'balance the maximum differences in a problem to find a more general/stable format of the inputs which is likelier to be correct (a solution)' or 'find functions that are adjacent/interim to a high ratio of information'
                            - 'power' can be used as a primary problem-solving function in the form of 'finding functions that can reduce the work of all other functions, making these functions more powerful' or 'find functions that store high ratios of information'
                            - 'potential' can be used as a primary problem-solving function in the form of 'finding functions that can interact with the highest ratio of other functions, making these functions higher potential in the higher variability of their interactions' or 'find functions that have a high ratio of usage functions (can find/build/derive a high ratio of information)'
                            - 'intent' can be used as a primary problem-solving function in the form of 'finding functions that determine what other functions are used for (intents) and what theyre useful for (adjacently/optimally fulfilled intents)' or 'find functions that are more useful for a high ratio of information-related intents (like information storage)'
                            - 'cause' can be used as a primary problem-solving function in the form of 'finding functions that identify input/output (causal) sequences of connected variables (such as why a function is useful, as in the reasons the optimally fulfilled function intents develop to be common, such as the efficiency of using the functions and the commonness of the requirement of their outputs)' or 'find functions that cause a high ratio of information'
                            - 'certainty' can be used as a primary problem-solving function in the form of 'finding functions that change certainties slightly, as these slight changes are likely to solve new problems unsolved by existing/known certainties' or 'find functions that require/determine/generate/describe a high ratio of certain information'
                            - 'abstraction' can be used as a primary problem-solving function in the form of 'finding functions that can store/embed most variables adjacently' or 'find functions that can support a high ratio of information'
                            - 'perspective' (as a filter formatted as a set of priorities, or structurally as an angle that makes a subset of information obvious/certain) can be used as a primary problem-solving function in the form of 'finding functions that can change angles/filters to make connecting/differentiating any variables trivial' or 'find functions that can identify perspectives that are useful for a task like embedding information or identify perspectives that can make all information trivially stored'
                        - given how these concepts interact with information (can store a high ratio of information or otherwise are useful for information-related intents), the structure of an interface emerges
                        - the primary abstract concepts (power, balance, complexity, stability) offer good candidates for interfaces bc of the fact that they are high information/variation-storing and every structure has these attributes in some way, so they are a way to access different fields of connections (differences/similarities) to other structures

                - most of these have common variables, such as including 'known common useful structures', applying common useful intents like 'reduce/connect/compare', identifying structures that are relevant to 'problems', applying interface structures as a useful compression/explanation/other intent fulfillment structure, etc
                    - they differ in their method structure variables, such as how 'identifying the type of object that a useful structure is' differs from other insight sets in that it applies a bottom-up direction (generalize from specific examples) from a 'specific' starting point

            - identifying interface structures like 'efficiencies' that align with regression problem space structures such as 'symmetries' or 'densities' by relevant structures like 'causes' ('this point is a hub' meaning 'a lot of inputs created this output bc it was particularly efficient for something') and identifying functions to connect those interface structures with causes that can be mapped to other variable sets (other efficiencies can be generated and checked for, once the reason of 'efficiency' is known as a cause of some 'variable interaction structure'), meaning other relevant useful 'low-cost, high-reward uses of inputs' for known intents can be hypothesized and checked for in the data set to determine the prediction function, as well as 'other causes of efficiency such as energy limits, which could cause other structures related to energy like power imbalances, power concentrations/compoundings, power takeovers, power dispersions, power vacillations, etc'
                - this is useful bc structures like 'efficiencies' are related to 'symmetries' and other specific useful structures in the regression problem space like 'densities' in their usefulness for predicting/explaining/limiting change
            - identifying useful structures like 'questions to answer (like "where does change change") to solve different sub-problems of the problem (like "how to divide the data set into subsets which are likely to contain different change types/rates/degrees/etc")' which make the rest of the problem trivial (makes solving the other sub-problems like "find the most different subsets (which could possibly contradict a function for another subset)" trivial) and identify the sub-problems they make trivial, and filter these by which subsets of sub-problems can replace other subsets or the whole set for some metric like 'finding an approximation of the solution'
            - identifying useful structures like 'adjacencies' as a way to determine if alternate variants of the data set are more obviously compliant with a pattern, as 'adjacent transforms' of a data set are likelier to be valid than other transforms, and some of the adjacent transforms may have more obvious patterns/averages/densities/other useful structures than other types of transforms
                - similarly, identifying useful structures like 'repetitions' to apply to relevant structures like 'subsets' (specific sets) to generate structures (like 'repeated change types across different local subsets') which are more relevant to a prediction function (the complete general set), or 'differences' to relevant error structures like 'non-local subset change types' (which are likelier to be less relevant to the prediction function, as in inaccurate, than local subset change types and therefore applying changes to these errors structures is useful to generate the actual prediction function)
                - relatedly, a 'mix of simple/complex transforms' applied to a base function to balance the extremes of the dichotomies that are useful/determining of the problem space and allow resolution of them by testing small differences favoring either priority may be useful as a testing/diversification structure to hedge bets in the prediction function, as well as embedding other variables reflecting other dichotomies (certainty/uncertainty, specific/general, discrete/continuous, adjacent/extreme), which is similar to retaining multiple different equivalent alternate functions which are ambiguously correct as they fulfill solution metrics similarly
                    - organizing these differences by 'which are likely to be adjacent' & related metrics is a useful way to filter out functions that are exactly obviously incorrect (by placing a constant in exactly the maximally wrong subset, for example)
                - similarly, if there is a subset of input variables that can be used to create the exact opposite (or similarly different variants) of the found prediction function indicating that the variables can be easily made to contradict each other, the symmetry uniting them is likelier to be more true than either the found or the opposite function
            - identifying useful structures like thresholds to filter possible alternate interface variables of a function, like determining the function up to a local subset size that its possible to determine its change types, like whether its exponential or linear, as the meaning of a subset emerges and is more obvious, the greater the number of points in the subset (subset ratio), where the meaning of a point is difficult to determine in isolation of other points
            - identifying the primary interaction functions between subset structures, such as how a subset might represent a fraction such as a quarter of a data set shape, so it should be reflected across two symmetries to create the rest of the data set, or a subset might be an 'orbit' or 'boundary' of the rest of the data set, so checking if these interaction functions apply to the rest of the data set is useful and identifying these interaction functions is useful
        	- identifying useful structures to filter out possible errors like 'randomness in the data set' by checking for 'associated structures of randomness (when defined in that problem space)' like plus/x shapes of lines with equivalent/similar numbers of intersecting points with the data set, or ambiguity shapes like squares (which could represent any line with positive/negative or constant/zero slope with equal probability) or similar areas reflecting randomness when found to describe the whole/most of the data set, or look for indicators of usefully biased shapes like rectangles/circles which dont represent completely equal/ambiguous change
        	    - similarly, identifying the set of 'lines with equivalent/similar intersecting points, optionally using lines as connections between adjacent points in the set (to add simplicity as a proxy of generality)' as indicators of equally probable solution functions or 'lines which skip the fewest points to form a straight or otherwise simple line' are useful structures to use as generators of possible solution functions, similar to other generative strategies like applying variable changes like 'embeddings on other variables' to generate possible maximally different functions, just like how 'any line crossing the data set' is similarly useful as a 'probable line-of-best-fit' to check errors and determine shape/slope of the correct function by assembling 'adjacent errors (to form shapes like lines/curves/boundaries)' and 'error changes like sign changes or phase shifts' into the solution function shape
        	    - randomness is particularly important to identify bc structures that exist are unlikely to be random (equivalent alternates that seem ambiguously similar usually resolve into favoring one or merging or differentiate further and become obviously different in equivalently useful ways, if the host system continues to exist bc multiple equivalent alternates are redundant, high-cost to maintain, and therefore less likely to occur & continue to exist), so determining inputs/components of non-random structures (such as biases, or simple rules) produces a set of 'probable structures to exist'
            - identifying useful dichotomies like 'parabola or line' is particularly useful as the core question to answer in the regression problem space, to generate algorithms such as 'connect the outputs at the lowest/highest x-values, and check midpoints in between the lowest/highest x-values to check for an error indicating a parabola', as its more important to identify when there is a parabola/wave vs. a straight line than to identify any other structure except more core unit structures like averages/extremes/inflection points and errors like 'gaps in data' and other structures resulting from core structures like extremes (such as limits/infinities)
               - the inputs to this algorithm are the x-range extremes (lowest/highest x-values) as well as the midpoints in between which would be useful to check for parabola-type errors at (the selection function of subsets)
               - identifying trends in error structures (like if the error is always 1, the function should probably be shifted up/down by 1, or the program is only checking values at a wave peak/valley where the magnitude is 1 while identifying the horizontal line crossing the wave at its midpont) is useful for identifying corrections to try early on, and identifying how to correct them such as by 'evaluating values at different intervals rather than at the same interval' and 'identifying the simplest line that intersects with the data set the most' to avoid this error, are similarly useful to reduce the probability of repeating that error
               - as another example, if you have a horizontal s-curve (one up and one down peak), identifying that is as simple as identifying that three other pieces of information are required, and identifying those three of the y-values at regular x-intervals (or the two/three determining points of the peaks/inflections), at which point the s-curve will be obvious if curvature is used to connect these points and your interval function identified the extremes of the peaks, if the program specifically checks for that type of curve as a common curve type (the skew/concavity or the squareness vs. linearity of the curves is another useful variable to determine as an important specifiying variable, the squareness indicating less likely change types and the curvature/linearity indicating more probable change types), so an algorithm to identify the variable set of 'curve peaks, x-ranges, and squareness/skewness/volatility at some subset of the function' is likely to produce a generally accurate function, on its own regardless of other variable sets, which can be enhanced by other known variables capable of producing extreme errors such as 'threshold phase change' variables which can make a function act like a totally different function in some continuous subset
            - identifying useful sequences of interface structures like a 'maximal difference connecting line indicating the connection between the most different points' and a 'inflection-point intersecting line indicating a smaller direction of change' which can provide a simple set of linear functions to base changes on to find the regression line quicker, which in a curve with exponent 3 (which starts lower, has an upward parabola, then a downward parabola, then increase indefinitely) would indicate the primary deviation from the primary summary of the change patterns (a line with positive slope connecting the low initial points and the high later points as the primary summarizing line, specified by an interim line with negative slope crossing the inflection point to indicate the primary deviation from that primary summarizing line, which can be further specified by tangents indicating extremes or alternately limits of vertical change)
            - identifying useful sequences of filters to reduce regression solution sets by useful trivially calculatable structures like change type variables, such as a sequence like 'check for a parabola, which implies an exponent, then given that an exponent implies other powers could exist like even/odd powers which determine maximal differences in the resulting function, check if powers are even/odd, then proceed to other variables to determine the rest of the function' which starts with an easily found structure and proceeds to other structures made possible/probable by that structure
                - this is related to other workflows like 'check for a change type, then infer other change types based on that, given change type interactions' but involves finding alternate sequences which are maximally differentiating/filtering or otherwise useful
                - these sequences' usefulness is maximized in cases like 'when the first item in the sequence is an interface variable that can support embeddings of other variables (like a unit exponent variable or other variable structure to differentiate function types) and the following variables are embedded in that interface, so the filters begin with the maximal differentiating filter and then decompose the remaining variation on that interface'
        	- identifying optimal algorithms involves finding useful (such as 'measurable') variables with 'obvious errors/optimals', such as how an algorithm in between simplicity/complexity is likelier to be optimal than an algorithm that is at either extreme (this is useful for filtering all possible algorithms to reduce the search space)
        	    - similarly, algorithms that involve some common useful interface structures (such as symmetries, which are like information wormholes, as well as limits and maximal differences) are likelier to be useful (and true and relevant) than other algorithms
        	- identifying variable interaction functions that could not be true given some system of reality where those functions could exist that is not true, like identifying that a 'wishing reality system' is not an accurate model of reality by identifying that 'agents wish for problems to be easy to solve (they wish for types of freedom)' and 'problems agents have are not usually easy to solve' and 'some common wishes of agents contradict other wishes of their own and of other agents, and would also contradict a wishing reality system for other agents' and therefore a 'wishing' variable interaction to connect problems/solutions is not a valid problem-solving function applied absolutely, which is useful to rule out variable interaction functions to find variable interaction functions that are possible/legitimate as a way of finding a reality system model, though finding states that move in the direction of that system is possible using realistic rules (increasing agent intelligence makes problems easier to solve and reduces agents' contradictory wishes against other agents, which is a solution of removing the intents that make the 'wishing reality system' impossible to logically sustain)
            - similarly, identifying high-variation explanatory variables like 'incentives' and 'interaction levels' from a typical data set where structures like 'defaults' and 'input/output similarities, differences, and requirements enabling interactions' (as more probable than other variable values) and 'adjacencies' (in casual degree) and 'efficiencies' (in benefit/cost ratio and stability) and 'interactivities' (as default interactions between variables) and 'types' (as efficient captors of information) appear more common than others, 'incentives' being a common factor in these common variables, as these structures are incentivized compared to other structures that may be more complex/difficult in some way, so simple queries like 'find common high-variation variables in common high-variation variables to identify other common high-variation variables' are useful in typical problems like regression, as if a common variable seems high cost its likely that we just havent identified the incentive yet, as the incentive is the determining variable more often than not, just like randomness is not usually real equivalence of probability in outcomes, but is likelier to just be lack of information about variable interactions that makes a variable interaction falsely seem random
                - for example, the dynamic between 'filters (as measurement/input-selection tools), as a core structure of differentiating functions' and 'incentives to distort the definition/structure of the filter to exploitatively avoid/subvert it or otherwise exploit it' is a highly explanatory interaction, where once a filter is applied, the incentive is to apply differences to game the filter (either to become a false/illegitimate input, and/or to expand its input range, or to use the functionality guarded by it without having the filter applied to it), and once the filter function has changed enough to handle these predictable adjacent incentivized differences, the opportunity to exploit differences to the filter is closed and differences are applied elsewhere, where these 'differences applied to the filter to handle adjacent differences/errors used to exploit it' is a 'common useful change sequence' to be able to re-use across filters, as the filter structure usually is applied too simplistically initially and must usually be changed to handle obvious exploits, 'filters' being a variant of 'standards' and are therefore useful in core intents like 'finding bases/limits of adjacent similarities/maximal differences'
                    - usually the filter follows patterns of errors, such as being too simple, too specific, too rigid, too structural, etc - so that it benefits from common useful optimizations like 'generalization' (as well as fulfilling useful 'core optimization intents' like 'increasing interactivity with other structures that dont adjacently cause errors', etc), an 'optimization change sequence' that can be matched to an initial filter by its probable error metadata (simplicity, specificity, rigidity, structurality, etc)
                - this can be applied to algorithms in general, such as for example, 'abstracting an input filter' in the 'find a regression function' problem space, to "find points belonging to the same type and connecting those points in a 'type function' to differentiate them from other points" or 'find maximally different points that should be connected in the same function, as they are legitimate and/or representative points'
                - these common distortions of a filter definition can be anticipated in advance so the solutions to these exploits are built-in to the filter definition, rather than applying the 'optimization change sequence' after errors are exploited
                    - a useful question to identify these change sequences: 'what is normally found implemented as a filter in typical systems?' for example, a 'domain/topic/type sub-type filter' such as a 'specific sub-type of object within a type with different rules that should be applied to it', so that a 'filter to find items of this type' is applied, often with a hard-coded function/dict to identify those items, and given the value of differences created by those different rules, other items will try to seem like those items, in predictable adjacent incentivized ways
                    - what other useful functions are commonly implemented? 'functions to correct distortions/errors beyond some threshold used as a filter of acceptable differences', 'functions to batch or aggregate items having some attribute', 'functions to connect some different objects using maps/functions/filters', etc - all of which can be implemented using some filter function, which is why this structure maps to a core interaction function 'find'
                    - a useful application of this would be to 'find new differences to apply filters of in a system, where these filters would be more useful to differentiate some structures and arent already used in a system, to optimize the system'
                - as some algorithms/queries are applied, other interface structure become obviously useful, such as 'similarities which are not adjacent' and 'changes that are neither obviously adjacent similarities or maximal differences' or 'interim differences between maximal differences' (useful as variants of maximal differences to test more different possibilities that are more similar to some interim base) or 'similarities between maximal differences', as 'adjacent similarities' and 'maximal differences' are obviously useful, so connecting these structures using related structures (related by the same base structure of 'similarities/differences') and finding useful variants of these structures and useful intents these structures are likely to adjacently fulfill (such as 'more complex changes which are less obvious and likelier to describe complex systems as well as the differences between various standards/similarities/symmetries that describe a high ratio of variable interactions') and standards/symmetries within these structures ('similarities between maximal differences' to identify input/generator/descriptor/limit variables of these structures) is useful while applying differences to create new structures (which should be abstracted/fit into other interfaces to check for new types of useful structures)
            - identifying the structures like 'shapes that when overlapped and rotated and viewed from a distance look like more common shapes like circles' as useful for describing reality in that they cover and explain more variation than other structures (this structure has a structure embedded in it that indicates 'equivalent alternates (having an equivalence in their center & a similarity in their rotation)', the 'incompleteness of any one alternate in this set when viewed in isolation', the 'usefulness of general trends once a process is repeated/scaled', the 'balance inherent to multiple equivalent alternate perspectives', the 'ever-changing nature of foundational structures leading to changes in interaction levels', and other fundamental structures that are descriptive if not generative of reality)
                - relatedly, viewing reality as a 'set of repeating processes applied to partial overlapping/embedded structures at varying intervals/magnitudes/scopes' may be more useful than some 'networks with unique nodes or just functions as nodes/queries or just individual usages as nodes/queries' bc the repeatability models useful interactions like 'aggregations at scale', 'net/emergent effects', 'in/stabilities', and other important structures created by the interactions of real systems, so this model of reality may simulate a more useful interaction level than abstractions like function networks tend to capture
                    - similarly, 'similarities (like patterns) in "differences from incentives"' and 'similarities in entropy/uncertainty/potential reduction structures' and 'structures of differences (such as attributes like unfulfilled/impossible/required intents) between differences/variables and difference-connection/solution structures' are related useful structures that captures high variation and formats differences in a minimally complex structure
                    - a network/field of these sets of useful structures that describe high variation in reality is useful as an alternative to a set of useful interface structures which can generate these adjacently
                - relatedly, viewing reality as a set of 'differences from required limits (impossibilities)' or alternately 'commonalities (similarities to probabilities)' (produced by adjacent/input/causative structures of commonalities like scale/aggregation/repetition/isolation/efficiency/incentives/investments/adjacencies/examples/usages as ways to produce commonalities) and differences from commonalities (like those that occur when "previously isolated commonalities interact after repeating enough to reach each other's position") can be more useful in its specificity as different from other useful formats like just any high ratio of all combinations of 'similarities/differences', this specificity being useful in the certainty it provides to base changes on
            - identifying the structures like combinations/ratios/networks/interaction levels of optimals, stabilities, requirements, errors, and other interface structures that occur in reality is important for solving the 'find a regression function' problem, as there will inevitably be something that a real system 'optimizes for incompletely', something that a real system 'should do but doesnt given this priority/requirement/input/opportunity', something that 'stabilized after a particular error type', and other combinations of interface structures like 'conditions' and 'causes' and 'optimals' (such as rules like 'apply high variation or high potential-variation variables first to decompose high variation'), which can explain most variable interactions but are not default/core structures already identified/required (these structures are more complex variants resulting from more interactions), which are obviously useful despite their complexity, as more useful to retain and use as defaults that re-generating them from core components every time, similar to how solution automation workflows can be more useful to retain than re-generate
                - these are useful for determining when a data set (or the system it reflects or the parameters of the analysis process like definitions) is incomplete/incorrect or otherwise suboptimal/erroneous in some way
                - relatedly, structures like 'monodromies' can be useful to apply existing math structures to indicate which points/lines/other structures that could represent 'limits which should not or need not be crossed' (which differences should not be resolved, such as where an ai program might 'resolve a difference' of a medical problem which is a 'state of difference from health' by allowing patients to die to create a 'difference from the requirement for health/health resolution to invalidate the problem' which is the difference to avoid except in extreme circumstances like where a 'health generator is adjacent, invaliding the requirement for health' and otherwise resolving unnecessary differences like 'resolving a difference that is already a known constant' or 'resolving a simple difference that is obvious' and 'resolving a difference that leads to an extreme such as hyperbolic change which is unhandled')
                - relatedly, retaining structures that are more useful to make constant (such as where its more useful to retain a map of inputs/outputs than to find a connecting function, like where extremely high variation is observed/possible and not reducible or where reduction is contradictory to other intents) and use as input defaults than to re-generate is a way of identifying the stable structures of reality that should not usually be changed/tested but rather applied as default inputs, combined in increasingly complex ways to describe more variation, rather than replaced with more descriptive variables, as these defaults are already the best at description
        	- identifying patterns/limits/useful representations (like areas)/other interface structures of useful 'false similarities' of functions (at some point, subset, in relation to some threshold, or range), such as when '(x^2) + 1' depicted as an area will look like a rectangle (when x is near 1) and when it will look like a square (as x approaches infinity), and identifying the point where these 'false similarities of areas' change (when x is sufficiently large that 1 looks trivial by comparison), and whether other changes are possible/defined/likely (whether those 'trivializing' changes will apply again at some point in that direction) or whether the pattern will continue, to identify 'different change types possible with a function', which is useful for determining the remaining sets of a function given a determined set of input/output relationships and in filtering functions that are equivalent or seem equivalent in some subsets but are not
        	    - similarly, finding 'standardizing' functions like to remove 'obviously non-impactful variables' such as the 'shift to move intercept value at y-axis to y = 0' that make the change type patterns of x^2 more obvious (the inevitability of the 'increase in area created by x' being exponential being more obvious once the 1 constant additive is removed as the impact of adding 1 at higher x values is more trivial which obscures the otherwise obviously exponential area increase, and the addition of 1 being increasingly trivial in changing this exponential increase in an invalidating direction)
        	    - similarly, other 'variable combinations with obvious/predictable impacts' exists like how a 'sum of constantly increasing and constantly decreasing variables' could easily be a horizontal line depending on their ratio
        	    - depicting these 'possible variable interactions' in a way that their commonalities are depictable as common points/overlaps or other obviously similar structures (such as combinations of inputs having an attribute in common crossing the same line/point or having the same shape or area) is useful for filtering probable interactions that are relevant to some other interaction (like an input/output interaction)
        	- similarly, finding a subset of points that represents an average/midpoint of some other set of points can replace the calculations required to connect the set of endpoints and instead just using the midpoint is acceptable as an approximation algorithm of the others, if enough different points are preserved to retain the general shape of the data set, just like 'removing extremely similar points' as 'redundancies' (but not redundant in all cases, as in the case of points around a density center, which preserve the weight of the density given its surrounding adjacent points) and removing 'removing non-adjacent (but non-maximally different) points connectible with local constant functions' as 'improbabilities' is another way to reduce the set of points required to connect in the same function
        	- finding useful formats like the 'set of angles/areas creating the components/features/products created by an input' which are summed to generate the output and which can be easily checked as un/applicable to other inputs by changing the angle of perspective in viewing these angles/areas so the impact of component areas/coefficients is obvious (viewing the interim products of operations in between x and y as a stacked set of areas, angles referring to the differences between area upper/lower limits compared to input x), rather than repeating the multiplication/sum operations on the other input, which is related to the format of a 'network of areas (representing products of variable pairs/sets) that is separated and aligned to make the output (sum) more obvious to avoid re-computing it', these formats being useful for approximating/prediction/avoiding computations by identifying similarities in component metadata like ratios of areas, and identifying obviously wrong 'product component sums', similar to filters like types that determine 'information about what else is also true' from a 'set of input facts'
        	    - this format is maximally useful when the interim components are few & large or otherwise obviously different, 'either very large or small' and 'very few' (meaning more adjacent to the final output)
        	    - once probable 'large components' (or otherwise simplified components) are identified as probable, filtering the sets of possible smaller input components of these components to resolve the ambiguity of 'which smaller inputs created these larger outputs' is a reduced problem compared to 'filter all possible solution functions' and possibly also 'apply incremental changes in a direction that reduces error' in some cases, which can be repeated up to the point where additional component-resolution is trivial (differences added by further component identification are trivial, or equivalent to other differences created by some other component set or simple layer like some function of randomness)
            - similarly, the 'equivalences in differences' from a particular 'possible solution function' are useful to identify 'probably useful summary functions', as functions that produce 'equivalent differences' are likelier to be more adjacent to the actual solution function (transformable to the solution function by some trivial transform to produce these equivalent differences, like a shift or scale change)
                - relatedly, finding these useful standardization functions that produce an 'equivalence in differences' from one function to another is useful to find these functions which have equivalent differences
                - relatedly, the '(patterns and other interface structures of) differences in functions that have equivalences at various subsets' are useful to identify, as having a common interface defined by their equivalent subsets which vary adjacently by some variables that generate the different functions united by those subsets, which make the problem of 'selecting between equivalent alternates such as ambiguous solution functions' more trivial
        	- identifying the 'maximally different subsets' that will stay under an error threshold for the same given general solution function is useful to solve for and apply once the 'maximally different subsets' of a data set are identified to filter the possible solution functions by taking a subset
        		- similarly finding the probability of a particular solution function by comparing the ratios of coverage of these 'different subsets, which are equivalent in their error range of a general solution function' (which function corresponds to a higher proportion of maximally different subsets under a minimum error range, this proportion representing an approximation of the probability of that function being the solution function)
        	- checking for set of 'points and directions' within a horizontal slice of a probable range (where the densities are, indicating where the solution function probably is within that range) is more trivial than checking for inputs corresponding to any outputs, allowing a method to skip points corresponding to outputs outside of that slice, as a subset of 'points with adjacent directions of change' is an alternate format of the solution function that can be used to determine the rest of the function, as finding the actual points in a horizontal slice of the data set and adjacent directions of change moving away from those points captures the value of a subset of determining points (in a high-variation slice of a probable range area), as an alternative to the usual determining points like extremes/inflections/averages, which are further determining of the rest of the function when paired with adjacent change directions, if the slice is in a high-variation section so that most variation of the function is captured in that slice (the same can be said for any horizontal slice of the data set but moving it to within the probable area range of the solution function and specifically to a high-variation subset makes it more useful, and pairing points with adjacent change directions increases their determinability of the rest of the solution function), so 'finding the most useful position of the slice' is the problem to solve, in addition to 'finding the point/adjacent direction pairs that determine a function, instead of the usual determining points/metadata'
            - identifying useful structures like different formats to handle cases where it's not possible to select between alternate solution functions, such as where a range of probable area is too large and functions within that range are equally or similarly possible, in which case every possible function in that range is more useful to format as a node on a function network, as every connection can be distorted to be every other connection type by applying some variable or error, these variables or errors being possible to apply generally across systems, so any data set could be the product of an error applied to some correct function and could therefore require some distortion to correct, so a set of weights indicating which function is likelier in a function network is more useful than a single function, or alternately formatted as a function generating a set of 'certainties' like angles (indicating a probably correct point, after which there is a divergence creating a probable area), and 'randomnesses' acting like 'ambiguities' where many possible functions exist
                - this is related to other methods involving handling alternate functions but applies the insight that 'every possible connection function between points could be valid and could be altered by excluded/hidden variables its likely to encounter until it becomes every other function, and having a function network where identifying the node where a function is gives useful information about adjacent/probable alternate functions, which is useful for identifying the probability of the function becoming other functions and the probability its an incorrect function, given some known correct input variables, this function network organized by probability of becoming adjacent functions'
            - identifying patterns of error feedback indicating obvious solutions to correct the errors is useful, such as where the errors of a function that is incorrect bc its shifted to another position follow obvious error patterns that can be easily identified and corrected
            - identifying points that, when some simple connection function is applied, can generate the most other points is useful to identify as simpler connections are likelier to be true (if the whole data set can be reduced to a set of center points and circular border points around these centers, these patterns are useful to identify as being significant in their repetition, structure, simplicity, and difference from simpler/core functions like lines, even if the effect of these overlapping circles seems like randomness or a linear function in some subset)
        	- identifying useful components to describe data set subsets like adjacent point sets such as 'filters/bottlenecks/convergences (connect only different surrounding directions) and random/circles (connect points in all surrounding directions) and extremes/limits/peaks (connect similar but opposite surrounding structures) as default structures of adjacent point connections to use as components'
        	- 'filtering structures similar to outputs to find the outputs' is a useful function such as how its useful to know a set of the most unique (maximally different) functions to filter maximal differences, and its useful to know the 'most similar function of the set of similar functions' to filter similar functions bc the representative/average of these functions is more similar to each item in the set and is likely to be relevant to other items in the set
        	- identifying new densities from a known density is often trivial bc identifying the midpoint (or other representation like density) of a one-dimensional set of points in some direction to discover densities in (from the perspective of the known density looking in that direction) is trivial compared to the task of graphing all points and finding all densities, where finding 'trivial to identify' densities is more efficient and can benefit from some transforms like angle of evaluation changing the problem space of 'filtering all points past this x-value' into a one-dimensional set (comprised of a subset of points in that direction) to identify a 'direction to move in', finding local averages in many subsets, densities being likelier to be less common and therefore more trivial to identify than the local subsets used to calculate local averages to select/create points to connect, and structures like 'equally distributed points in some direction which would make the task of identifying a density or average trivial' is less likely to occur in a realistic data set and calculations can be skipped in those cases
        	- identifying what point interactions (like high volatility, extremes in ranges, high slopes) can indicate possible sources of high variation, which are the most important points to identify, to identify whether an average line (or base solution regression line) needs to be adjusted, so that identifying these 'high variation-causing points' can be an approximation or alternate of a regression line-finding method
        	    - similarly, identifying which regression lines correspond to 'what types/sets/ratio/other metadata of points indicate the points that must be ignored in order to make the regression lines seem accurate', like how 'change rate changes' and 'difference from base function slope sign changes (positive/negative difference from base function slope)' have to be ignored to make a constant line seem accurate, etc, so that checking for these types of points once a base solution function is found which might contradict it is trivial
            - identify what variable interactions look like with various error type structures like combinations and filters of these possibilities to apply tests to input variables or gather more input info (expanding the workflow loop to include changes to test/data gathering variables)
            - identify at what point inferring that a 'more complex function with more peaks exists after how many negative indications indicating the opposite' is the wrong inference and can be contradicted 
            - identify filters to select/switch between function formats like 'probable function range areas' (useful when an area seems more random) or 'average functions with surrounding error vectors' (useful when an error or legitimate function range seems too ambiguous to resolve with current info) based on different complexity structures and cases in data sets
            - applying variables to create randomness from a non-random data set (or similarly linearity from a non-linear data set) and identifying whether some randomness/linearity-resolution functions also use those connections required to create it (if some system dynamic like 'system collisions or overlaps' is a randomness-resolution function and a collision/overlap variable created randomness, do the systems explain the original data set before the collision/overlap)
            - identifying error/legitimacy resolution structures (at what point does an outlier seem like a trend-change predicter rather than an error)
            - identifying gaps in uncertainty/certainty, randomness/linearity, difference/similarity, complexity/simplicity, and error/legitimacy spectrum variables that cant decompose some change type adjacently and the spectrums which can resolve them using some resolution structure (or the other structures using these spectrums which havent been identified yet)
            - identifying the most useful tests to apply (like 'a method that can identify new inventions as well as non-adjacent high-variation variables or sources of randomness in systems like "high-distance high-randomness variables like neutrinos" and "powerful/change-triggering variables like incentives as explaining most variable interactions"' as well as reversing the function to 'find system structures created with incentives/randomness' or 'a method that adjacently identifies useful interface structures from highly different as in non-interface structures') to check if a solution-finding method is successful at which point different solution-finding methods can be generated combinatorially and tested iteratively
            - identifying similarity structures between subsets of the data set (similarities such as 'similarity in intersection with some line type having some attribute', "similarity in a point's difference types from neighbors") which explain the data set the best, as points which are similarizable by these attributes are likelier to be related in that way in the system producing the data set, these similarities serving as inputs to probability of relatedness and therefore can be usable as a filter of points to incorporate in some algorithm to determine a representative line, compared to some difference set to generate a composite ratio of relevant similarity types compared to difference types
            - identifying inevitabilities of structures and the related structures like points/ratios/distances required to identify them (such as two slopes on either side of a peak making the peak inevitable, given the input variable interactions implied by the slopes and given the distance between them and the existence of other peaks already identified)
                - identifying evocative structures (which are similar enough to other structures to be useful in deriving them)
            - identifying relevant truths like how the 'extremes/borders/upper/lower limits of the probable area range' might be a better representative format of the function than a line bc different behavior at a higher vs. a lower value is a realistic possibility that occurs in real systems, or the 'extremes with another structure' (like an average or a probability distribution of a point being in a particular sub-area, as in a different probability distribution for each 'x-value' or 'local x-value subset')
            - identifying useful structures to add/remove such as 'areas of randomness' such as a cube of evenly distributed points, which its possible to filter into possible solution functions to connect it with more certain lines around it but is also possible to remove as a structure of randomness that indicates neither priority so can be removed as possible noise and added back in when new information might help filter the possible solution functions, in cases such as where the more certain lines around the random structure are equivalent and indicate none of the solutions in the random area as more probable
            - identifying useful alternatives (like whether to 'apply multiple average methods and merge the average outputs (like an ensemble of networks)', or 'whether there is room for improvement in the average methods and finding new variants of them given their variables is worth pursuing (like a function of method errors where a more optimal point is implied on the error curve)') in existing methods by applying interface structures like 'input-output sequences' applied to 'average methods' as an important component of the regression problem space
        	- identifying unit structures to apply as components of a data set range (such as a data set density or probable function range) include units like 'overlapping circles which are likelier to describe a probable function range of a typical data set which is usually an area rather than a clear line', 'semi-circles shifted around a linear average line', 'shapes that indicate exponential change and also some non-trivial area like curved rectangles or circles as tiles of the probable function range' to indicate variable interaction structures that could represent the legitimate variable interactions mixed with some error likely in a system of some probable complexity (local change tiles which are likely to be simple shapes distorted by errors in some degree/way), where a 'probable function area as opposed to a probable function line' indicates either a complex system, a system of maximally different or locally representative alternates indicated by edges/corners or local averages of these shapes where the interim points are errors, a lack of complete input variables, an error like randomness injected in the input data set, where a better (simpler, clearer & more accurate) representation of the data set would be a previous variable set on a prior causal node where the differences creating the area are graphed as vectors and the variable interaction can be graphed with a line
        	- just like some structures are useful in their simplifying and explanatory effect (like 'core components' leading to outer 'interaction levels' supporting 'maximal differences (like a function network but in every direction)' where 'interactions are adjacent between nodes'), other structures that combine important useful interface structures like 'core components' and 'interaction levels' to achieve useful intents like 'adjacent/linear interactions' are similarly useful, and can enhance the usefulness of these structures, such as 'cross interaction level errors' like what errors can happen when an output layer interacts with the core input layer, or a way to organize the structure so that frequently interactive functions are adjacent even across interaction levels
        	- identifying 'sequences of change sets' that are commonly seen across systems, to identify common useful functions found in sequences like a 'creativity/generation' step, a 'maximally different/uniqueness' step, a 'standardization/grouping' step, an 'abstraction' step, a 'contradiction/neutralization' step, a 'incentives/efficiency' step, a 'stress/competition' step (like to see which variables stay constant and which can maximize their variation), an 'explanatory/decomposition' step, a 'understanding/organization' step, a 'ambiguity or other error generation/identification' step, a 'randomization' step, in a way that reflects real change patterns, and applied to connect known certainties (probably certain trends in the data set) and uncertainties (the remaining variation), these 'sequences of change sets' being more useful than other function formats to identify commonly repeated useful functions, as well as other patterns in change sets like cycles and equivalent alternate change sets, some of which are simpler/more useful than the original connection between original inputs/outputs, and to identify different useful function formats like 'sequences of vector structures & maps' applied to inputs (which move in the direction of function networks and allow queries like 'which are the most commonly useful function networks' and 'which function network compresses all other function networks adjacently'), as structures of 'useful incompleteness' created by mixing cross-interface structures that leave irrelevant variation unhandled and are adjacently transformed into useful variation-capturing functions
        	- condensing variables as 'some or any change in this variable set of the same type (rather than specific changes in each variant of the type)' is also useful to quickly identify sources of variation on a different interaction level
        	- identifying structures like areas/slopes of maximum volatility (adjacent input-extreme output connections) and connecting them to areas of constance as a way of identifying the maximum volatility allowed by a filtered set of functions in a more constant subset of the data set, as connecting the filtered set of a function in a more determinable subset and the volatility allowed by that filtered set can determine other local subsets of the data set
        	- identifying useful similar structures like overlaps/convergences as indicators of similar but different types of change (overlaps being an indicator of robustness or probability, and convergence being an indicator of some limit structure or an average structure)
        	- given that embedded variables (meaning 'embedded on interface variables') are likelier to create 'maximal differences', testing those as a default filter to describe complex data sets that can easily look random in some subset is useful in this problem space
        	- given that there may be a transform of the data set (like a subset of input variables) may be a more efficient way to generate the probable data sets found in real life (as in, there is a subset of input variables around which most of the variation is clear and easily explained by more common functions, indicating the original input variables include random noise), its useful to identify subsets of variables with simpler variation as complex systems are less likely to be stable and are likelier to contain random noise from variance injections, similar to how identifying simple structures like 'evenly spaced repeated data set subsets' and 'densities' can identify common patterns that are simpler to model as a pattern or a component of the solution function if they represent a sufficient ratio of the data set
        	    - relatedly to the workflow involving removing simple structures, removing some subset of the 'points' that are connectible in 'straight lines' in a data set is useful to remove structures too simple to be useful (in the sense of relevance for the intent of capturing information beyond a general simple summary base function to use in finding the actual solution) in describing a more complex data set as is more commonly found than a simple linear data set
        	- some structures are more probable than others 'a range of data points around a pattern (as in an area of probable range of a function) which describes likelier structures like errors' is likelier than a 'highly variable specific function (like a function with many peaks)', so these likelier structures can be applied as more default than other structures
        	- function sets which are adjacently transformed into each other which have some base of similarity in common (like accuracy, variation, input/output patterns, or input variable subset ratio) are likelier to be equivalent alternates and probable solution function sets than other function sets
        	- 'maximally different data sets with known error types applied, mapped to solution functions' are another useful starting point for reducing the regression problem
        	- variables like 'dependence/connections between inputs' (such as a 'sequence using the previous adjacent input as an input', as opposed to a 'function of independent variables') that create dependent/independent variables are also useful to identify, as these variable differences are important to filter out, where independence of variables is not obvious but a sequence is easily detected
        	- identifying structures (like isolatable structures, including obvious components, such as 'anomalies') which are likely to explain some variation interaction patterns (like an 'occasional extra peak disrupting a more probable pattern') as common error structures to remove to find simpler functions and add to find error-handling functions or erroneous versions of data sets, similar to how modeling a 'system cascading to destruction' is useful to identify signals of these error structures in data sets, just like identifying 'probable new more stable states' of a data set is also useful
        	- increasing the 'directness' or 'adjacency' of variable interactions as being more explanatory, more likely to be linear/simple, and less likely to be subject to undetectable noise/errors is a useful intent rather than trying to identify distant connections between indirectly causally linked variables, as indirect connections are likelier to change (but also likelier to re-occur when removed so extremely distant effects are still useful to model)
        	- similarly, given how the problem of 'finding a function to summarize a data set' is a problem of 'finding missing variables (coefficients, powers, etc) that are not in the input', other formats of a problem are similarly useful to apply as defaults (what are the patterns of missing variables, are they more difficult to detect, are they less visible using data gathering techniques, do they have similar complexity like 'host dna' & 'pathogen dna' which allows inferring the 'existence of dna and pathogens in a host', and 'default function errors (like immune errors)'), similar formats like 'stable variables' to identify variables that are more stable than other variables so they will become 'high-variation'-causing variables, a useful structure to identify with 'high-variation variables'
        	    - for a more complex specific example, identifying people from their dna involves identifying 'missing variables' (like phenotype-determining genes, as well as 'genetic change variables' such as epigenetic changes, errors in dna-phenotype mappings, mutations, disorders in dna editing, etc and causes of these 'genetic change' variables) and filters of these missing variables (tests of these variables to help filter them or legitimate stable differences to filter/separate these), which are variables in between the inputs/outputs in different interaction levels than the inputs/outputs like the 'gene function' interaction level
        	    - similarly, the machine-learning problem involves finding 'missing in-between variables' (like a 'function to create differences (to find variations of structures like combinations of inputs)' and a 'function to attribute/connect useful differences to filter out (like connecting an error difference to a node or node structure)' and a 'function to calculate differences from correct values (calculate the error/loss from a particular input variation combination)')
        	    - these are highly complex sets of variables (input/output subset filters, high-variation change inputs, 'instruction' functions generating similarities, variable interaction functions, generative functions, difference-filters) which fits with highly complex systems, decomposing them to slightly less complex variables on different interaction levels, which can help with identifying missing variables (such as inferring 'alcohol' as a cause of 'genetic changes' when other complex sub-systems arent sufficient to decompose all variation in a data set, as a source of variation from another interactive complex system, given how complex systems usually are created by interactions between multiple complex sub-systems)
        	    - variable interaction functions like 'add/subtract/neutralize', 'limits/extremes', 'filter/differentiate', 'repeat (like as a default)', and 'interfaces/averages' are common (or even required) across systems so are likelier than other variables/functions to re-occur in unknown systems, which makes them useful to apply as default variable interaction structures
        	- similarly, applying known 'contradictory cases' of 'when known rules are wrong' such as when a more general function is wrong compared to a less general but more accurate function despite a common solution metric like generality
    	- similarly, identifying the 'core shape of a local subset representing a change unit of the more general function' and 'its interaction function (such as overlapping with other core units)' and its change functions (such as how it can 'rotate to some degree or vacillate in some way') as a way to identify the general function from a local subset that is sufficiently representative of the differences in the data set that it can be used to find the 'core change unit shape' that can be repeated/shifted/scaled/rotated/otherwise changed to find the rest of the function using some interaction function
    	    - relatedly, finding the network that filters the possible core change type combinations (rotate, scale, shift, vacillate/cycle, embed variables, repeat, abstract, connect, format, etc) in a maximally efficient way for most functions is a useful intent to fulfill as a default implementation strategy to start with
    	- similarly, identifying useful structures like 'maximally different directions of change such as the cardinal directions (or high/low left/right directions of change)' as useful structures to use as a filter to identify different change types that are common and highly different, as a 'maximally different unit of change' which is useful to find adjacent changes
        - similarly, identifying connections between variable interaction structures (like how a variable structure such as a 'variable upper bound and a constant lower bound' have a useful structure of "implying but not guaranteeing (making them probable and useful to test)" other variable structures like 'more change (either expansive/reductive) happening in the upper range' or 'fewer limits on change in the upper range' or 'more change incoming to the lower bound' or 'interface variables and/or constants relevant to the lower bound') which can be connected to possible filters/limits of those structures, such as whether other variable interaction structures (or specific known problem space system rules) filter/limit/prevent a possible implication
        - similarly, identifying local subset representation structures (like densities or average lines explaining the majority of differences in the subset/densities) for one local subset and then removing those probable components of various possible representations in another subset to find additional possible variables in maximally different subsets to filter the set of possible alternate representation structures (and their components) from the original subset, after identifying the maximally different subsets likeliest to have differences in probable representation structures & their components
        - similarly, identifying connections between 'uncertainties' (like subsets of the data set that are more uncertain/variable where other subsets are more easily determined) and 'uncertainty resolution functions'
            - resolution functions like 'intersecting with the most points in the subset with the simplest line' or 'the simplest line that remains some minimal distance away from the most points in the subset' or some balance of 'averageness' and 'intersection' which are useful variables of filtering/generating these 'alternate regression lines' in 'highly uncertain subsets'
        - similarly, identifying useful structures like the 'variation range' necessary to create a useful structure like an identified 'acceptable error range area' (with a trivially identified area of coverage that is likely to be useful in finding a maximum of points falling within that area when moved across the data set 'probable regression range') that can be moved across a 'probable regression range' and tested in various subsets to cover a 'ratio of the data set in that subset', this 'acceptable error range area' being useful to avoid calculating the error for a possible line at every input value and just checking if it falls within the range area centered in some 'probable regression range', either after calculating some 'probable regression line' or using the 'probable regression range' to fit it inside that range, or finding the trajectory of the 'maximum coverage direction' as the area is moved across subsets
        - similarly, identifying the relative usefulness of structures like 'intersecting/overlapping summarizing lines of data set subsets' as opposed to 'adjacent local subset summarizing lines with strict range limits' to allow for possible overlaps in local subset selections bc the borders of these ranges might not be guaranteed/required by data and to allow for known variable interaction patterns like 'multiple possible interaction types/states involving the same variables, variable interactions which can explain variation in outputs with similar/adjacent inputs
        - similarly, identifying alternates of useful structures like 'randomness' (such as how 'randomly dropping a ratio of data points can reveal robust variables') alternatives such as 'combinations/sequences of common/powerful functions across systems' can identify more probable structures to replace these less likely structures, using probability structures like 'commonness' to replace the less accurate/likely structure like 'just any randomness at all (a random selection of randomness)', which applies 'more probable & less random' randomness created by a higher degree of certainty through alternate structures like 'commonness'
            - similarly applying more relevant/useful structures of randomness like 'obvious structures like simple shapes like densities removed from the data set' which are more relevant to workaround (as simple-minded agents are likelier to intervene with obvious human error and likelier to be required to stop the interference of), as true randomness is unlikely to be easily verified like obvious randomness and is less relevant/useful to account for (above some ratio of expected noise), as more complex randomness could easily be hidden legitimate variables undeterminable from the original data set requiring more data/variables to identify such as 'incoming changes' to the data set
        - similarly, identifying a function to convert non-linear to various probable linear functions (more adjacently computable, or using fewer variables involving equivalent alternate variable subsets of the data set) is a useful intent to fulfill with structures like logarithms, topologies, & mappings to model 'interaction levels where interactions are adjacently computable with objects defined on that level'
    	- similarly, an 'index of pre-computed regression lines to compare with original data set subsets' to fulfill useful intents like 'avoiding computation' is useful for connecting original data set subsets and pre-computed regression lines for alternate subsets
    	- similarly, how clustering relevant structures (like inputs) by differences/similarities (such as organizing by the 'similarity/equivalence of output value') can identify useful structures like input patterns of similar outputs given assumptions like 'non-volatility' and 'non-randomness' and 'non-uniqueness of inputs for each output' (like input subsets that have clear patterns, like how a wave function organizing the inputs by similarity of output would have clear patterns of magnitudes/amplitudes in a few subsets that prevent requiring checking the whole input space for the pattern, as the output y-value would have a few data points associated with it such as 'input value points graphed vertically with the y-value on the x-axis' representing inputs having that output, where this vertical input pattern associated with a y-value would have clear patterns such as obvious differences in distance between points that are clear after a few points rather than many), which reduces the problem to 'find n y-values having the same value m times to check for obvious patterns in x-values for each y-value, if there are multiple x-values'
    	    - preemptively testing for validity of 'assumptions of algorithms' is an example of 'alternate equivalent structures' (like limits/requirements/intents) and an application of the 'input/output sequence' of optimal algorithm filters
    	    - this applies a useful function like 'sort' to the outputs rather than the inputs to get useful information about similar outputs once sorted that way such as how variable/cyclical/maximally different the function might be by comparing inputs of equivalent outputs
    	    - once outputs are sorted, it is possible to find highly different outputs easily, which is useful for intents like 'check for maximally different outputs' or 'find output range or output extremes'
    	- similarly, 'finding average magnitude/amplitude/count of peaks in a data set for some subset' is useful just like 'finding the general average value' and 'finding local subset averages' and 'finding reoccurring subsets' and 'finding highly different local subsets and their connecting functions' are useful
        - similarly, a 'adjacent points merging function (using some representation like an average midpoint, or using a similarity metric like distance from/angle to local densities, and using some ratio selection like merging n points at a time within some distance m of each other)' and a 'adjacent point non-merging function (leaving some points unmerged bc of their representativeness of legitimate differences)'
            - similarly, an algorithm to identify variables that can capture high variation when applied together (representation metrics, similarity metrics, ratio of input metadata like count/difference score) as useful complementary capturing variables of information, acknowledging differences in data sets like that averages are sometimes more useful than similarity metrics and sometimes the opposite is true and there is a useful input type like 'input count/ratio of the total count' that is optimal for some structures like high-variation data sets with some noise level
        - similarly, applying similarities between structures that have a reason why theyre useful for summarization/representation intents (like how 'big/simple' shapes like 'equilateral' shapes are particularly useful to identify to simplify the task of identifying a regression function bc finding their centers/averages/densities/patterns is more trivial and likelier to reflect reality as big/simple shapes are less likely to occur by accident in a data set and therefore likelier to summarize the data set, up to a certain point, like how identifying that a data set as a whole has a generally square shape makes it likely to be equivalent to random)
        - an example resolution function between 'densities and regression lines' is 'divide into subsets, then find one representative density for each local subset, then expand densities until an overlap/equivalence is reached with another expanded density' by applying the 'reason' for why it would be useful to connect 'representations (like density averages) of adjacent local subsets' (bc adjacent local subsets are connected in the original input data set, so the reason to connect them (or a variant of them like a representation of them) later is that connecting them later 'aligns with the original input' in a relevant way, relevant by 'preserving the information of the original data set' which is useful for the 'find a regression function' intent) and applying the structure of 'how' to connect them through 'expanding' them (and the reason why to use that, which is an adjacent transform applied to a density average and is therefore useful, where equivalents are also trivial to determine, and these operations in total can beat other regression algorithms in some solution metrics)
        - similarly, a function that connects the 'points that vary' and the 'points in common' across multiple probable regression lines is a useful function to solve for 
            - finding the sections of the regression line that would be variable in variations of the bias vs. variance tradeoff, to focus on finding functions to connect the 'points in common across probable regression lines' that should be optimized for in the final function, where the 'points that can vary across regression lines' can be averaged or otherwise represented by known probable points at discrete intervals rather than a continuous line, where a point not on those points can be approximated by adjacent points, where the 'points in common' can be approximated by a range that is narrower than the range for the 'points that vary' and the range representing a range of acceptable solutions, so finding a function to resolve the reduced solution set of the 'points in common' connecting functions by connecting these subsets with the functions describing the 'points that vary' is a useful function to solve for
	     - finding the connecting function between different sets of summarizing functions like the 'average' and a 'slope-standardized function (to find the useful standard to compare changes to, to find the core differentiating vectors from a straight/average line) and its scalar to scale it to the original' and the 'lines that describe local subsets to the points of extremes (similar to eigenvectors)' and the 'lines that connect averages of densities' is a useful function that connects these alternates which offer the same representation attribute but also capture different information in the data set, as connecting 'efficient representations' is more trivial than connecting 'every data point'
	         - finding the 'useful core function representing the most standardized (such as de-scaled) function' is useful to find a 'component function' to check against multiple subsets of the data set (do any known variable interactions create change types other than this component function or component function range or do they follow the structure of the component function/range) and look for variables that adjacently create/scale the core component function to check it for realistic probability of representation of variable interactions
	     - finding the most important structures to check for when filtering possible solution functions (such as how its important to check if an amplitude of a polynomial is different across different peaks to determine if a peak pattern can be applied/found, how its important to check multiple local subsets of the function input range, etc) can act like maximally differentiating filters of the solution set
    	- finding the useful ratios & other structures of inputs to an algorithm like 'find the common slopes of connection lines between points in local subsets of the data set', where the algorithm to find the 'useful ratio/count of slopes in common (a ratio compared to some standard, like the number of possible connections)' is the target to solve for, as the other structures that are useful are already known or easily determined and the uncertainty is in finding the threshold values or other values to optimize implementations of those structures
       - connecting alternate formats of the data set/regression functions like 'maximally different connectible shapes (like interfaces) that can be formed by a data set subset of some ratio' which can be used to indicate 'embedded variables' (like variations on that interface) is useful for determining one function format from another which may be more trivial than another method
        - other structures than standard regression structures (averages, connection lines, subsets) like 'maps' can be applied as a useful structure in the regression problem space bc of how mapping one subset to another through substitution can be an efficient way to decompose a more complex set of points into a more standardized or otherwise useful set that is likely to represent the original set and requires less memory to store, which are useful as components of solution-finding methods
        - finding useful metrics like 'degree of erroneous difference to ignore' between obvious average functions of local subsets is useful to find out what information to ignore when an average line of one subset differs to some degree from an average line of an adjacent subset, especially if the next subset confirms the original subset average line, applying the concept of 'data corruption' to describe some degree of error deviating from some implied metric, resolving these 'implication' structures (like the implication of a 'common subset average line') into 'conclusion' structures (like a degree of commonness of that line across subsets above some ratio), and finding useful tests of these differences, to find out when a difference may reflect a common or otherwise probable/implied structure (implied by adjacent inputs, common patterns, similarity to known implications, etc) rather than an erroneous anomaly to ignore
	- in the 'network (fuzzy space) of structures' fulfilling intents, interface structures like 'overlaps' exist between structures adjacent to or otherwise useful for multiple alternate intents, these interface structures indicating their usefulness for other intents like 'deriving alternate intents' and 'building a maximum ratio of structures'
        - in the space of useful structures, concepts like 'balance', 'alignment', 'simplicity', 'probability', 'composability', and 'uniqueness' will be obvious, which can be used as 'conceptual filters' of useful structures that are likelier to be useful than other structures
        - an example of this fuzzy space includes structures (like 'angles, partial closed shapes (like sides and corners), connection functions of partial closed shapes, higher-dimensional closed shapes, shapes that when combined can produce a closed shape in between them') as the set comprising the fuzzy space of a 'closed shape', this space being composed of components adjacent to or otherwise useful for fulfilling intents (forming/describing/differentiating a 'closed shape') related to a 'closed shape'
        - a network of similar/equivalent alternate spaces include a 'non-repeatable/unique component space (which is optimal for storage minimization)', a 'repeatable component space (which is more optimal for displaying usages/queries of components)', a 'usage adjacency component space where frequently co-used components are adjacent (that is useful for finding probably useful structures using a component)', a 'difference as adjacency space (where maximal differences are possible with adjacent queries)', a 'layered space with both intent/structures (where the fuzzy space of an intent contains maximally different structures fulfilling/adjacent to fulfilling that intent)' bc of the adjacency of these structures for these intents related to components
            - relatedly, a 'usage network (apply)' has adjacent corrollaries like a 'filter network (find)', a 'component network (build)', a 'difference-resolution network (derive)' due to the core functions it is an equivalent alternate to
        - graphing an intent by its 'surrounding related structures' such as by 'structures that use it' or 'structures that fulfill/build/create/cause it' or 'its input/output structures' or 'structures that filter out everything in some relevant subset but that intent' (or similarly 'structures that determine that intent') is another way to visualize structures like intents that are more useful when defined as a set of alternate definitions which can represent examples of them in some other system
    - framing common structures with relevant metadata like useful intents in standard terms to maximize the optimal positioning of these structures in queries
        - a network (which depicts uniqueness and similarity) is useful for 'finding new/different unique similarities by the gaps & other structures in the network' as well as 'identifying difference/similarity of two known structures'
        - a map is useful for 'finding unique connections & other metadata about connections' (like the commonness of connections having equivalent/similar connection/input/output) as well as 'translating a structure in one format (of a set that can be described by a network indicating uniqueness/similarity) to a corresponding position in another format (of a set that can be a network)'
        - a filter is useful for 'finding a similar subset of points in a network/set of points' (as in similarity to some attribute, like a solution structure/metric/requirement)
    - finding a good starting point to start applying interface structures is crucial for deriving adjacent solutions, like how a limited subset of 'logical rules such as definitions' or 'physics rules' or 'truth limiting rules (what is definitely not true)' might be useful as a starting constant input to start applying interface structures (like changes) to, to derive other rules that follow logically, are required to be true, are implied, are not contradicted, or have other structures of truth associated with them
        - similarly, finding a 'useful structure to describe common patterns in changes' is an example of a useful isolateable structure that can be a good approximation of a full implementation on its own, answering questions such as 'are most variables an adjacent combination (or other core structure) of some subset of interface structures', which is findable with iteration
        - similarly, finding a 'reason for similarities/differences' (reasons like 'its an efficient/useful combination of few inputs commonly available, so is often repeated across irrelevant systems') is another isolateable rule set that is a good approximation of a full implementation of all logic rules of interfaces
        - the differences between these 'equivalent alternate' isolateable rule sets that are good approximations of interface analysis make them useful to combine in an adjacent combination as offsetting 'ensemble' structures to weight the impact of their outputs against average outputs & other representations of outputs
        - this is like how everything can be framed as a component that can be added/multiplied to other components, but thats not always useful in terms of reducing computation requirements, such as how knowing 'addition' and 'multiplication' are capable of describing all other structures, but that doesnt capture a useful degree of complexity of the potential interactions of those two operations, where knowing concepts like 'self' and 'embedding' is more adjacent to the complex operations/functions possible with addition/multiplication (self-multiplication like 'powers' and embedding as in 'embedding of operations'), these two concepts being adjacently derivable with interface analysis through core structures like 'unit/identity (self)' and 'application/usage (embedding)', similar to how matrixes (aligned multiplication of ordered sets) and inner product spaces (spaces where some product is trivial to compute) and convolutions (complete multiplication product of sets) are not adjacent to just the functions add/multiply
             - this is a useful structure for tasks like 'encryption' as the 'set of concepts that are adjacent to a useful structure' is more difficult to guess (from the set of all possible concept sets) but is easy to verify
    - applying interface structures to optimize with ml, such as by applying 'input/output sequences' to position 'generative feature layers before filter feature layers' and other patterns that make sense to server as a useful contradiction to offset the irrelevant variation introduced by the preceding layer, or applying 'self' to apply neural networks to select preprocessing & algorithm functions/parameters
    - to handle common error structures like 'dead ends', apply structures like patterns to find the solution to (the 'way out of') traps using these errors, such as by applying insights like 'nothing is unconnected to everything' which means 'there are no real dead ends' as 'everything is both true and not true in some way' and find the distortion of the perspective that created the error of the 'dead end' and connect it to the balanced perspective (where interface structures exist or are adjacent), creating a difference that allows other differences to be embedded/connected/supported
        - if there are filters allowing for only one possibility (a 'dead end' error that leads away from a network), find the filters that represent the errors in those filters, reversing the perspective back to the balanced perspective and exiting the perspective creating the error of the lack of variation leading to that error, creating opposition to the incorrect difference
        - inject more variables to connect the 'dead end' position back to the balanced perspective, applying the interface metadata to create new differences (such as random differences through interactivity) where required to offset the incorrect differences of the over-reductive perspective, and use those differences to build differences on them and create change in another direction
        - where one structure seems to capture everything (like an attribute network), apply differences to identify other networks that capture alternate complementary information (usage networks, limit networks, difference networks, function networks, etc) to limit the limits of that over-reductive perspective
            - like how an attribute (such as 'criminal') can obscure info or over-simplify info, despite being an efficient way to store some relevant info (like the 'crime of jay-walking' being equated to all other crimes despite the fact that it is mostly only criminalized to collect fees to fund police stations and other far more harmful behaviors are not punished at all, such as stalking/copying inventors), whereas a network with contextual or functional info can more accurately depict that info to reflect its true variation
            - this is similar to how one infinity can be used to create other infinities (like the Banach-Tarski paradox) bc these graph structures are like topologies or fields in that they are capable of describing reality at every point but offer useful alternate advantages when starting from different points
        - this is like the structure of a 'mobius strip' or an 'isolated system describing the system containing it by identifying variables of observations of subsets of its own structures (regarding the incompleteness theorem)' where one structure seems ambiguously equivalent to different structures that seem to contradict each other but actually can co-exist in the same structure using the perspective interface, or like the structure of a 'rule set' occupying a point on a torus where what determines one error doesnt determine another error bc different 'rule sets' are supported and the 'rotation' and 'injection of new interaction levels (as different concentric circles allowed to be the base/core/stable level)' allowed in the torus shape enables endless (stable) balanced variation
        - a 'balance' of variation is a core structure driving interfaces, as there needs to be some variation-supporting structure like a ratio of change (like 'potential/kinetic energy' and 'momentum') and counter-change (like 'gravity', 'energy preservation/transfer limits'), or a structure like 'caring' ('connection to the most stable foundation, where this connections acts like an equivalence') that is supported and stable, otherwise the interface cant exist or similarly cant support any change, which can be used to find interfaces
        - on the other hand, filters may seem restrictive/limiting/reductive (like a trap that is a 'required error' such as a trap hiding a 'dead end' error) but may enable endless complexity/variation, like how a 'reality' filter may seem boring/reductive/simple until you identify how complex reality is, and that the 'reality' filter is powerful in that it empowers other structures to exist like 'clear descriptions/representations & measurements/experiments' as well as constants/variables and consistencies/contradictions to occur, enabling the pursuit and identification of truth and falsehood and the application of differences to both
        - "filters to identify errors such as common structures of paths leading to 'dead end' errors" are a way out of errors of 'constance' (like 'over-prioritizations' such as 'over-reductions') just like finding 'abstractions' or 'interaction levels' or 'equivalent alternates' or 'embedded perspectives' or 'connecting function of perspectives' are a way out of traps
        - finding a common perspective that hosts both a trap/error and also a solution/way out of it (or generally a 'perspective that can trap any trap' as in 'capture the variation of incorrect differences driving traps') is an example of a useful interface to apply to other problems
        - similarly, 'traps/errors may be more adjacent to a solution than another trap/error', so creating a 'path of differences leading to errors' is not just a way to find errors but also find out what is not a solution and therefore what is a solution, and value can be created from a trap if there is a way to convert it into a solution like by applying it to itself ('trap the trap')
        - the 'maximally different structure' that can support the most difference types is also the structure that can support adjacent structures of differences, like 'ambiguities' (like the ambiguity in an equivalent distance of one error from the center of this structure to the distance to another error, and like 'contradictions/paradoxes' and 'counterintuitions/complexities' and 'alternate representations' as different variations of 'maximally different structures producible with the same inputs, supportable in the same system')
    - finding a solution base that is optimal for different algorithms allows finding different possible solution bases, at which point finding the more optimal adjacent solution to those bases is possible with known/adjacent algorithms (as opposed to finding 'maximally different solution bases' to start from with one particular algorithm)
        - finding the maximally different functions that make an adjacent optimum findable with some algorithm helps 'filter out these more adjacent solutions if theyre incorrect' and 'find counterexamples of alternate possible solutions' or 'divide/filter the solution space' faster, such as by finding 'common overlaps in solution spaces generated by different algorithms' where these overlaps are more trivially calculated in some way than by applying either/both algorithms
        - deriving the 'solutions findable/verifiable with an algorithm' is a useful way to filter the solution space, find common solutions across algorithms, and find variables of algorithms to find other algorithms or match algorithms with metadata like intents/metrics optimized for
        - finding a 'network of algorithms with rules for switching between algorithms in certain cases like with certain input patterns or certain solution metrics' is also adjacent using these structures of connections between 'algorithms' and 'solutions/ranges/sets adjacently found/filtered with those algorithms'
    - finding the set of concepts that builds a solution (such as how 'adjacency' and 'generality' help build a 'regression' solution) which help to offset each other ('adjacency' in the form of 'adjacent/local subsets' or 'adjacent/local optima or adjacent/local density averages' offset by 'generality' in the form of 'representative subsets' or 'representative summaries like averages') can help form a common base set of solution structures to build on top of
        - these are useful specifications of more general spectrums like 'specific local variables, as opposed to general abstract patterns/summaries', 'adjacency' being a non-definite partial format of 'specificity' in the 'regression' problem space, having the additional concept of 'triviality' included in the subset of concepts driving 'specificity' to relevant variables like 'position (point, density, extreme)' in the 'regression' problem space
        - finding regression lines whose differences from data points frequently follow a common component pattern (like 'having the same distance from data points' or alternately 'use the fewest components (one component as in the same value)' which fulfills a 'simplicity' or 'linearity of combination' metric) is another useful intent to fulfill in the 'regression' problem space
        - similarly, finding a set of components that commonly connects extremely different points (like points from different non-adjacent subsets at different limits (upper vs. lower) connected to some representative subset like an average) is a useful intent to fulfill in a regression algorithm, the specificity of this intent making the algorithm trivial to find
        - similarly, applying rules of relevance such as 'removing variables with equivalent information coverage is acceptable if one variable remains to cover that information in an intent related to an information-preservation intent but its better to leave the variables/rules generating those variants in the data set rather than leaving in one variant or all known variants in some cases like when minimizing memory storage is prioritized'
    - apply common structures like filter/reduce/match/add/change/map/sort to format functions to identify the maximally different functions, then find the reason why those functions are useful as a way to identify 'rule sets that can act as limits of usefulness to identify the areas and boundaries of usefulness'
    	- example: 
	    	- bc its useful to have a 'map of a keyword to different variants of it', its possible to identify that "a unique signal has variations because of alternate definitions/formats it can take while still remaining unique (bc of 'definition routes')" (allowing this 'variable interaction' & 'variable interaction-structure rule' regarding usefulness to be derived: 'bc this variable interaction exists, keyword maps/definition routes are useful')
	    	- bc its useful to have filter functions in general, its possible to identify that 'not all differences are adjacently useful for every intent' and 'different variations of structures are not organized by default' and 'different structures can coexist'
	    	- bc maps (connections between 'user-assigned (relatively arbitrary)' rather than 'absolutely-defined' values) are useful, 'arbitrary connections' are also useful for intents related to randomness/uniqueness, like when connections dont matter absolutely but are still useful to assign (like organizing a filesystem to optimize common queries such as using symlinks or naming conventions, even though that organization doesnt reflect absolute truths of the universe, or such as how mapping several terms to one identifying term allows quicker identification of unique term usage while minimizing memory storage, or how an explicit map to identify a category of an attribute value set is useful when identifying the causal variables is non-trivial, to skip that causal analysis, or how substitution maps can create the appearance of randomness bc they are relatively arbitrary)
	    	- bc maximally different functions (filter, map, change) often co-occur indicating their usefulness, its possible to identify the 'common complexity of problems solved more frequently recently (bc of existing solutions to simpler problems)'
	    	- bc similar functions often co-occur (filter, sort) indicating their usefulness, its possible to identify that those functions are cooperative for various intents ('sort' speeds up some filters in some cases)
	- applying rules to identify when a regression function can be incorrect mapped directly to problem space structures
		- example: when a high ratio of incidental/random variables (like variables about a context that any species can exist in which is not relevant to identifying a species) are included in the function, or when an important variable is ignored to fulfill an incorrect priority like simplicity, these errors are mappable to differences between the incorrect function and data set subsets
		- when a more simple function is found but it contradicts the more complex function that would cover more cases bc of the data set subset chosen/available that makes the simpler function seem correct, its violating other known solution priorities like generality which can offset over-simplification errors
		- more importantly, structures of this error can be mapped to a set of example possibly relevant regression structures (data set subsets and regression lines)
		- for example, when there is a 'small pattern in a local subset indicating a more complex function' which is 'within the boundaries of a potential field of variable interactions' and 'not overlapping with probable error areas' but is ignored in favor of a simpler function, thats a structure that could be an error of 'ignoring a general trend bc of a data set subset selection (applied either before receiving available data or after)'
	- applying interface structures to known useful structure (like gradient descent) given their definition can identify relevant structures useful to those useful structures like optimizations to apply to inputs before applying the useful structure
		- the idea of 'gradient descent' is most useful when applied to a 'function that is adjacently connectible to the optimal solution' (meaning the inputs are already adjacent or equal to the actual optimal inputs) bc of the definition of 'gradient descent' which applies 'adjacent change combinations', assuming that 'adjacent change combinations' are capable of finding an optimal (which is best used when 'everything is adjacent' or 'maximally different examples are adjacent', etc)
        - this makes 'maximally different interaction levels' a useful structure to identify, to make most variable combinations or 'the most variable' variable combinations adjacent (which is what interface analysis does by default)
        - for example, to identify the structure of 'imaginary numbers (like i)' as a possible useful structure, some intents (like 'create a circle' or 'create common structures (like a circle)') make it obvious to try as a component of formulas and its not adjacent change combinations as theyre typically implemented ('addition/multiplication') but rather as an adjacent combination of interface structures (like identifying how addition/multiplication arent easily describing all structures and solving all problems, so try applying the 'opposite' of 'components of those core operations' to try to generate differences to use to describe different structures), for example to solve the problem of finding 'equivalent alternate maximally different descriptions/generators of a circle as a useful structure to describe/generate/apply', otherwise the idea of 'square root of negative one' is not adjacent to most intents but is obviously useful to identify, so identifying 'useful intents that would make such structures obviously useful' are useful to identify by applying interface analysis to fulfill probably useful intents like 'create common core components in different ways'
        - as another example, the way I discovered that circles are related to primes is by picturing multiplication in my head, in which I could easily generate squares/rectangles but I noticed there were gaps between these shapes' corners which were easily generated by integer factors, and these gap points were likely to be more describable as on a curve rather than as a product of integer factors
        	- https://twitter.com/alienbot123/status/997394393471516672
        	- it also reminded me of the fibonacci sequence bc of its embedded growth between numbers in the sequence rather than clearly exponential or clearly linear growth, as a different type of default growth that was neither of the two simpler types
        	- the idea of 'gaps between integer non-1 factors (when graphed as pairs of factors)' is not actually definitive as indicating non-linearity but is still evocative and therefore still useful in adjacently deriving that non-linearity is relevant, these 'evocative' rather than 'implicative' ideas are still useful in deriving new probable structures to test
        	- this insight path would be easily identified with interface analysis 'which asks questions like "what something is not" and "what is not easily generated by these variables" and "what is not adjacent" and "what is different" and "where would similarities be expected (like with other integers) but differences are found instead (like with primes) and what interface unites these (like the dichotomy of curves vs. rectangles)" whereas machine-learning dumbly applies a few insights at a time such as 'adjacent change combinations eventually can find some useful functions if you have enough compute',
        	    - while applying other relevant structures, like filters to avoid irrelevant exceptions/rules like pieces of the definition that most closely overlap with other more relevant functions like the 'identity function (using multiplication)' being more relevant than 'addition/multiplication' as the identity function is less meaningful than non-1 factors)
        	    - identifying the alignment between 'numbers between non-1 integer products in 2-d space' and 'numbers between non-1 integer products on the number line (primes)'
        	    - other useful structures to apply are:
        	    	- 'equivalent alternate formats' of primes and their factors (the relevant default variables) such as 'sets of alternate factors and lines connecting sets'
        	    	- the basic 'interface' structure which connects the 'equivalent alternate formats (number sequence of primes and factor sets of primes)' using a common standard they both adhere to in their similarity of patterns/gaps
            - relatedly, deriving other useful structures like 'e' can be formed by maximizing output change types (differences between adjacent values) while minimizing input variables (the 'previous value' and the 'previous previous value' and the 'addition' operation) without using exponents and using a simple function (minimized differences necessary to create the extreme differences)
        - similarly, 'adjacent change combinations' like addition/multiplication are not frequently useful for useful intents like 'differentiate a wave from a circle' which a machine-learning algorithm typically would not adjacently achieve but interface analysis would by applying structures like circles by default as a common core structure on the structure interface
    - applying interface structures like 'causal sequences' to identify useful structures like 'alternate points where variation is useful to inject'
    	- for example, injecting variables at the point of data-creation/gathering rather than data-processing (pre-pre-processing) could influence the value of data (its reflectiveness of reality) such as how 'smiling' before taking pictures can make some variables more obvious/detectable, which is a 'reality change rule' that is useful for making variables less ambiguous/more obvious so a classification algorithm can be simpler, so an algorithm to identify these points/rules is useful to improve data quality, or to identify more useful problems to solve like 'if the subject was smiling, what would these hidden variable values probably be, based on similar expressions as smiling that are generatable with existing data' (similarly, 'identifying a dye to inject (input side solution) or a treatment to try (output side solution) that would differentiate cells more easily' is a useful structure for a classification algorithm to be able to identify to improve data quality and delay decision-making of the model structure until a data quality ratio/info minimum is reached), which applies the insight 'bc there are some differences between items in different classes, there will likely be other differences that are detectable'
