      - Examples of additional error structures to apply as interface structures in interface queries implementing solution automation workflows

        - Error structures can come from complexity added by over-prioritization of priorities like:
          - adding more variables/features to everything just bc its possible so that 'every format (like urls) can do everything' when not necessary (executing commands)
            - a counterstructure is 'isolation of functionality' rather than 'replication of all functionality in every tool', which is the default bc once a tool is built, its easier to apply changes to it (treating it like an interface) than to write new ones
          - trying to make tools easier to use ('pre-configuration')
            - non-standard process, differentiated from the standard with an opinion like 'it should be easy to setup'
            - the fatal flaw of this perspective is that 'tools that are easy to use may also be easier to exploit', in edge cases where there is an overlap, as this easiness of an authorized process also makes other processes easier like hijacking sessions
          - implementing custom auth/security processes that deviate from protocols/standards, rather than applying the standard/recommended/required protocol implementation
            - leaving these implementations as variables for devs to choose allows them to inject their misunderstandings, instead of the constants these implementations should be (there doesnt need to be a custom install process for every app, its possible to choose just one that doesnt expose vulns and re-use it)
          - lack of tools to identify deviations from standard/recommended/required protocol or process implementations
          - trying to integrate/connect everything with everything (over-interoperability, as in 'not every app needs to talk to every other')
          - the addition of a feature usually allows more actions than it restricts bc of the extra variables/complexity and therefore is also likely to allow more exploits than it fixes, so there should be as few features as possible
          - the integration of 'high-vuln components' (like web UIs, as not every app needs a website, and where unnecessary they just increase the attack surface)
          - lack of prevention of actions by intent (only very important info should be able to bypass security or run high-impact processes that could cause other issues like 'logging in as root', which can by filtered in checks by other components before being sent/allowed to access these actions)
            - allowing these actions is sometimes not necessary at all, and only is useful for malicious users as it adds a variable increasing the attack surface
              - structures of volatility where 'small input changes can drastically change outputs' are useful for 'differentiation' and 'comparison' intents but not 'connecting' or 'equalizing' intents, as an opposite structure of 'ambiguity'
              - structures of invalidity such as where 'an area produces a level of error in a prediction function that invalidates fulfillment of a required solution metric' for the 'find a prediction function' problem
              - when 'alternatives' are actually 'mutual dependents'
              - when an 'abstraction mix' is useful: 
                - all variables are related to all other variables, so removing variables is inherently an error structure in the form of removing information that could be used to predict the output variable, but some errors are un/acceptable and some are useful, such as how the errors contributed by an over-predicting function and an under-predicting function can offset/neutralize each other, so an 'average' error structure (removing some data points/variables) is one of the 'useful/optimal' error structures, rather than one of the 'sub-optimal error structures' that can be neutralized by a similarly probable error in the form of its 'opposite error structure'
          - there may be 'contributing variables' in all possible alternate formats of a particular input set (in its original state, after change combinations are applied, after being filtered, as described by summary stats like averages, in identifying/generative function form, etc)
            - example: a 'set of variable values' or a 'set of values of variables' or a 'variable value' may be just as useful on their own, as 'change combination & filter functions applied to multiple data points', so the combination of both the 'data point' or 'variable value' and the 'variable created from those values by a change combination function' should both be handled as inputs by a solution-finding method, rather than removing prior values and replacing them with new calculated values, which is what a neural network does in forcing a linear direction of operations to create the output prediction value, rather than including the original inputs as optional inputs for each node layer/unit, implying that the original inputs cant exist on the same interaction layer as a variable created from those inputs, which is not an absolute truth that should be enforced by the network structure
            - complex structures like cases where an original input is one possible input of a variable created with change combinations, but other inputs exist and show up in other original inputs, so calculating the variable just from the original input and removing the original input is not optimal in cases where the other original inputs should be used and the original input can co-exist unchanged with the 'changed' variable ('coexist as output variable-influencers'), but using the original input instead of the other original inputs might remove the original input's influence on other processes when its in different states (other change combinations are applied), rather than applying an identity weight (weight of 1) to preserve original inputs whose impact can be replaced by changes to other variables that dont impact other processes
            - this is an example of a case where 'keeping data in different formats' (such as 'changed' & 'unchanged' inputs) is optimal

      - apply the 'success/error cause' of useful structures: why some structures are more/less useful than others
        - bc they are more adjacent or directly fulfill definitions of usefulness, such as 'power' in the form of 'inputs' (which control other structures by activating some functions) or 'controls of other structures' in general
        - example: why is heat a useful structure? bc it is an 'input' ('power' structure) to (as in 'creates' or 'increases') the attribute of 'interactivity' which is a very useful structure for intents like 'connect' and 'change', acting like adjacent power structures of 'heat' (like 'energy'/'electricity') for other dependent structures to function

      - functions representing core/component operations of generally useful functions on different interaction/abstraction levels and in different implementations
        - example: 
          - for the 'find' function, useful functions include 'check if structures are equal/similar' 
          - for the 'build' function, useful functions include 'check if structures are core/interactive/coordinating/opposite/fitting'
          - for the 'apply' function, useful functions include 'identify variables in a structure' and 'apply different value of a variable to a structure, as specified by this change function' and 'check if structures are different after applying change function'
        - these specifically relevant functions would allow adjacent implementations of the general functions like 'find'
      
      - functions whose outputs can be their inputs, like iteratable functions, where the inputs/outputs are relevant to or are useful structures
        - where problem-solving structures or methods can be iterated, apply them iteratively where complexity isnt reduced by prior iterations
          - 'find solution-finding methods of solution-finding methods of solution-finding methods'
          - 'find structures in structures in structures'
          - 'approximate apaproximations of approximations'
          - 'predict prediction function of prediction function inputs'
          - 'alternate alternatives of alternatives'
      
      - apply sources of ambiguity as possible interfaces and variables
        - if an ambiguity is maintained, the measurement/metrics of the ambiguous alternatives are wrong, or the assumption that they were similar alternatives is wrong, or the assumption that a selection should be made is wrong, or the assumption that either is correct (as opposed to neither) is wrong, or the question being asked about their similarities/differences in metrics is wrong, or the optimal alternative is a combination of alternatives or different option from either alternative, or its a structure of randomness or equivalence, or change types are involved which are not measurable or which dont have a measurable impact in that system/interface, or it can support enough change types of a unique conceptual foundation that it qualifies as an interface
        - if it doesnt resolve, it can be a set of interchangeables forming an interaction layer, which can act as an interface, like how 'variables' & 'state' are both structures of change that would seem like ambiguous alternatives under the wrong measurements
      
      - some specific functions like 'learn (as in update/improve on a metric)' as opposed to general function (like 'change') are more useful than others, bc of their alignment with & relevance to problem-solving intents like 'improve an existing solution'
        - existing solutions either already exist or can be adjacently derived, such as 'try every combination or possibility', and these can be used as a standard or base solution to apply improvements to
        - 'updating to improve a metric' can take the form of 'moving/changing a structure toward a solution position/structure' having a 'state sequence'
        - there may be multiple solutions in the space in which case determining the most adjacent solution, ambiguous alternative solutions, solutions for different optimization metrics, and the most optimal solution for the original metric are relevant sub-queries, and only framing it in terms of 'connecting two states' is not useful as it will miss the other possibly optimal states to aim for
        - 'build a learning function' is more useful than a specific 'learn a function or pattern' function with data & a metric to optimize as its inputs, but requires interface analysis to identify concepts like 'deactivation' & structures like 'alternate coefficient sets', 'incremental changes', 'accuracy feedback', and a 'stopping point' as useful structures for a learning function
        - a function fulfilling a problem-solving intent like 'improve an existing solution' would ideally have multiple implementations for various fundamental solution automation workflows, workflows like 'connect problem/solution states' or 'apply solution structures to the original problem structure' or 'apply differences to the problem or errors' or 'apply differences to sub-optimal solutions' or 'generate solution from problem space or solution space components' or 'apply filters to the solution space' or 'apply useful structures to the problem'
          - the 'improve function on a metric' function would have:
            - a 'connect problem/solution states' implementation, where it would start with the data or a standard solution and connect it to a more optimal solution, such as through iterative state updates
            - a 'build solution out of components' implementation, where it would start by identifying available components in the problem space or required components of the solution and aim at building those & then combining them in a way that fulfills the original solution structure
            - a 'apply solution structures to the problem structure' implementation, where it would start by identifying solution structures such as a data set point that should intersect with the prediction function or a solution metric to fulfill like a level of accuracy to aim above, and then applying those to the problem structure, which might be a standard sub-optimal solution or the data set
            - a 'apply filters to the solution space' implementation, where it would start with the solution space, which might be a set of functions that intersect with or represent at least some subset of the data set, and iteratively filter out solutions based on a solution metric
          - these various implementations would allow the solution to be found in various situations like different available inputs
      
      - solution (prediction function) metadata should include attributes like:
        - which data points from the original data set are prioritized by the solution given their better representation in the function, and if other points are prioritized, how much does the solution change, and why are those points prioritized (what do they represent, such as whether they are an average or other statistic)
        - which data points will fail with this solution and by how much
        - which representative examples of the original data set will succeed/fail 
        - why this solution was selected having been constructed this way, given the infinite equally accurate functions available (to use a standard representation metric, to generalize across other data sets, to retain a percentage of original data set information, to account for other latent variables)
        - how much randomness and how much variation in how many alternate data sets this solution will handle
        - in what thresholds in what cases this solution will become clearly sub-optimal, and which solutions will be optimal after those thresholds are reached (format of a 'network of alternate conditional functions')
        - how likely it is that there is an alternate solution that is interchangeable in terms of robustness across some variation in different data sets, similar fulfillment of solution metrics, or similar function metadata like a similar generative/descriptive function
        - how much & what quality of the original data set information is retained (quality of information like its average)
        - why variable interactions are justified & used in the solution
          - example: if two variables are usually around 1.67 times the other (like an animal's height & width), why would that be the case:
            - useful functions for the animal like 'speed and balance when moving' could be inferred from that data set if variable label information is retained and has access to a scientific/language dictionary
            - structural concepts like 'optimal ratios' would be noticed across various problems with different data sets once it was repeated enough, which could be identified through a repeated pattern or derived through filtering out alternatives like 1:1 or 1:2 ratios, which are less common because it's uncommon to find integer relationships in nature, it's less likely that one change unit of each variable type is likely to be optimal, and it's less likely that an excess of one is optimal for a unit of the other, given how related variables in a type or interface have some similarity given their common or related causes such as genes & functions, or derived through useful structures like 'comparable but not equal quantities' often found in 'related variables with a trade-off structure while an uncertainty in the form of an optimal ratio is being resolved'
            
      - identify absolute rules as useful structures in the form of reliable functions to fulfill their original intents by those rules which apply absolutely in all cases, such as functions whose output cant be changed from a solution to an error or sub-optimality for differences in inputs, being useful as a 'structure of certainty'
      
      - differentiative definitions of error structures like those determining sub-optimality, neutral structure & error structure
      
      - error-correcting structure requirements for error structures such as:
        - 'using only internal components of the structure in error to generate a solution to that error' for an error that occurs in one position
        - 'removing structures added with no or mismatched intent alignment for structures in error having no associated intent'
          - example: in various 'pooling' functions, one option may be selected at random (for no reason other than to generalize) when there could be a reason to select one option above another (it fulfills an intent like 'increasing solution accuracy' or 'reduces mismatch error between intended and actual functions')
      
    - variable interaction rules
      
      - standardizing variables of a data set to the same interface increases the power of those interface structures so that only one interface needs to be applied
        - as opposed to variables including cross-interface structures, like 'type variables' and 'core variables' which should be standardized to one or the other, rather than including both formats
      
      - variables follow similar interaction patterns such as 'building the next interaction layer by creating a new set of interchangeable alternates', and while these variable interactions are being identified in available & updated data sets, alternate solution functions should be maintained to avoid an error structure of 'prematurely selecting a solution' as a subtype of the error of 'hard-coding a variable value', which may also be an 'irreversibility' error structure, so that a pattern of change in a particular direction can be identified (which could vary as the system having these variables could be moving towards or away from forming an interaction layer or other structures), as most functions will be better formatted as a function network than as one function of input data set variables, which not only handles 'missing information' and 'ambiguity' error structures, but also handles 'predicting variable interaction structures' as 'finding the direction of change of the variable system' is useful input information to the 'predict variable interaction structures' problem, as variable interaction structures may change in predictable ways given change patterns
        - 'finding alternate/component/generative functions' and 'finding system change pattern update functions' are intents supported by the 'function network' or 'parameterized/generative function' structure, as opposed to the single function structure as the solution format
      
      - allowing some errors is useful to avoid over-reducing a function's inputs, when solutions may have a false similarity with error structures, so that reducing/preventing known errors also prevents useful signals from being handled, such as other errors or solution structures that resemble errors, as an optimal 'approximation' (fuzzy) solution that can be changed within a certain range to cover possible alternates
      
      - why is a 'variable network' less optimal than a '"change combination" function network' where many 'scalars' (weights) as functions are applied to many 'change combinations' (node inputs & input sums) as functions
        - a 'network' is sub-optimal in general where its useful to compare 'uses/queries of the network', in which case 'sequences of steps (where steps represent objects, variables, states, functions, etc)' are more optimal structures that allow comparison of step patterns and step sequences, whereas the network would format these 'uses/queries of the network' as routes between network nodes, which are not as clear for comparison intents
        - the variable network could represent different change types/combinations as queries/routes of the variable network, but this wouldnt allow clear description of inputs/outputs of various routes as separate from other queries using the same variables in a slightly different route (change combinations are created by flows through or uses of the network where 'change combinations' are represented by 'variable node sequences linked by change-generating functions'), as opposed to a structure supporting separation of change combinations (creating different nodes & layers to temporarily store 'change combination state' applied to possibly useful change combinations represented by sequences of weight paths overlapping at the end node to create multiple different trees creating different coefficient functions, so different change types/combinations are represented in a way that allows them to be structurally separable from & comparable to other change types/combinations in a standardized way (comparing relative trees of inputs moving in the same direction, as opposed to comparing unrestricted or randomly assigned network routes))
        
      - possible structures to use as 'function' structures in the 'neural network' problem space system
          - explicit structures
            - node-weight units
            - feature organizing functions (position or other similarity of features as a structure of relevance), weight initializing/update functions, input routing/combination functions, weight application functions, etc
          - adjacently implied structures
            - node-weight structures such as node-weight sequences (paths)
            - overlapping weight sequences (trees)
          - interim inferrable structures
            - node-weight structure neutralization/magnification structures
            - weight deactivation, near-deactivation, & activation node-weight unit sub-networks
            - node-weight weighting/prioritization functions (structures allowing repeated node-weight unit values increasing the priority of that node-weight unit value)
            - error structures like ambiguities, volatilities, improbabilities
        - possible structures to use as 'variables' structures
          - explicit structures
            - 'weights, weight change, & node output values' are the default variable structures specified by the network
          - implicit structures
            - the 'net impact of weight sequences' is an implied variable
            - 'change combination' structures including parameters of 'weights' as a sequential set and 'weight paths' as a sequential set that is sequentially updated, converging on a final 'weight path' sequential set or a final 'weight' value to find variants of the prediction function inputs that are likelier to be useful
            - 'change unit' structures such as where a node/weight unit combination can produce a 'big change' as opposed to a 'small change', a 'cross-threshold change', a 'cross-output differentiating (category) change'
        
      - other possible useful neural network structures
          - 'consolidation' or 'filtering/selection' nodes randomly applied in deep learning layers to cover cases where increasing the dimensionality of a weight coefficient set is not optimal and sufficient dimensionality has been reached, like when a 'type' variable doesnt usually need more than x changes applied to inputs (given allowed weight update structures) to be identified, as in cases where the 'type' variable is adjacently computable with inputs, at which point exploring further changes to that variable would be sub-optimal, but the type variable is predicted at row 5 in a network with 10 layers, and the type variable is more useful in predicting final values without further changes (further changes that result in higher prediction inaccuracy), and the type variable produced by those changes to inputs overlaps with another variable in a similar position, so preventing further changes to that variable would be the right move for the 'type' variable but the wrong move for the other variable produced by those changes to inputs, in cases where the data set indicates a different category or phase or interaction level of the variables of the data set, so randomly consolidating that node's output (the node where the type variable is accurately identified) & preventing further changes (instead propagating it as a constant that is not modified by subsequent weight changes) might preserve the information of two variables identified by that weight path
            - structural similarities like this weight path covering predictions of two variables with similar change types that occasionally occupy the same input because the data wasnt separated by these different variable sets or standardized so the alternate variable would be in a different position can identify optimizations to rules limiting network weights & weight updates
              - general network update rules can be injected in a way to prevent network-wide impact that individual nodes may not always be able to calculate (how would a node identify that its optimally encapsulating a type variable or multiple variables, invalidating further changes, without feedback at the network level optionally at each iteration)
            - assumptions are built into the 'node layer' structure such as assuming the 'same number of change types should be applied to the input feature vector' or that 'just-over/under-threshold values can be ignored a percentage of the time as the threshold is likely to introduce an inaccurate barrier separating values, for some proportion of the time'
            - deciding where to host changes is an important decision in aligning 'certainty of a change' with the 'enforcement of that change' (such as 'hard-coding variable change rules in pre-processing functions' because 'those pre-processing changes are more certainly correct so they are injected as assumptions'), whether to host changes in:
              - the default structures of the data set input features & weights & the weight update/propagation functions
              - the implicit/emergent/interim structures such as the 'change structures supported by the weight update/propagation functions', functions limiting network-wide or sub-network iteration changes, functions limiting net impact on 'variation of weights tested by the algorithm', functions that have an output of decreasing required training iterations, etc) 
          - a useful graph would be a 'prediction function change impact map' indicating how structures of explicit/implicit change structures such as 'change types supporting differentiation across ambiguous alternate predicted values' and 'change sequences supporting multiple variable interaction structures' and 'change sequences supporting various difference-resolution functions', so that some areas of the map would indicate a particular alternate prediction function or function having a particular 'type' or 'solution metric range' would be predicted, and if areas indicating different prediction functions are equal in area, then multiple conditional functions may exist, or the functions should be required to be weighted if theyre not equally or more optimal than combinations of them, or an ambiguous alternate may be unresolved in which case more info is necessary to select a prediction function
            - impact of neural network structure, algorithm, initial weights, & other parameters on the change types supported by the network according to one data set can be combined with the same graph for other data sets to create a 'net or weighted impact' or 'conditional impact' graph across different data sets that point to the same general prediction function (or parameterizations of it, or alternate/subset/combination functions) given the priority/weight represented by each data set
            - other useful attributes can be identified for the network, such as 'common core functions' of 'alternate optimal prediction functions' identified by the network
            - one prediction function might actually be a 'subset function', given some 'variable invalidating' structures present in the trained model for that data set that effectively removes some variables
            - other 'prediction function types' are relevant as other possibilities, indicating that a set of alternate prediction functions should be identified & their actual function type should be identified so they can be combined or kept separate in a function network or parameterized function representing the alternative optimals to improve the chances of finding the actual prediction function which may be a combination of those alternate functions
            - deriving the set of 'probable alternate optimal prediction functions' is useful as a way to connect the solution format with a more adjacent structure (the set of 'probable alternate prediction functions') so neural network parameters can be initialized with or calculated to have parameters likelier to converge to those 'probable alternate prediction functions' and identify the correct function(s) faster
              - in other words, find 'probable alternate prediction functions n change types away from the initial functions represented by initial network parameters if that would differentiate the alternate prediction functions faster, or n change types away from a combination of these probable prediction functions if a combination of them is expected to be useful' and then apply a neural network 'supporting those change types/amounts'
              - other functions would be useful such as 'probable median functions between alternate sub-optimal prediction functions' and 'probable weight functions prioritizing probable component functions of the probable prediction function(s)'
            - finding which input-output structures are unique in a set of 'possibly useful functions' is useful in filtering the set of functions required for a complete solution (involving the 'alternate sets of unique input-output structures')
          - other change types than the defaults supported by the network, which would be useful in various cases, like cases where two variables are probably related/correlated (but not clearly similar/different such as equal or opposite of each other), in which case when the value of one variable is calculated, the other is not directly determined by that value, so a network that has a net impact of finding an equal/opposite correlation between those variables can be ruled out
            - other variable interaction structures can be applied to make sure the network tries changes corresponding to all possible/probable variable interaction structures
            - variable correlations can have a corresponding structure of 'variables adjacently connected' such as a 'variable causing another variable with n equivalent linear operations (being n degrees of separation away)' which should be reflected in the possible/probable/guaranteed/required/optimal/implicit/explicit change types allowed between those nodes if theyre on different layers
            - nodes that represent randomness injections by randomizing their output can also offset some assumptions about change types that the network is required to support in order to converge to the correct coefficients quickly
            - identify change structures like 'weight update sequences' and 'weight paths' that cover many change types by default, like where a coefficient change can be produced by many structures of change types
            - identify change structures that cant be guaranteed or probably/adjacently produced by a network and/or data set and add their impact to the outputs during training, if theyre likely to be relevant given their input/output-connecting capacity (variables that cant be probably/adjacently derived by the inputs & neural network structures/parameters)
        - 'run-time deactivation' functionality can be built by:
          - integrating other information from prior predictions/feedback (integrating 'adjacent output values' as a filter of which nodes should be deactivated if they are producing values that will produce extreme differences between adjacent output values and the output value currently being calculated by the iteration, given weights available in the next layers that could change an extremely different value)
          - integrating requirement info, like which 'sums of change combinations' are required to generate the differences that are useful in selecting a solution
        - whichever change structures can be guaranteed to be produced by variables like weight paths can identify the error structures that those structures miss
          - if a network parameter set can produce change structures 'having a certain degree of difference', it has an error structure of 'missing the possible solutions between those different values'
          - if a network parameter set can produce change structures of 'extreme change', it has an error structure of 'producing values that may not be adjacently convertible into target outputs' such as where 'a particular node output is not convertible to the accurate output value, given the following available weights/nodes'
        - change structures such as 'incremental changes' are derivable as useful to building an accurate prediction function that involves improving a standard or base solution which is already near its optimal value
