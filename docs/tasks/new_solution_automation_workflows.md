  - add to solution automation workflows

    - find/build/derive a relevant comparable system where the answers are more clearly determined and apply rules from that system to the original problem system, finding comparable systems by which attributes are relevant, as associated with useful/determining/identifying (constant after application of various change types) attributes that the original problem system & other systems have in common (a similarity in their interaction types/structures)
      - this could be constructed (bottom-up) (build) by identifying important attributes & combining structures having those attributes in a way that doesnt invalidate the attributes (to simplify/generalize the important process so it can be measured more clearly & more easily determined to be useful to inject in the problem system in a particular position/structure) or by querying known systems (find), or derived by applying requirements of the system as filters of possible useful system structures/components/variables to fulfill the requirement intents (top-down)
      - example: if you noticed that a particularly important sub-connection in the problem system seemed to be that 'current position & a random function are inputs to the next position', you could find/build/derive the Markov chain model from this connection & apply that to verify that connection's structure
      - how would you identify your initial theory to verify, linking 'current/next position' in the first place - by applying common/core/other interface structures to determine possible/probable functions in the problem system, random being a core conceptual structure on the math interface, and the 'adjacent connection' between the 1-degree of change linking current/next position in the 'position sequence' or 'state sequence' being a 'core useful structure' on the 'system interface' as well as 'a unit structure of change' on the 'change interface'
      - once you have possible/probable connections, they can be tested by finding/building/deriving a system to verify them as mentioned above (fulfilling a core 'check/test' intent or the 'filter' intent), which helps with the 'find a prediction function' problem, solution-generating methods to which can be used to solve other problems - so this is primarily useful as a way to filter solutions to the 'find a prediction function' problem, to create prediction functions to solve general problem-solving intents like 'filter solutions' or sub-problems (like 'sub-queries of an interface query')

    - apply functions that 'create/maintain info' for intents that 'require more info' (and the opposite for intents 'requiring less info') as an alternate useful structure (the standard info format of 'info changes' as opposed to 'inputs/outputs' or 'connections/equivalences/similarities/differences/opposites') to input/output differences/sequences (as in 'a sequence of functions that add info' as a solution-finding interface query, rather than a 'sequence of input-output connections/differences'), since problems are often adjacently formatted as a 'missing info' problem, so 'functions that add info' are a particularly useful solution structure

    - apply reasons to use/not use structures as a filter of structures to use for a solution
      - example: the reason you dont want to use just rewards/costs is because if they have errors (which theyre likely to have), the solution fails
        - identify other structures that can cause the entire solution to fail if they have errors, which make a solution fragile
      - if multiple structures produce the same results, you need a structure that produces different results as a backup/alternative handler of differences, in case it has a reason to change (find the reasons a prediction function would change, or 'change causes' and apply them as a reason to add a change handler to a solution)

    - integrate other sources of certainty than 'an update function' with a solution, so it applies interface analysis to generate a specific solution generator for a problem, rather than the general solution-generator that is interface analysis used to automate problem-solving:
      
      - rules of optimization & certainty-development like:
        - build from understanding (meaning interface) first, rather than building to get understanding
          - new information & rules discovered should usually sync with previous information & rules & other interface structures, as there is rarely a discovery that requires a change to many/all known rules, and these discoveries follow patterns & rules as well (like that 'changes develop on interfaces that can contain them' and 'changes follow change patterns/rules & comply with other change interface structures like change components')
        - build in learning/update functionality at every point to allow injection of new information to correct understanding & understanding-derivation/generation/finding methods
        - build in functionality to assess the comparable reasons a solution/prediction/answer is right/wrong
          - this is to filter out solutions that have more reasons it could be wrong compared to other solutions or compared to reasons it could be right (like being 'possible' with known rules)
        - build in functionality to evaluate the probable error types & cost of those error types for any given solution, compared to other solutions, and rules to minimize error costs (like generalization, updating quickly with new information, identifying the reasons for contradictory/confirming information, identifying sources of bias & other error types, etc)
        - build in functionality to evaluate solvability & resources required to solve a problem before solving it, compared to its possible solution's value
      
      - so that the system can do other operations than just 'make a prediction of a variable interaction function', like:
        - predicting updates to their prediction function
          - by understanding that the differences in data sets & prediction functions follow patterns with varying success, and that ml is a pattern-identification tool (creating an 'intent' or 'input/output' or 'structural' match between the problem structure & a solution to resolve it), ml can be applied to predict how a particular algorithm will create a prediction function from a data set, given patterns of differences between inputs/outputs of a particular algorithm/parameters/network applied to a data set, and given how ml is applied in similar cases like with complex data sets of many variables that vary on subtle differences that humans are often unable to identify, or on understanding that humans havent built in that domain yet
          - by understanding that the types of problems humans find difficult is likely to mimic the meaning & understanding & other interface components of other systems, ml can be applied to check for patterns of those structures first as an initial filter
        - predicting error types & costs of a solution
          - understanding can generate error types of a prediction function like specificity, and these error types can be integrated into a solution with more certainty sources & types built in
        - predicting solution structures, variables, & optimal implementations
          - understanding can identify: 
            - patterns in prediction functions (which make them easier to check/generate/find/derive)
            - the standard attributes & structures of prediction functions, which can act as solution filters/structures, and the reasons why these are standard solution attributes/structures (an average is a standard solution structure of a prediction function because of the cross-system insight rule that 'variation typically occurs around a symmetry, like an average')
              - continuity, conditional subsets/alternates, averages, specificity, & probability distribution
            - interface structures of prediction functions
              - sub-interfaces, assumptions, examples, contradictions, errors, causes, concepts, intents, logic, & probable changes
            - solution structures like topologies of solution variables that have peaks at optimal solutions, or solution components/inputs/requirements
        - identifying whether a solution confirms/contradicts existing structures & other sources of certainty, to identify probability of being correct (like whether it has known contradictions or flawed assumptions or uses logical fallacies or implications rather than information)
        - check that predictions/solutions from various certainty sources match up with each other, as an additional certainty structure, given that other certainty sources are integrated & can be applied, either as solution generators/filters/verifiers
        - identify that the host system where a prediction function is being injected can be used to verify the prediction function, so if that information is available it can be used instead of the original data set

      - this applies interface analysis & specifically the understanding/meaning interface to a specific solution (as opposed to building a general solution-finding/deriving/generating/applying system implementing interface analysis)
        - so a prediction function for example would have:
          - an 'information injection' structure to integrate new information like 'online learning')
          - functionality to split itself into subset functions or alternate functions when it identifies that different functions are required
          - a prediction function to predict its own changes
          - a reason (intent/cause/meaning) for its own structures, like that it complies with other known rules or fits into other known systems
          - a set of interface structures that it uses as understanding of the variable system, like variable types, variable interaction types, and variable structures like combinations/alternates, and may apply other methods like ml or concepts like randomness/outliers/change to reduce any remaining uncertainties

    - apply other changes/differences (like 'opposites' or 'gaps') (or patterns/generative functions of changes/differences) of error or error-relevant (like 'sub-optimal solution' structures) to identify what is not a solution & differentiate from it to find solutions
      - finding the 'gaps' in sub-optimal solutions identifies spaces where optimal solutions can exist by definition
      - this provides a different target when solving a problem (a solution filter that enables adjacently identifying multiple optimal solutions, once applied) than aiming for the solution itself

    - apply specific useful system concepts like 'democracy' or 'freedom' to problem systems given their usefulness for general problem-solving intents like 'organization' (of a neural net architecture or another problem system)
      - example: apply it to the ml system to generate networks where a node can do whatever processing it wants, as long as its output doesnt contradict another node's processing functions and its output contributes to the global intent and doesnt require more default processing inputs/functions than other nodes
      - these useful structures can be found with connection sequences to problem-solving intents
        - example: for the problem-solving intent of 'organization', the connecting structure would be 'government', which organizes interface structures of a complex 'society' system such as 'info' like laws & 'functions' like law enforcement & defense
          - other structures that 'organize' interface structures could also be possible connecting structures to useful system-specific or abstract concepts related to those organizing structures
          - the same applies for other problem-solving intents than 'organization'

    - find/generate/derive & apply structures where they are determined to be useful by some structure of usefulness
      - the structures are known to fulfill an intent, given the known intent of the structure
      - the structures are/generate/derive/identify either inputs to subsequent functions, or outputs of the function itself
      - example: the combination of 'freedom' & 'interactions' is a useful structures to 'generate difference' to fulfill intents like 'resolve uncertainties' bc of the structure of usefulness 'find/generate/derive inputs' (to the 'resolve uncertainties' intent, bc if you have a way to generate differences, you can test if these differences explain the uncertain/complex system, so 'differences' are an input to that intent and anything that generates an input to that intent is a useful structure)

    - apply error structures to known standard/sub-optimal solutions to account for & correct probable errors in known standard/sub-optimal solutions (which can be bad guesses/approximations, so hardly a solution but still a useful origin structure to base changes on)

      - apply error & change structures to error structures to generate solution structures (the opposite of an error is a solution)
    
      - apply variable & error structures to interaction structures (like between input variables, interim variables like weights, or processing functions) to account for errors in their interactions, generate possible interaction structures connecting them, and generate alternative structures to filter out
        - weights are an interim structure that can be an input to the solution so they can be treated as variables to analyze on a secondary basis
        - example: 
          - apply rules of variable interactions like 'high variation variables are likely to be found together or in a connection structure, unless some distribution structure occurs between them to distribute variance' to weights, inputs, network layers, etc
          - apply error structures like 'opposite' of the correct value to existing variable values to generate altnrnates to filter or form an alternative base
      - apply change & error structures to structures of standards to generate possible solution structures if the structures of standards dont fulfill solution metrics
  
    - apply difference patterns in how solutions change with different info to:
      - identify whether the minimum info to reach a optimal solution is available
      - generate changes to existing solutions to test if their change patterns match changes in solutions with different info in a way that leads to an optimal solution

    - apply connection structures to connect useful structures so other useful structures can be found/derived/generated from input useful structures which may be more adjacnet to the problem space structures
      - example: find connection/difference patterns between structures like the following, so that either can be generated from the other:
        - requirements & vertex variables
        - efficiencies & ambiguities
        - expectations/predictions & outcomes
        - problem/solution structures
        - filter input/output (solution space & solution)
        - solution components & solutions
        - connection structures/interim states & problem/solution structures
          - applying pattern & difference-identification algorithms to interim states between problems/solutions generated by other solution functions to identify patterns that solutions typically take and connections between interim states to reduce the time to generate interim states & solutions (like weight path patterns in an ml network are an 'interim state' of the input data while its being converted to a solution), and so these interim states & solutions can be generated without the original tools used to generate them (for the same problem or other problems)

    - find combination of interface structures (like change types/sequences) that creates a topology of solution optimization that can be navigated with methods of finding global minima/maxima
      - generalization: apply other interface structures that can solve a relevant problem (like 'finding a maximum value') once info is formatted in a particular structure (like a function or topology), and convert problem system info (like possible solutions) to that input structure, optionally using interface structures (like solution filters to reduce possible solutions), as a way of applying the 'input-output sequence' structure using useful functions on various interfaces
      - example: cluster analysis to 'identify similar solutions of different types' is another example

    - different origin/inputs/data can produce a solution faster, even invalidating any update/learning process (if you have the right info, a solution may be obvious and no learning is required, as your data is optimized so no learning optimization is required)

      - apply data optimizations (pre-processing) to find/create optimal data to reduce learning requirements
        - prioritize rules that generate or form a basis for other rules, like physics rules, so any info that reveals physics rules is highly prioritized
        - this includes rules like 'apply change patterns & other change structures to data that typically produce solutions'

      - understanding based on meaning can also reduce learning requirements
        - applying interface analysis to identify useful interface structures like 'probable variable interaction structures' or 'approximate variable interaction structures' improves the probability of a successful variable interaction prediction function, which is based on the insights that reflect understanding & reduce the requirement to learn a new function with no understanding, which interacts with the meaning of these structures
          - 'probability is a relevant concept to prediction'
          - 'variable interactions follow patterns so they have corresponding probabilities'
          - 'approximation is a relevant concept to prediction'
          - 'approximations are related to predictions in the form of "minimal difference"'

      - anything that can find/derive/generate understanding or find/generate/derive optimized data can also act as an alternative to learning, understanding, or optimized data, with varying success rates
        - for example, a known solution database may act as an alternative
        - a rules database of known interactions may act as an alternative to a meaning-deriving/finding/applying/generating function
      
      - these are static alternatives so are likelier to be sub-optimal, so other structures of optimization (like generalization, or generating causes/inputs rather than original components) should be applied
        - an alternative that can update itself is by definition likelier to be optimal
          - an alternative that can find/generate/derive/apply itself is likelier to be optimal
            - an alternative that can find/generate/derive/apply components/inputs of itself is likelier to be optimal
              - an alternative that can find/generate/derive/apply components/inputs of anything (including other alternatives) is likelier to be optimal
                - an alternative that can find/generate/derive/apply components/inputs of find/generate/derive/apply functions is likelier to be optimal

      - why is it important to have alternatives to learning functions/structures?
        - bc learning structures usually use some form of reward/cost to identify information that is more valuable, given information that was previously valuable
        - but rewards/costs that are specific to solving one particular problem is how bias develops, so learning functions will inevitably produce errors some of the time

      - this is why its important to have other interface structures in place, which can identify alternatives that could contradict/neutralize/prevent bias from developing
        - this interface structure can be another function network, or a parameterized bias-assignment function that changes learning costs/rewards to avoid bias development
        - other examples:
          - change-function interface structures: function network with updated weights to indicate prioritized useful info (ml)
          - cause-function interface structures: cause-identifying/generating/deriving function, generative functions in general
          - potential-structure-logic-function interface structure: applying logic to rules about what is possible to create filter functions to filter out impossibilities in the solution space of possible solution functions

    - apply differences in problem structures as a filter of solutions (solutions to different problems have to have a correlated level of difference in the solution)
