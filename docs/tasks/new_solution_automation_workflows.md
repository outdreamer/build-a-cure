- add to solution automation workflows

    - identify useful structures automatically given their common attributes & their definition & apply as inputs to other problem/solution structures
      - example:
        - a trade-off is useful bc any change to it can produce info about a particular error type ('which priority is sacrificed when another priority is promoted'), and error structures are by definition a useful structure relevant to problem-solving

    - identify structures of useful structures that are useful for specific problem-solving intents or problem/solution core interaction functions & apply those specific structures of useful structures as an initial solution set (or solution component/input set) to solve the problem of 'designing the interface query'
      - example: structures like 'sequences' of useful structures like 'requirements, alternatives, interchangeables, equivalents' are useful for core interaction functions like 'connect' or problem-solving intents like 'connect problem/solution'

    - apply useful structures to each other (like error-neutralizing structures & interaction structures) in a useful structure (like an input-output sequence or integration structure) to generate probably successful solution structures
      - example:
        - the 'balance of power' structures present in some government structures has a default 'error-correcting' structure built-in, to prevent any one component from becoming too powerful (avoiding the error structure of 'centralized power')
        - interaction structures (like input-output sequences) are useful on their own to reduce the solution space, and connecting them to the problem/solution is a good workflow
          - this workflow can be improved by adding error-neutralizing (optimization) structures & other useful structures, in a way that allows them to be validly connected (in a way such as an input-output sequence or merged/integrated structure)

    - apply structures of useful structures and derive their interaction rules to generate a solution automtion workflow to guide the interface queries that can optimally be executed on these structures
      - example: 
        - useful structures like cost/benefit, input-output sequence, supply/demand, power/complexity/balance are alternate structures that capture a high ratio of variation
        - some interaction rules come with these useful structures, whether definitively/implicitly
          - cost/benefit analysis comes with the implied rule:
            - 'identify the cost & benefit of a move and check if the benefit is greater than the cost'
        - the interaction of these useful structures would also have interaction rules, between the structures & their defined/implied structures
        - for an interface query analyzing the power & cost/benefit of a structure to determine useful structures or solution solutions or solutions, the interactions of power & cost/benefit structures would need to be derived to guide the interface query design
          - an example interaction rule would be:
            - 'identify the structures with power in the problem system & assign them as benefits & assign responsibilities associated with power as costs, to find solutions that maximize the benefit/cost ratio'
          - this rule applies the rules implied by the cost/benefit structure and the definition of power, in a way that fulfills the 'connect a problem/solution' problem-solving intent
          - this rule is not specific to the problem, but is a workflow acting as a guide for the interface query applying these structures to 'connect a problem/solution' (a problem-solving intent of various solution automation workflows)
        - workflow fit: this process is applied specifically to structures that are useful for capturing a high ratio of variation, and applies solution automation workflows to integrate them with problem/solution structures to filter out less optimal interface queries implementing the workflow generated from these interaction rules

    - apply the cause of (reasons for) variable interactions to derive alternate probable structures of variable interactions to filter the solution space & integrate as valid alternates until any are ruled out by additional info or interface applications
      - example: 
        - for the 'find a prediction function' problem, there may be multiple reasons why the variables would interact in the patterns reflected in the data
          - some of these reasons would involve 'false correlations', 'missing variables', and other common variable interactions
          - alternate prediction functions based on these reasons for variable interactions can be used as conditional functions (called conditionally based on probability of those reasons being relevant for a given input)
          - additional interface applications can be used to filter these alternate prediction functions further, like:
            - applying the 'change' interface to find out if these alternate functions are essentially interchangeable, or if they can functions as bases of each other, or have other function interaction structures
            - applying the 'pattern' interface to filter for more probable variable interaction structures

    - derive the info that is capable of producing the info of the solution & find that info
      - example: 
        - for the 'find a prediction function' problem, info that can produce the 'prediction function' includes info like 'similarities between independent/dependent variables' and 'conversion functions to create similarities'
        - once you know what info is required to produce (of capable of producing) the solution, the interface query can be designed to produce that info
          - example of an interface query subset:
            - 'find similarities between indpendent/dependent variables'
              - 'find methods to identify similarities between values'
                - 'find methods to identify patterns of similarities between values'
                - 'find methods to identify equivalents/differences between values'

    - find the symmetries in a problem system based on which components are interchangeaable & identify the interaction level where they exist & the symmetry theyre based on, then find a conversion function to connect those symmetries to understand a problem system quickly 
      - understanding allows identification of required components to optimize the system & alternate interaction methods between components
      - example: 
        - in the 'find a prediction function' problem, symmetries exist around an 'average' function, between 'base' functions, between prediction function requirements/limits & the prediction function, between function patterns & the prediction function, between variable interaction patterns & the prediction function, between data & the prediction function, between 'subset' functions & the 'composite' function
        - these symmetries provide alternate ways to find the prediction function, with alternate adjacent operations connecting objects formed by changes in each symmetry

    - identify required structures necessary for a solution to be adjacently optimizable, such as a 'contradiction' structure in the form of an 'opposite solution' to identify when a solution is nearer to its opposite structure ('not a solution', as in an 'error'), and build those structures instead of the original problem's solution structures, then apply optimization structures once its in an adjacently optimizable state

    - identify structures required to identify useful structures (solutions, optimizations) in every structure of structures (combination/group/mix/sequence of structures) in any system & apply as default/core/component/interaction/input/reference structures or other useful problem-solving structures
      - example of structures that can identify optimal structures in many cases on their own:
        - 'sets having just a few attributes like consistency & stability'
        - useful structures like 'alignments between change types'
        - 'priority combinations that create consistency or other optimization structures' in a system
      - an example of a way to apply this is identifying whether a solution is moving toward useful structures standardized to that system or away from them
        - this means finding the structures of 'consistency' & other useful structures in a system and identifying whether a solution is more similar or less similar to that structure as its changed, as a way to find optimal solutions in that system

    - identify change causes (like benefits/costs) and apply change functions to those change causes (like applying 'distribute' to 'benefits' to 'change their position') & check if a solution is produced by the changes, bc if the original problem system produced an error, a probable cause is that the rewards/punishments/other change causes were in incorrect positions/amounts/structures
      - generalization: find other 'probable causes of error/solution structures' that can act in place of 'known error/solution causes', as well as other variations of important attributes like solution success cause such as solution/error inputs/defaults/variables/requirements, including:
        - patterns/approximations of error/solution structures
      - alternate generalization: find other functions that when applied to other structures, can produce solutions
        - in general, this solution automation workflow is based on the insight that an 'error is a difference from a solution', but there are many ways to apply that insight to generate workflows involving identifying/correcting errors or generating solutions from them
      - related workflow: a related workflow is 'apply changes to see what you can convert a structure into, bc if its adjacently convertible into an item, its similar to that other item in some way, as a method of generating an identification function ("identify" being a core problem/solution interaction function)

    - identify other structures like bases (such as errors, standards, differences) that a solution or other useful structure can use as a reference like a limit to define itself & apply those to generate the solution/useful structure

    - identify different interaction structures of structures that are particularly useful & apply them as default/core/component/interaction/input/reference structures or other useful problem-solving structures
      - an example interaction structure is an 'overlap of different structures of usefulness', like an overlap of optimizations/cooperation/stability/certainty/consistency structures
      - a structure that can act like many other structures (abstractions, structures of commonness/reusability, base structures)

    - identify structures of success (cooperation/consistency/stability) & other useful structures (like overlap structures) between useful structures (like overlaps between optimization structures & structures of certainty), which are more useful than structures of any one type of usefulness on its own & apply them as default/core/component/interaction/input/reference structures or other useful problem-solving structures
      - a 'consistent system where different change types interact in way that allows them to coexist' is likelier to survive & be stable

    - identify the reasons that a structure might be misidentified as an error by another structure, and apply as contradiction/opposite structures to optimize an error identification function that can automate finding/generating/deriving solutions by identifying what is not an error
      - reasons like:
        - 'its not similar to previous success/solution structures, like a previously successful priority'
        - 'its not similar to our structures'
      - example:
        - a small group might be misidentified as an error by a large group, even though all necessary changes/improvements might start at a unit size
      - this is an example of bias (an incorrect constant rule/function) that comes from rewarding itself & its own perspective, without acknowledging the possibility of another path to success

    - apply error structures as components/inputs/other useful structures of solutions, either as filters of solutions, adjacent states to solutions that can be inputs to approximation structures ('partially/almost correct', as opposed to 'irrelevant'), or alternate states of solutions that are conditionally correct in a different context

    - apply useful structures (like solution success causes, useful structure filters, generative functions, patterns of alternate routes) to find/derive/generate useful structures, apply them to generate/find/derive useful structures, and apply those useful structures to fulfill problem-solving intents

    - apply useful structures like 'similarities' to fulfill core interaction functions like 'connect' for relevant problem/solution structures like 'problem inputs & solution outputs'
      - example: a 'complex' system has structures that can function adjacently as 'inputs' to 'complexity', like 'multiple alternatives/options', 'multiple variables/differences', 'contradictions', 'conditions', 'conflicts'
        - there are 'similarities' between these structures and structures of complexity, so 'similarities' can be used to fulfill common problem-solving sub-intents like 'generate outputs from an input' or 'identify inputs of an output'
        - 'similarities' are also useful for core interaction functions of problem-solving like 'connect' bc they reduce the work required to connect different structures
      - this can be generalized to other useful structures that adjacently fulfill core interaction functions (like connect) between relevant problem/solution structures

    - apply useful interface structures like the physics/info interface to filter out possible info or other interface structures that can be used to find/build/derive solutions
      - only some info is possible given how physics works (specifically how energy can stabilize into structure), and only some info interactions are possible (such as how charged particles can interact like 'neutralizing each other' or 'repelling/attracting each other')
      - these physics rules can apply to info interactions, like calculating the certainties generatable with inputs/functions (determining what info can be produced with other info)
      - example: can a boolean variable with this distribution determine a type variable with this distribution, with x available functions & a limit of n function calls?
      - this can be generalized to other interface structures like combinations/applications of interfaces that have functions useful for fulfilling connection/filtering/reducing or other interaction functions
        - physics is a particularly powerful interface impacting all the others, and the info interface has 'certainty' structures defined on it by default, so thats why a combination of these interfaces is useful for finding functions connecting certainty structures that can be used to filter possible connections in a problem space
      - the generalization of this is 'find the interface structure of interfaces that can determine specific useful structures (like "interaction rules of certainty structures") that can be used to solve a particular problem, given the definition of these interfaces which includes their core structures & which interface structures of interfaces would make finding interactions between those structures adjacent/trivial, and apply these to fulfill core interaction functions between problem/solution structures
        - only some interface structures can adjacently determine the useful structures that can solve any problem (like certainty structures or difference structures), which is why this is different from generally applying interface analysis - this is specifically applying it to find interaction functions between these useful structures that can be plugged in as inputs to core interaction functions to fulfill problem-solving intents
        - this is a similar operatino to 'finding a different interaction layer/structure that highlights a particular interaction' (as opposed to just 'finding a space where a difference is maximized')

    - find patterns/functions connecting generally useful (such as 'reusable' as in 'cross-interface') input/output & core interaction function (like 'connection') structures (like 'certainty structures') & apply them as filters of possible interaction & output solution structures for an input problem (or input structures for a given interaction & output structures, etc)
      - example: some inputs & functions can only produce output certainty structures with a particular attribute (number, type), and these interactions can be used to determine which inputs/connecting functions can be used to produce a given solution output, providing a filter of possible solution-generating functions

    - identify attributes of a problem that indicate a particular solution automation workflow is optimal for that problem, and find base problems with different optimal solution automation workflows, and use these base problems to determine how different an original problem is from the nearest base problem, and apply differences to that base problem's solution to find a solution to the original problem quickly
      - the only problem space where you would need to apply 'trial & error' is where the outputs are perfectly random (so only when trying to solve the problem of 'find all the values in a perfectly random sequence') 
      - every other problem has useful structures (such as patterns) that allow the problem to be solved more efficiently
      - solutions can be designed by applying differences to the solutions of this problem where the solution of 'trial & error' is required, differences in solutions that match the differences in problem structures
      - example:
        - if a particular random function has known error structures making it non-random, those error structures can be used to reduce the sections of the sequence that are considered actually random, thus reducing the number of values that need to be checked with 'trial & error' (if an error structure in a particular random function is an occasional non-random sequence with identifiable attributes, those sequences can be checked for & their bounds can be checked, invalidating the need to check every value in the error sequence section, reducing the total number of operations)
          - so the 'non-random error structures' can be used to reduce the search/solution space of 'random sequences to apply trial & error to'
          - this solution requires a structure (like a function) to handle 'finding the error structures & removing them from the search/solution space'
          - this matches the difference in the problem space (meaning "an occasional 'non-random error structure' in the sequence") with a corresponding difference in the solution (a 'function to differentiate these error structures from random sequences to reduce overall value checks'), fulfilling the solution metric of 'a solution with fewer than n operations (such as value checks), n being the size of the sequence'
          - there are many ways to fulfill this solution metric, with:
            - varying trade-offs of other solution metrics like accuracy, depending on the method used to fulfill the solution metric
            - varying applicability, in terms of which methods are useful, given a particular input or problem space (like a known error in an imperfect randomness function)
          - methods of fulfilling this metric include:
            - finding patterns in the sequence (such as sub-sequences of length k that have a relatively even distribution of possible values) approximating/predicting answers rather than checking them, based on which sub-sequences of length k would have an even distribution of possible values
              - identifying sub-sequences indicating randomness/non-randomness & their probability of occurring with frequency x in a random sequence, and reducing the search/solution space by identifying these sequences & applying their bounds to avoid checking every value (whats the likelihood of '111111' vs. '111151' in a random sequence with a particular random function implementation, and to what extent does each value need to be checked to assume its one sequence vs. the other)
              - applying the same method, but applying it only to a subset of values (checking some values & predicting others)
            - finding error structures in the random function & applying error structures to reduce the solution space
            - feeding the sequence into a function that requires a random sequence to avoid replication of work, and which produces some structure of certainty like a processing halt or error response when it finds a non-random sequence
            - replacing with a generated random sequence, since the problem definition only requires that values in a random sequence be identified, and if a sequence is really random, it wont matter which random sequence is used
            - mapping common/all/probable/average random sequence patterns and once a sequence is identified as belonging to a certain type or group of random sequence patterns, apply a conversion/generative function between the sequences in that type/group to tell which sequence it is likeliest to be, without checking all the values
              - similarly, organizing a space of random sub-sequences where adjacence is determined by probability of occurring in the sequence at all, or next/previously in the sequence, and checking endpoint values rather than every value, to see if a continuous path in the space can be traversed

    - identify & apply patterns of a particular problem space's variables (like 'input context', 'solution-finding method params', 'variable interaction structures', 'problem/solution structures') as inputs to data/function optimizations
      - example:
        - apply patterns of differences between network params & interface structures like variable interactions (such as 'adjacent features') & variable types (like 'type' variables) it can identify, to generate the right network params to identify the maximum number of relevant structures & not identify irrelevant structures
        - apply patterns of differences between worst/best/average/other/outlier/alternate/noise/error case contexts to identify if a data set is a 'worst case' or 'average case' or 'best case' context, to generate the right differences to identify other cases to weight it against
        - apply patterns of differences between an original problem & a useful system to apply rules/structures from to find relevant useful systems to apply rules/structures from to solve the original problem

    - apply specific examples of other specific but generalizable problem formats with known solution methods having useful connection structures to the original problem, like similar inputs/outputs or similar problem-solving intents like 'reduce' (such as 'find a prediction function', 'find a sort algorithm of a sequence to find a particular value quickly'), to apply as sources of solution formats & methods for the original problem, converting the specific problem/solution format to the original problem & applying solution methods from the specific problem to solve the original problem once converted to specific problem structures

      - example: 'sorting algorithms' applied to the 'find a prediction function' problem could take the form of 'starting the search in the middle' which would take the form of the 'average/regression line of the data set' or 'applying two sorts in parallel starting from different positions' which would take the form of 'alternate prediction functions from maximally different bases or standard functions' when converted back to the original problem 

      - these specific problems have solutions that are generalizable to other systems
        - 'sorting' structures can be used in place of 'filter/test' structures bc they have similar inputs/outputs
        - just like an 'average' structure or a 'dimension-reducing' or 'feature-selecting' method can 'reduce' the problem of 'find a prediction function'

      - the generalization of this is 'find the interface structures that are relevant to solving problems & connect them to problem-solving structures', with examples like:
        - 'specific function' ('abstract' variable as 'specificity' applied to 'function' interface structure), which is useful for solution workflows involving 'finding specific functions that all problems can be converted to'
        - 'function cause' (or 'function inputs', like 'function requirements/assumptions/variables/triggers'), which is useful for solution workflows involving 'finding a generative function of a function to identify & solve problem cause'
        - these structures are particularly useful for solving problems bc they involve inherently useful interface structures (functions) which is a format all problems can be converted to, and apply useful attributes to those useful objects (like 'cause' or 'abstraction') to identify other useful structures that would be useful for solving problems, given that a function is already known to be a useful structure for solving problems

    - identify patterns in limits on what info a function can find/derive/generate & apply those to rule out possible solution-finding functions to filter other functions that shouldnt be applied, or apply differences to those limited functions to find functions that can find/derive/generate the info

    - a generalization of my invention is 'abstracting a problem/solution in a structural way', thereby connecting the 'abstract' & 'structura' interfaces, the abstract interface being useful for identifying types, concepts, patterns & meaning, and the structure interface useful for identifying connections/similarities/differences, so their link is useful for finding connections/similarities/differences between definitions, types, meaning & other abstract structures
      - other interface structures combining/integrating other interfaces to each other are useful for other problem-solving intents, but this is a particularly useful structure (specifically a 'connection' structure) of interfaces
      - the reason why interfaces are useful is bc having the attribute of 'flexibility' in the form of 'multiple alternate formats' of info (multiple ways to format the same input info) that dont lose relevant information like interactions, makes it possible to use different functions and compare them more easily to other structures that another structure may already be in, making some comparisons (or other adjacent useful core operations) more trivial than others
      - interfaces can be formatted as a symmetry, filter, or standard depending on the interface applied to its definition

    - generalize your method of finding a specific useful structure (like lattice multiplication uses) to find a solution to a specific problem (like the multiplication problem), rather than relying on standard useful interface structures to easily compare/connect/reduce/organize problem/solution structures
      - how would you find a specific useful structure to solve a specific problem, such as the structure used in lattice multiplication or a tensor (matrix of matrices) used to solve the problem of 'determining differences between objects'?
        - find a structure that makes the required info obvious (structural)
          - a structure that makes the required info structural is one in which the required info would have an obvious structure (a shape, area, position, difference, etc) that is different from other structures possible in that space
            - a tensor is useful for finding equal/different matrixes bc it involves creating the different configurations of the matrix, which allows them to be easily compared, once the various different matrixes are standardized to the same format (matrix)
              - other structures like the inverse/diagonal of a matrix are included in this structure, but those structures arent the core structures aligned/differentiated/comparable by this format
            - the lattice is useful in multiplication bc it aligns relevant numbers by position (the 'digit' math structure), standardizing their position & allowing combination of similarly or equivalently positioned values
              - there is other info in this structure (like pairs of adjacent numbers), but the individual digit positions themselves are aligned, making each position's differences in value comparable, rather than making other numerical structures comparable
          - the common factor is that the standardization reduces other differences to allow the important difference to be obvious, where a similarity (like digit position or matrix row/column numbers) acts as a symmetry where the differences (in digit position value or row/column value) can be easily compared
          - so finding a structure useful for solving a problem like 'compare to find useful structures like differences/equivalences' can be reduced to 'find similarities that can act as symmetries in the problem/space, on which the important difference will be structural, & clearly different from other difference types'
          - 'finding differences/equivalences' is a structural problem-solving intent common to many solution automation workflows, and is only a sub-intent of the example problems given above, but this same method can be applied to find other specific useful structures to solve other specific problems
          - this specific method of 'find a similarity that can act as a symmetry (sub-interface)' as a solution to the problem of 'finding equivalences/differences' can be generalized to other useful structures than symmetries (like 'filters') but how would you match a 'symmetry' with a 'find equivalence/difference' problem, as a particularly useful structure?
            - the functions possible with a 'symmetry' ('find inputs that create changes' and 'find inputs that dont create changes') can connect the 'find equivalence/difference' problem input with a solution output (equivalences/differences), given the definition of equivalence (lack of change) & difference (change) standardized to the change interface (the 'change' definition route of an equivalence/difference)
            - so this workflow should include a step to 'find the important useful structures, using useful interface structures like definitions, inputs/outputs, & differences to final solution format to determine relevance/usefulness of structures' that can be an input to the function 'solving the specific problem' that make the problem-solving function's logic trivial to structure or finalize (finding a function to 'connect a symmetry and an equivalence' is more trivial than 'connect the original problem system to an equivalence')

    - find/build/derive a relevant comparable system where the answers are more clearly determined and apply rules from that system to the original problem system, finding comparable systems by which attributes are relevant, as associated with useful/determining/identifying (constant after application of various change types) attributes that the original problem system & other systems have in common (a similarity in their interaction types/structures)
      - this could be constructed (bottom-up) (build) by identifying important attributes & combining structures having those attributes in a way that doesnt invalidate the attributes (to simplify/generalize the important process so it can be measured more clearly & more easily determined to be useful to inject in the problem system in a particular position/structure) or by querying known systems (find), or derived by applying requirements of the system as filters of possible useful system structures/components/variables to fulfill the requirement intents (top-down)
      - example: if you noticed that a particularly important sub-connection in the problem system seemed to be that 'current position & a random function are inputs to the next position', you could find/build/derive the Markov chain model from this connection & apply that to verify that connection's structure
      - how would you identify your initial theory to verify, linking 'current/next position' in the first place - by applying common/core/other interface structures to determine possible/probable functions in the problem system, random being a core conceptual structure on the math interface, and the 'adjacent connection' between the 1-degree of change linking current/next position in the 'position sequence' or 'state sequence' being a 'core useful structure' on the 'system interface' as well as 'a unit structure of change' on the 'change interface'
      - once you have possible/probable connections, they can be tested by finding/building/deriving a system to verify them as mentioned above (fulfilling a core 'check/test' intent or the 'filter' intent), which helps with the 'find a prediction function' problem, solution-generating methods to which can be used to solve other problems - so this is primarily useful as a way to filter solutions to the 'find a prediction function' problem, to create prediction functions to solve general problem-solving intents like 'filter solutions' or sub-problems (like 'sub-queries of an interface query')
      - this can be generalized to apply to other problems which, when a solution-finding method is found for those problems, the solution-finding method can be used to solve most/all other problems, bc the problem is a format that other problems can be standardized to
        - general structures like 'connect problem/solution' or 'reduce problem' or 'break problem into sub-problems & merge sub-solutions' are general workflows that offer structures that other problems can be standardized to
        - however, some specific problems can also be useful formats that other problems can be standardized to, bc of the definition/relevance of their related components, like the 'find a prediction function' problem
        - given the definition of a 'prediction', the solution-finding methods of the 'find a prediction function' problem are inherently useful to solving other problems, as an alternate problem-solving method to more general or specific methods, bc once you have a prediction function, you have a way to 'connect inputs/outputs', which is a general solution automation workflow intent
        - other functions that produce a function to solve a general solution automation workflow intent can also be general solution-finding methods that can be alternatives to other problem-solving methods

    - apply functions that 'create/maintain info' for intents that 'require more info' (and the opposite for intents 'requiring less info') as an alternate useful structure (the standard info format of 'info changes' as opposed to 'inputs/outputs' or 'connections/equivalences/similarities/differences/opposites') to input/output differences/sequences (as in 'a sequence of functions that add info' as a solution-finding interface query, rather than a 'sequence of input-output connections/differences'), since problems are often adjacently formatted as a 'missing info' problem, so 'functions that add info' are a particularly useful solution structure

    - apply reasons to use/not use structures as a filter of structures to use for a solution
      - example: the reason you dont want to use just rewards/costs is because if they have errors (which theyre likely to have), the solution fails
        - identify other structures that can cause the entire solution to fail if they have errors, which make a solution fragile
      - if multiple structures produce the same results, you need a structure that produces different results as a backup/alternative handler of differences, in case it has a reason to change (find the reasons a prediction function would change, or 'change causes' and apply them as a reason to add a change handler to a solution)

    - integrate other sources of certainty than 'an update function' with a solution, so it applies interface analysis to generate a specific solution generator for a problem, rather than the general solution-generator that is interface analysis used to automate problem-solving:
      
      - rules of optimization & certainty-development like:
        - build from understanding (meaning interface) first, rather than building to get understanding
          - new information & rules discovered should usually sync with previous information & rules & other interface structures, as there is rarely a discovery that requires a change to many/all known rules, and these discoveries follow patterns & rules as well (like that 'changes develop on interfaces that can contain them' and 'changes follow change patterns/rules & comply with other change interface structures like change components')
        - build in learning/update functionality at every point to allow injection of new information to correct understanding & understanding-derivation/generation/finding methods
        - build in functionality to assess the comparable reasons a solution/prediction/answer is right/wrong
          - this is to filter out solutions that have more reasons it could be wrong compared to other solutions or compared to reasons it could be right (like being 'possible' with known rules)
        - build in functionality to evaluate the probable error types & cost of those error types for any given solution, compared to other solutions, and rules to minimize error costs (like generalization, updating quickly with new information, identifying the reasons for contradictory/confirming information, identifying sources of bias & other error types, etc)
        - build in functionality to evaluate solvability & resources required to solve a problem before solving it, compared to its possible solution's value
      
      - so that the system can do other operations than just 'make a prediction of a variable interaction function', like:
        - predicting updates to their prediction function
          - by understanding that the differences in data sets & prediction functions follow patterns with varying success, and that ml is a pattern-identification tool (creating an 'intent' or 'input/output' or 'structural' match between the problem structure & a solution to resolve it), ml can be applied to predict how a particular algorithm will create a prediction function from a data set, given patterns of differences between inputs/outputs of a particular algorithm/parameters/network applied to a data set, and given how ml is applied in similar cases like with complex data sets of many variables that vary on subtle differences that humans are often unable to identify, or on understanding that humans havent built in that domain yet
          - by understanding that the types of problems humans find difficult is likely to mimic the meaning & understanding & other interface components of other systems, ml can be applied to check for patterns of those structures first as an initial filter
        - predicting error types & costs of a solution
          - understanding can generate error types of a prediction function like specificity, and these error types can be integrated into a solution with more certainty sources & types built in
        - predicting solution structures, variables, & optimal implementations
          - understanding can identify: 
            - patterns in prediction functions (which make them easier to check/generate/find/derive)
            - the standard attributes & structures of prediction functions, which can act as solution filters/structures, and the reasons why these are standard solution attributes/structures (an average is a standard solution structure of a prediction function because of the cross-system insight rule that 'variation typically occurs around a symmetry, like an average')
              - continuity, conditional subsets/alternates, averages, specificity, & probability distribution
            - interface structures of prediction functions
              - sub-interfaces, assumptions, examples, contradictions, errors, causes, concepts, intents, logic, & probable changes
            - solution structures like topologies of solution variables that have peaks at optimal solutions, or solution components/inputs/requirements
        - identifying whether a solution confirms/contradicts existing structures & other sources of certainty, to identify probability of being correct (like whether it has known contradictions or flawed assumptions or uses logical fallacies or implications rather than information)
        - check that predictions/solutions from various certainty sources match up with each other, as an additional certainty structure, given that other certainty sources are integrated & can be applied, either as solution generators/filters/verifiers
        - identify that the host system where a prediction function is being injected can be used to verify the prediction function, so if that information is available it can be used instead of the original data set

      - this applies interface analysis & specifically the understanding/meaning interface to a specific solution (as opposed to building a general solution-finding/deriving/generating/applying system implementing interface analysis)
        - so a prediction function for example would have:
          - an 'information injection' structure to integrate new information like 'online learning')
          - functionality to split itself into subset functions or alternate functions when it identifies that different functions are required
          - a prediction function to predict its own changes
          - a reason (intent/cause/meaning) for its own structures, like that it complies with other known rules or fits into other known systems
          - a set of interface structures that it uses as understanding of the variable system, like variable types, variable interaction types, and variable structures like combinations/alternates, and may apply other methods like ml or concepts like randomness/outliers/change to reduce any remaining uncertainties

    - apply other changes/differences (like 'opposites' or 'gaps') (or patterns/generative functions of changes/differences) of error or error-relevant (like 'sub-optimal solution' structures) to identify what is not a solution & differentiate from it to find solutions
      - finding the 'gaps' in sub-optimal solutions identifies spaces where optimal solutions can exist by definition
      - this provides a different target when solving a problem (a solution filter that enables adjacently identifying multiple optimal solutions, once applied) than aiming for the solution itself

    - apply specific useful system concepts like 'democracy' or 'freedom' to problem systems given their usefulness for general problem-solving intents like 'organization' (of a neural net architecture or another problem system)
      - example: apply it to the ml system to generate networks where a node can do whatever processing it wants, as long as its output doesnt contradict another node's processing functions and its output contributes to the global intent and doesnt require more default processing inputs/functions than other nodes
      - these useful structures can be found with connection sequences to problem-solving intents
        - example: for the problem-solving intent of 'organization', the connecting structure would be 'government', which organizes interface structures of a complex 'society' system such as 'info' like laws & 'functions' like law enforcement & defense
          - other structures that 'organize' interface structures could also be possible connecting structures to useful system-specific or abstract concepts related to those organizing structures
          - the same applies for other problem-solving intents than 'organization'

    - find/generate/derive & apply structures where they are determined to be useful by some structure of usefulness
      - the structures are known to fulfill an intent, given the known intent of the structure
      - the structures are/generate/derive/identify either inputs to subsequent functions, or outputs of the function itself
      - example: the combination of 'freedom' & 'interactions' is a useful structures to 'generate difference' to fulfill intents like 'resolve uncertainties' bc of the structure of usefulness 'find/generate/derive inputs' (to the 'resolve uncertainties' intent, bc if you have a way to generate differences, you can test if these differences explain the uncertain/complex system, so 'differences' are an input to that intent and anything that generates an input to that intent is a useful structure)

    - apply error structures to known standard/sub-optimal solutions to account for & correct probable errors in known standard/sub-optimal solutions (which can be bad guesses/approximations, so hardly a solution but still a useful origin structure to base changes on)

      - apply error & change structures to error structures to generate solution structures (the opposite of an error is a solution)
    
      - apply variable & error structures to interaction structures (like between input variables, interim variables like weights, or processing functions) to account for errors in their interactions, generate possible interaction structures connecting them, and generate alternative structures to filter out
        - weights are an interim structure that can be an input to the solution so they can be treated as variables to analyze on a secondary basis
        - example: 
          - apply rules of variable interactions like 'high variation variables are likely to be found together or in a connection structure, unless some distribution structure occurs between them to distribute variance' to weights, inputs, network layers, etc
          - apply error structures like 'opposite' of the correct value to existing variable values to generate altnrnates to filter or form an alternative base
      - apply change & error structures to structures of standards to generate possible solution structures if the structures of standards dont fulfill solution metrics
  
    - apply difference patterns in how solutions change with different info to:
      - identify whether the minimum info to reach a optimal solution is available
      - generate changes to existing solutions to test if their change patterns match changes in solutions with different info in a way that leads to an optimal solution

    - apply connection structures to connect useful structures so other useful structures can be found/derived/generated from input useful structures which may be more adjacnet to the problem space structures
      - example: find connection/difference patterns between structures like the following, so that either can be generated from the other:
        - requirements & vertex variables
        - efficiencies & ambiguities
        - expectations/predictions & outcomes
        - problem/solution structures
        - filter input/output (solution space & solution)
        - solution components & solutions
        - connection structures/interim states & problem/solution structures
          - applying pattern & difference-identification algorithms to interim states between problems/solutions generated by other solution functions to identify patterns that solutions typically take and connections between interim states to reduce the time to generate interim states & solutions (like weight path patterns in an ml network are an 'interim state' of the input data while its being converted to a solution), and so these interim states & solutions can be generated without the original tools used to generate them (for the same problem or other problems)

    - find combination of interface structures (like change types/sequences) that creates a topology of solution optimization that can be navigated with methods of finding global minima/maxima
      - generalization: apply other interface structures that can solve a relevant problem (like 'finding a maximum value') once info is formatted in a particular structure (like a function or topology), and convert problem system info (like possible solutions) to that input structure, optionally using interface structures (like solution filters to reduce possible solutions), as a way of applying the 'input-output sequence' structure using useful functions on various interfaces
      - example: cluster analysis to 'identify similar solutions of different types' is another example

    - different origin/inputs/data can produce a solution faster, even invalidating any update/learning process (if you have the right info, a solution may be obvious and no learning is required, as your data is optimized so no learning optimization is required)

      - what info is stored in memory matters, and the learning rules used to update it or the alternatives like understanding that can be used to replace it, but the format in which info is stored also matters
        - info stored in a non-findable way is not useful, just like repeated info is not useful
        - this can be thought of as the corrollary (or pre-requisite to/invalidator of) of 'input data optimizations', as 'output data optimizations' before storage, so that info is stored in a way that maximizes its usefulness for future uses/intents
        - 'data processing optimizations' is another position where optimization structures can be added to optimize other intents like learning/understanding/finding

      - apply data optimizations (pre-processing) to find/create optimal data to reduce learning requirements
        - prioritize rules that generate or form a basis for other rules, like physics rules, so any info that reveals physics rules is highly prioritized
        - this includes rules like 'apply change patterns & other change structures to data that typically produce solutions'

      - understanding based on meaning can also reduce learning requirements
        - applying interface analysis to identify useful interface structures like 'probable variable interaction structures' or 'approximate variable interaction structures' improves the probability of a successful variable interaction prediction function, which is based on the insights that reflect understanding & reduce the requirement to learn a new function with no understanding, which interacts with the meaning of these structures
          - 'probability is a relevant concept to prediction'
          - 'variable interactions follow patterns so they have corresponding probabilities'
          - 'approximation is a relevant concept to prediction'
          - 'approximations are related to predictions in the form of "minimal difference"'

      - anything that can find/derive/generate understanding or find/generate/derive optimized data can also act as an alternative to learning, understanding, or optimized data, with varying success rates
        - for example, a known solution database may act as an alternative
        - a rules database of known interactions may act as an alternative to a meaning-deriving/finding/applying/generating function
      
      - these are static alternatives so are likelier to be sub-optimal, so other structures of optimization (like generalization, or generating causes/inputs rather than original components) should be applied
        - an alternative that can update itself is by definition likelier to be optimal
          - an alternative that can find/generate/derive/apply itself is likelier to be optimal
            - an alternative that can find/generate/derive/apply components/inputs of itself is likelier to be optimal
              - an alternative that can find/generate/derive/apply components/inputs of anything (including other alternatives) is likelier to be optimal
                - an alternative that can find/generate/derive/apply components/inputs of find/generate/derive/apply functions is likelier to be optimal

      - why is it important to have alternatives to learning functions/structures?
        - bc learning structures usually use some form of reward/cost to identify information that is more valuable, given information that was previously valuable
        - but rewards/costs that are specific to solving one particular problem is how bias develops, so learning functions will inevitably produce errors some of the time

      - this is why its important to have other interface structures in place, which can identify alternatives that could contradict/neutralize/prevent bias from developing
        - this interface structure can be another function network, or a parameterized bias-assignment function that changes learning costs/rewards to avoid bias development
        - other examples:
          - change-function interface structures: function network with updated weights to indicate prioritized useful info (ml)
          - cause-function interface structures: cause-identifying/generating/deriving function, generative functions in general
          - potential-structure-logic-function interface structure: applying logic to rules about what is possible to create filter functions to filter out impossibilities in the solution space of possible solution functions

    - apply differences in problem structures as a filter of solutions (solutions to different problems have to have a correlated level of difference in the solution)
