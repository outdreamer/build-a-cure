  - example: identify common successful sub-interface queries (which frequently involve useful structures like 'core' interface queries like 'find differences') and combine them or drop them in at rnadom whenever an interface query is suboptimal, randomness being a useful testing tool and a useful method when no information about relative optimals is available
  
  - example: find alternate structures that preserve info as alternate problem/solution formats as an initial rule to apply in interface queries to check if the problem is adjacently solvable in a generatable alternate format preserving the information (like a data set with the same statistics and a similar distribution but which has some outliers handled by a separate function)

  - example: "applying 'limits' as an 'input' structure to a function that applies 'differences' to those limits to 'generate possible structures within the limits'" is an alternate intent to 'generate possible structures by applying variable combinations, then test if each one is within a limit by finding the difference from the structure to the limit'
  - identifying these as alternate useful structures involves changing the 'direction' of the interaction of the structures:
    - one generates limits first and applies differences within those limits to generate possible structures
    - one generates possible structures first and filters them by limits to find the reduced set of possible structures within limits
  - but both have the same or similar outputs (the first successful structure or the set of structures that pass the test)

  - example: identify relevant difference structures (difference that are related in some way, like inputs/outputs, adjacent changes like opposites or one attribute change like an angle change, etc) as opposed to irrelevant difference structures like randomness to find useful differences to apply, such as for intents like 'correcting a corrupted data set', 'finding relevant change types to include in a filtered data set', or 'generating a similar but more solvable data set' (like by finding adjacent patterns and standardizing to those patterns, which are likelier to describe the data than randomness), as differences are useful for common intents like 'identify errors', 'identify useful differences', 'identify similarities', 'compare structures', and 'identify variables'
  - randomness optimizes its usefulness in cases such as where no information is available, to test a structure for stability in unpredictable/uncertainty circumstances like a randomized variable being applied, or where there is no reason to add structure to the variable (where a random value doesnt create unacceptable errors for a system that can correct those errors)
  - identifying which variables like randomness vs. similar (adjacent) differences are similar to each other can identify where a variable would have the opposite effect, where it can be replaced with an adjacent substitute, where its value is unique, and where its value is optimized
  - applying randomness can identify a 'hidden interface' error structure (using queries like 'apply trial and error to all possible attribute combinations that are not in a known structure'), but so can finding the 'similarities in differences' which point to the similarity (interface structure) those differences are based on
  - to derive the 'electricity' interface, a 'similarity' can be found in the 'energy requirement of all biological structures' indicating that 'structures whose energy mechanism isnt known still have one, its just not visible' and in the 'preservation of energy (through heat transfer)' which indicates a hidden interface that is 'not visible in most cases like when heat emanates', implying that if energy transfer was isolated, it could be controlled, and other mechanisms than just energy were relevant (electricity and related concepts like charge determining interactions of core components of visible structures)
  - alternatively, random combinations of interface structures like attributes can be tested for relevance, like a combination of structures like attributes such as 'energy' and 'core components' or 'energy' and 'input-output sequences' enabling 'energy transfer', as prior to identifying electricity, 'energy dynamics (rules governing energy)' werent completely known, and interface structures like 'limits', 'usage', and 'time' can be used to infer relevant energy concepts like 'energy storage' which can fill in the missing info about energy or identify what info is missing

- identify that useful structures like 'connections' can be derived between 'error types (like "false similarities") connecting solutions/errors' and the 'filters used to check for structures indicating an error or solution' which can be connected with functions like 'find the most relevant (such as most variation-separating) filters of a structure' or 'most difference-creating differences of a structure' to create this connection structure, which can be used to derive filters to resolve the 'false similarity' error (identify solution/error) from the input 'error type' structure
  - example: identify that error structures can look like a solution structure if other structures are present, such as the 'relative inefficiency or improbability of correcting the error', or 'incomplete error correction, such as where only some examples of the error are corrected' or 'errors that havent reached a threshold value that would trigger error-correction function application or the development of error-correction functions', which is a 'false similarity' that can be resolved with tests/filters such as 'intentionally re-triggering the error to see if a correction occurs at a particular scale and if not, it could be a solution structure, like a valid function to use as a solution component'
  - the 'connection between the "false similarity" of these solution/error structures and the tests which can detect the "false similarity"' can be derived as a useful structure to 'differentiate errors/solutions'
  - identifying the filters that would identify an error/solution in a set where there are false similarities between errors/solutions involves deriving structures that can create differences such as 'triggering errors at scales likely to trigger error-correcting limit functions if they exist to handle the error, which is a sign that its an error', which adds a 'difference structure' in the 'scale/number of errors'

- identify useful structures like 'precursors to certainty structures' or 'certainty-adjacent structures' which can be useful for identifying/creating useful structures like 'certainty structures'
  - example: a 'measurement tool' can be useful as a cause of 'measurable structures' as it creates a 'certainty-adjacent structure' which energy/change can adjacently fill to create a 'complete certainty structure', converting that energy to the information required to convert the 'certainty-adjacent structure' into a 'certainty structure' such as 'information', similar to how 'known structures' are likelier to be similar to 'unknown structures' than different from them, as 'known information' is useful as a 'base to apply changes to', just like the 'measurement tool' is useful as an 'incomplete certainty-adjacent structure' to complete, as 'similar structures' are subject to a force prioritizing similarities, reflected in the 'adjacency of most change types'
  - the 'measurement tool' acts like an 'opposing force' to the structures its designed to measure which can apply changes to those structures to convert them into a certainty structure that it can measure, like a 'liquid filter' is likely to be designed in response to seeing a 'liquid' as an 'opposing force' that can 'separate the liquid (oppose the continuous attribute of the liquid in order to measure it)', so the 'liquid filter' could also 'produce/complete a liquid' if adjacent to a liquid-adjacent structure like 'separated liquid components'
  - these structures of 'opposing structures of change types' are useful to identify/build/derive functions that can detect change types, which is another way to derive solution-finding functions

- identify useful structures like specific useful filters of certainty structures, like 'opposing forces' which result in most variation occurring between the two extremes rather than at the two extremes, given the inherent structure of a 'spectrum created by opposing forces'
  - example: given that 'chaos tends to overwhelm systems rather than be contained in systems', there is no system that can handle chaos ('perfectly random disorder'), which is a way to identify whether a system is real or not ('does it generate randomness' or 'does it generate perfect order' as a filter of systems likelier to be real, as the opposing forces between randomness and non-randomness are in a constant interaction and vacillation between either extreme), as systems are unlikely to perfectly fulfill any intent, specifically extremely ordered or extremely chaotic intents, as a system is likely to be required to be very simple to create extreme order, and extremely simple (isolated) systems are unlikely to occur in reality

- identify useful structures that fulfill useful functions like 'oppose' to identify useful structures like 'stability' and 'balance' achieved by 'opposing a structure' so that it cant 'destabilize a system', as 'stable systems' are useful in intents like 'creating organization or order' such as a 'prediction function-finding function' which 'identifies a stable, organized, certainty structure out of difference structures like variables', as any structure which is sustainable enough to be useful to identify (such as a 'regularly occuring error type' which is useful to identify) also likely has a possible opposing structure that limits it, as such a structure is likely to be a difference made possible by an interface which has a 'change-limiting structure' built-in
  - example: identify the 'limiting structures' associated with various interface structures like 'combinations' of structures like 'attributes' of 'differences/similarities' which provide a way to calculate the 'limit' of the 'functionality or usefulness' of these structures where their potential to generate useful structures like useful differences is 'limited', 'limits' being useful as an 'opposite' structure of 'differences' that can produce 'similarities' or 'stability' (a 'system with limited change' is likelier to be 'stable' than 'chaotic'), in that limits are a 'constant' (a 'similarity type, as in a similar value') which can act as an opposing force to 'differences', as 'similarities in differences' are a useful structure on their own and specifically 'limiting structures of differences' are a specific useful 'similarity in differences'

- identify useful structures like 'connections between core structures like differences/similarities' and their attributes (like types, filters, examples, and exceptions) as a way to 'identify commonly useful specific connections between various specific differences/similarities', given the usefulness of these core structures for core interaction functions like 'connect' which commonly occur in workflows like 'connect problem/solution'
  - example: 'differences' are likelier to generate 'differences' than 'similarities', so identify 'differences that cause other differences' as a way of identifying these 'difference sequences' which are useful in connecting complex problems with solutions, as opposed to 'differences that cause similarities like randomness'
  - identifying the probability of differences as higher in the intent of 'generating differences' than in 'generating similarities' and the relatively higher probability compared to 'similarities' as a useful structure is useful on its own, but also when combined with other useful structures like the 'similarities that are likely to generate useful differences' such as when 'adjacent combinations of core components' are likely to generate 'other useful structures like functions in a system', these 'similarities that are likely to generate useful differences' being useful as a filter of 'similarities to filter out when identifying useful differences to generate differences'

- identify useful structures (like 'connections between known connected structures and structural similarities in differences from those connected structures, to connect other related connected structures') to connect which are useful for fulfilling a problem-solving intent like 'find changes to apply to a problem to solve it' or 'find changes to apply to a known solution to solve another problem'
  - example: identify that 'combinations of attributes like uniqueness (difference from other structures like other solutions), lack of coincidences (presence of valid reasons), and requirements' are frequently enough to identify a solution function for a problem aligning with those attributes (like a problem that is different from other problems), thereby applying 'structural similarities' in the 'differences between problems/solutions and alternate problems/solutions', as 'similarities in differences' are a common useful structure to apply when two structures are known to be connected, as problems/solutions are
  - identifying the structures (like 'known connected structures') to apply these useful structures (like 'similarities in differences') which can help fulfill intents like 'identify useful structures' (like 'structural similarities') can fulfill intents like 'find changes to apply to one problem to solve it or find changes to apply to another solution, based on the differences from another problem/solution'
  - the connection between 'known connected structures' and 'structural similarities in differences from other known connected structures' can be used to derive a missing structure connected to another structure (identify the solution from the problem)

- identify structures like the 'improbability of similar/equivalent resources being useful in a combination' by identifying the 'commonness' of attributes like 'diversity' in solution-finding methods, given the relative 'improbability that similar structures will be useful for extremely different intents' such as the 'different intents required to produce different info', 'different info' like that which may be required to solve a complex problem such as a 'find a prediction function' problem with 'an input data set of many variables'
  - example: functions like 'find' and 'test' are commonly useful enough to identify the solution function with the right information, given that one function is unlikely to have the inputs/functionality required to be useful in most contexts in isolation of every other resource like other functions
  - solution-finding functions are unlikely to require few or similar functions to be useful, but are likelier to require different functions which provide 'additive information' (each function identifies different info) so building a solution-finding function by 'combining extremely different functions' is likelier to be correct than 'combining extremely similar functions'

- identify connections that can resolve common error structures (like 'missing information' errors) like connections between 'commonly missing structures' and 'commonly required or otherwise useful structures' across solution-finding functions such as solution automation workflows
  - example: identify structures like 'common required structures' (such as how almost every change occurs 'adjacently') by applying the 'reason for common change types', in order to derive structures (like 'requirements' and 'important structures') in a context with errors like 'missing information', such as a problem of 'creating a new change type', 'identifying a common change type', or 'deriving the reason for a change type', the 'reason for adjacent changes' being that 'changes are usually required to be adjacent', so 'changes which are not adjacent' (such as the 'opposite' change between a positive/negative charge with no interim potential, so any change must be an opposite rather than an adjacent change) can be filtered out as unlikely or impossible in most realistic system interactions
  - the fact that most change types occur adjacently (in a connectible state sequence) is not a coincidence but is rather a requirement of physical reality and most other systems, given the need for 'incremental adjacent changes' to fulfill multiple intents like 'system stabiility', 'system efficiency', 'negative feedback-handling mechanisms to produce reversions to an adjacent previous state', etc
  - these intents override other intents in terms of importance, so finding these intents which could be used to derive the change types allowed by these intents is useful, as opposed to other intents like 'finding every possible difference' which could produce different structures that favor more volatile systems
  - similarly the 'meaning' structures (like 'intelligence') generated by 'combinations of components' (like 'cells') is another common 'requirement' of most systems including physical reality, given how alternate structures like 'intersecting fields' (like 'radiation' interacting with 'life forms') are less likely to be capable of fulfilling these intents like 'system stability' than other structures
  - the reason these structures are common (they 'fulfill these intents optimally') is unlikely to be a coincidence but rather an indication of the importance/requirement of these structures, which is suboptimal to ignore and useful to use in most interface queries, as a structure that adds certainty given its reflection of truth
  - the "connection between 'reasons' and 'probable change types generated by those reasons'" are useful structures to identify to find/build/derive one from the other
  - the "connection between 'system structures' and 'probable change types allowed in those systems'" is another useful structure to identify as a way to find/build/derive one of these structures given an example of the other
  - these 'connections' are particularly useful given that they are commonly used in interface queries to solve problems (its common to need to predict a change type in a system without perfect info about that system), as problems frequently involve a lack of information about some of these structures, so finding connected structures to these missing structures is a way to find/build/derive these structures
  - these commonly disconnected structures which are useful to connect are often additive in usefulness
    - 'connections between reasons and the change types they generate' are additively useful to 'connections betweeen systems and the change types they allow', since one can help filter the set produced by the other, given their common structure of 'change types'
    - therefore 'connections between commonly missing/useful interface structures' like these which have 'structures in common' can be identified as useful to identify in a combination, as opposed to only identifying one
  - 'adjacently connected structures' are also likely to generate 'isolatable components' which can form a new 'interaction level'

- identify useful structures like 'alternate structures that remove variation in another structure like the original problem' such as 'adjacent variables of a data set' which can be used to 'remove the variation to solve for in the original data set' which is useful for intents like "solve the 'find a prediction function' problem"
  - example: solving for 'adjacent data sets of alternate variables' (like causal variables of the original data set) which are derivable from the original data set and which are easier to solve for than the original data set problem is an example of applying 'adjacent changes' to 'specific problem/solution structures' in a way that is useful for deriving the solution (solving the variable interactions surrounding the original variable interactions will also solve for the original variable interactions)
  - this workflow specifically focuses on 'removing variation to solve for' by applying 'adjacent structures' to generate 'alternate more solvable problems' to 'reduce the variables to solve for in the original problem'

- identify useful intents like 'identify networks/algorithms that can identify different variables' and use these as 'interaction structures' which can fulfill intents like 'filtering variables that are already identified' in specific interface structures like a specific problem space/solution format (like 'find a prediction function' using the 'neural network' solution format)
  - example: a 'network of parallel trainers, each trainer using different inputs from the data set' can 'help each other get to the output value faster' or 'help find the certainty structures of the prediction function faster' if they are optimized to 'find certainty structures' (like convergence points, thresholds/phase shifts, or patterns) where a change type becomes clearer in one trainer before a parallel trainer, at which point that trainer can share this 'probable/certainty' structure with the other trainers, without a sizeable increase in computation cost given the relatively few important certainty structures to identify, so that other trainers can branch off to skip ahead using the certainty/probable structure and check if it generates the correct answer
  - changing the 'weight update algorithm' applied to each trainer can create this variation in the 'order of certainty structures found', where variation in the inputs' reflection of certainty structures does not create that variation (such as where one input sets' coefficients for a variable are all zero for a particular trainer, thereby missing the opportunity to identify the certainty structure of that variable's value when nonzero)
  - optimizing parallel trainers to identify different variables can be more efficient if its possible to determine the 'remaining change to explain' and create 'alternate parallel networks to identify those change types', or 'reduce the inputs by the change explained by already-identified variables' so that further iterations are applied to identify the 'composing variables of remaining change'
    - if the remaining change is trivial and could only specify the output function beyond the required solution metric value like 'accuracy range', skipping the remaining training as a generalization function is an alternative optimization
    - optimizing to find certainty structures can fulfill the intent of optimizing to find different certainty structures that, when combined, are likely to explain most of the change in the inputs
  - applying structures of importance (like a 'high-variation variable' which is likely to reflect/cause variation, 'required variable' or a 'unique variable' of a 'required function') as components of a network to check for those variable structures first and filter them out as possibilities first is another optimization that can be used to filter inputs initially or at regular intervals, re-checking new variables as theyre built from previous layers, as there are only so many variables that can be important in a data set, as usually not every variable is equally important

- identify useful structures like connection structures of useful structures like core interaction functions and relevant useful structures like 'connecting structures of connecting structures' which are useful for intents like 'finding a solution-finding function'
  - example: a solution like 'machine-learning' is likelier to have a higher percentage of core interaction functions (as a machine-learning workflow involves standardization, involving the 'reduce' and 'compare' functions, the 'combine' function to create 'variable combinations' as inputs/components of the solution 'prediction function', the 'filter' function to 'reduce possible solutions', and the 'connect' function to connect 'input/output variables') given the complexity of converting a set of functions into a useful structure (like a 'solution-finding method' or a solution to the problem of 'finding a solution-finding function' or a general input solution to the problem of 'find a prediction function'), so finding the functions that are different enough, combinable enough, interactive enough and otherwise sufficiently useful to be usable as adjacent components of a general solution-finding function is a useful intent, and other solution-finding functions are likely to be composed of similar components/inputs/sequences
  - finding the function/structure (like 'overlap') that can tie these functions together in a useful way is similarly useful, as 'overlaps' in 'change combinations' of the neural network create a useful structure in 'applying changes to common inputs (previous node layer) to create differences', as opposed to 'applying changes to different inputs to create differences', which is less 'standardized', and such a lack of standardization would make the network inefficient, having to consider many changes to different inputs instead of basing changes on the same inputs iteratively (in 'continuous trivial adjacent changes'), which is similar to how most change types work (as opposed to 'volatile/random changes')

- identify useful structures like 'change type connection structures' as useful structures like 'alternate structures of cause structures' to use in fulfilling intents like 'find alternate useful structures' uxing useful structures like 'probability' and 'requirements' to find the useful structures like 'change type connection structures'
  - example: there are relatively few constant/boolean variables compared to continuous variables, as its less common to have a clear separation between relevant variables like types or components indicating a constant/boolean value, as spectrums are more common, just like 'normally distributed variables' are a common variable type and parabolas or a 'change applied to itself' are a common variable type reflecting a core interface structure ('change developing around a foundation (like an average, a limit like masses gravity applies to, or a core component set) but not invalidating the foundation') where the parabola is a common change type bc the 'change applied to itself' frequently occurs given the 'structure (like boundary) development patterns' of core structures like a variable, where initially it would only have itself to interact with until the boundaries preventing interaction were removed and it begins to interact with other variables, which is how systems tend to develop or develop in response to new variables, which are either isolated once introduced or begin to interact with other variables once interaction barriers are removed
    - these 'normally distributed variables' are likely to interact with, reflect each other, and have causes in common, given the similar change types reflected in these variables, and the interfaces theyre based on are likely to be adjacently connectible
    - boolean/constant/type variables are likelier to be based on spectrum variables, rather than the opposite, given the structure of these constants as 'separable values', which are likely to be a subset of some spectrum variable
    - these 'probable or required structural connections between variables' can be used to determine 'interactivity of variables', which can be used to fulfill problem-solving intents like 'find interactive structures' to fulfill intents like 'build a solution out of interactive structures'
    - these types of structural connections are alternate derivation structures as 'causes' of a variable structure, as 'acting like an interface, providing a foundation for change' is a type of cause and 'interactivity' is another cause, so these alternate structures to 'causes' can be used where structures used to explicitly derive cause ('intent', 'requirement', 'output' info) arent available
  - a related workflow is to find structures that connect 'causal structures' (such as 'variables that frequently cause the most changes' like 'randomness', 'structure', and 'requirement') so one cause (like the 'cause of structure' or 'possibilities') can be converted into another more relevant cause like 'direct cause' (like 'structure' or 'requirements'), such as how 'probability filters' can connect 'possibilities' with 'requirements' (as in 'probable structures are likelier to be required structures'), so 'probability filters' are the 'connecting structures of causal structures', which are useful in fulfilling problem-solving intents like 'find a prediction function' or 'find variable connections'

- identify structures like 'info preserving change structures' that fulfill useful structures like 'requirements' to fulfill intents like 'create useful structures' (like 'difference from lack of input/output connection', 'difference from equal input/output', 'difference from constant output', 'difference from random output', 'difference from volatile input/output connection') which can be used to fulfill intents in specific problem formats like 'find a prediction function'
  - example: a 'function that changes inputs in a way that is different from producing zero change (an identity function or returning a constant value) loses all the information of the inputs or retains only the input information (and cant produce an output, like a dependent variable)', and a 'function that produces enough change to essentially be so volatile that it creates a random or random-like probability distribution of outputs' would be equally useless functions for the intent of 'produce a function capable of learning the change types connecting inputs and some dependent output' (like a machine-learning algorithm or regression function), because this 'lack of connection between inputs and outputs' takes various structures including producing a 'constant distribution' every time (like a random or normal distribution), or a 'constant value' every time, or the 'equivalent value as the inputs'
  - these structures of 'equivalence' can be used to filter functions that are capable of connecting inputs and some dependent output, to try various functions that can produce changes which are possible solutions to the 'find a prediction function' problem
  - these 'change structures' should preserve the input info in some structure like 'degree', bc the output is connected to the inputs
    - structures of 'info preservation' (like 'degree of info preservation') can be used to filter the change types which are possible values for the function that can act as a 'learning function' or a 'function-deriving function', including structures of 'info embedding', 'info compression', 'info encoding', 'adjacent info conversions', 'info representations like averages and aggregations' and other info-preserving structures
    - structures of 'volatility' are less likely to be useful for most functions, so functions that create volatile changes to inputs can be filtered out as possible solution functions that can act as a 'learning function', as 'trivial adjacent changes' are more probable than 'extreme adjacent changes'
  - this workflow involves deriving error types ('differences to avoid') in a particular useful interface structure ('input/output-connecting change structures') in a problem space ('find a prediction function') and identifying interface structures that can avoid those error types ('info-preserving change structures'), then identifying the interface structures ('requirements') of problem-solving intents ('create useful structures') that this fulfills
  - an extended and more specific version of this workflow would identify the specific 'variable values' of the function-finding function that tend to produce these errors ('volatility', etc) or solutions ('info-preservation') in a general function-finding function
  - this is true in machine-learning, where 'perfectly preserving inputs (equivalent inputs and outputs)' is useless (such as when an algorithm finds a prediction function that perfectly aligns with the input data set) and where 'constant change' and other simple change types are also useless (as they lose almost all the input information), which reflects the bias vs. variance problem
  - the dichotomy between 'info preservation' and 'info loss' is an alternate way to derive the bias vs. variance problem (and the solution of the interim function-finding function that can reflect change in a different way than just preserving it perfectly)

- identify structures that have useful structures like useful 'attributes' for various workflows like 'combinability' (given that various workflows involve both 'combinations' and 'combining workflows') or useful contexts like 'common problem space states, such as combinations of errors common to many problems'
  - example: identify structures that are 'probably true' with other workflows like 'apply patterns to find probable truths' and then identify 'testable variables that could change the solution the most of the remaining possibilities' and filter those variables to derive the remaining information
  - this applies multiple filters that are cooperative such as 'probability' and 'high-variation producing variables' to reduce variation in the set of solutions to filter ('filtering a set of variables determining the remaining possible variation after a filter like probability has already been applied to either the solution set or the variables of solutions, that is already filtered for being testable and for having a probable high impact on the solution', as opposed to 'filtering all possible solutions' or just 'filtering all probable solutions')
  - these are 'additive filters' which can be combined to add accuracy in reducing a set in most cases
  - this attribute of filters is useful in cases where different types of missing information are likely, which is a common case when solving most problems
  - identifying these useful structures of 'useful filters' with 'useful attributes like interactivity/additivity' as well as useful structures like 'structures of interface structures which are common to many problems, like common combinations of errors' are new useful structures to identify and apply

- identify structures that use probably available structures or more solvable problems (like 'more predictable subests of a data set') and use these as a starting point to apply other workflows, like finding interface structures that can connect them, using the attributes of the problem space (like that 'adjacent subsets can be connected in a sequence to create a function' in the 'find a prediction function' problem space)
  - example: a 'change structure sequence pattern' is likely to be useful in determining which change structures (like change rates) follow which change structures, such as when a 'probable correct subset' of a function is known or almost certain and other subsets of the function are to be inferred, given that a solution to the 'find a prediction function' problem is likely to have more predictable subsets (such as denser, less distributed, common, more predictable or simpler subsets) in some sections of the data set, and a pattern that can connect these change structures like a 'change structure pattern' in a sequence which is useful for connecting adjacent subsets, could be useful to create the solution out of these more predictable subsets
  - this works bc some components (like 'subsets') of a function are likelier to be more 'predictable' (more solvable) than other subsets, and these components may be usable to derive the others in certain position patterns of the subsets, like 'determining subsets' or 'alternating subsets', which can easily be used to connect adjacent subsets of the function
  - 'predictable' here means 'simpler' (meaning 'isolatable', 'adjacently constructed from few core inputs'), 'follows common patterns', and other structures of predictability, as opposed to structures of unpredictability such as complexity, randomness, or rarity
  - a related workflow is to find structures that are useful for some intent related to the problem, such as 'predict' or 'filter uncertainty structures' or some general intent like 'connect', 'identify core components', or 'identify change structures', which can be plugged in to some workflow, and combine the useful structures in one workflow to derive the missing information
    - for example, if some 'subset' can be predicted, that can be used in workflows like 'break problem into sub-problems' where the 'sub-problem' is 'solve for the subset of the function'
    - then this workflow can be applied to identify other subsets, and where they cant be identified, use 'certainty structure filters' to derive the remaining missing information such as by applying common patterns of 'function subset connections', 'commonness' being a 'certainty structure'
    - this 'combination of useful structures' of 'predictable structures', 'sub-problems like subsets', and 'certainty structure filters' can be used as a general 'structure filter'
      - for example, one subset can be more easily predicted bc its denser than the other subsets, and another subset contains an average when tested with a set of samples of the data set, and theres a pattern that the 'subset containing the average tends to be surrounded by subsets with high change rates', which can be used to connect the subset having the sub-sampled average and the dense subset

- identify useful structures like info formats that have variables which can determine relevance/usefulness, to identify variables of info formats that can be used to create new info formats, to fulfill problem-solving intents like 'identify useful formats to solve a problem in'
  - example: what makes 'all possible versions' of a representation useful, like 'what makes all possible versions of a representation of an object as a trajectory crossing a set of adjacent graphs, each representing object attribute values' is the fact that some of these 'adjacent-graph positioning' functions will make some interface structures like change types or similarities clearer than other positioning functions
    - this variation in 'all possible versions of a representation' allows for possible usefulness of some versions over others, and allows for the representation format to exceed the usefulness of other formats in some cases
  - what makes this representation more useful than representing objects as a point on a graph, where similar objects are adjacent, is that it relies on an aggregate of some similarity metrics like 'the sum of the subtracted differences between attribute values' which can cancel out information like where subtracted differences are opposing values that cancel each other out, removing that information from the point
  - the 'adjacent graph trajectory', even with a meaningless position function making adjacent attribute graphs possibly irrelevant (as opposed to a meaningful function that selects related attributes to graph in adjacent graphs), preserves info about attribute value differences across objects in a way that doesnt involve possible neutralizations of differences, as no aggregation is involved
  - this structure also adds a benefit of standardizing change types, using an x/y graph to represent each attribute, even if the values have different data types
  - an attribute graph-positioning function could also be designed to intentionally highlight interface structures like 'causal connections', 'trajectory intersections/overlaps and differences', 'attributes varying on the same interface', and 'similarities of data/changes types'
  - the variables of this representation like ('graph position in a graph network', 'adjacent graph continuity', 'adjacent graph relevance') allow enough variation that could be useful for representing info in a clear way
  - similarly, in a graph of a function's logical steps (without using a function/variable network, just a x/y euclidean graph), 'all possible variations' of how to represent the changes made by each function step (as represented by a particular direction, distance, or value change on another embedded numerical attribute like primes/integers), some representations will be more useful than others (some changes made by the function may be better represented as distances between states rather than direction differences between states, where states are represented as points)
    - representing logical steps as functions connecting points in a sequence/network/tree in 3-d space is almost always incomplete, unless its a math function using only changes graphable in that space
    - alternate function representation options:
      - a set of 'representative example sequences' that accurately and completely represent the possible changes produced by the function, where further example sequences would be redundant
      - the 'reasons for changes created by a function', 'causal connections', and other interface structures are also not represented in this format, which is why multiple graphs would be frequently useful
        - a mapping function between related graphs (both adjacent and non-adjacent) would be possible to graph in between attribute graphs, especially with a 'representative subsets of input/output or state sequence examples'
      - standard math structures like a 'separating line of a set of points' or a 'sequence of a set of points reduced to another sequence of filtered points, in a logical sequence where the reduced set occurs later' or an 'identifying attribute added to the reduced set of filtered points' or a 'space where only filtered points follow a pattern (like where an even number filter has a clear pattern on an equidistant integer line)' can be easily mapped to useful structures like 'filters'
      - how much each step in the function reduces information is another useful format, for example in functions that compress a list into a single value, where a line/cluster representing the list can be compressed into one value (such as an average or an attribute value of the list, which might be a point on the line or near the line or a boolean value like 0 or 1
        - other workflow interaction functions than 'reduce' such as 'connect' or 'change' or 'filter' can be used as the primary structures to graph in a function, such as 'connections between change types', given the usefulness of these structures for these workflows, such as how 'changes applied to known structures' are useful for problem-solving intents like 'generating new variables' or other abstract intents like 'generating inputs to core interaction functions of workflows', so graphing 'known structures' can be a useful representation for identifying that useful structure 'changes applied to known structures'
      - 'differences from other functions' can also be represented in the sequence of functions representing the function
      - 'interface structures connected in the function' can be best represented on multiple graphs or a graph of interface structures, but could also be represented using default change type structures, for example graphing a variable with values from 0 to 1 as a probability distribution from 0 to 1, and connecting variables represented like this using a causal 3rd dimension where direction represents cause
      - 'functions connecting each variable pair' connected by a third 'causal dimension' in a 'sequence/network/tree' of attribute graphs is another format that could be useful
      - representing interface structures is possible on a 2-d graph, like by representing a 'set of handled input/output pairs' as an interface around which changes can be applied as 'points outside this set or outside of connections within this set likely to represent other valid input/output pairs', or a 'probable prediction function' as an interface around which 'alternate probable prediction functions would vary, with lower probability of accuracy than the interface prediction function'
  - how to identify that an 'equidistant integer line' would be a useful format to filter out odd numbers and leave only even numbers:
    - even numbers have an attribute (divisibility by 2) that can be relied on to calculate the adjacent numbers (apply a subtraction/addition of 2), when even numbers are formatted in a sequence
    - given this calculation (subtract/add 2), the constant value of '2' can be used to derive the attribute of 'equidistance' of adjacent numbers in the sequence
    - so a format that portrays adjacent items in the sequence as equidistant can be used to clearly filter out even numbers (any number that is more or less than exactly 2 from an even number is not an even number)
    - this format can be used in place of other ways to calculate even numbers, where a divisibility function is not available or where only info about adjacent numbers of 'a number to determine evenness of' is available
  - these are useful in that they make interface structures like 'patterns', 'differences', 'overlaps', 'intersections', and 'causes' clearly identifiable/differentiable from other interface structures like 'random errors'
    - any format that differentiates specific interface structures can also be used to connect them, so can be useful for a workflow that uses those structures
  - a 'set of coefficients/constants applied to a variable' might be a useful format for representing 'function steps' or 'differences between functions' if the positions of the coefficients/constants are standardized across functions, since coefficients/constants can be applied to different terms in a 'sequence of operations', as opposed to representing the function as a 'continuous line' or 'set of points', which leaves out the unique function logic (as many operations can produce the same function from different base functions), and as opposed to representing the function by its 'change pattern' (an abstract version of the function that can be more easily standardized) or a 'function area/set of equivalent alternate functions, equivalent in error sum/degree'
    - if a function can be standardized to one variable (like a variable representing an interface structure), this format could be useful to represent that function as a sequence of coefficients/constants representing operations on that variable, where the sequence can represent more powerful changes applied to the variable in reverse (less powerful changes first as closer to zero)
    - representing a function by changes applied in a sequence will likely involve multiple graphs unless the changes can be standardized to a subset of variables and the dimensions allow the variable interactions to match mapped math functions
  - superimposing other structures (like a 'language graph query' as a filter between connected sequential points) is an alternate format to represent more variable changes like 'finding similar words as those in the sentences in a configured list', such as representing sentences in the list as a sequence of 'language graph query shapes (paths on the language graph network)', which are represented as connecting structures of two points if the input point matches those 'language graph query paths', producing a set of lines that represent matching 'language graph query shapes' of the input point when connecting it to another point (like a point in the next state-representing graph in the sequence)
  - euclidean 2-d graphs are mostly useful when changes occur sequentially and produce unique changes (like in a line/curve, as opposed to producing circular trajectories which can overlap so much that they become useless, such as if a value remains the same after applying a change type such as a 'language graph query-similarity filter')
  - adding embedded dimensions like 'language graph query paths' can be useful as filters, and especially useful when a set of related items has a clear similarity (like if similar meanings have similar shapes or start/end points on the language network graph) and more useful when they are relevant to the host graph (when a structure like 'adjacence/position' of a 'language graph query' produces a change in that direction/magnitude/position in the host graph of the input)
  - 'inputs/outputs' of a set of language graph queries represented in this way might be less useful than the 'sequences of language graph queries themselves'
  - the value of 'possible changes' producible by a function is relevant as a possible useful structure to represent instead, like how a function can 'add a missing value in a sequence if the sequence is missing a value' and 'have no effect if the sequence is not missing a value' which are the 'change types' supported by that function

- identify useful structures like the 'net impact of input changes on output' of a useful structure like a 'required function' (in various specific structures relevant to that useful structure, such as 'input contexts' to create useful structures like 'predictable output changes') and the usefulness of these structures for intents like 'approximate a solution' or 'find a solution range' to fulfill problem-solving intents like 'find alternate solution structures adjacent to the solution'
  - example: to 'derive' a value (predict or approximate it using uncertainties applied to certainties) rather than 'calculate' it exactly, like solving the problem of 'deriving a number that is the product of two factors' as 'being between a known lower/upper limit of the value' ('limits' of the 'solution' value which can be applied as 'solution requirements'):
    - interface query to 'derive' a value (instead of calculating it exactly by multiplying the factors)
      - apply the 'change' interface ('change types') and the 'structure' ('input/output', 'similarity') interface to the 'function' interface (apply changes like 'adjacent changes' to inputs/outputs to identify 'similarities' between 'outputs' when some 'change types' are applied)
      - identify that the 'functions allowed/specified' in the 'problem statement' or to 'solve this problem' ('multiply') have:
        - a similar output of 'increasing the output value' with an 'increase in one input factor with the other input held constant' or an 'increase in both input factors'
        - a proportional output to the input change ('multiply' being directly related to 'ratios' which are the core structure of its 'opposite' function, 'division')
      - apply the 'meaning' interface to identify 'useful/relevant' structures of this function
        - identify that applying this function can therefore create an output of an 'approximate value', by 'applying a value near to the actual input factor'
          - apply the 'meaning' interface to identify 'useful/relevant' structures of this output
            - identify that this 'approximate' output is useful when some calculations are:
              - 'simpler than other calculations'
              - 'already executed and the inputs/outputs are stored'
              - 'when the actual calculation is not required to be accurate'
              - 'where a range of the solution is acceptable in place of an exact unique solution'
      - identify that the solution can be a 'range in between a set of minimum/maximum limits'
        - apply the 'multiply' function to two sets of factors, one lower than the original set and one higher than the original set (given that 'multiply' function has a 'change type' of 'proportional net sign-associated change to inputs'), these two sets being more calculatable than the original set
        - alternatively, apply a 'filter solution space' workflow to select the most useful/relevant factor sets (such as those nearest the original factor set, rather than any factor set above/below the original), then apply this 'multiply' function to get a 'solution' in the format of a 'value range'
    - the 'net impact of input changes on output' or 'signed field of input changes on output' structure is particularly useful for identifying when a 'change type' (position impact on output) changes into another 'change type' (negative impact on output), such as the points where 'pairs of factor changes' have a position impact on an output relative to a 'particular original factor set', so that structures like 'combinations' of 'change types' can be used to create other 'change types' (useful 'change combinations' or 'change sequences' like an 'increase in both factors will always increase the output of a multiply function' and an 'increase in one factor will always increase the output of a multiply function'), and used to derive other structures of change types ('an increase in one factor and a decrease in another factor will sometimes increase the output of a multiply function'), to identify useful structures like 'points where factor sets change from increasing to decreasing the output of the multiply function', given the usefulness of determining the thresholds where this variable changes, once:
      - the 'change structures' (like 'change sequences/combinations, such as the "changes applied by input changes on outputs"') of the relevant structures (the 'multiply' function) are identified
      - the variable of the 'impact of the (increase in one factor and the decrease in another factor) on the output of the multiply function' is identified
    - the specific example 'change types with variables yet to be resolved for the problem' (the variable of the 'impact of the (increase in one factor and the decrease in another factor) on the output of the multiply function' which is unresolved as its "sign change threshold, where factors' impact on output changes its sign" is unknown) of the structure "change structures like change types and change patterns and change thresholds applied by input changes on outputs" is a new useful structure, as these 'change types (impact of input change types on output) with unresolved variables (the sign change threshold)' are useful for intents like 'approximation', since 'adjacent input changes' create 'adjacent output changes' and 'input change sign change patterns' have 'calculatable sign change thresholds' (meaning to solve the original problem '2 x 5',  multiplying '3 x 5' will have a net impact on the output that is equivalent to '1 x 5' (zero sign impact, as they both change it by 5), and will have a greater impact on the output than multiplying '3 x 4' (positive sign impact, as it changes the original value by more than multiplying '3 x 4'))
      - where this change in impact occurs between factor sets is the 'sign change threshold that is useful for determining positive/negative impact and direction to apply changes in to get farther/closer to a particular value, such as the original factors to multiply'
    - finding the specific useful structure ('sign change threshold') of the problem space that is associated with this useful structure ('net impact of input changes on output') applies the workflow:
      - 'find change structures of functions' ('impact of input changes on output')
        - 'find similarities in change structures of functions' ('find similarities in "impact of input changes on output"')
        - 'find variables in change structures of functions' ('find input changes that vary in the sign of the output')
          - 'find change types with unresolved variables' (the 'sign change threshold' variable of the change structure of 'multiply input changes')
            - 'find variable values' ('find sign change threshold value relevant to problem, like with "adjacent factor sets as the original factor set"')
              - the threshold occurs when factor sets' impact on output are equal, meaning:
                - 3 x 4 and 2 x 4 have an equal impact on output compared to an original factor set of 2 x 5 bc they are equivalent change types applied to the original factor set (decrease one factor by one to lower the output, and 'increase the lower factor' and 'decrease the larger factor' by one to 'increase the output', given that these changes wouldnt produce an equivalent output as the original factor set, meaning '3 x 4' is still higher than '3 x 3.33' or '2.5 x 4', which would produce the same value as the original factor set, therefore since '3 x 4' is above both of these 'equivalent alternatives' and given that 'multiply' has a 'proportional effect of input changes on output', '3 x 4' is guaranteed to be higher than '2 x 5', so its guaranteed to have a 'positive sign change' as in it will 'increase the output')
                  - identifying these 'equivalent alternates' of the 'factor sets', having one factor in common (3 or 4) with a factor set (3 x 4) to determine the sign change impact of, is useful when the 'impact of the function' (proportional effect) is known, as it can be used to identify when a factor set (3 x 4) will result in a 'positive/neutral/negative sign change'
                  - anything above 3 x 4 would just get more inaccurate compared to 2 x 5, increasing it too much
              - knowing the specific input factor sets that create a 'sign change threshold' can be useful to solving the specific 'multiply factors' problem, if this interface query is wrapped in an 'approximate solution' or 'find solution range' intent
              - applying these interface structures (calculating 'equivalent alternates' of 'input changes') can be used to determine useful structures like 'sign change thresholds'
              - 'sign change thresholds' are useful for identifying 'input points or input change points, beyond which inaccuracy increases' as useful for 'identifying solution ranges'
    - 'functions of a problem statement' or 'required functions of a solution' are useful when used with 'input changes' bc identifying structures of 'input changes' ('impact of input changes on outputs') is useful for determining what direction changes should be applied to structures relevant to a function ('inputs') to get other relevant function structures ('outputs') that are 'farther/closer to a point (like a solution point)'
    - the '"change type" change limit' (as in "a limit on changes of a certain type, like 'signed changes'") or the 'net impact of input changes on output' are 'equivalent alternate' structures to derive the 'sign change threshold' as particularly useful in solving this problem when solving it with a 'approximation' or 'find solution range' intent
    - these 'equivalent alternate' change structures of the 'change' interface structures can be found just like the 'equivalent alternate' changes in 'factor sets' of the 'multiply' function can be found
      - 'change type' change limits are similarly useful as 'impact of input changes on output' for similar intents, beyond which other changes applied to these structures in various positions/directions will be less useful for those intents
      - the 'connection' between these 'equivalent alternates' act like 'changes applied to an interface' (an interface of 'probable/expected changes'), given the 'similarity' of "change type" and 'input changes' (which are a specific change type) and given the 'similarity' in usefulness of 'change limits' and 'input change impact on output' (changes which have limits in their impact) around which changes develop on the foundation of that interface structure ('expected outputs') while fulfilling its original intent ('find/approximate expected outputs without directly calculating them'), some changes making these structures more useful for various intents that dont contradict the original intent of the 'expected outputs' structures ('find/approximate expected outputs')
      - these structures are 'equivalent alternates' bc knowing the 'impact of all input changes' can be used for similar intents as knowing the "threshold of changes in input changes' impact on output", one being more efficient than the other for various purposes, given functions like 'retrieve stored value' or 'check if value is above/below threshold'
      - 'useful "change type" change limits' (like 'adjacent sign change thresholds') are another change applied to this change structure that make it more useful for intents like 'approximate solution'
    - 'change types (like input changes)' and related useful structures (like 'impact of change types like input changes') are useful for intents that involve 'changes', such as 'finding an alternate solution structure' (a solution structure with 'changes' applied to it), which is another way to derive these structures ('impact of input changes on output') as particularly useful
      - 'find structures that align across intents of different but related interaction levels (like aligning structures/functions across the "problem/solution" interaction level and the "change structure" interaction level' is a way to derive this 'alternate interface structure set' ('change types' and 'related structures of change types') that can derive the useful structures ('impact of input changes on output'), as 'changes' are inherently related to the 'change solution to find alternate solution structures' workflow
        - the 'impact of input changes on output' and the 'change solution, to identify alternate solution structures that can act like the solution' both have the function/structure 'change' in common (these structures align on the 'similarity' of their 'function' structures, despite being on different interaction levels (the problem/solution interaction level and the change interaction level)
        - using 'interaction level' here instead of 'interface' bc its a useful structure to depict 'multiple function levels' in a way that shows 'composing functions/intents (like change functions)' of another function like a 'higher-level function (like the problem-solving function)'
    - related workflow: so applying changes to structures of the workflow or its sub-intents (like its core interaction function, such as 'change' or 'connect') is a way to generate probably useful structures for that intent (in this case, with the 'change solution to find alternate solution structures' workflow, this would generate the 'change' interface, and all of its interface structures like change patterns/types)

- identify interface structures describing problem-solving structures like 'problem-solving state sequences' in solving a specific problem, then identify the interface queries that could generate those interface structures or identify the 'differentiating factors' to filter out interface queries that could not adjacently generate those structures
  - example: the 'connections' between related numbers like pi/i/e can be described by interface structures like 'cross-interface alignments' (a 'combination' of an 'alignment' of 'multiple wave functions'), a structure which can be identified by structures like 'common' interface structures between the numbers (such as 'change types' like 'relative changes', 'adjacent changes', 'aligning changes', 'change rates')
    - this useful interface structure solves the specific problem of 'finding ways to generate pi'
    - identifying this useful interface structure while solving this specific problem is trivial once the structure of 'multiple waves' or 'multiple sequences' is found/generated/derived
  - the 'filter' here is 'find interface structures in "common" between different structures like "different change types"'
  - identifying this 'filter' of useful interface structures from the original useful interface structure ('cross-interface alignments') is trivial by finding structures that can fulfill the intent of 'generating the useful interface structure used to solve the specific problem'
    - 'create a "cross-interface alignment" using connection structures (like equal/similar structures in common among different structures)'
  - this applies the workflow:
    - 'solve the specific problem'
      - 'identify useful interface structures of the problem-solving process'
        - 'identify input interface structures that can build the useful interface structures'
          - 'apply these input interface structures to filter useful interface structures' (find structures in common across change types (aligning valleys/peaks/inflection points in wave functions), to find cross-interface alignments (aligning wave patterns combining to useful components of a sequence to generate pi) which can be used to combine change types (sequences) to create another related change type (sequence converging to pi))
            - 'input interface structures of useful interface structures' are the useful 'differentiating variable' that can fulfill problem-solving intents like 'find useful structures' and 'filter useful interface queries', if the inputs can be found for a particular problem, these 'differentiating variables' being useful to 'filter' other structures

- identify useful structures which are useful for fulfilling requirements (like 'preserving info') of problem-solving intents
  - example: structures which preserve info when applied in either direction are useful for problem-solving intents like 'convert to a format where the problem is more solvable, then convert back once solved in that format', since the solution in the other format can be converted back into the original format, therefore these structures are candidates for fulfilling this problem-solving intent

- identify useful structures like 'variable-crossing' structures as a way to fulfill common, useful, & problem-solving intents like 'convert between useful structures' (such as 'common' structures like 'variable sets')
  - example: it is already established that 'cross-interface' structures are useful (to create other useful structures like 'conversion', 'alignment across interfaces')
    - a 'cross-variable' structure (like '4') commonly found in formulas converting between relevant attributes (like 'area' and 'surface area') of 'variable sets' of unit structures ('circle', 'sphere') is useful in converting between these 'variable sets' as 'change structures', 'converting between variable sets' being a useful intent for problem-solving intents like 'convert to a format where the problem is more solvable, then convert back once solved in that format'
    - this structure can convert an attribute of a circle into a relevant (as in 'corresponding', as in 'occupying the same function/position/sub-structure as the other structure in a different structure') attribute of a 'higher-dimensional variant of a circle', and 'converting to a higher or lower dimensional space' is a common and useful intent in various problem formats like the 'find a prediction function' problem format
    - a 'cross-variable' structure doesnt necessarily mean 'increasing the "count" attribute of a structure (variable set)', but may involve another change type of the 'variable set' structure, as its just a way to connect different variable sets which are related through their corresponding structures, indicating a 'threshold' across which one 'variable set' becomes another indicating a 'phase shift' (such as an 'identity' change of the structure, as a sphere is differentiable from a circle, requiring its own definition, while being dependent on the circle, this 'dependence' indicating the 'relevance' of these structures, in addition to their 'corresponding' structures like radii/pi/area)
      - this 'cross-variable' structure of the '4' multiplier answers the question 'at what threshold point (in a multiplier) does two dimensions correspond to three' for the 'circle area to sphere surface area' example

- identify useful structures like 'connection' structures of structures that are commonly required to connect (like incomplete/complete structures) in various problem-solving intents like 'derive missing information'
  - example: a way to identify a 'complete' structure from an 'incomplete' structure is to identify the 'limits on core change types' of the incomplete structure (four cardinal direction points of perimeter of a circle) and find an alternate way to connect those 'limits on core change types' of the incomplete structure (such as a square, connecting the four points with right angles in the midpoints between these four endpoints)
    - generating a 'square' and a 'circle' is trivial by combining core structures (like 'multiply', 'constant', 'angle', 'unit'), and finding the formula to connect their definitive attributes (like area or side length) is useful since circles are useful as randomness-generating structures in their relevance to pi (an extremely random number)

- identify useful structures by which structures are useful for usefully (like 'adjacently') fulfilling core functions (like 'connect') of other useful structures (like 'common/core structures')
  - example: identify that e is useful by its adjacent connectivity when applied to pi and i, which are core structures

- identify useful structures (like 'trivial differences that are relevant') that are inputs to identifying other useful structures (like error types, such as 'trivial errors which are incorrectly ignored') to fulfill problem-solving intents like 'identify error types & apply differences to them to generate solutions'
  - example: similar to how a neural network has a possible error type of 'over-prioritizing the more common or more represented class in a data set' bc of the 'structural similarity' between the 'under-represented or less frequent class' and 'random noise' or 'trivial errors which can be ignored', given that 'trivial differences' can be under-identified as relevant in a neural network bc of its requirement to generalize across multiple data set input points, rather than designing a function specifically adhering to every input example data point
  - identifying that 'trivial differences' are ignored/discarded by a neural network involves applying interface structures like 'unit' or 'minimum' to other interface structures like 'difference' and checking for data that can produce an error such as 'ignoring a trivial difference that is actually relevant (has a reason to be included, such as an outlier that indicates a pattern change)'

- identify the 'causes' of usefulness of a structure (what problem does it solve, what does it reduce), which align with the 'usage intents' of that structure (the 'reasons to use it'), to fulfill problem-solving intents like 'identify/filter useful structures'
  - example: some structures are more useful than others, like how 'filters of relevant info' are more useful bc they reduce the 'amount of info to focus on (process) to solve a problem'
  - this expands the definition of useful structures to include other specific useful structures that are generally useful
  - 'specific structures which apply across systems in general' are useful structures for adding focus (from certain structures) or reducing processing requirements to problem-solving processes
  - 'adding focus' and 'reducing processing requirements' are 'aligning intents' which can be used as 'interchangeable alternates'
  - 'inputs to certainty structures' such as 'tests' (which identify the certainty of a structure like an equivalence) are similarly useful, in that they can generate useful structures like certainty structures

- apply workflows until a useful workflow-filtering structure like 'minimum info required to check if it has at least one successful example' is reached, as a way of fulfilling the problem-solving intent 'select useful solution automation workflows to solve a problem'
  - example: finding out whether a workflow or solution-finding method has at least one successful example is trivial, and finding out which workflows can reach this example is trivial in the set of core workflows, and once a succesful example is achieved, changes can be applied to the original core workflow to improve the next application of the workflow variant
  - this applied 'trial and error' to useful structures like 'workflows', with the difference that each item in the iterated set is itself iteratively tested until it fails to a certain degree ('recursive trial and error')

- apply interface structures to identify 'new variables' to fulfill intents like 'identify new changes to problem/solution structures' which are useful for intents like 'apply changes to existing solutions until its optimized'
  - example: to identify a new variable of a structure like a 'song' with core structures like 'notes', apply interface structures (like 'subset') to other interface structures (like 'time' in the 'physical information' interface of the 'information' interface), to generate 'intervals of notes'
    - this generated variable of an 'interval' is a variable which can be varied in its structural metadata, such as the 'length of the interval', given that a variable is an 'interface unit' structure (supports variation in a range that does not break the structure like the definition of the variable), so varying the 'length' of the interval (or other metadata like the 'position' of the interval) doesnt break the 'interval' definition

- apply interface structures to filter structures as being more probably true or false to fulfill the problem-solving intent 'find certainty/probability structures'
  - example: if there is no 'reason to support a theory', it doesnt have to automatically be considered true and is a candidate for a falsehood, like when someone's actions dont match their words, as 'words' are the far more easily controlled variable than 'actions', so 'words' are more easily faked than 'actions' and can therefore be subjected to more scrutiny (as opposed to trust)
    - the 'reason compounding the certainty of that possibility' is the interface structure associated with the insight 'more easily controlled variables (by agents with incentives to lie) being more subject to noise and other forms of falsehood' to generate other structures like 'a set of falsehoods where the truths are so rare they might as well be a random error' to identify a data set as part of a 'liar' or 'error/falsehood' cluster
    - deriving the 'reason compounding a certainty (the reason a statement could be true)' from the insight 'more easily controlled variables (by agents with incentives to lie) are likelier to contain falsehoods (where falsehoods are more efficient and accomplish the same or a similar goal)':
      - if a statement is true, it not only requires that it 'lacks relevant contradictions', but also requires an 'incentive to tell the truth' ('to share or communicate new info', 'to solve a specific problem', 'to contradict a falsehood', 'to draw attention to the truth', 'to make the truth useful for other people', 'to help others solve problems'), which is an 'opposite' structure of a structure in the insight ('incentives to lie'), if there are incentives to lie, otherwise there is no reason to say it
    - deriving the insight 'more easily controlled variables are more subject to falsehoods'
      - apply 'probability' to the 'requirement' concept applied to the 'input' structure ('variables', 'intents') of the 'function' structure
        - an agent that uses a function with total control is less likely to need that function, and can survive while faking that function

- apply interface structures to problems to generate new solution automation workflows that involve a reduction of some solution metric (problem variable set, distance from solution, number of un/solved sub-problems as a ratio of total sub-problems, etc)
  - all of the solution automation workflows in problem_solution_diagrams.svg involve changes to the variables of problem/solution structures
    - change problem until its a solution (changes the problem state until its useful in some way)
    - solve different problem (change problem identity, like to the 'causal problem')
      - find a way to connect all problems (using structures like metadata) and solve a problem that is related but more useful or more solvable than the original problem
    - break problem into sub-problems (change problem scope/complexity)
    - connect problem/solution (change problem/solution position)
    - reduce problem (change problem variable set to a subset)
    - filter solutions (change problem space possibilities to a subset)
    - change existing solution until its more optimal (change suboptimal solution)
    - change problem format, solve it in the different format, and change the format back once solved
    - change problem boundary/structure to a more solvable problem
    - solve for generally useful functions for general solution metrics and apply them to solve problems
      - find alternate interchangeable optimization points of solution metrics and solve for the most computable optimization point
    - organize problem/solution structures (like sub-problems) until the original problem is trivial to solve (such as where solving one problem is useful as an input to solving the next problem, once organized such as being 'sorted' in the right 'sequence' structure)
  - the interface structures (changes) applied to the variables of problem/solution structures need to fulfill definitions ('make sense, having no relevant contradictions')
    - for example: 'changing a suboptimal solution' is only useful when the change fulfills solution metrics more optimally than the previous state
    - so the 'change' applied would only qualify as an implementation of a solution automation workflow if it 'improves the suboptimal solution' (fulfills the definition of 'solving a problem')
  - sub-intents of each workflow, like 'find input info of the workflow', are components of the workflow implementation (interface query)
    - workflow: connect problem/solution (change problem/solution position)
      - sub-intents (alternative, overlapping, sequential sub-problems)
        - 'find input info of the workflow' sub-intent: find connective/interactive structures to use in connections
        - 'find structures to input to the connect function' sub-intent: find structures required to be connected
    - various workflows can be used to implement these sub-intents:
      - 'find input info of the workflow' sub-intent: find connective/interactive structures to use in connections
        - 'derive the structures that are adjacent inputs to the solution by applying solution requirements like solution metrics'
          - this implements the workflow 'derive structures in between problem/solution using input-output sequence'
      - 'find structures to input to the connect function' sub-intent: find structures required to be connected
        - this implements the workflow 'apply requirements to derive certain structures that can be connected with uncertainties (variables)', where the uncertainties here are the 'structures or methods to connect those structures which need to be connected'
        - the output of the previous sub-intent can be used as input to this workflow
        - the 'adjacent inputs of the solution (derived from solution requirements)' can be input to the 'connect' function
          - meaning the 'problem structures' should be connected to the 'solution-adjacent inputs'
  - these sub-intents of each workflow are each problem-solving intents, bc they all add useful information to use a solution automation workflow, even if the current workflow cant be completed (just the sub-intent is completed)

- identify useful structures like 'alternate structures' that can be combined to create solution automation workflows which are not default components of solution automation workflows (solutions, problem-solving intents, etc)
  - example: the 'iteration' structure of a 'for loop' is the corresponding structure of 'try every possible solution, score each one, and compare scores' or 'try every possible solution & score each one, until one score is above the solution score threshold', as 'for loops' are often used to 'build a reduced list out of another list'
  - similarly, the 'function parameters' (inputs, values being changed) & 'function return values or value changes' (outputs) are corresponding structures of the 'find input-output sequences that connect problem/solution' workflow
  - the 'filter the solution space' workflow also corresponds to the 'iteration to reduce a set' structure represented by a for loop with a filter applied to a list
  - given that core logic structures are corresponding structures of core workflows, combining core logic structures as components of other workflows is a way to generate workflows

- identify useful structures (like 'inputs') of solution automation workflows and identify methods to convert structures into those requirements, to select which workflow to apply when one is more optimal than other
  - example: the 'break problem into sub-problems' workflow has a requirement of 'isolatable change types', given the structural impact of the workflow which is 'isolating change types'
    - these change types can be sub-problems (reductions of the problem), which can take the form of:
      - 'solving the original problem, for a subset of the original problem variables'
      - 'solving the original problem, broken into a sequence/tree/network of sub-problems'
      - 'solving the problems causing the original problem which are "components" of the problem in that they are "inputs" of the problem'
    - similarly, the 'filter a solution space' workflow has a requirement of 'existing solutions to try' or 'existing possible combinations/changes (in the problem space) to try (which may or may not be solutions but are possibilities)'

- identify useful structures which store info such as 'representative data (data set shape) of other info (data set)', where the stored info can be used as a proxy or substitute for the other info to fulfill problem-solving intents like 'reduce required computations'
  - example: knowing the 'general shape' of the data set is useful information to assess whether a directional change or parameter of a prediction function is likely to produce a correct prediction function, and may be more useful in some cases than the actual data point, as the 'general change types like change patterns and change rates and variable interactions' are included in the 'general shape' of the data set but are not included in individual data points, which are used as the input to most 'find a prediction function' solution-finding methods
    - the 'representative data' is also useful for other intents like 'generalizing a function', as 'specific data points' are likelier to be corrupted, mistaken, or influenced by other irrelevant factors than the 'general shape of the data set'

- identify useful structures (like 'additional inputs', as in 'additional input data derived/inferred/imputed' or 'probable additional input data') for specific intents in a problem space like 'find a prediction function' that could fulfill solution metrics like 'accuracy'
  - example: if predicting whether a particularly high output (target y-value) exists (or is likely to exist) in a data set (for intents like 'augmenting data' and 'inferring new data' and 'imputing data (filling in missing data)')
    - first find the x-range of lowest/highest x-values, then select random subsets of the data set and find a function that describes each random subset
    - then find the maximum y-value of this function over the x-range, using a method such as 'find out if there are adjacent points to the target y-value in a pattern that would make it possible for the y-value to exist in the function (like "are there two points surrounding the peak of a curve, where the peak is the y-value" or "are there two points surrounding the y-value in opposite directions, at a change rate that is plausible for the function as it exists elsewhere in the function")' or "is it adjacent to known points in the function within an accuracy range"
    - apply this maximum y-value of the random subset function to check if the particularly high target y-value could be output by that function (do any known approximations/representations of the data set - or metadata of these approximations/representations like the 'maximum y-value of the approximation' - indicate this output is possible/probable)
    - repeat the steps starting from the 'selection of random points' until there are enough statistically significant functions that could output the y-value, up to a number of repetitions that is fewer steps than applying a standard method like a regression method
    - alternatively apply a filter like 'random points in a set of sub-ranges of the x-range' to evenly distribute the random points selected to make the random selections more meaningful in similarity to the actual probable prediction function
    - this applies info about the possible y-variable value, compares it to corresponding info about the representative data (maximum y-values of random subset functions) and filters out possible y-values based on a threshold selected to be a similarity identifier (the 'accuracy range' allowing a degree of adjacent points to be included in a function)
    - this method looks for a 'corresponding structure in a known structure' (maximum y-value of known representations) to compare a 'possible structure' (possible high target y-value) with
    - similarly, when applying a standard regression method, looking for a 'corresponding structure' ('most adjacent point') in a known structure ('function implied by calculations already done') to compare a 'possible structure' ('possible next or other y-value') with can fulfill intents like 'reduce computations required' (as given the lack of volatility in most functions, points can be connected by curves that match the change rates shown in the rest of the function, so given a 'most adjacent point of a probably correct function', the relevance in the form of the accuracy of a 'possible next point' can be assessed except where the change types like change rates of the function experience a phase shift or other change requiring that some other points' averages be computed as well (instead of applying inference rules to identify other points))
      - similarly, a 'random next point' can be chosen and corrected once compared with change types/rates of the probably correct function, which is likely to reduce computations as well
    - finding other data to include in a data set can increase the accuracy of a solution-finding method for the 'find a prediction function' problem, so is generally useful for fulfilling problem-solving intents like 'improve base solutions on some solution metric'

- identify useful structures like 'overlapping structures in common' between useful structures like 'stored structures' which can be applied as inputs to functions like 'merge' to generate new possible useful structures
  - example: a 'generative filter' (such as an iteration of a set of possible solutions which applies 'differences' to filtered out solutions to generate more possible solutions to apply as future possible solutions in the iteration of possible solutions) can be derived by noting that 'apply differences to error structures to generate possible solutions' and 'filter possible solutions' have structures in common, that being 'errors in the set of possible solutions which are filtered out as not solutions' and 'known error structures as input to a change function to generate possible solutions', therefore identifying that 'generating possible new solutions from solutions determined to be not solutions in the set of possible solutions' is a useful structure is a matter of identifying this structure in common and the possible usefulness of this new structure (identifying new possible solutions not already in the set of possible solutions by applying differences to the possible solutions once theyre identified as errors), which makes use of the overlapping common structure to integrate/merge these structures to generate a possible new useful structure, which is useful in cases when the set of possible solutions is unlikely to be complete, such as when a function is applied to generate structures which are different from optimal solutions, such as 'maximally different possible solutions' or 'base solutions' as the set of possible solutions

- identify useful structures by generating a set of possible useful structures (like "possible useful functions with an 'intent' but no logic built yet") and filtering them by metrics of usefulness like 'common components', 'alternate intents theyre useful for', 'what problem-solving intents they would fulfill or functions they would reduce requirements/computation for'
  - example: making a list of useful functions to build to fulfill intent likes 'finding errors' and 'fixing errors' (a function to 'identify useful structures like "root causes" or "common inputs" in error information', a function to 'identify solutions already tried which failed in error information', a function to 'identify possible/probable errors before they happen') would have the output of useful functions like a function 'that generates errors in a codebase/function', as this generative function can generate the inputs/outputs (like 'error information' and 'causes of errors') to other useful functions in the list and can fulfill the primary intents like 'finding/fixing errors' with just one function rather than several, or a function set that can be combined to generate the functions in the list (like 'get definition of solution/error' and 'check if a structure matches the definition like a solution/error'), which are useful bc they reduce the number of functions required, can generate other useful functions, can generate useful information for other functions like 'cause' info, fulfill multiple intents, and/or can be combined/chained to fulfill other intents
  - 'identifying a list of useful functions to build' can be a simple matter of 'identifying common work done which is not implemented in a function yet' (like 'tasks which are done manually'), and filtering this list can be as simple as 'identifying which functions have overlapping functionality, which functions have common component functions which might be more useful to build, which functions enable other functions, which functions fulfill useful metrics like "reduction of requirements of other functions"', rules which can be applied structurally to filter the set of possible useful functions to build
  - a 'useful structure-identification function' is an adjacent alternative to a 'solution-finding method', as the only structure required to convert it into a 'solution-finding method' would be applying adjacent changes to combine & otherwise connect the useful structures found until they fulfill solution requirements
  - this workflow involves a method of identifying 'useful filters of useful structures' (variables which can separate the set of possible useful structures) and 'generative methods of useful structures' (such as by applying workflows like 'derive solutions from solution requirements'), and applying these structures as opposing structures (like 'functionality' and 'requirements' are opposing structures) when generating and filtering possible structures like solutions to the problem of 'identify useful structures to apply in a specific problem or across problems'

- identify structures which are alternates to other structures that fulfill useful intents like 'reducing storage requirements' or 'reusing existing structures' which are generally useful for problem-solving intents like 'reduce resources required to solve a problem'
  - example: 
    - 'specific information' (like 'examples') can be useful for intents like 'function implementations' (like storing input/output maps if theyre relatively static, or applying changes to these input/output maps based on input similarity, instead of implementing specific function logic) and 'filtering out reasons to use abstract variants like types' which are useful for other intents like 'identifying which structures to store (known constants) and which to generate dynamically (uncertainties/variables)'
    - 'reasons to use a structure (such as the required or probable outputs of using the structure) like a "specific" or "abstract" structure' can also identify when to use a specific structure vs. an abstract structure 
  - these structures ('reasons to use a structure', 'expected outputs of the structure', 'known intents fulfilled by a structure') are alternate structures which can be useful in intents like 'filtering what structures to use'
    - storing the 'reasons to use a structure' vs. storing the 'expected outputs of the structure' (a specific example of the 'reasons to use a structure') vs. storing 'known intents fulfilled by the structure' can provide optimization opportunities, if some of these structures are used more frequently across interface queries, functions, or solution automation workflows
    - other structures with similar functionality as these structures include:
      - 'requirements including the structure or its outputs'
      - 'known contexts/systems where the structure is useful or required'
      - 'structures that are useful for determining the probable output of a structure (like a function), when applied in a system where it hasnt been tested' (such as by finding any structures in the system that could neutralize or alter the steps or requirements of the function, which are capable of interacting with the function or its requirements in that way)
  - other relevant structures, like 'phase shifts where one structure stops being more useful than an alternate structure', are similarly useful for this workflow

- identify interface structures based on 'interface structures (like change types, cause, etc) of interface structures', to fulfill useful intents like 'find interface structures that are probably found adjacent to each other' to fulfill intents like 'connect interface structures'
  - example: the structures nearest to an interface are likely to be composed of 'core structures' of that interface, are likely to be in the shape of 'overlapping pyramids' (describing multiple 'combinations of core structures' (bases) with different outputs (peaks)) than other shapes like a 'set of circles returning to the interface' or a 'tree with outputs pointing at the interface', where 'similar changes across these pyramids' are likely to represent 'phase shifts' and 'interaction levels' and 'system layers (of a system layer diagram)', and the changes farthest from the interface are likely to be 'outputs' of the other changes rather than 'adjacent changes resulting from core combinations of core structures', given how changes on an interface usually interact, such as in 'fractal patterns'
  - 'finding connected interface structures' is useful for many problem-solving intents, like 'find alternate interface structures (like alternate "definitions")', 'find cause of interface structures (like changes)', etc

- identify useful structures that fulfill required intents like 'preserving an interface' which is required for a system to exist, interfaces being a core structure of systems, representing stable states of the system
  - example: a 'limiting structure' and a 'changing structure' are required 'opposite' structures, which are required to fulfill required 'preservation' intents like 'make sure change doesnt get out of control' in a system which would destroy the system, which is necessary if the system is to exist or be useful, and necessary while the system is still more useful/beneficial than costly
    - similarly, a 'change that is possible on an interface' (like a 'base line') is a 'change that doesnt destroy or destabilize the interface' (like a 'smaller line than the base line, applied in a direction other than that of the base line')

- identify useful structures determining relevance, such as 'similar differences that make variables relevant' to supplement, replace, or implement known useful structures like 'interaction levels' to fulfill problem-solving intents like 'find relevant variables'
  - example: a predator's 'claws' and 'teeth' might be the most relevant variables to its prey, unless it also has a tail which can attack and reach the prey, which is relevant to the other variables by a 'similarity' in the 'possible values' of the 'position' variable, answering the question, 'which variables are relevant to the prey, when the prey is in position x' to identify 'variables that can have a "similarity" in position', thereby making these variables possibly relevant to each other ('teeth' and 'claws' and 'tail' could be relevant to each other by being 'interchangeable alternates' for the 'attack' function), so the question 'which variables are relevant to the prey, when the prey is in position x' can be converted to 'what variables can occupy the same position as each other, and the same position as the prey'
    - similarly, 'tools which can be used by some variable to "extend their range"' would also be relevant variables, as they change a 'previously irrelevant' variable for a position into a 'relevant' variable for that position by increasing the 'positions the variable can be relevant to', which is a target structure for identifying relevance of a variable
      - this can be used to derive relevant variables, by identifying 'variables which can make another variable relevant' (what types of change would interact to fulfill intents like 'extending distance reachable' which are relevant intents to variables like 'position' which are relevant for being under the control of the prey) and identifying 'whether those variables exist'
    - 'variables' can be determined to be relevant by a 'similarity' like this, which makes them 'interactive' (interactions such as 'coordinating', 'neutralizing', or 'overlapping') in some way, so that they can be 'combined', which is a core interaction function of 'variables', and this means that variables can be derived by applying 'similar differences' which allow finding other variables which are different enough to be classified as an isolatable variable but similar enough (in a similarity like 'possible positions') to be interactive (on the same 'interaction level' as another variable)
    - this is similar to how finding an orthogonal dimension can be found by applying a 'similar difference' to another dimension ('rotate the x axis 90 degrees to find the orthogonal dimension y'), which is not so different that it is equal to the original dimension (as 'rotate 360 degrees' would accomplish) or 'irrelevantly different' (as 'rotating 180 degrees' would accomplish) both of which would essentially generate the same line as the original x axis, and not 'irrelevantly different' in that the rotated line has an overlap in the 'change type' it describes, in describing horizontal as well as vertical change, and applies the x axis as a base for this change, which can be used to derive the new axis by applying core change types, or applying intents like 'find the changes required to create a useful difference, like describing different differences (vertical as opposed to horizontal) by identifying the changes described by the original structure (horizontal) and identifying changes not described by that structure and identifying a base (y axis) that could describe it', creating a useful difference in a line rotated to 90 degrees which can measure an 'isolatable (different change type) but interactive (similar interaction level)' change type (these changes are not so different that they cant interact, but are not so similar as to be interchangeable)

- identify structures of relevance to find relevant structures for functions commonly used in solution automation workflows like primary functions like find/build/derive/apply to reduce the computations run by these functions, and therefore reduce steps to implement workflows
  - example: the 'apply' function applies changes in the form of a structure to another structure, in which the 'structure being applied' has to match the inputs of the 'structure its applied to', otherwise the structure being applied is not relevant to the structure being applied to, for example, applying a 'collision function' to 'blocks' is only relevant if there are 'multiple structures like "blocks" which can be collided' and if the 'blocks dont have an overriding function which prevents collisions like magnetic forces', meaning 'does the collision function have its inputs (blocks to collide, no collision-prevention structures) fulfilled'
    - so checking for '"inputs" which can make a structure useful to apply' is a filter of 'structures which can be applied to other structures'
    - similarly, with the 'find' function, only 'structures which can be found (structures which are measurable, structural, stable, stored, unique/determinable/identifiable compared to other structures)' can be found, so only these structures are relevant to the 'find' function
    - similarly, 'structures that can be connected' are relevant to the 'derive' function, and 'structures which can be combined' are relevant to the build function (as well as any structures which can make other structures findable, derivable, buildable, and applicable, making these structures relevant to these functions)
  - this workflow applies a definition of relevance to find structures that are useful (in the sense of being usable and therefore relevant) to the primary interaction functions, to filter out structures which are irrelevant when using these functions
  - this means 'structures which are findable' are a reduction of the solution space when applying the 'find' function
  - similarly, 'reduction structures' (such as 'any attribute which can differentiate a subset from another subset' applied to the 'solution space' of 'possible solutions'), are another useful structure to the 'find' function, given its definition
    - these reduction structures can be particularly useful when designing a filter sequence/tree/network that can optimize a query based on relevant 'find' intents
    - for example, identifying 'differentiating attributes (variables) of a solution space' can identify which 'differentiating attributes' should be applied as filters to 'maximize coverage of the solution space', to 'filter a subset of the solution space', to 'increase the chance of finding multiple alternative solutions or a unique solution', and other intents related to 'find', like 'filter', 'identify', 'differentiate', etc
  - identifying 'variables that can be controlled' is another example of variables which can be filtered (focused on) to identifying 'possible variables of a solution', as 'variables which can be controlled' are irrelevant

- identify useful structures like 'sufficient similarity' to find structures which can be used to fulfill common intents like 'find alternates of a structure' which are useful for problem-solving intents like 'find alternate solutions'
  - example: applying code validation rules like the following can be used to correct logic, given the general or common applicability of such rules
    - 'if an input variable is tested for and not found to be initialized, and is assigned a value if this condition applies, apply the output of that assignment as a filter of the input value, if the input is initialized'
    - example of this code validation rule:
      - if the 'interfaces' input variable has a value, apply the primary interfaces found with the find('list', 'interface') function call which returns all defined interfaces, as a filter of valid values of the interfaces input variable
  - this code validation rule is not always true, but is generally or commonly useful
  - finding 'invariant structures' such as 'rules which are always true, in all possible cases or case types' which can be used to derive definitions, as structures of certainty such as structures which dont change
    - similarly 'rules which are generally or commonly true' can be used to find 'probable structures of certainty'
  - given that 'structures which dont change' can be used to find structures like 'definitions' given their common relation to 'certainty' structures, structures which are sufficiently similar can be used to identify/derive/substitute each other

- identify the structures that can fix the causes of a problem, applying insights about problem causes such as that 'problems or problem causes are usually useful in some way to some entity if they exist enough to be noticed, or they wouldnt exist'
  - example: if a problem exists, its either beneficial to some entity with the ability to use the problem to its advantage or create the problem, or its not a big enough problem for enough entities to trigger the structures (like "priority increase to highest priority") that would be necessary to fix it
  - so finding structures like 'functions that find alternative ways to benefit those entities than the problem' would be useful in addressing the 'causes' of the problem (the 'benefit to some entity', the 'problematic attributes of that entity like "parasitic/predatorial strategies to benefit from causing problems"', or the 'lack of solution-triggering structures')
    - the root causes of these causes includes:
      - 'inability of all entities to understand & prioritize meaning (meaning in the form of the impact on the host, as if a pathogen kills the host, its also hurting itself, but it cant detect that bc it cant grasp context, time, emergent conclusion like "host death" of structures such as "parasitic patterns applied at scale which dont encounter reducing/opposing structures like limits", and other structures that could make it aware of the impact of its structures on other structures)'
      - such a solution would apply the 'distribute solution-finding methods and priorities to all structures, such as functions/objects (like pathogens relying on a host to survive), rather than limiting problem-solving capacity to some structures (like immune cells)', which would possibly take a form such as 'distributing host immune markers to all cells and creating costs to all cells based on those markers, so the pathogens get negative feedback for parasitic strategies, rather than short-term positive feedback leading them to repeat that strategy until the host dies' or 'creating short-term feedback that represents long-term feedback for pathogens or mutations capable of killing the host, if they can only use short-term feedback and dont have the resources like "storage" or functions like "computation" to make long-term feedback useful', or 'find all specific strategies (or strategy inputs/states/components) used by cells that eventually lead to host death and prevent cells from using them'
  - similarly the problem causes may be useful bc they cause the problem, meaning the problem is 'caused by some function' and 'not solving the caused problem makes that function useful to some entity by reducing the work involved in that function', so functionality to 'minimize resources needed' would help that entity avoid creating such a function, so 'distributing this functionality to all entities' would be a useful structure to fix the problem causes, one of which is the 'inability to minimize resources needed'

- identify structures like 'high variation causative structures' that are useful in changing relevant variables like solutions/errors, as in 'causing a solution to be an error structure' or 'causing an error to become a solution structure', which are useful variables for many problem-solving intents like 'filter solution space' or 'change existing solution to improve a solution metric', as the 'opposite of a solution' is an important structure to avoid, and knowing the cause of an 'opposite' is useful in avoiding that
  - example: identify 'high-variation causing structures' like 'reasons that completely change the meaning of a structure (such as how "testing for a reaction" can completely change the meaning of a particular "statement")' or 'cycles (such as circular or wave functions) are a structure which completely change the resolvability of an ambiguity and change what info is required to resolve it, like adjacent point context of the original point to find the output value for', as 'high variation causing structures' are likely to be useful in identifying 'opposite' structures which can change a solution into an error
  - this workflow involves identifying useful structures to avoid to fulfill problem-solving intents, and the structures that can make that identification trivial

- identify useful structures like 'contradictions of insights' that can identify useful structures like 'relevant changes to apply to existing solution-finding methods' in a particular problem format like 'find a prediction function' to 'filter the solution space' of 'possible solutions' to the problem of 'find changes to apply to solution structures' in the workflow 'improve an existing solution by applying changes to it'
  - example: a 'regression' or 'machine-learning' method might find no correlation between two sets of variables (like the 'number of products purchased in one region' and the 'environment impact in another region') but as 'all variables are related', this would be a false result, so a robust solution method would have a mechanism to find correlations given this insight that 'all variables are related', allowing for distant causation or different interaction levels than those in the original data set like 'agent decisions', 'time', 'markets' and 'market manipulation', which can be produced by applying interface structures rather than simple 'combinations of input variables', thereby applying inferred or general or common variables (like 'variable patterns'), rather than just the input variables and just the 'combine' operation
  - just by applying this one insight, interface structures can be inferred, so identifying a 'contradiction of the insight' present in specific problem-format solution-finding methods like regression & machine-learning methods is particularly useful
  - an adjacent method to derive interface structures is useful on its own, but is also useful for deriving new useful structures adjacently
  - similarly other insights like 'identifying connections of an isolated variable set is rarely useful without context' is another insight that can adjacently derive interface structures (like the 'cause' of the variables and the 'intent' of identifying the connection between isolated variables, as in 'to correct lack of info' or 'to find specific info like causal variables' or to 'filter out non-causal variables')
  - if the insights are in fact insights, they should be confirmed or reflected in a good solution that reflects the truth, rather than contradicted by that solution
  - similarly, a statistical or machine-learning solution-finding method might identify a 'correlation' between the 'green skin' and 'aliens' but a solution-finding method using interface structures would identify a 'combination of errors' that would cause a 'false similarity', such as 'filtering out or under-prioritizing examples of failures (where a similarity is not significant or related or accurate)' and 'over-prioritizing simple structures like structural similarities (like similar colors or shapes)', leading to 'agent' interface structures like 'conspiracies' ('systems created by lack of information in the form of an over-prioritization, that seem based on truth until a counterexample is found to identify the missing information resulting from the priority'), since interface structures apply 'cause' to find the cause of a structure like a 'similarity' (causes like errors of over-prioritization, missing information, flexibility resulting in unenforced truth adherence, tendency to over-simplify, such as by simplistically classifying rare variable values like 'green' as meaningfully similar to other rare variable values like 'foreign' despite no reason to associate these)

- identify the reason ('equivalence' in inputs/outputs) for the usefulness of a structure (like commonly useful structures such as 'simple' and 'complex' structures) and apply it to find useful structures like 'simple/complex set that have equal inputs/outputs', given that 'equivalences' are generally useful for problem-solving intents like 'connect problem/solution' or 'find a function implementing an intent'
  - example: identify that structures like sets of 'simple and complex structures with the same inputs/outputs' can fulfill useful intents like 'find implementation/fulfillment structures of an intent', as a simpler structure like an 'intent' is likely to be fulfilled by a more complex structure like a 'function fulfilling that intent' if the inputs/outputs of the two structures are equivalent, which makes one structure useful to the other for various intents ('find a simplified representation of the function (such as the intent)', and 'find a function that fulfills the intent')
  - these structures are useful for other intents, like 'find the variables to apply that convert a simple structure (like the intent) into an equivalent complex structure (like the implementing function)' which is useful for problem-solving intents like 'find a function that implements an intent'

- identify useful structures such as 'solution component filters' that are useful for common intents like 'filter' to apply in workflows that use those intents like 'filter the solution space'
  - example: a function necessarily changes some inputs, so anything that doesnt change an input can be ruled out as a possible solution function to the 'find a prediction function' problem, and any 'combination of changes' can be included in the set of possible solutions to the 'find functions' problem
  - this workflow involves identifying useful structures by applying common core intnets like 'filter' to common problem/solution structures like 'solution components'

- identify useful structures like 'contradiction example input/output mappings' that fulfill useful intents like 'find useful structures like "change filters"' which are useful to many problem-solving intents, like 'improve a solution by applying changes to a function'
  - example: storing a 'input/output mapping' of 'contradiction examples' as metadata to store with a 'function definition' is a useful structure to store with a function bc it provides a target for improvements to the function which is useful for filtering future changes to the function (based on a solution metric of whether they improve handling of those contradiction examples), as well as taking the place of logic that would be required to handle those specific counterexamples

- identify useful structures like connections between problems and structures connected to solutions (like functions, each function being a solution to a problem)
  - example: solving 'which problems create which functions' (which problem maintains the need for a function or leads to the creation of that function) as a way of determining alternate implementation methods of functions (applying problems as a function-generating structure, if the problem is adjacent enough to the function with existing resources that it can be converted into the function by default or otherwise adjacently)
  - this workflow involves applying the 'inputs' of solutions (including 'problems') as an 'input' to solve problems like 'find a function' which may be applied in interface queries, answering the question 'what problem would develop the function required to solve a problem like "find/build/derive a function"'

- identify useful structures like 'probability' and 'reason for probability' that are useful in identifying useful structures like 'certainty structures' which are useful for many intents like 'identifying truth structures like insights', 'testing if a structure is true/certain', and 'implementing functions that are useful in that they reflect reality' which are usable in many problem-solving intents like 'filter the solution space'
  - example: certainty structures like constants & limits are relatively uncommon compared to uncertainty structures like variables, bc certainty structures are useful for changing the changes allowed by variables and act like a controlling structure on variables, which is useful when a problem occurs bc of the variables, and given the corrective power of certainty structures on variables, this further indicates that fewer certainty structures are required compared to variables (to explore useful differences to new problems), as a problem is only likely to result from an uncertainty structure that doesnt reflect truth and is only likely to be changed by a certainty structure correcting that incorrect uncertainty structure so it better reflects truth, this dichotomy serving as a useful opposite structure to balance problems/solutions between, as solutions must have some variation (uncertainty) to allow for adaptation to different contexts and different new problems, but must have some degree of certainty (constants, limits) in their reflection of the truth
  - the reason for the 'lower probability' of the truth structure is the 'lower requirement for truth structures to manage uncertainty structures', the indication that 'if a truth structure is found, its likely bc a problem (a problematic difference from reality) had occurred which the truth structure corrects', and the 'higher usefulness of uncertainty structures like variables in many intents except for intents like "correcting an incorrect structure generated by an uncertainty"'
  - a variant of this workflow is to 'apply changes to certainty structures' (like insights, patterns, or known solutions) to create useful differences like improvements to suboptimal known solutions, changes that stay within the limits created by those certainty structures (not so different that the differences violate the truth), where certainty structures are not sufficient to solve a problem, so uncertainty structures like variables/differences can increase the usefulness of a solution, since 'previous truths' act like a base for 'new truths' so applying changes to 'previous or knwon truths' can produce 'new/unknown truths', which applies this as an insight generating this solution automation workflow, which applies truth structures as an interface around which uncertainty structures like changes vacillate before resolving into truth structures (the 'interface network' being a structure that requires the fewest differences in order to become useful and reflective of reality while maintaining its ability to change)

- identify useful structures like 'opposites' that are useful for multiple core intents like 'find differences' and 'apply difference' and 'filter' which are adjacently useful to problem-solving intents like 'filter solution space', when applied to specific structures like 'requirements' to identify other specific structures like 'the set of (interchangeable alternates and requirements)' which are useful in specific contexts like 'where requirements are suboptimal' such as by 'applying variables to create interchangeable alternates of the requirements' to solve the problem of 'removing suboptimal requirements' or 'apply changes to requirements to make them different from requirements (optional)'
  - example: identify structures like 'interchangeable alternates' as an 'opposite structure' to structures that can be suboptimal like 'requirements'
  - a related workflow involves identifying problems where a particular solution automation workflow would be more useful to correct the problem (such as how this workflow is particularly useful when 'variability in solution requirements would be useful', or when a 'requirement is a problem cause') and applying changes to change the problem into that problem (changing the original problem into a problem of 'invalidating, removing, or changing requirements'), by applying differences to the solution automation workflow to find problems that would benefit from that workflows' connection structures
    - example: apply differences to the 'identify "opposites" as useful for intents like "find differences" to identify other useful structures like "interchangeable alternates" as useful for creating other useful structures like the "set of interchange alternates and requirements"' workflow in order to find the problems (like 'removing suboptimal requirements') where the workflow is particularly useful
    - "identifying interchangeable alternates" would help with the problem of 'removing requirements'
    - applying 'changes' to "identifying interchangeable alternates" would result in identifying 'opposite' structures of those structures ('requirements'), thereby identifying any problems that involve 'problematic requirements' as a useful workflow to solve that problem
    - keeping 'example' structures in a solution automation workflow definition is useful in 'reducing computations' to 'find relevant structures to that workflow', which is useful for intents like 'find similar/different workflows' or 'find interactive/connective structures with a workflow like a problem where the workflow is particularly useful'

- identify structures like "connections between solution metrics (like 'fairness') and structures that can be identified with existing functions (like 'certainty')" as a 'reason' to apply the 'existing (certainty) structures' to fulfill the 'solution metrics' (fairness), as opposed to other structures, to fulfill problem-solving intents like 'find structures that fulfill solution netrics' and 'filter the solution space (of structures that fulfill solution metrics)'
  - example: identify structures (like more structures such as variables or structure-generating structures like questions) that capture info better than simpler structures (like fewer structures), such as how the 'question of whether an individual is good or evil' is optimized by asking additional questions like 'what is the connection of their abilities/functions and their requirements (inputs and required outputs)' and 'what is the impact of their decisions (weighing the importance of outputs (as interactive with other structures) as being similarly important as their inputs, at various scales like global/local and other variables like time (the impact on their future decisions) and examples (the common impact on future decisions by other recipients of that feedback)' and 'are their decisions common (invalidating the focus on an individual as opposed to a group)' and 'are their decisions avoidable' and 'what is the impact of feedback on their decisions', questions which apply 'additional' interface structures (which act like connection structures and certainty structures) and apply 'fairness' as a 'solution metric like accuracy' of a solution (solving the problem of 'finding solutions reflecting truth' as a way of identifying the solution metric for identified alternate question sets of 'does the question generate fairness')
  - this workflow involves identifying better formats (like a 'minimized' variable set or 'accurate' variable set) for structures (like a variable set) that fulfill an intent more optimally despite impact on other solution metrics of problem-solving intents like 'minimizing storage requirements', bc these better formats apply additional interface structures and therefore fulfill the problem-solving intent of 'improving a solution (by solution metrics like completeness, accuracy, robustness)', and identifying how to convert to those formats, such as by applying "core interface structures like 'input/output'" to "other interface structures like interaction level structures (like 'decisions' on the 'agent' interaction level)" to identify structures of meaning 'what is the impact of their decisions'
  - a variant of this workflow would identify 'structures (like 'additional') of interface structures' that improve solutions, such as how a solution involving additional connected interface structures is likelier to be robust, correct, and relevant/useful/meaningful

- identify structures that can fulfill problem-solving intents like 'filter error structures' when fulfilling useful structures like other problem-solving intents such as 'find and remove error structures'
  - example: identify that 'selfishness' is a useful structure bc of the 'reason' that its a 'high variation structure', and identify that 'high variation structures' are unlikely to be an 'error structure' that should be 'removed' (as opposed to 'changed') when applying the 'find error structures like error causes and remove them' problem-solving intent
  - for example, the 'selfishness' structure is not 'required' to be applied (its optional, which adds a variable), and it doesnt have to be applied in the same way (like only to protect the individual, as opposed to protecting groups they belong to, which adds another variable), so when selfishness is suboptimal (like defending a group member just bc of membership), it can be avoided, and when its optimal (like in deadly contexts), it can be prioritized
  - the high number of variables in how selfishness is applied indicate its an important structure and also that its unlikely to be an error structure, given its variable usefulness (rather than 'absolute uselessness')
  - this workflow involves identifying useful structures such as 'high variation structures' which can 'filter the solution space' when applying problem-solving intents like 'find & remove error structures', as 'structures too important to remove' are a useful structure to 'filter the solution space (of structures to remove)', as 'required tasks' like 'organizing resource allocation' and 'sustaining life' would be almost impossible with existing brain functions if people didnt organize their attention using 'selfish' priorities like 'finding resources fulfilling their own basic requirements'

- identify useful structures like 'high information reduction structures' (such as the 'similarity attribute') that fulfill common useful intents (like 'reduce calculations required', 'summarize', 'identify alternatives/equivalents') in a problem space, as these structures are likely to be useful for other intents, some of which may be problem-solving intents ('reduce calculations required')
  - example: a program could only store the 'similarity' of content rather than searching all the attributes of the content or the content itself, and use that similarity to show recommended alternatives, rather than requiring a custom complex algorithm to find what users are likely to view next, applying the insight that users are likely to be interested in similar content to that they intentionally viewed, rather than deriving their intent in a complex algorithm, as some problems can be reduced to fewer attributes, and these fewer attributes can be useful for fulfilling intents like 'privacy' or 'reducing memory use' or 'reducing computations required'
  - this workflow involves finding structures (like an 'attribute' such as 'similarity') that encapsulate a high amount of information (such as whether information is related to other information, to fulfill intents like "identify whether the information is connected, alternative, similar, coincidentally similar, or some other intent that can use 'similarity information' as an input") without losing usefulness for other intents such as semi-derivability of that information
    - if information is similar to other information, it can likely be derived to some degree (or the set of possible information can at least be filtered) by applying the difference represented by the similarity to the other information, given the similarity definition
    - the information is required to be slightly different, otherwise there is no 'reason' that could trigger a 'view decision' (such as 'viewing new content (new meaning slightly different)')
  - related workflow: 
    - a variant of this workflow involves identifying structures (like the 'similarity attribute') that can be simultaneously useful for commonly useful intents like 'deriving a degree of info' and 'identifying similar info' and 'preserving privacy' which are likely to be required simultaneously
    - a variant of this workflow involves identifying the insight ('find similar information to show users bc its likelier to be something they want to view if its similar enough to what they already indicated they like, through structures of intent that are also structures of certainty') associated with the useful structure (the 'similarity attribute') that can be used to identify the useful structures, which would involve identifying the 'reason' ('similarity to content that was defined to be liked') of the target structure (the 'view decision'), as the user supplies a 'certainty structure' and a 'structure of intent' in their 'view decisions' (the solution metric being optimized), and the 'reason' for the 'view decision' (they wanted to view it, liked it, it was relevant/interesting, they liked the specific information in the content, etc) can be applied as an 'input' to trigger additional 'view decisions', an algorithm that prioritizes the 'view decisions' as trusted information indicating what they want to view, and prioritizing the 'specific information in the content' as the structure that the users are aiming to get (rather than 'structures of lack of intent' like clicking accidentally, or because it was the top search result or otherwise available/adjacent), an algorithm that could apply the 'apply changes to a base solution to find other solutions' workflow to 'identify the insight that similar content might also trigger a view decision'

- identify useful structures (like 'attribute count') that are useful when a particular interface query is run (like 'applying the probability-structure interface') to fulfill a particular problem-solving intent (like 'filter solution space') to solve a problem (like 'implement a function such as identify/find structures')
  - example: an ambiguity is likelier to be a 'core structure' (with fewer attributes) bc of the improbability of finding 'ambiguous alternatives with a high number of attributes' or a 'high number of missing attributes' leading to the ambiguity, which applies the 'probability-structure' interface to solve the problem of 'filter the solution space' of possible solutions to the problem of 'identifying/finding structures' (find 'ambiguities' by finding 'core structures'), like how saying "we're (verb)-ing" could be interpreted as a decision of a group or a description in a conversation
  - solution success cause: this is useful bc of the counterintuitive nature of the connection between 'ambiguity' and 'core structures' (as in, its not specified by any definition route of ambiguity or core and can be added as an alternative definition route when more certain definition routes fail)
  - generalization: this workflow can be abstracted to find the specific intents/problems and interface queries that would produce a useful structure like a 'counterintuitive connection', such as identifying the probability-structure interface as being usable to connect an 'ambiguity' and a 'core' structure, given their similarity in an attribute metadata attribute (attribute count) which is a structure of 'probability', so identifying the probability interface as useful amounts to the operation of 'identifying the reason for common structures between two structures to connect' (probability being a reason for commonness) and 'applying that reason for commonness as a method of connecting the structures' (connecting them in the sense of finding relevant similarities, relevant to the task of 'identifying alternate/connected structures of a structure, as a way of identifying that structure')

- structures like 'functions applied' are useful to identify useful uncertainty structures like 'assumptions/predictions about structures like the usefulness, resources, requirements of those functions', 'assumptions/predictions' which may or may not be certain or otherwise useful insights that can be generalized or applied to find other useful structures, insights which are useful to derive
  - example: if an agent calls a function, the agent assumes and predicts that the 'outputs will be useful for its intents'
    - the 'reason the outputs are useful for their intents' may not be clear but are likely derivable and likely to be useful to derive
    - these are similarly useful structures as 'implications' of 'using a function', as 'structures of probability' (as opposed to certainty) adjacent to the function (there is a probable reason to find the outputs useful, there is a probable reason that is considered useful, there is a probable resource making that useful) which are useful in finding 'connecting structures' to connect the function with other structures like other useful functions fulfilling those predictions, given that if it exists, its likely to fulfill its predictions better than average

- identify alternate routes to fulfill intents like 'identify useful structures' that are likely to already exist (like problem structures are likely to be already known)
  - example: identifying 'problem/suboptimality' structures is a way of identifying 'functions & inputs that dont exist yet' and 'differences not handled by existing functions, which are suboptimal for some useful intents given that these problems have been identified as a problem for some agent', where 'functions that dont exist yet' are a useful structure to determine adjacent structures like opposite structures, such as 'functions that do exist or are known', applying the insight that 'if a structure (like a problem) exists and has not been reduced (solved), the functions to reduce it (solution functions, solution-finding functions) are less likely to already be known or applied to solve it, if there is a reason to reduce it (the structure is a problem)' which indicates that this structure (a problem) can be used to determine which 'reduction functions' (solution functions) exist

- identify useful structures like 'connecting structures' of 'problem/solution structures' and 'causal sequences' which are useful in intents like 'finding alternate connections between problem/solution structures' (finding new solution automation workflows)
  - example: the 'cause' of a 'new solution-finding method' might be identifiable at any point in its causal sequence (a 'suboptimality/error is identifiable (measurable) & identified', a 'solution is required', 'solution is possible', a 'solution is tested', a 'solution automation workflow (like 'applying all known solutions until one works') is applied and didnt fix it' a 'solution is identified as an improvement'), but only some structures in that sequence are useful as identifiers of a 'new, more optimal solution' ('inputs', 'requirements', 'reasons to find a new solution, such as a suboptimality/error'), as other structures in the sequence are irrelevant (the 'requirement' of a solution is irrelevant if it is 'impossible' to be solved so 'possibility' filters would have to be applied first in a sequential workflow, and the 'identifiability of a suboptimality' doesnt cause the solution, and none of these structures cause the specific solution structures applied in the optimal solution, unless there is only one solution, making those structures 'required constants')
  - if a workflow like 'trial & error applied to known solutions' is applied and found to not find a solution, that is a cause of applying functions to fulfill problem-solving intents like 'find new workflows', and is a cause of identifying the problem, but is not a direct cause of the solution

- find useful structures like functions to apply to useful structures to find other useful structures
  - example: an abstract variant of a 'dichotomy' (with the requirement of a 'count' attribute value to be a constant value) is an 'exclusivity', which is useful for intents like 'find structures that dont co-occur' (like an 'impossibility' and a 'possibility' being connected as 'equivalents' of the same structure, these connections not occurring in any system that follows rules (a logical system, a real system, etc)
    - 'find structures that dont co-occur' is useful for other useful intents like 'selecting between alternates' and 'find opposite/different structures' which are useful for intents like 'differentiate/connect structures' and 'filter structures'
  - 'mutual exclusivities' are a 'core structure' related to other core structures like 'differences'
  - a 'falsehood' combined with the 'exclusivity' (a 'false dichotomy') is a useful structure in that it applies a core error structure 'false' to a core structure 'exclusivity' of core problem structures like 'difference', identifying a 'specific example of the error structure' from applying this combination function (thereby applying a 'false' filter to the set of abstract exclusivity structures to identify the specific structures that are 'error structures')
  - 'abstraction to find alternate examples' and 'combination with other useful structures like error structures to find specific error structures' are the useful functions to apply to the useful structure of a 'dichotomy' to find other useful structures to fulfill other useful intents

- find structures that are useful in other problem-solving intents like 'finding other useful structures'
  - example: find 'new examples' of a structure like a 'type', since new examples are likely to enable adjacently identifying other useful structures like the 'route to find the useful structure', such as by connecting the 'system context' to the 'new example', which is likely to be a 'new route to useful structures' (like the example structure) if the example is genuinely new, and since there's likely to be a 'reason' for the 'difference' in the new example if its an actual unique example, a reason such as a 'new interface structure' causing the newly different example (if the difference is on the right interaction level and not just any difference in any attribute), and as differences in examples of the same structure are an important signal since 'examples of the same structure' are likely to have multiple similarities, probably more similarities than differences
    - 'finding attributes that connect very different structures' is a useful structure to 'find more adjacent routes connecting structures' (like how finding the opposite of a structure like a 'counterexample of a statement' or the limits of a structure can be more adjacent than finding the original structure) and 'find new differences to apply to create different structures' (like 'like creating maximally different alternate solutions to filter the solution space or the solution space of base solutions to change')
  - 'finding connections to useful structures' is a useful intent to make 'new examples' useful structures
  - 'finding a new example of a useful structure' is likely to enable adjacentlying identifying a new reason for the structure being useful, which is useful to apply to 'find new useful structures' or 'find alternate useful structures'

- find structures that are useful for problem-solving intents like 'filter solution space' as useful structures like 'filters' applied to fulfill the problem-solving intent of 'find useful structures'
  - example: the 'reason for the irrelevance' of a 'structural similarity' is particularly useful to fulfill core interaction functions like 'filter' or 'differentiate' between relevant & irrelevant structural similarities, which are difficult but useful to identify/filter/differentiate, as 'structural similarities' are very useful structures when relevant
    - the 'reason for the relevance/irrelevance' of a 'structural similarity' may just be that the structure found to be similar to another structure is just a 'common structure' or a 'core structure', rather than its similarity to the other structure being similar for a relevant reason, like a similarity in the reasons why the structure was caused (like to 'apply it for the same intents')
    - the 'similarities in cause (reason) of the structure' and the 'similarities in intent of the structure' are therefore better structures to check for filtering structures that are relevant, as opposed to just applying 'structural similarities' on their own
    - the reason why 'causal similarities' and 'intent similarities' are useful to find relevant structures that are similar (as opposed to irrelevant structures that are similar) is that the cause/intent structures are likelier to be aligning with the reason for identifying/finding that structure in the first place (structural similarities arent typically sought to 'find common structures', theyre typically sought to 'find alternate structures to fulfill a particular purpose' or 'find all the examples of a structure'), therefore there is a (useful) 'structural similarity' between intent/cause of 'structural similarities' and the intent/cause of 'finding structural similarities' (which include intents like 'find all the examples of a structure')
    - in the reverse direction of derivation, 'structural similarities' can be a useful structure to fulfill intents like 'find common structures' or 'find core structures', as common structures are likely to be in a set of similar structures, and common structures are likely to be core structures
      - this fulfills the useful intent of 'find the context where a structure would be useful' to enable fulfilling other intents like 'check for that context to find useful structures to apply in an input context'
    - 'cause/intent' are therefore useful as a filter of 'structural similarities'
      - another reason a structure might be found to be similar is the structure is 'optimal' in some way, such as that its useful for some intent like 'find stable structures', so the structure is likely to be repeated as its optimal for some intent which means it may be useful for other intents or that intent (and the associated optimal structure fulfilling it) may be required elsewhere
      - the 'repetition' attribute is a useful 'connecting structure' of related structures like 'similarity' and 'commonness' as a 'cause' or 'output' of both structures, which makes it useful for intents like 'connecting/differentiating structures' which are useful for problem-solving intents like 'connect problem/solution' and 'filter solution space', which enables intents like 'identify useful reasons why a structure would have attributes' like 'similar but not common' (its a new optimization that hasnt been deployed elsewhere yet), 'similar and common' (its repeated often), etc, which is useful to differentiate the 'related and similar but unique' structures and differentiate when one structure can be used to identify the other or as a proxy for the other (when commonness is relevant vs. when its not)
      - 'related and similar but unique' structures are useful for many intents like 'approximate', 'substitute', etc, but should be filtered by relevance (the 'reason for the similarity is the same', or the 'input/output connections are the same' (inputs/outputs being a core useful structure))
    - adding a similarity of a 'similar system' in which the structures both develop is another structure indicating the relevance of the 'structural similarity', acting as a 'cause of relevance' which is a useful structure to find/evaluate in place of finding/evaluating relevance to fulfill intents like 'find useful structures'
    - this emphasizes the usefulness of structures like the 'count of similarities (like cause, intent, and structure) between structures' which fulfill the useful intent of 'increasing the probability of a structure being useful' which is useful for 'finding useful structures', as the more similarities in interface structures of a structure there are, the likelier that structure is to be useful, as on its own, one similarity may not be enough for usefulness
      - the more interface structures that two structures have in common, the likelier it is that they have the same meaning and related structures of meaning like usefulness
    - having a cause/intent in one position of a system means its likely there are other causes/intents that are similar to that cause/intent bc there is no 'limiting rule enforcing uniqueness of structures in a system' and 'repetition of structures in a system is a common attribute of system structures', so 'identifying if the structures both exist in the same system' is another useful intent to fulfill in finding out whether a 'structural similarity' is relevant to intents like 'find all examples of a structure'
      - 'having the same cause/intent' means that those causes/intents exist and are likely to be repeated (occur elsewhere), and the causes/intents are likely to have the same output when found elsewhere

- identify & store the interface structures associated with interaction levels that make those structures more useful for useful structures like 'dichotomies' (like by fulfilling 'required' or 'otherwise useful' intents) in fulfilling a problem-solving intent like 'fulfilling as many problem-solving intents as possible'
  - example: the 'attribute interaction level' would have identifying differences like the 'solution metric value (such as the accuracy) of a particular solution function'
    - the 'adjacent structure interaction level' would have identifying differences like 'similarity of sub-function or function component sets with solution functions'
    - these interaction levels are optimal in both different & overlapping contexts, for both different & overlapping intents
    - different operations can be fulfilled more adjacently on different interaction levels, like:
      - how components on 'adjacent structure interaction levels' (such as 'function components' or 'incomplete functions' or 'sub-functions' or 'input functions') can be more adjacently converted to a solution function with fewer steps than other formats
      - how components on 'attribute interaction levels' can be more adjacently abstracted, as they already depend on a particular attribute, so abstracting the attribute or removing components of the attribute's interaction with specific structures or specific problem/solution structures (removing the 'solution function' specification to allow connecting the attribute to other functions like 'solution-finding functions') is an adjacent operation on that interaction level
    - 'finding a solution function with an accuracy level' may be more useful than 'finding a solution function composed of known incomplete solution functions', given the associated interface structures of these interaction levels (the contexts, intents, & functions more easily fulfilled with those structures) and the 'info requirements', 'change requirements', 'success probability', 'general usefulness across common problem-solving intents' and other useful metrics of these interface structures & interaction levels
    - this applies the 'functionality/resources vs requirements' dichotomy to find useful structures for problem-solving intents like 'fulfill solution requirements', like 'functions fulfilled by applying an interaction level' (a mix of interaction levels & other interface structures)
    - this identifies a new useful function structure as 'incomplete functions' (useful for having a common error type built-in to a solution-finding function, that is resolvable with derivation methods like 'applying patterns', to fulfill useful problem-solving intents like 'apply solution to real information which is likely to have common error types, to "connect erroneous inputs to solution outputs" as well as "connecting correct inputs to solution outputs"', applying the insight that 'errors are probable in inputs' to fulfill the problem-solving intent of 'find a solution-finding function that handles common or otherwise predictable error types')
    - this identifies another new useful structure of 'different and overlapping structures (like functions/attributes)' as useful for finding commonalities, similarities & differences
  - differentiation: this workflow applies an abstraction ('dichotomy') to the core problem structure of 'differences' and finds interface structures that can find/build/derive problem-solving structures to resolve that problem structure, to fulfill the problem-solving intent of 'resolve problematic differences' (a variant of 'connect problem/solution'), as 'different and overlapping functionality' is a useful structure for fulfilling 'difference resolution structures' to fulfill 'requirements of a solution', thereby connecting the 'dichotomy of functionality/requirements' with 'interface structures that can adjacently fulfill its opposite structures (to resolve the difference between functionality & requirements)'
    - this workflow connects interface structures on multiple interaction levels (the problem/solution interface, the structure interface, the difference interface), thereby making it likelier to be useful by default
    - a variant of this workflow would 'find structures that can also connect structures on these interfaces' as inherently useful structures to fulfill problem-solving intents
