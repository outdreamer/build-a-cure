- identify structures like 'connections' between useful structures (such as example problems, and connection structures of intents like 'finding other useful structures, given one useful structure like an example suboptimal solution to solve the problem') to use as variables (such as components) of a solution or solution-finding method, to fulfill intents like 'find useful structures'
  - example: a function like 'multiplication' can be derived as the 'connection function' between variables in a data set (like a set like (2,4,8), (4,4,16)) that should be connected (like in the 'find a prediction function' or 'find causal variables' problem spaces) by interface structures like:
    - 'data set' -> 'vectors' -> 'separate input/output' -> 'data type' -> 'identity unit' -> 'stack identity units (bc they can be fulfilled with existing operations (added without additional operations to calculate a ratio) and require few transforms of inputs (identity value of x) so theyre likelier to be relevant/useful)' -> 'fill' (create area of 8 or 16)
      - convert inputs into vectors, separate into possible input/output sets (x,y combine to create z the bigger or higher variation value), apply changes that would produce the same data type (scalar) since the intent is to connect variables of the same data type (scalars), stack the output in units like integers to represent a 'unit/identity combination' (which can also be added rather than multiplied for efficiency, and representing an identity unit like (1 * x) of one type of change like the x variable to avoid changing this variable to get the combination), stacking them in order to 'fill' the structure which is a common intent (fulfilling other common intents like 'find area of a structure')
      - stacking means applying the output in a way that explains the x/y variable connection function (multiply), in the form of applying the outputs (8, 16) to units and/or ratios of the other variables
        - if x = 4.5, using the identity value of y = 1 is useful to avoid calculating a combination of ratios (x * 0.5 + x * 0.5, or 4 * 1 + 0.5 * 1)
        - another useful unit would be an adjacently combined unit (if you can derive that the y value can be separated adjacently into n components like how y = 4 can be a combination of 2 units of 2, calculating one of the components like the value of the area at 2 is useful)
        - determining the value of an area that is an integer is more adjacent when a lattice at integers is applied, at which point the multiplication can be replaced by finding the intersection point of the two constant lines nearest to an integer (estimating from far away is possible with a lattice at integers, similar to how estimating an area that includes a half-unit factor like 2 * 4.5 is also easier when the integer lattice is known/visible as the midpoints of integers like 4.5 are more easily identified with the integer lattice)
        - meaning a constant line at y = 1 up to x = 4 representing 1 * 4, a constant line at y = 2 up to x = 4 representing another unit of 1 * 4, these two constant lines creating by stacking the 8 vector up to x = 4 at unit values like y = 1
        - the 'integer' unit is used to create 'stackable components' like 'areas' from the scalar values ('areas' being combinable into other 'areas'), converting a value like x = 4 into a line at y = 1 up to x = 4, then into an area by applying y = 1 (a unit of change) and y = 0 (no change) as boundaries, filling the structure between the boundaries of y = 1 and y = 0, which applies the definition of area ('shape with connected sides'), applying these boundary lines and the unit integer line (x = 0 up to y = 1) as 'side' of a 'shape' to be 'connected'
        - "converting the 'input vectors' into a 'unit of area' or 'ratio of area', followed by converting the 'area units' or 'area ratios' into the 'complete area' by 'stacking area units or ratios'" is an 'input/output sequence' workflow that can be applied once the inputs/outputs are separated (to solve the problem of 'find a prediction function for z, such as by applying common/adjacent operations to x and y' or 'find a way to determine area, such as by applying common structures like repetitions')
        - the units are stacked until a ratio is reached (like if y was 2.5) in which case the same stack operation is applied (a constant line at y = 2.5 up to x = 4) at the limit value (as opposed to a unit value like the next integer such as 3)
        - the 'integer' units are useful for addition operations bc they dont require finding a 'ratio of area', (the integer unit applied to x = 4 is itself, requiring no further operations to find the value to be added, so no multiplication has to take place, just repeating the value of x is enough to find the integer unit of x at y = 1, as repetition is lower cost than multiplication) which is a more efficient operation than finding a ratio of area (which is why the program wouldnt first multiply 16 x 2 to get half-unit areas by applying the same 'stacking' operation at y = 0.5, 1, 1.5, bc it involves applying another operation to an input such as multiplying it by 2, just to require more unnecessary steps, of having to add more values to get the area, without adding value in the form of useful information, except in cases where another unit like 0.5 is relevant, such as if an input like y = 2.5 was created by halving a value like 5, indicating that formatting 2.5 as a 'combination of 5 units such as 0.5' would be relevant information to retain)
        - once you know that x = 4 and y = 2.5, you also know that you can just repeat x once (to get 8) and then the only remaining work is to find the area of the ratio y = 0.5 applied to x
        - "finding 'repeatable' or otherwise low-cost units" is a useful intent to fulfill to avoid higher-cost operations like 'determine the area by multiplying inputs'
    - an alternate way to derive the concept of 'multiplication' is finding the variables required to describe a shape with area (side length of a shape) which are also the highest-variation variables of the shape
    - an alternate way to derive the concept of 'multiplication' is applying a common operation like 'repeat' such as 'repeat 4, y - 1 (1) times' (to get 8)
    - the fact that 'repetition' and 'combination' can be used to connect the variables is a result of these structures being common, useful, probable, and core (simple, composable) structures
      - other more complex structures can be used to generate z from x & y (y ^ x / 1/(x ^ n)) but given alternate input/output examples in the data set, these structures are easily filtered out if not applicable by easily generating a test point example which would disprove a more complex possibility
      - other more complex structures (like create a parabola from each input and then apply an operation to reverse the previous operation) can be filtered out for being pointless, having no reason for the extra set of neutralizing operations, as they dont change the input/output
      - what these structures in these alternate methods have in common is their useful structures like efficiencies which make these alternate methods more useful than other methods (like with 'pointless neutralization structures' or 'more specific/complex structures and less probable')
  - this workflow involves identifying alternate methods to connect inputs/outputs (such as by identifying the components ('side lengths', 'side length units', 'side length unit identity', 'side length unit area') at various interaction levels (vectors, ratios, unit, areas) of the solution format (the 'area' being found in the problem of 'find a way to determine area of a shape') and finding alternate methods of deriving them such as 'find adjacent (repeatable, stackable) interaction units (integer and ratio unit areas, area unit combinations)' to 'connect those interaction levels' in an 'input/output sequence' ('start with inputs' -> 'find area units of inputs' -> 'find area combinations' -> 'find complete area'), as those 'interaction levels' are 'connection structures' of the inputs/outputs (input vectors, output areas)), 
    - then finding the common structures of those alternate methods ('efficiency', 'interactivity'), and applying those common structures to fulfill the intent in different ways ('repetition' is used in multiple alternate methods bc its an 'efficient' low-cost structure, and bc its 'interactive' with many structures, which makes it likely that it can be applied in another way that would be useful), this 'interactivity' making the efficiency structures useful for fulfilling intents like 'creating alternates', as 'interactive' structures have the potential for interactions with many different inputs, so applying them is useful to 'create differences' (which is an alternate way to 'create differences' than just 'applying variables')
    - identifying a useful example problem (like 'find an alternate method to derive multiplication' or 'find an alternate method to determine area') that makes this useful structure (the combination of 'efficiency' and 'interactivity') obvious ('adjacent') is the differentiating factor in this workflow, all from the starting point of the original 'different input/output sequence' (vector -> unit area -> complete area) to 'determine area', which made the 'repetition' structure obviously useful, made the 'alternate methods to use repetition in a useful way' obvious, and which made the 'reason' for the usefulness of the 'repetition structures' obvious (theyre highly 'interactive')
    - this 'connection between structures' ('identify useful structures like repetitions', 'identify alternate methods to use that useful structure in a useful way', 'identify the reason for the usefulness of the structure across alternate methods', 'find other structures having the attribute that is also the reason for the usefulness of the useful structure, such as by applying adjacent changes to the attribute (changes in interactivity) of the useful structure (repetition) which are also likely to be useful, or find other interactive structures') is a useful structure to fulfill intents like 'find alternate useful structures'
  - a problem that can adjacently derive the usefulness of a common structure (like a repetition structure) is one where a common structure like a repetition structure can be adjacently determined (using integer lattice/constant functions) or required (requiring 'unit combinations to be used' is allowed in the 'find the area' problem space)
    - this relates to another useful structure like the 'opposite of requirements', such as where a structure is 'not required to exist' ('not definitely existing' or 'has the option of not existing') or is 'required not to exist' ('impossible' or 'doesnt have the option of existing') or 'not required to not exist', ('not definitely not existing' or 'can exist' or 'has the option of existing') rather than being 'required to exist' (applying opposite to either or both structures 'required' and 'exist')
      - this structure is useful to determine filters of varying certainty like possibility/definition

- identify useful structures like 'useful inputs like "structures of suboptimality" to find useful differences to apply' to fulfill intents like 'apply changes to a suboptimal base solution'
  - example: a '"rectangular data set" or a "data set representing parallel sequences of points" that is exactly intersecting with a parallel line that represents its sequential average value' is a structure that represents a case where a 'straight line' is the correct solution format, so differences from this 'rectangular' data set structure indicate where differences should be applied to the 'straight line' solution format of the 'prediction function'
    - similarly 'differences from differences from the "rectangular data set" prediction function, where differences represent limits that should be opposed' are another useful structure that can be used to derive useful differences to apply to create a prediction function
    - meaning if a data set doesnt have a 'rectangular' shape, its unlikely to be representable with the straight line that perfectly represents that data set, so finding out whether a data set is rectangular is a useful function to apply to filter out 'straight lines' as a possible solution or applying associated changes to a base straight line to improve the solution
    - this workflow involves identifying the cases ('specific data set inputs') where known prediction functions (or structures of them, like line 'straightness') would apply, identifying tests of those cases, and applying those tests to filter out those cases or find useful differences to apply associated with those cases, 'useful differences to apply to improve a suboptimal base solution' being a generally useful structure to be able to find adjacently

- identify alternate methods to derive useful structures like 'relevant differences' of 'structural similarities' to problem/solution differences, which can be used to fulfill problem-solving intents like 'find alternate solution-finding methods'
  - example: the 'specificity' of a solution format (like a 'predicted output value' of a 'prediction function', which is useful bc it has fewer computation requirements than a 'range of solutions' and is therefore more 'actionable') can be used to infer the usefulness of other structures capable of 'building specificity' like 'filters'
  - the 'solution range limits' (functions representing two opposing limits on a possible solution range) have a 'structural similarity' to other useful structures like 'maximally different solution functions' (which might be similar to the 'solution range limit' functions, more similar than they are similar to the average)
  - identifying the optimal positions of specificity is similarly useful as identifying the relevant change structures, bc structures which have a specific requirement can be used to filter possibilities, like filtering possible solutions of which change structures are relevant
  - 'incremental changes' have a 'structural similarity' to interface structures like 'core components' and an 'adjacency' to 'specificity' of the solution
  - alternate methods of finding 'maximally different solution functions' include:
    - applying a 'solution range limit' function and an 'average' function as 'functions to be averaged' to produce a possible solution function that is within a 'more computable' range of difference from the 'average' and the 'data set limits'
  - these structures have an 'alignment' in the 'differences between the problem/solution'
    - the problem can be formatted as the 'lack of specificity of possible solutions' given the available 'more computable range of possible solutions', and the solution can be formatted as 'specificity applied to that range'
    - the problem can be formatted as the problem of 'finding a difference' between 'solution range limit functions' and 'average/representation functions', given the fewer requirements to compute these as adjacent structures to the input data set, and given the 'known inaccuracy/suboptimality of these more computable functions' (these more computable functions are 'different' from the solution structure of an 'accurate prediction function'), which combines the 'apply changes to suboptimal base solutions' and 'filter solution space' problem-solving intents
    - the problem can be formatted as the problem of 'finding a representative interim structure between maximally accurate and maximally simple functions', given the difference between 'probable outputs of accuracy and simplicity', given the 'known suboptimality of these maximally accurate or maximally simple functions', where the solution can be formatted as a 'representative difference from opposing suboptimal functions'
    - "finding 'aligning differences/similarities' between problem/solution structures and other structures" is useful for problem-solving intents like 'connect problem/solution'
    - this applies differences/similarities in a useful position in the interface & problem/solution structure networks, without violating their definitions
  - finding these 'aligning' structures is a matter of finding 'relevant differences' having 'structural similarities' or other similarity structures to the original problem/solution difference, like:
    - the 'difference from suboptimal functions or solution function limits', which has a requirement of 'identifying suboptimal functions or solution function limits' to fulfill the intent of 'find functions that are different from suboptimal functions or solution function limits'
    - the relevant difference between 'similar structures of problem/solution structures', which is relevant for intents like 'connecting those structures, as approximations or more computable queries than the original problem'
    - the 'relevant difference in solution metrics like specificity/accuracy/curvature, found in adjacent problem inputs (like more computable suboptimal functions) or the original problem input (data set)' to 'identify possible functions fulfilling those requirements' to apply differences to those problem structures (such as creating a set of possible solutions as a set of functions with 'curvature, some level of accuracy, some level of specificity' and applying other filters to this set)
  - '"relevant differences" with similarity structures like "structural similarities" to problem/solution differences' (like connecting "adjacent" or a "subset" of problem/solution structures) are a useful difference/similarity structure to apply to problem/solution structures, which is dynamic in a useful way, as it can adjacently identify useful structures like 'alternate solution-finding methods' when one is suboptimal
  - these structures apply the 'interface' structure, identifying problem/solution structures as an 'interface' to apply changes to, to find 'relevant differences' with similarities like a 'structural similarity' to the original structures

- identify 'extreme differences in similarly useful structures' (like 'alternates' and 'approximations' being similarly useful for 'reducing requirements') is useful for reducing the steps required to identify alternate similarly useful structures, such as 'finding coordinating changes that create similar optimizations of the same metric like steps required', these coordinating changes being useful to identify
  - example: identifying 'ranges of limits' of a 'possible function area' or 'set of alternate possible functions' is more useful in that it requires fewer steps than alternatives like identifying 'limits' of a 'function area' or identifying the 'optimal function', as a 'range of limits' can be filtered with 'low-resource steps' like 'random selection' to find the optimal function
  - this is similar to 'how viewing a data set from a different angle' can reduce the steps to 'identifying an average value of the data set for one variable', 'averages' being useful for 'finding base solutions to apply changes to', in the 'find a prediction function' space
  - applying 'approximation' structures like 'ranges' and 'subset' structures like 'alternating subset' or 'random subsets' are similarly useful in reducing required steps, 'approximation', 'alternate', and 'reduction' structures like 'subsets' having an overlap in their reduction of 'requirements' ('approximation' reducing the required tests or solution metric value requirements, 'alternate' reducing requirements by finding alternate structures to invalidate a specific requirement, and 'reduction' structures having a direct connection to 'reductions in requirements')

- identify structures like spaces where the most uncertain variables become the most differentiable/filterable and sortable/organizable such as the 'inputs to usefulness of a interface structure combination', which is the more optimal structure to graph as the output bc known usefulness isnt useful enough to allow the types of uncertainties typical in predicting usefulness of a 'new structure' or 'for a particular new intent'
  - example: identifying that alternate definitions such as 'networks of additive incremental changes with change limits' and 'try every possible solution in the set of possible changes to apply that increases a solution metric (pursuing a direction of change with a sufficiently consistent sequence of rewards incentivizing additional incremental changes in the same direction)' and 'filtering a reduced set of probable solutions' have an overlap in the opposing structures to the change structures ('limits on changes', 'solution metric used as a filter of possible changes', 'pre-applied filters like probability that are reduced further by additional filters'), and the common areas of this overlap can be used as an interface to base other changes on and find alternate opposing structures that can be used to derive the solution-finding method
  - finding alternate definitions that have enough information to identify a possible overlap (including information about both the change and the limit) is a matter of identifying this core structure of a 'change and a limit' as sufficiently useful to solve problems like 'resolving a difference having some ratio of complexity'
  - identifying a space where these alternate definitions are most frequently adjacent is a useful structure, to identify possible useful interface structure combinations that could indicate the probability of 'possible meanings of a structure' (probability indicated by adjacence in that space)
  - identifying a space where the similarity is determined by a structure other than the actual solution structure, like a space of different interface structure combinations where similarly is not calculated by the usefulness of each combination for various known intents, but by adjacent structures like inputs to the usefulness (such as 'compounding/additive structures', 'measurable structures', 'differentiable structures), which can be used to determine the usefulness, as determining the usefulness for known intents is unlikely to be useful for predicting usefulness of any given interface combination for a new intent, bc adjacent changes in interface structures can produce extreme changes in usefulness, so known usefulness is not a useful differentiating metric to create such a space of possible useful interface structure combinations
  
- identify useful structures like 'connecting structures formatted as combinations of interface structures (like conditions and changes)' which can be used to check for cases where that structure would be optimally useful to apply, given the relatively low number of cases and the relatively high reduction in requirements (which offers an improvement on 'trial and error' requirements)
  - example: the 'meaning' of an 'incremental/adjacent change' could be possible interactions of interface structures like 'crossing a threshold' or 'triggering a phase shift' or 'completing a set', or 'conditional' structures like its 'standard isolated definition (an adjacent change) unless other incremental changes are applied in an additive structure', structures which are adjacently testable/filterable to check for or adjacent structures being similarly testable (such as testing for the input conditions required for that structure), as applied to problem/solution structures like 'crossing a solution metric threshold'
  - filtering this set of 'possible meanings' is a useful way to reduce possible solutions like 'possible changes to apply that would reach a solution metric value', such as 'translating a value like a weight in a neural network to a possible meaning (like a relevant output to the solution prediction function)'
  - similarly the possible meanings of a filter include the standard definition like 'filtering a set' but also 'removing the best solution'
  - these alternate possibilities can be generated by finding interactive interface structures to identify the possible outputs of a structure, such as how a 'solution set' can be a possible input to the 'filtering a set' function, which are interactive structures bc of the similarity in the 'set' structure
  - the connecting structures of these interface structures have useful structures, such as patterns in 'lack of jdentifiable interface structures' which can be applied to predict the position of interface structures adjacently or distantly producible with accessible changes
  - the structures of intents that a structure like an adjacent change is 'not useful for' can have interactions that make them useful, such as how an 'adjacent change' is not likely to be useful for 'creating big changes', such as where an 'adjacent change' is required to convert an 'approximately big change' into a 'big change', so these opposing structures can be adjacent in some cases like where 'several variables are immediately adjacent to thresholds that would trigger change cascades when combined'
  - given the potential of 'variables near thresholds that produce change cascades when combined' to make 'opposing structures adjacent', that structure and similar structures can be applied to check for similarly useful connections to create, such as 'structures that would be more optimal when adjacent', such as a 'set of inputs that should be adjacently combinable into outputs' (like when 'reducing the solution space' to adjacently find a solution)
  - checking for cases where these structures ('near-threshold changes that can produce change cascades when combined') would be optimally useful is trivial and would allow skipping otherwise required steps if the case applies
  - the number of cases where these useful structures are optimally useful is relatively low by comparison and offers a reduction in the search space of useful structures to apply, and is likelier to be lower than the possible reduction in requirements, even when applied with other case tests, given that these are edge cases

- identify useful structures like the 'solution success cause' of specific useful structures like 'filters/reductions' such as being 'applicable to different variable subsets of the items to be filtered/reduced, like solutions' which can be combined in a trivial operation like 'addition', 'different variable subsets' being trivial to identify, and 'filters/reductions' of variable subsets being trivial to derive (as 'the maximum differentiators of a set') to identify structures ('additive filters of different variable subsets') that have increasing value when that trivial operation is applied
  - example: apply the 'reason' why a structure is 'additively' useful, such as that it 'filters/reduces' 'different variable subsets' (variables of the items to filter, like possible solutions), and why 'additivity' is useful ('adding different filters/reductions' is a 'trivial operation' to fulfill intents like 'filter solutions')
  - other trivial operations are similarly candidates for 'possible components of useful functions (like a function to "add filters to filter a solution set") applied to useful structures', like 'multiplication' (multiplicatively useful such as when filters have a factor of value when combined with another filter, an increase that is greater than the addition in value, or exponentially useful such as when the same filter has increasing value when repeated)
  - this is similar to identifying a solution metric, identifying a value that optimizes it, and reverse-engineering a solution by applying that required solution structure:
    - like applying a 'reduction' to 'every' in 'try every solution' and applying that 'reduction to every' as in 'not every' to identify gpossible improvements to 'try every solution')
    - but in this case, applying a 'filter of different variable subsets' as a useful way to filter possible functions that could be 'useful filters', useful in the sense that its trivial to make them more useful (by 'adding them')

- identify useful structures like aligning structures (such as the 'size' attribute) of useful structures like "connections between 'attributes of patterns' and useful structures like problem/solution structures such as 'base solutions'" that can be used as inputs to problem-solving intents like 'improve a base solution'
  - example: a 'bigger' pattern in a data set is likelier to be a more 'general' pattern just like 'small incremental changes' are likely to be useful for improving accuracy as theyre more specific structures, but thats not explicitly specified in the standard isolated definition of 'general', making it useful to connect to the definition so structural variants or other variants of the definition can be applied in place of the standard isolated definition, so 'bigger patterns' are likely to be useful as 'base solutions' to describe general trends to apply incremental changes to specify and improve accuracy, to fulfill intents like 'improve a base solution', so 'finding big patterns' can be used as an initial step in a solution-finding method to 'find a prediction function', as 'incremental changes' are more useful when 'big (general) patterns' are known, and 'big patterns' are more easily testable/filterable than 'small patterns', so 'big patterns' are also a structure that makes other useful structures like 'incremental changes' more useful

- identify useful structures for inputs like 'maximally different functions' to problem-solving intents like 'apply changes to base solutions'
  - example: generating maximally different base solutions by randomly selecting a starting point and finding the 'most different point' from that point in the data set, as the next point in a sequence, to find general change patterns in the data set from these 'most different points' from the previous point in a sequence of subsets of the data set
  - another example is generating maximally different base solutions by applying connections of a random subset of points where the only structure applied is finding a point in a subset of the data set, as a random subset of points is likeliest to produce maximally different base solution functions

- identify useful structures like 'overlapping functions' that are the most filterable (the 'maximally different' functions in the set), then find the interface structures like the interface function of that set to find useful filters of functions of that set, such as functions being 'above a threshold value of difference from the symmetry function' being less optimal due to over-specificity/generalization errors
  - example: identify functions that can produce a subset of input/output examples, identify the symmetries in that set of functions that act like an interface on which changes are based, and identify the filters that are most useful for filtering out the functions in that set
  - for example, if x^2 and x^4 overlap on some examples, find the full set of functions having a similar level of overlap (but having different change structures), then find a representative function to use as the symmetry function in that set of functions, then find variables to filter those functions the quickest (rule out the most suboptimal functions the quickest)

- identify useful structures like common structures like 'spectrum variables' which form other useful structures like 'continuous areas' representing useful structures like 'probability' of a 'representative input/output connection (a representative/descriptive point like an average/extreme such as an optimal point)'
  - example: if there is a unique optimal point in a set of possible solutions, its likelier to be found in a set of continuous points forming a dense cluster than it is to be an outlier, so identifying these dense areas of continuity is a useful filter to apply when finding optimals, as a solution is likelier to 'be slightly different from adjacent suboptimal points' than it is to be 'extremely different from known suboptimal points near the solution metric', given the commonness of spectrum variables compared to other variable types, and given the likelier contribution of spectrum variables in creating a continuous area or another structure of adjacence (like iterative curvature) than other variables are

- identify useful structures like 'sorts' which make other structures like 'filters' useful for various problem-solving intents like 'reduce steps required' such as by 'reducing the solution space' in a particular problem format like 'filtering a set'
  - example: the 'sort' function is useful for other functions like 'skip' and 'halt' functions, which are functions that 'make specific filters more useful than other filters', such as 'predict', 'reduce steps', 'select first solution having a metric value', 'connect alternating subsets', 'find patterns', which could be applied with implementations such as:
    - apply multiple sorts to a set to be iterated in a way that can halt after a value is found, to iterate in parallel and optimize for the probability of finding a solution quickest
    - find halting structures like 'differentiating conditions' that a particular sort could optimize for, 'differentiating conditions' which can be compared to solution metrics for their ability to approximate/replace/build solution metrics
    - find filters that would filter the most possible sorted variants of a set (where each variant is generated by sorting on a different attribute to apply a different definition of similarity in each sort)
    - finding 'skipping' patterns that would be the likeliest to iterate a subset of the sort more optimally than attempting every possible value in the sort order, given the pattern identified by the sort

- identify useful structures like 'differences from suboptimal structures' or 'differences from info used as input to alternate filters' as useful to identify structures that are more useful the more different they are, like 'filters of complementary info instead of the same info, which can be applied additively' or 'differences from error structures'
  - apply 'differences' to the suboptimal structures in a workflow, like applying 'differences' to 'every' in 'try every possible solution' can reduce the steps required to solve the problem, even if the resulting algorithm is a trivial improvement
  - example: applying 'differences' to the 'solution metric requirement used to filter solutions' can identify alternate structures that would 'reduce the resources required' to 'find the solution'
    - for example, if a solution metric test 'requires four steps on average', but another function 'reduces the search space by half and uses less than eight steps on average', that function is a possible improvement to the solution requiring applying four steps to all solutions, so all other functions fulfilling the same improvement metric are possible improvements to the original workflow that applies the test to every possibility, which reduces the search space of possible improved workflows, which can be filtered by applying other workflows, such as workflows evaluating the 'solution success cause', such as 'solution set filters' that have a reason to filter out possibilities
      - such as how 'randomly removing possibilities, or randomly trying a subset of possibilities' has a reason why it would be useful, such as 'being unlikely to exclude enough data to indicate the general pattern of solutions and being unlikely to exclude the optimal solution or a near-optimal solution' and reducing the steps required (reducing 'trying every possibility' to 'try a subset of possibilities')
      - or alternatively how 'selecting alternating subsets' is useful bc 'once a set of alternating subset is known, the adjacent connecting subsets are more trivial to infer, since connecting adjacent structures like alternating subsets is more trivial than connecting non-adjacent structures'

- identify useful structures like workflow filters using info that derivable by definition or likely to be accessible such as 'workflows with clearly testable optimal conditions', to select & apply more optimal workflows for a problem
  - identify cases where the set of workflows can be filtered as more optimal for that case, like where:
    - having minimal information makes trial and error more useful, or where 'applying more filters isnt better, even given the possible additive usefulness of filters' if there is 'one filter that would accurately filter solutions'
    - 'randomly selecting a subset of a function to find local optimals' in that subset can only fail absolutely in some uncommon situations which are easy to test for (such as where the subset happens to be a group of outliers that dont indicate a useful developing future or conditional alternate state of the data), to filter out the possibility of this workflow being extremely suboptimal
  
- identify positions in a standard problem format like the 'find a prediction function' format where optimizations can be applied to reduce required resources to fulfill a problem-solving intent
  - identify 'subsets of the function' as 'separate search spaces that probably contain a minimum', and then apply methods to find the determining or optimal structures of that subset like the 'minimum' or 'probable minimum' in those subsets (such as 'iteratively identifying subsets of a subset to identify probable patterns connecting subsets to avoid identifying all subsets, up to the level of granularity of subsets that doesnt usually involve a change structure like a direction change', etc), which reduces the steps required to find local minimums while increasing accuracy of the local minimum selected, solely by applying approximation structures (checking a few points rather than all points necessary to infer the difference of one subset from another)

- identify & apply useful structures like 'incompleteness' as a way of identifying 'structures useful to complete' such as 'unconnected structures in a problem space' to connect with interface structures to fulfill useful intents like 'reduce steps required to fulfill a function', in useful positions like where 'every possibility is tested' that are optimal to reduce
  - example: connecting different variable structures like 'connections between incremental changes' and other interface structures like "patterns/limits" that make these structures easier to fulfill an intent like 'filter' with, given that theyre a high-variation variable that is useful to filter (filter the 'probably useful change structures to apply to inputs')
    - 'the combination of a "change and limit" structure' of an 'incremental change vector and its related limits like limits it tends to contribute to reaching/exceeding'
    - 'the difference between a base/optimal solution function that "incremental change vector sets" can connect'
    - 'patterns of combinations of incremental change vectors that stay within or exceed useful limits, like limits on changes that are likely to keep incremental changes within a probable solution function range or result in changes that are combinable with other changes without exceeding the function range' which would enable more quickly identifying 'sequences of variable combinations' to stop/continue investing in when patterns of invalid/valid variable combinations using incremental change variables are known, to filter change structures like change sequences or combination sequences to stop/continue investing in
    - 'incremental changes' are a useful structure bc they allow a suboptimal solution to be iteratively changed to test if iterative changes can produce an improvement in a solution function metric, but reducing the number of changes to test is useful bc of the search space, so any filter that can reduce these changes in a relevant way, like identifying 'change combination sequences that are likely to exceed a relevant limit' without evaluating the rest of the change sequence (reducing the changes to test), is useful for reducing the possible values of this useful structure
    - structures become more useful when they are complete, such as when a change is accompanied by a filter/limit on that change for a particular intent like 'connecting a suboptimal/optimal function'
    - similarly a 'function that applies an incremental change and checks if the output exceeds a limit' is incomplete in the sense that a 'trivial change' like an 'incremental/adjacent change' on its own is unlikely to be useful for an intent without other structures like 'other incremental changes/limits' and 'combinations of these change/limit structures', which is a way to derive whether a function is worth applying non-adjacent changes to (to generate a function structure like a 'neural network'), as most problems involve non-adjacent differences to be resolved, so an adjacent change is unlikely to be a 'complete' solution
    - similarly a 'curved' function like a polynomial is incomplete without identifying the core components ('linear variables') that can be used to build it, so a solution that applies 'incremental changes' would likely be able to apply these 'linear variables' in a 'complete' solution
      - similarly a data set (a 'set of related points') is incomplete without the 'patterns, change vectors, filters, areas, or connections relating those points'
      - these 'incompleteness' structures of 'high-variation variables' can be used to infer 'interim connecting structures' that can be used to 'complete' them, so the resulting structure is more meaningful in the sense that its useful (reducing the steps required to predict untested outputs in a complex system once the prediction function is known)
    - a variant of this is to identify error structures and apply them as filters, such as how identifying an over-specific and over-general function can act like opposing filters to create a probable function range from those limits, and once this 'probable function range' is known, changes that are likely to be used to identify the correct function in that range can be calculated and applied, rather than applying 'every incremental change until local error minimums are found'
      - similarly 'finding downward changes' in a function is approximately as useful as 'finding local error minimums', as a 'function to find downward changes' can be used in place of a 'function to find local minimums' to predict where to search (identify 'sets of probable minimums') as opposed to identifying a specific value (identify 'one local minimum')
      - filtering these 'downward change structures' for 'opposing structures' like 'unexpected upward changes' is a similarly filterable solution space by checking for the 'most disadvantageous (worst case) upward changes'
      - after checking for interface structures like 'phase shifts' or 'randomness', change structures are more predictable, reducing the probability that a change will deviate from predictions based on change patterns and filtered out interface structures
    - this involves connecting unconnected structures in a problem space like the 'find a prediction function' problem space which are not adjacently connected, so that these connections can be used in place of existing adjacent connections or to make intents like 'reduce required steps' more possible/useful

- identify useful structures like the 'alignment between the usefulness of separating variable differences within that variable' and the 'separating structures that can produce that usefulness' like 'ordered sequences', as well as the 'structures that can preserve useful info like info about high-variation variables' by identifying the 'right position to apply differences/similarities in' to highlight/preserve that information (differentiate horizontal position, apply similarity in position across example input sequences, in order to differentiate changes in a particular variable, as opposed to applying the same standard to all values like graphing every input as a set of y-values, which would differentiate the values but remove the information of which variable corresponded to which value by failing to isolate them horizontally, attaching just enough meaning to the x variable to fulfill the 'isolation of change sources' intent)
  - example: the 'sequence' structure can be useful for other intents than just 'applying an order to a set', such as 'outputs of order', including 'isolating change sources like variables' and 'comparing differences in a particular change source' by separating the differences into a vertical structure and applying structure to the 'order' so the differences in a variable can be compared across examples and isolated from other variable changes
  - similar to how representing an 'example input of a data set' as only a 'set of y-values' takes away the information of which variable has changed by applying the same standard to all variables (converting them all to y-values on an x-y graph) which ignores the fact that theyre likely to be different in structures such as their change structures and causes so isolating them is a valuable structure for intents like 'comparison of differences in a variable, in isolation of other variables'
  - this is another 'expansion' structure that is useful for identifying relevant structures like 'high-variation causing structures' or 'determining structures', similar to how storing 'inputs of a data set' in a sequence or 'storing interim variables in sequences of sequences' to 'create a volume from a data set that would otherwise have one output variable' is a useful expansion structure to counteract the info loss of the standard function format
  - identifying extremely useful structures like the "alignment between the 'variation' (of separate variables) and the 'abiility of a sequence to preserve that variation' (by applying order and horizontal separation to the variable values) to prevent 'variable interactions'" is useful to identify as a way of finding structures that are likely to make core intents useful

- identify formats that are useful for preserving info that is usable in more workflows than other formats or a specific workflow that can benefit from that info
  - example: preserving the info of the data set by formatting it as a volume created by 'sequences of input variable values' where 'adjacency of a sequence' represents similarity of inputs, as a way of making it clearer where the 'variable structure interfaces', 'change structures like outliers', and the 'error structures like input example gaps' are, as opposed to the standard function format, which compresses a variable or variable set into another variable, removing the 'interim layers' of causation in a clearly visualized structure, as many different variables like coefficients can produce similar functions and many different input data sets can produce similar functions, and the standard function format removes that information that could more clearly differentiate those different inputs, meaning the other variables like 'causal variables of x' or 'coefficients of x' are not clearly visualized in a standard function format, but expanding the input data set by applying 'example sequences' as well as alternate/interim sequences applied to connect those input sequences with the output value(s) would retain more info that can be more useful to identify these overlaps in 'similar outputs' of 'different inputs' and error structures of the inputs
  - these 'sequences of input examples' applying order to individual variables of each example to enable 'comparison' can apply a scheme of connecting these example sequences such as connecting adjacent examples at unit intervals like integers, and can also be connected to the original outputs, where a 'sequence of inputs' results in an 'output' through additive operations, so the 'output' is the final item in the sequence
  - the standard function format has an error of over-compressing the inputs so that the interim cause and change structures, as well as the differences from 'different inputs that could produce the same outputs' or 'similar inputs that could produce different outputs' are less visible
  - similarly, representing the 'additive' or 'neutralizing' value of variables like terms of a function or coefficients can produce another alternate format like 'stack graphs of change components' where the shape of a function is the result of adding other functions that represent different isolatable sets of terms of the function when combined in their variable interaction structures, or alternatively the result of counteracting/coordinating variables (increasing/decreasing) or variables having impact on multiple variables (combination of 'horizontal and vertical') that can be represented in a similar structure as area, such as vector sets, which are better at representing negative/neutralizing values
  - finding a function to connect this format with 'changes in function statistics' like extremes, change rates, and change types like inflection points or 'points where a change pattern is determined' of a function is another useful intent to fulfill in the 'find a prediction function' problem space, as 'connecting interim change structures with output function statistics' is a useful structure to be able to apply if interim change structures are known or findable/derivable/buildable
  - these interim states like 'additive functions' or 'subset functions' or 'input and coefficient sequences' are useful to visualize bc they may be more useful in determining the output function or errors in that function than the 'inputs/outputs connected in the standard function format' on its own
  - another alternate formats include:
    - 'function compressing most info of a data set with the points that contradict the function enough to provide examples of a reason to change the function without providing a reason to create a separate conditional function for those points'
    - formatting variables as a network of core functions to describe adjacent/probable states of the variable, a direct input/output map (with no interim value states), a set of sequences connecting inputs/outputs (with interim value states), or a state sequence that describes the change sequences of the variable
      - when variables are formatted as a 'state sequence' (which can take the form of 'adjacent parallel vector sets' or an 'area formed by adjacent parallel vector sets' or a 'function describing the value changes of the variable'), the change contributed by each variable is more clear
      - aligning variables on the same graph is somewhat useful for visualizing the impact of each variable in a standardized way, but forming a sequence is more useful for visualizing additive changes
      - visualizing variables as a 'sequences of areas' or 'sequences of vector pairs (representing a coefficient and variable)' can also be done by converting the function into an 'addition of multiplied terms', where each area in the sequence is the product of a coefficient and variable in a term, and the sequence of these term areas can be additive to visualize the contribution of each term to the output
      - this involves formatting functions like 'filter' as a multiplication operation (such as that creating a fraction of an input since a filter reduces the input to a subset) or a subtraction operation (removing some items in the input)
        - once functions are formatted this way (as additions/multiplications), they can be more clearly visualized and their impact also visualized in a standard graph, which can be used in a sequence to illustrate variable combination states building an output
        - illustrating 'causation' in a graph like this can be simplified to illustrating structures of cause (like 'direction/position in an input/output sequence', 'requirements', 'system context', 'interactivity', 'im/possibility or probability') by their corresponding structures (like 'adjacent points in a sequence', 'limits', 'previous state', 'possible combinations', 'impossible or probable states')
        - cause is also easier to illustrate when variables are formatted as areas/functions/vectors, connecting the points in those structures that indicate the 'usage sequences' that connect variable values in 'variable interaction' structures like 'variable combinations', as well as other interface structures like error structures such as 'false similarities' which are easier to identify once variable values are visualized in this way
          - aligning variable changes so that a 'variable structure (area/vector set/function)' is aligned in its input/output sequence with other variable structures, so that a straight line can be used to 'connect a sequence of caused variable values' in the 'variable usage sequence', is another useful structure for visualizing relevant variable values (organizing relevant variable values as on the same vertical or horizontal line, so that causation is more identifiable) applies a useful standard that allows comparing differences across 'causal variable usage sequences', formatting variables as either a sequence of 'possible values' or a 'sequence of sequences' like a sequence of 'possible inputs' followed by a sequence of 'possible outputs' or the same structure applied to 'sequences of input states of a variable'
          - if these sequences arent used, the alternative is using an additive property like opacity to represent 'additive variable impact' which has more ambiguity than sequences
      - these alternate formats have associated costs, such as how using a function to represent a variable can over-generalize to example inputs that the variable cant handle by assuming continuity where a limit should be assumed instead for a particular input/output

- identify useful structures like 'inputs to base solutions' such as 'linear functions representing subsets representing isolatable patterns of the data set' created with available resources like applying 'average' to 'extremely different alternate subset connections' to fulfill workflows like 'improve a base solution'
  - example: applying 'representation' structures like 'averages' can be useful in an intent like 'finding a merged function representing two linear functions', once two linear functions are found that would be useful to find the average of, such as in positions where a 'shape of the data set range' is known (created from 'limits of the data set') and 'alternate extreme connection structures' (like alternate lines connecting 'alternate extreme values' of a subset of the data set range, the subset indicating an isolatable pattern of the data set) are found to create the 'two lines to find the average of', fulfilling intents like 'find representative structures of alternate connections representing subsets of ranges of the data set'
  - this is useful to find probably representative base solutions to use in workflows like 'improve a base solution'
  - 'find representative structures of alternate connections representing subsets of ranges of the data set' can be generated as a useful intent in the 'find a prediction function' problem space by applying sequences of useful structures, like how 'ranges' and 'subsets' and 'patterns' have additive value in 'reducing required steps' to describe the data set, and 'representation structures' and 'alternate possible structures' have additive value when applied in a particular sequence to find an 'average of possible alternate values' as a possible version of the 'likeliest value'
  - to increase the value of 'extremely different alternate connection functions' of 'data set ranges', finding the 'intersection point' of these 'extremely different functions' is useful to find the 'midpoint representing a subset' that can be used in other structures like '"local average" connection structures' (to create global averages, as in a function representing the whole data set rather than a local adjacent subset) to find the 'base solution' structure, as 'finding the intersection point' of two 'extremely different connection structures' of a 'data set range' is fewer steps than finding the average of the original data set range functions, since the 'intersection point' is obvious once the 'extreme different connection functions' are created, so this reduces the information stored in the representation structure (storing a point instead of a line) and may therefore reduce errors resulting from storing the line instead of the point to be connected with other representative subset points, given the unlikelihood that the original input is the most accurate input reflecting the actual data set, and 'connecting representative points of subsets' is likely to be fewer steps than 'connecting functions representing subsets', as functions are less likely to align at the endpoints and therefore are likely to require more steps to align them than just connecting representative points
    - this can be generalized to 'decrease the dimension of inputs to useful structures' as a way of generating intents to fulfill that would likely result in finding solutions in a more optimal way (such as 'decreasing the dimension of inputs to "connect subset functions" intents to "generate a base solution"', by finding 'points to connect' instead of 'lines to connect', finding 'representative points of subsets' being possibly fewer steps than finding 'representative lines of subsets')
    - a variant of this workflow is 'finding shapes within each subset of the data set' that are likely to contain the 'representative point', to 'reduce the search space of possible representative points' by identifying a subset of the subset which is likeliest to contain the most representative point without applying many steps (generating the space likeliest to contain this representative point by applying general limits and clear structures like 'obvious densities'), at which point a 'randomly selected point' can be found within that 'subset space of the subset of the data set', thereby reducing the steps required to find the 'representative point of the subset' to connect with other 'representative subset points'
  - this can be reverse-engineered from variable combinations of core structures like 'averages of connections', to find a position to apply that structure in which would be useful for some intent relevant to solving a problem, such as how finding a way to make 'averages of connections' useful in the 'find a prediction function' problem space would be to find a position where connections should be averaged, like when there are two connections with 'opposites' or 'alternates' that are unlikely to be useful (in the sense of being 'representative') but in different ways (such as 'having different errors', like opposite errors) which would be useful to apply 'representation' structures to (average) in order to reduce these errors, thereby identifying 'averages' as a way to 'reduce errors' when 'errors are extremely different', and then finding a way to make 'averaging extremely different errors to find a more correct function' useful by finding 'extremely different errors' (subset functions connecting 'alternate extreme values' of 'data set ranges') to apply as input to the 'average extremely different errors to find a more correct function' function
    - this method can be generalized to applying structure to core structures ('averaging alternate connections') to problem/solution structures ('averaging alternate connections to reduce errors') and checking if those problem/solution structures are identifiable/generatable with available resources ('finding alternate connections with errors to average') after applying requirements of those functions to identify where they would be useful ('extremely different alternate connections with errors' would be useful to average, as opposed to 'very similar alternate connections'), then finding structures to generate those requirements ('data set ranges' would be useful to generate 'extremely different alternate connections' bc these ranges represent limits from 'opposite angles' of the data set so have an 'opposite' structure explicitly built-in)

- identify useful structures like input structures to a useful core interaction function like 'merge' that can fulfill intents like 'reduce errors', once other useful structures like a 'set of suboptimal solutions' is known, given how errors within a difference range of suboptimalities are likely to offset each other
  - example: given a set of suboptimal solutions, a merge strategy can be applied to merge them in a way that reduces the impact of each error and neutralizes most of the errors resulting from the suboptimalities, as a 'merged solution of suboptimal solutions' is likelier to have fewer errors and be more optimal than any of the individual suboptimal solutions, given the likelihood that the error structures will neutralize each other, assuming a level of difference in the suboptimalities and a level of similarity in the solution structure to allow the merged structure to function as a better representation of the original problem's solution (find a prediction function for a data set)
  - similar to other workflows, this structure can be used to reverse-engineer the solution or solution requirements or interactive requirements, so that the solution requirements can be used to connect it to a problem structure or other interaction structures (like adjacent alternate solutions or abstract solutions)
    - the 'mostly similar solutions with trivial differences in their errors' is a useful structure to apply to find 'possible sets of similar solutions' that can be merged as in this workflow
    - given one suboptimal solution, applying trivial differences in errors (determined by solution metrics) can generate the other suboptimal solutions to merge as in this workflow
  - this is similar to 'additive effects of independent random variables' having a clear resulting pattern that can be relied on as a certainty structure
  - this is a useful workflow bc 'suboptimal solutions' are easier to generate than optimal solutions, and provides a way to link these more adjacent structures with more useful optimal structures
    - 'connection' structures between 'suboptimal' and 'optimal' structures arent always clearly optimization structures, but can be derived according to the intent of the problem, such as how a function that is a better 'representation' structure is likely to be more successful in the 'find a prediction function' problem space, so 'averaging a set of suboptimal functions' is likely to improve the 'representative' nature of the resulting function, meaning the resulting merged function is likelier to be optimal for having applied representation structures that are likely to increase the representative nature of the solution, which is useful for the 'find a prediction function' intent, which aligns with representation structures by definition of 'prediction' as 'finding patterns' (which includes structures like averages on which change is based, as a general pattern of change in the data set)
    - even though 'representation' structures arent always an 'optimization' structure for any problem (unlike 'efficiency' structures like 'reducing resources required to create the same information'), they can be derived as an optimization structure in the 'find a prediction function' problem space, and applied in derived useful positions to maximize the benefit of these structures

- identify useful structures like 'limit-improving patterns' for useful intents like 'apply curvature to a set of linear limits of a function' to generate 'alternate sets of improved limits' that could possibly describe a 'prediction function' for a data set, using 'initial input limits like linear limits' as more adjacent to identify using 'general patterns of the data set'
  - example: for the 'find a prediction function' problem, the 'limit-improving structures' are various sets of connecting lines forming triangles that describe the original data set or adjacent probable versions of it, which is similar to an integration method applying structures like 'fractal variants' to produce 'curvature', similar to how 'fractal variants' applied on an interface can identify variables that can be applied on that interface without contradicting the base structure of the interface
  - finding 'alternate limit-improving structures' like 'alternate connecting line-improving structures, such as alternate functions to vary the connecting lines produced by a set of general limit patterns to produce alternate triangle sets that could describe the prediction function' and the variables of these like 'adjacent structures like fractal variants' which can be applied to generate them are useful to identify to identify 'probably successful change structures' to apply when 'improving a base solution'
  - these 'limit-improving' structures can be generalized to other improving structures (like 'pattern-improving' or 'linear line-improving' structures), which are improving some solution metric like 'accuracy' by applying change structures that 'similarize a base structure to another structure' (thereby improving accuracy), such as by applying 'specificity' using 'fractal variants'
  - patterns in 'limit-improving' structures, like a pattern in the 'ratio/count of changes to apply when applying fractal variants to specify a structure to avoid over-specificity', can be useful to avoid over-prioritization errors, and similarly can be applied to similar structures like 'adjacent variants of the input' as a limit to 'limit the changes applied to avoid over-prioritization errors'
  - structures which are useful for avoiding various error types should be part of the solution to cover various known error types, such as how 'limits' are useful to 'avoid over-prioritization errors', and 'filters' are useful to avoid 'lack of specificity/selectivity errors'
  - patterns are useful in that they allow 'skipping ahead' once the pattern is known, which reduces required steps to apply the pattern, as skipped sections can be predicted or connected as required without calculating the pattern for every point in those sections

- identify useful structures like 'combinations of opposites and adjacent functions' as filters of other useful structures like 'interim functions' which are useful as a 'common solution target format'
  - identify useful structures like the 'opposite' of a structure that make other structures more adjacently findable/buildable/derivable given the ease of applying opposites of a structure which are usually clearly calculatable or known, to apply when building 'base solutions to change' or a 'set of solutions or base solutions to filter', such as applying the 'opposite' sub-functions of a function that fulfills the 'opposite' intent of the target function's intent
  - for example, if one of the 'opposite' structures of an 'ai algorithm' (finds info) is an 'encryption algorithm' (hides info), applying the 'opposite' of encryption sub-functions (like 'randomized mappings' to obscure inputs) in structures like a 'neutralizing offset function' or 'structured known or otherwise certain (such as probable) mappings' can be a useful function for the 'opposite' intent
    - such as how de-randomizing a randomly filtered/modified data set (like with some points randomly removed/modified, applying random selections between ambiguous alternatives like initial weights, or randomly generating variable removals/compressions to identify the original inputs from the compressed version), given the unlikelihood of the original data set being a complete/accurate reflection of the actual variable interaction, can be a function that if fulfilled by an ai algorithm, would improve its accuracy and robustness across different inputs, as certainty structures like a 'consistent pattern' is likely to survive random changes up to a degree
    - also how applying 'random changes' to create 'differences' from the data set can be useful for finding 'contradictions' of a possible representation function, to test if an 'opposite assertion' of the 'representation function' is true
  - similarly, standardizing a function's generative interface query to a set of opposite structures like 'yes/no' questions can make it trivial to find alternate sub-function sets/sequences/networks/trees that can generate a yes/no in a useful way when combined in that structure (what 'combination' of relevant structures like 'adjacent/available structures' such as 'available core/standard functions' can produce a 'yes/no value' for what attributes like 'inclusion in a set', and how can these yes/no values be used to create the target output like an 'input reduction effect' such as a 'filtered set')
  - the 'combination' of 'opposite and adjacent' structures can also make it trivial to derive interim functions (as opposed to opposite/adjacent functions) which are useful in that a problem is rarely asking to solve for adjacent/opposite structures but rather is likelier to be asking to solve for interim functions with more complexity than the adjacent transforms resulting in similar or opposite functions, or the complete opposite structure with every possible difference from the original available input
  - the 'opposite' and 'adjacent' structures form a useful dichotomy that can act as a spectrum offering limits, where structures within those limits are candidates for the interim functions that are more useful to find

- identify useful structures like the 'alternate sets' of 'sets of data-crossing lines to connect the midpoints of' to fulfill intents like 'find a base solution structure to improve'
  - example: useful structures for the 'find a prediction function' problem include the 'set of intersecting lines' that indicate the cross-sections of a data set where the midpoints of those data-crossing lines should be connected, this 'midpoint connection structure' forming a useful base solution function to improve or use as an approximate solution, either forming it as a set of linear connections to apply 'various variables of curvature' to, or applying the 'default connecting curvature' between these points
  - deriving the alternate line sets to fulfill this intent is another useful structure, in case some lines are more adjacently derivable, or deriving alternate structures of these line sets, like 'alternating data set subset-crossing lines' that are adjacently connectible without finding a crossing line for every subset of the function indicating a different change structure or pattern to describe

- identify useful structures like 'filterability' of a structure like 'set of alternate solution sets' that reduces the steps required to fulfill a workflow like 'filter alternate solution sets or alternate solutions or alternate solution set-finding functions' to optimize for solution metrics like 'required steps to test a solution' such as by 'maximizing filterability, to minimize points required to test a prediction function'
  - example: apply 'overlaps' as a way of filtering 'points that need to be tested' to 'differentiate between possible alternate solutions', as 'points that overlap between alternate solutions' can be excluded as possible filters of solutions
  - finding solutions and solution ranges that differ in terms of their 'filterability' (some alternate solution sets are more filterable than others, as theyre more different) is a useful intent to fulfill
  - solution functions/ranges that overlap at only a few points as opposed to most subsets of the function can be easily differentiated, assuming the other non-intersecting points arent adjacent, as only a few points need to be tested to differentiate it from other possible solutions
    - prioritizing/optimizing for 'filterability' can identify 'maximally different functions' as an 'optimal' structure to generate, as these can be filtered with minimal point testing
  - if alternate solutions are similar, their few differences could be caused by random error or trivial changes which may be ignored or used to build alternative, conditional, embedded, or subset functions (as opposed to one solution function)
  - finding the 'points that need to be tested' in order to 'differentiate between solution structures like functions or function ranges' is a useful intent to determine the filterability of a set of alternate solutions, so alternate solution sets with low filterability can be filtered out from the 'set of possible alternate solution sets', 'alternate solution sets' being useful for improving the accuracy of a solution-finding method that can filter them, and 'sets of alternate solution sets' being useful to find 'less filterable alternate solution sets' and 'maximizing difference between alternate solution sets' to filter 'solution-finding methods used to generate alternate solution sets'
  - 'intersects with important data set points', 'has the same general shape as the data set shape', 'is a useful summarizing function for a random or local subset of the data set' are all 'reasons' why a solution would be successful in the 'find a prediction function' problem space, bc they involve similarities between 'important representative/determining info about the data set' and the 'prediction function'
    - applying 'representative/determining info about the data set' as interface structures like 'inputs', 'components', 'patterns', or 'limits' of the 'prediction function' is another method that applies this similarity as a determining structure of a solution-finding method, and are relevant bc the 'prediction function' is also intended to be a 'representative' structure of the data set, so other 'representative' structures are likely to be able to create it using adjacent transforms

- identify useful structures 'linear/circular functions' which represent a 'spectrum of difference in curvature' which is a useful variable to identify a 'difference (in impact on area) from linear/circular functions' that would be useful in determining structures like 'areas determined by curved functions', as a 'compression' structure like compressing the 'impact of change rate sequence on area' into a single numerical value like a 'difference ratio from linear/circular functions', as a way of fulfilling intents like 'find area determined by a curve' that would be useful for filtering out functions having metrics based on areas, like 'functions having an area of difference from a function like a regression line'
  - example: finding the change structures like 'proportional impact of an exponent change on the difference in adjacent values of the function' or the 'combination of the difference structures from a linear function and the difference structures from a circle' is useful for intents like 'quickly find the area of a shape', like the area determined by a curved non-circular function, the impact which can be tested by applying to all 'change structure types' (types of subsets of the function like adjacent increasing slope next to a maximum, to re-apply to places where a change structure is repeated in a function so the already computed area or difference ratio can be re-used instead of re-calculated), which is useful for intents like 'find a function minimizing area of a type/position/other attribute (like area above a curve/line)'
  - finding the 'linear/circular functions' as a useful dichotomy to use as a spectrum variable to find out the 'difference in impact on area' on this spectrum represented by a curve, which can be used to adjacently calculate an area as opposed to other integration methods is trivial to find, such as by applying an 'extreme' structure to the 'curvature' variable
  - connecting the 'linear/circular function' spectrum variable to the 'difference in impact on area' is the next step to fulfill, 'finding area trivially' being a useful structure to 'filter functions having an area-related solution attribute'

- identify useful structures like 'useful differences to apply to create a more searchable/filterable solution space' like the differences required to create a space with similarly shaped solutions graphed adjacently, so that optimizations like 'identify suboptimal subspaces that can be filtered out of the possible solution space as an area rather than a point' can be applied to 'reduce steps to find a solution'
  - example: identify a 'solution space' where adjacent points are likely to be similarly useful, such as a space graphing quantifiers of similarity between high-variation variables of possible solutions like 'intent' and 'system impact' or 'system structure' as the primary structures relevant to a solution (the intents fulfilled by the solution, and the impact on the system where the solution is optimally applied), or alternatively 'intent' and 'change structures', to enable quickly identifying areas of the space which are likelier than others to contain a solution (or solution-finding method, if those are graphed as solutions), as 'iteratively applying change combinations' in this space is an adjacent method made trivial by that space once found, and this structure is useful for many workflows like 'improving a base solution'
  - 'finding the right structure to format a problem' is a useful intent, such as where axes arent necessary to depict 'general similarity of points' that might be useful for intents like 'finding a connection network between points' as opposed to 'finding one general sequential pattern to fit the points' which might benefit from being positioned on axes
  - an example is how if a set of possible functions is graphed on a space by similarity between function parameters, a 3-rank function is likely to be considered similar to a 2-rank function and positioned adjacently, but they have very different shapes and are unlikely to be similarly useful for describing a particular data set, so graphing them by their impact (the degree of change or other change structures like 'changes in change rates' they exact on inputs) is likelier to be useful for intents like 'grouping functions with similar shapes' which is likelier to be useful for intents like 'graphing solutions by similarity in usefulness'
  - structures relevant to usefulness should be included where possible as factors graphed, such as 'solution success cause' being a particularly important differentiator/grouper of solutions into solution types (like solutions having similar shapes)
    - 'type differentiating filters' are particularly useful structures that can be applied in workflows where 'identifying patterns', 'filtering', 'identify example maximally different base solutions of maximally different solution types' and other common intents fulfillable with types are required in the workflow, where types are useful as an 'encapsulation of attribute sets' to reduce steps required to fulfill intents related to 'identification/filtering/differentiation', as 'types' are a 'representation structure' that stores a 'range' and a 'center or centers' where attribute value differences around that center create the range of the type
    - 'compressions of a structure' like the 'determining points of a function' are useful to 'reduce the info required to be graphed', such as how depicting the determining attributes of shapes required to determine a function shape can reduce the number of points required to determine the function
      - for example, a one-cycle wave (one peak and one valley) can be represented as a center & radius of two circles determining the wave (4 total attribute values), where an algorithm to create the function from the circles' parameters has to avoid/circumvent the circles generated by those 4 attribute values of 2 circles, additionally requiring a starting point to indicate the first 'avoid' pattern to indicate the direction of avoidance, requiring 6 attribute values total, which can be reduced to 5 by using the slope coefficient of a constant line intersecting with that starting point that indicates the first 'avoid' direction, instead of the two values (x,y) of the starting point, where normally the one-cycle wave would be determined by a maximum, minimum & inflection point coordinates (6 attribute values total, of 3 data points)
      - this exchanges the 6th attribute value for additional computation, as the algorithm is required to compute a line that intersects with the starting point, after determining useful starting points, to find the starting point determining of the avoid direction that a constant line could intersect with, exclusively compared to other possible starting points determining of the avoid direction
  - other determining interface structures of similarly useful solutions can be used to graph solutions on a space
  - similarly, prediction/summary functions that have the 'same accuracy' would not be adjacent in a typical x-y graph, but would be adjacent in a space where similar 'difference or change' structures were graphed adjacently, such as 'differences from the solution function, indicating an error', and where 'differences applied' such as 'shifting peaks/valleys to create similarly accurate functions' can indicate 'adjacent version of a function' that would be graphed adjacently, given that only one 'difference' structure ('a peak/valley shift') was required to create that alternate function
    - this alignment between 'adjacent versions of a function, created with interface structures like difference structures such as "adjacent shifts" applied to function component structures like peaks/valleys' and attributes of a solution that are unlikely to change with these changes such as 'accuracy' is useful for approximating a function to predict the accuracy of a function, by finding ways to connect similarly accurate functions other than accuracy (which is likely to be unknown in this scenario, where the program is trying to find an accurate solution without knowing the accuracy of all solutions, so approximations of accuracy like 'adjacent interface structures applied to a known function at a particular accuracy level' are useful to create a space to graph these solutions in, to traverse adjacent solutions to find alternate similarly accurate solutions, or to filter out areas of the solution space)
    - these interface structures are unlikely to change the accuracy, bc the accuracy is one number that encapsulates a 'difference from another function', which can take many forms, and the changes produced by these interface structures like 'adjacent shifts of interface structures (like components, such as peaks/valleys)' is unlikely to generate a difference in that number below a particular degree, because the overall function shape is maintained by these changes
    - applying adjacent changes while 'maintaining the shape of a function' or 'maintaining the difference from a summary line' are useful interim intents to fulfill, which could replace other queries such as 'find interface structures to apply to create differences below a level that would change a metric like accuracy', while 'finding alternate solution functions to test'
  - this method can be reduced by finding a subset of points on the graph to make sure theyre different/similar as required, and then applying differences to the useful points to find alternate solutions (or similarly, finding errors)

- identify useful structures like 'surrounding data points' as input to intents like 'identify local patterns' which are useful bc local connections like 'immediate surrounding connections' as opposed to 'adjacent connections' are likely to be similar to 'adjacent connections', so applying them as differences should be useful to reveal a robust local pattern, which is useful as an input to 'combine local patterns of function subsets into a global pattern of the whole function' in the 'find a prediction function' problem space
  - example: connecting adjacent data set points as well as surrounding points can indicate if a predicted change type for a function subset is accurate, as the local pattern should hold even with some data removal/filtering so that a surrounding point is used as opposed to an adjacent point, which can be offset by applying this local subset function-finding solution to more complex subsets of the data set with fewer clear patterns and connecting these more complex subsets once theyre found with a solution that requires more steps like this, to make the output algorithm more efficient than one that checks every data point
  - reverse-engineering this to first find the structures that would 'reduce required steps' (like a way to identify particularly important subsets of the function that need to be more correct than other subsets, like around an inflection point, determining a limit, or more complex subsets with less identifiable patterns, which can be prioritized with more steps to reach the accuracy required, and then using simpler methods for the remaining subsets like filling in a curve according to connections between these high-priority subsets by applying probable adjacent versions of the identified subsets so that a continuous curve is reached with the fewest parameters)
  - this can also be reverse-engineered from the 'local patterns which are adjacently connectible into a continuous function' which can be trivially estimated from a data set (or a subsets of these local patterns that are more important to accurately identify can be trivially estimated from a data set and trivially used to infer the remaining subsets)

- identify useful structures like function sequences that will reduce the steps to fulfill a workflow like 'improve a base solution'
  - example: once a linear function or other approximate/summary function is found for a data set, the problem is 'find the limit of the changes to apply to improve the baes solution' (find the maximum point on a curve above a linear function that represents the actual prediction function), which is a problem that can be solved in fewer steps than the original problem, by finding patterns in 'isolatable subsets indicating a one-limit curve', then finding an approximately predicted limit of each subset pattern, and checking points around that approximately predicted limit with decreasing difference from that limit, until errors in other positions improve, for each subset of the function that deviates from the linear base function in an isolatable structure like a 'concave curve which can be explained by one limit'
  - this reduces the steps required bc the goal becomes to "find a determining point of each 'one-limit curve' function subset", after finding a pattern for each 'one-limit curve' function subset

- identify useful structures like 'overlaps of probable alternate function ranges/limits/statistics' given the interactions of those useful structures, like their 'equal probability' given the filters used to generate them, and given the additive probability represented by repetition of structures like 'function ranges/limits/statistics', repetition which can be identified as interface structures like intersections/overlaps
  - example: 'overlaps of general function ranges/limits/statistics, given different filters like outlier filters or linear filters to remove data points that are clear outliers or follow an unlikely linear pattern' are another quickly identified interface structure of function structures visible more easily from different perspectives like how 'general patterns' are more identifiable in different perspectives, as an additive signal that a particular function range is likelier to be correct, bc its repeated across different outlier filters, repetition being a useful indication of truth in this position in this problem space

- identify useful structures like 'perspective changes' that are useful in a problem format for a particular intent for a workflow ('improve a base solution'), like how 'increasing distance from input' is useful to quickly identify general patterns which is useful for workflows like 'improve a base solution', since 'general patterns' are likelier to be simpler, such as a 'base solution to incrementally improve' would be
  - example: identify useful structures like a 'perspective change' like "different 'positions in relation to the object being described' or 'distance from input' (like the 'zoom')" necessary to fulfill an intent like 'improve a simple base function', such as by specifying a function subset given its complexity, for example, 'zoom out' to quickly identify larger patterns of a data set that can be represented as linear functions that can be applied as simple base functions to improve incrementally into a more optimal solution
  - this perspective change requires 'fewer data set points' to be represented in a way that they are identifiable, which makes the data set patterns easier to spot, without making decisions about which points to remove, such as an algorithm that relies on removing outliers would require, as 'zooming out' is a function that can make decisions about removing points based on overlaps of those points which are simpler decisions than selecting outlier identification filters
  - these 'perspective changes' which are useful for different intents can be applied in an interface query like 'improve a base solution', or as a solution function like 'use the linear base pattern to predict points within these subsets that can be adjacently approximated by the linear function, and for other subsets that are more complex, apply a perspective change to identify smaller patterns or other interface structures like limits in those subsets'
  - this is similar to how a person can quickly visually approximate a function's limits and patterns by viewing the whole data set but algorithms typically process every point to calculate the mean and other relevant statistics for methods like regression, and how a data set becomes simpler automatically when fewer points can be represented, such as when viewing it from far away, which is useful for intents like 'identify the general patterns of the data set'
  - the structural similarity between the 'general pattern' and the 'base solution' is another useful structure to identify, as generating general patterns is trivial with some perspective changes, and is useful in that it makes 'finding a base solution' as 'input' to the 'improve a base solution' workflow more trivial
  - this is related to applying 'incremental changes', but in the position of the 'specificity/abstraction' of a data set to find more specific interface structure like patterns where necessary or optimal, rather than by default, increasing the specificity in high-error subsets to find more specific interface structures to explain differences from these patterns
  - this is related to the concept of 'interaction level', where a 'perspective shift' changes the interaction level of the input to that of 'interacting subsets represented approximately with fewer points', rather than 'interacting data set points', 'interacting subsets' involving fewer structures, and therefore requiring fewer steps to identify structures like patterns in those interactions
  - this perspective shift also makes other structures more trivial to identify, like outliers, extremes, change types, noise, isolatable subsets, examples to use as indicators of useful differences to identify (like data set points that would be useful examples to use when finding determining limits of 'one-limit curve' subset functions)
  - another perspective change would be to change the angle of viewing the data set, to identify which changes disappear more quickly on the horizon as more extreme values which can be applied as outliers to filter out, or which points seem to overlap given a perspective change, indicating they can be merged into one 'overlap' structure or a single data point, rather than identifying the connecting distance compared to other point pair distances as particularly useful for identifying overlap structures

- identify useful structures like 'alternatives to a function' and structures that can be used to derive them (like 'change structures' such as 'opposites' and 'change inputs' like 'outputs of function outputs') to fulfill intents like 'improve an existing solution'
  - example: the reasons an 'opposite' structure is useful include the reason that 'opposite' structures are generally useful, and also specific reasons like the 'opposite' of a function output (sending output somewhere, in a 'request') is the 'response' indicating the success of that function output in the intent it was applied for
  - applying the 'opposite' structure to the 'function output' can identify the structure of the 'resulting response information in the opposite direction', which is useful bc it provides information about errors or success of the function
  - the 'output' of the 'function output' can be used as a 'function input', to improve the function (deriving the useful concept of 'feedback' by applying the 'opposite' structure to the 'function output')
  - a variant of this would apply the 'opposite' structure to the 'input/output connections' of the 'function' to generate an 'alternate opposite function' that could be applied in parallel to identify whether a change to either function would result in an improvement of the function based on which of the opposing functions is more successful at the original function intent, as 'alternate opposing functions' and 'applying output of function output as function input' are alternate useful structures to fulfill intents like 'improve a function'
  - both of these structures generate useful structures like 'alternatives to a function', one applying changes incrementally based on feedback and one applying changes like opposites to generate parallel functions

- identify structures like 'reason for a variable structure' which are useful for determining other useful structures like 'adjacent causal structures of the variable structure', which are useful for intents like 'identify causal structures of a data set' in problem formats like the 'find a prediction function' problem format
  - example: identify useful structures like the "connection between a 'set with one difference to resolve' and a 'threshold value' (how different is it from the threshold) or 'differentiating value' (how different is it from members of the other group)", this useful connection creating a 'boolean' variable
  - the reason for a variable structure like a boolean is a useful structure to identify, as it can be used to identify structures around it, like its causal structures
  - a boolean would primarily be useful to 'summarize/represent/filter a data set that can be differentiated using one threshold/difference value', so this 'adjacent specific causal/input (data set)' structure can be derived from the 'boolean' structure of a variable
  - similarly a 'reason not to change a variable' is useful for 'filtering which changes to apply' when fulfilling intents like 'generate possible alternate structures' or 'improve a structure', such as when a structure is sufficiently useful, as in it fulfills the intents its applied for without 'identifiably suboptimal side effects'
    - for example, applying 'balance' structures as an alternate to 'generalization' structures can be sufficiently useful, bc of the 'similarity' between 'balance' and 'generalization' structures (applying 'balance' to identify the midpoint between the alternate biased/variance structures can replace a 'generalization function that doesnt directly apply an explicit balance structure like an average' as the impact is likely to be sufficiently useful for the 'generalization' intent to be an optimal replacement)
    - this 'sufficient usefulness' can be applied as a 'reason not to change a variable' to fulfill intents like 'filter the solution space' (to avoid 'trying all possible changes')

- identify variables of useful structures like functions (like organize/summarize) which are generally useful for their 'uncertainty/certainty connection structures' which are generally useful for problem-solving intents like 'resolve differences'
  - example: intents like 'summarize' and 'organize' are similarly useful bc they convert 'high variation inputs' to 'low variation outputs' that are more useful for making decisions, which is an intent fulfilled by optimal solutions (the solution provides information, which is used to make a decision, so the solution differentiates some information from other information), and have aligning intents fulfilled by each function
    - 'summarize' can 'reduce the required resources to represent a structure'
    - 'organize' can 'increase the probability of structures in or interacting with a structure being determined' (as once theyre organized, there is less uncertainty in their interactions, such as how 'sorting a set' reduces the 'uncertainty in where a new item will be optimally positioned', so the 'position of the new item' is likelier to be determined, as itll be more certain where the 'right position' is for a particular 'intent' once organized)
    - both of these functions fulfill 'similar, overlapping, and equivalent alternate intents' in addition to the different intents fulfilled by these functions
    - the 'reason' they can be used for similar intents is their 'certainty/uncertainty connection' structures
    - 'summarize' identifies the variables of a structure which can be a 'reduced representation' of it
    - 'organize' identifies the variables of structure interactions and positions them in a useful way, optimizing for metrics like 'success probability' and 'stability' (organizing in a way that is likelier to be useful, or defined to be useful like 'stable' structures)
    - both functions reduce the work required to fulfill similar intents like 'connecting uncertainty structures like a data set (which may have noise/outliers) with certainty structures like a function (which is likelier to generalize across uncertainties)', since 'summarize a function, by finding a prediction function' fulfills the 'summarize' intent and 'organizing variables, by connecting variables by their causal structures' fulfills the 'organize' intent
    - both of these functions add certainty structures, 'organize' adding structures useful for 'determining other structures', and 'summarize' adding variables ('coefficients') useful for predicting other structures like 'outputs of new/different inputs'
    - their structural alignment comes in the form of the alignment of 'causal structures' (found in 'organize') and 'combination structures' (found in 'summarize'), since these are 'equivalent alternate' structures that can be used in place of each other ('combinations of causes can be used to predict the output dependent variable')
    - these structures can be connected by applying the structure of 'sequence', which can generate 'combinations of causes' after identifying 'sequences' of 'causal structures', which is useful for identifying 'patterns' in 'causal sequences' that can filter the 'combinations' to apply to causes
    - other 'equivalent alternates' of these functions can be generated by identifying other variables to apply changes to which are similarly determining of these functions, or identifying alternate values of that variable (including 'input/output connections', 'function intents', 'requirements', 'filters', 'sequences', and other structures that can be used in place of 'representations/summaries' and 'organizations', which would generate alternate functions like 'standardize' from 'filters', 'fulfill' from 'requirements', etc)
    - the 'similarity between the uncertainty/certainty connections' of these functions is another example of a variable that determines these functions
      - 'summarize' identifies certainty structures ('coefficients', 'exponents', 'operations') to 'combine' or 'reduce' inputs into outputs
      - 'organize' identifies certainty structures ('connected causal structures' like 'sequences') to 'connect' inputs/outputs
    - other variations of these can be generated by applying variables to certainty structures (like 'constants' or 'fully connected causal sequences') that are useful for fulfilling core interaction functions (like reduce/combine/connect) between inputs/outputs
    - other functions that connect uncertainty/certainty structures can be derived by identifying 'useful structures to connect' (uncertainty structures having a difference/uncertainty to resolve, and the certainty structures resolving that uncertainty) and connecting them with variables applied to available functions

- identify structures that can increase the usefulness of useful structures, like attributes such as the 'count' of useful structures
  - example: 'identify equivalent alternates' as a useful intent to fulfill, as it increases the usefulness like 'number' of a useful structure like 'equivalent alternates', and increasing a usefulness metric like 'number of available instances of a structure' increases the usefulness of that structure, as the higher the count of the useful structures, the more they can be used
  - otherwise deriving structures like 'count' to apply that can increase the usefulness of a structure can be applied to fulfill other known structures of usefulness for useful structures to increase their usefulness
  - similarly, an 'alignment' between structures can increase the usefulness of a structure, bc the 'alignment' structure is useful in positions like aligning 'input/output connections' between two structures
    - for example, solving the problem of "identifying whether an 'attribute' causes a 'decision' in 'group A'" would benefit from applying logical filters, like 'does every agent make that decision or just those in group A' (applying the interface structure of the 'extreme variable value' of 'all' to the variable of the "group of the members having the hypothesized caused variable, the 'decision'"), where the structures of the differences between inputs/outputs align across these structures (the difference from 'all' and 'group A' aligns with the problem of 'identifying if its only group A or other groups', which is a problem of 'resolving a possible difference')
      - whereas applying 'all' in the position of the 'group A' structure ('does every group A member make the decision') would be the wrong position to apply this interface structure in, bc its defined as already being true (the question is whether its just group A or other groups, not if there is uncertainty about whether group A does it, as its already defined in the problem statement that group A makes the decision)
      - this would be comparing equivalent structures ('group A having this behavior' and 'whether every member of group A has this behavior') as if theyre different
      - therefore the structures dont align as one structure resolves a difference (the question 'does every member of group A have this behavior'), and the other structure is two known equivalent structures ('group A having this behavior' and 'every member of group A having this behavior' being equivalent, as its defined that every member of group A has this behavior, so asking the question of 'if each member has this behavior' would ignore that definition, creating the illusion of a difference to resolve)
    - the difference in the other structure ('group A having this behavior' and 'other groups that are not A') aligns with the question resolving the difference, as there is a possible difference between these two groups to resolve
  - the 'alignment' is useful on its own (the question resolves a difference that 'exists in the input structures being resolved', so the question and the input structures have an alignment that makes the question useful to ask in an interface query) to direct where variables (like 'all' or 'other' applied to the 'group' variable value in the problem) should be applied, so that identifying those variable values is useful and can be included in an interface query, rather than being pointless (like comparing known equivalent structures as if they could be different which is already known in advance, so is pointless to compute)
    - knowing 'in what position to apply variables' to 'generate questions regarding useful differences to resolve' is a useful intent to fulfill, as a problem-solving intent like 'apply variables in useful structures like positions to identify useful structures like "useful differences to resolve" in a "connect problem/solution (resolve differences)" workflow'
  - these 'alignments between queries and input/output connections' are useful like the 'equivalent alternates' are useful, just like 'causal sequences' are useful for their alignment with other structures like 'function sequences'
  - applying these structures that 'increase the usefulness of other useful structures' can fulfill problem-solving intents like 'optimize other workflows and interface queries'
  - these structures can be generated with adjacent applications of interface structures:
    - apply a 'change' to the variable value of the 'number' of useful structures to generate 'equivalent alternates'
    - apply a 'structural similarity' to relevant structures that should be similar, like the 'difference resolved by a function/intent' and 'required input/output connections like specific differences between inputs/outputs' generate the 'alignment' of relevant structures as a useful structure that increases the usefulness of the 'function/intent' or 'input/output difference'

- identify useful structures that can find/build/derive alternates to replace other known useful structures, as structures that can be used to find/build/derive alternates of other useful structures
  - example: applying 'extremes' to check for possible errors is all that is required to identify 'every possibility is sub-optimal' as a 'possible error structure', such as in cases where 'every solution/component is greedy' creates an error for a 'group of possible solutions or components' ('all' applied to the 'solutions' or 'components' variable), rather than applying a structure like a 'system' to generate the structure of a 'surrounding context of a structure like a component or possible solution' (as 'possible solutions' are adjacent just like 'components of a system' are adjacent, so they are relevant to each other in the sense that they can identify/determine/change each other, making their interactions useful to predict)
  - this is useful for providing alternate structures like 'extremes' of 'core components/variables' or 'known errors' to another structure on another interface to identify useful structures like 'system context' or other structures of relevance
  - 'extremes' are also likelier than other structures to produce 'errors' given that 'any structure that isnt optimal or a solution is likely to be an error' and 'suboptimal structures are likelier than other structures', given how 'extremes' are a 'maximum' of a structure like 'difference' that is capable of producing 'differences' which are more commonly inputs to 'errors' than 'solutions', as 'resolving differences' is a variant of a core problem-solving intent 'connect problem/solution'
    - another example of applying this 'extreme' structure is in applying the 'extreme' structure to the 'number of objects represented by a structure' to create a 'type' structure that can convert a 'network of objects' into a 'network of types' (abstractions)
  - structures that can fulfill many or core intents like 'identify/determine/change' other structures are good candidates for 'structures of relevance' which are important in their ability to fulfill these functions, as well as the corresponding structurees of those intents like 'oppose/limit/alternate' which may be more accessible than the general intent-fulfilling functions for 'identify/determine/change'
  - this is useful for finding useful 'opposing' structures to act as 'limits' (like 'expand' to 'every possibility', or to 'extreme differences') on the errors that a particular function (like 'reduce') may introduce when applied, which is another useful structure to apply when generating a function such as an interface query

- identify structures that coordinate to fulfill useful intents like 'connect problem/solution structures' such as connecting 'variables of structures filtered by applying solution requirements and solution success cause' with the 'solution requirements and solution success cause' and then the 'problem inputs' to fulfill workflows that involve 'reverse-engineering' such as those using problem-solving intents like 'filter the solution space'
  - example: in the 'find a prediction problem', this would take the form of 'generating probably useful interface queries like "connect connection units to form a connection using some definition of relevance to form the connection units" to implement the "connect problem input and solution output" workflow, or "apply the "reason" for the solution like the representative nature of the prediction function to find solution metrics like averages to use as a filter of possible solution function components like points, to implement a workflow like "apply solution metrics and solution success cause as a filter of possible solutions to filter the solution space"', then following that generative process by 'applying variables to the inputs of these queries, such as variables to create alternate definitions of relevance'
  - the 'alternate definitions of relevance' in the "connect connection units to create the prediction function" would take the form of "different ways to connect data set point with different reasons why they might be relevant", which are useful to apply as variable inputs to that query to generate alternate solution-finding methods likely to be good candidates for solutions
  - "applying variables to inputs to generate different interface queries" is used in other workflows, but not after identifying specific interface queries generated to solve the 'find a prediction function' problem, with the right level of variation that applying variables can maximize the value of
  - examples of 'reasons why a solution or solution-finding method would be useful' include:
    - that "each component of the solution-finding method increases the usefulness of the other components", such as where a "filter that filters a set" is followed by a "filter that filters a set to one item", bc this "sequence of filters" has a reason to be useful in that the output of the first can be used to fulfill the input requirements of the second without contradictions, such as would occur in the opposite sequence where one item would be output by the first as an input to the second, which would have an impact of "making the second filter unnecessary", thereby contradicting the "reason to use the filter in that sequence"
    - a solution-finding method has randomness applied in the right positions (to "resolve uncertainties like unknown variables" or to check if a "constant should be a variable")
    - a solution-finding method finds 'maximally different possible solutions', which are useful for reducing the solution space
    - a solution-finding method has useful (like efficient or quick or high-probability) 'stopping conditions' to halt pursuing a particular interface query or possible solution or solution-finding method when its clear it is unlikely to create a solution
  - specific examples for the 'find a prediction function' problem include:
    - a solution-finding method applies 'local representation' globally, to find 'locally representative lines of subsets of the data set' as components of a 'globally representative function', applying a 'subset' to the 'reason why the solution is useful' to find 'subsets of the solution'
      - this 'reason why the solution is more useful than the problem input' is a corresponding structure of 'solution metrics/requirements' which can be used to filter the solution space by applying that reason ('the solution-finding method needs to find (representative averages or subset connections or incremental changes applied to improve a suboptimal base function), that can be used to predict a point given independent variables') as opposed to the more structural requirements ('a solution function can differ from data points by this much') or other solution structures like solution success cause (its a good 'representative function' or 'representative function-finding method')
      - this 'reason why the solution is useful compared to the problem input' also applies the alternate intents fulfilled by the solution ('summarize', 'represent', 'predict', 'compress') 
      - a variable is injected in a useful location in this 'reason why the solution is useful compared to the problem input', creating alternate structures to test in the position of 'what structures can build the solution in various workflows' resulting in 'representative averages or connections or incremental changes applied to improve a suboptimal base function' identified as possible solution components to find in different interface queries, those structures ('averages', 'subset connections', 'incremental improving changes') being 'known useful structures' in the 'find a prediction function' problem space for the intents fulfilled by the specific solution function or solution-finding method
      - by applying core interaction functions like 'connect', 'filter', 'change', and intents like 'summarize', 'predict', 'compress', these structures ('average', 'subset connections', 'incremental improving changes') can be adjacently derived
      - similarly, useful functions in the problem space like 'find local rather than global extremes' by variable methods like 'applying incremental changes' can also be used to derive these structures as useful, once variables are again applied to the method of 'how' to implement the function ('applying incremental changes', 'deriving function shape with random samples', 'applying probable/common function patterns to derive function shape')
        - 'find local rather than global extremes' by 'applying predictions about function shape' using various methods (probability, random samples, etc)
      - identifying these 'functions which are useful to apply variables to' in order to 'generate alternate methods of fulfilling these functions' which could be more useful and adjacently derivable, are other useful structures to identify in a solution automation workflow
    - the 'reason why the solution is useful (as opposed to the problem input)' is another useful solution structure to apply in workflows
      - the reason why a prediction or representative function is useful as opoosed to a data set is to fulfill 'predict' or 'summarize' intents
    - for example, the 'reason' why a function that 'predicts a point from adjacent points' is a useful alternative solution-finding method to a function that 'finds the average line representing a data set' or 'finds a point given all input/output examples' or 'finds a function that minimize errors to some degree, if not absolutely minimizing it' is bc a function represented by a data set can be expected to follow the same pattern throughout, so that a 'function predicting a subset of it' can be used for 'any subset of it', and is adjacently derived by applying the usefulness of 'reduce' to identify 'component' structures like 'subsets' or the usefulness of 'core/component' structures like 'subsets' for intents like 'build' or 'connect' (applied to build/connect with intent to create a prediction function), so such a function like that would be both locally and globally useful, and can be used on 'component subsets' of the function to build the solution function
      - similarly, applying a variable to a solution metric (applying a variable to 'accuracy' to generate 'some accuracy', as opposed to 'absolute accuracy') can also be used to derive an alternate solution target (a 'prediction function that has some accuracy', as opposed to the original target of a 'prediction function with absolute accuracy above a certain range'), which doesnt contradict the 'reason' for the usefulness of the original solution target, as a function that minimizes error to some degree would still be useful for the same reason, even after the variable has been applied to change the solution target
  - connecting these 'reasons for solution success' with 'solution requirements' and applying variables to generate alternate possible interface queries connecting them is a useful structure to identify, which assists with workflows that involve reverse-engineering from solution requirements by adding more structures (reasons for solution success, variables of structures left possible by applying solution structures as filters) to add structures like filters to the solution space

- identify structures that can increase the value of other structures if both are identified, such as 'complementary' structures that are valuable bc they handle 'error cases' of a structure like 'requirements'
  - example: 'requirements' are a generally useful structure, but as theyre not 'absolutely useful', meaning there are cases where requirements are insufficient to use as a prediction structure, such as where 'requirements are not fulfilled' or 'other requirements are higher priority' or 'functions to fulfill requirements is not required or available or required to be available', other structures add 'compounding' or 'complementary' value when added to 'requirement' structures, such as 'functionability to fulfill requirements', 'priorities', 'overriding structures like more functionality/resources', 'system structures that lead to resolutions of requirement conflicts' 'contradictions between requirements leading to conflicting requirements, where only one can be fulfilled by definition or with high probability, leading to a requirement that one requirement is fulfilled at a time, which overrides the other requirements'
  - this 'complementary' info can be derived as useful to identify to resolve 'requirement errors', to increase the value of the 'requirement' structures in fulfilling other intents
  - this workflow identifies the reason that structures can have additive value when identified with another structure, the reason such as 'providing info to handle an error, in the case of an error with that structure'
  - identifying 'info that can handle an error with a structure', the 'errors possible with a structure', and the 'structure' itself are also complementary structures that should be identified as a rule when identifying useful structures, to increase the value added by those structures

- identify useful structures like 'overlapping reasons' which are useful for intents like 'identify alternate versions of a structure' to fulfill intents like 'find alternate useful structures' or 'find alternate solutions'
  - example: identify 'overlapping reasons' as a useful structure to identify, such as how if a structure is 'interactive', it has 'overlapping reasons' why it would be useful, such as 'interactive structures are useful for building connection structures', and 'interactive structures are useful for building causation structures', given the 'overlap' in structure between 'connections' and 'cause'
  - this overlap can be used to identify alternate useful structures from one example of the useful structures, as well as variables of these differences, to generate other useful structures

- identify useful structures like 'intersections' which can be used to find/build/derive structures like 'alternate variables' to adjacently find/build/derive structures like 'adjacent structures' like 'interchangeable alternates' of a structure, to fulfill intents like 'apply useful differences'
  - example: useful structures like 'intersections of variables (like intersections of dichotomies)' can be a useful structure for intents like 'identify the direction of change to fulfill an intent' or 'identify a change direction from another change direction in the intersection to fulfill an intent, given a difference between directions that is known to produce any differences or the required difference in intents'
    - 'differences in directions' and 'directions of change' are useful structures as inputs to fulfill intents like 'find/build/derive adjacent changes to a structure to fulfill an intent'
    - these structures can be adjacently derived from structures like 'intersections of variables like spectrums/dichotomies' which describe the similarly useful information in a different variable which can be used as a filter of the data set
    - 'intersections' are useful bc theyre a structure of adjacence, where related structures can be derived from the intersection, which represents variables that are 'interchangeable alternates' describing the same structure, so that knowing one variable's structure can be used to derive adjacent alternate variables that can be used to derive the info from the other variable
    - for example, the 'diagonals' of a square are similarly useful in deriving the shape of the square, and 'adjacent connecting lines to the diagonals' are similarly useful, in their difference of not connecting corners but points adjacent to corners, where 'points adjacent to corners' can be used to derive corners, and therefore the shape of the square
    - this difference in intents fulfillable with a difference applied to an intersecting variable can be generated by applying differences to inputs/outputs 
    - the 'diagonal' has a 'direct' connection to the 'corner' object which can be used to derive the square in one step (connect the corners)
    - the 'line adjacent to diagonals, also intersecting at the center' has a 'indirect' connection to the 'corner' object, which can be used to derive teh square in two steps (derive the corner, connect the corners)
    - the 'difference in intent' (directly derive the square, indirectly derive the square) can be derived by applying an adjacent change to the 'points connected' variable, where the change applied allows the inputs to the 'connect corners' function to be derived, creating a similar but different degree of directness
    - so all changes in the direction of 'changing the points connected' can still be used to derive the shape of the square, but have an impact of changing the 'directness' of the intent fulfilled
    - this 'direction change' indicates that further changes in this direction (changing that variable) would further decrease the intent variable (the 'directness' of the intent, 'derive the square')
    - connecting these 'direction of changes' of 'variables' with the resulting 'changes in variables of intents fulfilled' is useful in deriving useful differences to apply to fulfill a particular intent
    - similarly, other changes can be used to derive an intersecting change, just like the 'line adjacent to the diagonal' can be derived from the 'diagonal', so connecting these 'differences in directions' with the usefulness of the resulting change in intent variables is another useful set of structures to connect
    - this is useful in a case like where a program is 'traversing a set of connected points on a square' to 'identify whether the object is a square', similar to using 'gradient descent' to 'identify shapes in a function'

- identify useful structures (like 'thresholds') of useful structures that are connected (like 'opposing structures' like opposing certainty/uncertainty structures like confirming/complementary structures) that can be used to find/build/derive those useful structures to fulfill related intents
  - example: identify 'interface structures' (like opposites, components, connections, and merge/integration functions of components) as 'adjacent structures' of a structure if the structure itself (like a 'prediction function') cant be identified, where the usefulness (like 'find errors') of the interface structures (like opposites, such as 'errors' or 'not solutions' or 'suboptimal solutions') of the structure can be derived from the function used to create the interface structure
  - similarly, 'confirming' and 'complementary' structures are useful 'interface' structures of a structure for their opposition, which is useful for intents related to differentiation ('identify whether a structure is another structure or not') and limitation/filtering ('use what a structure is not to filter possible structures')
  - 'confirming' structures (which are a 'certainty' structure) are 'repetition structures' or 'equivalence structures' which can be used to identify the same info or the certainty of that info, just like 'complementary structures' are 'opposing structures' which can be used to identify different info
  - there is a threshold where a structure becomes a 'confirming' or a 'complementary' structure, a threshold which can be used to derive structures that are likely to be on either side of the threshold, thereby determining whether a generated structure is a confirming or complementary structure, providing certainty of the same info or providing different info
  - this threshold is useful to apply when finding/building/deriving confirming/complementary structures for various intents like 'limiting/filtering' or 'identification/differentiation'
  - differentiation: this is similar to the workflows that involve deriving 'interactive' structures to fulfill the 'connect' core interaction function for intents like 'connect problem/solution, but instead involves an alternate way to derive confirming/complementary structures to fulfill 'differentiate' or 'filter/limit' core interaction functions for problem-solving intents like 'identify the opposites of errors (solutions)' or 'filter the solution space'

- identify useful structures like structures that can adjacently solve a problem such as 'connections between input state types (case scenarios) and patterns that are adjacent/probable/associated/findable with these input states' as a way of avoiding finding these patterns to fulfill problem-solving intents like 'reduce computations required to solve a problem'
  - example: identify useful structures like filters of the 'worst possible outcome or worst case scenario' (like a 'data set with no patterns, randomly distributed' as an input to the 'find a prediction function' problem) to identify whether the 'worst case scenario' is applicable, such as testing if 'subsets of the data set' can be used as 'random number generators', to identify whether the problem can be solved with that method or if there is an error preventing it, and how to resolve that error (like by 'applying random data removals to check for an adjacent pattern in the data set')
  - the other scenarios than the worst/best case scenarios also have structures that can be tested for and filtered out, to filter the solution-finding methods or solutions that can be applied to the 'find a prediction function' problem, like an algorithm to 'exit processing once a distribution is identified as sufficiently random to avoid unnecessary computation'
  - for example, the 'best case scenario' is a data set with a clear pattern that is either easily found or already known in a database of patterns, so 'data sets with clear known or easily found/built/derived patterns' are structures of the 'best case scenario', and other structures in between these clear patterns and random structures are candidates for non-worst/best case scenarios that are likelier to be the inputs to a problem
  - the 'connections between input states' are another useful structure to identify, to tell which cases are likely to be probable alternates, adjacent states, opposite states, etc, so the probability of the relevance of a solution-finding method can be assessed from the connections between these possible input states, for a query like 'find the ratio of non-worst/best cases that cover 80% probability of possible states, find the structures that can identify these states and the associated patterns findable and probable of each case state'
  - the 'probable/findable/adjacent patterns associated with each case state' are another useful structure to identify since problems can be standardized to the 'find a prediction function' problem
  - structures specific to a problem format like the 'find a prediction function' problem format such as 'tangents' can be derived by applying interface structures like 'structural similarities to other interface structures', such as applying a 'structural similarity' in the form of an 'adjacent change' such as the 'change of the variable representing the value at the linear function intersection with the data set, from an average intersection to an extreme intersection' to a 'representative linear function' that acts like a 'descriptive intersection or average' to build a 'tangent function' that acts like a 'descriptive limit or filter', as tangents are similarly useful to other linear functions associated with a data set due to their ability to reduce the variables to describe the data set, only storing the linear function variables instead of all the data points of the data set.
  - identifying the most useful structures like 'combinations of filters, intersections, and extreme examples' is a matter of applying changes to the 'definition' of a 'known pattern (like a constant line)' or 'known lack of patterns (like randomness)', as 'differences from patterns' are likelier to be useful to identify, since 'similarities to patterns' are more trivial to identify and therefore less useful to identify, and identifying combination structures that can identify various change patterns that differ from known patterns and known lack of patterns is similarly trivial and useful to make 'identifying real patterns (which are likelier to be interface structures like combinations and distortions of known patterns)' more trivial

- identify useful structures like 'inputs' or 'generative functions' or 'partial determining structures' of useful structures in a format which problems can be standardized to, like 'a resource limit' or a 'difference to resolve', to fulfill intents like 'connect problem/solution'
  - example: identify useful structures like 'generative structures' of 'opposite' structures such as 'resource competitions' which can generate these structures (like 'opposite incentives'), which can be used to fulfill intents like 'generate differences' and 'resolve differences between alternates' which are useful as 'difference-resolution' ('difference-connecting') structures which are generally useful for workflows involving 'connecting the problem/solution'
  - 'resource competitions' create 'opposite incentives', which causes a 'conflict', which causes a 'conflict resolution'
  - this is similar to how 'applying randomness' can fulfill intents like 'more quickly identifying a pattern resistant to randomness, like the correct pattern in a data set to use as filtered input to a prediction function', such as by applying 'randomness' to create a structure of a 'subset' in the position of 'input structures like the data set' is useful to identify 'robust patterns in the data set', as 'robust patterns will be clear even after randomly removing a ratio of inputs', as a 'pattern' can be identified with a subset of inputs, as a pattern is by definition a 'partial structure (like "just enough values to make the pattern clear") or a smaller structure (like a function) representing another larger structure (like a series) which is useful bc of that incompleteness, for computation purposes, like representing a series accurately without storing the whole series', and this makes the original input data set useful in the format of a 'partial subset of the data set' as an input to pattern-identifying functions like regression or machine-learning or outlier-filters, which can often find a pattern without the entire data set, so a 'subset of the data set' is likely to contain other 'partial structures' like 'patterns', which can act similar to 'limits' or 'filters' or 'representative formats like summary statistics' in that they are 'determining structures of the data set'
  - the 'differences required to create an opposite' is a useful structure to identify as well, in order to identify variables that create opposite structures like opposite incentives
  - for example, identifying 'how many changes it takes to convert a false statement into a true statement' is useful to 'identify other useful structures, like variables of false statements', such as 'an agent falsely stating that a constant negative about themselves is actually a variable that could be positive in value' reveals that 'negative self-constants portrayed as variables' is a useful structure to 'identify false statements' and 'create true statements out of false statements & vice versa'

- identify useful structures like 'abstractions' of useful structures like 'solution automation workflows' to identify useful structures like 'variables' of those workflows, given 'connections between these useful structures', such as that 'variables' are more adjacently identified when a structure is 'abstracted'
  - example: 'apply structures to a problem' is an abstraction of workflows, as 'structures' are useful bc of the certainty they add to a problem, and a function that 'converts this abstraction into a specific version that can be used as a workflow' is a useful structures to identify, such as a function that 'identifies structures that are available, which can be applied to problem structures which will fulfill an intent to find/build/derive information about the problem', as known structures are useful bc their impact on another structure can generally be determined in advance, such as how 'applying randomness to a ratio of a data set' is known to be useful for intents like 'generating differences' which is useful for known useful intents like 'generating alternate data sets', so 'generating different structures (like connection, combination, randomness, difference)' is useful in order to 'identify useful structures to apply to a problem', which can be filtered for 'identifiable probable or certain impact on another structure' such as how 'differences' are likely to be useful for 'changing a structure', to 'identify useful structures to apply to a problem which have a reason to be useful, such as they fulfill a problem-solving intent or an input of the same'
  - once you identify that 'applying structures to a problem' is an abstraction of solution automation workflows, you can generate possible inputs to that like 'find/build/derive different structures' (or inputs to that like 'new variables') as a way of fulfilling other workflows using those new structures, or as a way of finding/building/deriving new workflows

- identify useful structures like 'causes of an increase in usefulness or probability of usefulness of a structure' such as 'uncertainty' structures like 'randomness' being useful for generating 'differences' which are useful when structures like 'adjacent structures' are known to have 'reasons to be suboptimal', so generating 'different structures than adjacent structures' is useful for problem-solving intents like 'find/build/derive a new solution or solution-finding method to a problem'
  - example: 'known existing structures' are likelier to be 'adjacent and simple structures', so to find 'non-adjacent' or 'complex' structures, 'opposite' structures of 'known' structures can be applied, such as 'randomness' structures, such as 'randomizing components/attributes/functions of a structure' to increase the probability that the resulting structure will be non-adjacent, given that common innovations tend to be adjacent and suboptimal in some way, so generating structures that are definitively not adjacent is useful to create solutions or solution-finding methods that are not suboptimal in the ways that adjacent structures usually are, given the improbability that a problem will be solved by adjacent changes only
  - this applies the idea of 'cognitive distance' as being useful in generating 'more optimal innovations', such as how 'mashing visible physical things together' is an 'adjacent structure' but is unlikely to produce the idea of 'electricity' or an implementation of that idea like a 'light bulb', however if other functions are applied (such as 'identify variables of core components like electric charge', 'identify structures that are controllable like metals', 'identify how controllable structures can be used to change variables like electric charge', 'identify why using those controllable structures in that way would be useful like for intents such as "creating a controlled electric current"', 'identify similarities between possible outputs like electric current and useful target solution structures like light', 'identify structures that can make an electric current visible and therefore useful'), then 'electricity' and 'light bulb's become adjacently derivable (as opposed to adjacently buildable by combining existing components like 'wood' and 'sunlight')
  - these structures have a 'reason why they might be useful, making them likelier to be useful, which makes them useful to try first as opposed to last', reasons such as 'controllable structures and controllable variables being more useful to agents looking to fulfill intents like "build", as uncontrollable structures are likelier to cause errors (using lightning to create light bulbs is less likely to be adjacently useful than using electric currents in wires), as building a structure requires maintaining a structure, and structures are less likely to be maintainable the more uncontrollable they are, as uncontrollability is correlated with instability'

- identify interface structures like 'apply existing resource combinations for different intent' of useful structures like 'changes in solution-finding methods or new solution-building methods' that are useful for intents like 'finding the next solution-finding method in a sequence'
  - example: the 'next inventions in a field' are usually 'apply one of the adjacent combinations of available resources that havent been tried', 'apply an existing resource combination for a different intent', 'an alternate input/output path between existing available resources using existing resources', and other adjacent conversions of existing and available resources, and rarely involve other structures like 'hidden interfaces'
  - identify which structures are generally useful for useful intents like 'build a new solution or solution-finding method to a problem' can be useful as an alternative workflow to workflows that prioritize using existing resources like existing solution-finding methods, wherever those existing methods are suboptimal or as an alternative workflow in general
  - these interface structures are the 'outputs' of various workflows applied to various interface structures, so storing these for reuse in other workflows/queries is useful as opposed to applying each workflow every time
  - for example, the interface structure of 'apply one of the adjacent combinations of available resources that havent been tried' is the output of applying 'trial and error' to interface structures like 'existing resource combinations', which is useful across problems and can be stored instead of applying a workflow to interface structures to find out which applications are useful, or just to avoid calling the workflow function
  - finding the functions that are useful to skip (storing their inputs/outputs or the useful outputs instead of calling the function) is another useful structure to identify, functions which will be low-variation in their outputs or have a particular output that is more useful than the others, so that useful output would be useful to store as opposed to calling the function

- identify useful structures like 'subsets' of 'alternates' as adjacently applied core functions like 'combine' to create 'complementary subsets' constituting a useful structure like a 'function' that is useful for intents like 'filter the solution space' (here the set of 'maximally different' functions that can act like a base for the function) in a workflow (like 'find maximally different functions, filter them according to some metric, and apply changes to the best maximally different functions to reach the actual prediction function')
  - example: identify 'maximally different' possible solution functions, take 'complementary subsets' of each function, and train on a function composed of the complementary subset functions, discarding one subset of a function when it reaches a low error rate too slowly (when too many diffs between data points and the function are high enough that they couldnt possibly beat the performance of an alternate function subset)
  - this is a useful structure to apply when solving the 'find a prediction function' problem, as a useful way of filtering the solution space of 'maximally different' solution functions to find the function most likely to be similar to the actual prediction function, while not checking every example input/output data point of the data set for all possible 'maximally different' functions, but rather stopping iteration once an error is minimized too slowly for the subset of the possible 'maximally different' function, to avoid further tests, as if its slow to optimize for that subset (relative to the other subsets), its less likely than other subsets to reflect the actual prediction function (or a good base of the actual prediction function, to apply changes to in order to reach the actual prediction function, which is the point of applying 'maximally different' functions in this context)
  - this is useful bc a 'subset' of a function is more useful to check than the whole function (it reduces the points to check, and therefore the steps to check the solution for optimality)
  - these 'subsets' can be combined to create a 'mixed base function' to feed into a neural network as formatted input data (training on a set of a few maximally different functions for optimality is lower cost than training on data sets in general), or to apply a regression algorithm to or similar solution-finding method to the 'find a prediction function' problem, as it will quickly become clear if a subset of a 'maximally different function' is suboptimal for the data points in that subset, more quickly than checking the whole function
  - when a 'subset' of the 'mixed base function' is clearly suboptimal, the remaining points of the subset of that function can be skipped, thereby again reducing the steps to check the solution for optimality, and the other subset can continue to be checked until theyre filtered out or until there are a few candidates remaining so other workflows can be applied or until there is a clear solution remaining

- identify functions like 'combine/add' of other structures like 'coincidences' that create other structures like 'probability' that are useful in finding structures like 'structures that are not a specific structure' which are useful intents for problem-solving intents like 'find structures that are not error structures as possible solution structures'
  - example: identify structures like 'additive certainty structures' which fulfill an intent like 'identify certainty structures with some probability', given that probability of a structure being a certainty structure (like a fact, a constant) can be increased through some complementary probabilities, such as 'coincidences' which when added, indicate that a 'structure is probably not a coincidence with increased probability', which is a certainty structure that the structure is relevant, as opposed to coincidental/random and irrelevant, such as how coincidental 'similarities' are unlikely to occur in large numbers, and the 'similarity' is likely to be relevant rather than coincidental false similarities (for example, an object that seems to have paws and ears is extremely unlikely to be a random assortment of objects and is likelier to be an animal, given that 'multiple coincidental similarities' are less likely the more there are, so these 'coincidences' are additive in being combinable to create probability of 'relevance')

- identify useful structures like 'adjacent incomplete' structures which can be useful for problem-solving intents like 'find a suboptimal base solution to apply changes to in order to improve the suboptimal solution'
  - example: given that 'high variation-capturing variables' are likely to be explanatory of most of the change structures in a function, generate functions using primarily these structures as inputs of 'base functions' to fulfill problem-solving intents like 'change a suboptimal base solution to generate an improved solution'
  - this can be generalized to any structure that is 'adjacently incomplete' given known requirements ('high variation-capturing structures' and 'tuning variables' being requirements to adjust functions composed of 'high variation-capturing structures'), as any incomplete structure can be used as a suboptimal base solution structure
  - 'adjacent incomplete' structures are useful as 'uncertainty' structures bc other useful structures like 'approximations', 'base solutions', 'partial solutions' can be adjacently generated from these structures, and bc 'uncertainty' structures are likelier to reflect complex systems like reality (such as how a 'mix of possible alternate functions' is likelier to be useful than a 'single prediction function', as the other 'alternate functions' are likelier to be able to predict changes to the function than the 'one prediction function on its own', as well as providing 'probable alternatives' to the 'one prediction function')
  - for example a 'known adjacently inaccurate or incomplete function having a mix of constants and variables' is likelier to be useful as an 'approximate solution' to the 'find a prediction function' problem bc its more likely to be adjacently tunable to improve the solution than a 'function having only constants', which is unlikely to be adjacently composable into a solution to a complex problem like 'find a prediction function' or 'find a solution-finding method to find a prediction function'
  - this reflects for example why 'fuzzy logic' is useful, as an 'unchanging function' is less likely to be useful in complex systems like reality than a 'changing function', and an 'approximating' or 'fuzzy' function is likelier to change and handle changing inputs in a useful way, given the randomness (such as vacillation) of change structures in complex systems
  - finding an 'adjacently inaccurate' solution can be a matter of identifying whether some subset of solution metrics are un/fulfilled where at least one is not fulfilled, rather than checking all solution metrics
  - finding 'adjacently inaccurate or incomplete functions' is a useful alternative to finding 'maximally different functions' or 'known error functions', for fulfilling problem-solving intents like 'apply changes to a base structure to find solution structures'

- identify useful structures like 'structural similarities' between interface structures like 'high variation-capturing structures' and 'non-standardizable change structures' as particularly useful structures to identify for common problem-solving intents like 'find structures which can adjacently identify/generate other structures', as 'similar structures' are likelier to be 'adjacent transforms' of each other than 'extremely different structures'
  - example: given that the primary interfaces are equivalently useful bc they capture 'high variation' through isolatable structures (structures like logic, systems, change, etc), equivalent structures on these interfaces can be used to test for relevance to uncertainty structures
    - for example, when 'finding the variables in a language network that are useful for predicting the next sentence in a conversation', a machine learning algorithm could identify the 'previous adjacent words', 'previous adjacent sentences' and 'paragraphs' as useful predictors of the 'next sentence in a conversation', but ideally would identify 'new/different information', 'perspectives distorting communications from standard', 'requirements to resolve info differences between agents', 'info finding intents', 'dependencies like causes of missing info' and other info structures related to the primary interface structures like cause/dependencies, logic/requirements, etc
    - applying these interface structures as 'high variation' structures likely to be relevant in predicting a 'high variation' variable like the 'next sentence' (or the 'next invention in a field') is useful bc of the 'structural similarity' in the 'high variation' of those variables, meaning that one 'high variation' variable like cause/intent is likely to predict other 'high variation' variables like the 'next sentence in a conversation'
    - applying these interface structures to relevant interfaces requires identifying relevant variables like 'information' in a 'conversation' structure (the 'primary intent' of a 'conversation' is to 'provide/receive new information'), then applying the primary interface structures (the 'cause of why an agent has missing info and needs to have the conversation')
    - this can be useful for decrypting the uncertainty in a system, where interface structures like 'requirements' or 'intents' of a function (like 'converse') are not known and the intent of solving the problem requires knowing those structures (like when solving the 'find a prediction function' problem for the 'find the next sentence' problem)
    - this means applying 'requirements', 'intents', etc to an 'uncertainty' (like an 'ambiguity' in a system) and seeing whether the uncertainty can be used for those requirements/intents, given that these are high-variation variables likely to be capable of adjacently explaining the function (meaning 'likelier than other structures, like core functions')
    - once the requirements/intents (and other alternate structures like info interface structures like examples or system interface structures like adjacencies or function interface structures like interactive functions or structural interface structures like connections/sequences/combinations) fulfilled by the function are known, the structure of the function is more predictable
    - for example, once its known that the 'intent' of the 'converse' function is to 'acquire/give information', the structure of the function can be adjacently generated by applying interface structures to the 'missing information' structure (applying 'requirements' to the 'give/receive information' sub-functions of the 'converse' functions to identify other relevant structures to requirements, like 'missing information' and 'new information', will help adjacently determine the output of the 'converse' function, such as the 'next sentence in a conversation', which will likely be directly related to these structures, such as where the next sentence will likely contain 'new missing information an agent requires')
    - the 'info' interface is most relevant to the 'conversation' structure bc the 'primary intent' of the 'converse' function is to 'give/receive information', so this is the interface to base the applied primary interface structures on for this problem
    - in a complex system like the 'find a prediction function' problem format, connecting a 'language network' input with a 'next sentence prediction function' output forms an 'uncertainty' structure that is useful to resolve in this way (applying priamry interface structures like 'requirements' to determine the inner logic of a function connecting these inputs/outputs)
    - this is bc the inner logic of a function fulfilling a particular requirement is likely to be determinable from that requirement, given that requirements are a 'high variation' capturing structure, which has a 'structural similarity' to 'functions' which are also 'high variation-capturing' structures
    - identifying these 'structural similarities' between primary interface structures (requirements, functions) is useful for 'finding filters to apply to reduce iterations of possible solutions' for specific problem formats like 'find a prediction function', since these structurally similar structures can often be adjacently conencted bc of this similarity, and connecting these structures is useful
      - meaning "one structure is likely to be known or easily found by testing for it, like testing if a function fulfills a requirement, and the other structure is likely to be useful to find, such as the function logic fulfilling a requirement"
      - useful structures in general involve a 'connection of a known or easily found/built/derived structure to an unknown structure that is useful to find/build/derive'
      - for example, testing a possible solution (a function that could connect a 'language network' with a 'next sentence prediction function') if it fulfills a requirement of such a connecting function like 'can the function generate a useful prediction function for a subset of documents' is a useful filter for a possible solution to the problem 'connect the language network input with the next sentence-predicting function output', so generating possible solution functions and applying this requirement test as a filter is useful to determine whether the input (language network) can be useful to fulfill that requirement
      - the function logic can be partially derived in most cases just by applying interface structures to requirements (sub-requirements, adjacent/approximate requirements) which a solution function might fulfill
      - these 'function templates' of 'requirement structures' can be filtered for fulfilling a solution metric of usefulness, like 'high variation capturing', 'info preservation', 'number of steps', and 'direct usage of available inputs/functions'
      - the possible solution functions can be filtered by which requirement structures are useful to fulfill with adjacent/available functions (if a 'trial and error' function is available, a function that fulfills the sub-requirement of 'trying every combination of language network structures to predict the next sentence' is not a useful function, so a function fulfilling that sub-requirement can be excluded from iteration when generating possible useful functions)
    - another useful structure to identify is 'non-standardizable change structures' (as in, 'not synonyms or other equivalencies or similarities', which are differences that are robust to standardization, indicating their relative independence), as useful structure to base changes on for intents like 'identifying/generating other non-standardizable change structures', as these structures are likely to be explanatory of differences that interface structures cant adjacently generate, given the structural similarity in non-standardizability (to identify 'non-standardizable change structures like "differences that interface structures cant adjacently generate"', generate/identify 'non-standardizable change structures' as possible inputs to these structures, similar to identifying 'high variation-capturing structures' as possible inputs to other 'high variation-capturing structures')
    - similarly, an 'opposite' structure (involving an adjacent transform of a structure using the 'opposite' function) is likely to be useful to identify the original structure that it has a 'structural similarity' to, since the 'opposite' structure can act like a limit/filter of what is not the original structure, given its useful difference created by a similarity (the 'opposite' function) to the original structure, useful in its similarity to the original structure, but providing a useful difference in maintaining that similarity to the original structure
    - 'non-standardizable change structures' are often a good candidate for a new interface, making them a useful input for intents like 'identifying high variation-capturing structures like interfaces'
      - applying a 'high number of differing change structures' can generate 'non-standardizable change structures' as a useful input for 'high variation-capturing structures'
      - a 'high number of differing change structures' is an adjacent structure to 'high variation-capturing structures', as many change structures occur on each interface given its definition
      - once identified as an adjacent structure to 'high variation-capturing structures', a 'high number of differing change structures' can be used as an input for intents like 'generate non-standardizable change structures' to fulfill intents like 'identify possible new interfaces', which is relevant to problem-solving intents like 'identify high variation-capturing variables' in problem formats like 'find a prediction function'

- identify useful structures like 'pointless functions' to apply as useful structures like 'limits to base changes on, where limits are error structures' to fulfill intents like 'apply changes to error structures to find possible solutions'
  - example: identify functions that are pointless to identify (random function, set of disconnected lines, step function, constant line) and filter out those possibilities first or exclude them from iteration of possible solutions completely as extremely unlikely or pointless to check for, and identify functions that have a reason why they would be useful (functions with sone variation, some generality, some constants, with enough variation to be different from a random distribution and a constant line and other pointless functions to check for) as the only solutions worth checking for, since other functions wouldnt be useful even if they happen to be accurate (given that theyre so unlikely to be correct in general that even if they are accurate for one distribution, its pointless to check for that as its probably an error) as a way of avoiding iterating through all possible solutions
  - this is similar to the 'find maximally different functions to use as base solution functions to apply changes to or filter out' workflow but involves finding functions that are maximally different from 'pointless functions' which are applied as 'limit structures' to base changes on, as a solution would likely be different from these limit structures which are definite errors, as opposed to finding maximally different functions which could very well be the solution function
  - a 'random distribution' would be pointless for a function describing variable interactions because randomness indicates structures like 'lack of organization' or 'chaotic variable interactions of many variables' as opposed to 'organized variable interactions' which a prediction function is intended to describe, as knowing the actual variable interactions is useful for predicting a system's behavior, so a prediction function that is a random distribution is unlikely to be a useful function to describe variable interactions, even if it accurately describes the original data set
  - a related workflow is 'find out how many changes it takes to convert a data set into a random distribution, such as how many points have to be ignored in order to convert the distribution into a random distribution, to check how different it is from a random distribution, and determine how useful it would be to find a function for this data set, or if its likely to be corrupted data or have some other error like lack of information contributing to the randomness'
  - sometimes the point of the 'find a prediction function' can be specified, such as 'finding out if a system is about to crash' which would mean its important to identify signs of stress like 'high variation' or 'volatile' functions or states that precede either of these function states, or 'finding out if the data set has enough info' which would mean its important to identify whether the data set has a random distribution
    - these specific intents of solving the 'find a prediction function' can change which filters are useful to apply and which function types or change structures are useful to check for, so are useful to help reduce the possible solutions to iterate through to fulfill problem-solving intents like 'filter the solution space'

- identify useful structures like 'limits of change structures' which are useful in structures like 'combinations' with structures that have 'structural similarities' like 'filters', for intents like 'filter the solution space' in specific problem format intents like 'minimize an error' to fulfill specific problem formats like 'find a prediction function'
  - example: when trying to find a local mimima like in gradient descent, apply useful structures like 'limits' on 'change structures' (like change rates, changes in change attributes like type/direction/degree, etc) to filter out various points to avoid unnecessary iterations, like when a curve is clearly determined by a sub-section of the curve, so that a directional change can be ruled out, and any points in that direction can be excluded from iteration
  - identify useful structures like 'points where implied adjacent change structures are determined' so that these implied structures can be used as 'limit filters' or 'probability filters'
  - these 'limits of adjacent changes' which can be applied as 'filters of adjacent changes' (to predict adjacent changes, so that some iterations in an algorithm like gradient descent can be skipped) are related to tangent bundles, applying these tangents as 'limits of probable adjacent changes to determine probable functions'
  - this works bc 'limits' are useful for intents like 'find filters to reduce a set' given the 'structural similarity' between 'limits' and 'filters', so 'limits of adjacent change structures' are useful for 'filtering possible adjacent change structures'
  - applying other change structures as filters can be similarly useful, such as how a 'phase shift' has a particular structure of a 'change in change structures' (a change in change rate or change type, like a sudden sign change or wave or constant structure in an otherwise non-wave and non-constant function, where this change type is repeated and/or compounded, as a phase shift is often found in increments in complex systems as opposed to one phase shift in a function), so checking for change structures like indications of 'phase shifts' can be another filter that reduces the structures to check for to determine the adjacent change structures

- identify alternate formats that can emphasize interface structures to fulfill intents like 'connect useful structures like variables in useful formats'
  - example: out of an optimization curve (indicating the highest value as the point where both x and y values are maximized), an x-y plot (standard orthogonal related point graph), an 'axis plot' where values are connected on the axes only rather than in the interim space (essentially graphing the related values as orthogonal number lines), aligned horizontal or vertical number lines indicating direction and magnitude/speed of change in both x and y axes relative to each other, connections between aligned horizontal or verticle number lines indicating direction and magnitude/speed of change in both x and y axes relative to each other, a graph indicating the changes in connection line length as either value changes, or an interim system graph where the variable connections are represented in a format in between an x/y axis and the actual physical system the variables occur in (like where a spectrum variable is represented as point trajectories on a line, and a spectrum variable that changes a spectrum variable is represented as connecting lines between two point trajectories on a line or some variant of this), which best represents a system interface structure like a 'trade-off', indicating a pair of variables that have a 'zero sum game' or 'mutually exclusive' pattern where an increase in one decreases the other? 
    - whichever graph indicates the important info about the trade-off, which is usually how the trade-off relationship changes (where its change rate is highest or how it changes) or where both values are maximized (optimization point on an optimization graph) or how the trade-off develops (where trade-offs can be identified and prevented in a causal network diagram) or the system where the trade-off occurs (to identify alternates to the tradeoff or alternatives to the tradeoff variables to invalidate the tradeoff or variables that can change the tradeoff)
  - generating these alternate graphs is a matter of changing graph variables like 'where changes can occur', 'what structures can be connected', 'how many output structures there are (one line or multiple lines)', 'alignment between variation (parallel or perpendicular axes)', 'whether an alternate variable interaction format like a particular graph is itself formatted in terms of its metadata like its change rate'

- identify useful components like 'find a function that can generate any useful structure' as a useful function to identify various problem-solving intents like 'find a learning function', and the interface queries that can adjacently identify these useful structures (like 'invalidate requirements' as a useful 'opposite' structure to fulfill as a general problem-solving intent)
  - example: a 'requirement' is a 'dependency' structure which can usually be invalidated so its not a requirement anymore, thereby creating 'alternate inputs' which can generate a useful structure like the function triggered/fulfilled by those requirements
  - the 'function fulfilled/triggered by requirements' or the 'outputs of requirements that fulfill/trigger the function' are other 'positions' on the 'causal sequence' which can be a target for 'reducing the requirement of that function' or 'reducing the requirability of that requirement' or 'reducing the requirability of the output of that requirement'
  - 'reducing/invalidating a requirement' can take the form of functions that 'find alternate paths connecting two points (like connecting the inputs of the requirements with the target function in an alternate path)'
  - applying 'invalidation' and 'reduction' structures to 'requirements' is an alternate way to reduce/invalidate these requirements, such as 'reducing usage of the function triggered/fulfilled by that requirement', 'finding alternate paths to the outputs of that requirement' to invalidate it, and applying other structures of reduction/invalidation
  - this workflow involves identifying structures of optimization like 'requirement-invalidating structures' to fulfill problem-solving intents like 'reduce requirements/dependencies' to fulfill problem-solving intents like 'find alternate solutions'
  - it can be generalized to identifying 'opposite' structures of a structure, to identify the structures that would invalidate/oppose the definition of a 'requirement' so its not a requirement anymore
  - the definition of a 'requirement' could be 'the only structure that can fulfill a function'
  - applying 'opposite' to this definition could result in 'applying "any/all/other/multiple" to the value of "only" as a difference applied to the "number" attribute of the word "structure"', then 'finding a way to connect the only structure with any/all/other/multiple structures' by 'creating other structures' and 'testing and filtering these structures for usefulness in fulfilling the function'
  - applying useful structures like 'opposites of suboptimal structures like requirements' can be generalized to 'identify structures that make a structure more useful (like optimization structures such as solution metrics like accuracy)'
  - applying the 'opposite' of a 'requirement' structure can build functions that 'need as few requirements as possible (solution-finding methods, learning functions)'
    - identifying this structure requires applying the query 'find a function that can generate any structure, including structures like alternatives to requirements' which can produce a 'solution-finding function' or a 'learning function'
  - identifying the 'find a function that can generate any useful structure' as an important function to build can adjacently enable identifying 'interface structures' as useful, where just 'find a function to find any prediction function' would likely not adjacently identify interface structures, as it is too specific and has too many specific functions like combination/filter functions available in the 'find a prediction function' problem space which dont have meaning or a concept of general usefulness built-in

- identify attributes of useful structures that make them easily identifiable to use as filters to find those useful structures, optimizing the 'filter' core interaction function by making structures more identifiable and therefore more usable as filters
  - example: 'functions with potential' are a 'generative function' of 'variation', and 'functions with potential' have 'overlaps' with other useful structures like 'important/powerful functions' and can also 'generate potential', so in response to an error they can generate any structure to handle it
    - the 'generates potential' function of a 'function with potnetial' is an easily identifiable attribute of a function (any function that cant generate potential can be quickly tested and identified)
    - identifying this as a function that these functions have is a matter of applying the definition of 'potential' as being a 'high variation generator and potential generator'
    - a 'function with extremely high potential' (such as a 'learning function', a 'solution-finding method', etc) would generate a 'reason to have a reason to become useful', rather than expecting a function without potential to become useful on its own in absence of incentives to be useful like adjacencies to useful states, whereas a 'function with potential' can act like a 'potential generator' by making other non-useful functions into useful functions, identifying the 'lack of incentives to be useful' as a problem it can solve (or learn to solve)
    - 'functions with high variation' are likelier to be 'functions with potential' as an interim structure between 'low-variation functions' and 'random functions', which are opposites connected by similarities like their common lack of usefulness for generating useful variations through organization, one having an extreme in organization such as a low-variation function, and another having the opposite extreme in disorganization
    - an example of applying this to a problem is in tricking cancer cells to think a hsot cell is a cancer cell which should be collaborated with and assisted/protected, or tricking cancer cells into having a reason (like an incentive such as a barrier between the cell and its harmful requirement sources or an adjacent path to those sources) to create their own requirement sources without borrowing from the host
    - this can be used as a 'filter of functions with potential' to find these especially useful functions
    - identifying a 'neural network' is a simple matter of applying an interface query like:
      - 'identify core components of the solution format ("prediction function") which are "change structures like variables" or "inputs of the solution format" which are "combined iterative change structures" and apply core interface structures like "combinations" to those prediction-function components ("variables") to fulfill the "generate possible solutions" problem-solving intent and filters to fulfill the "filter the solution space" problem-solving intent in the "generate possible solutions and filter the solution space" workflow, or applying the "start with a base solution and apply iterative changes to it and test each change for optimality" workflow'
  - reverse-engineering 'identifiable attributes' is a variant of this workflow to identify what attribute structures would be easily identified, filtering those by what attributes would be useful to identify, and filling in or connecting the remaining attributes of such structures using various workflows
    - answering the query 'what would be identifiable' is a matter of identifying 'differences' as the relevant structure that can be used to generate 'identifiable' structures to reach a useful structure like 'maximally different structures' as 'identifiable' structures
  - a variation of this workflow is finding interface structures like 'combinations' of useful structures like extremely 'interactive' and 'identifiable' structures, as being particularly useful for intents like 'connecting extremely identifiable attributes using interactive structures to find structures with those attributes' as useful 'cooperative' structures for such intents, as 'interactive structures' can fill in the gaps in certainty structures such as 'identifiable structures'
  - a generalization of this workflow is 'identify structures that make core interaction functions like identify/reduce/connect/filter easier to apply'

- identify interface query structures like patterns and components as inputs to fulfill useful interface query intents like 'stop processing a sub-interface query when its unlikely to become more useful'
  - example: identify common successful sub-interface queries, as in a 'interface query component which is also an interface query' (which frequently involve useful structures like 'core' interface queries like 'find differences') and combine them or drop them in at rnadom whenever an interface query is suboptimal, randomness being a useful testing tool and a useful method when no information about relative optimals is available
  - alternatively, applying sub-interface queries that have a reason to be useful other than 'commonly being useful in successful example solutions', such as 'providing a common input to other commonly used interface query components' or 'being useful for identifying when a particular query is unlikely to become more useful and can be skipped'
  
- identify useful structures like 'alternate structures having the same info' as a generally useful structure for problem-solving intents like 'find different formats to solve the problem in' or 'find a format where the problem would be more solvable with some set of functions'
  - example: find 'alternate structures that preserve info' as alternate problem/solution formats as an initial rule to apply in interface queries to check if the problem is adjacently solvable in a generatable alternate format preserving the information (like a data set with the same statistics and a similar distribution but which has some outliers handled by a separate function, as a multi-function solution that covers the same data set, preserving the original info of the problem)
  - this involves applying the 'set of available functions' as a 'limiting' structure of 'possible solutions', as in the query 'identify what structures can be connected with these available functions' and assuming the solution or solution-finding method is within the generated 'set of possible structure connections'

- identify ambiguities which can act like useful variables such as where there are multiple requirements of a solution automation workflow like 'generate possible solutions and filter them by some test' to create variations of the workflow like 'create the filters and apply differences to identify possible structures on the correct side of the filters', where the variable can be changed without impacting the output of the workflow (the 'reduced set of possible solutions that pass the filter test'), so it can create alternate equivalent workflows which have different advantages/optimality cases
  - example: "applying 'limits' as an 'input' structure to a function that applies 'differences' to those limits to 'generate possible structures within the limits'" is an alternate intent to 'generate possible structures by applying variable combinations, then test if each one is within a limit by finding the difference from the structure to the limit'
  - identifying these as alternate useful structures involves changing the 'direction' of the interaction of the structures:
    - one generates limits first and applies differences within those limits to generate possible structures
    - one generates possible structures first and filters them by limits to find the reduced set of possible structures within limits
  - but both have the same or similar outputs (the first successful structure or the set of structures that pass the test)

- identify useful structures like 'relevant difference structures' to fulfill common problem-solving intents like 'identify differences/similarities'
  - example: apply 'probability' to identify frequently relevant difference structures (difference that are related in some interface or other useful structure, like sequential patterns, outputs usable as inputs, or adjacent changes like 'opposites' or an attribute change like an 'angle' change, etc) as opposed to frequently irrelevant difference structures like 'randomness' to find useful differences to apply, such as for common problem-solving intents like 'correcting a corrupted data set', 'finding relevant change types to include in a filtered data set', or 'generating a similar but more solvable data set' (like by finding adjacent patterns and standardizing to those patterns, which are likelier to describe the data than randomness), as differences are useful for common intents like 'identify errors', 'identify useful differences', 'identify similarities', 'compare structures', and 'identify variables'
  - alternatively, apply 'cause' to identify 'relevant difference structures' such as 'differences which have a reason to be useful' (which includes common structures, pattern structures, etc)
  - randomness optimizes its usefulness in cases such as where no information is available, to test a structure for stability in unpredictable/uncertainty circumstances like a randomized variable being applied, or where there is no reason to add structure to the variable (where a random value doesnt create unacceptable errors for a system that can correct those errors)
  - identifying which variables like randomness vs. similar (adjacent) differences are similar to each other can identify where a variable would have the opposite effect, where it can be replaced with an adjacent substitute, where its value is unique, and where its value is optimized
  - applying randomness can identify a 'hidden interface' error structure (using queries like 'apply trial and error to all possible attribute combinations that are not in a known structure'), but so can finding the 'similarities in differences' which point to the similarity (interface structure) those differences are based on
  - to derive the 'electricity' interface, a 'similarity' can be found in the 'energy requirement of all biological structures' indicating that 'structures whose energy mechanism isnt known still have one, its just not visible' and in the 'preservation of energy (through heat transfer)' which indicates a hidden interface that is 'not visible in most cases like when heat emanates', implying that if energy transfer was isolated, it could be controlled, and other mechanisms than just energy were relevant (electricity and related concepts like charge determining interactions of core components of visible structures)
  - alternatively, random combinations of interface structures like attributes can be tested for relevance, like a combination of structures like attributes such as 'energy' and 'core components' or 'energy' and 'input-output sequences' enabling 'energy transfer', as prior to identifying electricity, 'energy dynamics (rules governing energy)' werent completely known, and interface structures like 'limits', 'usage', and 'time' can be used to infer relevant energy concepts like 'energy storage' which can fill in the missing info about energy or identify what info is missing
  - an example of a 'relevant difference structure' is in converting a pattern like '/ / / \' into a pattern like '/\/\' which is a wave function, a common useful structure, by making one adjacent change (flipping one point connection), which is relevant bc its similar enough to the original to be a potential correction of a trivial error in the original, and contains a common useful structure like a wave, which is a very significant structure ('vacillation between extremes' being a common useful change structure, which can be used to construct an interface) in many systems

- identify that useful structures like 'connections' can be derived between 'error types (like "false similarities") connecting solutions/errors' and the 'filters used to check for structures indicating an error or solution' which can be connected with functions like 'find the most relevant (such as most variation-separating) filters of a structure' or 'most difference-creating differences of a structure' to create this connection structure, which can be used to derive filters to resolve the 'false similarity' error (identify solution/error) from the input 'error type' structure
  - example: identify that error structures can look like a solution structure if other structures are present, such as the 'relative inefficiency or improbability of correcting the error', or 'incomplete error correction, such as where only some examples of the error are corrected' or 'errors that havent reached a threshold value that would trigger error-correction function application or the development of error-correction functions', which is a 'false similarity' that can be resolved with tests/filters such as 'intentionally re-triggering the error to see if a correction occurs at a particular scale and if not, it could be a solution structure, like a valid function to use as a solution component'
  - the 'connection between the "false similarity" of these solution/error structures and the tests which can detect the "false similarity"' can be derived as a useful structure to 'differentiate errors/solutions'
  - identifying the filters that would identify an error/solution in a set where there are false similarities between errors/solutions involves deriving structures that can create differences such as 'triggering errors at scales likely to trigger error-correcting limit functions if they exist to handle the error, which is a sign that its an error', which adds a 'difference structure' in the 'scale/number of errors'

- identify useful structures like 'precursors to certainty structures' or 'certainty-adjacent structures' which can be useful for identifying/creating useful structures like 'certainty structures'
  - example: a 'measurement tool' can be useful as a cause of 'measurable structures' as it creates a 'certainty-adjacent structure' which energy/change can adjacently fill to create a 'complete certainty structure', converting that energy to the information required to convert the 'certainty-adjacent structure' into a 'certainty structure' such as 'information', similar to how 'known structures' are likelier to be similar to 'unknown structures' than different from them, as 'known information' is useful as a 'base to apply changes to', just like the 'measurement tool' is useful as an 'incomplete certainty-adjacent structure' to complete, as 'similar structures' are subject to a force prioritizing similarities, reflected in the 'adjacency of most change types'
  - the 'measurement tool' acts like an 'opposing force' to the structures its designed to measure which can apply changes to those structures to convert them into a certainty structure that it can measure, like a 'liquid filter' is likely to be designed in response to seeing a 'liquid' as an 'opposing force' that can 'separate the liquid (oppose the continuous attribute of the liquid in order to measure it)', so the 'liquid filter' could also 'produce/complete a liquid' if adjacent to a liquid-adjacent structure like 'separated liquid components'
  - these structures of 'opposing structures of change types' are useful to identify/build/derive functions that can detect change types, which is another way to derive solution-finding functions

- identify useful structures like specific useful filters of certainty structures, like 'opposing forces' which result in most variation occurring between the two extremes rather than at the two extremes, given the inherent structure of a 'spectrum created by opposing forces'
  - example: given that 'chaos tends to overwhelm systems rather than be contained in systems', there is no system that can handle chaos ('perfectly random disorder'), which is a way to identify whether a system is real or not ('does it generate randomness' or 'does it generate perfect order' as a filter of systems likelier to be real, as the opposing forces between randomness and non-randomness are in a constant interaction and vacillation between either extreme), as systems are unlikely to perfectly fulfill any intent, specifically extremely ordered or extremely chaotic intents, as a system is likely to be required to be very simple to create extreme order, and extremely simple (isolated) systems are unlikely to occur in reality

- identify useful structures that fulfill useful functions like 'oppose' to identify useful structures like 'stability' and 'balance' achieved by 'opposing a structure' so that it cant 'destabilize a system', as 'stable systems' are useful in intents like 'creating organization or order' such as a 'prediction function-finding function' which 'identifies a stable, organized, certainty structure out of difference structures like variables', as any structure which is sustainable enough to be useful to identify (such as a 'regularly occuring error type' which is useful to identify) also likely has a possible opposing structure that limits it, as such a structure is likely to be a difference made possible by an interface which has a 'change-limiting structure' built-in
  - example: identify the 'limiting structures' associated with various interface structures like 'combinations' of structures like 'attributes' of 'differences/similarities' which provide a way to calculate the 'limit' of the 'functionality or usefulness' of these structures where their potential to generate useful structures like useful differences is 'limited', 'limits' being useful as an 'opposite' structure of 'differences' that can produce 'similarities' or 'stability' (a 'system with limited change' is likelier to be 'stable' than 'chaotic'), in that limits are a 'constant' (a 'similarity type, as in a similar value') which can act as an opposing force to 'differences', as 'similarities in differences' are a useful structure on their own and specifically 'limiting structures of differences' are a specific useful 'similarity in differences'

- identify useful structures like 'connections between core structures like differences/similarities' and their attributes (like types, filters, examples, and exceptions) as a way to 'identify commonly useful specific connections between various specific differences/similarities', given the usefulness of these core structures for core interaction functions like 'connect' which commonly occur in workflows like 'connect problem/solution'
  - example: 'differences' are likelier to generate 'differences' than 'similarities', so identify 'differences that cause other differences' as a way of identifying these 'difference sequences' which are useful in connecting complex problems with solutions, as opposed to 'differences that cause similarities like randomness'
  - identifying the probability of differences as higher in the intent of 'generating differences' than in 'generating similarities' and the relatively higher probability compared to 'similarities' as a useful structure is useful on its own, but also when combined with other useful structures like the 'similarities that are likely to generate useful differences' such as when 'adjacent combinations of core components' are likely to generate 'other useful structures like functions in a system', these 'similarities that are likely to generate useful differences' being useful as a filter of 'similarities to filter out when identifying useful differences to generate differences'

- identify useful structures (like 'connections between known connected structures and structural similarities in differences from those connected structures, to connect other related connected structures') to connect which are useful for fulfilling a problem-solving intent like 'find changes to apply to a problem to solve it' or 'find changes to apply to a known solution to solve another problem'
  - example: identify that 'combinations of attributes like uniqueness (difference from other structures like other solutions), lack of coincidences (presence of valid reasons), and requirements' are frequently enough to identify a solution function for a problem aligning with those attributes (like a problem that is different from other problems), thereby applying 'structural similarities' in the 'differences between problems/solutions and alternate problems/solutions', as 'similarities in differences' are a common useful structure to apply when two structures are known to be connected, as problems/solutions are
  - identifying the structures (like 'known connected structures') to apply these useful structures (like 'similarities in differences') which can help fulfill intents like 'identify useful structures' (like 'structural similarities') can fulfill intents like 'find changes to apply to one problem to solve it or find changes to apply to another solution, based on the differences from another problem/solution'
  - the connection between 'known connected structures' and 'structural similarities in differences from other known connected structures' can be used to derive a missing structure connected to another structure (identify the solution from the problem)

- identify structures like the 'improbability of similar/equivalent resources being useful in a combination' by identifying the 'commonness' of attributes like 'diversity' in solution-finding methods, given the relative 'improbability that similar structures will be useful for extremely different intents' such as the 'different intents required to produce different info', 'different info' like that which may be required to solve a complex problem such as a 'find a prediction function' problem with 'an input data set of many variables'
  - example: functions like 'find' and 'test' are commonly useful enough to identify the solution function with the right information, given that one function is unlikely to have the inputs/functionality required to be useful in most contexts in isolation of every other resource like other functions
  - solution-finding functions are unlikely to require few or similar functions to be useful, but are likelier to require different functions which provide 'additive information' (each function identifies different info) so building a solution-finding function by 'combining extremely different functions' is likelier to be correct than 'combining extremely similar functions'

- identify connections that can resolve common error structures (like 'missing information' errors) like connections between 'commonly missing structures' and 'commonly required or otherwise useful structures' across solution-finding functions such as solution automation workflows
  - example: identify structures like 'common required structures' (such as how almost every change occurs 'adjacently') by applying the 'reason for common change types', in order to derive structures (like 'requirements' and 'important structures') in a context with errors like 'missing information', such as a problem of 'creating a new change type', 'identifying a common change type', or 'deriving the reason for a change type', the 'reason for adjacent changes' being that 'changes are usually required to be adjacent', so 'changes which are not adjacent' (such as the 'opposite' change between a positive/negative charge with no interim potential, so any change must be an opposite rather than an adjacent change) can be filtered out as unlikely or impossible in most realistic system interactions
  - the fact that most change types occur adjacently (in a connectible state sequence) is not a coincidence but is rather a requirement of physical reality and most other systems, given the need for 'incremental adjacent changes' to fulfill multiple intents like 'system stabiility', 'system efficiency', 'negative feedback-handling mechanisms to produce reversions to an adjacent previous state', etc
  - these intents override other intents in terms of importance, so finding these intents which could be used to derive the change types allowed by these intents is useful, as opposed to other intents like 'finding every possible difference' which could produce different structures that favor more volatile systems
  - similarly the 'meaning' structures (like 'intelligence') generated by 'combinations of components' (like 'cells') is another common 'requirement' of most systems including physical reality, given how alternate structures like 'intersecting fields' (like 'radiation' interacting with 'life forms') are less likely to be capable of fulfilling these intents like 'system stability' than other structures
  - the reason these structures are common (they 'fulfill these intents optimally') is unlikely to be a coincidence but rather an indication of the importance/requirement of these structures, which is suboptimal to ignore and useful to use in most interface queries, as a structure that adds certainty given its reflection of truth
  - the "connection between 'reasons' and 'probable change types generated by those reasons'" are useful structures to identify to find/build/derive one from the other
  - the "connection between 'system structures' and 'probable change types allowed in those systems'" is another useful structure to identify as a way to find/build/derive one of these structures given an example of the other
  - these 'connections' are particularly useful given that they are commonly used in interface queries to solve problems (its common to need to predict a change type in a system without perfect info about that system), as problems frequently involve a lack of information about some of these structures, so finding connected structures to these missing structures is a way to find/build/derive these structures
  - these commonly disconnected structures which are useful to connect are often additive in usefulness
    - 'connections between reasons and the change types they generate' are additively useful to 'connections betweeen systems and the change types they allow', since one can help filter the set produced by the other, given their common structure of 'change types'
    - therefore 'connections between commonly missing/useful interface structures' like these which have 'structures in common' can be identified as useful to identify in a combination, as opposed to only identifying one
  - 'adjacently connected structures' are also likely to generate 'isolatable components' which can form a new 'interaction level'

- identify useful structures like 'alternate structures that remove variation in another structure like the original problem' such as 'adjacent variables of a data set' which can be used to 'remove the variation to solve for in the original data set' which is useful for intents like "solve the 'find a prediction function' problem"
  - example: solving for 'adjacent data sets of alternate variables' (like causal variables of the original data set) which are derivable from the original data set and which are easier to solve for than the original data set problem is an example of applying 'adjacent changes' to 'specific problem/solution structures' in a way that is useful for deriving the solution (solving the variable interactions surrounding the original variable interactions will also solve for the original variable interactions)
  - this workflow specifically focuses on 'removing variation to solve for' by applying 'adjacent structures' to generate 'alternate more solvable problems' to 'reduce the variables to solve for in the original problem'

- identify useful intents like 'identify networks/algorithms that can identify different variables' and use these as 'interaction structures' which can fulfill intents like 'filtering variables that are already identified' in specific interface structures like a specific problem space/solution format (like 'find a prediction function' using the 'neural network' solution format)
  - example: a 'network of parallel trainers, each trainer using different inputs from the data set' can 'help each other get to the output value faster' or 'help find the certainty structures of the prediction function faster' if they are optimized to 'find certainty structures' (like convergence points, thresholds/phase shifts, or patterns) where a change type becomes clearer in one trainer before a parallel trainer, at which point that trainer can share this 'probable/certainty' structure with the other trainers, without a sizeable increase in computation cost given the relatively few important certainty structures to identify, so that other trainers can branch off to skip ahead using the certainty/probable structure and check if it generates the correct answer
  - changing the 'weight update algorithm' applied to each trainer can create this variation in the 'order of certainty structures found', where variation in the inputs' reflection of certainty structures does not create that variation (such as where one input sets' coefficients for a variable are all zero for a particular trainer, thereby missing the opportunity to identify the certainty structure of that variable's value when nonzero)
  - optimizing parallel trainers to identify different variables can be more efficient if its possible to determine the 'remaining change to explain' and create 'alternate parallel networks to identify those change types', or 'reduce the inputs by the change explained by already-identified variables' so that further iterations are applied to identify the 'composing variables of remaining change'
    - if the remaining change is trivial and could only specify the output function beyond the required solution metric value like 'accuracy range', skipping the remaining training as a generalization function is an alternative optimization
    - optimizing to find certainty structures can fulfill the intent of optimizing to find different certainty structures that, when combined, are likely to explain most of the change in the inputs
  - applying structures of importance (like a 'high-variation variable' which is likely to reflect/cause variation, 'required variable' or a 'unique variable' of a 'required function') as components of a network to check for those variable structures first and filter them out as possibilities first is another optimization that can be used to filter inputs initially or at regular intervals, re-checking new variables as theyre built from previous layers, as there are only so many variables that can be important in a data set, as usually not every variable is equally important

- identify useful structures like connection structures of useful structures like core interaction functions and relevant useful structures like 'connecting structures of connecting structures' which are useful for intents like 'finding a solution-finding function'
  - example: a solution like 'machine-learning' is likelier to have a higher percentage of core interaction functions (as a machine-learning workflow involves standardization, involving the 'reduce' and 'compare' functions, the 'combine' function to create 'variable combinations' as inputs/components of the solution 'prediction function', the 'filter' function to 'reduce possible solutions', and the 'connect' function to connect 'input/output variables') given the complexity of converting a set of functions into a useful structure (like a 'solution-finding method' or a solution to the problem of 'finding a solution-finding function' or a general input solution to the problem of 'find a prediction function'), so finding the functions that are different enough, combinable enough, interactive enough and otherwise sufficiently useful to be usable as adjacent components of a general solution-finding function is a useful intent, and other solution-finding functions are likely to be composed of similar components/inputs/sequences
  - finding the function/structure (like 'overlap') that can tie these functions together in a useful way is similarly useful, as 'overlaps' in 'change combinations' of the neural network create a useful structure in 'applying changes to common inputs (previous node layer) to create differences', as opposed to 'applying changes to different inputs to create differences', which is less 'standardized', and such a lack of standardization would make the network inefficient, having to consider many changes to different inputs instead of basing changes on the same inputs iteratively (in 'continuous trivial adjacent changes'), which is similar to how most change types work (as opposed to 'volatile/random changes')

- identify useful structures like 'change type connection structures' as useful structures like 'alternate structures of cause structures' to use in fulfilling intents like 'find alternate useful structures' uxing useful structures like 'probability' and 'requirements' to find the useful structures like 'change type connection structures'
  - example: there are relatively few constant/boolean variables compared to continuous variables, as its less common to have a clear separation between relevant variables like types or components indicating a constant/boolean value, as spectrums are more common, just like 'normally distributed variables' are a common variable type and parabolas or a 'change applied to itself' are a common variable type reflecting a core interface structure ('change developing around a foundation (like an average, a limit like masses gravity applies to, or a core component set) but not invalidating the foundation') where the parabola is a common change type bc the 'change applied to itself' frequently occurs given the 'structure (like boundary) development patterns' of core structures like a variable, where initially it would only have itself to interact with until the boundaries preventing interaction were removed and it begins to interact with other variables, which is how systems tend to develop or develop in response to new variables, which are either isolated once introduced or begin to interact with other variables once interaction barriers are removed
    - these 'normally distributed variables' are likely to interact with, reflect each other, and have causes in common, given the similar change types reflected in these variables, and the interfaces theyre based on are likely to be adjacently connectible
    - boolean/constant/type variables are likelier to be based on spectrum variables, rather than the opposite, given the structure of these constants as 'separable values', which are likely to be a subset of some spectrum variable
    - these 'probable or required structural connections between variables' can be used to determine 'interactivity of variables', which can be used to fulfill problem-solving intents like 'find interactive structures' to fulfill intents like 'build a solution out of interactive structures'
    - these types of structural connections are alternate derivation structures as 'causes' of a variable structure, as 'acting like an interface, providing a foundation for change' is a type of cause and 'interactivity' is another cause, so these alternate structures to 'causes' can be used where structures used to explicitly derive cause ('intent', 'requirement', 'output' info) arent available
  - a related workflow is to find structures that connect 'causal structures' (such as 'variables that frequently cause the most changes' like 'randomness', 'structure', and 'requirement') so one cause (like the 'cause of structure' or 'possibilities') can be converted into another more relevant cause like 'direct cause' (like 'structure' or 'requirements'), such as how 'probability filters' can connect 'possibilities' with 'requirements' (as in 'probable structures are likelier to be required structures'), so 'probability filters' are the 'connecting structures of causal structures', which are useful in fulfilling problem-solving intents like 'find a prediction function' or 'find variable connections'

- identify structures like 'info preserving change structures' that fulfill useful structures like 'requirements' to fulfill intents like 'create useful structures' (like 'difference from lack of input/output connection', 'difference from equal input/output', 'difference from constant output', 'difference from random output', 'difference from volatile input/output connection') which can be used to fulfill intents in specific problem formats like 'find a prediction function'
  - example: a 'function that changes inputs in a way that is different from producing zero change (an identity function or returning a constant value) loses all the information of the inputs or retains only the input information (and cant produce an output, like a dependent variable)', and a 'function that produces enough change to essentially be so volatile that it creates a random or random-like probability distribution of outputs' would be equally useless functions for the intent of 'produce a function capable of learning the change types connecting inputs and some dependent output' (like a machine-learning algorithm or regression function), because this 'lack of connection between inputs and outputs' takes various structures including producing a 'constant distribution' every time (like a random or normal distribution), or a 'constant value' every time, or the 'equivalent value as the inputs'
  - these structures of 'equivalence' can be used to filter functions that are capable of connecting inputs and some dependent output, to try various functions that can produce changes which are possible solutions to the 'find a prediction function' problem
  - these 'change structures' should preserve the input info in some structure like 'degree', bc the output is connected to the inputs
    - structures of 'info preservation' (like 'degree of info preservation') can be used to filter the change types which are possible values for the function that can act as a 'learning function' or a 'function-deriving function', including structures of 'info embedding', 'info compression', 'info encoding', 'adjacent info conversions', 'info representations like averages and aggregations' and other info-preserving structures
    - structures of 'volatility' are less likely to be useful for most functions, so functions that create volatile changes to inputs can be filtered out as possible solution functions that can act as a 'learning function', as 'trivial adjacent changes' are more probable than 'extreme adjacent changes'
  - this workflow involves deriving error types ('differences to avoid') in a particular useful interface structure ('input/output-connecting change structures') in a problem space ('find a prediction function') and identifying interface structures that can avoid those error types ('info-preserving change structures'), then identifying the interface structures ('requirements') of problem-solving intents ('create useful structures') that this fulfills
  - an extended and more specific version of this workflow would identify the specific 'variable values' of the function-finding function that tend to produce these errors ('volatility', etc) or solutions ('info-preservation') in a general function-finding function
  - this is true in machine-learning, where 'perfectly preserving inputs (equivalent inputs and outputs)' is useless (such as when an algorithm finds a prediction function that perfectly aligns with the input data set) and where 'constant change' and other simple change types are also useless (as they lose almost all the input information), which reflects the bias vs. variance problem
  - the dichotomy between 'info preservation' and 'info loss' is an alternate way to derive the bias vs. variance problem (and the solution of the interim function-finding function that can reflect change in a different way than just preserving it perfectly)

- identify structures that have useful structures like useful 'attributes' for various workflows like 'combinability' (given that various workflows involve both 'combinations' and 'combining workflows') or useful contexts like 'common problem space states, such as combinations of errors common to many problems'
  - example: identify structures that are 'probably true' with other workflows like 'apply patterns to find probable truths' and then identify 'testable variables that could change the solution the most of the remaining possibilities' and filter those variables to derive the remaining information
  - this applies multiple filters that are cooperative such as 'probability' and 'high-variation producing variables' to reduce variation in the set of solutions to filter ('filtering a set of variables determining the remaining possible variation after a filter like probability has already been applied to either the solution set or the variables of solutions, that is already filtered for being testable and for having a probable high impact on the solution', as opposed to 'filtering all possible solutions' or just 'filtering all probable solutions')
  - these are 'additive filters' which can be combined to add accuracy in reducing a set in most cases
  - this attribute of filters is useful in cases where different types of missing information are likely, which is a common case when solving most problems
  - identifying these useful structures of 'useful filters' with 'useful attributes like interactivity/additivity' as well as useful structures like 'structures of interface structures which are common to many problems, like common combinations of errors' are new useful structures to identify and apply

- identify structures that use probably available structures or more solvable problems (like 'more predictable subests of a data set') and use these as a starting point to apply other workflows, like finding interface structures that can connect them, using the attributes of the problem space (like that 'adjacent subsets can be connected in a sequence to create a function' in the 'find a prediction function' problem space)
  - example: a 'change structure sequence pattern' is likely to be useful in determining which change structures (like change rates) follow which change structures, such as when a 'probable correct subset' of a function is known or almost certain and other subsets of the function are to be inferred, given that a solution to the 'find a prediction function' problem is likely to have more predictable subsets (such as denser, less distributed, common, more predictable or simpler subsets) in some sections of the data set, and a pattern that can connect these change structures like a 'change structure pattern' in a sequence which is useful for connecting adjacent subsets, could be useful to create the solution out of these more predictable subsets
  - this works bc some components (like 'subsets') of a function are likelier to be more 'predictable' (more solvable) than other subsets, and these components may be usable to derive the others in certain position patterns of the subsets, like 'determining subsets' or 'alternating subsets', which can easily be used to connect adjacent subsets of the function
  - 'predictable' here means 'simpler' (meaning 'isolatable', 'adjacently constructed from few core inputs'), 'follows common patterns', and other structures of predictability, as opposed to structures of unpredictability such as complexity, randomness, or rarity
  - a related workflow is to find structures that are useful for some intent related to the problem, such as 'predict' or 'filter uncertainty structures' or some general intent like 'connect', 'identify core components', or 'identify change structures', which can be plugged in to some workflow, and combine the useful structures in one workflow to derive the missing information
    - for example, if some 'subset' can be predicted, that can be used in workflows like 'break problem into sub-problems' where the 'sub-problem' is 'solve for the subset of the function'
    - then this workflow can be applied to identify other subsets, and where they cant be identified, use 'certainty structure filters' to derive the remaining missing information such as by applying common patterns of 'function subset connections', 'commonness' being a 'certainty structure'
    - this 'combination of useful structures' of 'predictable structures', 'sub-problems like subsets', and 'certainty structure filters' can be used as a general 'structure filter'
      - for example, one subset can be more easily predicted bc its denser than the other subsets, and another subset contains an average when tested with a set of samples of the data set, and theres a pattern that the 'subset containing the average tends to be surrounded by subsets with high change rates', which can be used to connect the subset having the sub-sampled average and the dense subset

- identify useful structures like info formats that have variables which can determine relevance/usefulness, to identify variables of info formats that can be used to create new info formats, to fulfill problem-solving intents like 'identify useful formats to solve a problem in'
  - example: what makes 'all possible versions' of a representation useful, like 'what makes all possible versions of a representation of an object as a trajectory crossing a set of adjacent graphs, each representing object attribute values' is the fact that some of these 'adjacent-graph positioning' functions will make some interface structures like change types or similarities clearer than other positioning functions
    - this variation in 'all possible versions of a representation' allows for possible usefulness of some versions over others, and allows for the representation format to exceed the usefulness of other formats in some cases
  - what makes this representation more useful than representing objects as a point on a graph, where similar objects are adjacent, is that it relies on an aggregate of some similarity metrics like 'the sum of the subtracted differences between attribute values' which can cancel out information like where subtracted differences are opposing values that cancel each other out, removing that information from the point
  - the 'adjacent graph trajectory', even with a meaningless position function making adjacent attribute graphs possibly irrelevant (as opposed to a meaningful function that selects related attributes to graph in adjacent graphs), preserves info about attribute value differences across objects in a way that doesnt involve possible neutralizations of differences, as no aggregation is involved
  - this structure also adds a benefit of standardizing change types, using an x/y graph to represent each attribute, even if the values have different data types
  - an attribute graph-positioning function could also be designed to intentionally highlight interface structures like 'causal connections', 'trajectory intersections/overlaps and differences', 'attributes varying on the same interface', and 'similarities of data/changes types'
  - the variables of this representation like ('graph position in a graph network', 'adjacent graph continuity', 'adjacent graph relevance') allow enough variation that could be useful for representing info in a clear way
  - similarly, in a graph of a function's logical steps (without using a function/variable network, just a x/y euclidean graph), 'all possible variations' of how to represent the changes made by each function step (as represented by a particular direction, distance, or value change on another embedded numerical attribute like primes/integers), some representations will be more useful than others (some changes made by the function may be better represented as distances between states rather than direction differences between states, where states are represented as points)
    - representing logical steps as functions connecting points in a sequence/network/tree in 3-d space is almost always incomplete, unless its a math function using only changes graphable in that space
    - alternate function representation options:
      - a set of 'representative example sequences' that accurately and completely represent the possible changes produced by the function, where further example sequences would be redundant
      - the 'reasons for changes created by a function', 'causal connections', and other interface structures are also not represented in this format, which is why multiple graphs would be frequently useful
        - a mapping function between related graphs (both adjacent and non-adjacent) would be possible to graph in between attribute graphs, especially with a 'representative subsets of input/output or state sequence examples'
      - standard math structures like a 'separating line of a set of points' or a 'sequence of a set of points reduced to another sequence of filtered points, in a logical sequence where the reduced set occurs later' or an 'identifying attribute added to the reduced set of filtered points' or a 'space where only filtered points follow a pattern (like where an even number filter has a clear pattern on an equidistant integer line)' can be easily mapped to useful structures like 'filters'
      - how much each step in the function reduces information is another useful format, for example in functions that compress a list into a single value, where a line/cluster representing the list can be compressed into one value (such as an average or an attribute value of the list, which might be a point on the line or near the line or a boolean value like 0 or 1
        - other workflow interaction functions than 'reduce' such as 'connect' or 'change' or 'filter' can be used as the primary structures to graph in a function, such as 'connections between change types', given the usefulness of these structures for these workflows, such as how 'changes applied to known structures' are useful for problem-solving intents like 'generating new variables' or other abstract intents like 'generating inputs to core interaction functions of workflows', so graphing 'known structures' can be a useful representation for identifying that useful structure 'changes applied to known structures'
      - 'differences from other functions' can also be represented in the sequence of functions representing the function
      - 'interface structures connected in the function' can be best represented on multiple graphs or a graph of interface structures, but could also be represented using default change type structures, for example graphing a variable with values from 0 to 1 as a probability distribution from 0 to 1, and connecting variables represented like this using a causal 3rd dimension where direction represents cause
      - 'functions connecting each variable pair' connected by a third 'causal dimension' in a 'sequence/network/tree' of attribute graphs is another format that could be useful
      - representing interface structures is possible on a 2-d graph, like by representing a 'set of handled input/output pairs' as an interface around which changes can be applied as 'points outside this set or outside of connections within this set likely to represent other valid input/output pairs', or a 'probable prediction function' as an interface around which 'alternate probable prediction functions would vary, with lower probability of accuracy than the interface prediction function'
  - how to identify that an 'equidistant integer line' would be a useful format to filter out odd numbers and leave only even numbers:
    - even numbers have an attribute (divisibility by 2) that can be relied on to calculate the adjacent numbers (apply a subtraction/addition of 2), when even numbers are formatted in a sequence
    - given this calculation (subtract/add 2), the constant value of '2' can be used to derive the attribute of 'equidistance' of adjacent numbers in the sequence
    - so a format that portrays adjacent items in the sequence as equidistant can be used to clearly filter out even numbers (any number that is more or less than exactly 2 from an even number is not an even number)
    - this format can be used in place of other ways to calculate even numbers, where a divisibility function is not available or where only info about adjacent numbers of 'a number to determine evenness of' is available
  - these are useful in that they make interface structures like 'patterns', 'differences', 'overlaps', 'intersections', and 'causes' clearly identifiable/differentiable from other interface structures like 'random errors'
    - any format that differentiates specific interface structures can also be used to connect them, so can be useful for a workflow that uses those structures
  - a 'set of coefficients/constants applied to a variable' might be a useful format for representing 'function steps' or 'differences between functions' if the positions of the coefficients/constants are standardized across functions, since coefficients/constants can be applied to different terms in a 'sequence of operations', as opposed to representing the function as a 'continuous line' or 'set of points', which leaves out the unique function logic (as many operations can produce the same function from different base functions), and as opposed to representing the function by its 'change pattern' (an abstract version of the function that can be more easily standardized) or a 'function area/set of equivalent alternate functions, equivalent in error sum/degree'
    - if a function can be standardized to one variable (like a variable representing an interface structure), this format could be useful to represent that function as a sequence of coefficients/constants representing operations on that variable, where the sequence can represent more powerful changes applied to the variable in reverse (less powerful changes first as closer to zero)
    - representing a function by changes applied in a sequence will likely involve multiple graphs unless the changes can be standardized to a subset of variables and the dimensions allow the variable interactions to match mapped math functions
  - superimposing other structures (like a 'language graph query' as a filter between connected sequential points) is an alternate format to represent more variable changes like 'finding similar words as those in the sentences in a configured list', such as representing sentences in the list as a sequence of 'language graph query shapes (paths on the language graph network)', which are represented as connecting structures of two points if the input point matches those 'language graph query paths', producing a set of lines that represent matching 'language graph query shapes' of the input point when connecting it to another point (like a point in the next state-representing graph in the sequence)
  - euclidean 2-d graphs are mostly useful when changes occur sequentially and produce unique changes (like in a line/curve, as opposed to producing circular trajectories which can overlap so much that they become useless, such as if a value remains the same after applying a change type such as a 'language graph query-similarity filter')
  - adding embedded dimensions like 'language graph query paths' can be useful as filters, and especially useful when a set of related items has a clear similarity (like if similar meanings have similar shapes or start/end points on the language network graph) and more useful when they are relevant to the host graph (when a structure like 'adjacence/position' of a 'language graph query' produces a change in that direction/magnitude/position in the host graph of the input)
  - 'inputs/outputs' of a set of language graph queries represented in this way might be less useful than the 'sequences of language graph queries themselves'
  - the value of 'possible changes' producible by a function is relevant as a possible useful structure to represent instead, like how a function can 'add a missing value in a sequence if the sequence is missing a value' and 'have no effect if the sequence is not missing a value' which are the 'change types' supported by that function

- identify useful structures like the 'net impact of input changes on output' of a useful structure like a 'required function' (in various specific structures relevant to that useful structure, such as 'input contexts' to create useful structures like 'predictable output changes') and the usefulness of these structures for intents like 'approximate a solution' or 'find a solution range' to fulfill problem-solving intents like 'find alternate solution structures adjacent to the solution'
  - example: to 'derive' a value (predict or approximate it using uncertainties applied to certainties) rather than 'calculate' it exactly, like solving the problem of 'deriving a number that is the product of two factors' as 'being between a known lower/upper limit of the value' ('limits' of the 'solution' value which can be applied as 'solution requirements'):
    - interface query to 'derive' a value (instead of calculating it exactly by multiplying the factors)
      - apply the 'change' interface ('change types') and the 'structure' ('input/output', 'similarity') interface to the 'function' interface (apply changes like 'adjacent changes' to inputs/outputs to identify 'similarities' between 'outputs' when some 'change types' are applied)
      - identify that the 'functions allowed/specified' in the 'problem statement' or to 'solve this problem' ('multiply') have:
        - a similar output of 'increasing the output value' with an 'increase in one input factor with the other input held constant' or an 'increase in both input factors'
        - a proportional output to the input change ('multiply' being directly related to 'ratios' which are the core structure of its 'opposite' function, 'division')
      - apply the 'meaning' interface to identify 'useful/relevant' structures of this function
        - identify that applying this function can therefore create an output of an 'approximate value', by 'applying a value near to the actual input factor'
          - apply the 'meaning' interface to identify 'useful/relevant' structures of this output
            - identify that this 'approximate' output is useful when some calculations are:
              - 'simpler than other calculations'
              - 'already executed and the inputs/outputs are stored'
              - 'when the actual calculation is not required to be accurate'
              - 'where a range of the solution is acceptable in place of an exact unique solution'
      - identify that the solution can be a 'range in between a set of minimum/maximum limits'
        - apply the 'multiply' function to two sets of factors, one lower than the original set and one higher than the original set (given that 'multiply' function has a 'change type' of 'proportional net sign-associated change to inputs'), these two sets being more calculatable than the original set
        - alternatively, apply a 'filter solution space' workflow to select the most useful/relevant factor sets (such as those nearest the original factor set, rather than any factor set above/below the original), then apply this 'multiply' function to get a 'solution' in the format of a 'value range'
    - the 'net impact of input changes on output' or 'signed field of input changes on output' structure is particularly useful for identifying when a 'change type' (position impact on output) changes into another 'change type' (negative impact on output), such as the points where 'pairs of factor changes' have a position impact on an output relative to a 'particular original factor set', so that structures like 'combinations' of 'change types' can be used to create other 'change types' (useful 'change combinations' or 'change sequences' like an 'increase in both factors will always increase the output of a multiply function' and an 'increase in one factor will always increase the output of a multiply function'), and used to derive other structures of change types ('an increase in one factor and a decrease in another factor will sometimes increase the output of a multiply function'), to identify useful structures like 'points where factor sets change from increasing to decreasing the output of the multiply function', given the usefulness of determining the thresholds where this variable changes, once:
      - the 'change structures' (like 'change sequences/combinations, such as the "changes applied by input changes on outputs"') of the relevant structures (the 'multiply' function) are identified
      - the variable of the 'impact of the (increase in one factor and the decrease in another factor) on the output of the multiply function' is identified
    - the specific example 'change types with variables yet to be resolved for the problem' (the variable of the 'impact of the (increase in one factor and the decrease in another factor) on the output of the multiply function' which is unresolved as its "sign change threshold, where factors' impact on output changes its sign" is unknown) of the structure "change structures like change types and change patterns and change thresholds applied by input changes on outputs" is a new useful structure, as these 'change types (impact of input change types on output) with unresolved variables (the sign change threshold)' are useful for intents like 'approximation', since 'adjacent input changes' create 'adjacent output changes' and 'input change sign change patterns' have 'calculatable sign change thresholds' (meaning to solve the original problem '2 x 5',  multiplying '3 x 5' will have a net impact on the output that is equivalent to '1 x 5' (zero sign impact, as they both change it by 5), and will have a greater impact on the output than multiplying '3 x 4' (positive sign impact, as it changes the original value by more than multiplying '3 x 4'))
      - where this change in impact occurs between factor sets is the 'sign change threshold that is useful for determining positive/negative impact and direction to apply changes in to get farther/closer to a particular value, such as the original factors to multiply'
    - finding the specific useful structure ('sign change threshold') of the problem space that is associated with this useful structure ('net impact of input changes on output') applies the workflow:
      - 'find change structures of functions' ('impact of input changes on output')
        - 'find similarities in change structures of functions' ('find similarities in "impact of input changes on output"')
        - 'find variables in change structures of functions' ('find input changes that vary in the sign of the output')
          - 'find change types with unresolved variables' (the 'sign change threshold' variable of the change structure of 'multiply input changes')
            - 'find variable values' ('find sign change threshold value relevant to problem, like with "adjacent factor sets as the original factor set"')
              - the threshold occurs when factor sets' impact on output are equal, meaning:
                - 3 x 4 and 2 x 4 have an equal impact on output compared to an original factor set of 2 x 5 bc they are equivalent change types applied to the original factor set (decrease one factor by one to lower the output, and 'increase the lower factor' and 'decrease the larger factor' by one to 'increase the output', given that these changes wouldnt produce an equivalent output as the original factor set, meaning '3 x 4' is still higher than '3 x 3.33' or '2.5 x 4', which would produce the same value as the original factor set, therefore since '3 x 4' is above both of these 'equivalent alternatives' and given that 'multiply' has a 'proportional effect of input changes on output', '3 x 4' is guaranteed to be higher than '2 x 5', so its guaranteed to have a 'positive sign change' as in it will 'increase the output')
                  - identifying these 'equivalent alternates' of the 'factor sets', having one factor in common (3 or 4) with a factor set (3 x 4) to determine the sign change impact of, is useful when the 'impact of the function' (proportional effect) is known, as it can be used to identify when a factor set (3 x 4) will result in a 'positive/neutral/negative sign change'
                  - anything above 3 x 4 would just get more inaccurate compared to 2 x 5, increasing it too much
              - knowing the specific input factor sets that create a 'sign change threshold' can be useful to solving the specific 'multiply factors' problem, if this interface query is wrapped in an 'approximate solution' or 'find solution range' intent
              - applying these interface structures (calculating 'equivalent alternates' of 'input changes') can be used to determine useful structures like 'sign change thresholds'
              - 'sign change thresholds' are useful for identifying 'input points or input change points, beyond which inaccuracy increases' as useful for 'identifying solution ranges'
    - 'functions of a problem statement' or 'required functions of a solution' are useful when used with 'input changes' bc identifying structures of 'input changes' ('impact of input changes on outputs') is useful for determining what direction changes should be applied to structures relevant to a function ('inputs') to get other relevant function structures ('outputs') that are 'farther/closer to a point (like a solution point)'
    - the '"change type" change limit' (as in "a limit on changes of a certain type, like 'signed changes'") or the 'net impact of input changes on output' are 'equivalent alternate' structures to derive the 'sign change threshold' as particularly useful in solving this problem when solving it with a 'approximation' or 'find solution range' intent
    - these 'equivalent alternate' change structures of the 'change' interface structures can be found just like the 'equivalent alternate' changes in 'factor sets' of the 'multiply' function can be found
      - 'change type' change limits are similarly useful as 'impact of input changes on output' for similar intents, beyond which other changes applied to these structures in various positions/directions will be less useful for those intents
      - the 'connection' between these 'equivalent alternates' act like 'changes applied to an interface' (an interface of 'probable/expected changes'), given the 'similarity' of "change type" and 'input changes' (which are a specific change type) and given the 'similarity' in usefulness of 'change limits' and 'input change impact on output' (changes which have limits in their impact) around which changes develop on the foundation of that interface structure ('expected outputs') while fulfilling its original intent ('find/approximate expected outputs without directly calculating them'), some changes making these structures more useful for various intents that dont contradict the original intent of the 'expected outputs' structures ('find/approximate expected outputs')
      - these structures are 'equivalent alternates' bc knowing the 'impact of all input changes' can be used for similar intents as knowing the "threshold of changes in input changes' impact on output", one being more efficient than the other for various purposes, given functions like 'retrieve stored value' or 'check if value is above/below threshold'
      - 'useful "change type" change limits' (like 'adjacent sign change thresholds') are another change applied to this change structure that make it more useful for intents like 'approximate solution'
    - 'change types (like input changes)' and related useful structures (like 'impact of change types like input changes') are useful for intents that involve 'changes', such as 'finding an alternate solution structure' (a solution structure with 'changes' applied to it), which is another way to derive these structures ('impact of input changes on output') as particularly useful
      - 'find structures that align across intents of different but related interaction levels (like aligning structures/functions across the "problem/solution" interaction level and the "change structure" interaction level' is a way to derive this 'alternate interface structure set' ('change types' and 'related structures of change types') that can derive the useful structures ('impact of input changes on output'), as 'changes' are inherently related to the 'change solution to find alternate solution structures' workflow
        - the 'impact of input changes on output' and the 'change solution, to identify alternate solution structures that can act like the solution' both have the function/structure 'change' in common (these structures align on the 'similarity' of their 'function' structures, despite being on different interaction levels (the problem/solution interaction level and the change interaction level)
        - using 'interaction level' here instead of 'interface' bc its a useful structure to depict 'multiple function levels' in a way that shows 'composing functions/intents (like change functions)' of another function like a 'higher-level function (like the problem-solving function)'
    - related workflow: so applying changes to structures of the workflow or its sub-intents (like its core interaction function, such as 'change' or 'connect') is a way to generate probably useful structures for that intent (in this case, with the 'change solution to find alternate solution structures' workflow, this would generate the 'change' interface, and all of its interface structures like change patterns/types)

- identify interface structures describing problem-solving structures like 'problem-solving state sequences' in solving a specific problem, then identify the interface queries that could generate those interface structures or identify the 'differentiating factors' to filter out interface queries that could not adjacently generate those structures
  - example: the 'connections' between related numbers like pi/i/e can be described by interface structures like 'cross-interface alignments' (a 'combination' of an 'alignment' of 'multiple wave functions'), a structure which can be identified by structures like 'common' interface structures between the numbers (such as 'change types' like 'relative changes', 'adjacent changes', 'aligning changes', 'change rates')
    - this useful interface structure solves the specific problem of 'finding ways to generate pi'
    - identifying this useful interface structure while solving this specific problem is trivial once the structure of 'multiple waves' or 'multiple sequences' is found/generated/derived
  - the 'filter' here is 'find interface structures in "common" between different structures like "different change types"'
  - identifying this 'filter' of useful interface structures from the original useful interface structure ('cross-interface alignments') is trivial by finding structures that can fulfill the intent of 'generating the useful interface structure used to solve the specific problem'
    - 'create a "cross-interface alignment" using connection structures (like equal/similar structures in common among different structures)'
  - this applies the workflow:
    - 'solve the specific problem'
      - 'identify useful interface structures of the problem-solving process'
        - 'identify input interface structures that can build the useful interface structures'
          - 'apply these input interface structures to filter useful interface structures' (find structures in common across change types (aligning valleys/peaks/inflection points in wave functions), to find cross-interface alignments (aligning wave patterns combining to useful components of a sequence to generate pi) which can be used to combine change types (sequences) to create another related change type (sequence converging to pi))
            - 'input interface structures of useful interface structures' are the useful 'differentiating variable' that can fulfill problem-solving intents like 'find useful structures' and 'filter useful interface queries', if the inputs can be found for a particular problem, these 'differentiating variables' being useful to 'filter' other structures

- identify useful structures which are useful for fulfilling requirements (like 'preserving info') of problem-solving intents
  - example: structures which preserve info when applied in either direction are useful for problem-solving intents like 'convert to a format where the problem is more solvable, then convert back once solved in that format', since the solution in the other format can be converted back into the original format, therefore these structures are candidates for fulfilling this problem-solving intent

- identify useful structures like 'variable-crossing' structures as a way to fulfill common, useful, & problem-solving intents like 'convert between useful structures' (such as 'common' structures like 'variable sets')
  - example: it is already established that 'cross-interface' structures are useful (to create other useful structures like 'conversion', 'alignment across interfaces')
    - a 'cross-variable' structure (like '4') commonly found in formulas converting between relevant attributes (like 'area' and 'surface area') of 'variable sets' of unit structures ('circle', 'sphere') is useful in converting between these 'variable sets' as 'change structures', 'converting between variable sets' being a useful intent for problem-solving intents like 'convert to a format where the problem is more solvable, then convert back once solved in that format'
    - this structure can convert an attribute of a circle into a relevant (as in 'corresponding', as in 'occupying the same function/position/sub-structure as the other structure in a different structure') attribute of a 'higher-dimensional variant of a circle', and 'converting to a higher or lower dimensional space' is a common and useful intent in various problem formats like the 'find a prediction function' problem format
    - a 'cross-variable' structure doesnt necessarily mean 'increasing the "count" attribute of a structure (variable set)', but may involve another change type of the 'variable set' structure, as its just a way to connect different variable sets which are related through their corresponding structures, indicating a 'threshold' across which one 'variable set' becomes another indicating a 'phase shift' (such as an 'identity' change of the structure, as a sphere is differentiable from a circle, requiring its own definition, while being dependent on the circle, this 'dependence' indicating the 'relevance' of these structures, in addition to their 'corresponding' structures like radii/pi/area)
      - this 'cross-variable' structure of the '4' multiplier answers the question 'at what threshold point (in a multiplier) does two dimensions correspond to three' for the 'circle area to sphere surface area' example

- identify useful structures like 'connection' structures of structures that are commonly required to connect (like incomplete/complete structures) in various problem-solving intents like 'derive missing information'
  - example: a way to identify a 'complete' structure from an 'incomplete' structure is to identify the 'limits on core change types' of the incomplete structure (four cardinal direction points of perimeter of a circle) and find an alternate way to connect those 'limits on core change types' of the incomplete structure (such as a square, connecting the four points with right angles in the midpoints between these four endpoints)
    - generating a 'square' and a 'circle' is trivial by combining core structures (like 'multiply', 'constant', 'angle', 'unit'), and finding the formula to connect their definitive attributes (like area or side length) is useful since circles are useful as randomness-generating structures in their relevance to pi (an extremely random number)

- identify useful structures by which structures are useful for usefully (like 'adjacently') fulfilling core functions (like 'connect') of other useful structures (like 'common/core structures')
  - example: identify that e is useful by its adjacent connectivity when applied to pi and i, which are core structures

- identify useful structures (like 'trivial differences that are relevant') that are inputs to identifying other useful structures (like error types, such as 'trivial errors which are incorrectly ignored') to fulfill problem-solving intents like 'identify error types & apply differences to them to generate solutions'
  - example: similar to how a neural network has a possible error type of 'over-prioritizing the more common or more represented class in a data set' bc of the 'structural similarity' between the 'under-represented or less frequent class' and 'random noise' or 'trivial errors which can be ignored', given that 'trivial differences' can be under-identified as relevant in a neural network bc of its requirement to generalize across multiple data set input points, rather than designing a function specifically adhering to every input example data point
  - identifying that 'trivial differences' are ignored/discarded by a neural network involves applying interface structures like 'unit' or 'minimum' to other interface structures like 'difference' and checking for data that can produce an error such as 'ignoring a trivial difference that is actually relevant (has a reason to be included, such as an outlier that indicates a pattern change)'

- identify the 'causes' of usefulness of a structure (what problem does it solve, what does it reduce), which align with the 'usage intents' of that structure (the 'reasons to use it'), to fulfill problem-solving intents like 'identify/filter useful structures'
  - example: some structures are more useful than others, like how 'filters of relevant info' are more useful bc they reduce the 'amount of info to focus on (process) to solve a problem'
  - this expands the definition of useful structures to include other specific useful structures that are generally useful
  - 'specific structures which apply across systems in general' are useful structures for adding focus (from certain structures) or reducing processing requirements to problem-solving processes
  - 'adding focus' and 'reducing processing requirements' are 'aligning intents' which can be used as 'interchangeable alternates'
  - 'inputs to certainty structures' such as 'tests' (which identify the certainty of a structure like an equivalence) are similarly useful, in that they can generate useful structures like certainty structures

- apply workflows until a useful workflow-filtering structure like 'minimum info required to check if it has at least one successful example' is reached, as a way of fulfilling the problem-solving intent 'select useful solution automation workflows to solve a problem'
  - example: finding out whether a workflow or solution-finding method has at least one successful example is trivial, and finding out which workflows can reach this example is trivial in the set of core workflows, and once a succesful example is achieved, changes can be applied to the original core workflow to improve the next application of the workflow variant
  - this applied 'trial and error' to useful structures like 'workflows', with the difference that each item in the iterated set is itself iteratively tested until it fails to a certain degree ('recursive trial and error')

- apply interface structures to identify 'new variables' to fulfill intents like 'identify new changes to problem/solution structures' which are useful for intents like 'apply changes to existing solutions until its optimized'
  - example: to identify a new variable of a structure like a 'song' with core structures like 'notes', apply interface structures (like 'subset') to other interface structures (like 'time' in the 'physical information' interface of the 'information' interface), to generate 'intervals of notes'
    - this generated variable of an 'interval' is a variable which can be varied in its structural metadata, such as the 'length of the interval', given that a variable is an 'interface unit' structure (supports variation in a range that does not break the structure like the definition of the variable), so varying the 'length' of the interval (or other metadata like the 'position' of the interval) doesnt break the 'interval' definition

- apply interface structures to filter structures as being more probably true or false to fulfill the problem-solving intent 'find certainty/probability structures'
  - example: if there is no 'reason to support a theory', it doesnt have to automatically be considered true and is a candidate for a falsehood, like when someone's actions dont match their words, as 'words' are the far more easily controlled variable than 'actions', so 'words' are more easily faked than 'actions' and can therefore be subjected to more scrutiny (as opposed to trust)
    - the 'reason compounding the certainty of that possibility' is the interface structure associated with the insight 'more easily controlled variables (by agents with incentives to lie) being more subject to noise and other forms of falsehood' to generate other structures like 'a set of falsehoods where the truths are so rare they might as well be a random error' to identify a data set as part of a 'liar' or 'error/falsehood' cluster
    - deriving the 'reason compounding a certainty (the reason a statement could be true)' from the insight 'more easily controlled variables (by agents with incentives to lie) are likelier to contain falsehoods (where falsehoods are more efficient and accomplish the same or a similar goal)':
      - if a statement is true, it not only requires that it 'lacks relevant contradictions', but also requires an 'incentive to tell the truth' ('to share or communicate new info', 'to solve a specific problem', 'to contradict a falsehood', 'to draw attention to the truth', 'to make the truth useful for other people', 'to help others solve problems'), which is an 'opposite' structure of a structure in the insight ('incentives to lie'), if there are incentives to lie, otherwise there is no reason to say it
    - deriving the insight 'more easily controlled variables are more subject to falsehoods'
      - apply 'probability' to the 'requirement' concept applied to the 'input' structure ('variables', 'intents') of the 'function' structure
        - an agent that uses a function with total control is less likely to need that function, and can survive while faking that function

- apply interface structures to problems to generate new solution automation workflows that involve a reduction of some solution metric (problem variable set, distance from solution, number of un/solved sub-problems as a ratio of total sub-problems, etc)
  - all of the solution automation workflows in problem_solution_diagrams.svg involve changes to the variables of problem/solution structures
    - change problem until its a solution (changes the problem state until its useful in some way)
    - solve different problem (change problem identity, like to the 'causal problem')
      - find a way to connect all problems (using structures like metadata) and solve a problem that is related but more useful or more solvable than the original problem
    - break problem into sub-problems (change problem scope/complexity)
    - connect problem/solution (change problem/solution position)
    - reduce problem (change problem variable set to a subset)
    - filter solutions (change problem space possibilities to a subset)
    - change existing solution until its more optimal (change suboptimal solution)
    - change problem format, solve it in the different format, and change the format back once solved
    - change problem boundary/structure to a more solvable problem
    - solve for generally useful functions for general solution metrics and apply them to solve problems
      - find alternate interchangeable optimization points of solution metrics and solve for the most computable optimization point
    - organize problem/solution structures (like sub-problems) until the original problem is trivial to solve (such as where solving one problem is useful as an input to solving the next problem, once organized such as being 'sorted' in the right 'sequence' structure)
  - the interface structures (changes) applied to the variables of problem/solution structures need to fulfill definitions ('make sense, having no relevant contradictions')
    - for example: 'changing a suboptimal solution' is only useful when the change fulfills solution metrics more optimally than the previous state
    - so the 'change' applied would only qualify as an implementation of a solution automation workflow if it 'improves the suboptimal solution' (fulfills the definition of 'solving a problem')
  - sub-intents of each workflow, like 'find input info of the workflow', are components of the workflow implementation (interface query)
    - workflow: connect problem/solution (change problem/solution position)
      - sub-intents (alternative, overlapping, sequential sub-problems)
        - 'find input info of the workflow' sub-intent: find connective/interactive structures to use in connections
        - 'find structures to input to the connect function' sub-intent: find structures required to be connected
    - various workflows can be used to implement these sub-intents:
      - 'find input info of the workflow' sub-intent: find connective/interactive structures to use in connections
        - 'derive the structures that are adjacent inputs to the solution by applying solution requirements like solution metrics'
          - this implements the workflow 'derive structures in between problem/solution using input-output sequence'
      - 'find structures to input to the connect function' sub-intent: find structures required to be connected
        - this implements the workflow 'apply requirements to derive certain structures that can be connected with uncertainties (variables)', where the uncertainties here are the 'structures or methods to connect those structures which need to be connected'
        - the output of the previous sub-intent can be used as input to this workflow
        - the 'adjacent inputs of the solution (derived from solution requirements)' can be input to the 'connect' function
          - meaning the 'problem structures' should be connected to the 'solution-adjacent inputs'
  - these sub-intents of each workflow are each problem-solving intents, bc they all add useful information to use a solution automation workflow, even if the current workflow cant be completed (just the sub-intent is completed)

- identify useful structures like 'alternate structures' that can be combined to create solution automation workflows which are not default components of solution automation workflows (solutions, problem-solving intents, etc)
  - example: the 'iteration' structure of a 'for loop' is the corresponding structure of 'try every possible solution, score each one, and compare scores' or 'try every possible solution & score each one, until one score is above the solution score threshold', as 'for loops' are often used to 'build a reduced list out of another list'
  - similarly, the 'function parameters' (inputs, values being changed) & 'function return values or value changes' (outputs) are corresponding structures of the 'find input-output sequences that connect problem/solution' workflow
  - the 'filter the solution space' workflow also corresponds to the 'iteration to reduce a set' structure represented by a for loop with a filter applied to a list
  - given that core logic structures are corresponding structures of core workflows, combining core logic structures as components of other workflows is a way to generate workflows

- identify useful structures (like 'inputs') of solution automation workflows and identify methods to convert structures into those requirements, to select which workflow to apply when one is more optimal than other
  - example: the 'break problem into sub-problems' workflow has a requirement of 'isolatable change types', given the structural impact of the workflow which is 'isolating change types'
    - these change types can be sub-problems (reductions of the problem), which can take the form of:
      - 'solving the original problem, for a subset of the original problem variables'
      - 'solving the original problem, broken into a sequence/tree/network of sub-problems'
      - 'solving the problems causing the original problem which are "components" of the problem in that they are "inputs" of the problem'
    - similarly, the 'filter a solution space' workflow has a requirement of 'existing solutions to try' or 'existing possible combinations/changes (in the problem space) to try (which may or may not be solutions but are possibilities)'

- identify useful structures which store info such as 'representative data (data set shape) of other info (data set)', where the stored info can be used as a proxy or substitute for the other info to fulfill problem-solving intents like 'reduce required computations'
  - example: knowing the 'general shape' of the data set is useful information to assess whether a directional change or parameter of a prediction function is likely to produce a correct prediction function, and may be more useful in some cases than the actual data point, as the 'general change types like change patterns and change rates and variable interactions' are included in the 'general shape' of the data set but are not included in individual data points, which are used as the input to most 'find a prediction function' solution-finding methods
    - the 'representative data' is also useful for other intents like 'generalizing a function', as 'specific data points' are likelier to be corrupted, mistaken, or influenced by other irrelevant factors than the 'general shape of the data set'

- identify useful structures (like 'additional inputs', as in 'additional input data derived/inferred/imputed' or 'probable additional input data') for specific intents in a problem space like 'find a prediction function' that could fulfill solution metrics like 'accuracy'
  - example: if predicting whether a particularly high output (target y-value) exists (or is likely to exist) in a data set (for intents like 'augmenting data' and 'inferring new data' and 'imputing data (filling in missing data)')
    - first find the x-range of lowest/highest x-values, then select random subsets of the data set and find a function that describes each random subset
    - then find the maximum y-value of this function over the x-range, using a method such as 'find out if there are adjacent points to the target y-value in a pattern that would make it possible for the y-value to exist in the function (like "are there two points surrounding the peak of a curve, where the peak is the y-value" or "are there two points surrounding the y-value in opposite directions, at a change rate that is plausible for the function as it exists elsewhere in the function")' or "is it adjacent to known points in the function within an accuracy range"
    - apply this maximum y-value of the random subset function to check if the particularly high target y-value could be output by that function (do any known approximations/representations of the data set - or metadata of these approximations/representations like the 'maximum y-value of the approximation' - indicate this output is possible/probable)
    - repeat the steps starting from the 'selection of random points' until there are enough statistically significant functions that could output the y-value, up to a number of repetitions that is fewer steps than applying a standard method like a regression method
    - alternatively apply a filter like 'random points in a set of sub-ranges of the x-range' to evenly distribute the random points selected to make the random selections more meaningful in similarity to the actual probable prediction function
    - this applies info about the possible y-variable value, compares it to corresponding info about the representative data (maximum y-values of random subset functions) and filters out possible y-values based on a threshold selected to be a similarity identifier (the 'accuracy range' allowing a degree of adjacent points to be included in a function)
    - this method looks for a 'corresponding structure in a known structure' (maximum y-value of known representations) to compare a 'possible structure' (possible high target y-value) with
    - similarly, when applying a standard regression method, looking for a 'corresponding structure' ('most adjacent point') in a known structure ('function implied by calculations already done') to compare a 'possible structure' ('possible next or other y-value') with can fulfill intents like 'reduce computations required' (as given the lack of volatility in most functions, points can be connected by curves that match the change rates shown in the rest of the function, so given a 'most adjacent point of a probably correct function', the relevance in the form of the accuracy of a 'possible next point' can be assessed except where the change types like change rates of the function experience a phase shift or other change requiring that some other points' averages be computed as well (instead of applying inference rules to identify other points))
      - similarly, a 'random next point' can be chosen and corrected once compared with change types/rates of the probably correct function, which is likely to reduce computations as well
    - finding other data to include in a data set can increase the accuracy of a solution-finding method for the 'find a prediction function' problem, so is generally useful for fulfilling problem-solving intents like 'improve base solutions on some solution metric'

- identify useful structures like 'overlapping structures in common' between useful structures like 'stored structures' which can be applied as inputs to functions like 'merge' to generate new possible useful structures
  - example: a 'generative filter' (such as an iteration of a set of possible solutions which applies 'differences' to filtered out solutions to generate more possible solutions to apply as future possible solutions in the iteration of possible solutions) can be derived by noting that 'apply differences to error structures to generate possible solutions' and 'filter possible solutions' have structures in common, that being 'errors in the set of possible solutions which are filtered out as not solutions' and 'known error structures as input to a change function to generate possible solutions', therefore identifying that 'generating possible new solutions from solutions determined to be not solutions in the set of possible solutions' is a useful structure is a matter of identifying this structure in common and the possible usefulness of this new structure (identifying new possible solutions not already in the set of possible solutions by applying differences to the possible solutions once theyre identified as errors), which makes use of the overlapping common structure to integrate/merge these structures to generate a possible new useful structure, which is useful in cases when the set of possible solutions is unlikely to be complete, such as when a function is applied to generate structures which are different from optimal solutions, such as 'maximally different possible solutions' or 'base solutions' as the set of possible solutions

- identify useful structures by generating a set of possible useful structures (like "possible useful functions with an 'intent' but no logic built yet") and filtering them by metrics of usefulness like 'common components', 'alternate intents theyre useful for', 'what problem-solving intents they would fulfill or functions they would reduce requirements/computation for'
  - example: making a list of useful functions to build to fulfill intent likes 'finding errors' and 'fixing errors' (a function to 'identify useful structures like "root causes" or "common inputs" in error information', a function to 'identify solutions already tried which failed in error information', a function to 'identify possible/probable errors before they happen') would have the output of useful functions like a function 'that generates errors in a codebase/function', as this generative function can generate the inputs/outputs (like 'error information' and 'causes of errors') to other useful functions in the list and can fulfill the primary intents like 'finding/fixing errors' with just one function rather than several, or a function set that can be combined to generate the functions in the list (like 'get definition of solution/error' and 'check if a structure matches the definition like a solution/error'), which are useful bc they reduce the number of functions required, can generate other useful functions, can generate useful information for other functions like 'cause' info, fulfill multiple intents, and/or can be combined/chained to fulfill other intents
  - 'identifying a list of useful functions to build' can be a simple matter of 'identifying common work done which is not implemented in a function yet' (like 'tasks which are done manually'), and filtering this list can be as simple as 'identifying which functions have overlapping functionality, which functions have common component functions which might be more useful to build, which functions enable other functions, which functions fulfill useful metrics like "reduction of requirements of other functions"', rules which can be applied structurally to filter the set of possible useful functions to build
  - a 'useful structure-identification function' is an adjacent alternative to a 'solution-finding method', as the only structure required to convert it into a 'solution-finding method' would be applying adjacent changes to combine & otherwise connect the useful structures found until they fulfill solution requirements
  - this workflow involves a method of identifying 'useful filters of useful structures' (variables which can separate the set of possible useful structures) and 'generative methods of useful structures' (such as by applying workflows like 'derive solutions from solution requirements'), and applying these structures as opposing structures (like 'functionality' and 'requirements' are opposing structures) when generating and filtering possible structures like solutions to the problem of 'identify useful structures to apply in a specific problem or across problems'

- identify structures which are alternates to other structures that fulfill useful intents like 'reducing storage requirements' or 'reusing existing structures' which are generally useful for problem-solving intents like 'reduce resources required to solve a problem'
  - example: 
    - 'specific information' (like 'examples') can be useful for intents like 'function implementations' (like storing input/output maps if theyre relatively static, or applying changes to these input/output maps based on input similarity, instead of implementing specific function logic) and 'filtering out reasons to use abstract variants like types' which are useful for other intents like 'identifying which structures to store (known constants) and which to generate dynamically (uncertainties/variables)'
    - 'reasons to use a structure (such as the required or probable outputs of using the structure) like a "specific" or "abstract" structure' can also identify when to use a specific structure vs. an abstract structure 
  - these structures ('reasons to use a structure', 'expected outputs of the structure', 'known intents fulfilled by a structure') are alternate structures which can be useful in intents like 'filtering what structures to use'
    - storing the 'reasons to use a structure' vs. storing the 'expected outputs of the structure' (a specific example of the 'reasons to use a structure') vs. storing 'known intents fulfilled by the structure' can provide optimization opportunities, if some of these structures are used more frequently across interface queries, functions, or solution automation workflows
    - other structures with similar functionality as these structures include:
      - 'requirements including the structure or its outputs'
      - 'known contexts/systems where the structure is useful or required'
      - 'structures that are useful for determining the probable output of a structure (like a function), when applied in a system where it hasnt been tested' (such as by finding any structures in the system that could neutralize or alter the steps or requirements of the function, which are capable of interacting with the function or its requirements in that way)
  - other relevant structures, like 'phase shifts where one structure stops being more useful than an alternate structure', are similarly useful for this workflow

- identify interface structures based on 'interface structures (like change types, cause, etc) of interface structures', to fulfill useful intents like 'find interface structures that are probably found adjacent to each other' to fulfill intents like 'connect interface structures'
  - example: the structures nearest to an interface are likely to be composed of 'core structures' of that interface, are likely to be in the shape of 'overlapping pyramids' (describing multiple 'combinations of core structures' (bases) with different outputs (peaks)) than other shapes like a 'set of circles returning to the interface' or a 'tree with outputs pointing at the interface', where 'similar changes across these pyramids' are likely to represent 'phase shifts' and 'interaction levels' and 'system layers (of a system layer diagram)', and the changes farthest from the interface are likely to be 'outputs' of the other changes rather than 'adjacent changes resulting from core combinations of core structures', given how changes on an interface usually interact, such as in 'fractal patterns'
  - 'finding connected interface structures' is useful for many problem-solving intents, like 'find alternate interface structures (like alternate "definitions")', 'find cause of interface structures (like changes)', etc

- identify useful structures that fulfill required intents like 'preserving an interface' which is required for a system to exist, interfaces being a core structure of systems, representing stable states of the system
  - example: a 'limiting structure' and a 'changing structure' are required 'opposite' structures, which are required to fulfill required 'preservation' intents like 'make sure change doesnt get out of control' in a system which would destroy the system, which is necessary if the system is to exist or be useful, and necessary while the system is still more useful/beneficial than costly
    - similarly, a 'change that is possible on an interface' (like a 'base line') is a 'change that doesnt destroy or destabilize the interface' (like a 'smaller line than the base line, applied in a direction other than that of the base line')

- identify useful structures determining relevance, such as 'similar differences that make variables relevant' to supplement, replace, or implement known useful structures like 'interaction levels' to fulfill problem-solving intents like 'find relevant variables'
  - example: a predator's 'claws' and 'teeth' might be the most relevant variables to its prey, unless it also has a tail which can attack and reach the prey, which is relevant to the other variables by a 'similarity' in the 'possible values' of the 'position' variable, answering the question, 'which variables are relevant to the prey, when the prey is in position x' to identify 'variables that can have a "similarity" in position', thereby making these variables possibly relevant to each other ('teeth' and 'claws' and 'tail' could be relevant to each other by being 'interchangeable alternates' for the 'attack' function), so the question 'which variables are relevant to the prey, when the prey is in position x' can be converted to 'what variables can occupy the same position as each other, and the same position as the prey'
    - similarly, 'tools which can be used by some variable to "extend their range"' would also be relevant variables, as they change a 'previously irrelevant' variable for a position into a 'relevant' variable for that position by increasing the 'positions the variable can be relevant to', which is a target structure for identifying relevance of a variable
      - this can be used to derive relevant variables, by identifying 'variables which can make another variable relevant' (what types of change would interact to fulfill intents like 'extending distance reachable' which are relevant intents to variables like 'position' which are relevant for being under the control of the prey) and identifying 'whether those variables exist'
    - 'variables' can be determined to be relevant by a 'similarity' like this, which makes them 'interactive' (interactions such as 'coordinating', 'neutralizing', or 'overlapping') in some way, so that they can be 'combined', which is a core interaction function of 'variables', and this means that variables can be derived by applying 'similar differences' which allow finding other variables which are different enough to be classified as an isolatable variable but similar enough (in a similarity like 'possible positions') to be interactive (on the same 'interaction level' as another variable)
    - this is similar to how finding an orthogonal dimension can be found by applying a 'similar difference' to another dimension ('rotate the x axis 90 degrees to find the orthogonal dimension y'), which is not so different that it is equal to the original dimension (as 'rotate 360 degrees' would accomplish) or 'irrelevantly different' (as 'rotating 180 degrees' would accomplish) both of which would essentially generate the same line as the original x axis, and not 'irrelevantly different' in that the rotated line has an overlap in the 'change type' it describes, in describing horizontal as well as vertical change, and applies the x axis as a base for this change, which can be used to derive the new axis by applying core change types, or applying intents like 'find the changes required to create a useful difference, like describing different differences (vertical as opposed to horizontal) by identifying the changes described by the original structure (horizontal) and identifying changes not described by that structure and identifying a base (y axis) that could describe it', creating a useful difference in a line rotated to 90 degrees which can measure an 'isolatable (different change type) but interactive (similar interaction level)' change type (these changes are not so different that they cant interact, but are not so similar as to be interchangeable)

- identify structures of relevance to find relevant structures for functions commonly used in solution automation workflows like primary functions like find/build/derive/apply to reduce the computations run by these functions, and therefore reduce steps to implement workflows
  - example: the 'apply' function applies changes in the form of a structure to another structure, in which the 'structure being applied' has to match the inputs of the 'structure its applied to', otherwise the structure being applied is not relevant to the structure being applied to, for example, applying a 'collision function' to 'blocks' is only relevant if there are 'multiple structures like "blocks" which can be collided' and if the 'blocks dont have an overriding function which prevents collisions like magnetic forces', meaning 'does the collision function have its inputs (blocks to collide, no collision-prevention structures) fulfilled'
    - so checking for '"inputs" which can make a structure useful to apply' is a filter of 'structures which can be applied to other structures'
    - similarly, with the 'find' function, only 'structures which can be found (structures which are measurable, structural, stable, stored, unique/determinable/identifiable compared to other structures)' can be found, so only these structures are relevant to the 'find' function
    - similarly, 'structures that can be connected' are relevant to the 'derive' function, and 'structures which can be combined' are relevant to the build function (as well as any structures which can make other structures findable, derivable, buildable, and applicable, making these structures relevant to these functions)
  - this workflow applies a definition of relevance to find structures that are useful (in the sense of being usable and therefore relevant) to the primary interaction functions, to filter out structures which are irrelevant when using these functions
  - this means 'structures which are findable' are a reduction of the solution space when applying the 'find' function
  - similarly, 'reduction structures' (such as 'any attribute which can differentiate a subset from another subset' applied to the 'solution space' of 'possible solutions'), are another useful structure to the 'find' function, given its definition
    - these reduction structures can be particularly useful when designing a filter sequence/tree/network that can optimize a query based on relevant 'find' intents
    - for example, identifying 'differentiating attributes (variables) of a solution space' can identify which 'differentiating attributes' should be applied as filters to 'maximize coverage of the solution space', to 'filter a subset of the solution space', to 'increase the chance of finding multiple alternative solutions or a unique solution', and other intents related to 'find', like 'filter', 'identify', 'differentiate', etc
  - identifying 'variables that can be controlled' is another example of variables which can be filtered (focused on) to identifying 'possible variables of a solution', as 'variables which can be controlled' are irrelevant

- identify useful structures like 'sufficient similarity' to find structures which can be used to fulfill common intents like 'find alternates of a structure' which are useful for problem-solving intents like 'find alternate solutions'
  - example: applying code validation rules like the following can be used to correct logic, given the general or common applicability of such rules
    - 'if an input variable is tested for and not found to be initialized, and is assigned a value if this condition applies, apply the output of that assignment as a filter of the input value, if the input is initialized'
    - example of this code validation rule:
      - if the 'interfaces' input variable has a value, apply the primary interfaces found with the find('list', 'interface') function call which returns all defined interfaces, as a filter of valid values of the interfaces input variable
  - this code validation rule is not always true, but is generally or commonly useful
  - finding 'invariant structures' such as 'rules which are always true, in all possible cases or case types' which can be used to derive definitions, as structures of certainty such as structures which dont change
    - similarly 'rules which are generally or commonly true' can be used to find 'probable structures of certainty'
  - given that 'structures which dont change' can be used to find structures like 'definitions' given their common relation to 'certainty' structures, structures which are sufficiently similar can be used to identify/derive/substitute each other

- identify the structures that can fix the causes of a problem, applying insights about problem causes such as that 'problems or problem causes are usually useful in some way to some entity if they exist enough to be noticed, or they wouldnt exist'
  - example: if a problem exists, its either beneficial to some entity with the ability to use the problem to its advantage or create the problem, or its not a big enough problem for enough entities to trigger the structures (like "priority increase to highest priority") that would be necessary to fix it
  - so finding structures like 'functions that find alternative ways to benefit those entities than the problem' would be useful in addressing the 'causes' of the problem (the 'benefit to some entity', the 'problematic attributes of that entity like "parasitic/predatorial strategies to benefit from causing problems"', or the 'lack of solution-triggering structures')
    - the root causes of these causes includes:
      - 'inability of all entities to understand & prioritize meaning (meaning in the form of the impact on the host, as if a pathogen kills the host, its also hurting itself, but it cant detect that bc it cant grasp context, time, emergent conclusion like "host death" of structures such as "parasitic patterns applied at scale which dont encounter reducing/opposing structures like limits", and other structures that could make it aware of the impact of its structures on other structures)'
      - such a solution would apply the 'distribute solution-finding methods and priorities to all structures, such as functions/objects (like pathogens relying on a host to survive), rather than limiting problem-solving capacity to some structures (like immune cells)', which would possibly take a form such as 'distributing host immune markers to all cells and creating costs to all cells based on those markers, so the pathogens get negative feedback for parasitic strategies, rather than short-term positive feedback leading them to repeat that strategy until the host dies' or 'creating short-term feedback that represents long-term feedback for pathogens or mutations capable of killing the host, if they can only use short-term feedback and dont have the resources like "storage" or functions like "computation" to make long-term feedback useful', or 'find all specific strategies (or strategy inputs/states/components) used by cells that eventually lead to host death and prevent cells from using them'
  - similarly the problem causes may be useful bc they cause the problem, meaning the problem is 'caused by some function' and 'not solving the caused problem makes that function useful to some entity by reducing the work involved in that function', so functionality to 'minimize resources needed' would help that entity avoid creating such a function, so 'distributing this functionality to all entities' would be a useful structure to fix the problem causes, one of which is the 'inability to minimize resources needed'

- identify structures like 'high variation causative structures' that are useful in changing relevant variables like solutions/errors, as in 'causing a solution to be an error structure' or 'causing an error to become a solution structure', which are useful variables for many problem-solving intents like 'filter solution space' or 'change existing solution to improve a solution metric', as the 'opposite of a solution' is an important structure to avoid, and knowing the cause of an 'opposite' is useful in avoiding that
  - example: identify 'high-variation causing structures' like 'reasons that completely change the meaning of a structure (such as how "testing for a reaction" can completely change the meaning of a particular "statement")' or 'cycles (such as circular or wave functions) are a structure which completely change the resolvability of an ambiguity and change what info is required to resolve it, like adjacent point context of the original point to find the output value for', as 'high variation causing structures' are likely to be useful in identifying 'opposite' structures which can change a solution into an error
  - this workflow involves identifying useful structures to avoid to fulfill problem-solving intents, and the structures that can make that identification trivial

- identify useful structures like 'contradictions of insights' that can identify useful structures like 'relevant changes to apply to existing solution-finding methods' in a particular problem format like 'find a prediction function' to 'filter the solution space' of 'possible solutions' to the problem of 'find changes to apply to solution structures' in the workflow 'improve an existing solution by applying changes to it'
  - example: a 'regression' or 'machine-learning' method might find no correlation between two sets of variables (like the 'number of products purchased in one region' and the 'environment impact in another region') but as 'all variables are related', this would be a false result, so a robust solution method would have a mechanism to find correlations given this insight that 'all variables are related', allowing for distant causation or different interaction levels than those in the original data set like 'agent decisions', 'time', 'markets' and 'market manipulation', which can be produced by applying interface structures rather than simple 'combinations of input variables', thereby applying inferred or general or common variables (like 'variable patterns'), rather than just the input variables and just the 'combine' operation
  - just by applying this one insight, interface structures can be inferred, so identifying a 'contradiction of the insight' present in specific problem-format solution-finding methods like regression & machine-learning methods is particularly useful
  - an adjacent method to derive interface structures is useful on its own, but is also useful for deriving new useful structures adjacently
  - similarly other insights like 'identifying connections of an isolated variable set is rarely useful without context' is another insight that can adjacently derive interface structures (like the 'cause' of the variables and the 'intent' of identifying the connection between isolated variables, as in 'to correct lack of info' or 'to find specific info like causal variables' or to 'filter out non-causal variables')
  - if the insights are in fact insights, they should be confirmed or reflected in a good solution that reflects the truth, rather than contradicted by that solution
  - similarly, a statistical or machine-learning solution-finding method might identify a 'correlation' between the 'green skin' and 'aliens' but a solution-finding method using interface structures would identify a 'combination of errors' that would cause a 'false similarity', such as 'filtering out or under-prioritizing examples of failures (where a similarity is not significant or related or accurate)' and 'over-prioritizing simple structures like structural similarities (like similar colors or shapes)', leading to 'agent' interface structures like 'conspiracies' ('systems created by lack of information in the form of an over-prioritization, that seem based on truth until a counterexample is found to identify the missing information resulting from the priority'), since interface structures apply 'cause' to find the cause of a structure like a 'similarity' (causes like errors of over-prioritization, missing information, flexibility resulting in unenforced truth adherence, tendency to over-simplify, such as by simplistically classifying rare variable values like 'green' as meaningfully similar to other rare variable values like 'foreign' despite no reason to associate these)

- identify the reason ('equivalence' in inputs/outputs) for the usefulness of a structure (like commonly useful structures such as 'simple' and 'complex' structures) and apply it to find useful structures like 'simple/complex set that have equal inputs/outputs', given that 'equivalences' are generally useful for problem-solving intents like 'connect problem/solution' or 'find a function implementing an intent'
  - example: identify that structures like sets of 'simple and complex structures with the same inputs/outputs' can fulfill useful intents like 'find implementation/fulfillment structures of an intent', as a simpler structure like an 'intent' is likely to be fulfilled by a more complex structure like a 'function fulfilling that intent' if the inputs/outputs of the two structures are equivalent, which makes one structure useful to the other for various intents ('find a simplified representation of the function (such as the intent)', and 'find a function that fulfills the intent')
  - these structures are useful for other intents, like 'find the variables to apply that convert a simple structure (like the intent) into an equivalent complex structure (like the implementing function)' which is useful for problem-solving intents like 'find a function that implements an intent'

- identify useful structures such as 'solution component filters' that are useful for common intents like 'filter' to apply in workflows that use those intents like 'filter the solution space'
  - example: a function necessarily changes some inputs, so anything that doesnt change an input can be ruled out as a possible solution function to the 'find a prediction function' problem, and any 'combination of changes' can be included in the set of possible solutions to the 'find functions' problem
  - this workflow involves identifying useful structures by applying common core intnets like 'filter' to common problem/solution structures like 'solution components'

- identify useful structures like 'contradiction example input/output mappings' that fulfill useful intents like 'find useful structures like "change filters"' which are useful to many problem-solving intents, like 'improve a solution by applying changes to a function'
  - example: storing a 'input/output mapping' of 'contradiction examples' as metadata to store with a 'function definition' is a useful structure to store with a function bc it provides a target for improvements to the function which is useful for filtering future changes to the function (based on a solution metric of whether they improve handling of those contradiction examples), as well as taking the place of logic that would be required to handle those specific counterexamples

- identify useful structures like connections between problems and structures connected to solutions (like functions, each function being a solution to a problem)
  - example: solving 'which problems create which functions' (which problem maintains the need for a function or leads to the creation of that function) as a way of determining alternate implementation methods of functions (applying problems as a function-generating structure, if the problem is adjacent enough to the function with existing resources that it can be converted into the function by default or otherwise adjacently)
  - this workflow involves applying the 'inputs' of solutions (including 'problems') as an 'input' to solve problems like 'find a function' which may be applied in interface queries, answering the question 'what problem would develop the function required to solve a problem like "find/build/derive a function"'

- identify useful structures like 'probability' and 'reason for probability' that are useful in identifying useful structures like 'certainty structures' which are useful for many intents like 'identifying truth structures like insights', 'testing if a structure is true/certain', and 'implementing functions that are useful in that they reflect reality' which are usable in many problem-solving intents like 'filter the solution space'
  - example: certainty structures like constants & limits are relatively uncommon compared to uncertainty structures like variables, bc certainty structures are useful for changing the changes allowed by variables and act like a controlling structure on variables, which is useful when a problem occurs bc of the variables, and given the corrective power of certainty structures on variables, this further indicates that fewer certainty structures are required compared to variables (to explore useful differences to new problems), as a problem is only likely to result from an uncertainty structure that doesnt reflect truth and is only likely to be changed by a certainty structure correcting that incorrect uncertainty structure so it better reflects truth, this dichotomy serving as a useful opposite structure to balance problems/solutions between, as solutions must have some variation (uncertainty) to allow for adaptation to different contexts and different new problems, but must have some degree of certainty (constants, limits) in their reflection of the truth
  - the reason for the 'lower probability' of the truth structure is the 'lower requirement for truth structures to manage uncertainty structures', the indication that 'if a truth structure is found, its likely bc a problem (a problematic difference from reality) had occurred which the truth structure corrects', and the 'higher usefulness of uncertainty structures like variables in many intents except for intents like "correcting an incorrect structure generated by an uncertainty"'
  - a variant of this workflow is to 'apply changes to certainty structures' (like insights, patterns, or known solutions) to create useful differences like improvements to suboptimal known solutions, changes that stay within the limits created by those certainty structures (not so different that the differences violate the truth), where certainty structures are not sufficient to solve a problem, so uncertainty structures like variables/differences can increase the usefulness of a solution, since 'previous truths' act like a base for 'new truths' so applying changes to 'previous or knwon truths' can produce 'new/unknown truths', which applies this as an insight generating this solution automation workflow, which applies truth structures as an interface around which uncertainty structures like changes vacillate before resolving into truth structures (the 'interface network' being a structure that requires the fewest differences in order to become useful and reflective of reality while maintaining its ability to change)

- identify useful structures like 'opposites' that are useful for multiple core intents like 'find differences' and 'apply difference' and 'filter' which are adjacently useful to problem-solving intents like 'filter solution space', when applied to specific structures like 'requirements' to identify other specific structures like 'the set of (interchangeable alternates and requirements)' which are useful in specific contexts like 'where requirements are suboptimal' such as by 'applying variables to create interchangeable alternates of the requirements' to solve the problem of 'removing suboptimal requirements' or 'apply changes to requirements to make them different from requirements (optional)'
  - example: identify structures like 'interchangeable alternates' as an 'opposite structure' to structures that can be suboptimal like 'requirements'
  - a related workflow involves identifying problems where a particular solution automation workflow would be more useful to correct the problem (such as how this workflow is particularly useful when 'variability in solution requirements would be useful', or when a 'requirement is a problem cause') and applying changes to change the problem into that problem (changing the original problem into a problem of 'invalidating, removing, or changing requirements'), by applying differences to the solution automation workflow to find problems that would benefit from that workflows' connection structures
    - example: apply differences to the 'identify "opposites" as useful for intents like "find differences" to identify other useful structures like "interchangeable alternates" as useful for creating other useful structures like the "set of interchange alternates and requirements"' workflow in order to find the problems (like 'removing suboptimal requirements') where the workflow is particularly useful
    - "identifying interchangeable alternates" would help with the problem of 'removing requirements'
    - applying 'changes' to "identifying interchangeable alternates" would result in identifying 'opposite' structures of those structures ('requirements'), thereby identifying any problems that involve 'problematic requirements' as a useful workflow to solve that problem
    - keeping 'example' structures in a solution automation workflow definition is useful in 'reducing computations' to 'find relevant structures to that workflow', which is useful for intents like 'find similar/different workflows' or 'find interactive/connective structures with a workflow like a problem where the workflow is particularly useful'

- identify structures like "connections between solution metrics (like 'fairness') and structures that can be identified with existing functions (like 'certainty')" as a 'reason' to apply the 'existing (certainty) structures' to fulfill the 'solution metrics' (fairness), as opposed to other structures, to fulfill problem-solving intents like 'find structures that fulfill solution netrics' and 'filter the solution space (of structures that fulfill solution metrics)'
  - example: identify structures (like more structures such as variables or structure-generating structures like questions) that capture info better than simpler structures (like fewer structures), such as how the 'question of whether an individual is good or evil' is optimized by asking additional questions like 'what is the connection of their abilities/functions and their requirements (inputs and required outputs)' and 'what is the impact of their decisions (weighing the importance of outputs (as interactive with other structures) as being similarly important as their inputs, at various scales like global/local and other variables like time (the impact on their future decisions) and examples (the common impact on future decisions by other recipients of that feedback)' and 'are their decisions common (invalidating the focus on an individual as opposed to a group)' and 'are their decisions avoidable' and 'what is the impact of feedback on their decisions', questions which apply 'additional' interface structures (which act like connection structures and certainty structures) and apply 'fairness' as a 'solution metric like accuracy' of a solution (solving the problem of 'finding solutions reflecting truth' as a way of identifying the solution metric for identified alternate question sets of 'does the question generate fairness')
  - this workflow involves identifying better formats (like a 'minimized' variable set or 'accurate' variable set) for structures (like a variable set) that fulfill an intent more optimally despite impact on other solution metrics of problem-solving intents like 'minimizing storage requirements', bc these better formats apply additional interface structures and therefore fulfill the problem-solving intent of 'improving a solution (by solution metrics like completeness, accuracy, robustness)', and identifying how to convert to those formats, such as by applying "core interface structures like 'input/output'" to "other interface structures like interaction level structures (like 'decisions' on the 'agent' interaction level)" to identify structures of meaning 'what is the impact of their decisions'
  - a variant of this workflow would identify 'structures (like 'additional') of interface structures' that improve solutions, such as how a solution involving additional connected interface structures is likelier to be robust, correct, and relevant/useful/meaningful

- identify structures that can fulfill problem-solving intents like 'filter error structures' when fulfilling useful structures like other problem-solving intents such as 'find and remove error structures'
  - example: identify that 'selfishness' is a useful structure bc of the 'reason' that its a 'high variation structure', and identify that 'high variation structures' are unlikely to be an 'error structure' that should be 'removed' (as opposed to 'changed') when applying the 'find error structures like error causes and remove them' problem-solving intent
  - for example, the 'selfishness' structure is not 'required' to be applied (its optional, which adds a variable), and it doesnt have to be applied in the same way (like only to protect the individual, as opposed to protecting groups they belong to, which adds another variable), so when selfishness is suboptimal (like defending a group member just bc of membership), it can be avoided, and when its optimal (like in deadly contexts), it can be prioritized
  - the high number of variables in how selfishness is applied indicate its an important structure and also that its unlikely to be an error structure, given its variable usefulness (rather than 'absolute uselessness')
  - this workflow involves identifying useful structures such as 'high variation structures' which can 'filter the solution space' when applying problem-solving intents like 'find & remove error structures', as 'structures too important to remove' are a useful structure to 'filter the solution space (of structures to remove)', as 'required tasks' like 'organizing resource allocation' and 'sustaining life' would be almost impossible with existing brain functions if people didnt organize their attention using 'selfish' priorities like 'finding resources fulfilling their own basic requirements'

- identify useful structures like 'high information reduction structures' (such as the 'similarity attribute') that fulfill common useful intents (like 'reduce calculations required', 'summarize', 'identify alternatives/equivalents') in a problem space, as these structures are likely to be useful for other intents, some of which may be problem-solving intents ('reduce calculations required')
  - example: a program could only store the 'similarity' of content rather than searching all the attributes of the content or the content itself, and use that similarity to show recommended alternatives, rather than requiring a custom complex algorithm to find what users are likely to view next, applying the insight that users are likely to be interested in similar content to that they intentionally viewed, rather than deriving their intent in a complex algorithm, as some problems can be reduced to fewer attributes, and these fewer attributes can be useful for fulfilling intents like 'privacy' or 'reducing memory use' or 'reducing computations required'
  - this workflow involves finding structures (like an 'attribute' such as 'similarity') that encapsulate a high amount of information (such as whether information is related to other information, to fulfill intents like "identify whether the information is connected, alternative, similar, coincidentally similar, or some other intent that can use 'similarity information' as an input") without losing usefulness for other intents such as semi-derivability of that information
    - if information is similar to other information, it can likely be derived to some degree (or the set of possible information can at least be filtered) by applying the difference represented by the similarity to the other information, given the similarity definition
    - the information is required to be slightly different, otherwise there is no 'reason' that could trigger a 'view decision' (such as 'viewing new content (new meaning slightly different)')
  - related workflow: 
    - a variant of this workflow involves identifying structures (like the 'similarity attribute') that can be simultaneously useful for commonly useful intents like 'deriving a degree of info' and 'identifying similar info' and 'preserving privacy' which are likely to be required simultaneously
    - a variant of this workflow involves identifying the insight ('find similar information to show users bc its likelier to be something they want to view if its similar enough to what they already indicated they like, through structures of intent that are also structures of certainty') associated with the useful structure (the 'similarity attribute') that can be used to identify the useful structures, which would involve identifying the 'reason' ('similarity to content that was defined to be liked') of the target structure (the 'view decision'), as the user supplies a 'certainty structure' and a 'structure of intent' in their 'view decisions' (the solution metric being optimized), and the 'reason' for the 'view decision' (they wanted to view it, liked it, it was relevant/interesting, they liked the specific information in the content, etc) can be applied as an 'input' to trigger additional 'view decisions', an algorithm that prioritizes the 'view decisions' as trusted information indicating what they want to view, and prioritizing the 'specific information in the content' as the structure that the users are aiming to get (rather than 'structures of lack of intent' like clicking accidentally, or because it was the top search result or otherwise available/adjacent), an algorithm that could apply the 'apply changes to a base solution to find other solutions' workflow to 'identify the insight that similar content might also trigger a view decision'

- identify useful structures (like 'attribute count') that are useful when a particular interface query is run (like 'applying the probability-structure interface') to fulfill a particular problem-solving intent (like 'filter solution space') to solve a problem (like 'implement a function such as identify/find structures')
  - example: an ambiguity is likelier to be a 'core structure' (with fewer attributes) bc of the improbability of finding 'ambiguous alternatives with a high number of attributes' or a 'high number of missing attributes' leading to the ambiguity, which applies the 'probability-structure' interface to solve the problem of 'filter the solution space' of possible solutions to the problem of 'identifying/finding structures' (find 'ambiguities' by finding 'core structures'), like how saying "we're (verb)-ing" could be interpreted as a decision of a group or a description in a conversation
  - solution success cause: this is useful bc of the counterintuitive nature of the connection between 'ambiguity' and 'core structures' (as in, its not specified by any definition route of ambiguity or core and can be added as an alternative definition route when more certain definition routes fail)
  - generalization: this workflow can be abstracted to find the specific intents/problems and interface queries that would produce a useful structure like a 'counterintuitive connection', such as identifying the probability-structure interface as being usable to connect an 'ambiguity' and a 'core' structure, given their similarity in an attribute metadata attribute (attribute count) which is a structure of 'probability', so identifying the probability interface as useful amounts to the operation of 'identifying the reason for common structures between two structures to connect' (probability being a reason for commonness) and 'applying that reason for commonness as a method of connecting the structures' (connecting them in the sense of finding relevant similarities, relevant to the task of 'identifying alternate/connected structures of a structure, as a way of identifying that structure')

- structures like 'functions applied' are useful to identify useful uncertainty structures like 'assumptions/predictions about structures like the usefulness, resources, requirements of those functions', 'assumptions/predictions' which may or may not be certain or otherwise useful insights that can be generalized or applied to find other useful structures, insights which are useful to derive
  - example: if an agent calls a function, the agent assumes and predicts that the 'outputs will be useful for its intents'
    - the 'reason the outputs are useful for their intents' may not be clear but are likely derivable and likely to be useful to derive
    - these are similarly useful structures as 'implications' of 'using a function', as 'structures of probability' (as opposed to certainty) adjacent to the function (there is a probable reason to find the outputs useful, there is a probable reason that is considered useful, there is a probable resource making that useful) which are useful in finding 'connecting structures' to connect the function with other structures like other useful functions fulfilling those predictions, given that if it exists, its likely to fulfill its predictions better than average

- identify alternate routes to fulfill intents like 'identify useful structures' that are likely to already exist (like problem structures are likely to be already known)
  - example: identifying 'problem/suboptimality' structures is a way of identifying 'functions & inputs that dont exist yet' and 'differences not handled by existing functions, which are suboptimal for some useful intents given that these problems have been identified as a problem for some agent', where 'functions that dont exist yet' are a useful structure to determine adjacent structures like opposite structures, such as 'functions that do exist or are known', applying the insight that 'if a structure (like a problem) exists and has not been reduced (solved), the functions to reduce it (solution functions, solution-finding functions) are less likely to already be known or applied to solve it, if there is a reason to reduce it (the structure is a problem)' which indicates that this structure (a problem) can be used to determine which 'reduction functions' (solution functions) exist

- identify useful structures like 'connecting structures' of 'problem/solution structures' and 'causal sequences' which are useful in intents like 'finding alternate connections between problem/solution structures' (finding new solution automation workflows)
  - example: the 'cause' of a 'new solution-finding method' might be identifiable at any point in its causal sequence (a 'suboptimality/error is identifiable (measurable) & identified', a 'solution is required', 'solution is possible', a 'solution is tested', a 'solution automation workflow (like 'applying all known solutions until one works') is applied and didnt fix it' a 'solution is identified as an improvement'), but only some structures in that sequence are useful as identifiers of a 'new, more optimal solution' ('inputs', 'requirements', 'reasons to find a new solution, such as a suboptimality/error'), as other structures in the sequence are irrelevant (the 'requirement' of a solution is irrelevant if it is 'impossible' to be solved so 'possibility' filters would have to be applied first in a sequential workflow, and the 'identifiability of a suboptimality' doesnt cause the solution, and none of these structures cause the specific solution structures applied in the optimal solution, unless there is only one solution, making those structures 'required constants')
  - if a workflow like 'trial & error applied to known solutions' is applied and found to not find a solution, that is a cause of applying functions to fulfill problem-solving intents like 'find new workflows', and is a cause of identifying the problem, but is not a direct cause of the solution

- find useful structures like functions to apply to useful structures to find other useful structures
  - example: an abstract variant of a 'dichotomy' (with the requirement of a 'count' attribute value to be a constant value) is an 'exclusivity', which is useful for intents like 'find structures that dont co-occur' (like an 'impossibility' and a 'possibility' being connected as 'equivalents' of the same structure, these connections not occurring in any system that follows rules (a logical system, a real system, etc)
    - 'find structures that dont co-occur' is useful for other useful intents like 'selecting between alternates' and 'find opposite/different structures' which are useful for intents like 'differentiate/connect structures' and 'filter structures'
  - 'mutual exclusivities' are a 'core structure' related to other core structures like 'differences'
  - a 'falsehood' combined with the 'exclusivity' (a 'false dichotomy') is a useful structure in that it applies a core error structure 'false' to a core structure 'exclusivity' of core problem structures like 'difference', identifying a 'specific example of the error structure' from applying this combination function (thereby applying a 'false' filter to the set of abstract exclusivity structures to identify the specific structures that are 'error structures')
  - 'abstraction to find alternate examples' and 'combination with other useful structures like error structures to find specific error structures' are the useful functions to apply to the useful structure of a 'dichotomy' to find other useful structures to fulfill other useful intents

- find structures that are useful in other problem-solving intents like 'finding other useful structures'
  - example: find 'new examples' of a structure like a 'type', since new examples are likely to enable adjacently identifying other useful structures like the 'route to find the useful structure', such as by connecting the 'system context' to the 'new example', which is likely to be a 'new route to useful structures' (like the example structure) if the example is genuinely new, and since there's likely to be a 'reason' for the 'difference' in the new example if its an actual unique example, a reason such as a 'new interface structure' causing the newly different example (if the difference is on the right interaction level and not just any difference in any attribute), and as differences in examples of the same structure are an important signal since 'examples of the same structure' are likely to have multiple similarities, probably more similarities than differences
    - 'finding attributes that connect very different structures' is a useful structure to 'find more adjacent routes connecting structures' (like how finding the opposite of a structure like a 'counterexample of a statement' or the limits of a structure can be more adjacent than finding the original structure) and 'find new differences to apply to create different structures' (like 'like creating maximally different alternate solutions to filter the solution space or the solution space of base solutions to change')
  - 'finding connections to useful structures' is a useful intent to make 'new examples' useful structures
  - 'finding a new example of a useful structure' is likely to enable adjacentlying identifying a new reason for the structure being useful, which is useful to apply to 'find new useful structures' or 'find alternate useful structures'

- find structures that are useful for problem-solving intents like 'filter solution space' as useful structures like 'filters' applied to fulfill the problem-solving intent of 'find useful structures'
  - example: the 'reason for the irrelevance' of a 'structural similarity' is particularly useful to fulfill core interaction functions like 'filter' or 'differentiate' between relevant & irrelevant structural similarities, which are difficult but useful to identify/filter/differentiate, as 'structural similarities' are very useful structures when relevant
    - the 'reason for the relevance/irrelevance' of a 'structural similarity' may just be that the structure found to be similar to another structure is just a 'common structure' or a 'core structure', rather than its similarity to the other structure being similar for a relevant reason, like a similarity in the reasons why the structure was caused (like to 'apply it for the same intents')
    - the 'similarities in cause (reason) of the structure' and the 'similarities in intent of the structure' are therefore better structures to check for filtering structures that are relevant, as opposed to just applying 'structural similarities' on their own
    - the reason why 'causal similarities' and 'intent similarities' are useful to find relevant structures that are similar (as opposed to irrelevant structures that are similar) is that the cause/intent structures are likelier to be aligning with the reason for identifying/finding that structure in the first place (structural similarities arent typically sought to 'find common structures', theyre typically sought to 'find alternate structures to fulfill a particular purpose' or 'find all the examples of a structure'), therefore there is a (useful) 'structural similarity' between intent/cause of 'structural similarities' and the intent/cause of 'finding structural similarities' (which include intents like 'find all the examples of a structure')
    - in the reverse direction of derivation, 'structural similarities' can be a useful structure to fulfill intents like 'find common structures' or 'find core structures', as common structures are likely to be in a set of similar structures, and common structures are likely to be core structures
      - this fulfills the useful intent of 'find the context where a structure would be useful' to enable fulfilling other intents like 'check for that context to find useful structures to apply in an input context'
    - 'cause/intent' are therefore useful as a filter of 'structural similarities'
      - another reason a structure might be found to be similar is the structure is 'optimal' in some way, such as that its useful for some intent like 'find stable structures', so the structure is likely to be repeated as its optimal for some intent which means it may be useful for other intents or that intent (and the associated optimal structure fulfilling it) may be required elsewhere
      - the 'repetition' attribute is a useful 'connecting structure' of related structures like 'similarity' and 'commonness' as a 'cause' or 'output' of both structures, which makes it useful for intents like 'connecting/differentiating structures' which are useful for problem-solving intents like 'connect problem/solution' and 'filter solution space', which enables intents like 'identify useful reasons why a structure would have attributes' like 'similar but not common' (its a new optimization that hasnt been deployed elsewhere yet), 'similar and common' (its repeated often), etc, which is useful to differentiate the 'related and similar but unique' structures and differentiate when one structure can be used to identify the other or as a proxy for the other (when commonness is relevant vs. when its not)
      - 'related and similar but unique' structures are useful for many intents like 'approximate', 'substitute', etc, but should be filtered by relevance (the 'reason for the similarity is the same', or the 'input/output connections are the same' (inputs/outputs being a core useful structure))
    - adding a similarity of a 'similar system' in which the structures both develop is another structure indicating the relevance of the 'structural similarity', acting as a 'cause of relevance' which is a useful structure to find/evaluate in place of finding/evaluating relevance to fulfill intents like 'find useful structures'
    - this emphasizes the usefulness of structures like the 'count of similarities (like cause, intent, and structure) between structures' which fulfill the useful intent of 'increasing the probability of a structure being useful' which is useful for 'finding useful structures', as the more similarities in interface structures of a structure there are, the likelier that structure is to be useful, as on its own, one similarity may not be enough for usefulness
      - the more interface structures that two structures have in common, the likelier it is that they have the same meaning and related structures of meaning like usefulness
    - having a cause/intent in one position of a system means its likely there are other causes/intents that are similar to that cause/intent bc there is no 'limiting rule enforcing uniqueness of structures in a system' and 'repetition of structures in a system is a common attribute of system structures', so 'identifying if the structures both exist in the same system' is another useful intent to fulfill in finding out whether a 'structural similarity' is relevant to intents like 'find all examples of a structure'
      - 'having the same cause/intent' means that those causes/intents exist and are likely to be repeated (occur elsewhere), and the causes/intents are likely to have the same output when found elsewhere

- identify & store the interface structures associated with interaction levels that make those structures more useful for useful structures like 'dichotomies' (like by fulfilling 'required' or 'otherwise useful' intents) in fulfilling a problem-solving intent like 'fulfilling as many problem-solving intents as possible'
  - example: the 'attribute interaction level' would have identifying differences like the 'solution metric value (such as the accuracy) of a particular solution function'
    - the 'adjacent structure interaction level' would have identifying differences like 'similarity of sub-function or function component sets with solution functions'
    - these interaction levels are optimal in both different & overlapping contexts, for both different & overlapping intents
    - different operations can be fulfilled more adjacently on different interaction levels, like:
      - how components on 'adjacent structure interaction levels' (such as 'function components' or 'incomplete functions' or 'sub-functions' or 'input functions') can be more adjacently converted to a solution function with fewer steps than other formats
      - how components on 'attribute interaction levels' can be more adjacently abstracted, as they already depend on a particular attribute, so abstracting the attribute or removing components of the attribute's interaction with specific structures or specific problem/solution structures (removing the 'solution function' specification to allow connecting the attribute to other functions like 'solution-finding functions') is an adjacent operation on that interaction level
    - 'finding a solution function with an accuracy level' may be more useful than 'finding a solution function composed of known incomplete solution functions', given the associated interface structures of these interaction levels (the contexts, intents, & functions more easily fulfilled with those structures) and the 'info requirements', 'change requirements', 'success probability', 'general usefulness across common problem-solving intents' and other useful metrics of these interface structures & interaction levels
    - this applies the 'functionality/resources vs requirements' dichotomy to find useful structures for problem-solving intents like 'fulfill solution requirements', like 'functions fulfilled by applying an interaction level' (a mix of interaction levels & other interface structures)
    - this identifies a new useful function structure as 'incomplete functions' (useful for having a common error type built-in to a solution-finding function, that is resolvable with derivation methods like 'applying patterns', to fulfill useful problem-solving intents like 'apply solution to real information which is likely to have common error types, to "connect erroneous inputs to solution outputs" as well as "connecting correct inputs to solution outputs"', applying the insight that 'errors are probable in inputs' to fulfill the problem-solving intent of 'find a solution-finding function that handles common or otherwise predictable error types')
    - this identifies another new useful structure of 'different and overlapping structures (like functions/attributes)' as useful for finding commonalities, similarities & differences
  - differentiation: this workflow applies an abstraction ('dichotomy') to the core problem structure of 'differences' and finds interface structures that can find/build/derive problem-solving structures to resolve that problem structure, to fulfill the problem-solving intent of 'resolve problematic differences' (a variant of 'connect problem/solution'), as 'different and overlapping functionality' is a useful structure for fulfilling 'difference resolution structures' to fulfill 'requirements of a solution', thereby connecting the 'dichotomy of functionality/requirements' with 'interface structures that can adjacently fulfill its opposite structures (to resolve the difference between functionality & requirements)'
    - this workflow connects interface structures on multiple interaction levels (the problem/solution interface, the structure interface, the difference interface), thereby making it likelier to be useful by default
    - a variant of this workflow would 'find structures that can also connect structures on these interfaces' as inherently useful structures to fulfill problem-solving intents
