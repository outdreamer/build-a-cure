- the only similarities I see which could possibly cause confusion because of the similarity in terms are:
	- a general intent to retrieve solutions from a database (which forms the basis of their invention, but is a minor optional detail in mine)
	- the integration of user feedback, which is required in their system, but is optional in mine because it doesnt rely on that to find/generate/derive solutions, because an invention of a new way to derive solutions automatically (using interface analysis) is part of my invention, which is why my invention doesnt need machine-learning or statistics
		- my invention identifies 'solution success probability' on its own, and the user-scoring feedback is a minor optional detail for 'personalization/augmentation' of that attribute, rather than a basis for the 'solution success probability'
		- my invention self-optimizes by identifying new structures like solution automation workflows, interface queries, insight paths, and other useful structures that can be used to speed up or otherwise optimize other interface queries
	- the use of a few words like 'search' in common, which refer to 'known business problems & processes with associated rules' in their system but refer to 'custom solution automation functions to solve those problems' in mine
	- the connection of a user-submitted problem and a system-returned solution, which in their system results from the static logic of 'finding a solution in the database for known business processes/problems & creating a slightly different solution to similar solutions if there isnt one & displaying that to the user to suggest a solution or similar solution that has been labeled as correct for the submitted business process problem' vs. the method of 'finding/deriving/generating solutions for general problem-solving intents like "implementing a solution automation workflow as an interface query" or "finding a new solution automation workflow to solve a problem" or "formatting problem/solution structures in a way that they can be compared & connected" to automate problem-solving'
		- their system is like a system that can solve only the 'find a prediction function' problem, where if a user submits a data set & a target variable to predict, it will find a 'prediction function' using 'find a prediction function' solutions stored in the database, given the common keywords ('taxonomy' terms) between the problem statement & the associated solutions
			- a system like this is extremely specific, fragile, & dependent on data stored in the database, defined business processes/taxonomies, and static implementation logic of this specific problem-solving workflow that can only handle specific problems
		- my invention can take any problem statement, describe the problem attributes in the related problem space, find/derive/generate a core interaction function/solution automation workflow/interface query to fulfill problem-solving intents to solve the problem, apply those structures to produce a solution set, check if derived or specific solution metrics are fulfilled, and return the solution(s) to the user with statistics like 'solution success probability' and attributes like 'solution success cause'
			- this is an extremely robust, powerful system built on understanding that can generate problem-solving logic (like new 'problem/solution core interaction functions' and 'error-avoidance algorithms') & solution structures (like 'solution automation workflows' and 'interface queries' and 'insight paths') on-demand

- key differences: 	
	- components
		- their system relies on a 'search function' & the structure of 'indexing functions by attributes', which I invented in 2013
	- what is automated
		- their system automates almost nothing, except search of existing solutions in the database & pattern analysis with machine-learning 
			- the 'indexing function by attributes' isnt even automated, solutions in their database have to be manually associated with taxonomy terms, unless machine-learning can identify a few terms people didnt add manually
			- solutions that are 'corrected automatically' by their application of AI need manual feedback to be corrected and their primary value is as a learning tool to identify gaps in automation that are not handled by the system
		- my invention automates problem-solving of solvable problems
	- from their abstract:
		- "A method includes receiving a text description of a system capability request, and converting the text description into a normalized description of the system capability request"
			- this is just 'standardizing language terms' which is a known common process like 'removing corrupt data from a data set', not an idea
		- "A repository is then queried, based on the normalized description and using a search algorithm, to identify multiple candidate application software units (ASUs)"
			- querying for functionality isnt new, I invented a function index for function searching many years ago
		- "The candidate ASUs are displayed to a user for selection"
			- this requires the user to do the work of 'selecting the solution', the system cannot do it for them
		- "The user-selected ASU is then deployed, either locally or to at least one remote compute device, in response to receiving the user selection"
			- deployment is a known common process, not an idea
		- "Deployment can include the user-selected candidate ASU being integrated into a local or remote software package, thus defining a modified software package that is configured to provide the system capability."
			- 'replacing a function with a user-selected function from function search results' doesn't automate problem-solving in any way or provide analysis capable of doing so at all

- differences in terms
	- component
		- 'components' in their system refer to known software structures (like software functions, software interfaces (which are 'lists of required attributes/functions'), software classes, or software libraries)
		- 'components' in my invention refer to any structure that can be considered a part of another structure, usable to build another structure or comprise a piece of its definition (like an attribute, network, concept, subset, assumption, definition, function, etc)
			- an 'interface' in my invention refers to a 'filter that, when applied, results in a set of isolatable structures that allow comparison of objects standardized using that filter' (like meaning, cause, intent, logic, math, structure, concept, potential, change, function, pattern)
	- taxonomy vs. definition
		- their system uses taxonomies to relate terms as a representation of domain knowledge with associate related solutions to problems including those terms to enable a basic keyword search function
		- my invention uses definitions of generally useful concepts like 'power' or 'balance' which are independent of user input (these definitions are configurations of the system, not user input)
			- my invention can query for information, definitions, functions, tools, & other existing structures which may be useful as inputs to a particular 'solution automation workflow' determined to be capable of solving the problem by my invention, but these are not user-submitted dependencies, just resources available with an internet connection or local information data store that may be used in a particular interface query that dynamically determines such information may add value to a problem-solving intent
	- integration
		- in their system 'integration' means 'update a component & test if the update broke anything'
		- in my system 'integration' can refer to 'merging' or 'organizing', as in:
			- 'merging structures to create another structure'
				- 'merging solution structures' such as 'merging sub-solutions to create a solution to the original problem'
			- 'merging structures to check if they fit'
				- 'merging a solution with a problem space system in which the problem occurs, to see if the problem is reduced by some solution metric' (alternatively 'apply a solution to the problem space system')
			- 'merge structures to check if they create a structure of meaning/relevance/usefulness'
				- 'merging functions in a sequence to see if the resulting function sequence can connect the origin input with the target output'
				- 'organizing structures in a structure that makes sense or has meaning'
			- 'merging/coordinating/organizing structures so they can operate on another interaction layer'
				
- differences in those business process/problem terms, vs. my custom problem-solving functions implementing a general solution of those problem-solving intents:

	- primary objects (their processes/problems & associated static specific manual/automatic processes vs. my general problem-solving automation functions)
		- in their invention, these terms refer to executing static logic to automate a few business processes that follow known rules
		- in my invention, words like 'apply' and 'select' refer to custom functions relevant to problem-solving intents with capacity to automate problem-solving, and with specific custom meanings in the context of custom structures defined in my invention like interfaces, problems/solutions & related objects that enables these objects to be connected in a way that automates problem-solving, not just automating a few business processes that follow known rules, which many systems do
	- concept definition
		- in their invention a 'concept' is just a term referencing an object, like a 'hat' is a concept in the 'fashion system'
		- in my invention, a 'concept' is a 'unique isolatable abstraction that applies across systems', like 'power' or 'balance' or 'truth' apply across systems, even though their structure may vary across systems (resulting in different 'definition routes' of the concept, as I call these alternate structures referring to the same concept)
	- query definition
		- in their invention, a 'query' is a standard database query to find data in the database about existing solutions stored in the database, like a 'sql select statement'
		- in my invention an 'interface query' is a set of interface structures & operations that can solve the problem, implementing a 'solution automation workflow' to guide the interface query design

	- similar to the machine-learning method of 'supervised learning', if their system is not given the answer (in the form of usage patterns or user-submitted data such as similar solutions), it will not produce the solution, whereas my invention will be able to produce a solution without the answer, user-submitted data, or similar solutions

	- these words (like apply, select, classify, match, learn, search, discover, create, update, deploy, reuse, operate, manage) refer to static processes in their invention that enables automation of business processes that follow known rules, but refer to custom functions with custom logic/meaning in my invention that enables general problem-solving automation
		- 'apply' 
			- in their invention this term means 'package the selected function with other functions & test'
			- in my invention refers to a custom general logic interaction function to 'apply' a structure (like a function or solution) to another structure (like an attribute or problem)
		- 'select'
			- in their invention this term means 'the user selects the solution if the pattern-identification algorithm cant select one'
			- in my invention refers to general logic interaction function that can implement multiple 'selection' processes like:
				- 'selecting a function to fulfill the problem-solving intent of "selecting a solution" that fulfills the solution structure of "solution metrics"'
				- 'selecting an interface query or solution automation workflow or core interaction function or origin/target problem solution formats to solve the problem'
				- 'selecting solution metric filter structures to filter possible solutions'
		- 'evolve'
			- their definition of 'evolve' to improve their system & suggest updates uses machine learning & prior usage data patterns to connect input problems with solution outputs, rather than applying 'understanding' of 'why a solution works' (solution success cause)
				- this means their system relies on users (such as the 'community ecosystem' they refer to), their usage patterns, the accuracy of their feedback, machine learning, the accuracy of their usage patterns in reflecting problem/solution connections, and the availability of data on usage patterns related to a problem, without which their system would fail
			- in my invention, attributes such as the 'solution success cause' are attributes of the 'solution' object that indicates why a solution worked or didnt work, so that the system can identify similar causes in new problems submitted by users and match solutions using this understanding of similarity in 'solution success cause' (why a solution worked), and these attributes allow understanding-based improvements to solutions & suggested solutions, rather than relying solely on basic usage patterns & user-submitted feedback like user ratings, which many systems rely on
		- 'adapt' in their system is a 'manual step' (as indicated in Fig 12), as are other processes like 'operate'
		- 'discover' in their system means 'look up in a database' or 'use machine-learning to identify a pattern'
		- 'solution-implementation' process
			- the 'implementation roadmap' is static and/or relies on user-submitted input of an 'implementation strategy' in their invention
				- their 'implementation roadmap' is executed with 'available solutions', but if none are available, it fails to automate the 'suggest solutions' process, and manual intervention is required if no available solutions are found in their database
			- in my invention, each problem may be handled differently by the system, which generates a custom solution-finding/generating query called an 'interface query' which may also be a solution automation workflow, or may apply a solution automation workflow if an existing workflow is suitable to solve the problem
		- their terms are not standardized 
			- 'update/evolve/adapt/learn/upgrade' are synonyms but here are used as if they are different
			- 'search/discover/match' are synonyms in their system but are used as if they are different to imply a complexity that isnt there
		- the other processes they refer to here like 'search' and 'classify' also follow known rules like 'apply a classification algorithm' or 'apply a search algorithm', which in all likelihood, hundreds of thousands of apps & hundreds of 'knowledge management & discovery platforms' or 'business process automation platforms' or 'business intelligence platforms/tools' do, using known tools like AI - rather than discovering, designing & building a custom problem-solving process for each input problem that enables knowledge derivation independent of machine learning, like my invention does.

- differences in dependencies

	- their method depends on:
		- user-defined 'business contexts', 'taxonomies', 'metric pools', 'errors', 'software code including software tests' and 'existing solutions to a problem'
			- their solution-integration technology can only work if there are user-defined software tests to identify if an updated component works or not
			- their solution-creation technology can only work if there are user-defined software components to combine
			- their solution-adapting technology (using 'prescriptive analytics' to adjust solutions) relies on information like a user-defined/enabled error in a log in order to detect that a solution is sub-optimal & needs adjustment
				- 'prescriptive analytics' can only make incremental improvements in most cases, which cant solve error types requiring anything other than incremental adjustments, for example:
					- if a file cant be delivered because 'a file is moved to the parent directory location' or 'the path variable was reset', prescriptive analytics may be able to identify that problem cause, because it's simple, since it involves changing one variable of the object (like the 'file location' attribute or 'application environment variables' attribute)
					- if a file cant be delivered because 'an internet cdn provider has a temporary outage' or 'an unreported config update occurred, making some DNS entry invalid', prescriptive analytics will likely not generate/identify that problem cause, because it's complex & therefore non-adjacent, as it involves multiple interactions across interaction layers, unless it's told the answer by a 'solution rule' stored in the database or if it learns the answer because the same problem/solution already happened before
			- the 'metric pools' they refer to are similar to 'feature stores' which they did not invent
			- their taxonomies dont capture the highest variation objects (like 'cause' or 'intent') & relies on user-submitted input
				- their taxonomy is largely submitted by & therefore dependent on users & their communities of partners/experts to update manually
				- their focus is specific to software/business and relies on the software/business interaction level, involving objects like: software vulnerabilities/fixes/API usage examples/tests/libraries/functions, business applications/rules, software designs, & business products
				- my invention which is broad enough to solve any problem type/structure, involving general objects like: errors, structures, types, concepts, potential, logic, cause, change, etc)
				- my invention can solve any problem with enough information, including math problems
					- my invention can invent a new method to solve a known math problem, even if there's no existing/similar solution solution in the database, because my invention automates all solvable problems, like 'inventing new math inventions'
			- they do have a concept of 'similarity/difference' built in to their system like any search system, any system that uses AI, any categorization system or pattern identifying/analyzing system, but this concept is only applied in the context where a 'similar' solution to the 'taxonomy' keywords exists or can be generated
				- this is one 'static specific workflow' of 'finding/generating a solution' (as I mentioned where they code static logic to implement their invention), rather than being a framework where solution automation workflows can be generated & applied on-demand
				- they also did not invent the idea of 'identifying similarities between objects' or 'applying differences to an object' or this particular implementation of those ideas
				- they apply the concept of 'similarity' in the same way as search engines apply it, such as by 'calculating cosine similarity to determine how similar a document is to a keyword'
				- my invention applies the concept of 'similarity' in various ways, including:
					- identify the 'similarity/differences' required to 'connect' a problem with a solution & identify functions that can create those similarities/differences & apply those functions in a structure that creates those similarities/differences in the required sequence/structure as the 'solution-generating function'
					- apply useful structures like 'structural similarities' to identify 'structures of meaning/relevance/usefulness'
					- apply interfaces to standardize structures so they are formatted in 'similar' terms that allow them to be compared & their differences identified, once the structures are 'similar' enough to make their differences obvious after applying the standard
		- logs of user activities to derive usage patterns 
			- this is similar to 'applying machine-learning' (which they did not invent) to 'user activity' data to suggest 'user activities' as a next step in a known business process with available user data
			- another analogy is 'forming a network graph of user activities' (they did not invent network graphs either) using 'user activity' data, and then querying for the 'next step' in a known business process
		- expert knowledge & updates manually input to the system (including a team of librarians, partners, community managers, as well as experts)
		- static functions like create/search/select/classify which depend on machine learning, statistics, & expert knowledge rules, which apply static logic that automates business processes that follow known rules
		- known tools: MCDA/ranking tools like ELECTRE/AHP, machine-learning & statistics tools for pattern analysis, similarity-identification tools like similarity metric analyzers, & knowledge graphs
		- 'reusability' of available solutions in their database, and/or 'similarity' of available solutions to optimal solutions for a problem without an exact match
			- without either of these attributes, their solution database (what they call an 'Asset Catalog') is useless

	- my method depends on:
		- a comprehensive definition set encompassing all the primary interfaces that differences occur on, where any problem can be solved on each interface with sufficient information, where the definitions dont require user feedback, as they are input configuration that enables the system to work independently of user-submitted info other than the problem statement
		- inference logic that can derive knowledge independently of machine learning, statistics, and expert knowledge rules

- differences in features/value added

	- their method offers:
		- incremental & specific updates to known business process automation, if a user's business process complies with their definition of the process & their logic to automate it
			- I dont feel like anyone who tried to automate solutions to business process problems would fail to identify 'reuse solutions' and 'solution requirements' as important objects, or wouldnt think of applying machine-learning to generate changes to similar solutions to generate new ones (which is applying 'trial & error' rather than 'understanding of why & how a change should be applied to optimize/adapt a solution')
			- they heavily rely on inventions they did not invent (machine learning, statistics, knowledge graphs, optimization methods, feature stores, etc)
			- their implementation of a 'business process problem solving tool' is the default implementation that most people would come up with if they tried to do so
			- their implementation of a 'solution reuse' and 'solution adaptation' method is not an innovation in its own right, but rather a requirement that any system claiming to 'automate a solution to a problem' must fulfill to qualify as a 'solution automation program', even a specific program for a specific subset of problems using specific solutions & static logic, so including default methods for these requirements is more like 'stating the problem' (stating that a 'requirement' exists, such as 'adapt solutions if there isnt an exact solution', and needs to be fulfilled in some way) and 'stating the default obvious solution to that problem' (applying AI to solve everything)
		- their solution of 'apply AI to connect all aspects of a problem/solution (like context/requirements, solution/updates, problem/solution, inputs/outputs, & interactive components') is a default obvious method, rather than a value-add or an innovation
		- identifying 'node performance or business problem/solution knowledge gaps' using AI which it tries to fill by 'applying changes to similar solutions using AI' which is like applying the method of 'trial & error' to see if any change produces a solution, without any kind of understanding of why a solution did or did not work built-in, and depending on info sources like 'application logs' indicating that a particular change did or did not fix existing errors or produce new errors, using existing AI methods like anomaly detection which they did not invent
		- a list of known business processes & problems (apply, select, classify, search, etc) with associated static specific logic or manual processes to implement them
			- in my system these are the names of some common user problem intents (they 'state the problem', as these are common problems), but names & associated static logic arent the only related objects to these processes & problems in my invention - in my invention these are general functions to solve these problems, which enable connecting other objects to fulfill problem-solving intents like:
				- 'automatically find a solution'
				- 'automatically reduce the solution space with filter structures'
				- 'automatically build a solution out of solution components if there isnt one'
				- 'automatically connect problem & solution formats'
		- pattern detection & application (indicated in Fig 30), which is also offered by many other 'business process automation' and 'knowledge management' systems like SAP & ServiceNow & other 'workflow management/automation' tools
			- for comparison, rather than being the primary if not only strategy to solve a problem in their system, the 'pattern interface' is one of many primary interfaces on which a problem can be solved in my system, and includes the entire interface definition (including other interface components on the pattern interface, like pattern interaction logic, pattern types, pattern probabilities, pattern change types, etc), rather than a specific definition of logic 'applying a particular type of pattern to a particular type of process'
		- a list of known software structures (Fig 46) like 'architectural patterns' and 'program libraries' and 'program generators'
			- for comparison, my invention includes new methods of finding, generating & implementing solutions automatically, like 'find useful structures such as "function input-output sequences" to connect problem input & solution output formats' (and ways of generating these new methods)

	- their system is missing:
		- a 'more precise semantic description of services' that they didnt quite reach
		- a way of dynamically applying analysis methods like cost/benefit analysis using just its definition, which my invention can do because my invention involves structural definition routes
			- their system requires defining which analysis methods should be applied in which contexts & associated rules of analysis methods that should be applied, and identifies 'cost-benefit analysis' as a static solution metric rather than an 'analysis method with associated structures like rules or definitions' that can be applied dynamically to solve problems
			- their system statically applies configured analysis methods like machine-learning methods such as prescriptive AI, rather than selecting analysis methods & other problem-solving methods dynamically
		- a concept of 'change demand & supply' which would simplify their reference to 'business problem gaps, user needs gaps, or solution gaps', which they did not identify as the simple structures of 'difference' or 'error' structures to correct by applying 'change' structures, nor did they identify useful error objects/structures/attributes/generative functions or associated error correcting-functions (an understanding-based approach using interface analysis, rather than 'guessing by applying prescriptive/predictive AI to apply incremental changes')
		- a concept of 'user intent' as a relevant structure to identify the related problem-solving intents that need to be solved, because they didnt identify the intent interface as an important primary interface like cause or logic or patterns
		- a 'derivation' function to derive solutions without the use of known methods they did not invent (like applying user-submitted patterns, statistics, machine-learning, etc)
			- my 'derive' function applies understanding in the form of useful structures like 'structural similarities', 'input-output sequences', 'information proxies', 'alternate routes to information', 'alternate definitions', 'alternate formats', & 'probabilities' to generate information where none is available
		- a concept of the 'problem/solution' interface as being particularly relevant to automating problem-solving, as well as primary interfaces like potential, logic, structure, math, etc to select & apply dynamically as needed, where each interface is sufficient to solve a problem on with sufficient information & a complete interface definition
		- structurization of the problem in a way that understanding (in the form of probability, definitions, patterns, rules, useful system structures, differences) can be found/derived/generated/applied as needed
		- anyone can apply machine-learning to generate a software function from 'software' input data, their system doesnt add value beyond this because it's not built on understanding and it doesn't automate anything valuable that hasn't already been automated in other ways by other Business Process Automation & Knowledge Management Platforms with similar requirements for manual intervention
		- their system is also similar in value to Incident Management systems like ServiceNow, which mostly add a problem/solution schema/taxonomy and allow searching of prior solutions, which essentially is just a standard application comprising a database + search interface + taxonomy, which has been implemented countless times
		- in addition to machine-learning & known pattern analysis rules, their system also relies on other things they did not invent, like knowledge graphs & taxonomies, neither of which they invented, nor did they even invent applying these to problems/solutions to automate known business processes
		- the value added by their invention is extremely minor, in relation to the inventions it relies on & the applications it is similar to, especially when considering the cost of implementing the expert knowledge integration & community feedback & manual processes involved
		- they did not identify any new concepts/structures nor did they implement existing concepts/tools in a new way that added value
		- they did not identify the following, which my invention adds:
			- the application of 'standards' as an input that it is possible to structure (as in 'interfaces, on which any problem can be solved once problem objects are standardized to the interface'), rather than an abstract goal of general 'standardization'
			- the structural nature of info objects (like insights & solutions), as well as structures such as patterns of these info objects (like 'insight paths' referring to a 'structural pattern that reliably produces insights across fields') & the structural nature & patterns/other attributes of other useful structures relevant to problem-solving
			- the possibility of multiple solution automation workflows (they apply one specific workflow that relies on user data like user activity patterns & tools like machine-learning) & the variables of solution automation workflows
			- the possibility of 'connecting' problem/solution formats automatically (as well as identifying other problem/solution core interaction functions than 'connect' like 'reduce' or 'break' or 'neutralize') once they are structured in a way so that they can be compared & equated (standardized to an interface)
			- the possibility of solving a problem on multiple alternative interfaces (my invention shows how to solve a problem on the concept interface, the structure interface, the pattern interface, etc)
			- the concept of 'interfaces' as a useful tool for comparison of structures (analogous to the concept of a 'symmetry' in math serving as a base for change to develop, or a 'filter' structure that highlights relevant interactive information & excludes other information)
			- the idea of automating logical analysis such as system analysis, with automated analysis functions like 'automatically identifying useful structures like contradictions, trade-offs, requirements, assumptions, & efficiencies' by 'importing rules into a system format like a variable/object/function graph where the definitions of these useful structures can be identified with a query'
			- the concept of an 'interface query' navigating structures of interfaces
			- the existence of useful structures that speed up problem-solving like 'alternate routes'
			- my invention is independent of statistics/machine-learning/graph databases (though it may use these tools, if an interface query determines they are useful inputs to a solution) & offers an alternative to these as solution-derivation methods
			- my invention includes analysis logic not found in other systems
				- for example, interface analysis logic includes rules like: 'apply other interfaces to an interface to generate the complete interface definition', such as 'applying the cause interface to the pattern interface' to generate structures in the pattern interface definition, such as: 'causal pattern objects (as in objects that cause patterns)' or 'patterns of cause'
			- my invention is built on understanding & adapts built on that understanding - rather than guessing, hard-coded logic, or other people's inventions
				- for example, my invention applies the insight that 'certainty structures can also be solution structures' because of the insight that 'solutions built on facts are more robust', so it indexes 'certainty structures' as a useful structure to apply when finding/generating solutions, by my system indexes insights like this in the first place & has the tools to apply this insight because it applies the 'structure interface' as needed, so that 'certainty structures' can be found & integrated with other structures