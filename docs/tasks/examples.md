# math-language map (connecting function) solution automation insight path examples

# solution metric/filter/test examples

## general solution filters
	- meaning
		- relevance
			- usefulness
				- clarity
				- adjacence (reduction of cost to reach solution)

## alternate variable set filters
    - when testing different variable subsets, you can select a variable set split by structures like:
      - vertex variables
      - variables on interim interfaces where other variables aggregate (in bottlenecks or hubs)
      - difference interactions
        - difference type (homogeneous sets of difference types)
        - differences in different types (heterogeneous sets of difference types)
        - which difference type sets would identify the most errors or are the most different from other difference type sets
      - which difference types are the biggest variance-reducers when combined
      - which difference types have an attribute (common, relevance, similarity)


## solution filters of an identification algorithm (to differentiate real & AI-generated content)
    - variable count/size (under-complexity, fragmentation, lack of smoothness/curvature)
    - wrong context for a pattern
    - over-repetition
    - over-similarity to previous information (lacking expected change structures, like change trajectory & types)
    - no matching reason/intent/priority for deviations from archetypes/patterns
    - over-correction when integrating a variable
    - variables identified in isolation
    - most clearly/measurably different variables identified
    - structure organizing variable structures (randomness injection points, enforcement gaps, info imbalances)
    - over-simplistic or erroneous automated sub-components
    - improbable level of randomness
      - clear composition of core patterns
    - sources of randomness
      - errors are evenly distributed among more complex adjacent sub-components not expected to change as much

## filters that reduce the problem space
    - identify the worst error types, as assumption combinations having the lowest solution metric fulfillment if incorrect
      - in the problem of 'predict cat vs. dog', the worst error types are:
        - an object from one category having all the features used to differentiate between categories, but with variable values of the other category (cat having dog features)
        - an object that is artificial identified as real (cat robot identified as a cat)
      - to predict these error types, certain concepts need to be inferred
        - the concept of 'agency' to design a machine that looks like an animal
        - the structure of 'false equivalence' to design situations where features would look like a category but not actually be that
    - identify all the feature ranges where it would be impossible to give high-accuracy answers (ai-generated cat image vs. real image)
    - organize these filters in a useful sorting structure (network, tree) can reduce the computations required to solve for a prediction function, such as:
    	- placing the most-reductive solution filter first, if the info required for that filter is already available
    	- placing a filter after another filter that generates/identifies the info required for the second filter

# problem-solution format maps

  - the problem is the solution in a different format, or a piece of the solution (problem being a sub-optimal state to optimize, or a difference that shouldnt occur, and the solution being a set of constraints forming boundaries, or an optimal structure to construct)
    - filling problem
      - missing info problem: the solution format is the complete structure
      - optimization problem: the solution format is the variables/system organized to comply with/fulfill the metric to optimize 
      - aggregation problem: the solution format is the aggregation method to form a structure (like combining core functions to get a function for an intent)
    - limit problem
      - constraint problem: the solution format is the removal/invalidation of that constraint
    - reduction/decomposition problem
      - complexity reduction problem: the solution format is the set of variables that reduces complexity of the problem
      - randomness reduction problem: the solution format is the set of variables that can replicate a semblance of randomness
      - problematic structure: the solution format is reducing the structure (identifying variables & invalidating those variables)
    - organization/mapping problem: the solution format is the set of relevant components in the right structure (positioning & connecting them)
      - conflict problem: the solution format is positioning the conflicting problematic vectors so they dont intersect
      - balancing problem: the solution format is the distribution of resources nearest to a balanced state (subset of matching problem, by matching distribution across positions)
      - combination problem: the solution format is the set of components in a combination structure that doesnt contradict combination rules (components fit together, like 'finding a system where a function can execute')
        - connecting problem: the solution format is the set of functions that connect the components, in the position where they act as connectors
    - finding problem
      - discovery (insight-finding) problem: the solution format is the set of generative/distortion/core functions or the set of filters to find the insight
      - route-finding problem: the solution format is the route between two points that doesnt contradict any solution constraints and/or optimizes a solution metric
    - other solution formats would be for adjacent/causal problems, solution formats that invalidate solving the problem, etc


# insight path examples

## solution automation workflow insight path examples
    - find an example & generalize
      - find core/unit objects, find example using those objects, & generalize
    - find an example & counterexample & connect them
    - execute a problem-reduction function/structure/question sequence
    - execute a solution-space reduction sequence before solving for remainder problem
    - run query to find interacting interface structures, then apply solutions for that specific problem space's interface network
    - identify vertex variables first & approximate
    - identify problem types & corresponding solution aggregation method for that set of types
    - identify alternative problems to solve (like whether to solve for organize, format, select, re-use, derive, discover, build, diversify, optimize, distort, or combine problems/solutions) & apply problem selection method, then solve

# structure examples

## error structures
	- errors defined as differences between intended/actual structures

## bias structures

  - identify bias structures as output of operations on structures, or by missing structures that cause bias
    - bias is a filter that leaves out relevant information
    - 'facts without connection to meaning' is a biased priority (current state of truth) and a biased lack (ignoring potential truth & potential connections that change the meaning/position of facts)
      - example: if you just focus on data set facts, you miss other facts (contradictions, counterexamples, alternative conditional variables/functions), as well as opportunities to derive other facts from the data set (given the favorability of the data set to influential entities, we can derive a guess that other facts might imply a different conclusion), and the connections between the data set facts & other facts (other facts imply a different cause than the data set facts) as well as the meaning of those connections (why this data set was selected)

## info asymmetry
	- associated with an info loss ('missing' or 'gap' structure) in a particular direction between info types/formats/positions, rather than just an info imbalance or a mismatch

## difference vs. similarity

    - similarities between difference & similarity
      - distance metric
    - differences between difference & similarity
      - amount of info that needs to be stored for a complete accurate description ('what something is not' may require more info to be stored compared to 'what something is')
    - the position of difference between difference & similarity may be on non-opposite positions on a circle depicting routes to get from difference to similarity
      - this is bc a similarity is a degree of difference (low/zero difference) & so is a difference (higher degree of difference that can be measured or is observed as noticeably different compared to a similarity)
      - the structure may be a circle or other loop bc if you stack enough differences, eventually you may generate the original object
    - the conversion of difference into similarity is based on the concept of a threshold, where a difference acquires enough similarities to similarity to cross the threshold or vice versa
    - the gray area in between the two concepts & surrounding the symmetry of the threshold also conflates the differences between the two concepts, making the difference not a simple 'opposite'

    - example: spectrum structure
      - handles different cases like 'near low/high/average value' (like between 0 & 1), which have differences in adjacent change types to produce relevant objects (like an integer)
        - change types like 'small change to produce an integer', 'doubling to produce an integer', etc
      - the isolated relevant difference structure (without additional info) 
        - the average value, which has multiple difference types in adjacent change types
      - conditional relevant difference structures
        - if the nearest integer triggers other change types, the value near that integer has a relevant difference structure

    - example: position structure
      - similar positions will be near according to the distance metric, creating a radius of similarity, which results in emergent structures of a boundary, center & circle
      - different positions can be represented as a structure lacking a circle/boundary/center
      - the differences in similarity/difference structures have emergent effects & coordinate with different interface objects (like adjacent structures, change types, relevant objects, etc)
        - a lack of an object can be used like other gap structures are used (as a filter, container or template)
        - an object can be used as a component or other base object to use as an input

    - this is why differences are not just the 'opposite of similarities' - it leaves out information like:
      - similarities of varying relevance between similarity & difference (both use a distance metric)
      - the reason why a difference is used vs. a similarity (like 'filtering' intents)
      - emergent/adjacent/relevant structures of similarity & difference, embedded in different structures (position/spectrum)
      - info about the structure of difference (difference paths/stacks/layers/trajectories), which may vary in ways that similarities do not
        - this indicates the important point that similarities are insufficient to predict differences
      - if similarities were equivalent to differences, you could use similarities to derive all info, reduce all uncertainty & randomness, and solve all problems - which is not guaranteed
        - meaning 'derive structures outside of the universe, using info from inside the universe' 
      - similarities may have similarities to each other, more than similarities to differences
      - randomness has a similarity (in outcome probability), but is better than similarity as an input to generate difference structures like uncertainty

## document uncertainty structures like randomness collisions & structures that produce certainty (combinations that stabilize)
    - randomness collisions generate structure
      - structure being the stabilized interaction of information (staying constant long enough to attain structure)
      - randomness being a lack of information (like a star or circle with equally likely directions of change)
        - where influences are equal enough in power to leave no clear priority of direction favoring one over the other
      - when an info lack interacts with an info lack, they may not generate another info lack, but a structure stable enough to organize them, depending on the angle/type of interaction and whether the info lacks are a similar or coordinating type


# generate solution automation insight paths

## generate solution automation interface query

    - iterate through interface objects (change type, problem type, assumptions, etc)
      - find interface objects in a problem space 
        - filter by relevance structures (like interaction directness/causation, such as change hubs)
          - apply problem structures related to relevant structures
            - apply solution structures (like organization structures, like a sequence of tests/queries) to problem structures

    - specific logic automation example
      - check for missing relevant info in info found with variables
        - change to add earlier window to mtime param bc its out of error window
      - find interaction type & change type in info metadata (filename, modification time relationship)
        - any logs changed in later would include logs modified earlier bc of lack of incrementing/rollover, so mtime increase is unnecessary
      - check assumptions for requirements
        - mtime param unnecessary bc most logs would be modified in original mtime param
      - check for relevant change-aggregation objects in structure (event objects in a sequence structure)
        - significant date (upgrade, reboot) was within original mtime param which could be a factor in error so mtime param is necessary

## to generate solution automation workflows
    - combine problem types
      - a reduction/decomposition problem + a filling/aggregation problem = the solution automation workflow 'break a problem into sub-problems, solve sub-problems, aggregate sub-solutions'
    - combine structures & connect structure combinations by problem types
      - the structure combination of 'a sequence injected in a network' is a structure matching a 'route finding problem', so apply solution structures that find a route in a network, such as filters using metrics or rules that can filter routes by which routes dont contradict rules
        - the solution automation workflow is 'find structures relevant to resolving problem structures like inequalities in other structures' (inequalities like the difference between start/end positions)
        - the workflow matches 'sequence in a network' with 'route filtering structures', connected by the problem format 'find a route'
    - combine structures & core functions
      - the structure of the core function sequence(find, apply, build, filter) = matches solution automation workflows like 'find components which, when this function is applied, can construct this structure, complying with these solution metric filters'
    - combine components of solution automation workflows (functions, queries, interfaces, problems/solutions, structures) that have a valid input/output sequence

## example of insight path to generate a solution insight path for a problem

    - apply solution structures like:
      - balance between supply/demand
      - maximizing benefit/cost ratio

      to problem structures (metadata like available/missing info) to produce solution (insight path) variables:
      - cost of ignoring/focusing on info vs. benefit of actions like executing functions
      - cost of acquiring more info vs. benefits of applying quick best-case solutions
      - supply of available info vs. demand for info to solve the problem

    - then select/change variable structures (variable values & variable sets) to produce components of solutions (insight paths) for a problem:
        - concept components: low-impact variables, high-variation variables, most causative problems, worst/best case context
        - function components: filter (ignore/focus/assume), prioritize (set as primary intent), apply structure (like subset)
        
    - then apply structures to combine insight path components for a problem:
      - 'ignore low-impact variables to prioritize high-impact variables'

    - then filter by problem structure (intents, sub-intents) to re-integrate insight paths with problem:

      - apply filters like:
        - 'is a direct cause of the problem ignoring local/contextual/worst-case/probability info?'
        - 'does a function applied to a component tend to cause problems in complex systems, and is this a complex system'

      to produce reduction of possible solution insight paths like:
        - 'then ignore insight paths using those structures'
        - 'then apply further filters to check for a reason (possible benefit) to ignore that'

    - functional insight path (what to execute) :: filter insight path (what to rule out or focus on)
      - 'breaking down problems into sub-problems' :: ignore non-isolatable problem types & non-combinable solution types
      - 'identify worst case scenarios first and solving those in order' :: ignore less harmful problems (local/output problems) to prioritize more harmful problems (causal problems, problem types)
      - 'identify vertex variables & standardize to them, using solutions that act exclusively on them' :: ignore less impactful variables to address root causes
      - 'identify position of problem in causal network and apply solutions local to that context' :: ignore systemic solutions to avoid side effects
      - 'find alternatives to solving a problem (delegation, solving abstract version)' :: ignore specific solutions or move problem position
      - 'identify problem type & apply related known solutions' :: assume problem type can be identified & covers enough of the problem & is abstract enough to apply related solutions with effective impact


# interface query design logic examples

## example of identifying query-changing (invalidating, embedding, stopping) conditions during an interface query or interface query-generating query
    
    - queries are implementation of components of control flow (supply: decision/action/function, demand: problem/error/task/conflict/limit)
    
    - example: execute a query to find structures of 'high-variation' in a data set
      - identify relationships within a variable (across potential values for that variable)
      - identify relationships within a variable's state changes (across potential values for that variable across its lifetime)
      - identify relationships (interaction functions & types) between variables
      - identify relationships between variable structures (subsets, combinations, alternatives) & variables
      - identify variable types (proxies, root cause, interdependent)
    
    - query-changing conditions
      
      - standard control flow conditions

        - query-stopping condition is where its clear the data cant:
          - fulfill the optimization metric or fulfill it more
          - find the information or find any more
        
      - meta conditions

        - query-invalidating condition is where the data set invalidates the concept of variation or data 
          - when a query has identified type/relationship/pattern information invalidating the data
          - when the data is not a source of truth (state has changed but data has not & has no variation and is not data anymore, if data is a source of truth)
          - when a query has identified a function to reduce the variation/data without losing info
            - or create a function to do so
            - or identified a need to trigger an embedded query to create a function to do so
            - and has identified & organized resources to create that function or execute that query, after identifying its need for the function/query (AGI)
        
      - query interaction conditions
        
        - query-connecting condition is where the query identifies that another running or previously run query might have identified useful info relevant to its task

          - examples:

            - a query that identifies similar structures 'difference-reducing/increasing structures' or 'sismilarity-filtering structures (leaving just difference structures)' might find the high-variation structures quicker
              - this alternative query could be found by applying the concept of 'similarity' to the 'query' object, allowing for the possibility that the query was almost correct

            - a query to identify query metadata and apply those metadata variables to generate other alternative queries
              - query metadata examples: accuracy, side effects (like unintended functions built during processing)
                - generating more accurate or faster alternative queries by applying optimization structures (like alignment, info/function re-use, etc)

            - a query that identifies 'change-reduction' structures (like types or interfaces) could be more efficient than this query to find high-variation, which may miss embedded query opportunities for embedded structures of change in the data (data about variables/functions)
            
              - how could the original query know to check for such a query running in parallel?
                - identify problems with its own query metadata (execution, design, connectivity, progress, probability of success) & calling query to generate alternative queries that optimize on problematic structures like performance metrics (execution time vs. relevant info found)
                - identify problems in the original problem of the query (sub-problems of original problem, encountered problems like a missing function to derive)
                - apply structures of robustness by default, like apply 'alternative' to 'query' object to run alternative queries by default, filtering by difference or relevance to maximize probability of finding useful info
                - identify relevance structures that would be useful (such as useful for sub-problems identified initially or encountered during execution, or planned problem to solve later in original query)
                  - it could apply the concept of 'type' to itself (self-aware that it's a query) by abstracting the 'query' concept, identifying its type, and querying for other queries of that type
                    - identify that its processing was not finding info as quickly as typical queries asked to find structures of concepts like 'variation' 
                  - it could identify that there is another route to the info during its processing
                    - by examining data for variance, find a structure consistently causing/generating variance that relates to change reduction
                  - it could execute some of its processing using conceptual core structure analysis, creating combinations to identify concepts (related to query concepts like variation & data) like 'change reduction'
                  - it could identify that a query-invalidating condition that reduces the variation in the data set has been met in another query
                  - it could use concepts like 'equal' and 'opposite' to apply a counter-query to check for the opposite structures, which can be faster
                    - just like checking for a difference may be faster than checking for a similarity or vice versa, or checking for a limit/conflict may be faster than checking for a function
                  - it could apply concepts related to the definition of 'change' such as 'potential', and identify that potential increases with more change structures, particularly change-expanding structures, the opposite of what this query is looking for
                
                - then using the output of such analysis types that can supply relevance structures, applied at intervals or decision points during its own processing, it could check for a query running with intent (or inputs, side effects/outputs during/after processing) to:
                  - identify 'change reduction' structures
                  - generate a function to generate 'change reduction' structures
          
        - query-embedding condition is where an embedded query is required
          - in a data set is data about functions/variables, an embedded query might be used to find embedded variation in embedded function/variable relationships/structures
            - data about functions/variables would expand the possible variation in the data set within each column/variable, with change types (functions/variables) as data
    
    - condition types
      - invalidate query (compare & find alternative solution)
      - embed query (correct an info gap)
      - connect query (delegate processing to another query)
      - stop query (apply a metric)

# interface query examples

## interface queries for problem 'find a prediction function'

	- apply information (definition) interface
		- apply error definition routes/attributes/functions/objects/structures
			- identify error types for problem 'find a prediction function' to use as filters of solution space
			    - false equivalence
			      - similar routes to different answers
			        - this implies similar patterns in variable structures & interactions across data groups
			      - overlap
			      - lack of differentiating variables in data set
			    - false difference
			      - merging/imminent similarity/equivalence
			        - functions that can act on other functions to produce a false or real equivalence to another function
			      - alternative routes to the same answer
			        - identify all the alternative structures (routes, combinations, trees) to an answer between function components like variables, data sets/subsets, & neural net components like weight path patterns, and the differentiating factors & vertexes, then use that to implement a filtering structure to sort through them to rule out the most possible answers the quickest
			      - alternative answer types
			        - identify all the different variable/function combinations that could create the most differences in similar answers (such as different types or contexts like a separate function for outliers), and a filtering structure to apply these as variation-reduction functions
			      - these filtering structures can act like interfaces, reducing variation in the possible answer set
			    - equivalent combinations
			      - alternative variable subsets that act as proxies to an answer
			    - equivalent variable structures
			      - find variable structures like functions that approximate other variable structures like variable networks

	- apply change interface to find variables in a problem statement
	    - find isolatable change types
	      - if the problem is 'predict movement of object', this means: 'find change in possible orthogonal directions'
	        - filter out redundant variables (like if variable A/B + randomness constant can be replaced with variable C + another randomness constant)
	        - filter out variables or variable structures like combinations that look like randomness to leave sets of variable/s
	          - find prediction function for variables with randomness excluded
	          - apply degree of randomness with randomness accretion patterns & interaction structures (like other objects on interaction layers) to prediction functions once variable dependencies are described, to generate prediction function set or prediction function with distortion vectors for possible ranges, then test on data 
	    - variable sets that cant be filtered out can be considered sub-problems to solve ('filter out this variable set') in addition to the original problem of 'finding a prediction function', as extra filtering tests to apply before the solution is selectable

    - interface query using concept-structure interfaces for problem 'find prediction function' 
      - find solution filters
        - find range of error allowed for solution
      - convert to problem interface
        - predict missing info 'future state of variables' with input 'past information'
        - standardize to structural interface
          - find vertex concepts
            - 'find prediction function' using past information involves:
              - risk structures like: possibility that an unknown structure is causative
              - randomness structures like: possibility that known structures will be distorted by randomness
              - change structures like: possibility that known structures will change & info needs to be found/derived to update variables
            - combine risk structures, randomness structures, & change structures
              - filter which combinations match data
                - filter which combinations match data within range required by solution filter

  - general interface query example for 'find prediction function'

    - change: find highest-variation variables in problem statement
      - structure: find combinations/subsets of variables
      - cause: find dependency structure of variable subsets
        - function: find input/output sequences of variable subsets
        - structure: filter the sequences by whichever sequences link the source/target structure
          - problem: solve sub-problems of organizing variable subsets
          - structure: aggregate sub-problem solutions

  - specific version of general interface query example for 'find prediction function'

      - change: find highest change problem variables in problem statement
          - which probability distribution it is
          - variable values given
          - whether alternate probability distributions can be ruled out using constraints/assumptions/parameters/change types & other info of problem
          - sub-problems
          - sub-problem structure (organizing the sub-problems)

        - structure: find subsets of variables
          - example problem variable subsets:
            - missing info + variables values given + sub-problems
            - probability distribution + variable values given + other problems or problem patterns

        - cause: find dependency structure of variable subsets
            - missing info + variables values given + sub-problems
              - with the missing info & variable values given, you may be able to infer the probability distribution (though not always if the problem statement is ambiguous) and derive the sub-problems to solve
            - probability distribution + variable values given + other problems or problem patterns
              - from the probability distribution & variable values given & other problems, you may be able to infer what the missing info is given questions usually asked with that distribution

          - function: find input/output sequence of variable subsets

          - structure: filter the sequences by whichever sequences link the source/target structure (variable values, probability distribution & missing info, 'probability of event')

            - problem: 'predict probability of event A given event B & some parameter/condition C'
              - sub-problems
                - identify problem metadata (probability distribution, variables & values) in problem statement
                  - identify missing info (specific problem to solve, like 'find the missing info that is a probability of a specific event')
                - identify alternate interpretations of problem
                  - filter alternate interpretations (to likeliest or the interpretation with no contradictions)
                    - match variables & values in problem with parameters of the probability distribution or relevant functions
                      - filter functions to functions with output type 'probability'
                        - filter functions to functions with specific output probability matching missing info
              - aggregate sub-problem solutions
                - missing info:
                  - apply variable values to relevant functions to generate missing info (specific output probability)

## apply distortions to vertex interface queries for solution intents

  - vertex interface query: high-impact query which can be used for finding optimal solutions quickly or used as a base for other interface queries in interface query design

    - query: reverse engineering solution metric with core structures as filters to find relevant metric structures

      - problem statement: 'find individual unit metric value in a container having equivalent & different components, without a function to measure individual unit metric value, and given total container metric value & unit count'
        - find relevant structures of the metric
          - apply insight relevant to 'calculations': 'apply the same standards when calculating if possible'
            - apply concept of 'similarity'
              - find relevant structures having the same metric
                - find relevant structures to 'unit'
                  - apply core concepts/structures to problem system structures
                    - apply core structures of 'combination'
                      - relevant structure: set of units, having an aggregate metric, usable input to an averaging function
                    - apply core concept of 'opposite' or 'not equal' and the core concept of 'total' (the complete set of all components in container)
                      - relevant structure: set of non-unit components in container, having the same metric, usable input to a subtraction function
          - find most measurable structure (with greatest accuracy or fewest steps) out of the relevant structures having the same metric
        - find calculation relationship between adjacent proxy metric of relevant structure and original solution metric (individual unit metric value)
          - calculation relationship between sets of not-equal components and equal components to the individual unit metric:
            - calculation relationship: "subtract not-equal component set metric value from total value, and divide by unit count to find individual unit metric"
          - to find this relationship, execute the opposites/reversals of the operations to find the relevant structure metric values
            - 'subtract' is opposing function of 'combine'
              - 'combine' was executed to get the list of sets of components (not-equal components & equal components)
            - 'divide' is opposing function of 'combine'
              - 'combine' was executed to get the set of equal components, relative to the individual unit
            - these two combine operations were used to create a path from the individual unit to the set of total components in the container
            - they can also be applied in reverse to get from the given total container metric value to the individual unit metric value


# solution format examples

## optimized network structure

    - the optimized network can be structured as versions for different intents like:
      - lowest-memory generator: the average network + distortion functions
      - relevant generator: the network nearest to the most useful versions of it
      - quick generator: the network with the components that can build other versions at lowest cost
      - core generator: the network with core components to build all other components
      - adjacent core generator: network with core components at an abstraction/interaction level where they are most adjacent (mid-level functions as opposed to granular functions or high-level agent-interaction functions or conceptual functions)

    - the optimized network (ark) has the interface components necessary to solve any problem, with no extra components
      - it has one of each parameter of required components (like definitions, bias/randomness/error structures, interfaces, core/change functions, etc) which provide enough functionality to decompose & fit all discoverable information into a system of understanding
        - for example, one example of each opposite end of a spectrum & the average in the center, or the average + distortion functions to generate the other possible values

    - can probably be adjacently derived from subatomic particle interactions, which implement the core objects of interfaces like cause & potential

## efficiencies from missing components
    - some functions are generated more quickly without a component, bc of the needs that the lack generates, which focuses generative processes on building alternate functions to fill the gap
    - this can be used as a way to predict what tasks the optimized network with missing components would be relatively good at
    - missing component metadata
      - how adjacently it can be learned/generated/invalidated/delegated/identified/borrowed
      - how likely it is to be learned/generated/invalidated/delegated/identified/borrowed
      - whether another missing component can be used instead
      - whether the system missing that component should be changed instead
      - whether a system having that component succeeds at the intent task (& fails at others currently fulfilled by the system missing that component)
    - example:
      - not having a function incentivizes:
        - identity: development of that function
        - abstraction: development of generalization of that function, parameterizing that function intent
        - alternate: development of a proxy or alternative or invalidating function, making the function itself unnecessary
        - cause: development of structure/function/attribute that invalidates the original requirement metadata (priority, intent, dependency structures), not just invalidating the function
        - alternate format: development of a structure/attribute that replaces the requirement for the function or allows the function to be generated as needed
        - derivation: developing a function to learn/derive/identify/borrow/cooperate functionality from external info, to generate functionality as needed
        - core: developing components capable of building all functions to generate functionality as needed
        - subset: developing components of that function so the function & other functions can be generated as needed
        - combination: development of a function capable of fulfilling that intent & other intents
        - distribution: distributing functionality-generating methods to all nodes requiring functions
        - organization: allocating gap requirements (uncertainties) to the gap in functionality (example: keep the gap so you can apply methods as a test to resolve the gap)

# apply structures to solve a problem (complete a task, fulfill an intent/requirement, apply a solution metric test)

## example of applying structure to identify rules that violate a metric 

    - requirement like: 
      - 'dont exacerbate inequalities'
      - 'protect minorities on the disadvantaged side of an inequality'
      - 'identify advantaged side'
    - power structures: required or non-specific/universal resources (such as inputs to any function, like 'energy' or 'information')
    - inequality structures: differences in distribution of required resources
    - generate structures that would exacerbate inequality structures
      - assumptions in rules (lack of guaranteed potential to follow rule)
        - rule 'close malls after business hours'
          - rule structure: 'limiting supplies' (access to facility)
          - rule assumption: that they have alternative supplies
        - rule: 'fine for not wearing mask'
          - rule structure: 'requiring function' (purchase mask)
          - rule assumption: that they have inputs to a requirement
      - these assumptions would disproportionately increase inequality's disadvantages in distribution
      - 'disadvantaging rules/assumptions' can be distributed more evenly or to offset inequalities

## example of applying structure to generate problem types

    - for instance, a common problem type is a mismatch/imbalance
      - by applying the 'mismatch' to the cost/benefit relationship, you get an 'inefficiency' problem type, which can be defined as a mismatch/imbalance between the cost & benefit, favoring the cost side (which is the negative version out of the cost/benefit combinations, negativity being part of a definition route of a problem)

## example of finding alternatives (alternate variable sets) in a problem space (exercise) for problem of 'predicting a change type' (predicting motion) 

      - apply interfaces to find relevant structures

        - exercise variables:
          - info (about optimizations, possibilities, rules, metrics)
            - attention/memory to focus on, remember & apply info
          - patterns
          - structures
            - point (metric threshold values, change points, decision points)
            - sequence: 
            - combination: multiple variables to make a decision
            - limits: time limits, energy limits
            - context
              - health
              - energy
              - environment
                - landmarks
                - agents
                - interactions/events
          - time
            - time structures (alternation, number of seconds, continuity of pattern applied)
          - functions 
            - core functions (test, start/stop, switch, remember, identify)
            - interaction level functions (decide when to speed up, plan decision points)
          - concepts
            - energy
            - agency
            - intent
              - exercise intents: recover, rest, test/find limit, test function, switch energy sources, apply info, identify landmark, align with music
              - other intents: what to do after workout, scheduling limits to work around, listen to new music, listen to music limited number of times

      - apply interface structures (like combination) to relevant interface structures found in problem space (like 'health' concept) to generate solution space (possible prediction variable sets)
        - alternative variable sets that can predict motion:
          - apply filter structures to problem & solution structures like 'opposite' (what cant be a solution)
            - time cant be used as a base on its own bc usage patterns may offer the illusion of equivalent alternatives that are actually different
              - example: pattern 'a-b-c' may occur just as often as 'a-b-d' without any distinguishable signals using available time info, so other interfaces need to be applied to predict c or d, such as contextual/intent probabilities, or patterns like intent patterns or change patterns 
          - agency rules
            - agents have known intents, which interact in a known way
          - interaction rules
            - energy, time, agents, & health interact in this way
          - energy rules
            - 'energy can be used to produce energy in other formats'
            - 'stored energy can replace agent prioritization'
            - 'excess energy can have these outputs when used optimally'
            - 'energy efficiency increases with usage'
            - 'high variation in usage increases energy coordination & distribution'
            - 'brain & muscle energy are related, in a pseudo-tradeoff'
            - 'high variation in energy usage can offset energy plateaus'
          - variable interaction patterns
            - 'using n number of variables to make a decision only occurs once out of every x decisions'
            - 'applying previously applied variable interaction rules is most common'
            - 'excess energy results in higher variability of variable interactions'
          - concepts
            - concepts & concept structures (concept set including 'energy' or 'health') can predict independently of other variables bc theyre a low-dimensional (conceptual dimension) representation of high variation (motion)

