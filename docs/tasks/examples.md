# structure examples

## structural concept definition routes
    - nothing (lack) structures, as opposed to randomness (lack of differentiating info among possibilities)
    - opposite vs. lack (of common attributes/values, connections, similarities, spaces)
      - opposite requiring a potential for extreme values to occur in a structural possibility where difference can develop
    - thinking definition as 'applying structure to uncertainty'
    - reasonable (making sense) definition as 'fitting an existing structure, like a pattern, without invalidating contradictions' 

## error structures

	- apply definition of errors as structures of difference (what is not correct, meaning different from correct) to generate error types (structures of difference, like stacking variable permutations/distortions or generating new variables) and error patterns
	    - create error types of ai using core combination generative function
	      - includes generating error type structures (combination of error types)
	    - identify error type patterns (when differences accrue in this pattern, an error of some type is likely to occur)
	    - create ai algorithm that is different in some variable from error type algorithms to guarantee an algorithm without those known error types
	    - identify interface queries (or ai algorithms) that generate error types to use as filters to differentiate & guide design of new queries algorithms
	    
	    - example of error type in structure: 
	      - any distortion can be used as an asset, & every position has an error inherent to its structure
	        - for example:
	          - 'occupying the center position' has:
	            - errors: having to do extra work to get to a position where it can handle less adjacent (outlier) problem types
	            - advantages: its work is distributed among many positions in every direction (many positions are adjacent) so if the problem is solvable with an adjacent position, and encountered problem types vary a lot, the center has an advantage
	            - 'a position in between most common error types' is another similar position that would have an advantage inherent to its structure, with the cost of having to do more work to get to a position where less adjacent error types are solvable, the less adjacent error types being more common than adjacent error types, but still less far than the average cost from other positionss
	          - 'having the most power' has:
	            - errors: intent of 'requiring dependency', inability to delegate, over-responsibility (imbalance in blame allocation), boredom
	            - advantages: freedom in movement/change, ability to handle stressors, ability to make decisions that favor itself or its goals
	      
	      - how to derive the error type from this distortion structure 
	        - distortion structure: 
	          - 'too far in the direction of power centralization'
	            - associated objects (inputs/outputs) to components (power)
	              - with power centralization (power being at least an input to everything), other things are also centralized, like inputs/outputs/sub-processess of power (responsibility, decisions, dependency)
	          - 'too central to reach outer positions quickly'
	            - variables (cost, function, priority) in structures (paths) to similar objects (positions)
	              - average cost to reach other positions may be lower than other positions, depending on density distribution or commonness of adjacent positions' associated error types, but cost to reach outer layers would be higher in the absence of efficient connecting functions
	                - this error structure can generate other error structures:
	                  - bc it cant reach outer positions quickly:
	                    - it cant identify/handle external stressors quickly without building functionality to offset that error, like an alarm system to get info to the center faster
	                    - it cant quickly generate new outer positions like it can generate new adjacent positionss

	- generating error types
	    - apply core structures to problem components
	      - lack error type
	        - lack of resource
	          - lack of dependency
	    - apply definitions of error/problem components
	      - apply functions that can generate an error type according to its definition
	        - incorrect: apply changes to variables to generate incorrect values
	        - imbalance: apply distribution function to create imbalance of a resource
	      - apply core functions of error types to problem space objects or interface objects
	        - apply core change functions to:
	          - structure/position/format
	          - data
	    - apply definitions of optimal/solution components
	      - apply core structures to definitions of solution (functional/stable/optimal) states
	        - 'requirements fulfilled': change requirements to create error types like imbalances or lacks
	        - 'functionality working': break functionality
	        - 'stable system': overwhelm the system
	        - 'optimal system': solution metric unfulfilled
	      - generate error types by applying distortion functions to an origin optimal or stable (error-free) state(s) to generate deviations from optimal state

	- error structures:

	    - over-structurization (specification) of an uncertainty/variable (assumption as fact, variable as constant)
	    - over-correction of an error
	    - over-prioritization
	    - over-reduction (over-simplification)
	    - over-variability (over-complication)
	    - misidentification of minimum info to solve
		- distortions from expectations

	    - false equivalence structures
	      - 'lack of functionality' bc of root cause of 'lack of memory' or 'lack of functionality to build functionality' or 'lack of intent for that functionality'
	      - the memory lack can look like a lack of ability, but its a false equivalence/similarity caused by a lack of an input resource, within a range of change potential where the memory lack & ability lack ranges overlap

	    - signals of the error type 'low-dimensionality':
	      - when motion approaches the solution metric (avoiding the error classification of not equaling the solution metric value), but never reaches it
	        - example: 
	          - vertical dimension: robot fell onto another level vertically but is still moving toward destination as planned
	          - alternative agent/force dimension: robot fell onto truck and is moving toward planned destination temporarily
	          - time/speed dimension: robot encountered barrier preventing it from reaching its planned destination in under the planned time limit

			- errors defined as differences between intended/actual structures
				- errors are a difference type in a specific structure (between expected/actual values) so theyre useful as example core problem signals
			      - stacking errors may be a better way to frame problems than other interfaces
			        - the level of randomness captured by the error structure
			        - errors can function as limits as well as difference types building a problem structure

	- success vs. error structures

	    - when applying reductive insight paths to reduce solution space, identifying the set of unique isolatable component types (success, neutral, error, metric, function) on an interaction level is necessary to isolate subsets

	    - applying causal interface to problems (like 'find a prediction function') is required for some intents (like 'reduce error' or 'handle change')
	      
	      - success cause structures (reasons, or why)
	        - finding the structure of similarity that explains 'why' an algorithm worked, such as a similarity in the form of an alignment in number of updates & degree of distortion allowed from a base function
	      
	      - error cause structures

	        - error cause structures can be used to predict errors & used as filters to reduce solution space to similarity structures
	          - example: structures of difference like difference between core/required functions

	        - error type causes
	          - other error types (lack of rule enforcement)
	          - variable structures (irrelevantly similar variable structures, missing variables, variables that are constant, variable allocation/interaction)
	          - bias error type structures:
	            - variable combinations/connections that should be disassociated

    - when should errors be allowed to continue: 
      - when they dont impact system functionality, dont interact with other errors to form cascades/compounding errors, and provide useful signals of unhandled variance
    - when should motion be allowed in the direction of risk (risk of error types): 
      - when uncertainties exist between alternatives

    - flexible abstract/conditional/temporary error definitions to allow for beneficial errors & error-correcting errors
      - example: 
        - 'two wrongs make a right'
          - when a robot instructed to go in a direction is forced off its trajectory by the first error, it has to make another error to get back on track, if an error is defined as 'motion in any direction different from original planned direction'
          - a solution is a definition of error types that is:
            - abstract: any intentional motion that brings robot nearer to its goal is not an error
            - conditional: any motion to correct an external error is not itself an error
            - temporary: motion in a direction different from planned direction sequence is not an error in some temporary contexts
  
## stupidity structures (like bias)

  - apply anti-stupidity structures to neural network structure 
    - lack of learning functionality
      - inability to remember (identify relevant info quickly when new info isnt necessary)
      - inability to identify relevance structures (meaning, usefulness, direct causation)
      - inability to optimize (identify a quicker route to an insight, like an insight path)
      - inability to model structures (enough memory to store a different structure, ability to explore/change it like a visualization)
      - inability to simulate difference structures (contradictions, paradoxes, lack of similarity)
      - inability to direct thoughts (focus)
      - inability to forget sub-optimal/inaccurate rules (bias)
        - function to apply bias structures to a neural network structure
          - thinking benefits from bias removal
          - remove bias structures in neural networks to improve their thinking capacity
          - example
            - apply removal of 'simplicity' bias in a neural network structure
              - simplicity (specifically over-simplification) definition on structural interface: 
                - lossy lower-dimensional representation
                - low-cost representation with relatively reduced learning reward
              - the simplicity bias shows up in a neural network structure in many possible positions
                - for example, a pooling function, which has no reason to aggregate other than adjacence, which may not be an indicator of relevance
                  - find the structures that can build/derive/apply/store relevance and remove structures with artificial relevance
                - general default params also tend to store simplicity where it's not needed
            - apply removal of 'similarity' bias 
              - similarity bias structural definitionss
                - relatively adjacent in variable values according to a distance metric applicable & relevant to that variable
              - the similarity bias shows up when adjacent structures are given relevance/meaning that they may not actually be capable of storing/building/deriving, like subsets of inputs or clustering thresholds

  - bias structures: 
    - bias cycle: where specifically/partially false statements are falsely categorized as completely false, which triggers increase in distorted view of the group making the miscategorization error
      - saying a specifically/partially false negative thing about a group often has a partially true sentiment backing it (most people in any group do negative behaviors enough to trigger negative sentiments), so even if the specific negative thing is wrong, the sentiment might not be
      - the lack of acknowledgement of their own negative behaviors by the group saying the specifically/partially false statement also triggers the same response in the group making the miscategorization error (the group saying the specifically/partially false statement is doing a negative behavior, so the miscategorizing group has a negative sentiment about them, and often says specifically/partially false negative things about the group)
    - conflating stereotype ('false statement about a group') with 'a statement about a group that is more true of a higher ratio of that group than it is of other groups'
    - stupidity manifests as similar structures (fulfillment of low expectations) across groups in response to low expectations, leading to feedback loop

  - identify bias structures as output of operations on structures, or by missing structures that cause bias
    - bias is a filter that leaves out relevant information
    - 'facts without connection to meaning' is a biased priority (current state of truth) and a biased lack (ignoring potential truth & potential connections that change the meaning/position of facts)
      - example: if you just focus on data set facts, you miss other facts (contradictions, counterexamples, alternative conditional variables/functions), as well as opportunities to derive other facts from the data set (given the favorability of the data set to influential entities, we can derive a guess that other facts might imply a different conclusion), and the connections between the data set facts & other facts (other facts imply a different cause than the data set facts) as well as the meaning of those connections (why this data set was selected)
  - neural network with anti-bias structures built in (a complexity structure, a difference structure, etc) to correct error types from common biases

## info asymmetry
	- associated with an info loss ('missing' or 'gap' structure) in a particular direction between info types/formats/positions, rather than just an info imbalance or a mismatch

## difference vs. similarity

    - similarities between difference & similarity
      - distance metric
    - differences between difference & similarity
      - amount of info that needs to be stored for a complete accurate description ('what something is not' may require more info to be stored compared to 'what something is')
    - the position of difference between difference & similarity may be on non-opposite positions on a circle depicting routes to get from difference to similarity
      - this is bc a similarity is a degree of difference (low/zero difference) & so is a difference (higher degree of difference that can be measured or is observed as noticeably different compared to a similarity)
      - the structure may be a circle or other loop bc if you stack enough differences, eventually you may generate the original object
    - the conversion of difference into similarity is based on the concept of a threshold, where a difference acquires enough similarities to similarity to cross the threshold or vice versa
    - the gray area in between the two concepts & surrounding the symmetry of the threshold also conflates the differences between the two concepts, making the difference not a simple 'opposite'

    - example: spectrum structure
      - handles different cases like 'near low/high/average value' (like between 0 & 1), which have differences in adjacent change types to produce relevant objects (like an integer)
        - change types like 'small change to produce an integer', 'doubling to produce an integer', etc
      - the isolated relevant difference structure (without additional info) 
        - the average value, which has multiple difference types in adjacent change types
      - conditional relevant difference structures
        - if the nearest integer triggers other change types, the value near that integer has a relevant difference structure

    - example: position structure
      - similar positions will be near according to the distance metric, creating a radius of similarity, which results in emergent structures of a boundary, center & circle
      - different positions can be represented as a structure lacking a circle/boundary/center
      - the differences in similarity/difference structures have emergent effects & coordinate with different interface objects (like adjacent structures, change types, relevant objects, etc)
        - a lack of an object can be used like other gap structures are used (as a filter, container or template)
        - an object can be used as a component or other base object to use as an input

    - this is why differences are not just the 'opposite of similarities' - it leaves out information like:
      - similarities of varying relevance between similarity & difference (both use a distance metric)
      - the reason why a difference is used vs. a similarity (like 'filtering' intents)
      - emergent/adjacent/relevant structures of similarity & difference, embedded in different structures (position/spectrum)
      - info about the structure of difference (difference paths/stacks/layers/trajectories), which may vary in ways that similarities do not
        - this indicates the important point that similarities are insufficient to predict differences
      - if similarities were equivalent to differences, you could use similarities to derive all info, reduce all uncertainty & randomness, and solve all problems - which is not guaranteed
        - meaning 'derive structures outside of the universe, using info from inside the universe' 
      - similarities may have similarities to each other, more than similarities to differences
      - randomness has a similarity (in outcome probability), but is better than similarity as an input to generate difference structures like uncertainty

## document uncertainty structures like randomness collisions & structures that produce certainty (combinations that stabilize)
    - randomness collisions generate structure
      - structure being the stabilized interaction of information (staying constant long enough to attain structure)
      - randomness being a lack of information (like a star or circle with equally likely directions of change)
        - where influences are equal enough in power to leave no clear priority of direction favoring one over the other
      - when an info lack interacts with an info lack, they may not generate another info lack, but a structure stable enough to organize them, depending on the angle/type of interaction and whether the info lacks are a similar or coordinating type

# math-language map (connecting function) solution automation insight path examples

## apply core structures to interface components for language map
    - opposite structure of interface (division by applying a standard) is an application/combination (multiplication by creating combinations of pairs, of one variable's range applied to another's)

## apply connecting function of math/logic 
    - a problem like the following is a logic problem ('find the logic connecting this input/output') that can respond to the general solution workflow (given a problem input format of a 'function' to check possible solutions with) of:
      - 'identify the unique correct solution in a solution set to a problem of equalizing the sides of this function' 
      - 'identify which solutions are not correct, reducing the set to a size of 1'
      - this can be converted to a math problem of:
        - iterating through solutions
        - checking each solution to see if it solves the problem ('equalizing both sides of a function')
        - removing it from the solution set if not
        - otherwise checking if the set of possible remaining solutions has a size of 1 yet to give a success signal
        - continuing iteration if not
      - the connection between these interfaces is in the structure of logic (math being structural information in core terms like numbers):
        - the set iteration has a 'sequence' (set, progression) structure
        - the remaining solution set size has a 'integer' (set, progression) structure
        - the success signal & the continuation condition has a '0/1' (core alternative) structure
        - the solution test has a 'function' and 'equal' structure (are both sides equal yet)
        - the remove operation has a 'subtraction' structure
        - the continue operation has a 'sequence' structure
        - the condition component has a 'direction' structure (change direction in logic network/tree) and 'multiple option' structure (a decision between differing & mutually exclusive options must be made)
        - the check/test operation has an 'equal' & 'inject' structure (inject variable values to see if both sides are equal)
        - the logic function has a 'directed network' or 'tree' structure (follow directed relationships between function components)
      - apply structural interface to connect logic & math structures:
        - 1. some of those structures have structural relationships which should be identified by applying interfaces, like structure (including components like the similarity concept)
          - similarity:
            - the similarity in structure between the solution set size & set iteration (a progression or sequence) is relevant, bc the iteration & the set size should:
              - move in opposite directions
              - equal the original set size when added
          - by applying the structural interface (with components like the concept of 'similarity'), the query can identify this relevance by checking if an adjacent connecting function between the similar structures exists & is relevant to the problem/solution
            - generate core functions & generate combinations of them, applying them to problem variables being examined for a connecting function (solution set size & set iteration)
            - filter by those applied core function combinations that move/change the problem (converted into a solution space, once identified) to be more similar or closer to the solution structure (solution set of size 1)
          - direction:
            - given the sequence & other direction-related components/attributes/structures of the problem, the input problem components & output solution structures can have a position structure applied
        - 2. given that the solution format is a 'set of size 1', and the input problem format is a 'set of size greater than 1', it can be derived that:
          - when executing problem-solving method, the method should include a step where:
            - an item(s) is removed from the set 
          - this connecting function between problem & solution format derives the solution requirement of the 'remove' operation (without being explicitly told to include that operation in the problem definition)
          - given the other structures involved (integers, iteration sequence), it can also be derived that the remove operation should apply a subtraction operation rather than another structure like division, which would introduce other less relevant & adjacent formats like non-integers
            - this applies problem-solving insight paths like 'adjacent solutions should be tested first in an absence of reasons to do otherwise', where reasons to do otherwise could be metrics like system complexity, info about adjacent solutions failing in that system, info about non-adjacent solutions succeeding in that system (info about non-adjacent solutions being optimal for a system metric)
          - interface query design should involve queries to check for inputs to a step given required sub-query tests for alternatives
            - before applying a step, apply its required sub-queries to test for its alternatives, like for an adjacent solution step, checking that alternative non-adjacent solution sub-queries have returned no contradictory info indicating an adjacent solution should not be applied

# solution metric/filter/test examples

## general solution filters

	- meaning
		- relevance
			- usefulness
				- applies solution structures (opposite to error structures) as structures of usefulness/relevance/meaning
					- clarity (structure, definition)
					- adjacence (reduction of cost to reach solution)
					- connection (connecting problem & solution formats)
					- reduction (reducing problem dimensions)
					- fulfillment (filling abstract structure)
					- optimization/organization (positioning components efficiently for a metric)
					- similarity (resolve conflict)
					- differentiation (identification)

				- example
					  - the most useful functions (including patterns) will be:
					    - cross-interface patterns: 
					    	- patterns linking interface objects
					      		- example of patterns linking all interfaces: error patterns
						    - patterns of interface object links
						        - change path patterns of randomness
					    - system patterns: 
					    	- patterns which unite other structures & form an interim structure in between meaning & problem-solving task intents
					    - core patterns & core interface components 
					    	- patterns which can build other components
					      - patterns in core interface components, like change/difference patterns

## alternate variable set filters
    - when testing different variable subsets, you can select a variable set split by structures like:
      - vertex variables
      - variables on interim interfaces where other variables aggregate (in bottlenecks or hubs)
      - difference interactions
        - difference type (homogeneous sets of difference types)
        - differences in different types (heterogeneous sets of difference types)
        - which difference type sets would identify the most errors or are the most different from other difference type sets
      - which difference types are the biggest variance-reducers when combined
      - which difference types have an attribute (common, relevance, similarity)

## solution filters of an identification algorithm (to differentiate real & AI-generated content)
    - variable count/size (under-complexity, fragmentation, lack of smoothness/curvature)
    - wrong context for a pattern
    - over-repetition
    - over-similarity to previous information (lacking expected change structures, like change trajectory & types)
    - no matching reason/intent/priority for deviations from archetypes/patterns
    - over-correction when integrating a variable
    - variables identified in isolation
    - most clearly/measurably different variables identified
    - structure organizing variable structures (randomness injection points, enforcement gaps, info imbalances)
    - over-simplistic or erroneous automated sub-components
    - improbable level of randomness
      - clear composition of core patterns
    - sources of randomness
      - errors are evenly distributed among more complex adjacent sub-components not expected to change as much

## filters that reduce the problem space
    - identify the worst error types, as assumption combinations having the lowest solution metric fulfillment if incorrect
      - in the problem of 'predict cat vs. dog', the worst error types are:
        - an object from one category having all the features used to differentiate between categories, but with variable values of the other category (cat having dog features)
        - an object that is artificial identified as real (cat robot identified as a cat)
      - to predict these error types, certain concepts need to be inferred
        - the concept of 'agency' to design a machine that looks like an animal
        - the structure of 'false equivalence' to design situations where features would look like a category but not actually be that
    - identify all the feature ranges where it would be impossible to give high-accuracy answers (ai-generated cat image vs. real image)
    - organize these filters in a useful sorting structure (network, tree) can reduce the computations required to solve for a prediction function, such as:
    	- placing the most-reductive solution filter first, if the info required for that filter is already available
    	- placing a filter after another filter that generates/identifies the info required for the second filter

# insight path examples

## example of mathematized insight path 
    - standardize variables to math interface structures & values
      - apply type interface
        - identify types
          - standardize variables with types to differentiated clusters
          - apply difference definitions (like variable subsets) until type separations are clear
          - apply difference types until type separations are clear
      - apply structural interface
        - identify relative difference (difference from reference point, like origin node)
          - apply adjacent structures (vector or spectrum or loop) to variables having the concept of 'opposite'
      - apply causal interface
        - identify causal structures like direction
          - apply structures with direction to variables having causation in their connections
      - apply function interface
        - identify variables with input/output relationships to form path between structures on meaning interface
      - apply concept interface
        - remove randomness
          - compress variables with randomness injections to lower dimensional representations
      - apply meaning interface (using a structural relevance definition)
        - integrate variables in one structure to relate them
          - identify any vertex variables as the preferred variables to standardize other variables to
          - connect variables once formatted using adjacent/interim dimensions like topologies with variable subsets that can act as interfaces between connected formatted variables 
            (can capture info from input & output variables in the connection)

## insight path of most useful structures for solution automation

    - these structures should be applied first in any generative process, including interface query design
      - standards: filtering comparison methods
      - definitions: problem/solution definitions
      - optimizations/improvements: possible/probable changes for intents
      - errors: difference
      - metrics: intents
      - similarity: adjacence, patterns
      - relevance: connections
      - organization: integration methods

    - these structures can be derived with system optimization principles, for attributes like:
      - reusability: generative functions, definitions/constants/examples
      - derivability: derivation functions & inputs (core functions, structure application functions, prediction functions, metric selection functions, test functions)
      - independence: relevance calculation functions (to calculate meaning), system application functions (to derive context), organization functions (to build components using other interactive components)
      - compartmentalization: core isolated unique components
      - importance: generative or vertex variables
      - efficiency: balance between variables & constants, derived/generated & stored functions based on usage & intent changes

	- most useful standardizing structures to apply for generating & applying solution automation workflows (structures that connect problem/solution formats)
	    - example: 
	      - combine useful structures (similarities, connections, & types) to generate a new solution automation workflow
	        - apply definitions of 'error' & 'success' to generate a new solution automation workflow: 
	          - identify positions of known error types (abstract structures of difference from correct function output variable values) & avoid those positions
	    - problem/solution
	      - similar/different
	        - nothing/something, container/component, negative/positive, equal/opposite
	        - balance/imbalance
	        - equivalence/comparison
	        - interaction/isolation
	        - dependence/independence
	        - relevance/irrelevance
	        - connection/disconnection
	        - type/subtype, type/other type
	        - substitute/alternative
	        - source/target
	        - constant/variable
	      - combined/standardized
	        - expanded/compressed
	        - attribute set/type
	        - function logic vs. input-output or intent-query map
	      - core/interactive
	        - root/meta
	        - unit/group
	      - set/reduction
	        - network/hub
	        - space/position
	        - possibilities/filter
	        - potential/adjacent
	      - requirement/change
	        - direction/force
	        - limit/efficiency
	      - intersection/separation
	        - contradiction/context
	        - conflict/alignment
	        - center/distribution
	      - uncertainty/certainty
	        - probability/outcome
	        - random/structured
	        - abstraction/information
	        - question/definition

## standard basic general insight paths (to apply structure to in order to make them specific to a context)

	- trial & error
	- reverse-engineering
	- break problem into sub-problems, find sub-solutions, merge sub-solutions into solution

## standard basic structural insight paths

	- generate adjacent structures & filter by relevant intents

	- find optimal structure (combination, path, direction, sequence) for a problem-solving intent (find predictive variable set, functions connecting input/output, priority direction, operation sequence) given metrics like adjacence (structural alignment, low-cost conversion potential) or available functionality/variation in that structure

	- find similarities (like fit, interactivity, coordination, direction, inputs/outputs, position) between available/adjacent/possible structures and connect problem/solution structures using these similarities (like function sequence with coordinating input/outputs)

## efficient insight paths (using fewest resources, with relatively good accuracy)

### identify interface object set necessary to get good approximate prediction results with existing algorithms & params

    - find the abstraction level or definitions necessary to get an approximation of system or conceptual analysis with a standard data set 
      - definitions may include structures of relevance, like structures of similarity/difference
      - the approximation may leave out other analysis logic like alternative/combination analysis (to identify sets of alternate prediction functions, or causal/functional/priority/missing/type structures in the data set)
      - however it may find objects on an interface by including interface objects (include concept definition of agency/skill/decision in the titanic survival data set may identify concepts like 'education' as causative, given that a combination of agency/skill/decisions can be used to produce concept of 'education' = 'an agent making a decision to acquire a skill')
      - similarly, including structural definitions of 'relevance' may improve prediction results with standard algorithms, allowing output structures of relevance like 'semantic variable connections on the relevance level input to the algorithm', such as an 'explanation'
        - 'including' meaning 'standardizing to relevance structures, such as similarity/adjacence, inputs, interaction level, etc'
        - first you'd apply standard analysis to get a set of probable dependency graphs, with paths like:
          - gender => lifeboat access => survival rate
        - then you'd apply standardization to relevance structures to the dependency graphs
          - difference in functional position (gender roles) => difference in function (skills) => difference in usage (responsibility) => difference in resource access => 'survival' intent inputs => 'survival' intent fulfillment
        - the output would be an approximation of meaning, allowing explanations like 'being female (variable value) increased probability (ratio of outcome among possible alternatives) of being prioritized (randomness structures like starting position as well as the concept of agency in filter structure) for access to survival tools (type of 'lifeboat') bc of less agency/responsibility/skills'

## solution automation workflow insight path examples

	- identify similar systems & solutions used to solve the problem in those systems, then convert & apply solutions from similar system to original system
	- when generating solutions, identify:
	      - contexts/cases/conditions that can filter it out
	      - variables that can generate the most solutions
	      - filters that can filter the most solutions
	        - apply filters to solution space by solutions that are ruled out in fewest cases, best cases where solutions are less required or least probable cases
    - generate solutions from problem statement using interface objects
      - core functions 
      - mixes/changes of previous or abstract solutions
      - insight paths (break problem down, trial & error, etc)
      - system structures
      - core structures (opposite, equal, adjacent)
      - function input/output chains
      - vertex variables
      - conceptual structures
    - apply solution format and reverse engineer solution
      - apply solution filters that are adjacently derivable from problem/solution metadata (most-reducing filters that rule out the most solutions)
    - apply both the generate solutions method & solution format method and connect them in the middle
	- rather than learning & fitting a function (applying new info to update standard equalized or randomized structure), apply structural insight paths that frequently produce accurate task completion (in general like producing problem/solution format connection sequence, specifically like producing prediction function)
    - find an example & generalize
      - find core/unit objects, find example using those objects, & generalize
    - find an example & counterexample & connect them
    - execute a problem-reduction function/structure/question sequence
    - execute a solution-space reduction sequence before solving for remainder problem
    - run query to find interacting interface structures, then apply solutions for that specific problem space's interface network
    - identify vertex variables first & approximate
    - identify problem types & corresponding solution aggregation method for that set of types
    - identify alternative problems to solve (like whether to solve for organize, format, select, re-use, derive, discover, build, diversify, optimize, distort, or combine problems/solutions) & apply problem selection method, then solve
    - change problem into more solvable problem
      - cause 
        - identify cause by applying network to causation, then select which cause to solve based on solvability with adjacent resources
      - problem
        - identify problem types of the problem & select which type to use known solutions for
    - apply structures
      - cause
        - vectorize problem system, filling in missing components with generative functions as needed
      - function
        - apply functions to move problem (origin) state position to solution (target) state position
        - apply function input/output connections to connect problem input & solution output with function sequences
      - system
        - apply system structures like difference & incentive to generate & filter solutions for a priority like speed
        - combine structures that avoid known error types & apply available functions to fit
        - use solution for adjacent problem & apply available functions to fit
      - intent
        - apply map structure between problem-solution intents & function intents
      - interface
        - find interaction level where problem is trivial to solve
        - apply structures of organization until problem is trivial to solve
      - concept
        - apply map structure between problem-solution concepts & sub-structure concepts
    - generate solution space first, then filter
      - core
        - apply core structures of solutions to generate probable solutions
        - apply core functions to generate possible solutions & then apply filters to reduce solution space
    - apply filters first, then match with generatable solutions
      - core
        - apply components of solution filters to generate filters
      - structure
        - apply solution filters to reduce solution space
      - system
        - apply structures of difference (what is not the solution) to filter solution space, then match to what core functions can generate as adjacent/accessible solutions
    - apply solution structures (filters) & problem structures (errors, reductions) in parallel and connect in the middle

# generate solution automation insight paths

## generate solution automation workflows by applying workflows to other workflows
    - solution automation workflow variables
      - starting/ending position/format & format sequence
      - interfaces applied, in what query structure
      - allocation of uncertainty & variation
        - problem to solve (generate solution filters, find workflow, break problem, solve original problem)

## generative insight paths that generate solution automation workflows

    - identify patterns in structures allocating structure (constants) & lack of structure (variation) in interface queries to find new insight paths
      - example: 
        - variation (like variables) allocated to structure & info interfaces, & constants (like definitions) allocated to the intent/concept interfaces
    
    - identify patterns in connecting structures as core components of interface queries (build interface queries with interface-connecting structures)
      - examples:
        - intent & function interfaces are connected as metadata & trigger structures, so the triggering structure can be followed by the triggered structure in interface queries
    
    - identify patterns of finding/selecting interaction levels for an interface query
      - examples: 
        - core functions linking these interfaces
        - structural versions of core functions linking these interface objects
        - abstract network of an interface used for interface queries
        - cross-interaction level conversion function applied before other interface query steps

## apply insight paths to generate other insight paths (such as a solution automation workflow)

    - example: 
      apply the insight path:
        'select commonly useful system objects for find problems' 
      to the problem:
        'find rules that fit a system such as a context'
      after applying standard interface variables like:
        - abstraction, intent, reusability, & complexity
      to get system object filters from relevant problem interface object metadata like intents:
        - problem intents: find, fit
      which can be used as a filter to selected system objects
        - 'fit' intent requires a structural similarity
      with actual parsed query like:
      - apply system object 'structural similarity' to find structural similarities in the problem system ('find rules that fit a system such as a context') after applying standard interface variables
        - iterate through standard interface variables
          - apply 'simplicity' to problem system
            - output: 'simple' rules, 'simple' systems (and sub-type of system 'simple' context)
          - iterate through system objects to find sources of efficiency in assembling a solution query
            - apply 'structural similarity' to problem system
              - output: structural similarity between 'simple rules' and 'simple system'
          - integrate output with original problem system to generate solution automation interface query for problem
            - apply 'simple' rules (as a source of efficiency) in finding rules fitting a 'simple' system

## generate solution automation interface query

    - iterate through interface objects (change type, problem type, assumptions, etc)
      - find interface objects in a problem space 
        - filter by relevance structures (like interaction directness/causation, such as change hubs)
          - apply problem structures related to relevant structures
            - apply solution structures (like organization structures, like a sequence of tests/queries) to problem structures

    - specific logic automation example
      - check for missing relevant info in info found with variables
        - change to add earlier window to mtime param bc its out of error window
      - find interaction type & change type in info metadata (filename, modification time relationship)
        - any logs changed in later would include logs modified earlier bc of lack of incrementing/rollover, so mtime increase is unnecessary
      - check assumptions for requirements
        - mtime param unnecessary bc most logs would be modified in original mtime param
      - check for relevant change-aggregation objects in structure (event objects in a sequence structure)
        - significant date (upgrade, reboot) was within original mtime param which could be a factor in error so mtime param is necessary

## to generate solution automation workflows
    - combine problem types
      - a reduction/decomposition problem + a filling/aggregation problem = the solution automation workflow 'break a problem into sub-problems, solve sub-problems, aggregate sub-solutions'
    - combine structures & connect structure combinations by problem types
      - the structure combination of 'a sequence injected in a network' is a structure matching a 'route finding problem', so apply solution structures that find a route in a network, such as filters using metrics or rules that can filter routes by which routes dont contradict rules
        - the solution automation workflow is 'find structures relevant to resolving problem structures like inequalities in other structures' (inequalities like the difference between start/end positions)
        - the workflow matches 'sequence in a network' with 'route filtering structures', connected by the problem format 'find a route'
    - combine structures & core functions
      - the structure of the core function sequence(find, apply, build, filter) = matches solution automation workflows like 'find components which, when this function is applied, can construct this structure, complying with these solution metric filters'
    - combine components of solution automation workflows (functions, queries, interfaces, problems/solutions, structures) that have a valid input/output sequence

## example of insight path to generate a solution insight path for a problem

    - apply solution structures like:
      - balance between supply/demand
      - maximizing benefit/cost ratio

      to problem structures (metadata like available/missing info) to produce solution (insight path) variables:
      - cost of ignoring/focusing on info vs. benefit of actions like executing functions
      - cost of acquiring more info vs. benefits of applying quick best-case solutions
      - supply of available info vs. demand for info to solve the problem

    - then select/change variable structures (variable values & variable sets) to produce components of solutions (insight paths) for a problem:
        - concept components: low-impact variables, high-variation variables, most causative problems, worst/best case context
        - function components: filter (ignore/focus/assume), prioritize (set as primary intent), apply structure (like subset)
        
    - then apply structures to combine insight path components for a problem:
      - 'ignore low-impact variables to prioritize high-impact variables'

    - then filter by problem structure (intents, sub-intents) to re-integrate insight paths with problem:

      - apply filters like:
        - 'is a direct cause of the problem ignoring local/contextual/worst-case/probability info?'
        - 'does a function applied to a component tend to cause problems in complex systems, and is this a complex system'

      to produce reduction of possible solution insight paths like:
        - 'then ignore insight paths using those structures'
        - 'then apply further filters to check for a reason (possible benefit) to ignore that'

    - functional insight path (what to execute) :: filter insight path (what to rule out or focus on)
      - 'breaking down problems into sub-problems' :: ignore non-isolatable problem types & non-combinable solution types
      - 'identify worst case scenarios first and solving those in order' :: ignore less harmful problems (local/output problems) to prioritize more harmful problems (causal problems, problem types)
      - 'identify vertex variables & standardize to them, using solutions that act exclusively on them' :: ignore less impactful variables to address root causes
      - 'identify position of problem in causal network and apply solutions local to that context' :: ignore systemic solutions to avoid side effects
      - 'find alternatives to solving a problem (delegation, solving abstract version)' :: ignore specific solutions or move problem position
      - 'identify problem type & apply related known solutions' :: assume problem type can be identified & covers enough of the problem & is abstract enough to apply related solutions with effective impact

# interface query design logic examples

## associating interface operations with intent 

    - solve sub-problem 'find combine structures after applying system interface' for sub-intent 'to find connecting structures in problem/solution system'
    
    - the intent of a sub-query should be defined in terms defined on that interaction level, to avoid gaps in connecting structures across sub-queries, so that further sub-queries of the sub-query can connect to the original triggering interaction level intent
    
    - example: 
      - when solving a problem with an insight path like 'break problem into sub-problems', the sub-queries to solve each sub-problem should be defined in terms used by the insight path & problem statement
      - a sub-query to solve a sub-problem like 'reduce & isolate dimensions of problem' should be defined using the problem statement components & the insight path ('break' as the original function mapped to sub-functions 'reduce' and 'isolate'), so when it comes time to integrate sub-solutions into a solution, the corresponding opposite function to 'break(problem)' can be applied to 'integrate(solution)', using a version of 'integrate' such as a specific version of 'merge' that connects to the version of sub-functions of 'break' used

## the decision between selecting insight path/query for a problem & generating a new one is dependent on:
    - problem metadata (complexity, adjacent formats)
    - available info (whether metrics are capable of capturing relevant info)
    - input data set metadata (whether variables are output metrics, variance-covering metrics, proxy variables, etc)
      - different input/output relationships will imply different interface queries that will be useful
      - beyond that, other (interface analysis-identified) methods to design an interface query for a problem type
      
      - apply interface analysis to interface query design (system including interface components, query components, metrics) - apply interfaces to the problem of designing an interface query
        - examine what are the core functions, efficiencies, incentives, error types, etc of the interface query system, and check that they match what Ive identified
        - check if you can skip some interfaces, like when you start with an input containing mixed-interface (concepts, functions, intents) or cross-interface structures (structures that apply/generalize to or connect interfaces), such as when you can identify common terms in input component definitions that can be used to frame all relevant objects
          - once you standardize terms of component definitions, is there an interim sub-interface youve standardized components to, which can be used in place of a full interface query
        - example:
          - adjacent formats: 
            - problem is route optimization, problem format is network, solution format is network path, interface query should include function interface, bc function format is adjacent to finding a path on a network
          - intent alignment:
            - problem is over-complicated system, problem format is network, solution format is reduced-complexity system network, interface should include math & structure interfaces, to find & apply dimension-reducing functions (interfaces already contain functions that align with 'reduction' intent)
          - required inputs:
            - problem is 'find a relationship between functions for calculation optimization intent', solution format is 'connecting function', interface query should involve 'connecting' functions, which are a required input to solution format of a 'function to connect functions that optimizes calculation efficiency'
        - this can optimize for problem/solution metadata, as well as general problem-solving methods
          - optimize for problem type: interface query for 'missing information' problem type should include the 'similarity/difference' sub-interface on the 'structure' to identify 'opposite' structures like 'what is not there'
          - optimize for solution format: interface query for a problem with solution format 'prediction function' should include either causal, potential, change, or function or structure.network interface, all of which can generate a structure connecting the in/dependent variables
            - causal: organize variables with causal diagram having direction & check for predictive ability (identifying correlation, applying causal structures like moving/deactivating variables, using variable proxies or aggregate variables) to filter diagram for probable causation
            - potential: identify potential alternatives (variable sets not in data set, randomness explanation) and filter if possible, possibly leaving original data set as last remaining solution
            - change: identify variable change functions, and evaluate distorted data sets using those functions for alternate prediction functions, filtering by functions that are robustly predictive with more change conditions applied
            - function: index variables as functions (functions using variable combinations/subsets) to check for input/output connectivity potential between in/dependent variables
            - structure: organize the variables as a network to find relationships & if there is a relationship between in/dependent variables
          - optimize for general problem-solving methods: 
            - example: 
              - 'generate set of possible solutions & apply filters to reduce solution space'
                - the interface query should have a format that is filterable once it reaches the filter step of the general solution method
              - 'break problem into sub-problems & combine & merge sub-solutions'
                - the interface query should have a format that is combinable/mergeable once it reaches the combine/merge step of the general solution method
  
## connecting problem & solution formats has a set of workflows based on structure & adjacent solution automation workflows that can direct the interface query design by the requirements of the steps in those workflows

    - examples:

        - connecting a problem of 'too much structure' and solution of 'reduced structure' has a workflow involving steps like 'reduce variables', with requirements like 'variables', so the function or change interface can be applied to identify variables before executing that step in the workflow
        
        - connecting a problem & solution with a particular solution automation workflow also has input requirements, like 'break a problem into sub-problems' workflow, which requires that structure of variables (error/differences) are identified (to identify sub-problems), so applying the structural, function, or system interface is necessary to identify those structures which act as sub-problems
    
    - interaction structures allow interactions to develop but are different from interfaces/standards that specifically enable communication/comparison interaction types, despite interaction structures acting as a connecting structure which has structural similarities to communication, communication being the exchange of info that is interpretable & actionable to source/target
    
    - find equidistant point to information to start parallel interface queries from

## connecting problems & solutions with error types (opposite of connecting with solution types)

  - associate error types (with interface metadata like intents, causes, structures) with problem & solution types, to identify connections like:
    - what errors can be present in a solution that can still be considered successful
    - what errors are considered a problem or equal to the input problem when combined in a structure
    - iterate through possible interface definitions of problem/solution
      - problem :: solution 
        - general connecting function:
          - sub-optimal state :: more optimal state
        - specific problem/error type connecting functions:
          - state with errors :: state with fewer errors
          - state with unused resources :: state with fewer unused resources (unnecessary dimensions)
          - state with no possibility for change: state with possibility for change (randomness injection points, variance sources, dependencies)
          - distorted state (specific intent) :: undistorted state (center)
          - state where organization is a dependency source (too big to fail) :: state where organization is an efficiency source (solution provider)
          - specific solution for specific parameters/values :: abstract parameterized solution
          - mismatched format :: matching format
          - misaligned intents :: aligned intents
          - info dependency :: info generating function dependency
          - unknown cause :: set of possible causes of varying directness
          - state with inability to self-correct :: state with self-correcting function
          - state with inability to interact :: state with core functions to build interaction function & function to change interaction level
          - lack of chaos :: variance injection, variance source
            - when a system has no errors, that means its either not finding new variation (unlikely if capable of doing so), not capable of finding variation, or is not learning
              - inject errors to try to produce variation
              - apply function to build functionality to find/generate variation
              - apply errors/changes to learning functions to produce new learning functions
          - structure :: different structure
            - direction :: position
              - goals (result, impact, resource) :: flexibility (increase in function, increase in power)
          - missing structures (sub-type of opposite structures, sub-type of difference structures)
            - lack of structure :: unnecessary structure
          - sub-optimal solution :: improved solution
          - solution set :: optimal solution
          - decision options :: executed decision
          - lack of decision :: decision options
          - lack of power :: locally concentrated power
          - too much (concentrated, high density, unnecessary, unmanageable) power :: globally distributed power
      - apply error & problem types to generate other possible definitions of a problem & solution, allowing functions connecting them to be built/stored specifically for those types
      - apply system optimizations to all interface components 
        - example: 
          - apply 'have multiple variance sources' to 'variance sources' for intent 'distribute power' of input variance across sources
        - filter optimizations by contradicting intents that are identifiable as useful for functions connecting problem/solution structures
    - apply error types to interface component design/optimization
      - applying error type solutions to functions
        - 'avoiding dependencies'
        - 'avoiding traps leading to dead-end static states where variance injections cant change the system'
      - to avoid the associated error types:
        - 'missing dependencies', 'cost of generating dependencies'
        - 'lack of flexibility', 'lack of potential', 'lack of functionality'

## problem-solution format maps

  - the problem is the solution in a different format, or a piece of the solution (problem being a sub-optimal state to optimize, or a difference that shouldnt occur, and the solution being a set of constraints forming boundaries, or an optimal structure to construct)
    - filling problem
      - missing info problem: the solution format is the complete structure
      - optimization problem: the solution format is the variables/system organized to comply with/fulfill the metric to optimize 
      - aggregation problem: the solution format is the aggregation method to form a structure (like combining core functions to get a function for an intent)
    - limit problem
      - constraint problem: the solution format is the removal/invalidation of that constraint
    - reduction/decomposition problem
      - complexity reduction problem: the solution format is the set of variables that reduces complexity of the problem
      - randomness reduction problem: the solution format is the set of variables that can replicate a semblance of randomness
      - problematic structure: the solution format is reducing the structure (identifying variables & invalidating those variables)
    - organization/mapping problem: the solution format is the set of relevant components in the right structure (positioning & connecting them)
      - conflict problem: the solution format is positioning the conflicting problematic vectors so they dont intersect
      - balancing problem: the solution format is the distribution of resources nearest to a balanced state (subset of matching problem, by matching distribution across positions)
      - combination problem: the solution format is the set of components in a combination structure that doesnt contradict combination rules (components fit together, like 'finding a system where a function can execute')
        - connecting problem: the solution format is the set of functions that connect the components, in the position where they act as connectors
    - finding problem
      - discovery (insight-finding) problem: the solution format is the set of generative/distortion/core functions or the set of filters to find the insight
      - route-finding problem: the solution format is the route between two points that doesnt contradict any solution constraints and/or optimizes a solution metric
    - other solution formats would be for adjacent/causal problems, solution formats that invalidate solving the problem, etc

## example of selecting problem/solution format

    - examples: 
      - every problem can be framed as 'reducing solution space', but some problems are more adjacent to this format than other problems, such as:
        - 'find the one item in the set that matches the filter value', which is more adjacent to 'reduction' operation because it involves a solution output format of a lower quantity than the original quantity, specifically a quantity of one, which implies that the original quantity is greater than one, given that this is framed as a problem that is not solved yet
        - problems have many possible formats, so an initial problem to solve is 'reducing the solution space of possible formats to the one most adjacent format'
        - the correct format is important to find, bc some formats will make the problem trivial to solve or solvable with existing methods
      - as another example, a prediction function can be formatted as a problem of:
        - finding causal network of variables (root/direct cause in structures of inevitability, lack of cause in interchangeable alternates)
        - finding variable network connected with functions (apply 'randomize' to root cause variable, then apply 'specialize', then apply 'standardize')
        - finding variable structure network (boolean causing vertex variable causing spectrum variable)
        - mapping variables to influencing & interaction power (to influence & interact with other variables)
        - isolating & filtering variables in data set by impact/contribution, filtered by probability of coincidence (coincidental structural similarity between independent variables & actual causative variables, leading to secondary structural similarity in apparent relationship to dependent variable)
        - finding coefficients of variables in data set
        - standardizing data set to a subset of variables (like a vertex variable) so core/unit functions can be applied
        - inferring other variables not present in data set
        - allocating randomness to explain lack of predictive power of independent variables & changing prediction function state
        - finding the data set's distortion from a base/central/standard data set having those variables
        - finding the probability of a prediction function given a data set (or vice versa)
        - finding a line/cluster/point (or generalized structure) averaging the data set relationships
        - finding concepts & other interface objects in the data set (concepts like 'power' relevant to predictive/influential potential)
        - filtering data set by which data can be ignored (outliers, corrupt data, randomness, worst/best case, prior outdated data)
        - finding a statistic representing target solution info
          - does 'average' represent the relevant solution 'prediction function' that is best able to predict y across adjacent/derived/given data sets, or is there a better statistic, like:
            - 'weighted average'
            - 'subset average sequence'
            - 'emerging average given state data'
            - 'derived average given randomness injection'

## example of filter for selecting formats

  - why shouldnt everything be formatted as a network (why should you use multiple interfaces or formats)
    - everything can be depicted as a path on a math/language symbolic map, including insight paths, so why shouldnt that map be used to solve every problem?
      1. all formats have assumptions embedded which distort the format from the central format (no structure, or randomness), having associated useful intents
      2. some definitions of complex components would require other structures than a single network path to be fully defined, like:
          - a layered network query such as a loop, which would be more optimally (like clearly) structured in another format, like a function network
          - complex functions/concepts could have very intricate structures on a language/math map, which would be more clearly defined on a function or core component network
          - paths between other paths
          - paths between attributes of nodes on a path rather than the whole node
          - multiple paths depicting the system context forming a sub-network around a path
            - the system interface where agent interactions occur or where stressors are clearly modeled is therefore the best format for some solution automation queries
      3. the standard network format assumes functionality & attributes should be bundled as components like objects/agents/words/concepts, which may not be optimal for queries like identifying conceptual structures or variable structures
        - even the attribute format assumes that some attributes should be grouped, and assumes values for certain attributes, where layers would be a better structure for attributes
        - depending on the interaction level, querying a comprehensive map including all functionality/attributes can be computationally prohibitive
      4. the interaction functions of solution components (like cause or intent) arent automatically defined on a language/math map
        - what type of query to run when the problem to solve is answering 'why', having an answer using the 'because' or 'reason' nodes
        - cause/intent/concepts/systems/potential/change arent immediately clear from the language/math map, where they would be in a format using those interface structures
      5. some concepts/functions/attributes/components will necessarily be missing from the map until theyre added to the map, and some terms are unnecessary, and some are false
        - missing components: components no one has used yet or thought of will be missing from the map
        - unnecessary components: you dont need every interchangeable synonym or every number to effectively communicate a path
        - false definition: some definitions would be defined suboptimally, giving incorrect query results until corrected
          - the components wouldnt have the definition routes specialized for different interfaces (like abstract paths generating or defining a component) that enable quick identification of connections & other useful structures
        - false variation: some changes to a language map would seem like variation but would actually not add much potential in terms of novelty/uniqueness in identifying a new concept
      6. the definition of difference in a standard language/math/symbol map might not be the best organization for queries, requiring other formats like central core functions with distortion & interaction layers around the center


## example of identifying query-changing (invalidating, embedding, stopping) conditions during an interface query or interface query-generating query
    
    - queries are implementation of components of control flow (supply: decision/action/function, demand: problem/error/task/conflict/limit)
    
    - example: execute a query to find structures of 'high-variation' in a data set
      - identify relationships within a variable (across potential values for that variable)
      - identify relationships within a variable's state changes (across potential values for that variable across its lifetime)
      - identify relationships (interaction functions & types) between variables
      - identify relationships between variable structures (subsets, combinations, alternatives) & variables
      - identify variable types (proxies, root cause, interdependent)
    
    - query-changing conditions
      
      - standard control flow conditions

        - query-stopping condition is where its clear the data cant:
          - fulfill the optimization metric or fulfill it more
          - find the information or find any more
        
      - meta conditions

        - query-invalidating condition is where the data set invalidates the concept of variation or data 
          - when a query has identified type/relationship/pattern information invalidating the data
          - when the data is not a source of truth (state has changed but data has not & has no variation and is not data anymore, if data is a source of truth)
          - when a query has identified a function to reduce the variation/data without losing info
            - or create a function to do so
            - or identified a need to trigger an embedded query to create a function to do so
            - and has identified & organized resources to create that function or execute that query, after identifying its need for the function/query (AGI)
        
      - query interaction conditions
        
        - query-connecting condition is where the query identifies that another running or previously run query might have identified useful info relevant to its task

          - examples:

            - a query that identifies similar structures 'difference-reducing/increasing structures' or 'sismilarity-filtering structures (leaving just difference structures)' might find the high-variation structures quicker
              - this alternative query could be found by applying the concept of 'similarity' to the 'query' object, allowing for the possibility that the query was almost correct

            - a query to identify query metadata and apply those metadata variables to generate other alternative queries
              - query metadata examples: accuracy, side effects (like unintended functions built during processing)
                - generating more accurate or faster alternative queries by applying optimization structures (like alignment, info/function re-use, etc)

            - a query that identifies 'change-reduction' structures (like types or interfaces) could be more efficient than this query to find high-variation, which may miss embedded query opportunities for embedded structures of change in the data (data about variables/functions)
            
              - how could the original query know to check for such a query running in parallel?
                - identify problems with its own query metadata (execution, design, connectivity, progress, probability of success) & calling query to generate alternative queries that optimize on problematic structures like performance metrics (execution time vs. relevant info found)
                - identify problems in the original problem of the query (sub-problems of original problem, encountered problems like a missing function to derive)
                - apply structures of robustness by default, like apply 'alternative' to 'query' object to run alternative queries by default, filtering by difference or relevance to maximize probability of finding useful info
                - identify relevance structures that would be useful (such as useful for sub-problems identified initially or encountered during execution, or planned problem to solve later in original query)
                  - it could apply the concept of 'type' to itself (self-aware that it's a query) by abstracting the 'query' concept, identifying its type, and querying for other queries of that type
                    - identify that its processing was not finding info as quickly as typical queries asked to find structures of concepts like 'variation' 
                  - it could identify that there is another route to the info during its processing
                    - by examining data for variance, find a structure consistently causing/generating variance that relates to change reduction
                  - it could execute some of its processing using conceptual core structure analysis, creating combinations to identify concepts (related to query concepts like variation & data) like 'change reduction'
                  - it could identify that a query-invalidating condition that reduces the variation in the data set has been met in another query
                  - it could use concepts like 'equal' and 'opposite' to apply a counter-query to check for the opposite structures, which can be faster
                    - just like checking for a difference may be faster than checking for a similarity or vice versa, or checking for a limit/conflict may be faster than checking for a function
                  - it could apply concepts related to the definition of 'change' such as 'potential', and identify that potential increases with more change structures, particularly change-expanding structures, the opposite of what this query is looking for
                
                - then using the output of such analysis types that can supply relevance structures, applied at intervals or decision points during its own processing, it could check for a query running with intent (or inputs, side effects/outputs during/after processing) to:
                  - identify 'change reduction' structures
                  - generate a function to generate 'change reduction' structures
          
        - query-embedding condition is where an embedded query is required
          - in a data set is data about functions/variables, an embedded query might be used to find embedded variation in embedded function/variable relationships/structures
            - data about functions/variables would expand the possible variation in the data set within each column/variable, with change types (functions/variables) as data
    
    - condition types
      - invalidate query (compare & find alternative solution)
      - embed query (correct an info gap)
      - connect query (delegate processing to another query)
      - stop query (apply a metric)

# interface query examples

## example of interface analysis applied to explain lack of perfect predictive power of a variable (like cell structure)

	- structural analysis of components (like cell shape/surface) is insufficient as a predictor of functionality bc it's missing info about:
      - components
        - other/possible components & their structures (other possible pathogens, foreign cell types, in other ratios/positions)
          - other/possible components with similar/contradictory shapes that might be interfering
            - like similar receptor/binding shapes that leave no room for the cell type being examined
        - internal cell components not measured or formed unless found in a particular environment context
      - change types
        - changes to the host system structure (like nerve damage)
        - changes to forces governing change (like motion, as blood flow) in the host system structure 
        - not measurable info
          - hidden non-structural variables (like blood flow/pressure, electrical effects, or prior exposure to nutrients like vitamin d triggering timers) or variable sets with similar net effects (activated lifecycle)
          - distortions commonly found in different cell types with same structure bc of different positions
          - functional implementation differences
            - different cell types have different method of achieving the same function using the same components, in a structure that varies within the data set but not enough to indicate different method
        - component interaction dynamics
          - interaction level
            - cells with same structure might operate on different interaction levels, given different position/system
          - structures of interaction object components
            - a cell with equivalent DNA might encounter 'jumping gene' functionality in one system position, where an equivalent cell in another position would not
          - determining interaction attributes/functions 
            - like how attributes like aggressiveness might be determined by missing info (indicating why one cell type would succeed at binding & another of a similar/equivalent structure would not)
        - limit/threshold dynamics
          - sample data might leave out variation in the form of determining cell type attributes like size above a threshold with emerging behaviors, or potential to change that attribute triggered by the environment
        - state dynamics
          - false equivalence: structure might be measured at two equivalent states across two different cell type lifecycles (like evolutionary paths or distortion patterns), giving illusion of equivalent structures
        - system dynamics
          - structural metadata (like position, which determines local system & adjacent cells/functionality)
          - invalidating functionality
            - system that deletes duplicates, where a particular cell type is handled second bc of some attribute (like size, indicating it needs to be broken down first), so its always found to be the duplicate & is deleted
          - functionality that is activated in environments & not obvious with structural analysis 
            - like a function that folds dna/proteins in a way that has more errors than other folding function in a particular environment
        - sequential dynamics
          - exposure to a pathogen might trigger a function in response to a cell type with a minor distortion that becomes determining in edge conditions


## interface queries for problem 'find a prediction function'

	- apply information (definition) interface
		- apply error definition routes/attributes/functions/objects/structures
			- identify error types for problem 'find a prediction function' to use as filters of solution space
			    - false equivalence
			      - similar routes to different answers
			        - this implies similar patterns in variable structures & interactions across data groups
			      - overlap
			      - lack of differentiating variables in data set
			    - false difference
			      - merging/imminent similarity/equivalence
			        - functions that can act on other functions to produce a false or real equivalence to another function
			      - alternative routes to the same answer
			        - identify all the alternative structures (routes, combinations, trees) to an answer between function components like variables, data sets/subsets, & neural net components like weight path patterns, and the differentiating factors & vertexes, then use that to implement a filtering structure to sort through them to rule out the most possible answers the quickest
			      - alternative answer types
			        - identify all the different variable/function combinations that could create the most differences in similar answers (such as different types or contexts like a separate function for outliers), and a filtering structure to apply these as variation-reduction functions
			      - these filtering structures can act like interfaces, reducing variation in the possible answer set
			    - equivalent combinations
			      - alternative variable subsets that act as proxies to an answer
			    - equivalent variable structures
			      - find variable structures like functions that approximate other variable structures like variable networks

	- apply change interface to find variables in a problem statement
	    - find isolatable change types
	      - if the problem is 'predict movement of object', this means: 'find change in possible orthogonal directions'
	        - filter out redundant variables (like if variable A/B + randomness constant can be replaced with variable C + another randomness constant)
	        - filter out variables or variable structures like combinations that look like randomness to leave sets of variable/s
	          - find prediction function for variables with randomness excluded
	          - apply degree of randomness with randomness accretion patterns & interaction structures (like other objects on interaction layers) to prediction functions once variable dependencies are described, to generate prediction function set or prediction function with distortion vectors for possible ranges, then test on data 
	    - variable sets that cant be filtered out can be considered sub-problems to solve ('filter out this variable set') in addition to the original problem of 'finding a prediction function', as extra filtering tests to apply before the solution is selectable

    - interface query using concept-structure interfaces for problem 'find prediction function' 
      - find solution filters
        - find range of error allowed for solution
      - convert to problem interface
        - predict missing info 'future state of variables' with input 'past information'
        - standardize to structural interface
          - find vertex concepts
            - 'find prediction function' using past information involves:
              - risk structures like: possibility that an unknown structure is causative
              - randomness structures like: possibility that known structures will be distorted by randomness
              - change structures like: possibility that known structures will change & info needs to be found/derived to update variables
            - combine risk structures, randomness structures, & change structures
              - filter which combinations match data
                - filter which combinations match data within range required by solution filter

  - general interface query example for 'find prediction function'

    - change: find highest-variation variables in problem statement
      - structure: find combinations/subsets of variables
      - cause: find dependency structure of variable subsets
        - function: find input/output sequences of variable subsets
        - structure: filter the sequences by whichever sequences link the source/target structure
          - problem: solve sub-problems of organizing variable subsets
          - structure: aggregate sub-problem solutions

  - specific version of general interface query example for 'find prediction function'

      - change: find highest change problem variables in problem statement
          - which probability distribution it is
          - variable values given
          - whether alternate probability distributions can be ruled out using constraints/assumptions/parameters/change types & other info of problem
          - sub-problems
          - sub-problem structure (organizing the sub-problems)

        - structure: find subsets of variables
          - example problem variable subsets:
            - missing info + variables values given + sub-problems
            - probability distribution + variable values given + other problems or problem patterns

        - cause: find dependency structure of variable subsets
            - missing info + variables values given + sub-problems
              - with the missing info & variable values given, you may be able to infer the probability distribution (though not always if the problem statement is ambiguous) and derive the sub-problems to solve
            - probability distribution + variable values given + other problems or problem patterns
              - from the probability distribution & variable values given & other problems, you may be able to infer what the missing info is given questions usually asked with that distribution

          - function: find input/output sequence of variable subsets

          - structure: filter the sequences by whichever sequences link the source/target structure (variable values, probability distribution & missing info, 'probability of event')

            - problem: 'predict probability of event A given event B & some parameter/condition C'
              - sub-problems
                - identify problem metadata (probability distribution, variables & values) in problem statement
                  - identify missing info (specific problem to solve, like 'find the missing info that is a probability of a specific event')
                - identify alternate interpretations of problem
                  - filter alternate interpretations (to likeliest or the interpretation with no contradictions)
                    - match variables & values in problem with parameters of the probability distribution or relevant functions
                      - filter functions to functions with output type 'probability'
                        - filter functions to functions with specific output probability matching missing info
              - aggregate sub-problem solutions
                - missing info:
                  - apply variable values to relevant functions to generate missing info (specific output probability)

## apply distortions to vertex interface queries for solution intents

  - vertex interface query: high-impact query which can be used for finding optimal solutions quickly or used as a base for other interface queries in interface query design

    - query: reverse engineering solution metric with core structures as filters to find relevant metric structures

      - problem statement: 'find individual unit metric value in a container having equivalent & different components, without a function to measure individual unit metric value, and given total container metric value & unit count'
        - find relevant structures of the metric
          - apply insight relevant to 'calculations': 'apply the same standards when calculating if possible'
            - apply concept of 'similarity'
              - find relevant structures having the same metric
                - find relevant structures to 'unit'
                  - apply core concepts/structures to problem system structures
                    - apply core structures of 'combination'
                      - relevant structure: set of units, having an aggregate metric, usable input to an averaging function
                    - apply core concept of 'opposite' or 'not equal' and the core concept of 'total' (the complete set of all components in container)
                      - relevant structure: set of non-unit components in container, having the same metric, usable input to a subtraction function
          - find most measurable structure (with greatest accuracy or fewest steps) out of the relevant structures having the same metric
        - find calculation relationship between adjacent proxy metric of relevant structure and original solution metric (individual unit metric value)
          - calculation relationship between sets of not-equal components and equal components to the individual unit metric:
            - calculation relationship: "subtract not-equal component set metric value from total value, and divide by unit count to find individual unit metric"
          - to find this relationship, execute the opposites/reversals of the operations to find the relevant structure metric values
            - 'subtract' is opposing function of 'combine'
              - 'combine' was executed to get the list of sets of components (not-equal components & equal components)
            - 'divide' is opposing function of 'combine'
              - 'combine' was executed to get the set of equal components, relative to the individual unit
            - these two combine operations were used to create a path from the individual unit to the set of total components in the container
            - they can also be applied in reverse to get from the given total container metric value to the individual unit metric value


# solution format examples

## solution of optimized network structure

    - the optimized network can be structured as versions for different intents like:
      - lowest-memory generator: the average network + distortion functions
      - relevant generator: the network nearest to the most useful versions of it
      - quick generator: the network with the components that can build other versions at lowest cost
      - core generator: the network with core components to build all other components
      - adjacent core generator: network with core components at an abstraction/interaction level where they are most adjacent (mid-level functions as opposed to granular functions or high-level agent-interaction functions or conceptual functions)

    - the optimized network (ark) has the interface components necessary to solve any problem, with no extra components
      - it has one of each parameter of required components (like definitions, bias/randomness/error structures, interfaces, core/change functions, etc) which provide enough functionality to decompose & fit all discoverable information into a system of understanding
        - for example, one example of each opposite end of a spectrum & the average in the center, or the average + distortion functions to generate the other possible values

    - can probably be adjacently derived from subatomic particle interactions, which implement the core objects of interfaces like cause & potential

## solution of efficiencies gained from missing components
    - some functions are generated more quickly without a component, bc of the needs that the lack generates, which focuses generative processes on building alternate functions to fill the gap
    - this can be used as a way to predict what tasks the optimized network with missing components would be relatively good at
    - missing component metadata
      - how adjacently it can be learned/generated/invalidated/delegated/identified/borrowed
      - how likely it is to be learned/generated/invalidated/delegated/identified/borrowed
      - whether another missing component can be used instead
      - whether the system missing that component should be changed instead
      - whether a system having that component succeeds at the intent task (& fails at others currently fulfilled by the system missing that component)
    - example:
      - not having a function incentivizes:
        - identity: development of that function
        - abstraction: development of generalization of that function, parameterizing that function intent
        - alternate: development of a proxy or alternative or invalidating function, making the function itself unnecessary
        - cause: development of structure/function/attribute that invalidates the original requirement metadata (priority, intent, dependency structures), not just invalidating the function
        - alternate format: development of a structure/attribute that replaces the requirement for the function or allows the function to be generated as needed
        - derivation: developing a function to learn/derive/identify/borrow/cooperate functionality from external info, to generate functionality as needed
        - core: developing components capable of building all functions to generate functionality as needed
        - subset: developing components of that function so the function & other functions can be generated as needed
        - combination: development of a function capable of fulfilling that intent & other intents
        - distribution: distributing functionality-generating methods to all nodes requiring functions
        - organization: allocating gap requirements (uncertainties) to the gap in functionality (example: keep the gap so you can apply methods as a test to resolve the gap)

# examples of interface operations

## apply an interface to a concept 

  - apply interfaces to concept of risk to find relevant interface objects like solutions to risk error type, risk structures, & other risk metadata

    - risk: adjacence to negative events (error types)
      - risk structures: 
        - cascading risk
        - compounding risk
        - interacting error types
          - adjacence of an error type to another error type
          - adjacence of input/output & other interaction formats enabling interaction

    - solutions to risk:
      - distributing errors or otherwise ensuring they cant interact
      - making sure if an error occurs, its at a dead-end trajectory where its side effects dont impact the system
      - distributing info sources to gather info on imminent risks (robot that can distribute a set of sensors to pick up signals it otherwise couldnt, like behind opaque objects)


## apply an interface to an interface

	- apply information & physics interface to math interface 

		- math is a connecting interface of abstraction & structure bc it maps fundamental structures to abstractions
		    - math describes info (stabilized structures) 
		    - relevant questions:
		    	- what structures have stabilized in the math interface, so math can be applied to describe stabilized structures of math

		- math interface as information (certainty) physics, specifying:
		    - what can be known/calculated & approximated
		    - what can be predicted
		    - what certainties can be connected using numerical relationships (like how logic specifies what inferences/conclusions can be connected)

		    - determine what can be calculated by applying information & physics interfaces 
		      - when information doesnt exist, math cannot solve the problem
		        - with information defined as 'stabilized energy storage', at what point does the definition of information break down:
		          - type level interactions
		            - gaps in the possible change ranges of symmetries
		          - structural changes
		            - lack of alignments, similarities, efficiencies or other structures enabling info to accrue/develop/stabilize
		          - incorrect assumptions
		            - reversibilities in time symmetries, or symmetries that are theoretically irreversible without a concept of symmetry operations
		            - constants like inevitabilities, absolute (acontextual) impossibilities, or limits on variable value ranges
		            - limits in how information overflows (info that cannot be stored in an existing structure) can be predicted (structures built to store it)
		              - building different info storage structures (different from brains, networks, topologies, matrices, & probabilities, like interfaces & superpositions) can change how patterns of uncertainty-to-certainty conversion (like with uncertainties n degrees away from pre-existing certainties) occur & their probabilities of occurring
		          - missing dependencies
		            - gaps in conditions enabling energy storage (definition of a fraction is stable while the numerator/denominator are still defined, complex numbers defined using the definition of square root of -1), creating a symmetry of stability, where the efficiency created by core functions of a new interface can dissolve once the functions buildable with core functions overflow the interface, so functions may dissolve to randomness when absorbed by other systems
		            - changes invalidating the unit structure combined to create other structures (where basis vector is not defined)
		            - where definitions used by info definition (value, difference) break down
		          - where certainty is universally distributed & no uncertainties are possible, so a definition of certainty is not needed
		          - where certainty is not allowed by the system
		            - system has distributed randomness injection points, or structures of certainty like interaction levels are prevented from developing

# apply structures to solve a problem (complete a task, fulfill an intent/requirement, apply a solution metric test)

## example of applying insight paths to find & apply cross-interface non-standard methods across systems to generate solutions

    - apply insight path: 'identify similar systems & solutions used to solve the problem in those systems, then convert & apply solutions from similar system to original system'
      
      - applying concept of 'bias' used to fulfill intent of 'creating a truth filter'
        - bias is usually used to evaluate intentions of agents when interacting with other agents with some level of variance in agent identities
        - after abstracting intentions as decision/function triggers:
          - apply bias as a truth filter to determine non-agent change/function triggers
          - this can work bc even components without agency respond to incentives bc of their common tie to physics
            - example: bias has a core error structure of over-prioritizing locality, which can be converted into the concept of adjacence as a solution to the problem of 'minimizing cost', so truth filters can be formatted as 'low-cost or otherwise adjacent distortions'
          - bias also interacts with the concept of randomness & randomness can explain false info signals, which connects to problem-solving intent of identifying truth

    - queries to generate insight path to find useful structures to apply across systems, for an intent like 'truth filtering'

      - apply insight path: 'apply insight paths to generate insight paths'

        - find structures with 'truth filtering' intent in solution (source) system
          - map system components across systems (map 'truth' in agent system to 'correct' in non-agent system, match 'intent' to 'incentive' bc non-agent systems always respond to incentives)
            - map connecting structures in source system to connecting structures in target system (what connects bias function in source system vs. corresponding connection in target system)

        - apply components of structures with 'truth filtering' intent across systems, to equalize problem (target) & solution (source) systems
          - apply metadata of 'truth-filtering' structures (bias) from agent source system to non-agent target system
            - apply bias/interface metadata (intent) to target system components
              - find intent ('reasons') for 'randomness' (find the change interactions producing false or temporary randomness in non-agent systems)

            - apply bias interface objects (intents/reasons to use biased rules) to target system components, due to commonness in intents across systems
              - bias intents/reasons: over-simplicity, lack of storage, lack of change type functions (update functionality)
              - 'if an info signal has bias intent signals (if its clearly caused by lack of storage), classify it as a potential false info signal (request from pathogen rather than from host cell, false electrical signal, illusion of an electron count)'

      - standard interface query 

        - apply structural interface
          - identify connections between structures in problem
            - problem: 'find true info in agent-based system interactions despite agent incentives to send false info & intentions/decisions to do so'
            - problem structures:
              - concepts: 'truth' (intention matches decision output = 'successful decision'), 'agency', 'incentive', 'intent', 'decision'
              - functions: 'interaction functions', 'decision functions'
              - other structures: 'decision function triggers', 'false info', 'true info'

        - apply combine function to conceptual interface
          - create combinations of abstracted versions of structures
            - problem: 'find true info in system interactions despite incentives to send false info & other sources of false info & change functions enabling that'
            - problem structures:
              - concepts: 'correct' (info implication matches its impact), 'incentive', 'change', 'randomness'
              - functions: 'interaction functions', 'change functions'
              - other structures: 'change function triggers', 'false info', 'true info'

        - apply connect function to abstract structures
          - find structures that connect abstract structures (randomness, false info, change/function triggers) without the specific attributes tying them to one system (agency)
            - test whether the connecting structures fit with the new system after removing attributes:
              - can bias be used to filter out false info or find true info in chemical interactions, despite elements not having agency, as an abstracted way to decompose randomness/noise or complex systems
                - for example, can an abstracted version of bias structures correctly model the integration of quantum physics with chemistry rules to explain some chemical phenomenon

## apply interface analysis to find alternative solutions for matrix multiplication problem

    - existing solution (apply multiplication method to smaller matrices) applies:

      - core structures: 
        - meta (matrix of matrices)
        - subset (sub-matrices)
        - substitute (addition for multiplication)
        
      - core functions:
        - apply substitution method to subset once matrix is formatted as a matrix of matrices
          = apply(substitution_method(format(original_matrix, 'subset')))

    - how to generate other solutions

      - multiple queries to arrive at the same solution of 'finding adjacent interim values & re-using multiplication operation, in case where adjacent interim values exist in a matrix'
        - you can start with the target solution formats as your interface query filter (equating "problem format + operations = solution format")
          - a more efficient operation than multiply
          - a more efficient combination of operationss than 'multiply then add'
        - or you can start with applying interfaces, and iteratively focusing on & applying useful structures found for the solution (problem-reduction or problem-compartmentalization)
          - apply structures known to generate solutions to fulfill solution metrics (move toward solution position or reduce solution space or reduce problem)
            - apply core/adjacent/efficient/similar structures

      - apply structural interface

        - apply core structures of structural interface
          - apply structural similarity to structures of problem (including value)
            - similar values enable addition instead of multiplication (multiply 5 * 8 & subtract/add 8 instead of multiply 4 * 8 and 6 * 8)
            - if there are similar values in a matrix, and storage is allowed, this can reduce multiplication count (ignoring storage search)
          - apply adjacence structures
            - find values adjacent to matrix values to find similarities in computation requirements
          - apply similarity structures
            - find values in matrix having a common factor (base) and standardize operations involving those values
          - apply sequence structures
            - find sequences in multiplication operations & apply sequence operations rather than individual calculations
              - find numbers in even number sequence (common factor of 2) and reduce to addition of coefficients of powers of two
                - 3 * 5 + 2 * 6 + 2 * 4 =  3 * 5 + 2 * 2 * 3 + 2 * 2 * 2 = 3 * 5 + 3 (2^2) + 2 (2^2) = 3 * 5 + 5 (2^2)

      - apply function interface

        - find functions that convert multiplication to addition or other lower-cost problem
          - replace/substitute
            - identify when multiplication can be replaced by addition
              - addition can replace a multiplication, if an adjacent multiplication has already been done
            - convert numbers to efficient multipliers like powers of 10 that involve moving digits rather than multiplication

      - apply core interface

        - apply core functions (replace) & core structures (unit) to problem components (problem functions of multiply & add)
          - apply interface interface (standardize problem to interfaces of problem space)

          - apply system interface
            - apply system structures
              - apply efficiency structures
                - identify efficiency structures in problem 
                  - inefficient operation (multiply)
                  - efficient operation (add)

                - apply change interface
                  - connect an inefficient function (multiply) to an efficient function (add) to change inefficient function to efficient function
                    - define one problem function as a transformation of the other problem function
                      - define multiply in terms of add using core functions/structures or problem functions/structures
                        - apply replace to one unit of original multiplied values with an add operation until multiply is defined in terms of add (standardize to add interface)

                - apply efficiency structures
                  - apply efficiency structure 'apply one operation instead of multiple operations'
                    - identify when multiple multiply operations can be replaced with this type of adjacent multiply/add operation 
                      - identify when a multiplication operation can produce an interim value in between other values so the multiplication can be re-used for another value

          - apply structure interface
            - apply structural interface structures
              - apply filter structure
                - identify matrix cases where these operations are inefficient or unusable
                  - identify operations/information needed to determine inefficiency/unusability of this solution
                    - apply function to determine threshold value for matrix dimensions or metadata like value variability (if values are in a known range or have a known type): 
                      - 'if there are more than x adjacent values with an interim value in a matrix of size n x n, this method can save computation steps even with the determining operation'
                    - add average cost of determining operation to cost metric (computational complexity)

      - apply system interface

        - apply system structures
          - apply efficiency structures
            - apply efficiency structure of 'reusing existing resources'
              - identify what resources exist or are created in original solution (values output by multiplication & addition operations)
                - identify condition where these can be reused for other operations
                  - when other operations are adjacent
          - apply symmetry structures
            - apply symmetry structure of 'interim value one change unit away from multiple values - one being addable in the position of a coefficient'

## apply insight path to solve a problem

	- insight path:
	    - when generating solutions, identify:
	      - contexts/cases/conditions that can filter it out
	      - variables that can generate the most solutions
	      - filters that can filter the most solutions
	        - apply filters to solution space by solutions that are ruled out in fewest cases, best cases where solutions are less required or least probable cases
	    
    - example problem: how to put shirt on underneath jacket without taking off jacket completely
      
      - alternative queries

        - identify sub-problem: 

          - find a format where sequence (shirt on top of jacket) can be changed into solution format (jacket on top of shirt)
            - identify adjacent format 'bunching into circle around neck' that allows changing sequence (which is on top) and transformation function into that format from origin format 'taking off sleeves'

        - apply adjacent formats to problem & solution formats
          - identify formats that have a sequence (stack, row) which is a structure implied in the solution format ('underneath')
            - apply functions to test if shirt can be transformed into one of those formats

        - generate adjacent functions (bunching) from core functions (move sleeve, lift, rotate) & try them to see if any useful structures emerge moving objects closer to solution formats/positions
      
        - generate default connecting function and apply structures of optimization (reusing functions, avoiding extra steps) to improve the default connecting function incrementally

        - identify filters that can filter out solutions

          - identify filters interacting with structures of variables (change types, potential, uncertainty) & constants (requirements, limits, definitions)
            - possibility filter: 
              - interaction filter:
                - in what ways can the shirt/jacket interact
                  - can the shirt occupy position (fit) under the jacket
            - requirement filter: 
              - does the shirt/jacket have to stay in its current position/format
              - does every step of functions ('removal' function) have to be executed (can you just remove pieces, like the sleeves, without removing the whole thing)
            - change filter: 
              - in what ways can the shirt/jacket be changed while remaining a shirt/jacket (bunching, removing sleeves) 
              - are these ways reversible (can it be put on after being taken off)

          - apply filters to reduce solution space
            - solution can involve variables:
              - position 
              - format
              - change functions (bunch, lift, remove)
              - components (sleeve)
              - interaction functions (stack in sequence)
            - solution must fulfill requirements
              - jacket must be in 'worn' position at all states
              - change functions cant change object identities (change jacket into shirt or into a not-jacket)
              - solution must reverse sequence of objects in stack structure)
            - any solution involving removing the jacket completely in a state, change functions that change object identities, and where solution format is not fulfilled are ruled out
            - other tests include:
              - minimize steps (did solution do any unnecessary steps)

## apply insight path to solve problem of 'finding factors to produce number without using multiplication of every combination'
    - insight path: use filters to reduce solution space instead of generating solutions (such as by identifying metadata of solutions & applying combinations of those attributes)
    - problem: find factors of 28 without using multiplication of every combination (trial & error)
      - factors of 28: 1, 2, 4, 7, 14, 28
      - remove: 1, 2, 14
        - divide by integer unit 1, divide by 2 bc even, divide by co-factor of 2 which is half (select midpoint without multiplication))
      - the remaining candidates are: 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13
      - apply filters to solution space
        - apply similarity of value structures as a filter
          - adjacent items can be ruled out by proximity (for example, 13 couldnt be a candidate bc its too close to 14 to be a factor of such a small number)
            - the remaining candidates are: 3, 4, 5, 6, 7, 8, 9, 10, 11, 12
          - apply similarity (of adding factors to sequence) as a filter
            - test sequences for adjacent computations
          - apply similarity of components (factors) in definitions (numbers definable in terms of their factors) to find relevant structures
            - test primes which are relevant bc of their definition being definable in terms of the factor standard
        - apply output patterns as a filter:
          - multiples of 10 and 5 can be ruled out bc it doesnt end in zero or 5
            - the remaining candidates are: 3, 4, 6, 7, 8, 9, 11, 12
        - apply combination structure to produce solution format (multiplied pairs of factors)
          - pairs are a combination structure
          - the remaining factors can form pairs, which can also have filters applied
          - apply filters to pairs 
            - apply output requirements
              - metadata of the output, 28, includes that its an even number, so multiplied pairs must produce an even number
                - odd number x even number can produce an even number
                  - 3 x 4, 3 x 6, 3 x 8, etc
                - even number x even number can produce even number
                  - 4 x 4, 4 x 6, 4 x 8, etc
              - apply reduction tests (what could not be the solution)
                - apply tests to inputs
                  - inputs must be spaced according to the output number
                    - adjacent numbers are unlikely to produce the output number (as a multiplied pair) for an increasing output number
                  - structures of inequality (not equal to solution)
                    - too large
                    - too small
                    - not even
                  - identify threshold structures (values) of input structures (values, value pairs) that would produce one of these inequalities
                    - filter out inputs if they would produce an output that was too large to be 28
                      - 28 is quite a small number so pairs of numbers above a threshold value (3 x anything above 9, etc)
                      - some pairs are clearly too big to produce 28, without checking the product
                        - 11 x 12 is clearly too big, so can be removed from list of possible pairs

## apply structures of concepts into function to find prediction functions
  
    - apply structure of time (state) into algorithms (network state algorithm)
    - apply structure of hypnosis (multi-interface alignment) to algorithm (hypnotized algorithm is static & cant learn, which is an error type)
    - apply meta structure to algorithms
      - an algorithm that cant see its own error types is one that cant:
        - change its perspective/position
        - change the variable creating the error type
        - receive negative feedback for errors
        - apply negative feedback to correct structure (like direction)
        - identify costs (indicating why its an error, as in what resource is lost)
        - structures that depend on the outputs of their distortion, becoming dependent on their distortion
        - structures that cant develop a function to correct the error (a power source that cant develop a power distribution/delegation function)
    - organize list of structures required for system optimization & make diagram & generative insight path & query
      - concepts
        - anti-chaos structures (organization)
        - lack of requirements (dependencies): an optimized system operates in a self-sustaining, self-improving way with as minimal requirements as possible with existing resources like functionality, and with decreasing requirements over time
        - multiple alternatives
          - example: having multiple definitions of cost avoid errors like 'lack of flexibility due to over-prioritization of avoiding costs like pain' and instead be able to sustain one cost type to reduce another cost type, for a duration like 'as needed' or 'while advantageous'
        - anti-complacence structures (checking for new error types that cant be measured with existing tools yet by always building new measurement tools)
        - other structures for optimizing systems
          - anti-complexity
            - apply filters to remove information that is repeated without value added
          - anti-trust
            - apply tests regularly to system components & structures of them, checking it for new variance sources & error types as well as known sources/types
          - anti-dependency
            - apply solutions to optimize system that increase similarity of components in the direction of independence, distributing functionality across components (like cross-training)
          - anti-static
            - add solutions that dont remove possibility of generating other solutions/error types (thereby reducing the variation the system can handle)
      - functions
        - apply error types to check a system for known optimizations (error types like 'structures that seem similar but are not')

## apply structural definition routes of adjacence (minimal units of work) to find efficiencies 

	- find efficiencies in core functions (multiply, find integral/derivative, find efficient method to calculate difference) by applying structures of adjacence (core functions) and clarity (isolatable structures, definitions)

	    - find product of factors
	      - apply core, pattern, & structural interfaces
	        - find pattern structure of factor sets (function connecting factor sets) & use that to calculate using more efficient addition/subtraction operations
	          - find approximating function given pattern function (adjacent more calculatable pair with adjusting operation)
	          - find derivation function of a factor in a set, given another factor & pattern structure

	    - find function for integral
	      - apply core & structural interfaces
	        - apply combinations of core components (coefficients, powers, values) to find equivalence to area

	    - find function for derivative
	      - apply core & structural interfaces
	        - apply core structures (like unit) to reduce calculations

	    - finding method to calculate difference: 
	      - apply intent, core, structure, change interfaces
	        - intent: differentiate data point clusters in a clear (easily measured) way
	          - identify problem metadata
	          - apply one-degree change to each attribute, like variable count
	            - add/subtract variable count
	              - list new components & component changes
	                - new variable
	                - new variable structures (combinations, connections)
	              - apply units of work to new components or changed components
	                - find functions of differentiating values (positive/negative, multiplication) & attributes (value range allowing very different values) for new variable
	                  - add variable of differentiating values to make overlapping 2d clusters clearly separable in 3d

## apply structures to generate an assumption identification function

    - define 'assumption' with alternate interfaces, like info/abstraction, filtering for assumptions that cause errors
      - definition route: any specific info is a potentially problematic assumption
      - example of an assumption: solving the problem by asking 'what function in the software caused the problem' assumes that the stack variable is a constant ('software' part of the stack), when really other variable values should be examined
      - since specificity is the root cause of the problematic assumption, remove specificity in the form of a constant by applying the opposite structure (change types to variable values)

## identify & apply optimal structures to connect problem & solution, using alternative definition routes & error structures

    - original problem statement: 
      - 'object is over-reduced'

    - identify optimal format to solve problem in:

      - standardize definitions of problem system components
        - standardized definition of 'over' = 'excess', which is a known error type causal structure
          - standardized problem statement: 
            - 'object has error of type excess, applied to reduction function applications'
        
      - identify adjacent error structures & alternative definition routes of problem components (or iterate through error structures, checking each for fit to problem components)
        - adjacent error types & definition routes of 'excess' include: 
          - imbalance
            - solution format would involve finding balancing structures - a more abstract (less clear) solution format than a difference from a standard
          - mismatch
            - solution format would involve finding matching structures between object & the system context - also a more abstract (less clear) solution format
          - difference from standard
            - 'difference from standard' has a clear solution format, in the form of a path structure, from the standard (origin) format to the distorted (over-reduced) format
            - this solution format is clear because it involves more core structures like 'distance', with clear mappings to the problem system components ('difference' mapped to 'distance' of 'network path' structure, measured in 'number of differences' as steps between origin & distorted object versions)
            
    - apply optimal format to problem:

      - problem, formatted using distortion structures as an error structure: 
        - over-distortion, caused by over-applying 'reduction' function 

      - solution, formatted using distortion structures:
        - reduction function of the reduction function, applied to un-distort distortions ('differences from standard')

## apply structures of definition routes of a concept (usefulness) like conceptual attributes such as clarity/adjacence

  - function to check a format for structures of usefulness/relevance like clarity, adjacence to determine usefulness/relevance of the format to a problem
    - check if 'difference from standard' is a useful (clear, adjacent) format for the problem 'object is over-reduced'
      - standardize problem statement: 
        - standardized statement: 'excess' applications of 'reduction' function to 'object' component
          - find standardized statement components:
            - 'object' component
            - 'reduction' function
            - 'excess' applied to 'apply function' function
              - formatted definition: function.attributes['call count'] excess
                - find structure of this definition:
                  - structure: 
                    - difference (integer) between optimal function.call_count and excess function.call_count
                  - check for adjacent method to find structure in problem system
                    - find structure of a difference formatted as an integer, in a problem system formatted in standard formats
                      - iterate through standard formats for problem
                        - function network
                          - network of problem functions, including 'application' and 'reduction'
                        - state network: origin state & excess state
                          - alternative format: state network with origin at center & distorted state, separated by distortion function nodes
                            - this format has a structural similarity between count attribute of 'distortion function nodes' and function.call_count attribute format, as both are in integer format
                            - check if this format is adjacent to convert problem to (low-cost, or similar)
                              - steps to convert problem to this format:
                                - map standard to origin
                                - map function.call_count to differences (steps away from origin), structured as distortion function nodes (representing the 'application' function that calls the 'reduction' function)
                                - map excess to distorted position, function.call_count steps away from origin
                  - if the conversion steps of that format are lower cost than those of other formats, try this method to see if the format is useful as well as adjacent
                    - check if applied format is 'useful', defined as:
                      - solves the problem
                      - makes the solution clear
                      - reduces the tasks necessary to solve the problem (connect problem & solution)
                    - once problem is formatted as a set of distortions from an origin, is the solution:
                      - reached (new problem format equals solution format)
                        - the format itself doesnt solve the problem - the object is still over-reduced
                      - clear
                        - the format adds clarity without losing info - the object & relationships are accurately represented, in a simple format
                      - fewer steps away
                        - the remaining steps to solve the problem involve connecting the new format ('differences from standard (origin)') with the solution format ('object is not over-reduced')
                          - remaining steps include:
                            - standardization of solution format
                            - converting standardized solution format to current problem format
                            - finding a connecting function
                          - example logic of remaining steps:
                            - standardize solution format:
                              - find structures relevant to problem & solution format
                                - 'over-reduced' and 'not over-reduced' imply the 'opposite' core structure
                              - apply 'opposite' structural definition to find structures relevant to the problem
                                - 'not over-reduced' applied to the problem can mean:
                                  - 'less reduced than excess position'
                                  - 'origin position'
                            - convert standardized solution format to current problem format
                              - convert 'less reduced object than excess position' to 'differences from standard (origin)'
                                - 'less reduced' applied to excess position in 'differences from standard' format has structure:
                                  - 'fewer differences (steps from origin)'
                                    - fewer can mean:
                                      - any integer less than current number of steps associated with excess position
                              - the converted solution format:
                                - 'less steps away from origin than excess position'
                            - find connecting function of converted standardized solution format & current problem format
                              - find 'opposite' structures of 'reduction' function:
                                - find 'opposite' structures relevant to an 'excess'
                                  - reduce the excess
                                  - convert the excess to zero (if zero is acceptable structure for solution format)
                                  - remove the object in excess (if zero is acceptable structure for solution format)
                                - find 'opposite' structures relevant to a 'reduction'
                                  - increase the component quantity that was reduced (object dimensions)
                                - find 'opposite' structures relevant to a 'function application' (call_count)
                                  - neutralizing
                                  - invalidating
                                  - reversing
                                  - reducing
                                - find opposite functions
                                  - find function that reduces the excess
                                  - find function that reduces the reduction
                                  - find function that neutralizes/invalidates/reverses/reduces a function.call_count

                      - this may not be fewer general steps away:
                        - every problem format change requires:
                          - checking new problem format for difference from solution format
                          - finding a conversion function to convert the standardize solution format into the current problem format
                          - finding a connecting function for the current problem format & the standardized solution format
                        - every solution format requires:
                          - standardization (can be done at beginning of interface query)
                      
                      - but the logic for these steps may be adjacent to create/derive, or it may already exist, so that solution fulfilling the general steps is trivial to assemble with existing logic
                        - example logic that would already be defined: 
                          - standardize structures
                          - pull definitions
                          - find similar structures
                          - find relevant structures (meaning)
                          - check for matches in similar structures
                          - check for usefulness (reduction of solution steps, clarity, or solution) of structurees

                      - other filters can then be applied, like intent (does the format make it more efficient to fulfill a problem-solving intent relevant to the problem)

## apply structures to find connecting functions

  - integrate (align & connect) structures of functions on multiple interfaces:

    - concept: 
      - 'aesthetic': generating aesthetic functions using simple/balanced/relevant structures, using assumption that aesthetic functions exist to connect variables
    
    - pattern:
      - generating formulas based on patterns & anti-patterns of other formulas
    
    - structure:
      - using limits that bound other formulas as assumptions to reduce solution space
      - finding vertex variables of formulas & applying variations to generate other formulas

## apply question (info asymmetry) structures to find answers to questions

	- questions have the structure of a possible connection sequence forming a path in the problem system, formatted as a network

      - the patterns of these questions in producing relevant info for a problem can be used as insight paths
      - alternatively, apply a general insight path of calculating which paths in the problem network have the sequence of input/output information that could produce the answering info to the query
        - formatting the system with structural interface metadata (such as info gaps, intents, incentives, equivalences, & vertex variables) will make these optimal query patterns more obvious

      - identify the connection between components with the uncertain connection using inputs & definition routes of the connection
        - example: 
          - find connection function: 'is it object A' uses the 'equal' connecting function
          - find inputs: the 'equal' connecting function uses the 'definition' object as an input
          - generate the interface query to solve this problem:
            - 'to determine equality, find the definitions of the objects whose connection is uncertain'
          - which can be abstracted into the solution automation workflow insight path:
            - find the inputs of the uncertain connection function and apply them to connect the objects with the uncertain connection

      - example questions:

        - is it object A (the uncertainty is whether 'it is equal to object A')
          - check definitions of object A & referenced object (it) for equivalence => if matching, convert to declarative statement with boolean => yes, it is
        
        - how to connect variables a, b, c with variable d in the direction of variable d (the uncertainty is 'are a/b/c predictive of variable d')
          - apply change interface to question
            - identify change functions applied to variables (or structures of variables) (or their components) that could change variables a/b/c into variable d, or move them to variable d's position
          - apply structural interface 
            - position variables in a variable/function/object network
            - convert to structural question: 
              - can structures of interaction between variables a, b, c, or their attributes/functions/components create variable d
                - apply structures (combinations, sequences) of interaction to variables a, b, c & their attributes/functions/components

## apply structural definition routes to differentiate similar or related concepts
	- change: sequence of difference structures
	- difference: non-equivalence on some metric
	- variable: attribute capturing an isolatable change type

## apply structure to find alternative filters/routes & identifying optimal filter/route structure, as well as optimal starting point (origin), direction (target) & steps (queries) to generate them

    - the below 'reverse engineering' example uses the following filter query to determine relevance, reverse-engineering a definition of relevance that can be used to find relevant structures, a definition that is formatted as a set of filters, using a structural definition of relevance (similarity)
      - relevance = reverse(similarity => core => combine => not structural alignment => adjacence)
      - relevance = a structural definition of relevance (similarity), with core functions derived, core functions which are used to create function combinations, which can be applied to the original structure to find adjacent structures, filtering out similarities that are one-interface similarities (like structural similarities) rather than relevant similarities (multi-interface similarities)

    - but it could also use alternate solution filters to find relevant info to the solution such as: (substitute || (similarity && quantity) || test)
      - apply 'substitute' structure: find a metric that functions as an identifier, filter, approximator, predictor, or proxy
      - apply 'similarity' structure to 'quantity' attribute: find a metric value for a quantity of more than one unit
      - apply 'test' structure to problem system structure: find tests with output information containing the metric value

    - these alternative filter sets optimize for metrics like:
      - filter set metadata
      - optimizing for different interface metrics (variance degree, interaction layer, abstraction level)
      - having a particular structure (paths to connect source/destination) that uses available functions
      - maximizing a particular change or difference type for identification/accuracy-related intents
      - connecting difference types in different spaces (standardization)
      - interface structure-fitting (like 'intent alignment' or 'lack of contradictions')

    - these alternative filters have different metadata, like:
      - cost
      - variation sources (equivalence definition)
      - variance reduction (degree, type, pattern, potential)
      - requirements (like required information access)
      - path (in the filter network, & also possibly a path in the problem structure network)
      - interfaces, structures, & definitions used ('questions' asked by the query, 'alternatives' used as 'approximations')
    
## apply structures to identify consensus perspective between opposing perspectives
	- transform a structure in each perspective to a structure in the target perspective
	    - identify structure of attributes/functions/objects common to both perspectives
	    	- connecting functions like: 'function connecting power and distribution', 'function describing dictatorship dynamics'
	    - identify interface objects within structures
	    	- change type in connecting function: 'direction of power distribution', 'changes in identity & size of group in power'
	    - identify similarities in interface objects within structures
	    	- similar change pattern in change type in connecting function: 'power favoring distribution', 'military coups after power abuses'

## apply structures to identify an object like 'contradiction' (contradiction of a statement, formatted as a route between network nodes)
    - query for conditions that would convert some input, component, or output of the statement function route into some structure of falsehood (invalid, impossible)
    - example:
      - query for intents that would require movement in different directions than the statement function route requires
      - query for causes or preceding/adjacent/interacting functions that would require development of functionality making some step in route impossible
  

## combine core operations (rotate, connect, combine, shift, filter) to convert the base subset/limit functions building or used by a neural network into the output prediction function

## example of applying interface structures to neural networks (core functions, interaction layers, etc) to generate different organization structures as components of a new neural network typ
	- organization structures represent applied concepts & structures like balance, functions/attributes like relevance/security, error type boundaries, abstraction levels, etc
    - a granular intent structure like "differentiate => maximize => combine => compare => select" can map to a high-level intent like "voting"
    - these structural equivalences/similarities across interaction layers (like different abstraction levels of intents) can be used to implement concepts like 'security' to neural networks, such as identifiable/possible error type structures as a boundary/limit (in the form of a threshold or weight-offsetting operation) across a metric calculated from an adjacent-node cross-layer sub-network (like 'function sequence' structures are often used in exploits)

## structures applied to these core structures to generate conceptual structures in neural networks
      
      - variables of the network include structures emerging from or embedded in algorithms/structures

      - core structures
        - change types 
          - difference type 
        - agency types
        - cause types (influence/power of structures)
        - structures
          - sequence (embedded concept of 'time' in structural interface)
          - list (unique index)

      - alternative cause: change applied to causal structures at training & prediction time
      - organization: difference type index
      - agency/govt: decisions about change types to apply

        - structures applied to agency objects like decisions (such as subsets/alternates) & other conceptual structures (like time)

          - sub-decisions
            - structures of neural networks with delayed sub-decisions
              - conditionally activated cell structures with enough info to make a sub-decision
            - structures applied to decisions can generate networks with other decision structures than 'consensus voting'
              - govt structures/algorithms
                - organization structures are a structural version of govt (agent-based) decision-making
              - finding the level of 'agency' to apply to a network is possible with problem complexity identification
                - apply agency: delegating decisions to subsets/groups/layers of cells to delay change decisions to another point in time

          - alternative decisions to make in interface query
            - decisions are a 'selection/identification/filtering' problem about a possible change type (like direction) to consider/implement
            - structures of neural networks exploring alternative variable structures & alternate decisions rather than the stated problem decision or default variable structure (identify direct causation, filter out non-directly causative variables)
              - alternative decisions
                - finding root cause
                - solving a proxy problem

          - decision (change-filtering problem-solving) times

            - standard time points: training time, data gathering/processing/standardization time, decision/prediction time, re-training/update time, parameter selection/update time
              - sub time points: activation time, pooling time, aggregation time, filtering time

            - optional points where decisions can be injected
              - decisions: 
                - network-level decisions: continue learning, select prediction answer
                - structural decisions: change direction, identify threshold, ignore info
                - meta decisions: delegate/delay decisions, consider alternative decisions
              - time where decision is clear/final/starts to emerge
              - time where direction change decision is made
              - time where more info/time is identified as necessary
              - time where decision is identified as not answerable
              - time where alternatives are identified, assigned probability, filtered out
              - time where possible routes to an answer are identified (what structure of variable values like 'ranges' can produce a clear answer)
              - time where possible decisions remaining are identified (and conditional remaining decisions if a change is applied)
              - time to check for a structure in the difference type index
      
## applying structure to structure to generate a particular structure/format (structure standardization)

    - example of converting structures into vectors
      - many vector structures can represent interface structures
      - example of selecting a vector structure to represent an interface structure on a particular interface, applying structure to indicate metadata about structures
        - example: causal loop
          - standard network structure translation: vectors to indicate direction of cause
          - relevant network structure translation: vectors of influence degree away from hub cause & other cause structures

## example of applying structure to identify rules that violate a metric 

    - requirement like: 
      - 'dont exacerbate inequalities'
      - 'protect minorities on the disadvantaged side of an inequality'
      - 'identify advantaged side'
    - power structures: required or non-specific/universal resources (such as inputs to any function, like 'energy' or 'information')
    - inequality structures: differences in distribution of required resources
    - generate structures that would exacerbate inequality structures
      - assumptions in rules (lack of guaranteed potential to follow rule)
        - rule 'close malls after business hours'
          - rule structure: 'limiting supplies' (access to facility)
          - rule assumption: that they have alternative supplies
        - rule: 'fine for not wearing mask'
          - rule structure: 'requiring function' (purchase mask)
          - rule assumption: that they have inputs to a requirement
      - these assumptions would disproportionately increase inequality's disadvantages in distribution
      - 'disadvantaging rules/assumptions' can be distributed more evenly or to offset inequalities

## example of applying structure to generate problem types

    - for instance, a common problem type is a mismatch/imbalance
      - by applying the 'mismatch' to the cost/benefit relationship, you get an 'inefficiency' problem type, which can be defined as a mismatch/imbalance between the cost & benefit, favoring the cost side (which is the negative version out of the cost/benefit combinations, negativity being part of a definition route of a problem)

## example of finding alternatives (alternate variable sets) in a problem space (exercise) for problem of 'predicting a change type' (predicting motion) 

      - apply interfaces to find relevant structures

        - exercise variables:
          - info (about optimizations, possibilities, rules, metrics)
            - attention/memory to focus on, remember & apply info
          - patterns
          - structures
            - point (metric threshold values, change points, decision points)
            - sequence: 
            - combination: multiple variables to make a decision
            - limits: time limits, energy limits
            - context
              - health
              - energy
              - environment
                - landmarks
                - agents
                - interactions/events
          - time
            - time structures (alternation, number of seconds, continuity of pattern applied)
          - functions 
            - core functions (test, start/stop, switch, remember, identify)
            - interaction level functions (decide when to speed up, plan decision points)
          - concepts
            - energy
            - agency
            - intent
              - exercise intents: recover, rest, test/find limit, test function, switch energy sources, apply info, identify landmark, align with music
              - other intents: what to do after workout, scheduling limits to work around, listen to new music, listen to music limited number of times

      - apply interface structures (like combination) to relevant interface structures found in problem space (like 'health' concept) to generate solution space (possible prediction variable sets)
        - alternative variable sets that can predict motion:
          - apply filter structures to problem & solution structures like 'opposite' (what cant be a solution)
            - time cant be used as a base on its own bc usage patterns may offer the illusion of equivalent alternatives that are actually different
              - example: pattern 'a-b-c' may occur just as often as 'a-b-d' without any distinguishable signals using available time info, so other interfaces need to be applied to predict c or d, such as contextual/intent probabilities, or patterns like intent patterns or change patterns 
          - agency rules
            - agents have known intents, which interact in a known way
          - interaction rules
            - energy, time, agents, & health interact in this way
          - energy rules
            - 'energy can be used to produce energy in other formats'
            - 'stored energy can replace agent prioritization'
            - 'excess energy can have these outputs when used optimally'
            - 'energy efficiency increases with usage'
            - 'high variation in usage increases energy coordination & distribution'
            - 'brain & muscle energy are related, in a pseudo-tradeoff'
            - 'high variation in energy usage can offset energy plateaus'
          - variable interaction patterns
            - 'using n number of variables to make a decision only occurs once out of every x decisions'
            - 'applying previously applied variable interaction rules is most common'
            - 'excess energy results in higher variability of variable interactions'
          - concepts
            - concepts & concept structures (concept set including 'energy' or 'health') can predict independently of other variables bc theyre a low-dimensional (conceptual dimension) representation of high variation (motion)

