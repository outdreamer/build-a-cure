- to do list items

    - example of resolving a conflict between structure/limits using a structural similarity between a structure (gradient of function) & its container/limits (gradient of constraints)
      - https://en.wikipedia.org/wiki/Lagrange_multiplier
      - also an example of a solution space (the whole function is the solution space of possible minima/maxima) and a filter applied to it (constraint)

    - make a function network of math domains (inputs/outputs of geometry, algebra, calculus that align)
    - finish list of logical rules
      - similarities
        - equivalents in connections: components of connected rules are equivalent by some metric (interchangeable, equivalent identities, sufficiently similar where differences are irrelevant, etc)
        - definition alignment: applies & aligns with definitions
        - equivalents in meaning: the meaning of a rule in one context (such as the interaction level) is equivalent to the meaning of the rule when applied to another context (it fits into the system the same way so the meaning is the same)
      - includes where possible, except as relevant/required
        - components & structures of certainty like facts where possible, and where not possible, certainty-deriving methods like math/stats & definitions
      - excludes where possible, except as relevant/required
        - uncertainty structures like ambiguities
      - example:
        - patterns are logically a relevant object to prediction functions, given the nature of the problem (predicting a variable with insufficient info, such as info building understanding about the actual variable connections)
  
    - resolve definitions of components so you can finish organizing useful structures like combinations of concepts such as "format sequence", "solution automation workflow", "insight path", "reverse-engineer solution from problem requirements or opposite structures", "connect problem & solution"


    - calculating a flow field numerically can be optimized by calculating:
      - possible/probable/fitting components of functions 
        - connections between prior calculations, to find connection structures that re-occur, as the output of a function
          - find core connection structures like waves, peaks, inflection points
        - connections between connection structures, to find: 
          - patterns in connections between:
              - identified patterns of difference, like degree of difference observed between adjacent points
              - structures of approximation & accuracy (what kind of structure could add prediction accuracy, in what contexts)
              - base functions & base function components & useful/stable functions
              - similarities in structures of difference, between function structures like variables & error structures like distortions
            - variables of & distortions to those variables common to connection structures & connections of those structures
          - connection components that coordinate for some intent
            - intents like calculation in an input/output sequence, or fitting into a structure that is efficiently compressed or retrieved
              - incentivized connections that are optimal for some calculation intent
        - generate & test predictions of future calculations, to find local/adjacent prediction functions
          - generate set of prediction functions to test predictions
            - apply operations & structures known to produce common connection structures to calculations in reverse to derive possible function components
          - generate set of prediction function filters (extreme prior connection structures) to select prediction functions
          - generate set of opposite structures (like non-adjacent points) to test predictions
      - filters of functions (reduce solution space of possible function & function components)
        - filtering functions by these connection structures & patterns, filtering out functions that wouldn't adjacently produce these connection structures
      - interaction structures of functions & function components
        - function components that can interact by coordinating, vs function components that invalidate each other & can't coexist absolutely or be adjacent locally/directly
      - function & function component causal structures
        - apply patterns of causation between functions (function of structure A causes function of structure B) to identify the level of cause a function is based at


    - example of how to generate monopoly case arguments
      - change variable 'location of power':
        - spotify is welcome to build their own app store with their own phones or team up with their coalition to do so
        - add variable 'time sequence' to 'location of power' change:
          - if spotify operates an app store someday, they will set rules to benefit themselves too, just like theyve done in the past
        - offer an alternative to charging app store rate
          - is there a one-click button to migrate from spotify to apple that could replace any difference in taxes on spotify
      - apply conceptual definition filter 'does concept of persecution (and related components of the definition like focus) apply to the behavior (does behavior have a specific target that is the focus of persecution)'
        - are apple's rules applied exclusively to spotify? if not, it's not anti-competitive behavior
      - apply intent filter
        - is spotify's mission nobler than apple's
      - apply system cost-benefit analysis
        - what features were improved bc spotify exists? are those features worth anything or required needs? did they develop those features better than competitors?
        - if spotify is just charging rent on a catalog, are they adding value to the market, so they should be allowed to dictate the market at all?
        - what products/features would apple develop if they didnt have to pay a fine, and what are those features worth, and are those features required?
      - apply logical fallacy filters
        - apply 'hypocrisy' filter
          - apply 'anti-competitive' conceptual definition structures & test if these structures fit the opponent
            - does spotify plan on raising prices at some point or will they keep prices low even if the app store rate holds? are they only keeping prices low to dominate the market & plan on raising prices later? isnt that anti-competitive behavior?
            - if they are so concerned about anti-competitive behavior, why arent they trying to compete by building their own app store? isnt there a risk that the apple app store is sub-optimal and needs to be improved with competition from spotify

    - value isnt created/lost by companies in the timespan of hype/short cycles, so stock market price swings aren't reflective of reality from a macro perspective
      - it takes years to build value, it doesn't happen overnight, excluding almost magical insights that create cascading efficiencies like my system
      - losing value also happens slowly, excluding extreme natural disasters, like the value of a community still being relatively high despite shared losses, bc of social network effects & organization/coordination effects
      - value can be calculated differently, using metadata like the lifetime & total possible value of a product
        - what is the total supply of the product inputs (fossil fuels)
        - what is the usability lifetime
        - what are the costs
        - what are the product intent alternatives (can intents fulfilled by the product be fulfilled by other products)
        - what is purchased with revenue from a product (research, insights, other more valuable resellable products, etc)
        - if this pricing method is applied to fossil fuels, oil companies would be paying people to use them

    - identify filters for definition routes

    - database polling/prompting user for update & predicting updates or searching for & receiving user-approved updates from other services, rather than being a passive receiver of input from user
      - based on local usage/change patterns or integrated usage patterns to identify expected transactions with other services
        - once a credit card is marked as lost in a banking service, a change of credit card numbers is expected by other services which can poll for updates to this flag
          - user option like 'yes, allow other approved databases containing my address poll to collect this update'
        - once a renters insurance policy is changed in an insurance service, a change of address might be expected by other services

    - make list of variable structure variables measured by algorithms & why they are measured by a network algorithm

  - make diagram of absolute reference connections with metadata structures like networks/paths
  - determine core graph variables (definition of adjacence/difference, connectivity, dimensions, info storage methods, interactivity of structures like sequences)
  - crypto as community consensus, where a decision can have value if backed by a community

    - examine connection between fractals, sequences, averages, origins, multipliers (self, as in power), & circles
      - fractals as a relevant structure for adding sequences of fractions (adding numbers similar to itself on a smaller scale, infinitely) as a way of producing inputs to circles created by transforming a fractal spiral, where the origin is the original number as a base for applying increasingly smaller scales
      - the set of points forming roots of an infinite negative number sequence (roots of unity, rather than roots of any negative integer number) as a way of producing a circle because of their common distance to their average (center) forming the radius
      - lines of equal length having a common average point (center/origin)
      - fractals & infinite sequences as a way of calculating area under a continuous line (increasing small subsets of structures with area calculatable with multiplication of x & y)
      - what continuous line segments would have an area equal to a circle of relevant proportions?

  - add to useful concepts

    - bitcoin and ai both benefit from integrating the concept of time into existing inventions (tx history, weight updates)
    - what other concepts could be equally powerful if injected into structures of existing inventions
    - why is time (or the structure of sequence) a powerful concept in those problem systems
      - historical info integrated into current & imminent info was a pre-existing gap in relevant info structures of the problem system
      - the usefulness of historical info wasnt identified or wasnt identified as integratable into existing inventions
      - the related concept of:
        - 'connection' could have served as a replacement, indicating relevance of previous info to new or derived info
        - 'position' could have also replaced 'time' or 'sequence', given the relevance of 'position' as a predictor of financial transactions and connecting one info state to another, given data that can access the destination from that origin
      - its also inherently relevant to know how space-times (states) connect (like in a sequence structure) in order to predict adjacent imminent space-times (or states) as imminent members of the sequence

  - exploit filter structure examples:
    - anomaly
      - non-standard data flows
        - does data normally follow this pattern
    - requirements
      - is there a required basis for communicating info
    - intersection:
      - data * time/sequence
        - state/content of a data flow intersecting with a possible access chain
    - code x time/sequence
      - state of a build to check intent multiple times during build phase of code

  - bio system general strategies
    - calculate & inject vulnerability in pathogen dna language
      - trial & error implementation example: 
        - use dna code switcher to apply change to all positions in pathogen dna
      - common pattern implementation example:
        - inject known dna error types to see if any work on new pathogen
    - standard pathogen dna to host dna language


    - how to identify the killer counterpoint
      - point: 'election fraud claims'
        - counterpoint: you dont think the other party has members?
          - followup points:
            - how are they organizing? why do you even hear about them?
            - who's benefitting from investing massive funding in creating a false illusion, if its all fake?
            - why wouldnt they choose cheaper methods of doing so than an elaborate illusion?
      - identify most extreme false assumption of point by identify causes of the output metric (vote count)
        - the primary/basic false assumption of the point is that the other side doesnt have votes that are comparable in quantity
          - possible causes (inputs) of 'not having similar vote count' include:
            - not going to vote
            - not being able to vote
            - not having members to vote
          - the most extreme false assumption is the most extreme cause of 'not having similar vote count' (that the other side doesnt even have members (input of votes) that are comparable in quantity, let alone votes (output))
      - identify 'incentives', 'side effects of party/member/vote existence & size' as other relevant concepts to generate followup questions




- examples

    - add to definitions
      - reason as source cause node ('this happened because of this causal factor, which can be an intent')
      - intent as target cause node ('this happened because this intent was the goal used as a cause guiding direction of decisions')

    - add to causation variables
      - ability to change (if a variable cant be changed, it is less causative for problem-solving intents)

    - give examples of identifying vertex variables
      - general vertex variables: topic, origin/destination, reason/cause/point/intent, errors, variables, types
      - comedy vertex variables: sincerity, stupidity, stakes, tension/expectations
      - music vertex variables: tone, structural tension-resolution pattern variation, lyrics
      - optimization vertex variables: solution metric patterns (what other solutions optimize for)

    - when is it optimal to store a mixed structure of varying specificity (like a type, intent, cause & a specific example)
      - when there are potential uncertainties to resolve, like whether the example represents a new error, type, or variable, bc the example doesnt fit known structures

  - proof/complexity/computation structures

    - what makes something easy to calculate
      - a solution structure where the solution metric is clearly defined (structural or having other structures of certainty like consistency or inevitability or requirement)
        - checking a path to see if it includes a node twice is clearly defined (it uses the structure of 'node visit counts' in the 'path' solution structure)

    - what makes something difficult to prove
      - where there are ambiguities (lack of certainty/structure/definition) between the input parameters & the output function value
        - ambiguities such as where multiple inputs produce the same output, like how different x-values can produce the same y value on a wave function

    - apply possible components to create an absolute or scalable definition include components framed in terms of interactions with other components that can be used with a consistent measurement (like a stable structure) & can also scale (boundaries), rather than framing them in terms that can have different meanings at different parameters (closed, hollow)

    - requirements of a proof (connect a function on the math interface with the logic interface)
      - describes all possible variation in the output in terms of components capable of producing that variation
      - complete set of test cases or representative example test
      - connects components using only logic rules & definitions
        - example: if we have connections 'a is a subset of b', and 'b is a subset of c', then we can derive new connections by applying the definitions of 'subset', 'equal', and 'container' to the components
          - 'a is a subset of c'
          - 'c is a container of b & a'
          - 'b is a container of a'
          - 'b is not equal to a is not equal to c'
        - definitions:
          - container: if a is a subset of b, b contains a
          - subset: if a contains only elements that also appear in b, but b has more elements that dont appear in a, a is a subset of b
          - equal: if a and b have the same elements and no different elements, a is equal to b
        - we've applied the logic of the 'subset-container' connection in the definitions to the subsets a & b, to identify their connected container components (b & c, and c)
        - proofs also have this restriction, in that they can apply definitions/rules to identify connections between components, if the rules specify certain (having certainty attributes like absolute, constant, required, or definitive) connections like equivalence
        - proofs are a combination of rules with certain connections
        - 'proof templates' (structures) can be constructed using connectible/interactive rule sets (applying structures like combinations to core components of a proof like rule sets)
        - you can match input-outputs (like question-proof) using proof templates (connection sequences between question & proof), if there's a proof template that matches the input question & output proof, but you may not know the proof in advance (like if doing combinatorial analysis on functions to see if anything can be proven about the functions)
          - example question: prove that 'a is a subset of c', given rules 'a is a subset of b' and 'b is a subset of c'
          - example proof: 
            - prove: 
              - 'a is a subset of c' = 'a has only elements that appear in c, but c has more'
            - apply definition of 'subset'
              - 'a is a subset of b' = 'a has only elements that appear in b, but b has more'
              - 'b is a subset of c' = 'b has only elements that appear in c, but c has more'
              - 'a is a subset of c' = 'a has only elements that appear in c, but c has more' (arrived at proof using 'subset' definition)
          - proof template:
            - specific template:
              - to prove that an attribute is transitive for a standard case (three input components),
                given example connection rules regarding the component with the attribute (subset) & the component giving it that attribute (set), 
                and given a definition that makes testing for definition fit as simple as a set of basic calculations ('does a only have elements that appear in b', and 'does b have more elements')
                where the connection rules connect two different component pairs in the set of three input components,
                and the proof requirement is to prove that the remaining pair is also connected in the same way,
                apply the definition of the connection rule to the remaining pair to test for definition fit
            - general template:
              - to prove that an attribute applies to all subsets in a set, 
                find a standard case that is extensible to other formats/inputs,
                and apply the definition of the attribute connection rule to all subsets in the set for the standard case
            - if you have inputs like a set of components, and rules connecting a subset of components, and a proof requirement of determining if the same rule can connect the remaining component subsets, these proof templates are applicable

    - 'false association' error type
      - just bc one item has both associative & distributive attributes doesn't mean another item with an associative attribute also is distributive
      - it would be incorrect to associate these attributes in the absence of meaning (relevance & context) connecting them with structure/certainty
        - multiplication can be done in any order bc its the same operation (find the area of the same shape)
        - division cant be done in any order bc the operation is 'apply a standard' where the standard is the base, so when the base changes, the operation changes

    - all primary interfaces can act like the problem-solving interface (start solving problem from the concept or structure interface and integrate all info back into that interface & frame the solution in terms of that interface) but the meaning interface (the interface interface) is the most powerful

  - visualizing higher dimensions with changes in a network of visualizable variable subsets like:

    - dimension subsets: displaying dimension subsets in groups of sizes that are already visualizable (from 1 - 4 dimensions), where orthogonality is preserved across the network of subsets
    - dimension groups: grouping similar dimensional changes into a change type across a dimension subset, to visualize the change types
    
    - relevant (robust) dimensions
      - dimension invalidations: grouping invalidating/neutralizing change types
      - causative dimensions: just visualizing higher-impact/causal dimensions
      - vertex dimensions: graphing variables as differences from vertex variables
    
    - mapping dimensions: group value sets as other structures like points or networks in a space where change types like 'continuous change' are supported
      - embedded dimensions: dimensions graphed visually using extra dimensions as parameters
      - base dimensions: dimensions standardized to a base and graphed in alignment, like multiple functions on a graph with a common base
      - mapping other dimension metadata: interactions between dimensions units/change types/limits/definitions are graphed
      - abstract dimensions: abstract value structures are graphed ('a point on a line') instead of specific values in a dimension
      - constant dimensions: adjacent limiting constant dimensions are graphed instead of dimensions of change

    - dimension interactions: 
      - interactive dimensions: dimensions that interact are condensed into an input/output of the interaction structure, and input/output dimensions are graphed instead
      - dimension interaction structures: structures of interactions between variables (like direction/circuits/networks) are preserved, where values may be lost
        - dimensional difference: difference between dimensions is graphed instead of different dimensions & values, where dimension values are structures associated with graphed dimension interactions
        - conversion requirements: conversion requirements to a visualizable shape are graphed instead of actual dimensions/values
        - interaction structures between value structures like positions on dimensions when graphed as a standardized shape
          - example: with dimensions formatted as a standardized form like 'lines of equal length, & values as points on these lines', the interaction structure would be the lines connecting the points on the dimension lines, when arranged in any order
