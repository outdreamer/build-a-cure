- error type identification

    - determine definition of errors (& metrics of costs as inputs to determine an error) for other system contexts/positions to identify the cost meaning (actual integrated cost, rather than cost given one position of an agent in the system) to identify the right error/solution metrics to solve a problem & avoid causing other problems (including avoiding causing problems for other users of the system)

    - identify error structures & components (components like attributes, such as 'error position') by:
      - missing or lower than expected required components (like cost)
        - rarely this might not be an unidentified error if it's just an efficiency
      - error-related or required components in a non-optimized system
        - any structure that is sub-optimal for an intent is a possible error

    - use opposite structures to generate error structures
      - identify what type of structure doesnt make sense or doesnt fit within a system and find a way connect it to system components/state anyway to see how that error could develop

    - identify error structures on interfaces & apply across interfaces
      - an error structure on the change interface is 'changes that develop with no relevant or useful usage intent' which can be used to identify errors across interfaces or formatted as error structures on other interfaces
        - a 'change type' translated to a 'variable' on other interfaces
        - a 'change' translated to a 'difference' on other interfaces
        - 'useful' translated to structures like 'relatively good at producing outputs that are inputs to other components' on the function interface

    - apply error interaction rules to identify error structures (missing/imminent/conditional) of error types (missing error types, imminent error types, conditional error types) on interfaces

    - identifying the error structures in the interface query or insight path used to generate/derive/find a function/insight can identify error structures in the insight, so its useful to store the context to which the interface query/insight path was applied to generate/derive/find the insight/function
        - similarly, error structures that are inputs to a problem space (missing info, lack of understanding, etc) can identify/predict error structures in solutions in that space

      - determining the maximum error/difference structures that a difference type can produce can be scaled to calculate other limits on errors, like the limit of differences in infinite sequences
        - a pair of infinite sequences that differ only by one value has an upper limit on how different they (& the outputs of operations on them) can be
        - this difference limit can be scaled & extended to other parameters of operations

    - apply interface components to identify error structures

      - add to error type structures as a quick solution filter (solution automation workflow)

        - error interaction structures
          - structures (like set/opposite/combination/sequence) of error structures
          - cross-interface structures
            - error structures that are interactive across interfaces
            - structures that form error structures when allowed to interact

        - analyzing errors on a different interaction layer applies the insight 'interaction layer is often a relevant variable (like other important system structures) determining outputs'
        - other variables (including system structures) applied to errors
          - core:
            - components of error structures
          - ambiguity:
            - errors that are equally likely to be solutions (useful functions/attribute structures)
          - default:
            - errors that are defaults are likely to have useful components or be an output of useful components

    - identify error types quickly

      - apply insight 'related components of a component can cause changes (including sub-optimal changes like errors or improving changes like solutions) in that component bc of interactivity' to problem/solution definitions

        - some error types are caused by errors in other components

          - example: 
            - if a dependency has a vulnerability, all its dependent components might inherit it through usage or inclusion
          - solution:
            - the solution could involve:
              - fixing the other component
              - communicating info to help the component fix itself
            - a general solution could involve:
              - delegating error-type identification to components in positions that can identify error types
          - relevant interface component: 
            - perspectives
              - in a system with optimal component communcation, solutions built/derived/found/identified by other components can prevent error types bc different perspectives of other components can identify errors before the origin component identifies its own errors (applying the concept of 'self-awareness' can also generate this insight)
            - function
              - similar components may be related by position, so they might have similar functionality which can be shared to fix adjacent components

      - interface query to solve the problem of 'finding new error types' or 'finding new ways to find new error types':
        
        - applying core functions to interface components of the problem system (or to related components) can identify error types quickly (and associated solutions)
        - this query can solve the problem of identifying:
          - sub-problems of finding new error types' like:
            - lack of info (about variable interactions/changes)
          - by identifying causes of the sub-problems like:
            - limits of info in specific positions/angles

        - apply interface components to origin system (standard cross-interface query), then apply core functions to those components to run another interface query to:
          - solve the problem of 'identify new error types' for different versions of them (change)
          - solve the problem using a different origin position (build starting from different origin position of components to build with)
          - solve the problem of connecting problem & solution (derive)
          - solve the problem of filtering possible error types (find)

          - example: 'perspective' mapped to a 'driving' problem system can have structures like:
            - 'position' bc its a physical system where differences in 'position' change perspective definition components (like 'visibility' or 'priority')
            - 'role' bc it involves agents where differences in 'role' may change 'focus' or 'priority'
        
        - build: 
          - the intent of 'building an error-free system' (like a sub-system of the origin system) can quickly identify new error types, if using non-standard building functions or other components
        
        - derive: 
          - deriving error types from problem spaces or problematic systems (rather than deriving from core system components or from core error components) can identify error types of a system
        
        - find: 
          - finding difference structures with intents that contradict approved intents can identify error types quickly
          - find interface component variables of the problem system (or versions of it or its compoments produced by 'change' function or other functions, like a 'related system') to identify new error types
            - system:
              - find a system where it's not an error and the cause of the error to fix (or cause of other error types) in the origin system will be a structure of difference between the two systems
                - example: 'bowing' is a social error in some systems - the difference between those systems and a system where it's not an error is a difference in the 'culture' sub-system
                  - error types involved include: 'culture', 'sub-system', 'interaction', 'social interaction'
        
        - change:
          - changing interface component variables of the problem system can identify error types quickly
            - structure: 
               - changing common structures of difference between similar components like instances of a type)
            - perspectives:
              - switching perspectives can identify problematic differences between solution metrics & actual values bc of the info obvious or focused on by another component
                - just like another car may be able to see if a car has a flat tire before the user driving from a perspective of the car with the problem
              - switching perspectives is a way to find error types faster, if the error is adjacent to or equals info that is focused on or obvious to another perspective
              - switching primary interfaces is one way of switching perspectives, but you can also switch variables in the origin system:
                - switch positions of agents or components with info-gathering functions (sensory functions) with a particular focus on info that may be in error
              - calculating the possible errors (like by determining variables that are difficult for the origin component to check, so errors can involve structures of those variables) is a useful process to run before identifying a perspective that can see errors in that set of possible errors
              - how to calculate a perspective that can see a possible error, once a possible error structure is identified
                - find a perspective that interacts with info related to the variables/structures/components of the possible error, by:
                  - intent perspective: focusing on or prioritizing the info
                  - input/specific perspective: using/implementing the info
                  - causal perspective: generating the info
                - construct a perspective that has one of the above attributes/functions as a solution structure, and find existing perspectives that match that perspective
            - intent: 
              - switching intent can find intents that are possible error type causes, for the original component, other components, or the system enabling them to exist
              - example: 'cause error' intent rather than 'solve problem' intent can solve the problem quicker in some cases 
            - change: 
              - change change components like variable types (what's required or default for some components may be intended or optional or not possible for others)
            - cause:
              - switching causes can make a component more robust
              - finding a way to build/execute a function given alternate causes can make the function more robust to changing inputs
              - this can be an error type in some cases
            - abstract:
              - mixing abstractions with specific examples/implementations the problem system can result in a quicker identification of error types
            - definition:
              - switching definitions of error/solution can identify possible error types of other systems that wouldnt define another system's survival as 'success', where their error would be 'failing to destroy another system'
                - this can identify possible error types like 'attack from a system with this definition of error'
      
      - solution success cause: 
        - this works bc some solutions can be used to solve similar problems, so solving for a similar problem instead can be more efficient & find a solution to the original problem
        - applying solution insights like 'solutions may solve similar problems as the origin problem' can find new solution automation workflows
      
      - identifying error types is important for functions like:
        - 'find solutions by avoiding known error types'
        - 'prevent errors'
        - 'identify error interactions, so the interaction of changes to one error & changes in related errors can be predicted'

- error type interaction rules
      - error type interaction rule: 'error types can lead to other error types, but also to learning (new error types or error interaction rules or solution structures)'
        - example: the human brain (and AI based on it) tends to over-prioritize patterns (some patterns get stronger when repeated, like in making/breaking a habit)
          - this means that when one error type is repeated, like over-prioritization of something, that abstract error type is likelier to be repeated (over-prioritization of things in general) 
            - this is bc the brain doesnt always store specific details, but the general pattern of how info interacts, so it often applies that pattern rule where its not needed or where its sub-optimal, bc info with similar patterns can be very different in context/meaning & required usage or supported usage intents
          - by over-prioritizing patterns as a source of useful info, error types are likelier to be repeated (with other variants of the abstract error type)
          - over-prioritization of other things can lead to other error types, which can lead to learning if used correctly (if the brain is aware of this process & is looking for new error types or repeated error types or variants of a repeated error type)
          - why does over-prioritization of patterns lead to a pattern of over-prioritization?
            - the logic interface connects these (using definitions, equivalent components across logical rules, & interaction functions)
              - patterns lead to repetition, which leads to over-prioritization
              - a pattern-prioritizing brain will therefore prioritize over-prioritization, by repeating it
              - repeating something is a pattern
              - a pattern-prioritizing brain will make a pattern out of over-prioritization
          - however, some structures can prevent this error, like prioritizing identifying error types, allowing them to occur where useful & using them as an input to learning
            - in this case, a pattern of over-prioritization errors would lead to fewer errors rather than more over-prioritization errors
        - example: 
          - stupidly copying the human brain as a source of structures to implement in algorithms to copy methods of generating solutions also copies the errors of the human brain like bias
          - copying physics structures like efficiencies such as 'rules to filter out info processing error types' can help prevent this error of stupidly copying the brain structure as it is to minimize work of improving the output with structural modifications
            - rules like:
              - 'identifying lack of information or other resources required to solve a problem'
              - 'identifying its own assumptions that may not be constant'
              - 'identifying its own over-prioritization of patterns over meaning'
              - 'identifying its own incentives towards cheaper solutions'
          - for a 'find a prediction function' problem, this could take the form of:
            - identify insufficient info: identifying when a function cant be specified further bc the info available is insufficient to resolve ambiguities in its structure
            - change constant/assumption: identifying when a variable is likely to have corrupt data bc it doesnt follow patterns of any other variable seen across data sets
            - identify over-prioritization of patterns: identifying the patterns in a data set accurately but not identifying the meaning of those patterns (the reason/cause of the pattern is 'researcher injecting bias')
            - identifying incentives prioritizing cheap solutions: identifying & implementing a low-cost solution like regression or Ai without recognizing efficiencies gained from other more expensive solutions that will produce cheaper solutions in future resolutions of this problem type
        - example: 
          - an error in selecting a prediction function will output errors in components depending on it (users of predictions made using the function)
          - this can be offset by weighting the predicting function or using it only according to its known error types (when those error type cases dont apply) or distorting it to handle known error types

        - using error type interaction rules to predict other error types
          - by knowing that 'over-prioritization' is an error type, we can see that 'over-prioritizing constants/assumptions' can generate errors, before a constant or assumption generates an error
          - by knowing that 'cheaper solutions are incentivized (meaning low-cost, even though theyre not necessarily optimal)', we can see that 'assumptions/constants' are a cheap resource and are therefore a possible source of error, before they generate en error (before we see them generate an error)
            - for 'find a prediction function' problem, the assumptions that are cheap error-causing resources may be assumptions of components of the solution or the method used to solve the problem
              - assumptions that data set variables should be primary components of the solution, which is found with a method like regression
              - as opposed to using higher-value components like understanding of:
                - how patterns interact & develop & the cause/reason for them
                  - sufficiently similar patterns may merge into a type or compete for dominance or cooperate for efficiency
                  - patterns develop when there is structural stability to support their development (causal structures are constant enough to rely on so they repeat and form a pattern)
                - how variables interact & develop & the cause/reason for them
                  - how change occurs within a variable to identify corrupt data
                  - how variable types interact
                  - how variable interact (by positional adjacence, input/output connections, similarities)

        - using error type interaction rules to predict solution structures
          - by knowing that 'cheaper solutions are incentivized', we can infer by applying the logic interface that:
            - 'cheap solutions often help fulfill intents optimally' 
              - 'a solution can be composed at least in part from efficiencies like low-cost solutions'
              - 'there may be efficiencies available in the system bc of this rule that will help with any given problem-solving intent'
                - 'bc if its a problem, there is a reason for the intent'
                  - 'so given that an agent will benefit from solving the problem'
                    - 'there is a way to solve it'
                    - 'if there is one way, there are likely other ways'
                    - 'of these other ways, some are more efficient than others'
                    - 'if its a problem, its probably complex'
                      - 'in a complex system, there will be at least some efficiencies'
                        - 'some of these efficiencies may be usable to fulfill the agent intent'

- example error types

    - example of 'indistinguishable cause' error type
      - multiple causes of the same variable
        - example: gravity (no agency, just granular intent of 'apply force to relevant objects') can cause an object to fall, just like throwing it down (concept of agency and intent to 'move object down' or 'move object to ground') can cause it to fall

    - example of 'false similarity' error type
      - set of interactions that are unrelated but appear correlated
        - examples
          - system B has an output that is structural similar to outputs of system A
            - interactions of system B that happen at a later time but their interim outputs may have a structural similarity to the outputs of system A
          - system B may cause a similar or equal output to that of system A, for a different intent/reason
            - if the output is used as a metric, they will seem similar or related
        - efficiencies are a reason (source cause node) that structural similarities occur
          - efficient structures follow patterns that may produce similar structural outputs, but the structures may not be related despite their similarity - theyre related bc of the efficiency that generated them, like a particularly useful method of compressing info

    - example of 'self-defeating' error type: killing their people to protect cultural artifacts that were only meaningful to the people being killed to protect them
      - this is also an example of error types:
        - bias:
          - prioritizing property (buildings, land) over human potential
            - humans can build property and give it meaning or take it away
          - prioritizing similarity 
            - protecting what they can see & what is nearby (similarity in position)
            - working with those who claim similar goals like weapons dealers who claim to be helping them
            - protecting anything similar to or related to themselves rather than empathizing or thinking
            - blaming dissimilar groups (the US & Russia) for their own violence
            - blaming the enemy (Israel & Palestine) for their own violence
            - blaming the past & the dead for their own violence (they did it to avenge someone dead, or bc they had to fulfill their family legacy of the feud, or for 'their ancestors' or 'their people/tribe')
            - blaming the future or people who will be alive in the future (they did it to protect future generations, or their own children, when they primarily put them in danger by maintaining the feud)
          - prioritizing simplicity
            - not thinking about how war benefits war industry market-makers, to figure out that they were tricked into waging war to profit weapons dealers so they could get profits in the form of financial resources, land, power, laws, etc
            - not thinking about impact of decisions, which is to hurt their cause, their claimed priorities, their culture, & themselves
            - not switching strategies when one clearly doesnt work
            - focusing on short-term goals that dont help their cause, like: 
              - 'bombing the correct position'
              - 'dodging bomb'
              - 'being a glorified heroic victim' 
              rather than long-term goals (or short-term goals that help fulfill long-term goals) like:
              - 'building a safe peaceful place for culture to survive/thrive'
              - 'saving money to move to a place where they can have time to think of solutions' 
              - 'manage emotions to make rational decisions by thinking'
              - 'organizing to help both groups understand how to think & come up with solutions'
              - 'come up with good arguments or inventions to convince other side not to fight them'
            - focusing on protecting property bc its simpler to protect an object that can be seen & touched
            - remaining in the one place where they'll almost certainly die a preventable death without fulfilling their goals bc its easier to be a victim & they think theyll be glorified if theyre a victim
          - false perspective of seeing themselves as a set of attributes rather than a dynamic individual capable of something other than defaults
            - seeing themselves as static attributes like an ethnicity, or having static roles like a citizen of a country, a victim
              - seeing themselves as a set of static attributes turns them into a static type (just another fighter destroying themselves in a doomed attempt to beat an enemy who is also just another fighter, rather than an individual)
            - seeing themselves as unable to avoid defaults like 'minimizing cost (by prioritizing similarity/simplicity)' or 'applying biased decision rules'
          - false perspective of prioritizing addressing local recent intents like 'protecting their own people temporarily' or 'dying a local temporary hero' instead of addressing root cause by 'fixing human nature globally'
          - false perspective of meaninglessness, where only meaning is in another dimension
            - meaning is wherever you create it (by creating understanding/science & other useful resources & sharing them so other people can evolve)
          - false perspective of peer pressure (social confirmation) being truth
            - large groups of people are regularly wrong bc theyre just saying whatever feels good rather than what is true & they confuse good feelings with truth so they never evolve past that to develop a brain that can make the truth good (using science, meaning, understanding, potential or another interface)
            - this social confirmation of lies leads to cascades of stupidity
          - false perspective of fighting individuals who are members of an enemy group, rather than fighting mental errors
            - fight mental errors like bias, incorrect perspective/priority, lack of thinking/understanding/information with tools like education in the form of understanding of mental errors & their causes, & information about solutions
          - false perspective of impossibility of fixing a crime (criminals can do useful work to help victims, even if it doesn't completely fix the crime)
          - false perspective of taking assumptions/defaults/current state or current cycle as constant & unchangeable
          - false perspective of ignoring the output/impact/endgame
            - what is their plan if the violence continues and shooting one additional person doesnt fix the violence

        - not analyzing causes that tend to lead to violence
          - violence
          - being physically adjacent to the enemy
          - having weapons
          - provoking the enemy
          - not showing difference from other victims
          - not showing similarities or the potential to be similar to enemies
          - not helping enemy or trying to help in some intentional way
          - not showing potential to think
          - not showing empathy
          - showing too much weakness
          - condemning the enemy (saying everything about their group is negative when it's not, without acknowledging any counterexamples)
        - not focusing on solutions or thinking

    - example of error type 'over-identifying error types, even when the error type is correctly interpreted as an error'
      - simple cost-avoidance isnt an optimal method in some cases, so a cost on its own cant be used as an optimal learning tool in all cases
        - when cost in one benefit-cost definition set is necessary to get required benefits of a different benefit-cost definition
          - example: 
            - it may be a cost to lose a battle (reduce one particular cost), but it may be a necessary cost to win a war (reduce all costs of a type)
            - if I learned every lesson taught by any cost, I would learn not to help anyone other than myself
              - a definition of cost as 'cost to one self or one component or given one cost metric' is too isolated to be useful on its own as an input to evaluate the integrated meaningful cost
              - the definition of cost has to be more complex than one metric, otherwise the lessons learned from these over-simplified costs (and other error structures applied to cost) would be false & lead to other errors
              - losing the battle of 'getting every possible benefit to myself' is a necessary cost to win the war of 'reducing all costs of contextually necessary interactions like trust'
                - if I avoid all possible costs like 'paying a fine when I display trust to criminals', I may learn the very wrong & high-cost lesson of 'avoiding all trusted transactions', bc I will technically be avoiding costs which will seem like success, at the cost of occasionally getting a benefit from trusting someone, which I will never experience if I avoid all costs
              - losing the battle of 'avoiding every possible cost to oneself' is a necessary cost to win the war of 'reducing all costs of selfish cost-avoidance (like inequality)'
                - if every company prioritizes avoiding all costs, it keeps a subset of the population unequal (which is self-defeating, and keeps the cost of equalizing high, requiring granular costs for every specific problems encountered by that population), whereas if they identify that not all costs should be avoided, they can correct the inequality at comparatively lower cost (enabling the population to take care of their own specific problems)
        - 'avoiding all costs' is by definition flawed bc the costs that can be seen by a simple mind that would select such a rule are necessarily incomplete
        - what type of mind would produce a decision to use that rule? one that prioritizes:
          - simplicity
            - simple rules like 'avoid possible costs (risk)' and 'avoid known costs (prior costs or known cost definitions)' or 'avoid costs to yourself or your dependencies'
           - selfishness
           - avoiding being exploited (used at relative/absolute cost to oneself) by a powerful component
        - in some cases, the rule would be optimal to use in those specific cases
          - if all costs are aligned with one cost (if one cost triggers other costs), there's a reason to avoid all costs
          - if a cost being avoided is unnecessary and doesnt serve any other purpose (doesnt benefit anyone), there's a reason to avoid that cost, so a rule to 'avoid all costs' wouldnt produce an error for that cost
            - sometimes the cost not being avoided is unnecessary to not avoid
              - the benefits of not avoiding cost (like 'drawing attention to the negative attributes of exploitation') are unnecessary
              - in this case, the rule is sub-optimal ('avoiding all costs' is now the optimal rule, bc 'not avoiding all costs' is unnecessary & producing no benefit)
        - sometimes optimal learning occurs not being accruing info from costs, but by creating costs for other learning nodes
          - this amounts to teaching other nodes to optimize them so they dont create costs that are unnecessary or sub-optimal in some way
      - how to identify when to use a rule like 'avoid all costs'
        - given the inclusion of 'all', this rule should rarely be applied bc there are few rules that apply in 'all' situations or to 'all' instances of a general component like 'error'
        - identify the above cases where this rule is optimal:
          - when all costs should actually be avoided
            - when any cost can trigger all costs (like if 'any cost you knowingly take on is interpreted as permission for anyone to allocate all costs to you')
          - when simplicity/selfishness/cost-avoidance is useful in the position of a rule
          - when a cost is unnecessary or doesnt benefit anyone so theres no possible reason to take on the cost

    - example of applying solution automation workflows to generate an error
      - apply various solution automation workflows to 'generate pathogen dna error'
        - 'trial & error' implementation example: 
          - use dna code switcher to apply change to all possible positions & position combinations in pathogen dna
        - 'common pattern' implementation example:
          - inject known dna error types to see if any work on new pathogen
        - 'standardize & equalize to apply existing components' implementation example:
          - standardize pathogen dna to host dna language & apply host dna error types

    - examples of causative error types of sub-optimal solutions

      - over-simplification error sub-type, relating to interface components/queries

        - one/basic/quick interface-based solution, when multiple are optimal, given the structures (false similarities on that interface) and attributes (complexity) & other components involved
        
        - example of this error type:
          
          - applying the simple version of the pattern interface in isolation, in a complex system that has false similarities on the pattern interface
          - a solution that only identifies patterns (& not other relevant interface components, like errors in pattern-structure mapping) will not identify false similarities where the position of similarities is in the pattern, but not the structures of the pattern implementation, where they should be in order for the pattern to be applied

          - solutions to this error:
            - you can still apply the pattern interface in isolation, if you map other interface components to the pattern interface
            - examples of solutions:
              - map structural, conceptual & system components to the pattern interface, such as 'false similarities', so that conversion to the pattern interface includes identifying 'patterns of false similarities', which would prevent this error type
              - map intent to the problem system before applying the pattern interface
                - query for patterns with associated valid/invalid intents, connecting the inputs/outputs by intent as well as patterns, to determine how different the intents are that a pattern can be associated with
              - query for patterns in errors applying pattern interface components

            - a 'simple' or 'quick' application of the pattern interface would apply an AI algorithm to the problem & consider it solved

          - causal paths/source nodes of this error:
            
            - reason this would be identified as an acceptable solution:
              - reason: 'there are patterns in the data between input/output, so a solution that identifies patterns & converts them into an efficient function is acceptable'
            
            - reason its not an acceptable solution: 
              - reason: 'there are other relevant structures in the system than just patterns between input/output from the data set'
              - other relevant structures include:
                - the above error-related components: 
                  - false similarities, complexity, structures associated with the pattern implementation
                - other causative or interfering variables not in the data set
                  - alternate causes of the data set variables
                  - contradictory emerging or conditional functions that will soon interfere with the data set interactions

    - example structures of 'missing info' error type (an uncertainty to apply structure to, or a question to answer)
      
      - question structures
        - question-predicting rules
          - filter by relevant intents to primary intent to identify probable questions
            - proxies for determining where info is unlikely to be distributed as needed
              - high ambiguity/complexity/specificity/variation or inaccuracy rate
              - info distribution gaps (where info is not distributed due to distribution barriers, lack of intent, or alternative distribution paths)

        - structures for 'required questions to reach info state':
          - structure like a network/sequence/combination of sub-problems, or missing connections between existing/known problem space network nodes, preventing movement from origin to destination on info network
            - movement for an intent requires knowing connections, bc the connections allow navigation/planning of movement on the network oriented toward a goal like the destination info state
      
      - question error type solution structures
        - deriving missing connections between relevant components ('how does x relate to y')
          - 'to get output variable y info, you need filter variable x info & variable z info'
          - finding correct position of a component in a structure ('where does x fit in y')
            - 'x is an input to y'
          - deriving usefulness of structures for intents ('is x interactive with y as a possible input')
            - 'x can be an input to y with changes 1-3'
          - deriving reason for a function/change ('why does x change y')
            - 'x changes y bc the structure of y is not stable when x is an input'
          - filtering optimal structures for an intent ('what is the best path from x to y for an intent z')
            - 'x as an input to function 1 is the best path to y for intent z'
