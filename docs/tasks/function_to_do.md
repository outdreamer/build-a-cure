# to do

  - summary of advice

    - invest based on:

      - prioritizing real value signals: 
          - products that fulfill a need
          - companies that self-invest (prioritizing growth by re-investing in growth)
          - leaders that guide by example
          - independence: invest in independent value sources & independence-generating assets, become an independent self-supplier of your required inputs
          - understanding: invest in assets you understand
          - established/proven (long-term) value
          - efficiency: take responsibility for your required inputs

      - avoiding false value signals: 
        - incentives: high-reward/low-cost reasons to invest that may not align with real value or be sub-optimal dynamics that destroy value in a different position of the system 
          - speculation
          - investment banker advice
          - ceo status
          - short-term value changes

  - future value signals

    - problem metrics: current problem-solution ratio, problem to problems-solved ratio
      - solution metrics: 
        - efficiency: sustainability, reusability, cost-benefit ratio
        - independence: self-generatability, self-awareness/regulation/correction
    - efficiency-generating metrics: 
      - integration/organization/optimization/regulation/generalization
    - info distribution (across teams, departments, ranks, customers, other businesses)
    - info proxy derivation/acquisition/building
      - education
      - group membership (do they outsource some processing to groups, and which ones, and how do they evaluate groups like partners/clients/competition)
      - understanding
        - group dynamics (do they help toxic groups improve, do they have a self-destruct mechanism if they become a toxic group, etc)
        - markets (do they organize & optimize markets to achieve their goals quicker)
      - flexibility or learning potential (intelligence derivation/generation/acquisition/building)

  - give example of mathematized insight path 
    - standardize variables to math interface structures & values
      - apply type interface
        - identify types
          - standardize variables with types to differentiated clusters
          - apply difference definitions (like variable subsets) until type separations are clear
          - apply difference types until type separations are clear
      - apply structural interface
        - identify relative difference (difference from reference point, like origin node)
          - apply adjacent structures (vector or spectrum or loop) to variables having the concept of 'opposite'
      - apply causal interface
        - identify causal structures like direction
          - apply structures with direction to variables having causation in their connections
      - apply function interface
        - identify variables with input/output relationships to form path between structures on meaning interface
      - apply concept interface
        - remove randomness
          - compress variables with randomness injections to lower dimensional representations
      - apply meaning interface (using a structural relevance definition)
        - integrate variables in one structure to relate them
          - identify any vertex variables as the preferred variables to standardize other variables to
          - connect variables once formatted using adjacent/interim dimensions like topologies with variable subsets that can act as interfaces between connected formatted variables 
            (can capture info from input & output variables in the connection)

  - functionalize insight paths & integrate functions in optimized program with parameters to select function subset & structure for input problem
    - give parallel/perpendicular insight path examples, for insight paths that add info that the other is less/more likely to retrieve

  - diagram with error types
    - examples: 
      - over-structurization (specification) of an uncertainty/variable (assumption as fact, variable as constant)
      - over-correction of an error
      - over-prioritization
      - over-reduction (over-simplification)
      - over-variability (over-complication)
      - misidentification of minimum info to solve

  - new insight-fitting algorithm for error type avoidance
    - when a new discovery is made, apply insight paths formatted as questions to spot error types before they occur
      - could this violate any assumptions/requirements/dependencies we rely on for other tasks like calculations or applying systems of understanding
      - could this cause cascading (self-sustaining) errors
      - could this cause emergent errors, given other knowledge like probably interactive trends or rules
      - what are triggers of this? what can it trigger with certain interactions, how likely are those interactions

  - diagram with joke types
    - 'annoying when they bring up human rights in a conversation'
      - conversation system context
        - functions
          - change topic 
            - change topic structure (sequence)
              - introduce a topic (first time topic is included in conversation)
          - expected interaction functions
            - criticism of a behavior
              - 'conversation with dictator' system context
                - criticism of power abuse (law violation, specifically human rights violation, which are a related object to dictators)
                  - interpreted as right in the 'conversation with dictator' system context
                    - expected interaction in this context
                      - 'should bring up human rights to a criminal'
            - norms:
              - for low-stakes interactions & interaction errors (manners, annoyance, disrespect)
            - laws: 
              - for high-stake interactions & interaction errors (rights violations)
        - placing a norm (or related objects) in the place where a law (or related objects) would normally go:
          - 'its annoying when someone doesnt let you end the conversation'
            - 'its annoying when someone keeps going on & on about your previous conversations where you ordered deaths of a dissident'
              - 'its annoying when someone keeps going on & on about your previous conversations where you ordered deaths of a dissident for being annoying & then abruptly stops without explanation'
            - 'its rude when someone doesnt let you end a conversation with a laywer interrogating you for war crimes' 

  - organize examples in indexes

  - make efficiency map

  - bias structures: 
    - bias cycle: where specifically/partially false statements are falsely categorized as completely false, which triggers increase in distorted view of the group making the miscategorization error
      - saying a specifically/partially false negative thing about a group often has a partially true sentiment backing it (most people in any group do negative behaviors enough to trigger negative sentiments), so even if the specific negative thing is wrong, the sentiment might not be
      - the lack of acknowledgement of their own negative behaviors by the group saying the specifically/partially false statement also triggers the same response in the group making the miscategorization error (the group saying the specifically/partially false statement is doing a negative behavior, so the miscategorizing group has a negative sentiment about them, and often says specifically/partially false negative things about the group)
    - conflating stereotype ('false statement about a group') with 'a statement about a group that is more true of a higher ratio of that group than it is of other groups'
    - stupidity manifests as similar structures (fulfillment of low expectations) across groups in response to low expectations, leading to feedback loop

  - abstract risk insurance: guarantee a relative position (without specific currencies or amounts), such as 'resources giving a top 10% economic position'

  - finish applying structure to info components (memory components like personality, forgetting, compression, uniqueness, generation, learning, bias, and organization components like relevance, differences, type, abstraction, standards, randomness) to generate full set of neural net types
    - filter by usage intent

  - example of applying structure to components like technologies to find emergent trends

    - tech, standardized to common terms
      - movie: sensory info emotion triggers & info/abstract paths (stories)
      - video game: decision visualization
      - music: audio emotion triggers & info/pattern paths
      - ai: prediction/generation
      - ar: integrate visualizations with real sensory info
      - screen: visualization interface
      - video conferencing: visualization sharing
      - text voting: decision aggregation
      - drug: direct sensory info semotion trigger
      - brain-scanning tech: visualize memories & thought processes

    - multi-player video game voting: applying voting tech of viewers to influence video game tactics/resources/problems/outcomes/decisions
      - generative query: switch input of decisions to another decision-producing tool (audience voting vs. player/algorithm decisions), for randomness/customization/reality integration intents
    - user character customization: applying AI to generate characters of real people or characters from other games to play as other players in video game
      - generative query: switch input of character personality/story with another source of that info, for customization/reality integration intents
    - memory-generated vidoe game: apply ai & brain-scanning to generate a game based on memories
      - generative query: change experience level or skills required (use memory as a tool or test memory functionality), for testing/customization/reality integration intents
    - emotional/sensory alignment games: query for desired emotional path & map a game/video/audio/drug to produce or match that path
      - generative query: change content-creation direction & other variables, from story => emotions to emotions => structure applied to emotion-triggering tools
    - brain-development games: apply AI & brain-scanning to identify missing functionality in brains & generate game to develop that function
      - generative query: use output of game (learning) as input assumption for learning intents using games as intent-fulfillment resource

  - diagram for structures of emergence
    - example: 1-1 input/output relationship up an interaction layer, where extra resources that dont dissolve immediately on the higher interaction layer aggregate & form core structures like combinations, where interactions between combinations & sequences have different dynamics than the individual output interacting with other individual outputs
    - emergent functionality/attributes come from interaction structures (sequences & layers)

  - calculate deaths caused by products/companies by proportional contribution to deaths from slavery & pollution/plastic/additive/medicine & other chemicals

  - generate other interfaces with interface components (connection, requirement, structure, abstraction, set, independence)
    - intent: future direction with benefit to agency
    - cause: preceding inevitability requirement in sequential structure
    - function: structure of task structures (conditions, assignments, iterations) consistently connecting input & output
    - logic: function to connect information using info structures (definitions, inevitability, pattern-matching, exclusive/inclusive conditions, requirements, assumptions)
    - potential: structures like combinations not certainly excluded by requirements
    - change: difference in an attribute value, according to a base (time, relative change, change type)
    - abstraction: general pattern of a specific structure set
    - pattern: a set of connecting functions, often in a sequence structure
    - structure: connections & change of measurable change & difference types
    - information: specific description of a structure
    - math: description-connecting functions
    - system: structure of independence, often having boundary, function & other component structures, at a particular interaction level

  - platform to apply a portfolio of AI models to price a stock given private company data like available resources, internal analysis, & implementation plans & publish the ai-generated prices, with comparative historical pricing of other companies using similar data pre-ipo or valuation, as an offset to price pumping & other forms of misrepresentation

  - identify accidental & intentional govts/laws/markets, based on function metadata concepts
    - responsibility (restricting functionality to the functions that can & should handle, based on whether they caused the problem resolved by that functionality)
    - relevance (restricting info only to functions that need it)
    - structure (functionality gaps)
    - potential (analyzing future functionality and paths to those states)
    - integration (analyzing impact of intent/responsibility/optimizations of a function)
    - optimization (storing functionality needed, generating functionality where possible & where usage allows)
    - organization (indexing functionality in a way with specific side effects like limiting possibilities, and organization through queries & changes of functionality)

  - solution investing app
    - Is the cost (implementation/opportunity/mgmt/bug-fixing costs & lost previous work/equipment value) higher than the benefit?
      It's a technology if its problems solved/created ratio & relative feature value is high enough; if not, it's just a tax/debt
      We don't need to create problems to create jobs, we need guaranteed scientist jobs to handle existing science problems
      Companies/agencies should exist to manage/build solutions to existing science problems (pay this company $x to get y% of pollution removed from outdoor air)
      Taxes are a relatively inefficient & obscure way to allocate funds to these public good solutions, compared to product purchase payment plans
      People should be able to see what their taxes are funding & opt-out to invest in other products/companies
      - example: "avoid a local road that needs local govt budget to repair, to decide where to route funds to more important projects"
      There should be an app to opt-out of taxes if they agree not to use local resources those taxes pay for
      Companies can have efficiency & problem-solving scores (problem-solution, solution metrics, cost/benefit, hidden costs, time to solution, problem solved/created metrics) in a solution investing app to win investments from citizen investors to solve problems relevant to them
      - identify which companies/traders have the right market signals or info by retroactively analyzing historical data about which agents' investments turned out to be correct in the intended investing timeframe

  - apply intents & other interfaces to other decision (transaction) types (code transactions, financial transactions, legal transactions)
    - code transactions (user action like 'clicking a button' or 'running a script' indicates what intents, according to relevant system contexts, like applicable laws/protocols)
    - resource transactions (financial resource trades), to manage intents of a transaction - money is deposited on resource delivery, otherwise in pending state for x days agreed on by agents
    - legal transactions (allow laws to be passed having any of a limited list of approved intents)

  - finding necessary forms for an intent subject to indexed rules like laws, filling them out, and optionally filtering info by regulations cited in forms, including relevant regulations applied
    - involves automated calls/faxes to request/send forms, where processes/forms arent online, like how calls to find appointments are automated
      - involves functions to:
        - derive steps to complete the task in the right sequence/decision tree/flow chart structure (fill out form, fax form, consult legal consultant, review auto-filled input)
        - derive steps in the digitized process (log in, submit form, schedule appointment)
        - find & apply relevant regulations
        - find related forms
        - auto-fill forms
        - derive & execute steps to complete sub-tasks (like 'send a form' or 'schedule an appointment')
        - identify & highlight input that needs manual review
        - find people with expertise to guide manual review (legal consultant, govt employee)
        - digitize process (auto-import to workflow management tool or multi-step form component)

    - alternatively an automated process to digitize a process/form with necessary security, search functionality, and integration with other services/processes as digitization tool variabless
    - the tool should be able to submit user-permitted/submitted input to a preliminary process (like 'apply for a license' or 'submit voter registration') form (like a wsdl, other api spec, url with html form, or just a form pdf template with unfilled fields) and guess the values based on accessible inputs (user address info), then lookup any relevant regulations or related forms & fill out those forms or apply the regulations, and then return suggested output, with highlighting for missing fields or predicted fields with certainty below threshold that user or a legal consultant can manually review, and a list of remaining action items, which can be triggered if the user is ok with the output or updates the output, such as faxing/sending/printing/mailing the form on remote servers or using task-running apps to find a person willing to run the errand, or scheduling a call/appointment (like a dmv appointment).
    - this should also be applicable to software updates (submit a current request to a current wsdl, and find/apply relevant or recent govt regulation updates as well as web protocol updates to generate updated wsdl/request/response as well as request/response wrapper/handling functions, like updating new field names or request structures in codebase)


  - search of local product supplies across exchanges

  - diagram of alternate interfaces (information = combination of structure, potential, change or structure, cause or structure, system)

    - example of applying alternate interfaces with examples of advantages of each

      - the structure (position) of the component can be used to determine/differentiate its meaning
        - 'logy' and 'logi' as prefix/suffix
          - '-logy' as a study of the prefix
          - 'logi-' as a permutation of 'logic'

      - the usage context (sentences where they're used) can be used to determine intent
        - '-logy' used when 
          - discussing science & interactions between fields/topics or changes in a field/topic
        - 'logi-' used when 
          - discussing reasoning/rationality

      - intent can be used to determine meaning 
        - use '-logy' to describe a studying activity & topic
        - use 'logi-' to reference logic, its interactions & permutations

      - structural interface (differences in position) can be replaced with: 
        - intent (reason to use within a system usage context)
        - system interface (usage context to derive reason for usage), and fit to system (meaning)

      - applying different interface queries
        - apply system context to derive intent
        - apply structure (position) as an alternative to system context & intent
        - apply intent to derive usage & system context

  - examine temporary stabilized filter structures in ozone to push co2 out of atmospheric layers away from earth & forces to do that

  - reverse engineering structures like bet types & their ratios (ratio of types like random guessing, price-dependent algorithm bets, temporary bets in non-viable companies for profit beyond actual value, actual investments in innovation/businesses) to identify & filter out trading cycles to isolate unidentified bet types

  - examine calculation errors from one partitioning method vs. other methods, & a function to balance their contribution to error to select an optimal partitioning method for an accuracy level
    - a way around the discrete vs. continuous dichotomy is combinations: 
      - discrete counts of continuous compositions (overlaps, layers, components)

  - find meta-math structure: 
    - which would allow/incentivize/generate the changes in info functions/variables (change types: interaction, aggregation, structure-filling, gaps, convergence, similarities) of known math operations
    - is it a metric like efficient stability that allows info to develop into a measurable structure in the first place (possibly changeable interaction), or is it enforced by a system of a set of limits forcing info to interact those ways (definitively inevitable interaction)
    - 'the information amount/type/variance stored in this definition/structure can only take form in or interact with these other structures/to these degrees/in these spaces/on these interaction levels'
      - information = certainty = definition = structure
      - 'this certainty/structure can only interact with or be formatted in these certainties/structures'
      - can you calculate the set of math relationships more quickly by examining opposing structures of uncertainty/randomness, by applying operations to existing certainties, or by finding a common differentiating standard in between, like abstraction'

  - algorithm to generate variables in a system

    - development of a 'concept' in a system: an object begins aggregating changes (like functions/attributes) in such a way that it develops unique interactions that differ from those calculated by a simplistic summing of the interactions of its components
      - example: a system may develop a concept like a 'layer'
        - structural definition of a layer: a set of components that separates other components & their interactions, inside a containing boundary
          - this definition differentiates it from a boundary, limit, line, or container structure
        - the definition also has dimensions beyond a simple line
        - the layer may aggregate functionality, such as:
          - being stacked or combined to create larger layers or structures on top of a layer
          - forming a base for interactions to develop on, if its a vertically stacked layer
          - acting as a filter, if there are openings in the layer
        - so the layer is not only measurably different from similar structures, it may also have significantly different functionality, earning it a unique term (meaning it has developed into a 'concept' in the local system)

      - the variable of 'structure' can describe the layer & generate it, but it doesnt capture the full definition of the 'layer' concept
      - other variables are necessary to fully describe the layer, such as:
        - adjacent structures (line, container, limit, boundary)
        - core function (stack, combine, bridge, support)
        - adjacent functionality (filter, separating interaction layers)
        - default structure (vertical layer related to stacking function)
      - because it stabilizes into a useful unique component, the layer concept begins to act like a vertex variable and/or an interface, since it starts becoming causative of changes due to its stability (rather than just being the output of changes to similar structures or iterated core functions or aggregated variance)
      - concepts in a system can be local interfaces that are useful to use as standards for comparison
        - standardize to the 'layer' structural interface
        - standardize to the 'local system structural concept' interface

    - so you can generate the sequence of a set of variables for a system by which change type structures are stable enough to act like concepts/interfaces for a given stage subset in the sequence of system development

      - system metadata: invalidating/triggering/development conditions

    - you can also apply core structures to get change types (multiply a number by the structural concept of 'opposite' to get the 'sign/direction' variable)

    - variable definition: isolatable, measurable change type 

    - component generation: identify components of a system & generate possible change types that enable/optimize interactions between those components
      - core generation: identify core change types that can be combined to create other possible change types & generate other possible change types & filter
      - subset generation: identify subsets of a system's components that are sufficiently stable in functionality/attributes to interact with other subsets without invalidating the system
    - limit generation: identify limits of a system & generate possible change types that can develop within those limits & filter
      - reverse generation: generate required functionality in a system & derive possible variables that could produce it & filter
    - filter generation: identify & apply filters that determine variable development functions (like change combination, change metadata pattern, change coordination functions)
      - apply 'variable' definition filters: generate possible isolatable/measurable change types & filter
      - apply 'efficiency' definition filters: generate structures that would be efficient & check for components that could generate those structures
      - other example filters: 
        - are there resources to sustain this change type
        - does this change type contradict a system rule
        - is there a reason/intent/usage for this change type that is not fulfilled elsewhere (by metrics like adjacence to justify creating the functionality)
          - is there a system-invalidating force requiring a new change type
          - is there another position that could use similar functionality to existing functionality that is inaccessible in that position
        - is this change type adjacently buildable with system resources
        - is this change type probable
        - would this change type trigger changes that invalidate the system or reach stability
        - how would this change type interact with other change types
        - does the environment system change enough to justify developing another or extra change types
        

  - finding formulas: equate structures like:
    - concept: 
      - 'aesthetic': generating aesthetic formulas using simple/balanced/relevant structures
    - pattern:
      - generating formulas based on patterns & anti-patterns of other formulas
    - structure:
      - using limits that bound other formulas as assumptions to reduce solution space
      - finding vertex variables of formulas & applying variations to generate other formulas
      - https://www.vice.com/en/article/xgzkek/machines-are-inventing-new-math-weve-never-seen

  - authorized pick-ups/drop-offs by people in your social circle, extra keys for drop-off in lock boxes or cars, picking up packages from warehouses
    - https://www.vice.com/en/article/v7mnga/amazons-megacycle-shift-will-push-some-delivery-drivers-out-of-work

  - 'shared responsibility pools' as a form of insurance
    - anyone who uses a particular proxy/VPN/cryptocurrency accepts some responsibility for requests/transactions executed on that service in cases where the actual criminal cant be determined
    - feature where they can pay to prevent non-verified users from using the service or pay to use an 'invite-only' service

  - differentiate change (sequence of difference structures) vs. difference (non-equivalence on some metric) vs. variable (attribute capturing an isolatable change)

  - give example of alternative filters/routes & identifying optimal filter/route structure, as well as optimal starting point (origin), direction (target) & steps (queries) to generate them

    - the below 'reverse engineering' example uses the following filter query to determine relevance:
      - relevance = reverse(similarity => core => (combine, not) => adjacence)

    - but it could also use alternate filters such as: (substitute || (similarity, quantity) || test)
      - apply 'substitute' structure: find a metric that functions as an identifier, filter, approximator, predictor, or proxy
      - apply 'similarity' structure to 'quantity' attribute: find a metric value for a quantity of more than one unit
      - apply 'test' structure to problem system structure: find tests with output information containing the metric value

    - these alternative filter sets optimize for metrics like:
      - filter set metadata
      - optimizing for different interface metrics (variance degree, interaction layer, abstraction level)
      - having a particular structure (paths to connect source/destination) that uses available functions
      - maximizing a particular change or difference type for identification/accuracy-related intents
      - connecting difference types in different spaces (standardization)
      - interface structure-fitting (like 'intent alignment' or 'lack of contradictions')

    - these alternative filters have different metadata, like:
      - cost
      - variation sources (equivalence definition)
      - variance reduction (degree, type, pattern, potential)
      - requirements (like required information access)
      - path (in the filter network, & also possibly a path in the problem structure network)
      - interfaces, structures, & definitions used ('questions' asked by the query, 'alternatives' used as 'approximations')
    
    - these questions have the structure of a theorized directed connection/path in the problem system formatted as a network
      - the patterns of these questions in producing relevant info for a problem can be used as insight paths
      - alternatively, apply a general insight path of calculating which paths in the problem network have the sequence of input/output information that could produce the answering info to the query
        - formatting the system with structural interface metadata (such as info gaps, intents, incentives, equivalences, & vertex variables) will make these optimal query patterns more obvious

  - organize list of high-impact queries which can be used for finding optimal solutions manually now while building product

    - query: reverse engineering solution metric with core structures as filters to find relevant metric structures

      - problem statement: 'find individual unit metric value in a container having equivalent & different components, without a function to measure individual unit metric value, and given total container metric value & unit count'
        - find relevant structures of the metric
          - apply insight relevant to 'calculations': 'apply the same standards when calculating if possible'
            - apply concept of 'similarity'
              - find relevant structures having the same metric
                - find relevant structures to 'unit'
                  - apply core concepts/structures to problem system structures
                    - apply core structures of 'combination'
                      - relevant structure: set of units, having an aggregate metric, usable input to an averaging function
                    - apply core concept of 'opposite' or 'not equal' and the core concept of 'total' (the complete set of all components in container)
                      - relevant structure: set of non-unit components in container, having the same metric, usable input to a subtraction function
          - find most measurable structure (with greatest accuracy or fewest steps) out of the relevant structures having the same metric
        - find calculation relationship between adjacent proxy metric of relevant structure and original solution metric (individual unit metric value)
          - calculation relationship between sets of not-equal components and equal components to the individual unit metric:
            - calculation relationship: "subtract not-equal component set metric value from total value, and divide by unit count to find individual unit metric"
          - to find this relationship, execute the opposites/reversals of the operations to find the relevant structure metric values
            - 'subtract' is opposing function of 'combine'
              - 'combine' was executed to get the list of sets of components (not-equal components & equal components)
            - 'divide' is opposing function of 'combine'
              - 'combine' was executed to get the set of equal components, relative to the individual unit
            - these two combine operations were used to create a path from the individual unit to the set of total components in the container
            - they can also be applied in reverse to get from the given total container metric value to the individual unit metric value

  - use isolatability/inevitability/uniqueness as a structural foundation for interface conversion/generation logic

    - identify 'inevitable' definition routes that are unique which can be used as a default generation intent for the core data included for app functionality
      - example: a definition route that cant be used as a definition of balance & power, just one
      - unique intents are also a useful foundation structure for the intent interface

  - apply structures to error types
    - false equivalence structures:
      - 'lack of functionality' bc of root cause of 'lack of memory' or 'lack of functionality to build functionality' or 'lack of intent for that functionality'
      - the memory lack can look like a lack of ability, but its a false equivalence/similarity caused by a lack of an input resource, within a range of change potential where the memory lack & ability lack ranges overlap

  - apply structures to overlaps in definition routes
    - find the adjacent structure without contradictions, that doesnt resolve to either specific option, within the limits of both definition routes
      - lack/limit :: resource 
      - function :: resource 
        - resource-generating function :: resource
          - resource :: function

  - give example of mapping to structures & identifying contradictions its safe to ignore for applying a structure

  - identify structures (like contradictions & distortions from expected normal) as input to info type generation algorithm
  - examine which operations (rotate, connect, combine, shift) convert the base subset/limit functions represented by a neural network into the output prediction function
  - examine the distortion vector paths that adjacently decompose a data set into a prediction function from a base point/function set
  - algorithm to identify contradictions (of a statement formatted as a route between network nodes)
    - query for conditions that would make some input, component, or output of the statement function some structure of falsehood (invalid, impossible)
    - example:
      - query for intents that would require movement in different directions, 
      - query for causes or preceding/adjacent/interacting functions that would require development of functionality making some step in route impossible
  
  - give example of how to embed interface structures in neural networks (core functions, interaction layers, etc) to select different organization structures as components of the network (concepts like balance, functions/attributes like relevance/security, error type boundaries, abstraction levels, etc)
    - a granular intent structure like "differentiate => maximize => combine => compare => select" can map to a high-level intent like "voting"
    - these structural equivalences/similarities across interaction layers can be used to implement concepts like 'security' to neural networks, such as identifiable/possible error type structures as a boundary/limit (in the form of a threshold or weight-offsetting operation) across a metric calculated from an adjacent-node cross-layer sub-network (like 'function sequence' structures are often used in exploits)
  
  - consensus-building perspective algorithm (transform a structure in each perspective to a structure in the target perspective)
    - identify structure of attributes/functions/objects common to both perspectives (connecting function: 'function connecting power and distribution', 'function describing dictatorship dynamics')
      - identify interface objects within structures (change type in conneecting function: 'direction of power distribution', 'changes in identity & size of group in power')
        - identify similarities in interface objects within structures (similar change pattern in change type in connecting function: 'power favoring distribution', 'military coups after power abuses')
  
  - structural concept definition routes
    - nothing (lack) structures, as opposed to randomness (lack of differentiating info among possibilities)
    - opposite vs. lack (of common attributes/values, connections, similarities, spaces)
      - opposite requiring a potential for extreme values to occur in a structural possibility where difference can develop
    - thinking definition as 'applying structure to uncertainty'
    - reasonable (making sense) definition as 'fitting an existing structure, like a pattern, without invalidating contradictions' 

  - example algorithm to identify rules that violate a metric 
    - requirement like: 
      - 'dont exacerbate inequalities'
      - 'protect minorities on the disadvantaged side of an inequality'
      - 'identify advantaged side'
    - power structures: required or non-specific/universal resources (such as inputs to any function, like 'energy' or 'information')
    - inequality structures: differences in distribution of required resources
    - generate structures that would exacerbate inequality structures
      - assumptions in rules (lack of guaranteed potential to follow rule)
        - rule 'close malls after business hours'
          - rule structure: 'limiting supplies' (access to facility)
          - rule assumption: that they have alternative supplies
        - rule: 'fine for not wearing mask'
          - rule structure: 'requiring function' (purchase mask)
          - rule assumption: that they have inputs to a requirement
      - these assumptions would disproportionately increase inequality's disadvantages in distribution
      - 'disadvantaging rules/assumptions' can be distributed more evenly or to offset inequalities

  - identify semantic processing necessary to get good prediction results with existing algorithms & params
    - example: find the abstraction level or definitions necessary to get an approximation of system or conceptual analysis with a standard data set 
      - the approximation may leave out other analysis logic like alternative/combination analysis (to identify sets of alternate prediction functions, or causal/functional/priority/missing/type structures in the data set)
      - however it may find objects on an interface by including interface objects (include concept definition of agency/skill/decision in the titanic survival data set may identify concepts like 'education' as causative, given that a combination of agency/skill/decisions can be used to produce concept of 'education' = 'an agent making a decision to acquire a skill')
      - similarly, including structural definitions of 'relevance' may improve prediction results with standard algorithms, allowing output structures of relevance like 'semantic variable connections on the relevance level input to the algorithm', such as an 'explanation'
        - 'including' meaning 'standardizing to relevance structures, such as similarity/adjacence, inputs, interaction level, etc'
        - first you'd apply standard analysis to get a set of probable dependency graphs, with paths like:
          - gender => lifeboat access => survival rate
        - then you'd apply standardization to relevance structures to the dependency graphs
          - difference in functional position (gender roles) => difference in function (skills) => difference in usage (responsibility) => difference in resource access => 'survival' intent inputs => 'survival' intent fulfillment
        - the output would be an approximation of meaning, allowing explanations like 'being female (variable value) increased probability (ratio of outcome among possible alternatives) of being prioritized (randomness structures like starting position as well as the concept of agency in filter structure) for access to survival tools (type of 'lifeboat') bc of less agency/responsibility/skills'

  - add to decision points
    - when a method & data set can be determined to be capable of deriving the answer to a prediction function problem

  - questions that a computer may not be able to answer even with unlimited memory/computation capacity, without trial & error or other memory-based approach (simply storing methods that worked & incrementally building on that info) to determine system analysis methods
    - what calculations will prove to be optimal (faster/more accurate), before or during processing

  - examine subatomic superpositions as such a fast aggregation of time that each possibility is occurring simultaneously
    - if superpositions are a core physical structure of uncertainty, examine whether they can be used as a base for the optimal neural network structure, where core problem types are handled by subatomic particle type structures & other structures relevant to superpositions
    - applications
      - anti-structures: enforced lack of structure to preserve lack of structure development to ensure scale of operations
        - performing calculations in places with less gravity to speed them up and send them back to places with more gravity to get answers relatively quickly
    - questions
      - what combinations of velocity/time/scale produce equal positions/perspectives, and are there stable paths between them
        - what differences in potential emerge in different perspectives (differences in potential like reversibility)
        - how many different perspective types are there, and do they stabilize to a particular perspective in a vacuum
      - which change measurement syncs the best with time progression
      - which metadata (scale) are the best sources of randomness structures found in
      - which structures can store one-directional time (aka information)
        - where info is measurable, leaves signals, and processes are irreversible
        - is there a structure that can permanently store information (unchangeable information)
        - what structures of cause (inevitability, certainty, stability, equivalence) exist at subatomic scales
      - time speed factors: 
        - more interactions have to happen at larger scales
        - fewer things change at large scales
        - there are more randomness injection points at larger scales
        - change-resistance (stability) occurs more at larger scales
        - change measurability varies across scales

  - examine efficiencies from missing components
    - some functions are generated more quickly without a component, bc of the needs that the lack generates, which focuses generative processes on building alternate functions to fill the gap
    - this can be used as a way to predict what tasks the optimized network with missing components would be relatively good at
    - missing component metadata
      - how adjacently it can be learned/generated/invalidated/delegated/identified/borrowed
      - how likely it is to be learned/generated/invalidated/delegated/identified/borrowed
      - whether another missing component can be used instead
      - whether the system missing that component should be changed instead
      - whether a system having that component succeeds at the intent task (& fails at others currently fulfilled by the system missing that component)
    - example:
      - not having a function incentivizes:
        - identity: development of that function
        - abstraction: development of generalization of that function, parameterizing that function intent
        - alternate: development of a proxy or alternative or invalidating function, making the function itself unnecessary
        - cause: development of structure/function/attribute that invalidates the original requirement metadata (priority, intent, dependency structures), not just invalidating the function
        - alternate format: development of a structure/attribute that replaces the requirement for the function or allows the function to be generated as needed
        - derivation: developing a function to learn/derive/identify/borrow/cooperate functionality from external info, to generate functionality as needed
        - core: developing components capable of building all functions to generate functionality as needed
        - subset: developing components of that function so the function & other functions can be generated as needed
        - combination: development of a function capable of fulfilling that intent & other intents
        - distribution: distributing functionality-generating methods to all nodes requiring functions
        - organization: allocating gap requirements (uncertainties) to the gap in functionality (example: keep the gap so you can apply methods as a test to resolve the gap)

  - optimized network structure

    - the optimized network can be structured as versions for different intents like:
      - lowest-memory generator: the average network + distortion functions
      - relevant generator: the network nearest to the most useful versions of it
      - quick generator: the network with the components that can build other versions at lowest cost
      - core generator: the network with core components to build all other components
      - adjacent core generator: network with core components at an abstraction/interaction level where they are most adjacent (mid-level functions as opposed to granular functions or high-level agent-interaction functions or conceptual functions)

    - the optimized network (ark) has the interface components necessary to solve any problem, with no extra components
      - it has one of each parameter of required components (like definitions, bias/randomness/error structures, interfaces, core/change functions, etc) which provide enough functionality to decompose & fit all discoverable information into a system of understanding
        - for example, one example of each opposite end of a spectrum & the average in the center, or the average + distortion functions to generate the other possible values

    - can probably be adjacently derived from subatomic particle interactions, which implement the core objects of interfaces like cause & potential

  - primary variables of brain functionality:
    - connectivity/alignment
    - position/adjacence
    - structural integrity
    - available structures
    - circuits (closed/open)
    - sub-systems (optimized for a function)

    - for some problems, some aspects of the optimized network should be deactivated/inaccessible - give example of how to calculate the structures necessary to solve a problem structure

  - in a market where uncertainty & unexpectedly correct predictions (unlikely predictions) have value, high-value contradictions of assumptions (high price of low-valued stock) are an error type structure

  - stock market (predicting uncertain value) x gaming (low-stakes task completion in a system)
    - stock market tasks in a (legal, business) system
      - deriving value of legislation
      - predicting legislation
      - legislation (more static rules) competing with more dynamic rules
      - forming business structures to aggregate/delegate/distribute risk
    - predicting uncertain value in tasks
      - predicting which tasks will win a game
      - insuring against risk of players completing or not completing a task
    - feature request & prediction market
    - game plot/cheat code/successful strategy prediction market
    - prediction games
      - false signals, gathering/deriving info, identifying important variables, applying successful analysis rules
    - insurance & other risk & financial products in games
    - stock market games allowed by legislation to allow a degree of collusion/organization in prediction markets
    - organization & risk structures allowed in a particular game, for a level of difficulty/complexity
    - games accessed with performance in previous prediction games to find best predictors and assign them more complex problems, like predicting emergent trends in interactions of complex systems
    - business & stock markets as an info-trading game to get products/features/prices and other company byproducts (clean energy practices, mergers, etc)

  - apply anti-stupidity structures to neural network structure 
    - lack of learning functionality
      - inability to remember (identify relevant info quickly when new info isnt necessary)
      - inability to identify relevance structures (meaning, usefulness, direct causation)
      - inability to optimize (identify a quicker route to an insight, like an insight path)
      - inability to model structures (enough memory to store a different structure, ability to explore/change it like a visualization)
      - inability to simulate difference structures (contradictions, paradoxes, lack of similarity)
      - inability to direct thoughts (focus)
      - inability to forget sub-optimal/inaccurate rules (bias)
        - function to apply bias structures to a neural network structure
          - thinking benefits from bias removal
          - remove bias structures in neural networks to improve their thinking capacity
          - example
            - apply removal of 'simplicity' bias in a neural network structure
              - simplicity (specifically over-simplification) definition on structural interface: 
                - lossy lower-dimensional representation
                - low-cost representation with relatively reduced learning reward
              - the simplicity bias shows up in a neural network structure in many possible positions
                - for example, a pooling function, which has no reason to aggregate other than adjacence, which may not be an indicator of relevance
                  - find the structures that can build/derive/apply/store relevance and remove structures with artificial relevance
                - general default params also tend to store simplicity where it's not needed
            - apply removal of 'similarity' bias 
              - similarity bias structural definitionss
                - relatively adjacent in variable values according to a distance metric applicable & relevant to that variable
              - the similarity bias shows up when adjacent structures are given relevance/meaning that they may not actually be capable of storing/building/deriving, like subsets of inputs or clustering thresholds

  - neural network with anti-bias structures built in (a complexity structure, a difference structure, etc) to correct error types from common biases

  - function to convert article/listing/social posts into variables to enable queries (product with feature x in budget y that integrates with app z and has attribute independent)

  - document locked objects that are inputs to core objects (like functions & concepts)

    - core functions like 'change', with locked objects which should be generated as inputs to other functions and should not be removed bc they enable other rules & core objects
      - a 'check for errors' function
      - a concept of 'self-correction/optimization'

    - these locked objects can be used to generate rule-generating/deriving/finding structures, by forming an initial structure of locked objects and filling that structure with conditional & changeable structures
      - these rule-generating/deriving/finding structures can be used as solution automation workflows

  - difference vs. similarity
    - similarities between difference & similarity
      - distance metric
    - differences between difference & similarity
      - amount of info that needs to be stored for a complete accurate description ('what something is not' may require more info to be stored compared to 'what something is')
    - the position of difference between difference & similarity may be on non-opposite positions on a circle depicting routes to get from difference to similarity
      - this is bc a similarity is a degree of difference (low/zero difference) & so is a difference (higher degree of difference that can be measured or is observed as noticeably different compared to a similarity)
      - the structure may be a circle or other loop bc if you stack enough differences, eventually you may generate the original object
    - the conversion of difference into similarity is based on the concept of a threshold, where a difference acquires enough similarities to similarity to cross the threshold or vice versa
    - the gray area in between the two concepts & surrounding the symmetry of the threshold also conflates the differences between the two concepts, making the difference not a simple 'opposite'

    - example: spectrum structure
      - handles different cases like 'near low/high/average value' (like between 0 & 1), which have differences in adjacent change types to produce relevant objects (like an integer)
        - change types like 'small change to produce an integer', 'doubling to produce an integer', etc
      - the isolated relevant difference structure (without additional info) 
        - the average value, which has multiple difference types in adjacent change types
      - conditional relevant difference structures
        - if the nearest integer triggers other change types, the value near that integer has a relevant difference structure

    - example: position structure
      - similar positions will be near according to the distance metric, creating a radius of similarity, which results in emergent structures of a boundary, center & circle
      - different positions can be represented as a structure lacking a circle/boundary/center
      - the differences in similarity/difference structures have emergent effects & coordinate with different interface objects (like adjacent structures, change types, relevant objects, etc)
        - a lack of an object can be used like other gap structures are used (as a filter, container or template)
        - an object can be used as a component or other base object to use as an input

    - this is why differences are not just the 'opposite of similarities' - it leaves out information like:
      - similarities of varying relevance between similarity & difference (both use a distance metric)
      - the reason why a difference is used vs. a similarity (like 'filtering' intents)
      - emergent/adjacent/relevant structures of similarity & difference, embedded in different structures (position/spectrum)
      - info about the structure of difference (difference paths/stacks/layers/trajectories), which may vary in ways that similarities do not
        - this indicates the important point that similarities are insufficient to predict differences
      - if similarities were equivalent to differences, you could use similarities to derive all info, reduce all uncertainty & randomness, and solve all problems - which is not guaranteed
        - meaning 'derive structures outside of the universe, using info from inside the universe' 
      - similarities may have similarities to each other, more than similarities to differences
      - randomness has a similarity (in outcome probability), but is better than similarity as an input to generate difference structures like uncertainty

  - document uncertainty structures like randomness collisions & structures that produce certainty (combinations that stabilize)
    - randomness collisions generate structure
      - structure being the stabilized interaction of information (staying constant long enough to attain structure)
      - randomness being a lack of information (like a star or circle with equally likely directions of change)
        - where influences are equal enough in power to leave no clear priority of direction favoring one over the other
      - when an info lack interacts with an info lack, they may not generate another info lack, but a structure stable enough to organize them, depending on the angle/type of interaction and whether the info lacks are a similar or coordinating type

  - document interacting AI error types (as in financial price & crime prediction models)

  - standardization application to generate logic automation algorithm

    - iterate through interface objects (change type, problem type, assumptions, etc)
      - find interface objects in a problem space 
        - filter by relevance structures (like interaction directness/causation, such as change hubs)
          - apply problem structures related to relevant structures
            - apply organization structures (like a sequence of tests/queries) to problem structures

    - specific logic automation example
      - check for missing relevant info in info found with variables
        - change to add earlier window to mtime param bc its out of error window
      - find interaction type & change type in info metadata (filename, modification time relationship)
        - any logs changed in later would include logs modified earlier bc of lack of incrementing/rollover, so mtime increase is unnecessary
      - check assumptions for requirements
        - mtime param unnecessary bc most logs would be modified in original mtime param
      - check for relevant change-aggregation objects in structure (event objects in a sequence structure)
        - significant date (upgrade, reboot) was within original mtime param which could be a factor in error so mtime param is necessary

  - examine function topologies (structures & structure change metadata that can maintain a particular function)
    - document intent structures (like intent sets) associated with function topologies
    - even if a structure maintains a particular function, its other metadata like adjacent interaction/change types & intents may change with the structure change
    - intent topologies dont necessarily match metadata of function topologies
    - interaction of interface object topologies as a source of variance reduction

  - why structural analysis of components (like cell shape/surface) is insufficient as a predictor of functionality
    - it's missing info about:
      - components
        - other/possible components & their structures (other possible pathogens, foreign cell types, in other ratios/positions)
          - other/possible components with similar/contradictory shapes that might be interfering
            - like similar receptor/binding shapes that leave no room for the cell type being examined
        - internal cell components not measured or formed unless found in a particular environment context
      - change types
        - changes to the host system structure (like nerve damage)
        - changes to forces governing change (like motion, as blood flow) in the host system structure 
        - not measurable info
          - hidden non-structural variables (like blood flow/pressure, electrical effects, or prior exposure to nutrients like vitamin d triggering timers) or variable sets with similar net effects (activated lifecycle)
          - distortions commonly found in different cell types with same structure bc of different positions
          - functional implementation differences
            - different cell types have different method of achieving the same function using the same components, in a structure that varies within the data set but not enough to indicate different method
        - component interaction dynamics
          - interaction level
            - cells with same structure might operate on different interaction levels, given different position/system
          - structures of interaction object components
            - a cell with equivalent DNA might encounter 'jumping gene' functionality in one system position, where an equivalent cell in another position would not
          - determining interaction attributes/functions 
            - like how attributes like aggressiveness might be determined by missing info (indicating why one cell type would succeed at binding & another of a similar/equivalent structure would not)
        - limit/threshold dynamics
          - sample data might leave out variation in the form of determining cell type attributes like size above a threshold with emerging behaviors, or potential to change that attribute triggered by the environment
        - state dynamics
          - false equivalence: structure might be measured at two equivalent states across two different cell type lifecycles (like evolutionary paths or distortion patterns), giving illusion of equivalent structures
        - system dynamics
          - structural metadata (like position, which determines local system & adjacent cells/functionality)
          - invalidating functionality
            - system that deletes duplicates, where a particular cell type is handled second bc of some attribute (like size, indicating it needs to be broken down first), so its always found to be the duplicate & is deleted
          - functionality that is activated in environments & not obvious with structural analysis 
            - like a function that folds dna/proteins in a way that has more errors than other folding function in a particular environment
        - sequential dynamics
          - exposure to a pathogen might trigger a function in response to a cell type with a minor distortion that becomes determining in edge conditions

  - structure standardization (applying structure to structure to generate a particular structure/format)
    - translating structures into vectors
      - many vector structures can represent interface structures
      - example of selecting a vector structure to represent an interface structure on a particular interface, applying structure to indicate metadata about structures
        - example: causal loop
          - standard network structure translation: vectors to indicate direction of cause
          - relevant network structure translation: vectors of influence degree away from hub cause & other cause structures

  - variables of the network include structures emerging from or embedded in algorithms/structures

    - core structures
      - change types 
        - difference type 
      - agency types
      - cause types (influence/power of structures)
      - structures
        - sequence (embedded concept of 'time' in structural interface)
        - list (unique index)

    - structures applied to these core structures to generate conceptual structures in neural networks
      - alternative cause: change applied to causal structures at training & prediction time
      - organization: difference type index
      - agency/govt: decisions about change types to apply

        - structures applied to agency objects like decisions (such as subsets/alternates) & other conceptual structures (like time)

          - sub-decisions
            - structures of neural networks with delayed sub-decisions
              - conditionally activated cell structures with enough info to make a sub-decision
            - structures applied to decisions can generate networks with other decision structures than 'consensus voting'
              - govt structures/algorithms
                - organization structures are a structural version of govt (agent-based) decision-making
              - finding the level of 'agency' to apply to a network is possible with problem complexity identification
                - apply agency: delegating decisions to subsets/groups/layers of cells to delay change decisions to another point in time

          - alternative decisions
            - decisions are a 'selection/identification/filtering' problem about a possible change type (like direction) to consider/implement
            - structures of neural networks exploring alternative variable structures & alternate decisions rather than the stated problem decision or default variable structure (identify direct causation, filter out non-directly causative variables)
              - alternative decisions
                - finding root cause
                - solving a proxy problem

          - decision (change-filtering problem-solving) times

            - standard time points: training time, data gathering/processing/standardization time, decision/prediction time, re-training/update time, parameter selection/update time
              - sub time points: activation time, pooling time, aggregation time, filtering time

            - optional points where decisions can be injected
              - decisions: 
                - network-level decisions: continue learning, select prediction answer
                - structural decisions: change direction, identify threshold, ignore info
                - meta decisions: delegate/delay decisions, consider alternative decisions
              - time where decision is clear/final/starts to emerge
              - time where direction change decision is made
              - time where more info/time is identified as necessary
              - time where decision is identified as not answerable
              - time where alternatives are identified, assigned probability, filtered out
              - time where possible routes to an answer are identified (what structure of variable values like 'ranges' can produce a clear answer)
              - time where possible decisions remaining are identified (and conditional remaining decisions if a change is applied)
              - time to check for a structure in the difference type index
      
  - real vs. ai detection algorithm
    - variable count/size (under-complexity, fragmentation, lack of smoothness/curvature)
    - wrong context for a pattern
    - over-repetition
    - over-similarity to previous information (lacking expected change structures, like change trajectory & types)
    - no matching reason/intent/priority for deviations from archetypes/patterns
    - over-correction when integrating a variable
    - variables identified in isolation
    - most clearly/measurably different variables identified
    - structure organizing variable structures (randomness injection points, enforcement gaps, info imbalances)
    - over-simplistic or erroneous automated sub-components
    - improbable level of randomness
      - clear composition of core patterns
    - sources of randomness
      - errors are evenly distributed among more complex adjacent sub-components not expected to change as much

  - the most useful patterns will be:
    - cross-interface patterns: patterns linking interface objects
      - patterns of interface components that link all interfaces: 
        - error patterns
      - patterns of interface object links
        - change trajectories of randomness
    - system patterns: which unite other structures and form an interim structure in between meaning and an agent
    - core patterns & core interface components that can build other components
      - patterns in core interface components, like change/difference patterns

    - errors are a difference type in a specific structure (between expected/actual values) so theyre useful as example core problem signals
      - stacking errors may be a better way to frame problems than other interfaces
        - the level of randomness captured by the error structure
        - errors can function as limits as well as difference types building a problem structure

    - when testing different variable subsets, you can select a variable set split by structures like:
      - vertex variables
      - variables on interim interfaces where other variables aggregate (in bottlenecks or hubs)
      - difference interactions
        - difference type (homogeneous sets of difference types)
        - differences in different types (heterogeneous sets of difference types)
        - which difference type sets would identify the most errors or are the most different from other difference type sets
      - which difference types are the biggest variance-reducers when combined
      - which difference types have an attribute (common, relevance, similarity)

  - identify bias structures as output of operations on structures, or by missing structures that cause bias
    - bias is a filter that leaves out relevant information
    - 'facts without connection to meaning' is a biased priority (current state of truth) and a biased lack (ignoring potential truth & potential connections that change the meaning/position of facts)
      - example: if you just focus on data set facts, you miss other facts (contradictions, counterexamples, alternative conditional variables/functions), as well as opportunities to derive other facts from the data set (given the favorability of the data set to influential entities, we can derive a guess that other facts might imply a different conclusion), and the connections between the data set facts & other facts (other facts imply a different cause than the data set facts) as well as the meaning of those connections (why this data set was selected)

  - design an optimal sorting structure for general interface queries to apply to problems manually

  - starting points of filters that reduce the problem space
    - starting point of identifying all the assumption sets that it would be most problematic to get incorrect to prevent the worst error types
      - in the problem of 'predict cat vs. dog', the worst error types are:
        - an object from one category having all the features used to differentiate between categories, but with variable values of the other category (cat having dog features)
        - an object that is artificial identified as real (cat robot identified as a cat)
      - to predict these error types, certain concepts need to be inferred
        - the concept of 'agency' to design a machine that looks like an animal
        - the structure of 'false equivalence' to design situations where features would look like a category but not actually be that
    - starting point of identifying all the feature ranges where it would be impossible to give high-accuracy answers (ai-generated cat image vs. real image)
    - organizing these filters in a useful sorting structure (network, tree) can reduce the computations required to solve for a prediction function

  - predicting prediction function error types
    - false equivalence
      - similar routes to different answers
        - this implies similar patterns in variable structures & interactions across data groups
      - overlap
      - lack of differentiating variables in data set
    - false difference
      - merging/imminent similarity/equivalence
        - functions that can act on other functions to produce a false or real equivalence to another function
      - alternative routes to the same answer
        - identify all the alternative structures (routes, combinations, trees) to an answer between function components like variables, data sets/subsets, & neural net components like weight path patterns, and the differentiating factors & vertexes, then use that to implement a filtering structure to sort through them to rule out the most possible answers the quickest
      - alternative answer types
        - identify all the different variable/function combinations that could create the most differences in similar answers (such as different types or contexts like a separate function for outliers), and a filtering structure to apply these as variation-reduction functions
      - these filtering structures can act like interfaces, reducing variation in the possible answer set
    - equivalent combinations
      - alternative variable subsets that act as proxies to an answer
    - equivalent variable structures
      - find variable structures like functions that approximate other variable structures like variable networks

  - example of how to predict most interactive/causal concepts in a system

  - make diagram 

  - list interface selection (based on inputs like available APIs/data sets/definitions)

  - the problem is the solution in a different format, or a piece of the solution (problem being a sub-optimal state to optimize, or a difference that shouldnt occur, and the solution being a set of constraints forming boundaries, or an optimal structure to construct)
    - filling problem
      - missing info problem: the solution format is the complete structure
      - optimization problem: the solution format is the variables/system organized to comply with/fulfill the metric to optimize 
      - aggregation problem: the solution format is the aggregation method to form a structure (like combining core functions to get a function for an intent)
    - limit problem
      - constraint problem: the solution format is the removal/invalidation of that constraint
    - reduction/decomposition problem
      - complexity reduction problem: the solution format is the set of variables that reduces complexity of the problem
      - randomness reduction problem: the solution format is the set of variables that can replicate a semblance of randomness
      - problematic structure: the solution format is reducing the structure (identifying variables & invalidating those variables)
    - organization/mapping problem: the solution format is the set of relevant components in the right structure (positioning & connecting them)
      - conflict problem: the solution format is positioning the conflicting problematic vectors so they dont intersect
      - balancing problem: the solution format is the distribution of resources nearest to a balanced state (subset of matching problem, by matching distribution across positions)
      - combination problem: the solution format is the set of components in a combination structure that doesnt contradict combination rules (components fit together, like 'finding a system where a function can execute')
        - connecting problem: the solution format is the set of functions that connect the components, in the position where they act as connectors
    - finding problem
      - discovery (insight-finding) problem: the solution format is the set of generative/distortion/core functions or the set of filters to find the insight
      - route-finding problem: the solution format is the route between two points that doesnt contradict any solution constraints and/or optimizes a solution metric
    - other solution formats would be for adjacent/causal problems, solution formats that invalidate solving the problem, etc

  - to generate solution automation workflows:
    - combine problem types
      - a reduction/decomposition problem + a filling/aggregation problem = the solution automation workflow 'break a problem into sub-problems, solve sub-problems, aggregate sub-solutions'
    - combine structures & connect structure combinations by problem types
      - the structure combination of 'a sequence injected in a network' is a structure matching a 'route finding problem', so apply solution structures that find a route in a network, such as filters using metrics or rules that can filter routes by which routes dont contradict rules
        - the solution automation workflow is 'find structures relevant to resolving problem structures like inequalities in other structures' (inequalities like the difference between start/end positions)
        - the workflow matches 'sequence in a network' with 'route filtering structures', connected by the problem format 'find a route'
    - combine structures & core functions
      - the structure of the core function sequence(find, apply, build, filter) = matches solution automation workflows like 'find components which, when this function is applied, can construct this structure, complying with these solution metric filters'
    - combine components of solution automation workflows (functions, queries, interfaces, problems/solutions, structures) that have a valid input/output sequence

  - problem objects: solution constraints/metrics, problem space variables, available functions, useful formats/structures

  - how to find variables in a problem statement
    - find isolatable change types
      - if the problem is 'predict movement of object', this means: 'find change in possible orthogonal directions'
        - filter out redundant variables (like if variable A/B + randomness constant can be replaced with variable C + another randomness constant)
        - filter out variables or variable structures like combinations that look like randomness to leave sets of variable/s
          - find prediction function for variables with randomness excluded
          - apply degree of randomness with randomness accretion patterns & interaction structures (like other objects on interaction layers) to prediction functions once variable dependencies are described, to generate prediction function set or prediction function with distortion vectors for possible ranges, then test on data 
    - variable sets that cant be ruled out can be considered sub-problems to solve ('rule out this variable set') in addition to the original problem of 'finding a prediction function'

    - concept-structure 'find prediction function' interface query
      - find solution filters
        - find range of error allowed for solution
      - convert to problem interface
        - predict missing info 'future state of variables' with input 'past information'
        - standardize to structural interface
          - find vertex concepts
            - 'find prediction function' using past information involves:
              - risk structures like: possibility that an unknown structure is causative
              - randomness structures like: possibility that known structures will be distorted by randomness
              - change structures like: possibility that known structures will change & info needs to be found/derived to update variables
            - combine risk structures, randomness structures, & change structures
              - filter which combinations match data
                - filter which combinations match data within range required by solution filter

  - general interface query example for 'find prediction function'

    - change: find highest change problem variables in problem statement
      - structure: find combinations/subsets of variables
      - cause: find dependency structure of variable subsets
        - function: find input/output sequences of variable subsets
        - structure: filter the sequences by whichever sequences link the source/target structure
          - problem: solve sub-problems of organizing variable subsets
          - structure: aggregate sub-problem solutions

  - specific interface query example for 'find prediction function'

      - change: find highest change problem variables in problem statement
          - which probability distribution it is
          - variable values given
          - whether alternate probability distributions can be ruled out using constraints/assumptions/parameters/change types & other info of problem
          - sub-problems
          - sub-problem structure (organizing the sub-problems)

        - structure: find subsets of variables
          - example problem variable subsets:
            - missing info + variables values given + sub-problems
            - probability distribution + variable values given + other problems or problem patterns

        - cause: find dependency structure of variable subsets
            - missing info + variables values given + sub-problems
              - with the missing info & variable values given, you may be able to infer the probability distribution (though not always if the problem statement is ambiguous) and derive the sub-problems to solve
            - probability distribution + variable values given + other problems or problem patterns
              - from the probability distribution & variable values given & other problems, you may be able to infer what the missing info is given questions usually asked with that distribution

          - function: find input/output sequence of variable subsets

          - structure: filter the sequences by whichever sequences link the source/target structure (variable values, probability distribution & missing info, 'probability of event')

            - problem: 'predict probability of event A given event B & some parameter/condition C'
              - sub-problems
                - identify problem metadata (probability distribution, variables & values) in problem statement
                  - identify missing info (specific problem to solve, like 'find the missing info that is a probability of a specific event')
                - identify alternate interpretations of problem
                  - filter alternate interpretations (to likeliest or the interpretation with no contradictions)
                    - match variables & values in problem with parameters of the probability distribution or relevant functions
                      - filter functions to functions with output type 'probability'
                        - filter functions to functions with specific output probability matching missing info
              - aggregate sub-problem solutions
                - missing info:
                  - apply variable values to relevant functions to generate missing info (specific output probability)

  - identify economic cycles not integrated enough with other economic structures so as to be considered essential
    - debts to entities who dont provide essential inputs or inputs further up the chain with x degree of distance from essential resource suppliers

  - add to insight path & solution automation workflow indexes
    - find an example & generalize
      - find core/unit objects, find example using those objects, & generalize
    - find an example & counterexample & connect them
    - execute a problem-reduction function/structure/question sequence
    - execute a solution-space reduction sequence before solving for remainder problem
    - run query to find interacting interface structures, then apply solutions for that specific problem space's interface network
    - identify vertex variables first & approximate
    - identify problem types & corresponding solution aggregation method for that set of types
    - identify alternative problems to solve (like whether to solve for organize, format, select, re-use, derive, discover, build, diversify, optimize, distort, or combine problems/solutions) & apply problem selection method, then solve

  - examine structures of trend convergence 

    - trends 
      
      - micro internet markets
      - micro/specific app favor markets
      - violent power transitions
      - competitor/competition bans/taxing
      - currency/wi-fi competition & dictators as a source of stability
      - anti-democratic activity as a specific case of anti-trust activity
      - investment in job creation/antiquated tech subsidies
      - customer product lock-in
      - dependent product price-raising
      - drug discovery automation
      - all-service companies
      - info derivation tools
      - temporary/sequential info markets as a social mobility/equalizing tool
      - delegation of high-cost/low-interest problems to AI
      - ending resource inequalities (tech, energy, internet)
      - hacking targets (democracies, big consumer markets like traders/gamers)
      - labor trends of balance between priorities (organization/innovation/optimization/integration/cooperation/research)

    - structures

      - cascading errors
        - AI is applied iteratively to tasks that people dont want to pay attention to bc they assume lack of relevant or changing variation, which may include monitoring AI errors or designing AI tests

      - interacting trend trajectories
        - price manipulation for investments in systemic price reduction (ending resource inequalities necessitating competition for moats)
        - markets for info, decisions, risks, intelligence, potential, justice, laws, independence, problems/solutions, customization, organization
        - competing prediction/computation tools: stats, system analysis, quantum tech, AI-optimized processing units
        - AI as an error-correction tool for quantum tech
        - checks & balances through competing evaluation tools: 
          - science experiment automation, automated testing tools, AI, quantum computing, system analysis, stats
        - evaluation/info-derivation/prediction/computation tools as components of a system building understanding
        - competing task runners: AI, robots, & gig workers
        - contact-reduction & independence tools like 3d printing
        - organization tools, encryption & dictator overthrow-planning/subversion, consensus-building, or dictator-manipulation
        - organization of competition in a problem market, for important optimizations only
        - market selection/optimization/automation

  - make interface query output diagram

  - organize examples
    - label examples so they can be queried more structurally
    - query for logic in examples when implementing functions
    - give structural query example diagram for GANs + image compression problem

  - generate default function list

  - add mapping for data sci use cases => tools

  - function to translate interface query logic into interface language (combination of core functions (find/build) & other core components)

  - de-duplicate logic
    - organize interface analysis logic definitions
      - organize functions in problem/interface definitions, before organizing functions in implementations/*
    - integrate problem_solving_matching.md
    - integrate find/apply/build/derive logic from system_analysis/ & maps/defs*.json
    - separate interface analysis logic into implementation/functions (functions dont need unique info)
    - add functions from workflows & analysis (to do list, questions answered, problems solved, interface definition & functions) as files in functions/ folder
      - organize into primary core functions & list sample parameters (like objects to identify for the identify function)

    - integrate rules from other diagrams not included in patent applications to relevant documents
        [0010] Example embodiments will be described and explained with additional specificity and detail through the use of the accompanying drawings. 
        [0011] FIG. 1. 'User Interface Module' illustrates a diagram of a user interface that can accept user input about a problem & program configuration. 
        [0012] Fig. 2. Interface Analysis Module 140 is a diagram of example components (such as functions & constants) of a program to automatically apply information formats to achieve an input intent. 
        [0013] Fig. 3. Machine learning system 120 is a diagram of an example wrapper component that would call a machine learning system to predict a variable. 
        [0014] Fig. 4. API finding/calling system 130 is a diagram of an example wrapper component that would call an API finding/calling system to retrieve data. 
        [0015] FIG. 5. 'Structure Application Function - Apply Function' illustrates applying a structure to another structure. 
        [0016] FIG. 6. 'Problem space visualization' illustrates an example visualization of a problem space. 
        [0017] FIG. 7. 'Network of related problems' illustrates an example of a network of related problems. 
        [0018] FIG. 8. 'Problem Types' illustrates a set of common problem types formatted as information or structural problems. 
        [0019] FIG. 9. 'Problem formats, with matching solution formats of problem formats' illustrates an example of various problem formats & solution formats that match them. 
        [0020] FIG. 10. 'Problem-solution structure-matching: apply a solution function to a structure containing the problem to find specific solution structures for that problem' illustrates an example of matching a problem with a solution. 
        [0021] FIG. 11. 'Finding alternate solution formats that fulfill different metrics' illustrates an example of selecting a solution format that fulfills a solution metric. 
        [0022] FIG. 12. 'Network of problem sub-problems, breaking a problem into components problems' illustrates an example of breaking a problem into a set of sub-problems, which once solved, can be aggregated with a solution-aggregation method as shown. 
        [0023] FIG. 13. 'Causal structure-matching' illustrates a method of matching causal structures to a variable set. 
        [0024] FIG. 14. 'Design Interface Query' illustrates a method of assembling input information into structural meaning relevant to the input intent, using a structure containing information formats. 
        [0025] FIG. 15. 'Concept definition network' illustrates a network of related concepts. 
        [0026] FIG. 16. 'Alternate definition routes' illustrates a set of definition routes for a concept. 
        [0027] FIG. 17. 'Match structure for a definition of a concept' illustrates matching a structure to a concept. 
        [0028] FIG. 18. 'Intent-matching' illustrates matching intent to structure & vice versa. 
        [0029] FIG. 19. 'Insight path application' illustrates insight path examples and an example of applying an insight path. 
        [0030] FIG. 20. 'Interface conversion & matching' illustrates an example of selecting an interface to traverse. 
        [0031] FIG. 21. 'Interface & traversal diagram' illustrates an example of a diagram indicating an example interface, & a diagram indicating which interfaces to traverse in what sequence (forming an interface query). 
        [0032] Fig. 22 is a diagram of a process that describes the general workflow for implementing interface analysis. 
        [0033] Fig. 23 is a diagram of an example usage of the system. 
        [0034] Fig. 24 is a diagram of an example environment in which systems and/or methods, described herein, may be implemented, including interface analysis module 220 in FIG. 22. 
        [0035] Fig. 25 is a diagram of example components of one or more devices of FIG. 22. 
        [0006] Figs. 1A - 1J contain diagrams of an overview of an example implementation 100 described herein. 
        [0007] Fig. 1A User Interaction Module 110 is a diagram of an example user interface implementation to gather input about a problem & program configuration for Solution Automation Module 140.
        [0008] Fig. 1B Solution Automation Module 140 is a diagram of example components (such as functions & constants) of a program to automatically find/derive/generate a solution for a problem, to implement the general execution workflow of Fig. 4. 
        [0009] Fig. 1C Machine learning system 120 is a diagram of an example wrapper component that would call a machine learning system to predict a variable. 
        [0010] Fig. 1D API finding/calling system 130 is a diagram of an example wrapper component that would call an API finding/calling system to retrieve data. 
        [0011] Fig. 1E Solution Output 150 is a diagram of an example output of the process in Fig. 4 that could be displayed & edited in the User Interaction Module 110. 
        [0012] Figs. 1F - 1I contain diagrams of an example problem-solving automation workflow (such as problem space structurization (formatted as filters/limits/functions/networks/vectors)) detailing a particular interface traversal format sequence that can be used to solve most problems. 
        [0013] Fig. 1F Finding matches between problem & interface components is a diagram of an example implementation of step 404 - 406 of the process of Fig. 4 (converting a problem to an interface, mapping between components of the problem & interface). 
        [0014] Fig. 1G Applying matching interface components to relevant problem system components is a diagram of an example implementation of step 407 of the process of Fig. 4 (applying matching mapped objects from the interface to the problem system). 
        [0015] Fig. 1H Applying solution metric structures to solution structures is a diagram of an example implementation of step 408 of the process of Fig. 4 (applying solution metric structures to solution structures). 
        [0016] Fig. 1I Example Object Definition Structures is a diagram of example structures forming the definition routes of an example system object on the structural interface. An example of a definition route is documented here: https://github.com/outdreamer/build-a-cure/blob/52c3461fdd3ff38284b63f8c2e71542f415d88d9/find_existing_solutions/system_analysis/maps/definition_routes.json 
        [0017] Fig. 1J is a diagram of an example usage of the system. 
        [0018] Fig. 2 is a diagram of an example environment in which systems and/or methods, described herein, may be implemented, including solution automation module 220 in FIG. 2 which refers to solution automation module 140 in FIG. 1. 
        [0019] Fig. 3 is a diagram of example components of one or more devices of FIG. 2. 
        [0020] Fig. 4 General Execution Workflow is an overview of an example process 400 for implementing problem-solving automation workflows in steps 402 - 410, from initial problem formatting to solution matching to solution application & analysis. 
          
  - using set theory in query operations:
    - edges as core organizing/formatting operations (find/apply) & interfaces (connecting/explanatory concepts/functions)
    https://en.wikipedia.org/wiki/Hypergraph


## examples

  - example: to identify false information across user requests:
    - example of applying intent interface: 
      - check with intent provider (site) if a request for an intent (request password) was just made, to validate messages
    - example of applying pattern interface: 
      - check if user access patterns (like 'navigate to site, then check email for site password reset') match the intent of a message

  - example of permuting assumption: "reports of power consumption have to be exact measurements" (platypus)
    - a temperature monitor sensitive to a hundredth of a degree might provide similar but non-specific power reporting for important/extreme usage patterns without revealing such specific information as that which could infer exact operations being done, bc the interval of temperature measurements allows for greater variation in calculations that could explain it

  - finish dilemma problem type example formats
  
  - query examples for use cases like:
    - lack of information stored (match problem of type 'information lack' with interface query 'check pattern interface for similar patterns')
    - query problem breakdown & integration diagram
    - calculating various different problem breakdown strategies first before executing normal query-building logic for each
  
  - give example of generating problem types by applying structure
    - for instance, a common problem type is a mismatch/imbalance
      - by applying the 'mismatch' to the cost/benefit relationship, you get an 'inefficiency' problem type, which can be defined as a mismatch/imbalance between the cost & benefit, favoring the cost side (the negative object out of (cost, benefit), associated with problems)
  
  - add examples of system/object/rule/type change patterns
  
  - include example workflows with example problems
    - include example of how to generate other workflows (different starting/ending points & trajectories)


## diagram
  
    - add diagram for intent-matching
    - add structures to diagram: interface overflow (to sub-interfaces), interface foundation
    - diagram for workflow 1: 
      - function to determine relevance filter ('functions', 'required') from a problem_step ('find incentives') for a problem definition, to modify problem_steps with extra functions/attributes ('change_position') to be more specific to the problem definition ('find_incentives_to_change_position') for problem_steps involving 'incentives', so you know to use the function_name to modify the problem step if it's between the type 'functions' and the object searched for 'incentives'
    - add conceptual math interface query diagram
      - use lattice multiplication as standard example, other than core operations (add/multiply mapped to language, concepts like irreversibility/asymmetry mapped to math)
    - interface conversion, matching, starting point selection (applying structure, checking if relevant information is found)
    - diagram to document sub-functions of core functions with distortions
    - make diagram for dimension links higher than 3d that are depictable in the same network space
      - should show variables that impact other variables, the change rates of these relationships
      - overall impact should be calculatable from these relationships
      - should show similar movements for correlated variables
      - should show skippable/derivable variables (variables that can be resolved later than they normally are)
      - should show meta forces for overall trends in change rules (direction of combined variable forces)
      - should show limits of measurability & threshold metrics
    - structurize (apply structure to) definitions of objects specific to interfaces
      - example: info asymmetry is associated with an info loss in a particular direction between info types/formats, rather than just an info imbalance or mismatch
      - diagrams for specific concepts, core functions, concept operations (combine, collide, connect, merge, apply), ethical shapes
        - variable accretion patterns (how an object becomes influenced by a new variable, complex system interaction patterns, etc)
        - make diagram of potential matrix to display the concept
          - map parameter sets to potential matrix shapes 
        - finish diagrams for cause (shapes & ambiguity), concept (evolution of concepts, networks, distortion functions)
        - diagram for argument
      - make a system layer diagram for each interface to allow specification of core interfaces & other interface layers (interface interface)
        - make a system layer diagram for structures to include layers of structures 
          (beyond core structures like curves, to include n-degree structures like a wave, as well as semantic output structures like a key, crossing the layer that generates info structures like an insight, a probability, etc)

# content/config

    - import insight history data to identify insight paths (info insight paths like 'lie => joke => distortion => insight', system insight paths like 'three core functions + combine function with this definition + n distortions to nearest hub')
    - define default & core objects necessary for system to function (out of the box, rather than minimal config necessary to derive other system components & assemble)
      - add default functions to solve common problem types
      - alternate utility function implementations have variation potential in the exact operations used to achieve the function intents, but there are requirements in which definitions these functions use because they are inherent to the system. For example, the embodiment may use a specific definition of an attribute (standardized to a set of filters) in order to build the attribute-identification function using a set of filters - but the general attribute definition is still partially determined in its initial version by requirements specified in the documentation, such as a set of core attribute types (input, output, function parameter, abstract, descriptive, identifying, differentiating, variable, constant), the definition of a function, and the definition of conversion functions between standard formats.
    - document time structures (concave time explaining compounding similarities up to a point of maximum concavity, a structure that can separate from the other space-times)
    - systematize your definitions of info objects, to include analysis that produces relationships of core objects like opposites to their relevant forms (anti-symmetry) in addition to permuted object states (asymmetry), such as an anti-strategy, anti-information, anti-pattern
      - organize certainty (info) vs. uncertainty objects (potential, risk, probability)
      - make doc to store insight paths, counterintuitive functions, hidden costs, counterexamples, phase shift triggers
      - add technicality, synchronization, bias, counterintuition, & certainty objects leading to inevitable collisions
        - the collision of compounding forces producing a phase shift
        - lack of attention in one driver and false panic in a second driver leading to a car crash given the bases where their processes originate
      - define alignment on interfaces (compounding, coordinating, parallel, similar, etc)
      - start with these info object transforms that filter the most info: opposite, invalidating, symmetric, core, aligning, boundary-breaking, phase shift activating, structure stabilizing, constant changing, converging
      - add core info objects (core strategies, core assumptions) so you can make a network of graphs for a system
    - concept analysis:
      - how new concepts (gaps in network rules) evolve once structure is applied to prior concepts 
    - interface analysis:
      - limitations of interfaces & how to derive them
      - how rules develop on stability & how foundations are connected & destroyed
      - explainability as a space limited by derivable attributes from data set & cross-system similarity
      - vertex definition & give examples (as an intersection/combination of interface variables, such as determining/description(compressing)/generative/causative/derivation variables), around which change develops
    - change analysis:
      - generated object change types
        - constant to variable
        - variable to removal of assumption in variable type/data type

    - map variable structures to prediction potential for problem types, given ratio of equivalent alternate signals

    - calculating solvability limit of a problem, without being given the answer
      - example: standard 'psychic' magic trick like guessing number of fingers held behind back, or which number people will choose
        - connected structural info:
          - when they choose the number
            - physical motion rules
              - how arms/joints move 
              - how their eyes move (indicating remembering or creative process or a local distraction or another input)
          - default input rules
            - hand motion dynamics, like how fingers interact & which motion types are favored/prioritized/likelier
        - general rules
          - alternative selection rules
            - how people make decisions from a set of similar alternatives (familiarity, understandability, simplicity, standard vs. non-standard choices)
          - intent rules
            - agent intents (trying to surprise the magician by subverting expectations of their choice)
        - related variables
          - attention
      - limits of solvability occur with non-interchangeable (not equal) alternatives that can't be distinguished with the given info, without being given the info of the answer (or info that makes it identifiable or possible to filter/reduce other options)
        - there may be some combination of movement, rule selection, default config, attention & memory that produces difference choices without giving clear info signaling this difference (limit of solvability is reached)

    - optimizability of a problem, given resource limits (market, time, info about alternative, related, & interactive products)
      - buttons vs. configuration (headphones with buttons)
        - variables
          - hardware
          - alternative/related/interactive products
          - usage patterns
          - sound functions (play, skip, switch to voice commands, reduce noise, highlight bass, use more capacity to clarify sound quality, change relative volume, predict lost sound)
          - buttons
          - attachability/detachability/migratability
          - compartmentalization/isolatability
          - buildability
          - configuration options
          - simplicity
          - memorizability
          - adaptability
          - app
          - higher-variation alternative interfaces
            - sound input/output (alternative input to a button)
          - probability (commonness of a usage pattern)
          - demand (need for a button, configuration, usage pattern, or a function)
          - variable structures (combination of variables, like a particular set of variables or a set of interaction rules between variables)

        - implementations
          - find common usage patterns & assign to buttons
            - buttons for common functions
          - find memorable button structures & assign to common usage functions
            - find memorable combinations & sequences, like double-click of a button, or a button combination click, and assign to common usage functions
          - inject crucial high-variation function in higher-variation interface
            - configurable button functions (configure options of how buttons connect to functions), using an app (higher-variation interface, allowing more buttons)
          - inject crucial high-variation function into a button
            - configuration button (configure options of how buttons connect to functions), by clicking a config button
          - embedded menus in buttons
            - access menu (list of functions) with a button or button structure (combination, sequence)
          - alternate input with higher-variation potential
            - voice commands rather than or in combination with buttons
          - allow buttons to be attached like legos
          - allow buttons/functions to be coded & switched out to do any function the hardware (or connected hardware) can support, including functions from other alternative products
          - integrate with existing hardware like glasses/hat/shirt (use materials to conduct sound, attach speakers/microphones to glasses rather than having wires, attach buttons to glasses)
          - allow each alternative to be selected so they can choose which config/button/sound interaction rules to apply to those variables

        - optimized mathematized implementation for intent (simplicity, highest features given simplicity, maximized features)
          
          - simplicity: assign common (high-probability) functions to buttons & simple button structures (low-dimensional buttons & button structures)
            - variables: button count, button function, button structure (combination, set, sequence), function probability, simplicity

          - highest feature count, given filter of 'simplest implementation': highest number of functions possible to implement simply (low-dimensional memorization)
            - variables: function count, memorization, simplicity, abstraction (type), button usage structure (scale like repeated clicks of a button, sequence like buttons clicked in sequence)
            - variable interaction rules:
              - 'when function count increases absolutely (all other variables being equal), memorization decreases'
              - 'when count increases but is organized simply (like accessing functions organized by type or scale with successive button clicks), memorization is constant'
            - variable structure: 
              - intersection of independent variable changes (function count & memorization)
              - alignment of simplicity & memorization changes
              - alignment of abstraction (type) & simplicity changes
              - substitution of proxy variables (substitute more measurable variable like simplicity for memorizability)
              - substitution of more measurable variables
                - substitute simplicity-filtering rules to identify complexity rather than using complexity identification rules
                - substitute similarity-filtering rules (what something is) to identify similarity than difference identification rules (what something is not)

            - optimized variable structure: 

              - maximized 
                - parameterization of variables that change on similar input
                - intersection of variables to optimize (intersection of highest function count and highest simplicity)
                - alignment of related variables (aligning memorizability & simplicity) that should be similar
                - opposition of variables that should be different
                - compression/merging/selection of variables that act interchangeably

              - structure application
                - sequence structure applied to causative variation (input/output)
                - topology structure applied where changes in variable values of a variable set can be mapped to distance (different changes do not produce equal points)
          
          - maximized features: use highest-variation interface as input to generate temporary/editable config (app configuring which implementation to apply, which custom functions to use, which hardware to combine when ordering/updating)
            - variables: config input (voice, button), variable variation, config adaptability, config source (custom user-defined function, open source/multi-vendor libraries)

        - how to generate optimized mathematized implementations for intents
          - apply structural definitions of components (rules, variables, intents, concepts)
          - find interface where these structural definitions of components can be depicted according to their variation (dimensionality), interactions (substitutability, causation), & metadata (accuracy)
            - interface where variable structures (constant, sequence, input) and function structures (interactions/alignments) can be found & connected as needed

    - add to internet optimizations: add local data backup centers to cache copies of critical data just like backup electricity generators to methods of recovering or rebuilding crashed systems with alternate data sources
    
    - research implementing your solution type (constructing structures (made of boundary/filter/resource sets) to produce substances like antibodies, using bio system stressors)
    
    - merge definitions into docs/tasks/implementation/constants/definitions.json
      - add to definitions.json
        - meaning
          - structure:
            - relevance (to a position/intent)
              - importance
                - utility value
                - uniqueness
              - similarity
            - system fit
              - understanding

    - clarify/resolve terms that can be conflated: 
      - shape/structure
      - rule/test/metric/limit/threshold/boundary/state change/phase shift
      - intent/priority/motivation/incentive
      - method/function/rule/pattern (pattern is a sequence of specific objects)
      - path/route/trajectory/traversal/order/list/sequence
      - object/entity/item/component
      - type/class/category/group/subset
      - closed/isolated/independence/unique/orthogonal
      - model/perspective/filter
      - standard/interface/index/symmetry
      - dimension/variable/axis
      - space/system/context
      - perspective/filter/standard/index & relationship to variables/operations on the interface
      - filter vs. rule is a similar question to attribute vs. rule - sometimes one format is better based on the info you have, sometimes its worth it to transform the format
        - interface network: a set of standardizing filters applicable to format information in way that it can be analyzed with interface-specific logic, 
        - a query of the interface network may also be a problem-solving automation workflow, if problems can be solved with the format sequence indicated by the interface traversal

        -  For a prediction function problem, the solution space is the range of likely prediction functions. 
        - The problem space is the route between independent variables and the dependent variable on a network - it can also be framed as the route between common prediction function terms for a data set like the input data set, and the prediction function. The original problem structure is also depicted as a subset of this problem space visualization.
        - The solution function can be a route on the problem space if the problem space is formatted as a network, for example.
        
        - interface: a useful standard for comparison consisting of the filtering object's definition routes, conversion function, core functions, objects, & attributes, and related objects like patterns & metadata specific to the interface. Abstract interfaces include cause, concept, structure, etc, whereas specific interfaces are other foundations where change develops in a clearly defined range that can be found in specific systems. The traversal of an interface implies finding a map between objects, functions, & attributes inherent to that interface to the problem objects, functions, & attributes. The application of an interface is an operation in an interface combination, mapping, injection, or other operation. 
        - function: a set of inputs, a sequential list of rules or rule lists (logic tree) applied to the input set, an output set created by changes to the input set executed by the sequential rule list, optionally including side effects from execution. A function may be formatted in a different way, such as a list of attributes, filters, or a network. The function definition includes: 
            - attributes: 
                - alignment: enforced/optional, core, required, emergent/output (built from core functions, with or without associated intent) 
                - interaction: cooperative/conflicting 
                - intent: generative, filtering, grouping, organization/delegation/ distribution/matching/grouping/filtering, classification, differentiation/ transformation 
                - scope: use case, context, range, host system 
                - related objects (like host spaces/systems & object positions in those) 
                - types: 
                    - core functions 
                    - meta (rule-modification/generation rules) 
                    - attribute rules (state, scope) 
                    - interaction rules (competition, binding, combination, sharing, collaboration, intersection, conflict resolution, trade rules) 
                    - assessment rules (metric, difference, definition, validation) 
                    - processing rules 
                    - change rules (update, distortion, maintenance, adjacency, conversion) 
                    - filtering rules (find, identify, define, alternate, organize, learn) - matching rules (fitting a structure, filling a structures) 
                    - application rules (inject, embed, apply) 
                    - derivation rules (structure, navigate, abstract) 
                    - decision rules (prioritize, select, compare) 
                    - formatting rules (standardize, isolate, cluster) 
                    - destruction rules (replace, invalidate, neutralize, remove, merge, de- duplicate) 
                    - government rules (monitor, correct, enforce, maintain, stabilize) 
                    - system rules (incentives, variance handling, optimization) 
                    - interface rules (change, intent, type, pattern, concept) 
                    - info rules (problem, strategy, insight, game, perspective) 
                    - variance (injection, leaks, combination, replacement, causal direction, uncertainty, risk, potential, probability, prediction) rules 
                    - information handling (storage, versioning, replacement, merging, monitoring, indexing, communication, interpretation, processing) 
                    - solution rules (variance/stressor/error detection, tracing, identification & handler) 
                    - structure rules (gap, boundary, system, limit, hub, object, link, network, filter)     
        - intent: any of an abstract goal with direction, a reason to use something, or an output such as the intended result or an unintended side effect, which can be an abstract priority like fairness, a concrete goal, and can have a starting & ending position. 
        - game: a set of intents/alternatives/limits/incentives/exploits/rules/risk & a definition of distance from intent fulfillment (position), usually resulting in the resolution of a clearly optimal route. The game definition includes: 
            - a game is a type of system & a mixed set, which can exist as a component of a system 
            - games can have many different structures like: 
                - a directed graph with a vector set representing possible agent intents/ functions/resources 
                - a system of nodes & links where agents need function input resources to traverse 
                - a decision tree where certain tree info becomes accessible only at certain nodes (adding uncertainty/risk) 
                - a set of trade options between nodes with different info change/update rules in a system to optimize a resource/trade/market metric 
        - concept: set of unique objects remaining once objects from other spaces have their context & contradictory attributes removed. For example: power is the object left when objects implementing it: resources => energy => input => potential) have their context removed, navigating up the abstraction stack from: 
            - the info layer (resources & energy), removing their contextual attributes/rules - to the abstract structural layer (input) 
            - to the abstract layer (potential, which is a related concept of power) 
            - so that the final object is defined in terms of other abstract objects on the top layer 
        - problem: may include any context or condition that causes a negative position or state determined by a metric for an agent in a system. The problem definition includes problem types like dependencies, leaks (variance, resource/info),  injection (assumptions/variance/control/randomness), mismatches, conflicts, imbalances, inefficiencies, incorrect metric, misidentification, gaps, limits, side effects: whether it's a closed system or leaks variance (function side effect example: before execution: pre-computing, during: memory access/overflow, after: process re-starting), specific problems like an enforcement gap (should have enforced rule but did not), an unintended use (involves integrated third party tech not under review), a malicious alternative route to get same output, a legitimate/alternative route to get malicious output. 
        - problem space: context relevant to a problem; the containing system(s) of a problem that may include related problems 
        - solution: may include any combination of events, methods, or steps that reduces the negative position or state for the specified agent. The solution definition includes solution types: 
            - solution-metadata solution: evaluating & comparing solution metadata for solution selection 
            - problem-metadata solution: evaluating problem metadata to evaluate metrics like problem-solving postponement 
            - generative solution: solution that generates solutions 
            - solution framework: provides starting point & structures for solutions to be built in/with 
            - problem decomposer: solution that reduces a problem's root causative (as opposed to just varying) parameters 
            - solution automator: solution that automates solutions of a type 
            - interim solution: clearly suboptimal solution while optimal alternative is built 
            - solution query constructor: solution that builds new solutions out of known solution types (existing structural solutions or core functions) 
            - structure-finding solution: solution that assigns a structure to information 
            - structure-fitting solution: solution that matches the gaps/limits in a problem structure to neutralize them 
        - solution space: set of possible solutions in a problem space, which may be reduced by applying interface traversals like solution space-reducing insight paths 
        - network: standard network graph containing nodes (sometimes called vertices) & linking functions (sometimes called edges), with or without node attributes, function relationship determining function shape, and direction as an indicator of intent or another metric. 

        - component: functions/attributes/types/objects/systems 
        - input information: can refer to original information input to the initial interface traversal, or traversal output information that has been converted, enhanced, formatted, or otherwise altered in a prior interface traversal, stored as a possible version of the original input information, and sent as input to another interface traversal 
        - interface: 
        - function: a set of inputs, a sequential list of rules or rule lists (logic tree) applied to the input set, an output set created by changes to the input set executed by the sequential rule list, optionally including side effects from execution. A function may be formatted in a different way, such as a list of attributes, filters, or a network. 
        - intent: any of an abstract goal with direction, a reason to use something, or an output such as the intended result or an unintended side effect, which can be an abstract priority like fairness, a concrete goal, and can have a starting & ending position. 
        - concept: set of unique objects remaining once objects from other spaces have their context & contradictory attributes removed.  
        - network: standard network graph containing nodes (sometimes called vertices) & linking functions (sometimes called edges), with or without node attributes, function relationship determining function shape, and direction as an indicator of intent or another metric.  

       - info conceptual relationships:
          priority = direction
          observation = insight = function = result = relationship
          conclusion = ordered_list(observations) + guess = coefficients + bias
          strategy = ordered_list(insights)
          strategy = insight + context
          problem = (combination of intents having different priorities) or (an resource distribution imbalance)
          intent = strategy + priority
          solution = (combination of strategies operating on variables with insight functions that reduce dimensions of problem (function-combination) or (resource-imbalance))
          type = combination(attributes)
          intents = function outputs, including unintended/emergent/unforeseen side effects (target/avoid)
          roles = functions
          relationships = treatments, intents, functions, insights, strategies, mechanisms, patterns, systems
          components = compounds, symptoms, treatments, metrics, conditions, stressors, types, variables

    - update links

    - integrate archive_notes/finder_info/functions
      Terms:
      - objects: a data set, function set, attribute set, class definition, type hierarchy
      - attribute value: value held by the attribute like True/False
      - attribute property: conceptual metadata property of the attribute like unique, identifier, static, etc
      - decisions:
        - choosing to execute one section of code over another; 
        - for example a conditional statement, design patterns, emergent usage/behavior 
          of user/system, bugs, assumptions, & possible input values are decisions since
          they may result in calling different code
      - relationship types: sub-type, causal factor, cooperating equal, different version
      - strategy: rule used to make decisions, possibly for a particular context
      - solution: strategy implemented for a particular context & problem type