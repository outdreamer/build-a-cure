  - fix indexing 'NNP NN NNS1 of JJ JJ JJ2' or postpone to pattern evaluation time
  - fix missing alts
      pattern_index::verb_phrase::plays a |VB NN| role::a NN role
  - fix one-letter alts
      pattern_index::phrase_identifier::ALL_N DPC ALL_N |VBG VBD|::N D N V
  - generalize alt logic to use embedded pair finding
  - add formatting to allow multiple items as keys in json and maintain order for interface network paths

  - fix supported stem assignment (endings like 'is': {'functions a', 'acts a', 'plays a', 'operates a', 'works a'})
  - fix charge function ('edit' is assigned positive score)

  - add core clause patterns 
  - fix pattern matching functions
  - finish pos, clause, modifiers code from find implementation
  - finish network creation function
  - strategy/insight graph
  - add a standard system diagram with radiating layer diagram
  - add other causal structures

  - analyzing just by change rate makes it less likely to spot other patterns like overlap/intersection of patterns

  - add diagram for question derivation for service list

    - deriving the questions customers will ask for a set of services

      - which processes are complicated or not optimized (need to be in person for certain transactions that people would rather do online)
      - which processes involve changing information (account balance, transaction approval)
      - which processes are likely to have errors (auth)
      - which processes people will likely be interested in using the most

  - whats after dependence market (value in exploiting people's positions)? 

    - an independence market (value in setting others free of their positions)

  - whats after a love/emotion-based society? 

    - a problem-solving society based on potential, independence, and understanding, where everyone can self-motivate

  - whats after an information market (value in certainty)? 

    - a potential market
      
      - value in managing/arranging/creating uncertainty, manipulating/protecting time/change/potential, ensuring time/change enables information that benefits people

  - whats after a chaotic society with voluntary organization that allows corruption to gather & hide to advance other governments (feelings)?

    - a society organized by intelligence (as a proxy for potential), with alternatives to:
    
      - use stressors to create intelligence
      - generate intelligence using system/interface analysis
      - teach/share intelligence
      - find intelligence in a data source like the internet
      - install intelligence (learning models/maps, regression tools, logic tools like decision trees, prediction tools like a simulation/imagination engine)


  - is math causative (rules to describe information of certain types, structures, & potential), or is physics causative of which information can be generated with stability, enabling measurements & therefore allowing it to be described?

    - why are there numerical symmetries that attract information & values? this implies they're true or continue to be true, if they keep generating information

    - which physics filters (forces, particles, states) beyond the set of constants could generate the math system?

    - if the math system is causative, is it also changeable (can you generate different physics rules by changing it)

    - make a number type/attribute layer graph to fill in gaps in combinations

    - if physics generates information like fuel exhaust, and math can decrypt the exhaust into useful patterns, and those patterns have potential to predict & control physics movements, the math system could have power over the physics system

    - can the math system capture every different type of distance, time, value, etc that the physics system can generate? if one lags behind the other that implies some level of causation

      - what definitions of distance, position, value are missing in the network of definitions

    - do physics concepts like cause, time, & mutually exclusive alternatives (true/false, of an attribute captured in a superposition) leak into the math system as sequence/adjacence, position/change, or valid/invalid for that space

      - these can be classified as physics concepts bc logic has rules/physics describing it, and is a unit interface of other change interfaces

      - the physics/rule sets of a system describe its potential to generate information - the physics of math determine what information is describable in the set of system-generated information
      
      - example with the math system:
        - the physics/rule sets of the math system can generate information
        - the physics of math determine what information is describable in the set of math-generatable information (generated information is input to generation functions)
        - there's a logical loop between generatable information rules & describable information rules

      - explore physics of math determining spaces (sets that can be coverted, spaces that are possible) that evolve around symmetries/interfaces enabling those spaces

      - what other outer layers of spaces can be useful/generated with known operations on known spaces?

      - what system would need to be kept in place by these universal constants (which can be inputs or interim tools used to connect/stabilize the other elements of the math space)

      - apply system analysis to math

        - try swapping value definitions (quantity, position, scale, direction) in different spaces to test for symmetries & patterns that can be used as predictors

        - look for system patterns (attribute alignments, etc) and objects (efficiencies, incentives) such as where something is more calculatable

        - intents of function types:

          - wave functions to model a spectrum of stable alts

          - log functions to model a boolean decision

            - if a log function represents each fork and the aggregate output function amounts to the decision tree, are composite log functions reflective of common patterns in aggregated decision tree output functions
            - or the other way around, so a decision tree can usually be modeled with a logistic function once the variables are formatted the right way, like in sets of attribute sets/paths for 0/1 values

          - decision trees 
            - are a good frame for symmetry stacks
            - need dead-end nodes that represent unanswerable decisions (where information doesnt contain minimum required features, is unclear/corrupted, or points to all possible categories with an ambiguous level of equivalence)

          - support vector machine
            - can be applied to causes on various causal layers, after clustering them to differentiate them 
            - similar to the problem of finding a symmetry, also finding an average

          - clustering
            - in addition to centroids/SVMs, you can infer the inherited group data set, & the core functions to get to the original data set groups, and check for variables used in/output by those core functions

          - combining these methods could be used to allocate methods by complexity & type 

            - start with decision trees, use clustering to differentiate between remaining type group features (attribute sets), use inherited type inference & core function derivation to model any remaining features

            - start on the data layer, navigate up to infer types, trace causal functions connecting them as well as output branches that dont become types

            - start with the core objects, combination types, limits, layers, & filters that are most powerful in explaining interactions & design a custom algorithm based on those or other common interaction objects

          - use new data & change patterns to improve the model prediction method - if two clusters tend to evolve sideways (forming a square) and then converge to other corners, that can be used to improve the predictor early on with a small amount of new data once you know what direction the changes are headed in, vs. waiting for a lot of new data to confirm its not just noise

          - noise patterns can be used to reduce data variance to probable isolatable relationships

          - specify a definition of range of degrees of dependence that is classifiable as 'independent' from another variable

          - use probability in selecting assumptions - how often are variables independent (in two separate sub-systems that are isolated & only interact indirectly up to n degrees of interactions away)

          - is there a better standard assumption set that describes more relationships between variables that create complex relationships justifying analysis?

          - is there a variable type that clarifies relationships, which can be applied before gathering data, or which can be removed to generate complexity?

          - univariate polynomials to model isolated motion (same independent variable applied to itself rather than just a constant like linear functions)

            - when does isolated motion (parabola like x^2) look like group motion (normal distribution), and what does this mean?

              - if the motion of the x variable looks like the group motion, either the x variable has the group's information as an input, or its a false similarity, or the motion of an isolated variable can mimic the group motion under certain circumstances, like when physics applies (physics of an isolated variable like throwing a ball up in a system with gravity, and the physics of group decisions differing from a standard averaage decision, where gravity/forces and lack of restrictions both exert a tempering influence on the motion as with a lack of restrictions, trends tend to gravitate toward the average with minor deviations from it in either direction)

    - what percent of changes are just from finding efficiencies, using those as a foundation for common distortion types (random change, directed change, connecting change, etc)


    - algorithms

      - when sorting or finding, use vertices & probability to split the list to traverse - a list of length 20 can have vertices at 3 - 5 different points where the value is likely to be within one traversal with x% likelihood - this is related to minimum information to solve

        - 'minimum information to detect pattern' is another key metric - how many nodes do you need to check in a list of length n before you can be reasonably certain if there's a function generating it or if that function applies within subsets or other structures?


    - do concepts like cause emerge logically from a system where interactions (through forces creating adjacence) & time (state changes are persistent & stable till next iteration of time unit) are possible

    - how does a topology of a number's attribute sets (related complex numbers, adjacent functions/relevant formulas, core generating objects, factors, etc) collapse to a number?

    - are there gaps in an incomplete topology depicting numbers, as dictated by physics rules?

    - what system of core objects/functions/attributes generate a space where:
      - circles & squares are fundamental or standard objects
      - there is a continuous spectrum of values (real numbers) around which alternate number types rotate (complex numbers, etc)
      - comparing change generated by two variables (one independent value function determining the dependent value) has patterns of measurement potential
      - isolating by attributes (like isolating direction & scale to transform to a vector space) or framing information in different structures (sets, matrixes, sequences) allows patterns that are calculatable (implying the framing filter is determining, so matrix attributes can be determined by its definition)

    - what objects describe lack of information like ambiguities or lost information, other than randomness (difficult to identify randomness), variance (lack of patterns), and infinities (lack of information being difficulty of computing the sequence except in terms of other infinities if it doesnt converge, or lack of guarantees that the sequence can be maintained/stabilized to continue)

    - do components that can be used to build real numbers map to conceptual network (complex numbers), just like structural-conceptual objects have clear mappings, or are complex numbers more like perspective facets of a number? which types of alternate numbers are missing from the type set given combinable attributes?


  - whats after time-based change assessment? interface, variance/potential, default-based change assessment

  - whats after system derivation & management? predicting systems from a boundary, the core input (boundary/limit management) - the goal is filter management, as filters can generate other objects

  - whats after humiliation/criticism & problem avoidance markets? achievement, solution automation, and solution-sharing markets

  - map core concepts to structures for use when building other concepts/objects

    - similar definition structures apply to similarly abstract concepts

    - change types (rule types like binding rules, boundary rules) can be used to generate different variations of a concept
      - power can be in the form of a connection hub (binding rules with an extreme value of the connection count attribute)

    - position is an important attribute in many abstract concepts (powerful position, power occupying the input position, position as an attribute of equivalence)
      - other core structural attributes can be used to generate the other concepts

    - concept definitions are useful so you can answer questions like:
      - is this system balanced/robust to change/have distributed power
      - what potential does this system have
      - what priorities does this system have
      - can this system impact many others (is it powerful)
      - does this system produce a conceptual attribute/object like trust/equality
      - what concepts (determining, generative, emergent, prioritized, optimized) do these systems have in common

      without translating them into structure yourself

    - power:
      - core structural factors
        - order
        - sequence

      - core definitions
        - input
        - cause
        - possible

      - core functions
        - enable
        - fulfill
        - change
        - force
        - cause

      - types

        - potential (freedom in options)

          - change power
            - can change many things
            - can transform to many things
            - can reverse/distort many types of change
            - can use many alternatives

          - cause power
            - can cause/create many things

          - interaction power
            - binding/limiting/filtering/coordination potential

        - structural
          - change rules (binding/boundary/interaction rules)
          - dimension set (determining which value/change types are possible)
          - connection (connection count, changeability, origin/destination/path potential as measurements of power)
          - set (powerful as a set)
          - intersecting (power from compounding/aligning attributes/objects/functions)
          - adjacent (power from interaction potential)

        - conditional power
          - powerful with conditions/context

        - possible power
          - stored power
          - formatted power

        - illusory power (appearance of power, from missing information)

      - attributes
        - abstraction/irrelevance/independence (context doesnt reduce/change power)
        - commonness (repetition can increase power)

      - related object definitions
        - option
        - alternative
        - input

      - interface definitions
        - the different definitions of 'power' types should be mappable with this attribute set
          - system:
            - powerful attribute sets
            - power in terms of system metadata like position (occupies powerful resource set or role in system like hub or filter)
          - change:
            - powerful regardless of applied change types/rates/patterns/definitions
          - intent:
            - executes a core power function intent (intent of change is optimize/correct)
          - function:
            - powerful in function/function metadata (enables a core operation like find/apply/build)
          - potential:
            - powerful in potential
          - cause
            - causative position

      - visualization
        - power indicates potential which may map to a lack of information (lack of filters/limits) or powerful information (common input, common activator)


    - truth:
      - core structural factors
        - match (does it match a trend/relationship)
        - fits (does it fit within the host system)
        - potential (is it possible/likely)

      - core definitions
        - importance
        - relevance
        - possible
        - certainty

      - core functions
        - calculate/estimate
        - predict/guess/theorize
        - measure/check
        - distort
        - map/derive (find connecting path)

      - types
        - logical
          - valid
          - factual
          - consistent
          - cohesive
        - structural
          - connection/path
          - origin/destination (given a particular definition of change & a value for that change type, how does motion create the object or emerge from the object) 
          - set (true within or as a set)
          - intersecting (overlaps with a truth at measured points)
          - adjacent (one transform away from a truth in the form of a shift operation)
        - subset (partial truth)
          - approximate
          - functional
          - alternative
          - interchangeable
        - distorted truth
        - conditional truth
          - true with conditions/context
        - possible truth
          - missing contradictory information
        - illusory truth (appearance of truth, from false similarities)

      - attributes
        - simplicity
        - curvature
        - clarity (is it structurable)
        - repetition (does it keep showing up or is there one example)
        - patterns (does it follow patterns of truth)
        - explanatory
        - knowability/determinability (is it possible to prove or measure, more than once)

      - related object definitions
        - distortion

      - interface definitions
        - the different definitions of 'truth' types should be mappable with this attribute set
          - system:
            - true within an attribute set
            - true in terms of system metadata like position (occupies same resource set in system or same role) or context (emergent output will be the same)
          - change:
            - true regardless of applied change types/rates/patterns/definitions
          - intent:
            - constructible with valid/true intents
          - function:
            - true in function/function metadata
          - potential:
            - true in potential
          - cause
            - true in causation/causal position

      - visualization
        - truth has structure, bc it indicates potential that has collapsed to information

    - equivalence: 

      - core structural factors:
        - position of determining points (for a line this is either endpoint)
        - distance/length/scalar
        - unit object/attributes/functions (what is the standard form, what core operations does it support)
        - potential field (what positions can it occupy with adjacent transforms)
        - angle of change
        - space/dimension set

      - core definitions
        - equal
          - all attributes/state/values match, irrelevant to the path to the object
        - similar

      - types
        - structural
          - origin/destination (given a particular definition of change & a value for that change type, how does motion create the object or emerge from the object) 
          - shape
          - path (same pattern between origin/destination)
          - set (same set of generative/output/determining objects or interface objects)
          - intersecting (y = f(x))
          - adjacent (one transform away in the form of a shift operation)
          - dimension (variable set, space definitions/conditions)
        - subset equivalence
          - approximate
          - functional
          - alternative
          - interchangeable
        - processed equivalence
          - duplicate
          - version
          - iteration
          - combination
          - standardized equivalence
        - conditional equivalence
          - equal with conditions/context
        - type equivalence
        - matching
          - opposite matching structures, opposite being an adjacent object to the original, and matching object being a fit of an object, indicating an opposite transform
        - symmetric
          - transformable/reversible transforms

      - attributes
        - degree of equivalence
        - conditions/filters
        - definitions

      - related object definitions
        - change
        - value
        - distance
        - position
        - scale
        - unit
        - angle
        - degree
        - space

      - interface definitions
        - the different definitions of 'equivalence' types should be mappable with this attribute set
          - system:
            - equal on an attribute set
              - differences
              - possibility/probability/related input/output/path probability distributions
              - emergent output
              - required inputs
              - types
            - equal in system position (occupies same resource set in system or same role) or context (emergent output will be the same)
          - change:
            - equal in change type/rate/pattern/definition
          - intent:
            - equal in granular intent fulfilled/neutralized
            - equal in output priority direction
          - function:
            - equal in function/function metadata
          - potential:
            - equal in range of potential

      - visualization
        - visualizing equivalence across definitions & types:
          - if you can standard objects to attribute sets, you can visualize as a graph of attribute sets where shapes map to attribute sets and visible or highlighted shape attributes are equivalent
          - you need to incorporate objects like conditions & definitions as system/space attributes

  - after identification functions

    - import rules for selecting interfaces to solve a problem on

      Function helped find unused functions
      Intent helped predict system priorities & find exploit opportunities
      System helped find efficiencies
      Pattern helped find insight paths/similarities

    - once you build function/attribute identification function
      - import insight history data to identify insight paths 
        - info insight paths like 'lie => joke => distortion => insight'
        - system insight paths like 'three core functions + combine function with this definition + n distortions to nearest hub'

    - make tutorial for interface analysis or at least reading list of posts & docs


  - extra tasks

    - add precomputing if a sub-pattern was already computed:
               'ALL_N ALL_N of ALL_N ALL_N'
         'ALL_N ALL_N ALL_N of ALL_N ALL_N ALL_N'

  - neural network nodes as facets of additional complexity/dimensions - calculating the complexity of a prediction function-determining problem (based on which distortions are likely from prediction function patterns) would allow selection of the necessary nodes
    - higher complexity requires that more combinations of weight path patterns be experimented on, and more nodes allow room for more experimentation
    - auto-configuring the network with common or data pattern-specific weight path patterns rather than random or equal could speed up training
    - reverse logic can be beneficial here - given the complexity of a problem, which features with which differences would have to occur for the training to be useful (or for the problem to be solvable)? check the data for those variable types/differences
    - use prior knowledge of patterns (insights like 'differentiating variables tend to cluster') as a way to organize analysis of the corresponding feature data pointed to by the insight (adjacent features are passed in to higher weighted node sets or node sets equipped to handle subtler differences)
    - different network or node sets can represent different problem-solving automation insights (one node can represent a 'differentiating variables' filter, another node can represent a 'cluster' verb/function filter)
      - if none of these node sets finds a pattern, the node sets can be recombined into new options (like another outer layer of a core function diagram) and re-trained
    - adjacent networks are created for stacked variables (like symmetry stacks, where features differ on a symmetry and symmetries are layered (like the hand-limb-spine symmetry stack)
    - phase shift points are identified first & the data is standardized around them - so when one type has extreme/compounding attribute values emerging as another type, that threshold is built in to the data (data near to that threshold is transformed to be higher to differentiate the types, or you add a third output category like 'transforming' or 'interim type')
    - possible causal shapes are identified first & the network is organized to fit them
      - for example, weight paths trained to highlight one attribute set are applied to other alternative equivalent attribute sets (an alternate causal route to the output)

    - what is the ratio of coverage of all possible feature interactions that is fulfilled by a standard neural network? what does the standard architecture reward in terms of clustering attribute sets for experimentation?
      - it rewards attribute sets that appear correlated in the data, with generalizations applied

    - should you use position as a determinant of feature importance? should position be removed from the data & another network trained on position-removed data?

    - would it be better to frame features in terms of system analysis (attributes, functions & routes)
      - an ear definition route can be framed as:
        - take other dimensions of change (than vision or taste) & assume methods to detect them (sound), then design a system (ear) based on efficiency as a priority to detect that change
        - stack a symmetry on top of the spine to make calculations & choose priorities, then stack another symmetry (face) on that symmetry (head) to host multiple change-detecting methods (facial features) to guide calculations
      - once you frame features in this way, training to find prediction functions should be trivial

    - can you partially reset the neural network mid-training to help improve generalization in addition to existing methods, so it doesnt tend too far in the data-dependence direction?
      - identify features that are likely to be data-specific, given whether the categories share that feature with different attribute values

    - training prediction functions on system data rather than data for objects within the system would add other gains
    ` - by knowing the structure of a system, you can infer insights like:
        - which object shapes are most compatible with the system
        - which forces/interactions are likely to evolve in the system
        - system metadata (stability, priorities, potential)
        - system info objects (assumptions, inputs, efficiencies, incentives, phase shifts, ambiguities, core functions, boundaries)
      - for the dog vs. cat classification, training on causal system data (the evolution system, the bio system, or the DNA system) might be a better target for the prediction function than training on images of the outputs of those systems
      - this analysis would help predict ambiguities (dogs & cats will have very similar features sometimes given how DNA & evolution works) and how to differentiate them (check for specific attribute sets in data, otherwise indicate that data is insufficient but here's the data you should gather for this problem type)
      - you might also be able to identify a sub-system that has the most valuable data for this prediction (mutation sub-system as a sub-system with a distortion function applied to the DNA system)


  - prediction model trained on conversations as encryption key/alg parameters, updated with new messages

  - how to check if a data set is similar to one that has already been trained, to avoid re-training to save CPU

    - store metadata about the data set like shape (groups/clusters, linear, random) and the metadata for those shapes (radius & overlap for clusters, distortion patterns & outliers for linear, starting point for random)

    - derive info metadata like type, cause, change patterns & check if determining/generative structures (core functions, symmetries, false similarities) match across the two data sets

    - identify patterns of variation once a similarity has been found, to avoid checking the whole data set
      - example: once you identify that both data sets have two output categories, what are the patterns of difference in the internal points of those categories (patterns in overlaps, misidentifications, corrupt/incomplete data, differentiating variables) - usually youre applying a categorization model to two categories that are very different (so its important to identify them correctly) but have some illusory similarities or features in common, making the categorization task non-trivial - so you'd look for patterns of differentiation within categories, to check if the data sets are approximately equivalent so you can use the same model without training - either specifically, storing patterns of differentiation for specific categories, in the model metadata, or generally, for general category differentiation patterns

    - store adjacent functions or specify a parameter range generated from the original function to identify functions that can be generated with accessible transforms or functions that are usually generated for similar variable sets (using common function patterns), to identify similarity in parameter values (using various types of parameters, at various layers on the causal stack, such as preceding functions like the function producing this function as its derivative, alternate functions like the series sum, or descriptive parameters like moment-generating functions)

    - identify function/data vertices, which are determining points like maxima/minima/inflection points as well as the minimum number of points necessary to identify the momentum of the curve, or points that indicate phase shifts in general

    - store semi-trained models and use them as vectors to create a complete trained model, without training


  - multiple servers/processors in one computer with one-way data transfers, so one server can be for local communication, one can be for offline work, one can be for browsing internet, and local/offline can communicate to internet-browing processor but not the other way around

  - rules-to-code translation tool - translating domain-specific plain language rules to robot code can be short-term useful for automation of service industry tasks like:
    - converting recipes/flavor-mixing strategies to cooking robot code (chefs can use a tool like this to make money short-term or sell their rules, if they have unique strategies)
    - converting new plant designs to genome editing code
    - converting local social insights to global code (avoid personalities like this, use these tactics to persuade, make this argument to get them to an insight position, etc)
    - converting adaptation insights to change-attracting system adaptation code
    - converting routing mechanisms/optimizations to drone code (short-term human insights like 'avoiding a particular street bc of construction' that data isnt adequate for)
    - the general task of converting rule sets (systems) or human-made visuals (graphs, blueprints) to code

    - machine learning can be used for initial conversion, then tweaked with coded filters like priorities, logic, organization, output
    - system analysis can be used to optimize beyond those standard filters
    - this needs to identify existing rules (or specific versions of abstract rules, distorted versions of standard rules) & filter them out
    - this is an alternative & and an interim step to raw code-generation given a set of intents

  - data viz can be automated using:
    - lie core function layer graph or individual lie type graphs, with an output intent layer (hide information, layer information, minimize information, obfuscate information)
    - intent-structure maps (this graph structure serves this intent stack, just like a function serves an intent stack)

  - each superposition contains components representing different possible filters for the physical laws they create at scale
    - some superpositions collapse into a particular attribute set
    - superpositions with different configurations may represent other interface queries or structures
    - knowing the internal structure of a superposition would mean we get to choose which queries come to life & become real
    - the design implies we shouldnt get to choose - but external forces (or unmeasurable/uncomputable forces inside the universe) should get to determine which configurations collapse & which differences are allowed
    - information has a lifecycle - its likelier to become more true the more its observed, up to a maximum - then it's likelier to erode as its depended on
    - observing a state (to produce the information of the observation) may initialize the static nature of that information, so other observers see either static information or lack of it depending on their perspective, as information becomes truer the more its observed, and they may focus on the lack of information or a different perspective than the initial state of the information

  - make AI models/graph databases of theoretical physicists opinions so I can suggest these concepts to them

  - search ideas:
      - inferring useful search filters based on customer usage history & intent
        - linked searches/user data with type/intent identification - if they are in a location with a certain pathogen and they search for cleaners, theyre probably trying to clean that pathogen so cleaners should be specific or at least an optional search results set should be linked to
      - automated attribute extraction/addition to search as a filter
      - search results as graphs: variables entered in search to display relationships found in data or graph images or graphetized articles
      - processed (aggregated) results - find the average/combined or plain language definition when searching for a definition
      - predicting what questions theyll ask next and adding those search results (or a summary) on the side
      - intent-based search guidance:
        - usually people who search for an answer are studying for a test, so additional widgets like suggested content could include snapshots of/links to: 'study guides', 'summaries', 'tutorials'
        - people searching for recipes are hosting a party & cooking other things, so suggested content could include snapshots of/links to: 'flavor graphs'
        - people searching for symptoms are trying to diagnose themselves or someone else, so suggested content could include links to diagnostic tools or graphs of symptom set frequency for conditions

  - shared custom meaning/dictionary maps so communication can be queries on their shared custom dictionary map - or a common map where queries specify pattern & sub-set to apply pattern to, and sub-sets are rotated

  - abstract functions

      - derive combinations & make sure you have full function coverage of all important combinations

          operations = ['find', 'get', 'update', 'apply', 'build', 'combine', 'connect', 'convert', 'balance', 'map', 'match', 'fit', 'filter', 'derive']
          objects = ['strategies', 'questions', 'incentives', 'efficiencies', 'metadata', 'definitions']
          structures = ['paths', 'limits', 'boundaries', 'bonds', 'gaps', 'layers']
          system_objects = ['attributes', 'objects', 'systems', 'sub_systems', 'types', 'functions']

        - check codebase function index for combinations
        - check that you have sample data in json for each combination

      - attribute/object/function match functions
      - specific interface identification function
      - standardization network-framing function to describe a system as a network (the standard structure) & position each object, identifying connecting functions
      - system analysis function (identify boundaries, gaps, limits, layers, incentives/intents/questions, & other system objects)
      - isolation function, representating function/attribute changes independent of system context with respect to position or time (snapshot/state or subset)
      - function to define (isolate an object/concept/function for identification, identify definition routes)


  - give example of each type of problem-solving workflows

    - workflow 1:

      - finish function to determine relevance filter ('functions', 'required') from a problem_step ('find incentives') for a problem definition, to modify problem_steps with extra functions/attributes ('change_position') to be more specific to the problem definition ('find_incentives_to_change_position') for problem_steps involving 'incentives', so you know to use the function_name to modify the problem step if it's between the type 'functions' and the object searched for 'incentives'

      - finish function to get all codebase functions & store them in a dict with their type, context/usage, and intents, just like functions are stored in the problem_metadata.json example for workflow 1
      - finish common sense check
      - finish defining objects in object_schema.json
      - finish organizing functions.json by type, with mapping between general intent functions like 'find' to specific info-relevant terms like 'get'
      - add common phrase check & filter problem steps by repeated combinations with common phrase check
      - finish get_type function to map info to structure using the new functions.json organization
      - finish apply_solution to problem_definition using problem_steps
        - involves building a function to evenly distribute objects (like information/types), given problem positions/agents/objects
      
  - types can be represented as directions (going farther from origin goes further up type stack, where similar types are adjacent)

  - need to fill in content:
    - finish intent/change type calculation for a system intent
    - selecting optimal combination interfaces to start from when solving problems 
      (how many degrees away from core functions, specific layers or sub-systems, what position on causal structures)
    - key questions to filter attention/info-gathering/solution
    - key functions to solve common problem types
    - development of key decision metrics (bias towards more measurable/different metrics rather than the right metric)
    - trajectory between core & important objects
      - example of choosing inefficiencies/exploit combinations in a system
    - research implementing your solution type (constructing structures (made of boundary/filter/resource sets) to produce substances like antibodies, using bio system stressors)
    - emergent combinations of core functions (include derivation of invalidating contexts for core functions)

  - change phases for causal analysis (interim, changing, diverging, standard, efficient state, constant, interacting, converging, on the verge of obsolescence, outlier, etc)
    - superficial cause, alternate cause in the case of a function, addressing input/output causes
  - framing on interfaces, decomposing causation, then identifying parameters of problem on layer & matching solution
  - independence (closed trade loops) as time storage
  - vertex as a pivot point for an interface



- notes

    - if something can generate a change predictably/consistently, it's a change supply - otherwise it's a change request, so output as well as causal position relative to the output is important when determining category
      - time may be a variance gap (a space where change is possible) to resolve a question/problem set - so not resolving it can preserve time, unless resolving it will allow for more potential or moving on to other variance gaps
