# to do

  - finish processes:
      
      - finish applying systematization of solution automation
      
      - finish interface analysis of physics & other interfaces to identify other useful components like efficiencies, incentives, trade-offs, closed systems
      
      - finish config
        - add useful structures & questions from index.md to systematize_solution_automation.md
        - useful structures
            - identify filters for useful structures like definition routes
            - the system structure format where the maximum number of interface queries can be executed structurally, with minimal conversions required? is it a merged format of variable/function/concept/cause network graphs, or system state networks, or a set of variable subset graphs, or differences visualized as vectors, or input-output sequence visualizations, or a network with all identifiable interface components visualized
            - interface queries optimizing finding useful interface component filters
            - useful perspectives/specific interfaces
              - useful to think of prediction functions as generative functions to select the variable interactions that are most likely
            - useful solution filters to apply in functions
            - aligning/balancing structures, to solve problems like 'a balance position of structures producing errors when unbalanced'
            - questions formatted as a disconnection between components like causal positions, paths, directions
            - subset indexes of an interface useful for solving most problems (structure indexed by metadata like problems solvable, fitting systems, interactive structures, supported intents)
            - ml structures with supported intents & solution success causes
            - most valuable interface queries & workflows
              - find the sets of differences/dependencies/formats/errors & other useful structures that are the most valuable in a particular structure like a sequence to solve a problem
                    - interface component definition routes
            - useful component/sub-structures of interface queries (interface components, interaction rules, cross-interface interactions, generative functions)
            - useful interface components (like abstract) of useful interface components
              - core interaction functions of core interaction functions
          - creating useful structures
            - organize automating useful structures like combinations of concepts such as "format sequence", "solution automation workflow", "insight path", "reverse-engineer solution from problem requirements or opposite structures", "connect problem & solution"
            - convert structural queries to insight paths
              - alignments present in security innovations (like alignment in inputs like keys)
              - source of rule development as structures of conflict between forced interactions like change causes & constant structures like limits
                - incomplete inevitability of interaction as a decision structure
              - group device history authentication: authenticate credit card by proximity to cell phone & continuity applied to user usage history pattern
            - functionalize insight paths & integrate functions in optimized program with parameters to select function subset & structure for input problem
        - default config
          - write some default interface queries to use until logic is written
      
      - finish scripts
        - create compilation script to compile code/config into a network graph on every change
          - add support for standardizing equivalent synonyms
            - add conversion to standard vocab

  - integrate logic
      - integrate objects/.md text with interface implementations
      - integrate archive_notes/finder_info/functions
      - organize interface analysis logic definitions
        - organize functions in problem/interface definitions, before organizing functions in implementations/*
      - integrate problem_solving_matching.md
      - integrate find/apply/build/derive logic from system_analysis/ & maps/defs.json
      - separate interface analysis logic into implementation/functions (functions dont need unique info)
      - add functions from workflows & analysis (to do list, questions answered, problems solved, interface definition & functions) as files in functions/ folder
        - organize into primary core functions & list sample parameters (like objects to identify for the identify function)
      - integrate rules from diagrams in patent applications to relevant documents
      - organize function logic (interface query design logic)
        - document default static config objects that are inputs to core objects (like functions & concepts)
          - core functions like 'change', with locked objects which should be generated as inputs to other functions and should not be removed bc they enable other rules & core objects
            - a 'check for errors' function
            - a concept of 'self-correction/optimization'
          - these locked objects can be used to generate rule-generating/deriving/finding structures, by forming an initial structure of locked objects and filling that structure with conditional & changeable structures
            - these rule-generating/deriving/finding structures can be used as solution automation workflows
        - design an optimal sorting structure for general interface queries to apply to problems manually
        - list interface selection (based on inputs like available APIs/data sets/definitions)
        - problem interface structures: solution constraints/metrics, problem space variables, available functions, useful formats/structures
        - function to translate interface query logic into interface language (combination of core functions (find/build) & other core components)
        - function-usage-intent::output or demand::supply combination/merging/building/matching functions (alternatively formatted as a solution-finding query for a problem or lack-resource matching function) as an alternative solution to ads
        - decision points (required/optional resolution of variables to constants, as in selecting a variable value)
          - identify when a method & data set can be identifyd to be capable of deriving the answer to a prediction function problem
        - alternative intent coordination & compatability of metrics
          - calculating interactivity by coordinating/adjacent/convertible structures
        - check reduced language components for any other useful functions (what terms cant be adjacently, clearly & accurately framed in terms youve defined) for completeness

  - integrate examples

      - index examples so they can be queried more structurally when implementing functions

      - move examples from:
        
        - drinkme/examples_from_faq.md
          - check other examples of high-value use cases (other than identifying important concepts) from faq:
            - identifying the important base to frame changes on (identifying new interfaces)
            - identifying the right interaction level to focus on (identifying the change-maximizing layer of a system to examine a particular relationship)
            - identifying the right perspective to filter with (like 'identifying whether the legal/scientific/progressive perspective is most useful for an intent')
            - identifying the right context/position for an object (derive context when it's missing or fit an object to a system)
            - identifying the most causative function set (like identifying core functions, or the most misused functionss, or the most change-causing functions)
            - identifying important differentiating types (like function types indexed by intent & structure types, like boundary/change functions)        
        
        - patent implementation_examples
          - identify any examples missing from patents in docs/tasks once examples are organized

        - specific examples from specific_problem_analysis
          - example of permuting assumption: "reports of power consumption have to be exact measurements" 
            - a temperature monitor sensitive to a hundredth of a degree might provide similar but non-specific power reporting for important/extreme usage patterns without revealing such specific information as that which could infer exact operations being done, bc the interval of temperature measurements allows for greater variation in calculations that could explain it
          - example of using set theory in query operations:
            - edges as core organizing/formatting operations (find/apply) & interfaces (connecting/explanatory concepts/functions)
              - https://en.wikipedia.org/wiki/Hypergraph
          - example of structural version of solution difference from original solution: 
              - this is like using a pair of connected lines at different angles to connect two points (multiplying alternate multiplier pairs to create a product), where summing the line lengths produces an equivalence, so different solutions would look like differently angled triangles connecting the two points
                - https://www.popularmechanics.com/science/math/a30152083/solve-quadratic-equations
          - examples of identifying vertex variables
              - general vertex variables: topic, origin/destination, reason/cause/point/intent, errors, variables, types
              - comedy vertex variables: sincerity, stupidity, stakes, tension-resolution/expectation-subverting pattern variation
              - music vertex variables: tone, tension-resolution/expectation-subverting pattern variation, lyrics
              - optimization metric vertex variables: solution metric patterns (what other solutions optimize for, to identify optimization metrics to apply)
          - example of resolving a conflict between structure/limits using a structural similarity between a structure (gradient of function) & its container/limits (gradient of constraints)
              - https://en.wikipedia.org/wiki/Lagrange_multiplier
              - also an example of a solution space (the whole function is the solution space of possible minima/maxima) and a filter applied to it (constraint)


## examples

  - why problems will always have new variables determining how solution automation workflows can be generated
    - problems are a 'lack of available structures that can fix the problematic structures', requiring new structures to be found, built, or derived to solve those problems
    - by definition, problems will always change from previous problems
    - therefore problems are a structural source of new change types
    - new change types map to new variables describing those change types
    - new variables may not be adjacently buildable, derivable, or findable with existing structures, leading to a requirement to find/build/derive new structures to structure those new problem variables
    - this may lead to new values of existing variables like new problem-solving intents or new problem/solution core interaction functions, or other new structures that may be representable as new variables, bc they exist on another interface where problems & solutions interact, such as new variables regarding a new function type that can be used for problem/solution interactions
    - even if a problem is static, there will always be new ways to solve it because of the inherent variation captured in the concept of 'problem', and these solution-finding methods will always have new interfaces or other structures which can be used to frame them
    - there will always be new structures that are adjacent to available tools to fulfill interaction functions (like 'connect') between given new change types generating new interaction functions, and new interaction levels to reduce a system to in terms of finding core components of the system, new structures of errors from new change types, and possibly new primary interfaces created by new interactive change types and new useful structures to connect other new structures

  - why understanding is usually a better option than guessing methods like prediction function-finding functions
    - understanding can be used more effectively to derive a 'guessing method' than the opposite
    - understanding is more generally applicable & can fulfill more functions than finding/deriving/generating a particular 'guessing method'
    - understanding is less dependent on data such as context data
    - understanding has compounding value, where guessing methods usually offer temporary & otherwise conditional value while & where the original prediction function is accurate
    - understanding can find/derive/generate error structures of a particular guessing method
    - understanding is better at guessing than most guessing methods in most cases (except where a guessing method handles new contradictory information, a new problem type, a new change type, etc better than existing understanding)
    - understanding, learning, simulation, organization, prediction, testing are interchangeable functions with varying advantages but generally can replace each other

  - add to tech
    - 'download/generate & delete app' process for each use as a preventative measure against malware that checks for an app thats installed before exploiting a vuln in the app
    - add concept of 'value overflow to adjacent nodes', 'local conditional deactivation', and 'local feedback' to nn
    - depending on propagation/routing function, a neural network node can contribute multiple features (influences) (in deactivating multiple weight paths) on the final set of generated coefficient sets for each iteration during training
    - examine other weight update functions, like those having an oscillating sequence that converges around a value rather than those having incremental updates in one direction
    - related problem: as weight updates occur, predict the remaining weight updates (what is the likelihood that weight updates follow one pattern over another or end at one value over another) so that some weight updates can be skipped
      - applying 'find prediction function' to various inputs/outputs in a particular solution-finding method like neural networks for the general 'find prediction function' problem format can be useful in some positions
    - add example of a 'generate variants' or 'generate possibilities' function by applying variables to a particular structure, to generate variations of functions including workflows, queries, or problem-solving functions like 'find structure in a structure'
      - 'structure x' can be replaced by 'alternative structures' like 'find generative functions of x' or 'find alternate routes to x or its outputs' or 'find generative functions of requirements & other inputs of x' or 'find variables of x and alternate variants of x' or 'find interchangeables of x' or 'find invalidating structures of x' with varying degrees of success according to various solution metrics
    - identify the 'reason why something works' (solution success cause) formatted as the 'structure of change applied to inputs/outputs that enables a later output to be optimized by some metric' and 'why the structure of change enables that or does so better than alternatives'
    - which format is optimal to standardize to when implementing function
      - using 'vectors' to represent everything require a function to map non-numerical values to vectors, which will normally either lose information or encode it inefficiently, storing more information than necessary, and not storing it in a way that can be compared with other numericalized non-numerical values in a meaningful way by default, without referring to the original non-numerical variable values
      - using 'inputs/outputs' to represent everything is more likely to avoid losing information, but does represent everything as a sequence structure, where sometimes sequences arent implicitly described, such as in a set, which would require a description in the form of a 'generative sequence' or an artifical order imposed on the sequence

  - add to science
    - 'metformin and a low carb diet' reduce 'serum vitamin b' which can be used as inputs to 'cancerous growth'
    - check how folding relates to growth patterns like exponential growth, learning & information storage, and related structures like waves
    - prioritize 'scouting nanobots' to clean out chemicals as a prevention measure
    - structures of invalidity like 'hiding health data avoids the solution to the health problem, which would invalidate the reason to hide the health data' and 'funding a prevention measure rather than a cure, which would invalidate the reason to fund prevention measures'
    - general relativity: does gravity exert limits on the change types/rates possible so that time cant pass as quickly/slowly depending on gravity, enforcing a ratio of similarity between states to conserve energy in higher gravity environments?
      - how do you direct gravity to restrict time in one position so it can occur in another
      - how to manage risk of 'matter-forming cascades' using technologies to form matter that can restrict time in some positions from gravitational effects
      - what changes occur if you decrease gravity of black holes like by positioning other black holes so they can exert gravity on the other
      - re-define time as 'change potential' so its clearer whether an increase in time means 'slower change, meaning more space-times or more opportunities for change' or whether an increase in time means 'faster change bc time is moving faster' or 'faster change of change types as in new change types' or 'relative change rate/types' or as 'unresolved uncertainties' or 'calculatabilities' as a measure of what hasnt been calculated or isnt calculatable in a space time given its relative change rate compared to other space times that will solve it faster and prevent change in other space times
      - whether 'time' has an ending, or a high probability of an ending, as determined by the configuration of space-time distortions in this universe, indicating theres no way to ultimately guarantee the preservation of time, as defined in the form of gravitational variations that ensure a space-time can slow down enough to be measured or distorted by high-potential or high-change sources like humans, or whether the current configuration guarantees that black holes will continue sucking up time until there's none left, or whether the current configuration has a high probability that change types allowed by lower-gravity areas will inevitably intersect in a way that creates a change type that will invalidate conditions necessary for space & time
        - what ratio of these scenarios are likely, possible, and involve risks or guarantees?
      - how time in one universe interacts with time in another - whether constants created by gravity and variables created by lack of it can interact with other constants/variables in adjacent, aggregate/net, reflective, or otherwise interactive universes
      - whether the similarities acting as inputs to standard gravity have corrollaries in the similarities acting as inputs to quantum gravity
      - what gravitational impact on time exists on the quantum scale - the farther away from standard scales a particle or other structure is, gravity applies less and time moves faster?
      - does matter (order) act like an opposite structure of time (disorder) so destroying matter can create time in a particular position
      - can you navigate to other space times by increasing entropy to the entropy levels adjacently surrounding the target space-time (after which any matter navigating there reduces entropy to the target space-time)
      - does that mean you have to destroy matter (order) in order to power time-travel - would that invalidate motion in certain time-directions, as increasing entropy tends to cascade and create irreversibilities
      - is there a possible energy configuration that can increase entropy with enough precision that it can reach the target level
      - are constants like irreversibilities more similar to the definition of order or is the initial lowest-entropy state of matter closer to the definition of order
      - how do different structures that can create order (time crystals, black holes, other semi-closed or independent systems) interact - are they interchangeable in any way
    - uninhibited cell division
      - a process of applying regular damage of cell types successfully handled by immune system, rotating between locations & systems, to make sure the immune system is being sent to these components on a regular basis
      - alternate methods of increasing blood flow/circulation to every cell & distributing energy in the form of heat through exercise/sweat by increased connectivity/circulation
      - alternate methods of increasing cell replacement rate to prevent proliferation of dna mutations
      - rotate periods of inflammation, immune activation, scar formation, cell lifecycle triggering & stem cell differentiation, fasting/recovery, by triggering these processes in a way that avoids excess or compounding tissue like in fibrosis or excessive scar formation
      - forming or adding antigens on cells with sub-optimal configurations (like rb-binding, p53-deactivations or oncogene activation or insertion, or malignant transformations) as vaccine targets & to trigger creation of cells with correct configuration
      - 'aggressive regulator structures' as opponents to 'aggressive cell division structures', given that dna has built-in vulns that allow cancerous mutations to develop easily, what is the dna configuration that would aggressively prevent these mutations, and what is the cost of this being mis-applied to one in every x normal cells as opposed to one in every x cancer cells
      - 'variation-generating structures' to ensure a variety of dna configurations in a location in case cell division occurs so theres always a supply of active p53 copies nearby
      - directing fibrosis and other constraints to induce apoptosis in tumors
      - engineer nanobots to generate uv light & other p53/rb-activators where cell communication is hindered in ways that indicate cancer
      - activating genes or proteins inhibiting s-phase or death phase of cell cycle once the cell is malignantly transformed (is missing a 'protogen' as opposed to having an 'antigen', an 'opposite' vaccine target)
      - preventing degradation of pain or other immune signals by repeated exposure to triggers like heat
      - decreasing cell cycle timing in susceptible or vulnerable cells to reduce chance of mutation persistence after cell division

  - add to definitions
    - solution-finding method can mean a solution-finding method on any interaction level, such as a domain-specific problem space (like how 'regression' is a solution-finding method in the 'find a prediction function' problem space), or a problem-solution interaction level (like how a solution automation workflow or interface query is a solution-finding method), or interim interaction levels (like how a 'find connecting input/output sequence' is a solution-finding method on the function interface)
      - if the problem is (or can be) to 'find a solution-finding method', and can be applied to solve the original problem, these will also be called 'solutions' to that problem
    - pattern: similarity created by repetition
    - a 'problem' could be defined in various formats including:
      - difference between the initial problematic sub-optimal state and the target more optimal solution state
      - lack of functions preventing the problem or its causes, or lack of error-correcting or error-containing/isolating functions, or lack of variable-handling functions
    - where a 'solution' would be a structure that fulfills the solution/problem-solving requirements, and a 'solution-finding method' would be the structure that found/derived/generated the solution, optionally using the problem or problem attributes like problem type as an input (or using another solution, or the problem space as an input)

  - add to examples

      - write example implementations of trial & error (various sorting methods, identifying solution requirements, testing function, identify solution variables like position/state that generate additional solutions, identifying solution space given solution variables, various solution automation workflows & interface queries applied), as an example of translating solution automation workflow into code that can be applied to example problem definitions
        - include example of creating all possible variable values, variable combinations & other structures, and other interface variables such as assumptions/context

      - solution automation workflow metadata, like for trial & error:
        - assume the problem space is equal to the solution space (rather than a normal workflow, which might filter the problem space of all possibilities to identify the solution space containing possible/probable solutions)
        - required functions: identity possible solutions, iterate through possible solutions, test possible solution
        - involves fewest changes to the problem space or problem structure

      - whether a solution or solution-finding method like an interface query is formatted as a set of requirements/sub-queries/sub-problems, change/function sequences, state sequences, input/output sequences, filter/limit/boundary sequences, format sequences, added structures, variable/function networks, parameterized graph traversals, other interface structure sequences like pattern or causal sequences, or other structures depends on adjacent structures and the requirements of the solution automation workflow & interface query applied
        - similarly, whether a solution automation workflow is applied to various problem/solution structures - such as to connect the problem-space and the problem ('differentiate a problem space so that the problem is easily solved by generating adjacent required functions') or the problem & a solution ('convert the origin problem state into the target solution state') or a standard solution & an improved solution ('apply changes to an existing solution until its improved') or a general solution intent ('find a line connecting subset averages of this data set') and a specific solution implementing it (code implementing a particular definition of 'subset averages') - depends on the problem/solution structures required by that workflow

      - example interface query of formats, requirements, metrics, variables & filters applied to tetris problem/solution structures:
        - select solution format as 'set of states (moves & rotations) of a block', 'set of final positions of a block', 'set of possible value of other solutions not used (options invalidated or opportunities closed by a solution)', 'set of possible positions of all blocks once games is over'
        - for the 'set of final positions of a block' solution format, the solution space is:
          - 'set of available final positions that a block can be fit to (in general rather than for a specific block)'
          - apply filter by 'specifying values of variables' (such as which block is referred to, in which position) to the above solution space
            - 'set of available final positions that a specific block (currently selected block, in its current motion trajectory & rotation) can be fit to'
            - optionally apply variables (moves and rotations) to expand or filter the solution space depending on what filters were previously applied
              - 'set of available final positions that the currently selected block can be fit to, once all possible changes (rotations and moves) are applied'
            - optionally apply filters in the form of solution metrics of other solution formats (impact on other future block positions rather than just the position of the current block)
              - 'set of available final positions that the currently selected block can be fit to, once all possible changes (rotations) are applied, and once impact on solution spaces of next incoming blocks is identified & optimized'

      - example of various implementations with 'user-submitted visit purpose' data and 'cellphone location' data, and requirement to "create an app to predict a user's wait time"
        - identify relevant intents to problem-solving intent to fulfill the solution requirement using the specific available input data
          - intents such as 'predict user wait times', 'import data', 'identify relevant data', 'predict end time of appointment', 'predict conditional average time taken by appointment', etc
        - apply function to find relevant data (out of existing available data, such as user location and user visit purpose) for intents relevant to the solution requirement 
          - 'location' data is relevant for importing 'actual appointment start/end times', 'user arrival times', as well as determining the 'order & number of people in the wait list' data, and may also be used for predicting 'appointment end time nearer to the end of the visit' data
          - 'visit purpose' data is relevant for finding 'static average appointment times' for a 'visit purpose type' or 'conditional average appointment times' data for various conditions like 'business' and 'hour of the day' which may be variables that change 'average appointment times'
          - 'audio' data could predict 'real-time updates to appointment end times'
          - these have differing accuracy for various intents: 'location' data is a proxy for wait list & appointment time data, whereas audio data could be a more exact representation, and location can exactly determine variables like arrival time
        - identify relevant structures to the solution requirement ('predict wait time') & determine variables of relevant structures
          - appointment times (predicted time before the appointment end, and actual time once known)
          - wait list
          - alternate relevant structures could include:
            - 'appointment complexity or urgency' which would mean the assigned staff should be considered unavailable for x period of time or even closing the office and routing users elsewhere
          - determine variables of the relevant structures to the solution requirement, like how 'business', 'hour of the day', 'scheduled appointments', 'urgent purpose visit', 'canceled appointments' may be relevant to the wait list as well as appointment times
          - apply variables of the wait list & appointment times to generate a set of possible/probable cases, and filter by whether available data can detect those cases (detect a chatty patient or a higher ratio of urgent cases)
        - determine variables of the solution requirement structures (user's wait time) (how many people in wait list, available staff, predicted wait times of preceding appointments, probability of more urgent cases arriving)
        - determine differences between variables of relevant structures of solution requirement (wait list, appointment times) and the solution requirement structure itself (user wait time)
          - which 'wait list' variables ('business', 'hour of the day', 'scheduled appointments', 'urgent purpose visit', 'canceled appointments') and 'appointment time' ('purpose visit', 'business', 'hour of the day') variables can change or connect to 'user wait time' variables (such as 'position in wait list', 'urgency of purpose visit', 'arrival time')
        - different implementation methods
          - predict any relevant interim variables like 'appointment durations' and 'wait list changes' as a way of finding inputs to predicting the original variable 'user wait time'
            - predict using conditional/average predictions of appointment times or sequential-data neural networks trained on location or wait list change data, based on prior data patterns
          - calculate exact wait list, arrival, start & end times of appointments and predict the biggest uncertainty (probable appointment duration) as inputs to a user's wait time
          - predict highest variation variables like 'purpose visit complexity' as an input to relevant interim structures like 'appointment duration' and plug into a standard calculation of wait time based on arrival/start times & wait list attributes
          - select one variable like 'audio' or 'location' data and apply sequential-data neural networks
          - find most explanatory features of as many variables as there are variables available & integrate them into a prediction function
            - 'co-occurrence' or 'adjacence' of variables like 'high urgency user location adjacent to another high urgency user' indicating a higher wait time for users after them
            - integrate with derived user routing information such as 'user prioritization' to determine what changes to location/audio/visit purpose will impact user prioritization (and position in wait list)
          - other various implementations using different inputs/outputs (predict original solution requirement variable from available data or data derived from available data)
            - use existing solution-finding method (regression, machine learning, etc) and apply "different input/output variables" to prediction function
              - 'given existing wait list & purpose visit data of other users, predict user wait time'
              - 'given location patterns of other users before current user, predict current user wait time'
              - 'given arrival & exit patterns of users in general, predict current user wait time'
              - 'given values of high-variation variables like purpose visit complexity & staff skill set & staff fatigue, predict user wait time'
            - use existing solutions and apply "different prediction function inputs/outputs" (predict changes to standard prediction function borrowed from other existing solution code base or model, based on differences in problem definitions & requirements & data)
              - 'identify problem type (queues, timing/duration predictions), generalize & apply generalized functions from existing solutions for that problem type to available data'
          - the above implementations require different functions to be found/derived/generated & applied in various interface queries (the above workflow requires a function to derive 'purpose visit complexity' from 'purpose visit', possibly using prior purpose visit & appointment time data)

  - add to useful structures
      - functions whose outputs can be their inputs, like iteratable functions, where the inputs/outputs are relevant to or are useful structures
        - where problem-solving structures or methods can be iterated, apply them iteratively where complexity isnt reduced by prior iterations
          - 'find solution-finding methods of solution-finding methods of solution-finding methods'
          - 'find structures in structures in structures'
          - 'approximate apaproximations of approximations'
          - 'predict prediction function of prediction function inputs'
          - 'alternate alternatives of alternatives'
      - apply sources of ambiguity as possible interfaces and variables
        - if an ambiguity is maintained, the measurement/metrics of the ambiguous alternatives are wrong, or the assumption that they were similar alternatives is wrong, or the assumption that a selection should be made is wrong, or the assumption that either is correct (as opposed to neither) is wrong, or the question being asked about their similarities/differences in metrics is wrong, or the optimal alternative is a combination of alternatives or different option from either alternative, or its a structure of randomness or equivalence, or change types are involved which are not measurable or which dont have a measurable impact in that system/interface, or it can support enough change types of a unique conceptual foundation that it qualifies as an interface
        - if it doesnt resolve, it can be a set of interchangeables forming an interaction layer, which can act as an interface, like how 'variables' & 'state' are both structures of change that would seem like ambiguous alternatives under the wrong measurements
      - some specific functions like 'learn (as in update/improve on a metric)' as opposed to general function (like 'change') are more useful than others, bc of their alignment with & relevance to problem-solving intents like 'improve an existing solution'
        - existing solutions always exist or can be adjacently derived, such as 'try every combination or possibility', and these can be used as a standard or base solution to apply improvements to
        - 'updating to improve a metric' can take the form of 'moving/changing a structure toward a solution position/structure' having a 'state sequence'
        - there may be multiple solutions in the space in which case determining the most adjacent solution, ambiguous alternative solutions, solutions for different optimization metrics, and the most optimal solution for the original metric are relevant sub-queries, and only framing it in terms of 'connecting two states' is not useful as it will miss the other possibly optimal states to aim for
        - 'build a learning function' is more useful than a specific 'learn a function or pattern' function with data & a metric to optimize as its inputs, but requires interface analysis to identify concepts like 'deactivation' & structures like 'alternate coefficient sets', 'incremental changes', 'accuracy feedback', and a 'stopping point' as useful structures for a learning function
        - a function fulfilling a problem-solving intent like 'improve an existing solution' would ideally have multiple implementations for various fundamental solution automation workflows, workflows like 'connect problem/solution states' or 'apply solution structures to the original problem structure' or 'apply differences to the problem or errors' or 'apply differences to sub-optimal solutions' or 'generate solution from problem space or solution space components' or 'apply filters to the solution space' or 'apply useful structures to the problem'
          - the 'improve function on a metric' function would have:
            - a 'connect problem/solution states' implementation, where it would start with the data or a standard solution and connect it to a more optimal solution, such as through iterative state updates
            - a 'build solution out of components' implementation, where it would start by identifying available components in the problem space or required components of the solution and aim at building those & then combining them in a way that fulfills the original solution structure
            - a 'apply solution structures to the problem structure' implementation, where it would start by identifying solution structures such as a data set point that should intersect with the prediction function or a solution metric to fulfill like a level of accuracy to aim above, and then applying those to the problem structure, which might be a standard sub-optimal solution or the data set
            - a 'apply filters to the solution space' implementation, where it would start with the solution space, which might be a set of functions that intersect with or represent at least some subset of the data set, and iteratively filter out solutions based on a solution metric
          - these various implementations would allow the solution to be found in various situations like different available inputs
      - solution (prediction function) metadata should include attributes like:
        - which data points from the original data set are prioritized by the solution given their better represented in the function, and if other points are prioritized, how much does the solution change, and why are those points prioritized
        - which data points will fail with this solution and by how much
        - which representative examples of the original data set will succeed/fail 
        - why this solution was selected having been constructed this way, given the infinite equally accurate functions available (to use a standard representation metric, to generalize across other data sets, to retain a percentage of original data set information, to account for other latent variables)
        - how much randomness and how much variation in how many alternate data sets this solution will handle
        - in what thresholds in what cases this solution will become clearly sub-optimal, and which solutions will be optimal after those thresholds are reached (format of a 'network of alternate conditional functions')
        - how likely it is that there is an alternate solution that is interchangeable in terms of robustness across some variation in different data sets, similar fulfillment of solution metrics, or similar function metadata like a similar generative/descriptive function
        - how much & what quality of the original data set information is retained (quality of information like its average)
        - why variable interactions are justified & used in the solution
          - example: if two variables are usually around 1.67 times the other (like an animal's height & width), why would that be the case:
            - useful functions for the animal like 'speed and balance when moving' could be inferred from that data set if variable label information is retained and has access to a scientific/language dictionary
            - structural concepts like 'optimal ratios' would be noticed across various problems with different data sets once it was repeated enough, which could be identified through a repeated pattern or derived through filtering out alternatives like 1:1 or 1:2 ratios, which are less common because it's uncommon to find integer relationships in nature, it's less likely that one change unit of each variable type is likely to be optimal, and it's less likely that an excess of one is optimal for a unit of the other, given how related variables in a type or interface have some similarity given their common or related causes such as genes & functions, or derived through useful structures like 'comparable but not equal quantities' often found in 'related variables with a trade-off structure while an uncertainty in the form of an optimal ratio is being resolved'
      - filters of solution automation workflow implementations (interface queries) are particularly useful bc there are many ways to implement trial & error ('try every possible solution'), even though it would seem like there is one way to implement it, there are other variables of the implementation, such as the order in which every possible solution is tried, as well as how to identify what is a solution (a combination of some items in a set, or an item in a set, or a sequence of items in a set, etc) and the full set of the solution space, and not every interface query or solution automation workflow that the query implements will produce the same answer for these variables, although some of the answers are requirements by definition rather than variables, like how applying 'trial & error' to find the right block in tetris would have a 'correct' solution space of the 'set of available positions that a block can be fit to (in general rather than for a specific block)'
      - identify absolute rules as useful structures in the form of reliable functions to fulfill their original intents by those rules which apply absolutely in all cases, such as functions whose output cant be changed from a solution to an error or sub-optimality for differences in inputs
      - differentiative definitions of error structures like those determining sub-optimality, neutral structure & error structure
      - error-correcting structure requirements for error structures such as:
        - 'using only internal components of the structure in error to generate a solution to that error' for an error that occurs in one position
        - 'removing structures added with no or mismatched intent alignment for structures in error having no associated intent'
          - example: in various 'pooling' functions, one option may be selected at random (for no reason other than to generalize) when there could be a reason to select one option above another (it fulfills an intent like 'increasing solution accuracy' or 'reduces mismatch error between intended and actual functions')

  - add to solution automation workflows

    - map interface structures to structure of the problem space system to apply useful structures like patterns of interface structures to the problem space system structures
      - example: for the 'find a prediction function' problem, this would take the form of identifying the impact of a particular change type in the system of variables that are inputs to the prediction function, thereby mapping interface structures like a change type to structures in the problem space system such as 'coefficient value changes', so that useful structures like patterns of that change type can be applied to calculate coefficient value changes, as opposed to a method like 'create a set of sets of independent variables, representing functions of input variables, (like a set of layers of neural net nodes) and test if their impact on a prediction function is useful'

      - other useful mappings from interface structures to problem space system structures include:
        - identifying if a particular probable interface structure (like a 'processing function' that is hypothesized to be applied to the inputs at some point in the problem space system) would produce a particular change associated with a function or function network applied to some function of input variables of a data set, to derive which functions should be present in the neural network to implement or filter out that interface structure
        - this connects the solution-finding method with interface structures of the problem space system, so a theory of which functions of interface structures determine the problem/solution are probable can be tested by changing a solution-finding method's structure to test that theory

        - identifying where effects of nodes' functions have effects like 'neutralization' to identify relevant structures to the solution-finding method's changes (the network's parameter (weight) changes), like 'deactivation probability of a node' given that structures of 'neutralization' would by definition not change the outputs of a network, and would therefore be relevant for processes like 'node deactivation', so 'neutralization' structures are relevant to the solution-finding method's changes so 'neutralization' structures can be considered an input to a function that changes (improves) a solution-finding method's parameters

        - identifying structures like 'weight update functions' that enable or create other useful structures like 'skipping training iterations'

        - identifying which combination of optimal standard functions would be useful to start filtering the solution space & derive the neural network for that function combination as the initial first iteration output of the final dense layers (which structures of functions of inputs such as 'functions of functions of inputs' would produce the changes to inputs necessary to create those optimal standard functions)

    - find/derive/generate specific structural structures (like structural sequences) that can be used to solve a problem and apply them as inputs to an alternate solution-weighting function or as parallel processes to find a solution function first or as inputs to another problem-solving intent like 'generating probable standard solutions'
      - example: for the 'find a prediction function' problem, specific structural sequences include the following, which are input-output sequences of structures that can generate a prediction function
        - non-adjacent subset connection functions + function weighting scheme = prediction function
        - adjacent subset functions + smoothing function = prediction function
        - functions having various average definitions + function weighting scheme = prediction function
        - common function components (peaks, inflection points, extremes) + adjacent transforms to fit the function = prediction function
        - standard solution function + transforms to fit the function = prediction function
      - these 'specific structural sequences' are useful as default components of interface queries to fulfill a solution automation workflow involving structural similarities in required structures such as 'connecting states' or 'finding input-output (connection, interactive) sequences', where the 'specific structural sequences' involve structures that are efficiently resolved in a problem space ('common function components' or 'smoothing functions' being more structurally defined than the 'find a prediction function' problem, thereby adding value in these specific structural definitions)
      - these 'specific structural sequences' might be the output of an interface query, thereby building interface queries in reverse, given interactive/connective structures in a specific problem space, so that interface queries can alternate between these structures ('iterate through & apply these input-output sequences') that could be generated by other queries such as 'find interactive/connective functions that can connect a data set and a function'
      - these 'specific structural sequences' are nonetheless still abstract in terms of the specific functions required to implement them, allowing variation in their implementation, so they can act like sub-interface queries ('apply these input-output sequences to find inputs to the function-weighting scheme'), interim interface queries ('apply these input-output sequences where relevant such as on-demand' or 'skip to these input-output sequences rather than finding alternative input-output sequences' or 'apply these input-output sequences as placeholders for an interface query to be executed when other input-output sequences are found to be sub-optimal for a problem'), or alternate interface queries ('apply these input-output sequences in parallel to see which finds a solution faster')

    - apply useful structures like 'alternate input-output sequences' to various problem/solution structures across interface queries & workflows & apply them in relevant workflows/queries/problems (relevant in being specific to the same standard problem format like the 'find a prediction function' problem format)
      - example: for the 'find a prediction function' problem, this would take the form of 'finding alternate input-output sequences for possible functions required by various interface queries', like how the specific sub-function 'find relevant connections between points' is used by various solution automation workflows implemented with various interface queries that solve the 'find a prediction function' problem in a particular way involving connecting data set subsets such as adjacent pairs or high-priority pairs (such as adjacent subset averages or high-density averages), so alternate functions such as 'clustering methods' that also require these specific sub-functions may be relevant to the 'find a prediction function' problem bc they have a similar input/output (data set input, connection function output), and applying 'clustering methods' to a data set can identify data points that can be connected using clustering functions like 'connect to the nearest points having the most adjacent points', where these 'subset connections' can be used to build a prediction function, just like how 'anomaly detection methods' can be applied to a data set to identify 'non-standard connections' which is a useful structure to the 'find a prediction function' problem

    - apply anti-error structures like a 'requirement of decisions/selections impacting the solution' where decisions ('selection' structures, acting like 'solution filters or specifiers') are relevant to solving the problem (such as where the solution format requires it or its required by a process required to generate the solution) to avoid an interim error structure such as 'false certainty in making a selection resolving an ambiguity' when applied to problem/solution structures (such as when choosing between alternative solutions randomly, rather than for a reason), and apply variables to the solution where selections are not required, to avoid other error structures caused by not avoiding that error structure which would impact the solution structure, like 'over-specifying a solution' or 'over-solving for a constant value that isnt required to be a constant'
      - example: for the 'find a prediction function' problem, this would involve applying variables to structures where optimization is ambiguous (such as regions where a function can vary without changing the optimization of the solution metric) to parameterize solutions restricted to a particular variable value
        - when applying 'trial and error', a decision about whether to 'sort the options first before applying the test of the solution metric attribute' or 'find all solutions or an optimal solution, rather than the first successful one' is not required in the standard problem format of 'find an item having this attribute in this set of items'

    - apply change structures to relevant structures for a particular problem format or problem type to identify other structures these relevant structures can be useful in for the original problem
      - example: for the 'find a prediction function' problem, structures of 'representation' are useful in finding a function that is 'representative' of the data set and can therefore be used for prediction/approximation
        - applying change structures to 'structures of representation' can generate alternate methods of finding a prediction function, such as 'representative structures' like:
          - 'connecting points by finding direction of greatest number of points to navigate in' (similar to clustering algorithms), which prioritizes connections & points that are in higher-density regions of the data set
          - 'averages of dense regions' indicating higher-priority data that should be more representative of the data set as a whole
        - these 'representative structures' can be used to identify high priority points to use as input to the 'find a prediction function' problem or to connect or otherwise interact in such a way that they are used to build the prediction function (like connecting various adjacent high-priority points to form subset functions and then weighting or connecting these subset functions)
        - to find/derive/build 'representative structures' like 'averages of dense data point subsets' or 'connecting prioritized points to form subset functions & then connecting subset functions', other representation structures like 'average' can be applied to problem space structures like 'data points' and 'sets of data points', and known calculations of weight/priority of a point can be analyzed to reveal the input data points that result in a higher weight (those in high-density regions or those representing an average of adjacent surrounding data points) in order to determine attributes of data points that should be prioritized, or identifying the subset of points that could adjacently generate a prediction function in other known example data set/prediction function pairs

    - find useful structures such as 'patterns' of 'differences between multiple alternate solutions' so those differences can be applied to the origin problem state where possible, to maximize chances of finding a solution
      - example: for the 'find a prediction function' problem, patterns of differences between multiple optimal solutions might look like adjacent functions forming an 'area of optimality', where for the 'find a local minima' problem, patterns of differences between multiple optimal solutions might look like 'separations by upward curves of a polynomial', indicating that optimal minimum points are likely to be separated by a parabola/peak or similar structure, where these difference patterns can be applied to an origin problem state like a data set or standard solution (linear regression function) or a random point on a 3-d graph or a standard solution (adjacently computable local minimum)

    - apply alternative structures (like differences from variables, randomness, opposites) to change problem/solution structures (like a specific solution-finding method for a specific problem format) in a way that optimizes a general solution metric (like 'reducing number of required steps/functions' or 'adding information') 
      - example: for the 'find a prediction function' problem, this would take the form of finding 'probability of a data point in this determining area of a function' or 'probability of a data point within range of determining point x of a function' instead of 'determining points of a function', applying interface structures like 'probability' and 'surrounding area of a point' and 'generative functions' rather than the structure of an 'exact point', as a more optimal structure for determining whether an area is 'representative' of a 'determining point of a function', applying 'find generative function of x' as an alternative to 'find x' to optimize a solution metric like 'number of steps required' to generate a different workflow than 'find determining points of a function', therefore 'finding inputs, alternatives, or approximations of determining points rather than the determining points themselves'
        - apply 'mix' interface structure to other solution-finding methods to find structures that add different information, such as 'function coefficients' and 'determining points':
          - generated different workflow: 'find determining points of a function and predict a subset or all of those from the data set, instead of or in coordination with predicting the function coefficients', 

    - find structures such as 'representative' structures that are useful for the problem-solving intent of 'finding a solution balancing various solution variables once multiple probable solutions are found' (solution variables like optimization, determined by solution metric fulfillment)
      - example: for the 'find a prediction function' problem, this would take the form of finding an 'average function' or 'base function' of probable functions that can be solutions, to represent the probable alternate solutions based on a weighting schema, or to represent the 'probable versions' of a function once some parameter change is applied to the 'solution base function'

    - find the relevant structures (like the 'representative' structures such as averages, representative examples like important data points, important variables, counterexamples like outliers, etc) for a particular problem given alternate definitions of that problem ('find a prediction function' as a 'find a representative function' problem)
        - these relevant structures give a different format to aim for when solving the problem, such as finding 'representative examples' as inputs to a 'find a prediction function' problem rather than the original input of the entire data set, 'representative examples of the data set' being relevant because the problem is to find a 'representative function of the data set'
        - workflow fit: this applies 'alternate' structures to the 'problem format' to find 'alternate problem formats'
        - generalization: this can be generalized to apply 'alternate' structures to other problem/solution structures to find 'alternate routes' to fulfill a problem-solving intent or solution automation workflow

    - derive useful structures that can act like alternatives to interfaces, such as structures including objects of a particular interface structure type (like functions, variables, vectors, data sets, or positions)
        - where problems are standardized to 'find a set of transforms to convert one vector/matrix into another', given interaction rules of the problem space system (such as available operations & interaction levels), a useful structure would be the format of a space where a vector is transformed into another vector, where points represent operations applied to an adjacent vector as specified by the interaction rules, so that solution automation workflows can be applied, such as 'finding alternate solution vectors similar to the original solution vector to aim for instead'
          - this space can function like a 'base network' similar to how a language map is used as a base network for solving nlp problems
          - these 'base structures' can act like sub-interfaces and can be useful as alternate structures to interface structures or other useful structures which can be used as 'standardized problem spaces' to convert a problem to, as opposed to converting a problem to an interface
          - the 'base structures' dont need to be comprehensive to be useful - examples of the problem system interactions to specify a vector at a point relative to another vector may be sufficient for solving some problems

    - find structures where functions can be applied to adjacently create the solution, such as 'solutions fulfilling opposing filters' where a 'connect' or 'average' function can be applied to 'solutions fulfilling opposing filters' to find a more optimal solution between these opposites, where opposites represent error structures of varying extremes (extremely general/specific)
        - example: for the 'find a prediction function' problem, solution space filters such as solutions fulfilling the filters 'most intersections with actual data points (most specific function)' and 'most general function or generally representative function' allow those solutions to act like opposing filters, where the optimal solution is between these filter structures implemented as solution structures (a specific function and a general function)
        - the opposite structures represent an 'extreme' error structure, fulfilled by the abstraction attribute ('general/specific')
        - 'finding an average between values (such as extremes)' is a relatively simple function
        - other alternate functions would be 'finding a function that reduces extreme metric 1 (number of data point intersections) and extreme metric 2 (linearity) while still representing the data set to some degree'
        - this workflow finds a structure representing an error structure of the solution (such as opposite extremes of a solution attribute, such as abstraction) then finds structures (solution space filters like 'most intersections with data points') that could create those structures (solution functions having opposing extremes of abstraction) then applies problem-solving functions (connect) or other functions (average) to correct those errors, finding the solution from those sub-optimal solution structures that are adjacent to the solution

    - parameterizing the impact of solution automation workflows on problem/solution structures like the solution space can offset the need to apply the workflow itself, rather than applying its average or otherwise representative output (like its impact on the solution space or the changes applied to the problem origin state)
      - differences in reductions of the solution space created by various solution automation workflows can fulfill various problem-solving intents and help solve various problems more optimally than others
      - a 'trial & error' workflow doesnt reduce the solution space at all, whereas other solution automation workflows like 'derive possible/probable solutions or solution structures like solution limits/boundaries based on solution requirements' would likely reduce the solution space
      - the difference between these workflows' impact on the solution space encodes information about what assumptions are made which can reflect whether a solution space reduction is useful for a particular problem or problem-solving intent
      - example: for the 'find a prediction function' problem, a workflow might derive solution space-reducing filters such as 'a prediction function has to come this close to the data set average for each data subset of size n' or 'a prediction function has to be this similar to these data subset-connecting functions'
        - the resulting reducing impact on the solution space is different from the unfiltered solution space by some parameter set

    - find/derive/generate requirements of various required functions of the solution-finding process like calculating solution success (in the form of determining calculation possibilities such as whether one solution is more optimal than another), requirements such as 'being able to calculate & compare optimality of solutions', to identify useful structures that would invalidate these processes or make them easier/more difficult to execute (such as 'areas of error or solution ambiguity' on a graph, which if you can calculate can make reducing the solution space or finding an acceptable solution easier)
      - example: for a 'find a prediction function' problem, this would take the form of finding/deriving/generating the sections where it would & would not be possible to derive if a solution was better than another (structures of optimization and structures of ambiguity), deriving which of these sections of ambiguity is more optimal than others (a 'solution ambiguity' rather than a 'error ambiguity'), and aiming for a function that intersects with those solution sections (there may be areas or other structures like function bundles or adjacent parameterized function points on a graph where it is clear that a function is sub-optimal, clear that a function is successful, ambiguous whether a function is a solution or error, areas where it is clear that a function is a solution but ambiguously successful when compared to similar solution functions, etc)

    - apply changes to the attributes like position of a useful structure applied in a solution-finding method to implement the intent of the useful structure in its original position in a different way
      - example: for a 'find a prediction function' problem, this would take the form of applying changes in the filter types or change types applied to create 'maximally different' weight path patterns, rather than applying changes to weight path patterns themselves, where 'maximally different' weight path patterns are a useful structure applied to a neural network structure or coefficient sets to test various prediction functions or use them as a base to apply changes to, but applying the useful structure to the filters or change types used to create the maximal differences in weight path patterns can be a more optimal structure than maximizing the differences in the weight path patterns themselves
        - this uses 'maximal differences' and variation in change/filter types bc those structures are useful for reducing the solution space the most 
          - changing a standard base prediction function with one change type like 'adding an exponent to a linear function' creates more differences when compared to another change type applied to the standard function like 'adding gaps in the function', which would create two functions that are more easily filtered out because they are more different, whereas very similar functions may not only be difficult to filter out as they are an ambiguity, but they may also be equally or similarly optimal functions that both could be considered correct
      - generative interface query: the interface query to generate this workflow is 'apply alternate routes to the input-output sequence of the intent of the difference-maximizing changes/filters rather than to the input-output sequence of the weight paths (or their patterns) to identify different solution automation workflows to fulfill the problem-solving intent of "connect the problem & solution" than those applying useful structures like difference-maximizing structures' (changing the position to the input (the change/filter types) rather than to the structure theyre applied to (weight path patterns))

    - apply changes to problem/solution structures, derive functions required to use these problem/solution structures, and from those required functions derive a solution automation workflow fulfilling those function requirements
      - example: in finding different solution formats like 'determining prediction function points' or 'function limits acting like the prediction function', a new requirement for a function is generated like 'find determining prediction function points', from which a solution automation workflow can be derived like 'find a different solution format and find the structures required for that different solution format' which involves different operations than solving the original problem, therefore comprising a different solution automation workflow

    - identify structures (such as a number of significant or accessible structural similarities) required to apply structures from other systems to automatically find useful structures to apply as templates for problem-solving structures, filtering the set of structures produced by an 'input-output sequence' similarity with additional similarities or other structures that indicate enough relevance to apply the structure to other problems requiring the original structure, those similarities or other structures having compounding value in indicating relevance
      - example: for the 'find a prediction function' problem, this could take the form of 'finding similar structures in other systems like a "superposition" that can act like a template to create the same output structures such as attributes like "certainty" or functions like "uncertainty resolution"' to apply to standard structures in other systems, fitting standard structures like functions & activation states in the various structure of a superposition (either having multiple possible activation states of many possible functions, or having an uncertainty in a function's activation state that is resolved at a later time), where the inputs (atomic components), interim uncertainty structures (superposition of many possible states), and outputs (measured metric value) have a structural similarity to the relative dimensions of the neural network, so the similarity in uncertainty resolution & possible dimension sequences create enough similarities to justify applying this structure as a template for a neural network to solve the 'find a prediction function' problem converting a data set into a function parameter set (and the associated conversion of a data set example of independent variable values into the dependent variable value)

    - apply useful structures like 'input-output sequences' or 'causal sequences' to other solution automation workflow inputs, such as inputs or generative functions of 'determining points of the prediction function', to identify new problem/solution structures to aim for in implementing the original solution automation workflow
      - example: for the 'find a prediction function' problem, this could take the form of 'finding functions separating data subsets that act like tangents to the determining points of the prediction function'

    - find useful structures in structures like combinations of interface structures such as 'optimization functions' and useful concepts like 'complexity' (as an error structure), to find for example 'points optimizing for minimized error structures' which are relevant to problem-solving intents
      - example: for the 'find a prediction function' problem, this would take the form of identifying what structures of complexity or simplicity the data set is likely adjacent to, and at what structures of simplicity the data set is likely to converge or stabilize to, at what point in the future, given adjacent stable points and the probability of changing and/or converging in the direction of any of those points, given that stable systems tend to be simpler, so if a prediction function is going to be correct for a longer period of time, it will likely be simpler than other possible prediction functions
        - if structures of complexity have just occurred at the time the data was measured, such as if a system has collided with another system, the effects will either invalidate the system, leading to dissolution of the system into core components, core components which may include the variable interactions of the data set, or the system may stabilize in a different or its original state depending on how it sustains variance injections - if an optimal minimum of simplicity can be derived as adjacent, that simpler function may be the best prediction function until a change occurs in the system hosting those variables that doesnt result in system invalidation

    - apply problem-solving functions like core interaction functions fulfilling a problem-solving intent to various problem structures in a particular solution-finding method
        - example: for the 'find a prediction function' problem, apply 'reduce' function to various possible problem structures like 'more operations than necessary' in a regression method, such as 'finding a representative line of more points than necessary' by reducing the number of points that need to be represented (by finding representative points of point subsets or finding the minimum determining/differentiating points that should be connected or averages to create the prediction function), or 'finding a representation or optimized error metric calculation for a subset of points at regular intervals in the data set so these representation metrics can be connected instead of applying the representation or error metric calculation at every point', at which point 'finding the points to connect or average or represent' or 'finding the interval at which error metrics can be calculated to find an approximation of the prediction function' becomes the primary problem rather than 'finding a representative function of the data set that minimizes an error metric or represents a ratio of points', and if a solution is found by applying core interaction functions to solve these 'problem structures', they can be considered problem structures of a solution-finding method

    - identify interchangeable solution structures (like interchangeable solution-finding methods or interface queries) and their variables to generate other items that could also be interchangeable solution structures
      - example: for the 'find a prediction function' problem, this would take the form of identifying interchangeable solution-finding methods like various regression methods such as 'connecting dependent variable value averages between different point pairs' and 'finding the line that minimizes an error definition' (or identifying alternate structures to this set of interchangeable functions, such as interchangeable interface queries or other generative functions to generate those solution-finding methods) and identifying the variables to generate the interchangeable structures on that interaction level, given that interchangeable methods act like structures having the same interaction level and will likely have adjacent variables in common as they are likely to find/build/derive/apply the same structures
        - for the two example regression-implementing solution-finding methods indicated above, both use an error metric, the first one implicitly ('minimize differences in averages between point pairs so they can be connected in a line') and the second one explicitly, which could be derived as a variable of a solution-finding method - an error metric determining whether the output is accurate (fulfills a solution metric) - that could be used to find other solution-finding methods (generate other error metrics that could determine a solution metric of a prediction function, and reverse-engineer a solution-finding method in deriving/finding/generating a solution-finding method that uses that error metric)

    - some interface queries are more efficient than others in arriving at solutions, other things being equal, so deriving interface queries of solution automation workflows & optimal solutions and selecting the query with the fewest or least computation-intensive steps is a way to find generally useful interface queries, which are likely to result in a set of interface queries that make use of 'useful structures' like 'alternate input-output sequences', so identifying these optimal interface queries is likely to be another way to identify useful structures bc those are likely the reason for the efficiency of those queries

    - apply changes to different problem/solution structures like applying changes to create different solution formats to create different structures that can act as inputs to problem-solving structures like solution automation workflows
      - example: with the 'find a prediction function' problem:
        - different solution formats include a network of conditional prediction functions, representative averages for subsets of the function, function limit structures (like function range caps/boxes), a solution-finding method or generative function of prediction functions, a prediction function for inputs of the original prediction function, a prediction function range or parameterized function to produce functions in that range instead of the original solution format of one prediction function
        - different solution-finding methods include different parameters of regression, merging different subset functions, prioritizing representative data points, data subsets or function subsets to weight their contribution to the prediction function differently, neural networks, applying solution metric filters, applying solution metric filter-fulfilling structures as solution structures, applying adjacent structures like transforms or combinations of standard or base functions, moving subsets of a standard regression line to be more general and/or more accurate & smoothing the resulting set of linear functions, etc
      - this works to generate new solution automation workflows bc applying 'changes' by definition changes the structures involved, so if changes are applied to problem/solution structures in a way that doesnt invalidate their definitions, that method can generate new problem/solution structures that use those changed problem/solution structures as input

    - apply interface structures to solution metrics to derive structures useful in generating those solution metrics, and aim to fulfill those structures of solution metrics instead of the original solution metric
      - example: for the 'find a prediction function' problem, use structures derived from solution metrics like 'sets of allowed function subset ranges' or 'alternate sets of allowed function subset ranges' as filters of how much the solution can vary in a particular location (like a set of 'open-ended boxes' capping variation in a particular subset position of the data set) while still fulfilling the level of accuracy or other solution metric specified by the solution requirements, given that many functions will produce the same solution metric fulfillment if solution metrics arent specific enough, so any function within certain ranges will be considered a sufficient or optimal solution in those cases
        - to find out what structure would fulfill the solution metrics of 'representative accuracy' and 'representative coverage' of a prediction function, an interface query would identify that there are many possible solutions, then identify a structure that would place a limit on what solutions are allowed to fulfill the solution metrics while allowing variation in which solution is selected, which would result in either prediction function parameters to allow for variation in the prediction function that would have the same effect as a 'set of function subset range boxes', or the structure of the 'set of function subset range boxes' itself, boxes having a 'containing' or 'boundary' effect that aligns with the 'limited range' structure of the 'accuracy' solution metric requirement, and subsets of the function having different boxes limiting their range, allowing the boxes to fulfill the 'coverage' solution metric requirement

    - apply interface structures such as a 'change' to the perspective applied at various interaction levels & in various problem/solution structures in solving a problem to change the functions required to solve it
      - example: for the 'find a prediction function' problem, converting a standard 'find a regression line' problem into the same problem from a different angle such as the perspective of an agent standing at the first data point at the lowest x value and looking in the direction of the data point at the highest x value, at which point the problem becomes 'minimize the distance from the center formed by a line intersecting with the point, where the first point represents a point on a line formed by the sequence of y-value points', which is a different way of framing the 'reduce distance from the regression line to the data points' problem that results from a different perspective applied to the problem, changing the functions required to solve it - instead of finding a function for a line fitting the data the best, its finding a function for the point that minimizes distance from other points on the line of y-values, where the y-value line can represent weights of values with minimized distance from higher-weight points have a greater count in the original data set
        - finding a line minimizing vertical distance to points  vs. finding a point minimizing distance to other y-value points, which is a way of finding the average value in a set, which can be applied to adjacent subsets and the averages of these subsets connected or used as input to the regression method, thus making the problem easier to solve by reducing the number of points to find a prediction function for, as the prediction function-finding method may be more costly than a method to find the average in a subset
                                                         /.                  .
                                                      . / .      vs.         .
                                                     . /                     .
        - this method reduces the data set to subsets using some partitioning method, then applies a perspective that reduces variation to one-dimension (y), finds a representative metric for that dimension for that subset, then connects the averages as a standard regression line to base improvements on, or inputs the averages to a regression method, thus reducing the number of points to find a regression line fitting
        - the application of the perspective adds value in isolating a change type, which is useful for finding attributes of that change type such as averages, ranges, change rates, probability distributions, & other attributes
        - the perspective cant be used on its own bc it loses info about the other dimension, so it needs to be integrated back into the original solution format (prediction function for the whole data set) by finding a line connecting or fitting the average y-values of adjacent x-value subsets
      - workflow fit: this is similar to applying a perspective to find perspectives that immediately precede or otherwise usually lead to a solution, but generalizes the application of interface structures to the perspective before applying the perspectives across various problem/solution structures, like in the problem system, to find useful perspective changes and other interface structures applied to perspectives that fulfill a problem-solving intent like making the problem easier to solve

    - identify identifying metrics of possible error cases where a particular found/derived/generated solution would be sub-optimal and apply those case-identifying metrics as filters (in the form of error structures) of a solution to design a solution that doesnt fulfill any of the case-identifying metrics, as a way of fulfilling the 'avoid errors' problem-solving intent
      - example: for a regression method, the case where the data points have randomness or outliers is a case where a regression method such as using an average definition is sub-optimal bc it loses useful information about outliers in cases where they exist, so check if the case fulfilling the identifying metric of 'outliers' applies before designing a solution using the associated sub-optimal solution for that case (the regression method), or apply changes to that solution if that case if either known to be fulfilled or probable, or design a solution that is different from the associated sub-optimal solutions for these cases in ways that are useful for the original problem (rather than just any difference from the sub-optimal solution such as 'not using that average definition' if the identifying metric of 'outliers' is fulfilled, a useful difference such as 'using an average definition that integrates the outliers such as a weighted average', which integrates the outliers' information and avoids the error of 'losing information' that applies to the sub-optimal solution)
      - this avoids errors for worst-case scenarios in a way that applies useful methods of avoiding the error structure of the worst-case scenario, rather than a standard method of avoiding errors, using efficient identifying metrics of cases (the outliers being the identifiers of a 'worst-case scenario' for that particular solution method)
      - this identifies worst-case scenarios where a solution would break by the variables that the solution doesnt handle which are required to be handled, like extra info, where this info should be preserved in the solution format
      - extension: this could be extended to include a method to integrate solutions for various cases, like a weighting scheme for case-specific solutions, and a method to identify worst-case scenarios using variables that are not handled by a solution, such as extremity of outliers or number of outliers which are relevant to the original problem, which is aiming for a prediction function fulfilling accuracy metrics

    - apply interface structures like 'conditional network' or 'combination' to problem/solution structures like solution formats where useful
      - example: for the 'find a prediction function' problem, a function might be better represented as a mixed structure including a function and a set of points or conditions linking those points in a network, if some points would distort the function too much if incorporated into the function given some solution-finding method that handles outliers in a way that would distort the function beyond their relevance to the other points, or a parameterized function indicating some parameter preserving the original difference between the outlier & average
      - generalization: this applies a variable to the problem/solution structures & their attributes (like the number of solution formats being a variable instead of a constant of one)

    - identify useful changes & other interface structures (like 'self-reference' structures such as 'connect connections', 'patterns', and 'common' structures, or formatting a structure by its 'opposite' structures identifying a 'difference-resolution' structure as opposed to a 'connection' structure) to apply to problem/solution structures like function types such as core interaction functions to make these problem/solution structures more specific & useful for a problem
      - example: for the 'find a prediction function' problem, this could take the form of 'apply difference-resolution structures like maximally-different point connection functions or tension-resolution structures such as wave-generation functions or momentum-application functions to account for variation in data that follows normal or common patterns of difference-resolution (connection) to find a prediction function'
        - the 'difference-resolution' structures would connect the 'most different' points in the data set, to find difference types and the common structures (like an average) resolving those differences in the form of a connection between different points or a connection between connections between different points of varying difference types (difference types such as 'greatest distance on both variables' or "greatest difference in a variable once standardized on the other variable such as adjacent points' vertical differences"), fulfilling a 'connect' or a 'connect connections' core interaction function
        - the 'tension-resolution' structures would connect the data set points using common connection patterns, such as patterns of momentum (like the 'wave generation' tension-resolution pattern resolves the difference created in the wave-triggering event), thereby fulfilling the 'connect with common connections' core interaction function
      - 'connecting connections between connections' is useful because it finds similarities between connections, allowing the identification of a pattern or common attributes including common differences which can be variables of the connections
      - 'self-reference' structures are useful bc they apply the same definition repeatedly, and are useful in identifying structures relevant to the 'connect' function, such as similarities, patterns, & common structures in identifying meta-attributes (filters of filters, reductions of reductions, structures of structures, limits of limits, connections of connections)
      - these create structures that are specifically useful for the 'find a prediction function' problem in identifying useful functions that can re-use known concepts like 'connections' which are already defined for that problem space (as in 'line structures having distance, intersecting with points as endpoints') to identify useful info like the 'probable prediction function', useful functions such as 'connect connections' to create useful info like the 'average function' that is relevant to this problem
      - the useful functions to create this useful info can be derived by which functions could connect the 'connect' function with the 'average function' (a 'self-reference' structure, thereby identifying the specifically useful 'connect connections' function), a useful function derivation process that can be applied to other problems, and the specifically useful structure being re-usable with other problems to create specifically useful functions, as well as the useful structure used to generate it, and the identification of the 'average function' as a useful interim target being derivable from its adjacence to known solutions or the original problem's solution as a base standard, 
      - workflow fit: this workflow involves customizing problem/solution structures such as core interaction functions with useful structures like self-reference or opposites to create specifically useful structures for a problem

    - identify opposite structures like 'gaps' of useful problem/solution structures like 'variable handling coverage' created by available structures of the problem & known solutions, structures such as filters, where available filters represent what can be or is measured, so anything not covered by them is a source of possible error structures, as a way of identifying/finding/deriving new error structures, fulfilling the problem-solving intent of 'avoid errors'
      - example: for the 'find a prediction function' problem, filters include requirements and assumptions built-in to the solution-finding method such as the definition of the concept of average that is used, or the method applied to tune the function such as generalization, smoothing, simplifying, regularizing, normalizing, standardizing, or curving methods, and given these filters, other change types may not be applied to the function, such as an outlier not handled by the function that is nevertheless valid, or a break in continuity that is valid, because the assumptions inherent to applying a 'smoothing' or 'generalizing' method will ignore these points in the data set and prioritize creating a continuous function to represent or connect certain subsets of the data set, so the 'gap' in the available filters including a particular definition or a solution-finding method contains the alternate structures not creatable with that definition or method, as a discrete point such as a valid outlier is not creatable with a method that produces a continuous function that 'represents' the data set, unless it coincidentally intersects with that structure, which is relying on randomness rather than a reliable method of preventing those errors
      - the 'variable handling coverage' refers to structures that can be built/derived/found with a set of structures in the problem space or known solution set
      - 'variable handling coverage' is a useful structure in determining what is adjacently & non-adjacently possible with a set of structures, such as a set of known solutions
      - 'variable handling coverage' could also be applied to problems, in finding gaps between problem structures given variables of problems in that problem space, which are possible solutions

    - identify interface structures such as the change types required to find/derive/generate a useful structure for a particular problem format given its definition and apply those change types to find useful structures for a particular problem
      - example: for the 'find a prediction function' problem, useful structures include the function's parameters, its determining points & requirements, and generative functions of the function
        - to convert 'find a prediction function' into these useful structures, various change types are applied which can be applied in other problems to find useful structures
          - changing its format to include other components (exponent terms), converting it to a set of 'constants' applied to these standardized exponent terms
          - changing its format into its 'determining points & requirements'
          - changing the causal position by identifying 'generative functions'
      - workflow fit: an alternative to the 'apply known useful structures for a particular problem after standardizing to that problem'

    - derive interface queries of known solutions and identify other interface queries that could be applied to generate other solutions, where known solutions are inefficient, impractical, or sub-optimal in another way
      - example: for the 'find a prediction function' problem, the following interface queries can generate existing solutions
        - the 'parameter matrix' is generated by an interface query to 'find "structure" that can store "output" of applying "multiple" structure to "generative parameters" of the "function"'
        - the 'regression' solution is generated by an interface query to 'find "required" structures given the problem definition such as "representative line"' (leading to the concept of the 'average' as an input to a 'representation' structure)
        - 'find "subset" prediction functions and apply a "average" definition to them'
        - 'find "adjacence" structures (like adjacent y-values) and apply adjacent change structures found'

    - once useful structures are found/derived/generated/applied for a particular problem, apply other useful structures like change structures that are likely to create differences that could make useful structures like patterns in problem structures like values more obvious
      - example: for the 'find a prediction function' problem, a useful value structure would be a matrix of alternate parameters of the function if one exists (if there are parameter sets with equal number of parameters across different generative function types), so that other changes could be applied to this structure, to create other value structures to examine for connections (such as representing this matrix as a grid of scalar vectors mapping to each value in the parameter matrix)

    - find possible errors in the problem statement such as assumptions that could change the other problem/solution structures, such as solution formats
      - example: for the 'find a prediction function' problem:
        - find different parameterized lines (such as lines converted into a set of segments with different formatting, where the parameters have relevance structures like complimentary variance handling or an overlap structure across different parameter sets) that can generate the line
        - find parameters of a matrix representing different parameter sets of a function (descriptive parameters, compression parameters, generative neural network parameters, wave function parameters, regression parameters, etc) & how they relate to actual function parameters (coefficients, powers)
        - or find a function for an area/shape rather than a line to represent different possible averages and probabilities that each average definition or type should be used as the output's predictor where the area indicates probability of an output occurring between the various lines
        - or find a function that emphasizes/prioritizes data points likely to be near to the actual prediction function or standardizes points in a way that makes them closer to the actual prediction function or removes points that are not closer, to give a shape of an implied function by a data set with a more obvious pattern
        - all of which have a different solution output format that is a variable when the assumption that the output should be a line and nothing else is removed, or that one definition of a component should be used
      - workflow fit: this is similar to other workflows that change the source/destination but applies error structures to find possible variables to apply to problem/solution structures like the solution output format

    - find a space where the prediction function's parameters will be more clearly identifiable (such as by extreme values like minima, or by converging values, or by values that signal a change in change types like inflection points or changes in change rate), like how adding a dimension to a 2-d prediction function might allow identifying a minimum point in the 3-d space where the change types stabilize, which reveals where the peaks & other features of the function are located, so that the task becomes finding the original feature-differentiating minima or other structures in the 3-d space

    - identify connections to generate different solution formats and how to find the format most adjacent and useful for a particular problem
      - example: with the 'find a prediction function' problem, the prediction function can take multiple formats - a set of function parameters like coefficients, a set of interactions with other structures of different dimensions like the pattern of a straight line's intersecting points with a curved function, a set of change rates/types in a structure like a sequence, a set of neural network parameters that could find the prediction function parameters, an infinite sequence, a set of minimal points that could derive the rest of the function (like how a parabola can be derived from 3 - 5 points depending on their positions & other requirements like positive values, where these points can be found with 'subset function' averages & other representations, or in finding the core structures like angles or change types like fast/slow curved/linear change of the function and determining points that determine those core structures), and some of these formats are more amenable to being used with particular solution-finding methods

    - identify multiple solution-finding methods and apply components of one to the other in a way that is likely to be useful
      - example: applying the method used to 'generate different variants' of the function coefficients in neural networks is likely to be useful in an altered method of regression, which doesnt inherently find alternate variants of a function and merge them in a weighting method, bc regression uses the concept of the 'average' already and incorporating another version of the concept of 'average' is likely to be useful

    - find/derive/generate solutions having specific error structures (such as an incorrect assumption that the 'average of a data set is the correct average but is incorrect') as a way to quickly identify what is not the solution by applying these incorrect solutions as an initial non-solution space to use as a filter to find the correct solution
      - workflow fit: this is similar to finding the set of 'maximally different' solutions as an initial solution space to filter, with the exception that known error types are injected in these initial solutions, which are likelier to be not solutions but by definition non-solutions that have a known probable error, as a way to determine the actual solution by its difference from these known error non-solutions

    - apply useful structures like system layer graphs as a general structure that is useful for problem-solving, integrating new problem & information learned from them into the same structure, adding as few structures as possible in the form of new objects, connections, and levels, which are default structures with which most other structures can be framed, similar to a network graph
      - new structures like problems/systems may take new structures to accurately represent on a graph like the system layer graph, and while that is being identified, vacillating between an 'exploratory' state (to consider the possibility of new structures encountered in a problem/system) and a 'learning fitting' state (to integrate new learned info into the system), the system layer graph has an 'uncertainty structure' to capture the extra possible complexity (in the form of an interface structure network or system layer graph, containing more useful interface structures nearer to the origin)
      - derive useful structures by which structures reduce the most variation (as in 'capture the most complexity')
        - other useful structures for problem solving would include networks/trees, interfaces (and more specific input-output sequences & core components) as different structures to standardize representations of information in a way that is likely to be useful for problem-solving in general

    - identify which structures would enable finding relevant structures for a particular problem
      - example: for the 'find a prediction function' problem, this would take the form of 'predicting other variables from various sets of other variables in the variable set', because 'causal dependencies' are a particularly relevant structure for this problem format, and applying 'change' to the important structures of the problem space like 'variable' structures using various change types generated by interface analysis will help in identifying these relevant structures for solving the problem ('causal dependencies')
        - apply interface analysis to determine probabilities of dependence structures
          - apply the 'subset' structure to the 'variable set' structure
            - can one variable predict multiple other variables
            - which variables are necessary to predict the others
            - which variables can be removed without high impact on most of these subset predictions
          - apply the 'change position' structure to the 'variable positions' determined by 'variable type' 
            - can the dependent variable adjacently determine an independent variable
            - is one of the independent variables a possible output variable (multiple output variables)
          - apply the 'change position' and 'subset' structure changes
            - can the dependent variable adjacently determine a subset of independent variables
          - apply the 'sequence' structure
            - can the variables be framed in a sequence structure, where one set determines an interim set that determines an output set
          - apply filters of possible variable interactions based on:
            - probability: 
              - how many operations are necessary to convert one variable set into another, given that fewer operations implies a structure of truth in an optimized system
              - do variable interactions align on multiple interfaces
              - are variable interactions probable given patterns of other variable interactions
              - are variable interactions possible given known limits, requirements, and rules

    - apply solutions to a more complex problem than the problem like how predicting a value in an infinite sequence (or determining differences between infinite sequences or their functions) is more complex than predicting a dependent variable value in a polynomial function, so filtering a set of many (possibly infinitely many) different prediction functions can be solved with methods from a related but more complex problem

    - identify information that is an output of a solution that can be used to derive/find/generate the solution, rather than identifying info about the solution or solution inputs, to use as a different target when solving a problem
      - example: for the 'find a prediction function' problem, this would take the form of identifying solution output metrics such as proxy metrics to a prediction function (including area under curve, number of peaks, probability distribution of local averages, etc) that can be used to identify/determine/approximate the prediction function, and change the problem to predicting those function metrics instead of function parameters like constants/multipliers/exponents applied to variables

    - apply interface structures to problem space to determine other problem structures than those directly relevant to the original problem that can be used to infer solutions to the original problem
      - example: 
        - in the 'find a prediction function' problem, apply interface structures such as 'alternates' and 'adjacence' to problem space structures such as 'input variables', to infer adjacent variables of a data set, such as direct inputs & outputs of the original inputs, to find a set of connecting variables that can be further connected to the original outputs to predict, or to find out if the original inputs can be corrected in some way given these adjacent variables, or if adjacent variables to the original inputs can more easily be used to predict original outputs
        - another example is how 'cause' is a directly relevant structure to the 'find a prediction function' problem, which attempts to identify variable dependencies, dependencies being a causal structure
      - an alternate general variant of this would be to apply interface structures to identify structures like 'alternate' applied to general problem/solution structures, like 'alternate' function types (such as finding an alternate general function than find/derive/apply/generate) 

    - apply interface structures to determine what structures could produce a useful structure for solving a problem in a particular problem format (such as how a 'variable relationship' is useful for solving a problem formatted as a 'find a prediction function' problem) and whether those structures have structures of truth (such as whether they are interactive, exist, or are probable), after standardizing the original problem to a problem format (such as 'find a prediction function' problem) where such a problem structure (such as a 'variable relationship') could be used as a target to generate with interface structures
      - example: in the 'find a prediction function' problem, identify which structures could produce a 'exponential to integer or boolean' variable relationship, including such information as what degree of structures applied could connect the variable structures, and identify whether these structures are possible in a standard or specific system given how structures interact, such as a 'continuous input variable' being used as an 'activation signal' or 'type variable' in a standard system
      - another example is whether these structures can be connected using other interface structure connections, like whether the intent sequence of a structure sequence is possible/probable, thereby indicating with multi-interface alignment that the structural sequence is likelier to be true

    - identify & fulfill other problem-solving intents such as 'solve for relevant structures to solving the original problem' instead of 'solving the original problem'
      - standard problem formats (such as 'find a prediction function' or 'sort a sequence') where all problems can be solved with known methods once formatted to these standard problem formats are generally found with standard interface structures, such as how the 'prediction function' involves a standard function interface structure of a 'variable relationship' and how the 'sort a sequence' problem format involves a standard structural interface structure of a 'sequence', where these structures are 'relevant structures' to solving these problems
      - 'solving for relevant structures to solving a problem' is another problem-solving intent
      - other problem-solving intents can be identified by variables of problem-solving intents, such as which problem/solution structures & functions are involved, which interface structures are applied or fulfilled in the problem/solution structure interactions such as an input/output sequence between problem/solution structures that applies across various problem-solving intents, which problem/solution structures & formats are required by a problem-solving intent (such as a 'set of solutions'), and the degree to which & method in which the problem is solved by that intent

    - apply interface structures like concepts to a problem system and find useful structures like interaction structures of those applied interface structures as a way of finding structures that can be used to determine probability of solution success to apply as a filter of solutions
      - example: in the 'find a prediction function' problem, apply concept interface to find concepts like 'average', 'correlation' and 'causation', and apply useful structures to these applied interface structures to find structures of possible relevance as a solution filter like 'functions of data subsets that could represent correlations' and 'correlation functions that when averaged could produce the prediction function' and 'functions that would indicate causation vs. functions that would indicate correlation and filters of these function types', that can be used as 'prediction function' solution filters, given the possible meaning of these structures

    - apply structures of absolute certainty (like variables that can absolutely vary in all cases and requirements such as required variable limits or interactions) to use as starting point to reach structures of approximate or probable certainties
      - example: 
        - in the 'find a prediction function' problem, absolute certainties include 'some points may be due to randomness', 'some points may indicate change in the underlying variable interaction', etc, which indicate the absolute truth of a variable of the data set (randomness, change in interaction function) that may vary
      - from these absolute truths, approximate/probable truths connecting absolute truths that are otherwise unconnectible with absolute truths can be identified as more probable than other approximate/probable truths, having filtered out 'impossible truths' and 'improbable truths' implied or required by the 'absolute truths'

    - apply solutions that are known to be exactly incorrect/sub-optimal as a way of finding/deriving/generating errors to generate useful information to find correct/optimal solutions
      - example: if you applied a regression algorithm to a classification problem, you would identify a difference in the output regarding data type, alerting you to the fact that the data type is where the error is and is related to or is the component of the solution that needs to be changed

    - apply other useful structures than 'connections', 'sequences', or 'interactive components' applied to components of problem/solutions in fulfilling problem-solving intent functions, problem/solution core interaction functions, or other functions related to problem/solutions
      - example: rather than finding 'input-output sequences' to connect problem/solution formats/states or components available in the problem space, or finding 'interactive components' that can be chained together (starting from the 'core component' structure), apply other core structures to fulfill core interaction functions between problem/solution
        - finding structures that are by definition related to the 'connect' function, such as 'input-output sequences', are a useful structure by default in fulfilling/implementing that function, but other structures like 'sets' or 'combinations' can be usedul as well, indicating objects that are frequently or by definition found together, indicating an 'implicit' connection by adjacence/approximation rather than 'explicit' connection by match/equivalence in definition

    - apply useful structures (such as 'mix') to connect various solutions or solution-finding methods in a solution space, to useful structures (like 'components') to generate new solutions or solution-finding methods
      - example: for a problem with multiple generated/derived/found solutions or solution-finding methods like 'find a prediction function', apply the combination useful structure 'mix components' to switch various components of the solutions or solution-finding methods with alternate variants
        - for the 'find a prediction function' problem, this could take the form of switching error calculation methods or solution metrics
      - workflow fit: this is similar to the workflow of 'generating different solutions' or 'identifying variables of different solutions & generating solution variants with these variables', but is another way of achieving similar or equal intents using structures of useful structures

    - generating example solution-finding methods of varying specificity & results by applying various alternate solution automation workflows, then abstracting & parameterizing differences between examples and imputing missing differences that could generate new solution-finding methods without changing output of known examples
      - example: 
        - the difference between 'regression methods' lies in the 'error calculation metric & associated function'
        - the difference between 'regression' & 'finding prediction functions of subsets & merging their outputs by some weight assignment function' includes differences in the different percent of data input to each function, the difference in outputs of subset prediction functions, the difference-resolution/merging method of subset functions
        - these differences can be added as abstract variables like 'alternate input-output routes' & 'alternate inputs/outputs' or as specific variables like 'alternate data subset routes' & 'alternate data subsets'
        - these differences can be increased through combination with other interface structures than 'abstraction'
        - these identified variables can be used to calculate other methods not already identified in the variable interaction space
      - workflow fit: this workflow is similar to other workflows that apply 'alternate routes' to structures to find/derive/build a solution, with a specific focus on 'solution-finding methods' which can function as 'standalone interface queries' if they fit into a workflow that uses a 'standardization' step so standard solution-finding methods can be applied, or if the solution-finding methods are found/derived/generated in a specific problem space, so theyre already in a format that is relevant to the original problem

    - convert solutions from another format for an adjacent solution-finding method into a useful format for the original problem when applying another solution-finding method
      - example: for the 'find a prediction function' problem, apply solutions associated with 'classification' to solution-finding methods like 'regression'
        - meaning, for a given subset of the data set, use 'classification' to predict a value out of several discrete possible values, as calculated by various different averages of that subset of the data set, rather than finding the prediction function using standard regression methods for the entire data set
      - workflow fit: this is applying 'alternate routes' useful structure to the 'solution-finding method' solution structure, rather than the normal position of applying it to the 'problem/solution connecting functions' to find 'alternate problem/solution connecting functions'
      - interface query: the interface query for this workflow involves 'finding 'approximation structures' or 'generative structures' or 'prediction structures' of solutions' ('solution-finding methods'), rather than 'finding solutions', where the 'alternate' structure is applied to 'type' of 'solution-finding method' as well alternating the 'solution-finding method of solution-finding methods' (like 'a method to find regression-calculation functions') from the standard 'solution-finding method' (like a 'specific regression function to find the prediction function')
      - generalization: in general this method finds/derives/generates variables of the problem space (like 'alternate routes between inputs/outputs' such as: 'method of finding y from x' or 'method of finding all ys from all xs' or 'method of finding a subset of ys from a subset of xs' or 'method of finding ys from adjacent ys' and injects relevant useful structures (like how 'alternates' are useful for 'variables', as they both have a common structure of 'change')

  - add to problem/solution structures
    - definitions & other truth structures as symmetries

  - add to problem-solving intents
    - solve for relevant structures to solve a problem 

  - add to predictions
    - where you say that 'activity interacting with a neuron is relevant in its functionality': https://www.quantamagazine.org/how-computationally-complex-is-a-single-neuron-20210902/
    - relevance of intelligence (successful learning/thinking) & disease (unsuccessful learning/thinking/stressor-handling): https://www.quantamagazine.org/brain-cells-break-their-dna-to-learn-more-quickly-20210830/

  - add to science
    - 'sets of n particles' that act in aligned ways like:
      - 'components of a temporary local field/lattice having n points'
      - 'a connection created from equivalence structures' based on:
        - a 'homotopy equivalence between points'
        - a 'set of rotations/arrangements transforming points into a set of equivalent duplicates by definition-compliance, like the infinite cloning paradox solution'
    - 'ozone hole' was a specific structure that gave people one achievable goal to aim for so they could organize resources around achieving that goal
    - what ratio of gene copies is required to offset cancerous processes/responses without interfering with non-cancerous process
      - can junk dna be altered to fix sub-optimal ratios?
      - are some dna copies acting as 'placeholders' that prevent other copies from being made or prevent certain illnesses
      - how to 'create dependency' of cancer cells on a particular process/structure so when removed it can create cell death
      - drugs that response to 'high quantities of a signal' leave room for an error of 'missing low quantities' - an error where the signal is assigned a boolean and should be a continuous spectrum with a threshold that can re-create the original problem
      - how to target useful cells with growth-enhancing mechanisms like extra-chromosomal dna
      - are there genetic sequences that can offer protection against extra-chromosomal dna interactions temporarily while cancer cells are being targeted (like if a protective sequence is spaced at intervals that disincentivize interactions with extra-chromosomal dna, such as when extra-chromosomal dna merges with a chromosome)
      - how to account for probability of a known possible error (like mutations compared against a mutated rather than healthy reference genome) and adjust data accordingly
      - if a structure continues to develop, that means it represents a 'stability point' that is incentivized - which incentives of extra-chromosomal can be altered without harming other systems?
      - if a package of growth/maximizing genes continues to develop bc of 'survival bias' (the genes that are useful for survival are protected & maintained & replicated), can incentives for dna repair/defense genes be created/increased to compete with these cancerous extra-chromosomal gene structures - why is the 'survival bias' of cancerous cells & cancerous mutations stronger than other cells/genes/mutations' bias - just bc theyre more useful for cancerous intents like 'unrestricted growth' or bc of some other cause like 'genetic adjacence' or 'genetic functionality like jumping/coordination', or bc of the fact that growth factors promote themselves by the definition of their own functionality, or bc these gene/mutation types like enhancement/growth have aligning intents like 'promotion' so theyre often found together?
      - can antibodies against high-growth/maximizing extra-chromosomal gene structures be developed or can vaccines be developed to produce antibodies against them?
        - https://cen.acs.org/pharmaceuticals/oncology/curious-DNA-circles-make-treating/98/i40

  - add to automation tech
    - more devs converting to 'task description writers' creating standardized task descriptions & identifying repeated work, automation opportunities & other high-level semantic tasks is a better career once programming turns into a 'task bidding freelance market' where AI coders frequently outcompete devs
    - organized & structured code by 'core component' (function input/output variables, input/output differences, patterns, core function-calling function (high-level functions, so building core functions is the source of changes in functionality of high-level functions), uses, logical derivation connections (equivalence, requirements, dependencies) as logic trees, logically directed graphs, changes/states, function, reference, relative position of thread/process/function, limits/requirements, dependencies, data, prior operations (cache), differentiating attributes, context, mix/iteration/cycles/sequence, type, example, value)
    - 'infinite compression' as a combination of infinite sequences & numerical references to those sequences as parameters (similar to rainbow tables), where infinite random sequence subsets can be recursively compressed into hashes, where infinite sequences can be represented as a sum of sequences (infinite random adjacent base sequence + an infinite geometric sequence to generate it), or a sequence & a set of vectors indicating changes to generate the sequence from an infinite random adjacent base sequence

  - additional attributes (metadata) of interfaces include attributes like validity & relevance, which vary absolutely across other interfaces, and contextually across interface queries/solution automation workflows & problems being solved with them

  - additional examples of error causes
      - example of priorities leading to errors:
        - the 'selfish' perspective prioritizes simple, structural objects (like thos within an adjacent range), which is why 'selfish' programs can only see & act on those things rather than producing solutions that can be used for many problems, even ones they dont have
      - example of how a 'lack of errors' in a closed system can produce an error in system-level interactions, like a cross-system merge

  - add to stats
    - give example of visualizing the stats object connections in a stat insight rule
      - add example of how to derive the rule 'choose algorithm with low bias/high variance for large data sets'
        - use 'random forest' bagging method to reduce high variance
        - high variance is a problem when a sample data set is not representative of the population, producing accuracy for the training data subsets & errors for all other data subsets
          - if a data set is relatively large in relation to the population, the 'sample data set is not representative of the population' is less likely to be a relevant problem
          - 'variance can only be reduced by averaging over multiple observations, which inherently means using information from a larger region'
        - high bias is a problem when sample data sets differ from the population mean by a lot, producing errors for most samples unless they happen to be represented by the general model
          - if a data set is relatively large in relation to the population, the 'sample data sets differ from the population mean' is less likely to be a relevant problem, so the large data set can be used as a basis for the general model
          - 'bias is reduced using local information'
          - 'If training set is small, high bias / low variance models (e.g. Naive Bayes) tend to perform better because they are less likely to be overfit.'
          - 'If training set is large, low bias / high variance models (e.g. Logistic Regression) tend to perform better because they can reflect more complex relationships.'

  - give examples of why other tech solutions are insufficient
    - why ml would be inaccurate on math problems (like 'predict convergence value for an infinite series, based on training data of infinite series param input & convergence value output')
      - regularization, bias/noise changes, and other feature changes for intents like 'generalization' or 'feature selection' may add to inaccuracies
      - the structures that can be composed by various function/node/weight/threshold unit combinations may not be capable of the math operations involved in transforming one value/format into another
      - the math problem has emergent structures that are not visible in the training data set, for example a point where the relationship function between the inputs/outputs changes (like an asymptote or a maximum)
      - the training data may reflect patterns that are simpler for the network to compose/filter/reduce, without enough data points representing more complexity (a parabola instead of a hyperbolic function)
        - the network may be good at providing filter/compose/reduce functionality, but not other functionality like 'converge' or 'select between similar alternatives'
        - the support for 'multiple alternative input-output routes' in networks may add too much complexity in reducing its ability to specify a particular answer, with built-in tools to find averages or other representative values rather than selecting one value over another
        - for some functions, there are too many inputs that could produce the labeled output, and those inputs may be too similar to differentiate/specify (very similar functions can produce the same area)
      - neural networks are primarily good at certain data transformations, like where the problem input can be converted into the solution output by a system of linear equations or matrix operations supported by the network, where coefficients/weights of versions of function components (inputs & interim weighted combinations of inputs) are the required output of the network, and those operations may not adjacently handle operations like summing an infinite series, which might require specialized structures like memory/state-embedding or online learning if those structures cant be produced by adding additional layers to a network
      - each feature of the infinite series input would contribute to the output, but a neural network is designed to learn weights of features, implying that some are less important, and the contribution of terms in a series to the convergence value can usually be determined by its size/position
      - if parameters of the series are used instead of the series, that is a low feature space compared to the input features available in other applications of neural networks
      - the operations in some formulas do not produce reliable learnable patterns (some structures of randomness would counteract the ability of the network to learn a function)
      - the inputs dont provide enough data for continuously differentiable spaces where methods like gradient descent can be applied, given that math functions often cover a wide range of possible inputs, and a training data set is unlikely to be representative of enough examples to fill in the gaps in this space

    - VAR & reservoir computing's random sample of matrixes is inadequate bc the randomness is an attempt to identify 'very different' difference types, without generalizing that into a unique set of differences that are likely to be useful (like differences distributed across pairs of variables, so many random samples dont represent difference types in the same pair)
    - regression is insufficient even if its a good temporary solution if you dont have other resources than the data set, bc its conclusion/outputs (in the form of the regression function) can have the opposite meaning 'random noise' as the intended meaning 'causal relationships'
    - statistics in general is built on the insight that 'probability is associated with certainty/truth', but it ignores other certainty structures like structures that are more useful than patterns/probability, such as definitions, concepts, meaning/understanding/integration/organization, cause, inevitability/requirement, determination/generation/description structures, functions of varying interaction levels, etc
    - machine-learning can have the opposite functionality given extreme data values or update functionality manipulation (to train it to give the wrong answer in its online learning functionality), as well as other exploits from interactions of the algorithm, network, parameters, emergent structures, & data, and it is not built on understanding
      - 'one-degree connection structures' which are present in 'foundation models' are incapable of capturing multi-degree connection structures like sequences/chains or structures of connections like trees/networks/groups, even though other structures can be formatted as core structures like 'connections', it doesnt mean a one-degree connection network will capture them, or that a network of connections is the most optimal structure for that info given its usage
      - machine-learning based on neuroscience leaves out other brain interfaces like psychology, chemistry, & language
        - a psychologist might interpret a thought as 'an emotional reaction to a chemical stimulus that retrieved a memory'
        - a neuroscientist might interpret a thought as 'a response to electrical stimulus given weighted connections between neurons that previously handled that stimulus'
        - a linguist might interpret a thought as 'a deviation from a previous phrase that captured an experience to handle a change to that experience'
        - a chemist might interpret a thought as 'a result of scaled electron dynamics in response to a chemical'
        - a biologist might interpret a thought as 'a useful way to produce serotonin to offset a signal from the gut'
      - https://thegradient.pub/has-ai-found-a-new-foundation/

  - add to problem/solution structures

    - add example of how to derive 'apply differences to inputs to see if they can change the output to see if the solution is true'

    - add example of impact of methods on various network types given the differences in method/network structures & include assumptions

    - give example of how structures could have been derived (symmetry, isomorphism/interchangeability as common important objects to derive an interface, alternative interfaces to solve a problem) from another direction

    - derive logic types that would be necessary to complete the logic interface & give examples of logical object interactions

    - a 'find a solution' function should be able to be converted into a 'generate a solution' function & other functions like 'test a solution' & 'apply a solution', bc as the brain learns, it can generate solutions on demand once understanding develops

    - give example of identifying meaning of emergent structures (like 'weight trees' in neural networks)

    - organize workflows using useful structures as being on the meaning interface, where useful structures from other interfaces overlap & connect with the meaning interface

    - write interface queries to generate each workflow

    - give examples of how each workflow can be applied to various standard problems (find a prediction function, sorting problem, ml configuration/algorithm-design problem), which can be used as a data source to derive implementation logic/interface queries to generate solutions

    - finish math mapping so you can find other useful/solution structures (interaction space as convolution, core functions as basis vectors, etc)

    - basic solution automation workflows
      - trial & error
        - use a rules/solution database & look up the answer (try known solutions)
        - apply machine-learning with various configurations (try known/probable configurations)
        - apply rules from other systems to see if they work in another system (try other known systems)
        - mix & match solution components/variants (try known solutions)
      - reverse engineering
      - break problem into sub-problems & merge sub-solutions

    - example of format/intent matching
      - formatting a 'tree' as a 'set of overlapping sequences with overlaps in either inputs or outputs' so functions can be formatted for different intents like in 'parallel processing'

    - add to input structures
      - input variable/trigger/requirement/component

    - add to output structures
      - limits on what a structure can be used to create
      - similarities/differences to inputs (inputs change can be preserved in outputs)

    - identify new interactions/structures
      - trying structures of structures that havent been tried yet (like how new words evolve as a 'combination' of other words to describe new experiences that are similar to both combined words)

    - 'testing/simulation' involves querying for related rules (like how 'gravity' rules are related to 'motion' rules so any change involving motion should have a 'gravity rule check' applied as a filter) & checking if they apply to relevant components (like how specific components are involved in 'motion', like 'energy', 'motion restrictions', 'motion functions', 'motion triggers/inputs/components')
      - this is an important process for checking if a structure is valid/consistent in a system, which is a useful function
      - this is different from basic testing, which is where a function is applied and the output is checked against an expected value, bc it involves testing for validity/consistency in a system context where the change is being applied

    - examine interaction space of tech stack layers (ml models, algorithms, data, apps, bugs, os, chips) as a source of new errors
      - example: 
        - ai applied to design chips
        - chips with data erasure bugs that exacerbate os data erasure bugs
        - chip designs that produce error types for various ai models/algorithms/parameters
        - how 'gpus are known to be better at building ai models'

  - add to error-finding methods
    - identifying & generating known useful structures like 'symmetries', 'variables', 'subsets', 'interchangeable alternatives', 'maximally different inputs' & 'bases' & 'type/phase shift thresholds'
      - identifying & generating combination structures of useful structures like 'maximally different values around bases'
    - identifying gaps in known useful structures explaining data points (where data points arent explained by those known structures) & generating inputs in those gaps other than those data points

  - add to conceptual math
    - example of a conceptual math operation that builds a boundary structure leaving an inevitability of a matching concept (numbers) filling the structure
      - the concepts of 'missing', 'multiple/more', 'unit', 'type', 'identifiable as similar/equal/different' and 'difference in amount' allow for/require/build the concept of 'numbers'
      - also functions like 'compare' or 'reduce' or 'expand' require the concept of 'numbers' when comparing objects of that data type or objects having a quantifiable attribute

  - add to causation variables
    - ability to change (if a variable cant be changed, it is less causative for problem-solving intents)

  - add to info problems
    - this manipulates:
      - audience objects:
        - ego
        - assumptions (about patterns, what you would notice/figure out)
        - attention
        - feelings 'opposite' to logic (safety, confusion)
      - using objects like distractions, activations, distortions, core structures like combinations/sequences, complexity, patterns, input/output similarities/alternatives (complex/simple implementations), logic, patterns of logic, logic avoidance, jokes
      - to produce:
        - errors in expectations (in order for the audience to expect y, they have to have assumption x, as x is an input to y)
      - these important variables can be identified by identifying the inputs to these objects
        - what 'input' is 'required' for this expectation error to happen? (an assumption)
      - https://www.smithsonianmag.com/arts-culture/teller-reveals-his-secrets-100744801/?all&no-ist

  - when is it optimal to store a mixed structure of varying specificity (like a type, intent, cause & a specific example)
      - when there are potential uncertainties to resolve, like whether the example represents a new error, type, or variable, bc the example doesnt fit known structures

  - all primary interfaces can act like the problem-solving interface (start solving problem from the concept or structure interface and integrate all info back into that interface & frame the solution in terms of that interface) but the meaning interface (the interface interface) is the most powerful

  - apply concepts to structures

    - concept of attention in structures
      - mixed interim high-variation & high-similarity structures tend to maximize attention
    - examine error type of conflating intent & requirement
    - consciousness as choice to move between neural nodes (rather than being directed) required:
      - the development of alternative node paths performing equal/similar functions, requiring:
        - the development of excess resources, delaying required decision time (making immediate decision unnecessary, avoiding a forced decision), requiring:
          - the existence & application of previous efficiencies & functions for alternative evaluation, energy storage, storage-checking, & energy requirement-identifying
      - the cause could be framed as structures such as an 'efficiency stack' or 'energy maintenance functions' or 'alternative options' or 'navigation/motion control' or 'lack of requirement/need'
    - examine similarity (alignment/overlap) structures between: 
      - extremely different components (when an error type is an incentive or a function used for other intents) 
        - when the solution format of some problem has similarities to the error type, like when you need randomness so errors generating randomness are a possible function to use for that intent
        - contradictory/opposite components (have some metric in common, with opposite values)
    - examine the distortion vector paths that adjacently decompose a data set into a prediction function from a base point/function set

  - add examples of:
    - mapping to structures & identifying contradictions its safe to ignore for applying a structure
    - system/object/rule/type change patterns
    - query examples for use cases like:
      - lack of information stored (match problem of type 'information lack' with interface query 'check pattern interface for similar patterns')
      - query problem breakdown & integration diagram
      - calculating various different problem breakdown strategies first before executing normal query-building logic for each
    - example of how to predict most interactive/causal concepts in a system


## diagram
  
  - diagrams:
    - error types
    - network of formats
    - efficiencies
    - alternate interfaces (information = combination of structure, potential, change or structure, cause or structure, system)
    - chart type: overlaying multiple 2-dimension variable comparisons to identify common shapes of variable connections (density of points added with a visible attribute like more opacity)
    - structures of emergence
      - example: 1-1 input/output relationship up an interaction layer, where extra resources that dont dissolve immediately on the higher interaction layer aggregate & form core structures like combinations, where interactions between combinations & sequences have different dynamics than the individual output interacting with other individual outputs
    - how emergent functionality/attributes come from interaction structures (sequences & layers)
    - intent-matching
    - interface overflow (to sub-interfaces), interface foundation
    - workflow
      - function to identify relevance filter ('functions', 'required') from a problem_step ('find incentives') for a problem definition, to modify problem_steps with extra functions/attributes ('change_position') to be more specific to the problem definition ('find_incentives_to_change_position') for problem_steps involving 'incentives', so you know to use the function_name to modify the problem step if it's between the type 'functions' and the object searched for 'incentives'
    - conceptual math interface query
      - use lattice multiplication as standard example, other than core operations (add/multiply mapped to language, concepts like irreversibility/asymmetry mapped to math)
    - interface conversion, matching, starting point selection (applying structure, checking if relevant information is found)
    - sub-functions of core functions with distortions (identify/filter of find)
    - dimension links higher than 3d that are depictable in the same network space
      - should show variables that impact other variables, the change rates of these relationships
      - overall impact should be calculatable from these relationships
      - should show similar movements for correlated variables
      - should show skippable/derivable variables (variables that can be resolved later than they normally are)
      - should show meta forces for overall trends in change rules (direction of combined variable forces)
      - should show limits of measurability & threshold metrics
    - specific concepts, core functions, concept operations (combine, collide, connect, merge, apply), ethical shapes
        - variable accretion patterns (how an object becomes influenced by a new variable, complex system interaction patterns, etc)
        - potential matrix to display the concept
          - map parameter sets to potential matrix shapes 
        - cause (shapes & ambiguity), concept (evolution of concepts, networks, distortion functions)
        - argument
      - system layer diagram for each interface to allow specification of core interfaces & other interface layers (interface interface)
        - system layer diagram for structures to include layers of structures 
          (beyond core structures like curves, to include n-degree structures like a wave, as well as semantic output structures like a key, crossing the layer that generates info structures like an insight, a probability, etc)
    - map variable structures to prediction potential for problem types, given ratio of equivalent alternate signals
    - vertex variable structures
      - quantum physics, prediction/derivation tools, build automation tools, testing tools, learning/adaptation tools, system rules, computation power are all vertex variables of information, since they can generate/derive/find information
        - which structure (sequence, network, set, or cycle) of vertex variables is most efficient
    - core component attributes: identify any missing attributes/functions that cant be reduced further
    - absolute reference connections with metadata structures like networks/paths


# content/config

    - import insight history data to identify insight paths (info insight paths like 'lie => joke => distortion => insight', system insight paths like 'three core functions + combine function with this definition + n distortions to nearest hub')
    - define default & core objects necessary for system to function (out of the box, rather than minimal config necessary to derive other system components & assemble)
      - add default functions to solve common problem types
      - alternate utility function implementations have variation potential in the exact operations used to achieve the function intents, but there are requirements in which definitions these functions use because they are inherent to the system. For example, the embodiment may use a specific definition of an attribute (standardized to a set of filters) in order to build the attribute-identification function using a set of filters - but the general attribute definition is still partially identifyd in its initial version by requirements specified in the documentation, such as a set of core attribute types (input, output, function parameter, abstract, descriptive, identifying, differentiating, variable, constant), the definition of a function, and the definition of conversion functions between standard formats.
    - systematize definitions of info objects
      - include analysis that produces relationships of core objects like opposites to their relevant forms (anti-symmetry) in addition to permuted object states (asymmetry), such as an anti-strategy, anti-information, anti-pattern
      - organize certainty (info) vs. uncertainty objects (potential, risk, probability)
      - make doc to store insight paths, counterintuitive functions, hidden costs, counterexamples, phase shift triggers
      - add technicality, synchronization, bias, counterintuition, & certainty objects leading to inevitable collisions
        - error of the collision of compounding forces producing a phase shift
        - lack of attention in one driver and false panic in a second driver leading to a car crash given the bases where their processes originate
      - define alignment on interfaces (compounding, coordinating, parallel, similar, etc)
      - add core info objects (core strategies, core assumptions) so you can make a network of graphs for a system
    - add function logic for:
      - concept analysis:
        - how new concepts (gaps in network rules) evolve once structure is applied to prior concepts 
      - interface analysis:
        - limitations of interfaces & how to derive them
        - how rules develop on stability & how foundations are connected & destroyed
        - explainability as a space limited by derivable attributes from data set & cross-system similarity
        - vertex definition & give examples (as an intersection/combination of interface variables, such as determining/description(compressing)/generative/causative/derivation variables), around which change develops
      - change analysis:
        - generated object change types
          - constant to variable
          - variable to removal of assumption in variable type/data type
    - examine implementing your solution type (constructing structures (made of boundary/filter/resource sets) to produce substances like antibodies, using bio system stressors)
    - resolve & merge definitions into docs/tasks/implementation/constants/definitions.json
    - update links
