  - fix indexing 'NNP NN NNS1 of JJ JJ JJ2' or postpone to pattern evaluation time
  - fix missing alts
      pattern_index::verb_phrase::plays a |VB NN| role::a NN role
  - fix one-letter alts
      pattern_index::phrase_identifier::ALL_N DPC ALL_N |VBG VBD|::N D N V
  - generalize alt logic to use embedded pair finding
  - add formatting to allow multiple items as keys in json and maintain order for interface network paths

  - fix supported stem assignment (endings like 'is': {'functions a', 'acts a', 'plays a', 'operates a', 'works a'})
  - fix charge function ('edit' is assigned positive score)

  - add core clause patterns 
  - fix pattern matching functions
  - finish pos, clause, modifiers code from find implementation
  - finish network creation function
  - strategy/insight graph
  - add a standard system diagram with radiating layer diagram
  - add other causal structures

  - if the point of the universe is not to find the initial filters but to prevent that information from being discovered, that could keep open options for other change sources

  - analyzing just by change rate makes it less likely to spot other patterns like overlap/intersection of patterns

  - add diagram for question derivation for service list

    - deriving the questions customers will ask for a set of services

      - which processes are complicated or not optimized (need to be in person for certain transactions that people would rather do online)
      - which processes involve changing information (account balance, transaction approval)
      - which processes are likely to have errors (auth)
      - which processes people will likely be interested in using the most

    - what percent of changes are just from finding efficiencies, using those as a foundation for common distortion types (random change, directed change, connecting change, etc)

    - what system of core objects/functions/attributes generate a space where:
      - circles & squares are fundamental or standard objects
      - there is a continuous spectrum of values (real numbers) around which alternate number types rotate (complex numbers, etc)
      - comparing change generated by two variables (one independent value function determining the dependent value) has patterns of measurement potential
      - isolating by attributes (like isolating direction & scale to transform to a vector space) or framing information in different structures (sets, matrixes, sequences) allows patterns that are calculatable (implying the framing filter is determining, so matrix attributes can be determined by its definition)

    - what objects describe lack of information like ambiguities or lost information, other than randomness (difficult to identify randomness), variance (lack of patterns), and infinities (lack of information being difficulty of computing the sequence except in terms of other infinities if it doesnt converge, or lack of guarantees that the sequence can be maintained/stabilized to continue)


  - now youve described core methods to decrypt changes within systems, high-level tasks that are next (after building core functions like attribute identification function)
    - mapping function to map problems to structures
    - solution decomposition function
    - solution aggregation function

  - identify attribute

    - attributes can be reduced to 'position', implemented as a:

      - relationship type (relative difference/importance)
      - structure type (shape)
      - change type (generators of difference)

    - the structural network can frame these position differences to capture all attributes

  - identify function

    - a function can be reduced to a 'change unit'

  - identify object

    - an object has attributes/functions and is not itself either of those (for standard definition of object, even though both attributes/functions can be framed as objects)

  - after identification functions

    - import rules for selecting interfaces to solve a problem on

      Function helped find unused functions
      Intent helped predict system priorities & find exploit opportunities
      System helped find efficiencies
      Pattern helped find insight paths/similarities

    - once you build function/attribute identification function
      - import insight history data to identify insight paths 
        - info insight paths like 'lie => joke => distortion => insight'
        - system insight paths like 'three core functions + combine function with this definition + n distortions to nearest hub'


  - stat problem: "Sunrise problem: What is the probability that the sun will rise tomorrow? Very different answers arise depending on the methods used and assumptions made"

    - interface analysis questions:

      - what are the shapes & patterns of errors in assumptions & selection/generation of methods? (what ratio of incorrect are people with each additional assumption, given the level of certainty per assumption & complexity of problem)
      - what are the consequences of not correcting those errors? (how wrong will the predictions be)
      - what are the shapes of cause in generating/selecting assumptions & methods
      - what is the usual correct assumption pattern once false assumptions are corrected, and whats the insight path to transform the incorrect to the correct version?
      - whats the rate of discovery of new sub-systems, objects, or variables in related systems like physics
      - whats the likelihood we created certainty out of what ratio of our assumptions (over-relying on assumptions to make them conditionally true)
      - whats the possible causative impact of measurements & application of science knowledge on other knowledge
      - whats the possibility that a subset/state of physics rules gathers in increasingly isolated space-times, but outside of it, the rules are more flexible
      - whats the possibility that every science rule we take as certain is a false similarity or other false object?

  - why are randomness & symmetry often found together, like with the central limit theorem or circles? 

    - because symmetries are a default object that develop in complex systems on top of efficiencies, and growth on those symmetries is random (undirected) because symmetries dont typically come with default filters/limits directing the output

    - the normal distribution would likely be generated by a set of subsets where each subset has most of its values in the 1 standard deviation range and a few outliers in either or both directions

      - theres some variation in the subsets, which could include other patterns in small quantities that dont influence the average

      - why would 'most values in the 1 standard deviation range + a few outliers' be the standard or most common pattern in random independent variables?

      - this involves random independent variables like dice 1 outcome & dice 2 outcome - meaning the common pattern takes the form of most values in the 3 - 4 range, and a few values in the 1-2 and 5-6 range

      - given the commonness of that pattern, it can be interpreted as a default state of the dice probability distribution, where other patterns are distortions of that default

      - that common pattern has a hard limit placed on the central values 3 - 4, and the set of values that are outliers are less limited (can be 1 - 2 or 5 - 6 or a combination) - it requires some central values, but the outlier values are more flexible

      - the alternate version of that common pattern is that 'the proportion of 5/6 values relative to the proportion of 1/2 values will be equal (the sides of the curve are similar in shape), and the proportion of 3/4 values will be greater than those outlier values'

      - randomness means 'lack of influence or direction' in this case, which translates to 'lack of default similarity in adjacent outcomes', removing the normal semantic value of position

      - this means in a large sample with random selection, that a sequence of different values is likelier than a sequence of the same value

      - the 3/4 set is different from the 1/2 set and the 5/6 set

      - this means the 3/4 set is likelier to appear with the 1/2 set and the 5/6 set (same relationship applied to the interim sets, 2/5, in relation to the extreme outlier sets 1/6)

      - given the ratio of the outliers in the set of 1 - 6 (4/6), the likelihood that the average set (3 & 4) will provide the most difference from the previous value in set type is higher than the likelihood that another outlier will provide the most difference from the previous value in the set type metric (average, interim, extreme set types)

      - this implies the dynamic is more of a circle shape than a spectrum, where the pairs of adjacent values in a random sequence occupy points in a circle and the central tendency holds (pairs are likelier to include a 3/4 than other values, pairs are less likely to include two consecutive extreme values like 1/6)

      - analyzing adjacent pairs as the important objects is more useful than analyzing individual outcomes in isolation

      - random can also mean 'difference is default' - difference in this case represents the efficiency, and the distortions from that efficiency represent the outlier values

      - so in random functions, given the definition of random, there are biases rewarding:
        - difference between adjacent values (only 1/6 probability of the next value being the same value, and 2/6 (1 and 6) values have only a 1/6 probability of the next value being one unit away)
        - similarity to default/efficient value
        - difference in set type (average vs. extreme)
        - compliance with common patterns (like 'many symmetry origin values (like an average) plus few extreme/different values')

      - violation of randomness about a symmetry:

        - a random variable will change if another symmetry is adjacent enough to exert gravity on the variable, at which point the random variable will conver to & be distorted around the new symmetry

          - if a fly gravitates around one light but in their random motion, they encounter the edge of the light's reach, and another light is more nearby at that limit point than the original light, they may gravitate toward the new light - same with symmetries that are sufficiently adjacent as to be nearer to another symmetry's limit than the original symmetry's radius 

            - factors include if lights are likely to be evenly distributed, if they have different radii, if they have different types of light, etc

          - the corresponding example with the dice includes factors like:
            - people who use dice often happen to buy magnets (or magnetic material is often used in building/furniture construction)
            - the dice contain metal that responds to that charge 
            - the charge is strong enough to exert a force if the dice are thrown near enough to those objects (standing near a wall or sitting on furniture)
            - people arent careful to remove magnetic material from the experiment location

          - this is an object called an 'efficiency/symmetry overlap', where radii (or the equivalent semantic object) provide an intersection between symmetries
          
          - this either must not happen much in nature, or the overlap isnt usually enough to cause more than a few outliers, or these trends arent often described

      - how would you generate an independent variable? 

        - for example, how would you generate a high degree of randomness in the compounds that your bio system encounters?

        - youd make sure to interact with many different objects (like plants, locations, and experiments combining other objects) 

        - youd seek out differences, and try to eliminate certainties & their patterns

        - the way you would seek out differences & eliminate certainties would probably not be random - it would be normally distributed - youd try one interaction for a while, then move on to another interaction within a similar range of difference, and the final output would gravitate toward an average, since different compounds are usually different to test out variable combinations or because of local/conditional optimality, rather than because theyre known to be absolutely optimal by nature, and the average is often the most stable & therefore the most efficient state, given that very different values are unlikely to coordinate with all other system objects as well as the average does given its commonness, which implies that functions to handle it already exist

        - so stability often develops from aggregating many sources of difference, around the sets that offer the most difference in set type (default/efficient/common/average vs. extreme/conditional type) and the least difference in value (1/3 and 4/6 rather than 1/6 and 2/5) because of efficiency/commonness/stability, which may as well be proxy variables for each other in this context

      - a ratio of randomness is allowed in nature because the rules limiting interactions are finite

    - why do symmetries evolve in complex systems? so that differences can develop/stack within the symmetry range, leading to more differences when those differences interact with differences from other symmetries

  - structural mismatches of solutions & problems

    1. algorithm structures: theres an inherent structural mismatch between some algorithms (decision tree, neural net) and some problem types (prediction) given the intent & abstraction layer 

      - example: 

        - some algorithms are too specific or have a structural mismatch (decision tree has structural split, specify & direction intents) with the problem type (predicting a set of variables that is about to change)

        - the decision tree occupies a very low-level, neutral, granular, abstract/structural position - that means it can be used for many intents 

        - the decision tree can be hacked with various edge case, phase shift & boundary manipulations

          - if two variables on different layers are about to converge, they should have been on the same layer or in the opposite causal direction or treated as one variable - the model will be increasingly wrong until it's updated or until these ambiguities and likely relationships are accounted for in the design

          - the reason algorithms can be hacked is bc of the structural mismatch between the algorithm and the problem type

        - the intents resulting from the decision tree's layers, direction, and thresholds can only contain so much variation in the data 

        - generated varied data with permutations of data objects (loops, sets, alternates) can be used to make the tree more robust to adjacent/likely changes

    2. test structures: a similar problem is when the test/method will identify false similarities or other objects & return the false version, limiting the potential for the correct version to be identified

      - the structure of the test can be a barrier to the truth, if its over simplified or excludes too much information

      - a test that's done iteratively on new data without changing the function according to new information would fail if the data changes more than the function can handle

    3. brain analysis structures: structural biases in human brains prevent us from seeing the truth - we're biased toward objects we understand or which are simple to derive

    - algorithms should identify mismatches (in complexity, variance, completeness & other metrics) & other problem types between the data/algorithm/problem type

  - algorithms should produce a set of solutions (an obvious/simple answer, a pattern-compliant answer, a common answer, a robust answer)

    - example of why youd want an 'obvious solution' tag on the output:

      - how could you build an algorithm that wouldnt overidentify a race attribute as being a potential criminal indicator?

      - query maps & causal shapes:
        - query for data on related word sentiment (if theres a negative association with a word related to that race)
        - query for data on intent ('identifying potential criminals' is the intent of the training process, which needs to be an input to the algorithm)
          - why would identifying an individual by a related attribute to a negative related term be useful for the 'identify potential criminal' intent?
          - if the intent is to 'identify potential criminals', which is a high-stakes intent
          - the algorithm should identify that a causal loop would be dangerous to treat as an input, given that a causal loop (like mistreatment or persecution of minorities leading to poverty which leads to crime) can hide original inputs (root cause being mistreatment)

      - alternatively, it could use interface analysis:

        - use inference & intent (to predict) to arrive at insights like:

          - 'if the answer is obvious, it must not be true bc otherwise I wouldnt have been asked to predict it, unless this is a test situation'
          - 'if the attribute value is common across the population, it must not be a predictor bc there are many people with that attribute value who are not in this data set bc they didnt commit a crime'

        - hard-code insights like above to be consulted if a variable is mistakenly identified as significant

        - treating one involuntary variable as high-impact belies the complexity of social games, which involve learning competitions (who has the best manners, whos the smartest) that would mean there is high potential for economic status variation within nodes having that attribute

        - given the changeability of that variable (easily changed with structural tools & also frequently changed in gene pool), it should not be treated as a predictor

        - it should identify location & economic status as an indicator of criminal activity given the health/drug addictions symptoms of the people whose data is used for training

        - it should also identify social games that are used for criminal activity & skill at those games (making intimidating or emotionless facial expressions)

        - it should identify culture as a key factor in variation in criminal activity, which can be specific to an attribute but has high variation within that attribute (producing gang violence, govt corruption, or a culture of karma/street justice) with low variation in outcomes (kill or be killed) - and identify that if it doesnt have cultural information, it cant make predictions

      - most algorithms dont have the complexity to identify complex sub-systems or related systems like social information games, communities, economies, or cultures

      - a really smart algorithm would immediately identify a few insights like:

        - example of deriving a social game like bullying:
          - with a priority of 'avoiding criticism'
            - most nodes would deserve criticism, because avoiding it is easy, especially at scale
            - this incentivizes criticizing nodes who dont deserve criticism

        - 'there are different reasons people do crimes'
        - 'some reasons they do crimes include need (resource acquisition, asset/reputation protection), goal attainment (enable a career), culture (avoiding crimes isnt a priority), social games (dares, threats, bullying, corruption, group dynamics), enjoyment (test if anyones paying attention, test how fragile the system is, rebel against authority)'
        - 'randomness is the biggest distributor of those reasons' (luck)
        - 'randomness can lead to lack of justice or other types of meaning'
        - 'people who have lack of justice are likelier to do crimes'
        - 'which people dont have justice - unfairly persecuted people'
        - 'which people are unfairly persecuted - different people, excellent people'
        - 'some privileged people with justice/meaning do crimes anyway just for fun to see how much they can get away with' (counterpoint - other reasons to do crimes than need, culture, social games)
        - 'which facial expressions are associated with the criminal reasons we're trying to prevent'
        - 'which factors do we need to complete the prediction function'
        - 'which expressions are often false signals despite being good predictors in subsets'

        - how would you build an algorithm to identify those insights just from maps & from mugshot data? 

          - the point is invalid once you have those insights, bc the answer is clearly not predicting who will do crimes but figuring out:
            - a system of social rules to prevent those situations from happening in society
            - at what point people start trying to have fun rather than contribute
            - deriving intent (either mechanically or with a prediction tool)

          - the algorithm would identify a high-variance data set and derive that its not the obvious visible attributes that will be predictors (except a few like visible signs of crack addiction, which will only predict the ratio that are crack addicts) but the subtle visible attributes, which will necessarily be incomplete without data about each person's specific traversal of the maps

          - the algorithm would identify that certain facial expressions are associated with criminal activity, such as:
            - dead eyed hopeless expression (lost hope of good treatment)
            - crazy expression (went on a rant after putting up with some stressor)
            - shocked expression (cant believe this happened to them)

          - most of them would also have signs of stress destructuring their faces, except resilient criminals, bosses, or some first-time offenders

          - the different routes to each expression should show up in traversals of social game, community, & culture maps as their status & decisions change, and cross concepts like randomness (in which ideas people encounter, which skills they learn, which social games they play), equivalence (in resource distribution), justice (as a counter object to randomness), & meaning (group belonging, goal attainment, success)

          - the output would be a set of paths that leave traces, some of which would show up in mugshots, some of which would require questions

        - identifying a concept (like 'reason to do crime') can be as trivial as exploring combinations of core functions

          - once the algorithm identifies the concept of language (from input data or insight maps) as 'information trades formatted as paths', it should be able to identify the concept of lies, as it will know that 'information can be inaccurately described' from its own errors

          - once it identifies lies, it can identify the concept of social games (reward from coordinating a lie with another node, reward from a successful lie, reward from adding a distant transform to another node's information)

          - once it identifies social games, it can derive their intent (the point of social games is to control other people or to get resources like enjoyment or funds)

          - then it can identify specific social games as anti-societal behavior that hurts the group

          - the counter-object is pro-societal rule compliance that helps the group

          - then it can move on to identifying specific social games (initial crimes, bullying) that remove inputs of pro-societal decisions (complying with rules), creating a 'reason to do crime'

          - then it can look for outputs of those games (stress, difference) and make an attempt to translate that to the data features (facial expressions, signs of addiction, signs of aging)

        - this requires that society-wide data needs to be integrated into the algorithms (like genetic variance) as well as data on criminal activity


  - rather than using data & the training process as an indicator of consensus, they can use patterns (patterns of data, patterns of change, patterns of variables) as an indicator of consensus

  - algorithm based on problematic adaptive systems like cancer bc theyre learning faster than the host system

  - extra tasks

    - add precomputing if a sub-pattern was already computed:
               'ALL_N ALL_N of ALL_N ALL_N'
         'ALL_N ALL_N ALL_N of ALL_N ALL_N ALL_N'

  - causal shapes integrated with networks (patterns of aggregation matching causal shapes like trees & circuits)

    - integrating system analysis with networks
      https://twitter.com/remixerator/status/1150578597339340805
      https://twitter.com/remixerator/status/1205724743741014018

    - system of causal types (integrated with type path example as a version of weight paths)
      https://twitter.com/remixerator/status/1156860484294852609

    - causal types
      twitter.com/remixerator/status/1126040476023279616

    - applying causal shapes to a network
      https://twitter.com/remixerator/status/1004579263507566592

    - position on causal type network
      https://twitter.com/remixerator/status/1018540899859607552

  - abstract functions

      - derive combinations & make sure you have full function coverage of all important combinations

        - check codebase function index for combinations
        - check that you have sample data in json for each combination

      - attribute/object/function match functions
      - specific interface identification function
      - standardization network-framing function to describe a system as a network (the standard structure) & position each object, identifying connecting functions
      - system analysis function (identify boundaries, gaps, limits, layers, incentives/intents/questions, & other system objects)
      - isolation function, representating function/attribute changes independent of system context with respect to position or time (snapshot/state or subset)
      - function to define (isolate an object/concept/function for identification, identify definition routes)

  - give example of each type of problem-solving workflows

    - workflow 1:

      - finish function to determine relevance filter ('functions', 'required') from a problem_step ('find incentives') for a problem definition, to modify problem_steps with extra functions/attributes ('change_position') to be more specific to the problem definition ('find_incentives_to_change_position') for problem_steps involving 'incentives', so you know to use the function_name to modify the problem step if it's between the type 'functions' and the object searched for 'incentives'

      - finish function to get all codebase functions & store them in a dict with their type, context/usage, and intents, just like functions are stored in the problem_metadata.json example for workflow 1
      - finish common sense check
      - finish defining objects in object_schema.json
      - finish organizing functions.json by type, with mapping between general intent functions like 'find' to specific info-relevant terms like 'get'
      - add common phrase check & filter problem steps by repeated combinations with common phrase check
      - finish get_type function to map info to structure using the new functions.json organization
      - finish apply_solution to problem_definition using problem_steps
        - involves building a function to evenly distribute objects (like information/types), given problem positions/agents/objects

      
  - types can be represented as directions (going farther from origin goes further up type stack, where similar types are adjacent)

  - need to fill in content:
    - finish intent/change type calculation for a system intent
    - selecting optimal combination interfaces to start from when solving problems 
      (how many degrees away from core functions, specific layers or sub-systems, what position on causal structures)
    - key questions to filter attention/info-gathering/solution
    - key functions to solve common problem types
    - development of key decision metrics (bias towards more measurable/different metrics rather than the right metric)
    - trajectory between core & important objects
      - example of choosing inefficiencies/exploit combinations in a system
    - research implementing your solution type (constructing structures (made of boundary/filter/resource sets) to produce substances like antibodies, using bio system stressors)
    - emergent combinations of core functions (include derivation of invalidating contexts for core functions)

  - change phases for causal analysis (interim, changing, diverging, standard, efficient state, constant, interacting, converging, on the verge of obsolescence, outlier, etc)
    - superficial cause, alternate cause in the case of a function, addressing input/output causes
  - framing on interfaces, decomposing causation, then identifying parameters of problem on layer & matching solution
  - independence (closed trade loops) as time storage
  - vertex as a pivot point for an interface



- notes

    - if something can generate a change predictably/consistently, it's a change supply - otherwise it's a change request, so output as well as causal position relative to the output is important when determining category
      - time may be a variance gap (a space where change is possible) to resolve a question/problem set - so not resolving it can preserve time, unless resolving it will allow for more potential or moving on to other variance gaps
