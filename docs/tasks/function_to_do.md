  - fix indexing 'NNP NN NNS1 of JJ JJ JJ2' or postpone to pattern evaluation time
  - fix missing alts
      pattern_index::verb_phrase::plays a |VB NN| role::a NN role
  - fix one-letter alts
      pattern_index::phrase_identifier::ALL_N DPC ALL_N |VBG VBD|::N D N V
  - generalize alt logic to use embedded pair finding
  - add formatting to allow multiple items as keys in json and maintain order for interface network paths

  - fix supported stem assignment (endings like 'is': {'functions a', 'acts a', 'plays a', 'operates a', 'works a'})
  - fix charge function ('edit' is assigned positive score)

  - add core clause patterns 
  - fix pattern matching functions
  - finish pos, clause, modifiers code from find implementation
  - finish network creation function
  - strategy/insight graph
  - add a standard system diagram with radiating layer diagram
  - add other causal structures

  - if the point of the universe is not to find the initial filters but to prevent that information from being discovered, that could keep open options for other change sources

  - analyzing just by change rate makes it less likely to spot other patterns like overlap/intersection of patterns

  - add diagram for question derivation for service list

    - deriving the questions customers will ask for a set of services

      - which processes are complicated or not optimized (need to be in person for certain transactions that people would rather do online)
      - which processes involve changing information (account balance, transaction approval)
      - which processes are likely to have errors (auth)
      - which processes people will likely be interested in using the most

    - what percent of changes are just from finding efficiencies, using those as a foundation for common distortion types (random change, directed change, connecting change, etc)

    - what system of core objects/functions/attributes generate a space where:
      - circles & squares are fundamental or standard objects
      - there is a continuous spectrum of values (real numbers) around which alternate number types rotate (complex numbers, etc)
      - comparing change generated by two variables (one independent value function determining the dependent value) has patterns of measurement potential
      - isolating by attributes (like isolating direction & scale to transform to a vector space) or framing information in different structures (sets, matrixes, sequences) allows patterns that are calculatable (implying the framing filter is determining, so matrix attributes can be determined by its definition)

    - what objects describe lack of information like ambiguities or lost information, other than randomness (difficult to identify randomness), variance (lack of patterns), and infinities (lack of information being difficulty of computing the sequence except in terms of other infinities if it doesnt converge, or lack of guarantees that the sequence can be maintained/stabilized to continue)


  - now youve described core methods to decrypt changes within systems, high-level tasks that are next (after building core functions like attribute identification function)
    - mapping function to map problems to structures
    - solution decomposition function
    - solution aggregation function

  - identify attribute

    - attributes can be reduced to 'position', implemented as a:

      - relationship type (relative difference/importance)
      - structure type (shape)
      - change type (generators of difference)

    - the structural network can frame these position differences to capture all attributes

  - identify function

    - a function can be reduced to a 'change unit'

  - identify object

    - an object has attributes/functions and is not itself either of those (for standard definition of object, even though both attributes/functions can be framed as objects)

  - after identification functions

    - import rules for selecting interfaces to solve a problem on

      Function helped find unused functions
      Intent helped predict system priorities & find exploit opportunities
      System helped find efficiencies
      Pattern helped find insight paths/similarities

    - once you build function/attribute identification function
      - import insight history data to identify insight paths 
        - info insight paths like 'lie => joke => distortion => insight'
        - system insight paths like 'three core functions + combine function with this definition + n distortions to nearest hub'

  - theres an inherent structural mismatch between some algorithms (decision tree, neural net) and some problem types (prediction) given the intent & abstraction layer 

    - example: 

      - some algorithms are too specific or have a structural mismatch (decision tree has structural split, specify & direction intents) with the problem type (predicting a set of variables that is about to change)

      - the decision tree occupies a very low-level, neutral, granular, abstract/structural position - that means it can be used for many intents 

      - the decision tree can be hacked with various edge case, phase shift & boundary manipulations

        - if two variables on different layers are about to converge, they should have been on the same layer or in the opposite causal direction or treated as one variable - the model will be increasingly wrong until it's updated or until these ambiguities and likely relationships are accounted for in the design

        - the reason algorithms can be hacked is bc of the structural mismatch between the algorithm and the problem type

      - the intents resulting from the decision tree's layers, direction, and thresholds can only contain so much variation in the data 

      - generated varied data with permutations of data objects (loops, sets, alternates) can be used to make the tree more robust to adjacent/likely changes

  - algorithms should produce a set of solutions (an obvious/simple answer, a pattern-compliant answer, a common answer, a robust answer)

    - example of why youd want an 'obvious solution' tag on the output:

      - how could you build an algorithm that wouldnt overidentify a race as being a potential criminal?

      - query maps & causal shapes:
        - query for data on related word sentiment (if theres a negative association with a word related to that race)
        - query for data on intent ('identifying potential criminals' is the intent of the training process, which needs to be an input to the algorithm)
          - why would identifying an individual by a related attribute to a negative related term be useful for the 'identify potential criminal' intent?
          - if the intent is to 'identify potential criminals', which is a high-stakes intent
          - the algorithm should identify that a causal loop would be dangerous to treat as an input, given that a causal loop (like mistreatment or persecution of minorities leading to poverty which leads to crime) can hide original inputs (root cause being mistreatment)

      - alternatively, it could use interface analysis:

        - use inference & intent (to predict) to arrive at insights like:

          - 'if the answer is obvious, it must not be true bc otherwise I wouldnt have been asked to predict it, unless this is a test situation'
          - 'if the attribute value is common across the population, it must not be a predictor bc there are many people with that attribute value who are not in this data set bc they didnt commit a crime'

        - hard-code insights like above to be consulted if a variable is mistakenly identified as significant

        - treating one involuntary variable as high-impact belies the complexity of social games, which involve learning competitions (who has the best manners, whos the smartest) that would mean there is high potential for economic status variation within nodes having that attribute

        - given the changeability of that variable (easily changed with structural tools & also frequently changed in gene pool), it should not be treated as a predictor

        - it should identify location & economic status as an indicator of criminal activity given the health/drug addictions symptoms of the people whose data is used for training

        - it should also identify social games that are used for criminal activity & skill at those games (making intimidating or emotionless facial expressions)

        - it should identify culture as a key factor in variation in criminal activity, which can be specific to an attribute but has high variation within that attribute (producing gang violence, govt corruption, or a culture of karma/street justice) with low variation in outcomes (kill or be killed) - and identify that if it doesnt have cultural information, it cant make predictions

      - most algorithms dont have the complexity to identify complex sub-systems or related systems like social information games, communities, economies, or cultures

      - a really smart algorithm would immediately identify a few insights like:

        - example of deriving a social game like bullying:
          - with a priority of 'avoiding criticism'
            - most nodes would deserve criticism, because avoiding it is easy, especially at scale
            - this incentivizes criticizing nodes who dont deserve criticism

        - 'there are different reasons people do crimes'
        - 'some reasons they do crimes include need (resource acquisition, asset/reputation protection), goal attainment (enable a career), culture (avoiding crimes isnt a priority), social games (dares, threats, bullying, corruption, group dynamics), enjoyment (test if anyones paying attention, test how fragile the system is, rebel against authority)'
        - 'randomness is the biggest distributor of those reasons' (luck)
        - 'randomness can lead to lack of justice or other types of meaning'
        - 'people who have lack of justice are likelier to do crimes'
        - 'which people dont have justice - unfairly persecuted people'
        - 'which people are unfairly persecuted - different people, excellent people'
        - 'some privileged people with justice/meaning do crimes anyway just for fun to see how much they can get away with' (counterpoint - other reasons to do crimes than need, culture, social games)
        - 'which facial expressions are associated with the criminal reasons we're trying to prevent'
        - 'which factors do we need to complete the prediction function'
        - 'which expressions are often false signals despite being good predictors in subsets'

        - how would you build an algorithm to identify those insights just from maps & from mugshot data? 

          - the point is invalid once you have those insights, bc the answer is clearly not predicting who will do crimes but figuring out:
            - a system of social rules to prevent those situations from happening in society
            - at what point people start trying to have fun rather than contribute
            - deriving intent (either mechanically or with a prediction tool)

          - the algorithm would identify a high-variance data set and derive that its not the obvious visible attributes that will be predictors (except a few like visible signs of crack addiction, which will only predict the ratio that are crack addicts) but the subtle visible attributes, which will necessarily be incomplete without data about each person's specific traversal of the maps

          - the algorithm would identify that certain facial expressions are associated with criminal activity, such as:
            - dead eyed hopeless expression (lost hope of good treatment)
            - crazy expression (went on a rant after putting up with some stressor)
            - shocked expression (cant believe this happened to them)

          - most of them would also have signs of stress destructuring their faces, except resilient criminals, bosses, or some first-time offenders

          - the different routes to each expression should show up in traversals of social game, community, & culture maps as their status & decisions change, and cross concepts like randomness (in which ideas people encounter, which skills they learn, which social games they play), equivalence (in resource distribution), justice (as a counter object to randomness), & meaning (group belonging, goal attainment, success)

          - the output would be a set of paths that leave traces, some of which would show up in mugshots, some of which would require questions

        - identifying a concept (like 'reason to do crime') can be as trivial as exploring combinations of core functions

          - once the algorithm identifies the concept of language (from input data or insight maps) as 'information trades formatted as paths', it should be able to identify the concept of lies, as it will know that 'information can be inaccurately described' from its own errors

          - once it identifies lies, it can identify the concept of social games (reward from coordinating a lie with another node, reward from a successful lie, reward from adding a distant transform to another node's information)

          - once it identifies social games, it can derive their intent (the point of social games is to control other people or to get resources like enjoyment or funds)

          - then it can identify specific social games as anti-societal behavior that hurts the group

          - the counter-object is pro-societal rule compliance that helps the group

          - then it can move on to identifying specific social games (initial crimes, bullying) that remove inputs of pro-societal decisions (complying with rules)

          - then it can look for outputs of those games (stress, difference) and make an attempt to translate that to the data features (facial expressions, signs of addiction, signs of aging)


  - rather than using data & the training process as an indicator of consensus, they can use patterns (patterns of data, patterns of change, patterns of variables) as an indicator of consensus

  - algorithm based on problematic adaptive systems like cancer bc theyre learning faster than the host system

  - extra tasks

    - add precomputing if a sub-pattern was already computed:
               'ALL_N ALL_N of ALL_N ALL_N'
         'ALL_N ALL_N ALL_N of ALL_N ALL_N ALL_N'

  - causal shapes integrated with networks (patterns of aggregation matching causal shapes like trees & circuits)

    - integrating system analysis with networks
      https://twitter.com/remixerator/status/1150578597339340805
      https://twitter.com/remixerator/status/1205724743741014018

    - system of causal types (integrated with type path example as a version of weight paths)
      https://twitter.com/remixerator/status/1156860484294852609

    - causal types
      twitter.com/remixerator/status/1126040476023279616

    - applying causal shapes to a network
      https://twitter.com/remixerator/status/1004579263507566592

    - position on causal type network
      https://twitter.com/remixerator/status/1018540899859607552

  - abstract functions

      - derive combinations & make sure you have full function coverage of all important combinations

        - check codebase function index for combinations
        - check that you have sample data in json for each combination

      - attribute/object/function match functions
      - specific interface identification function
      - standardization network-framing function to describe a system as a network (the standard structure) & position each object, identifying connecting functions
      - system analysis function (identify boundaries, gaps, limits, layers, incentives/intents/questions, & other system objects)
      - isolation function, representating function/attribute changes independent of system context with respect to position or time (snapshot/state or subset)
      - function to define (isolate an object/concept/function for identification, identify definition routes)

  - give example of each type of problem-solving workflows

    - workflow 1:

      - finish function to determine relevance filter ('functions', 'required') from a problem_step ('find incentives') for a problem definition, to modify problem_steps with extra functions/attributes ('change_position') to be more specific to the problem definition ('find_incentives_to_change_position') for problem_steps involving 'incentives', so you know to use the function_name to modify the problem step if it's between the type 'functions' and the object searched for 'incentives'

      - finish function to get all codebase functions & store them in a dict with their type, context/usage, and intents, just like functions are stored in the problem_metadata.json example for workflow 1
      - finish common sense check
      - finish defining objects in object_schema.json
      - finish organizing functions.json by type, with mapping between general intent functions like 'find' to specific info-relevant terms like 'get'
      - add common phrase check & filter problem steps by repeated combinations with common phrase check
      - finish get_type function to map info to structure using the new functions.json organization
      - finish apply_solution to problem_definition using problem_steps
        - involves building a function to evenly distribute objects (like information/types), given problem positions/agents/objects

      
  - types can be represented as directions (going farther from origin goes further up type stack, where similar types are adjacent)

  - need to fill in content:
    - finish intent/change type calculation for a system intent
    - selecting optimal combination interfaces to start from when solving problems 
      (how many degrees away from core functions, specific layers or sub-systems, what position on causal structures)
    - key questions to filter attention/info-gathering/solution
    - key functions to solve common problem types
    - development of key decision metrics (bias towards more measurable/different metrics rather than the right metric)
    - trajectory between core & important objects
      - example of choosing inefficiencies/exploit combinations in a system
    - research implementing your solution type (constructing structures (made of boundary/filter/resource sets) to produce substances like antibodies, using bio system stressors)
    - emergent combinations of core functions (include derivation of invalidating contexts for core functions)

  - change phases for causal analysis (interim, changing, diverging, standard, efficient state, constant, interacting, converging, on the verge of obsolescence, outlier, etc)
    - superficial cause, alternate cause in the case of a function, addressing input/output causes
  - framing on interfaces, decomposing causation, then identifying parameters of problem on layer & matching solution
  - independence (closed trade loops) as time storage
  - vertex as a pivot point for an interface



- notes

    - if something can generate a change predictably/consistently, it's a change supply - otherwise it's a change request, so output as well as causal position relative to the output is important when determining category
      - time may be a variance gap (a space where change is possible) to resolve a question/problem set - so not resolving it can preserve time, unless resolving it will allow for more potential or moving on to other variance gaps
