# to do

  - add to bias error type structures: variable combinations/connections that should be disassociated

  - organize list of solution automation workflows
    - break problem into sub-problems, solve in isolation & merge into solution
    - change problem into more solvable problem
      - identify cause by applying network to causation, then select which cause to solve based on solvability with adjacent resources
    - apply structures
      - vectorize problem system, filling in missing components with generative functions as needed
      - apply functions to move problem (origin) state position to solution (target) state position
      - apply function input/output connections to connect problem input & solution output with function sequences
      - apply system structures to generate & filter solutions for a priority like speed
      - apply map between problem-solution intents & function intents
    - generate solution space first, then filter
      - apply core structures of solutions to generate probable solutions
      - apply core functions to generate possible solutions & then apply filters to reduce solution space
    - apply filters first, then match with generatable solutions
      - apply solution filters to reduce solution space
      - apply structures of difference (what is not the solution) to filter solution space, then match to what core functions can generate as adjacent/accessible solutions
    - apply solution structures (filters) & problem structures (errors, reductions) in parallel and connect in the middle

  - generate solution automation workflows by applying workflows to other workflows

  - generative insight paths generating solution automation workflows

    - identify patterns in structures allocating structure (constants) & lack of structure (variation) in interface queries to find new insight paths
      - example: 
        - variation (like variables) allocated to structure & info interfaces, & constants (like definitions) allocated to the intent/concept interfaces
    
    - identify patterns in connecting structures as core components of interface queries (build interface queries with interface-connecting structures)
      - examples:
        - intent & function interfaces are connected as metadata & trigger structures, so the triggering structure can be followed by the triggered structure in interface queries
    
    - identify patterns of finding/selecting interaction levels for an interface query
      - examples: 
        - core functions linking these interfaces
        - structural versions of core functions linking these interface objects
        - abstract network of an interface used for interface queries
        - cross-interaction level conversion function applied before other interface query steps

  - interface query design:
    
    - connecting problem & solution formats has a set of workflows based on structure & adjacent solution automation workflows that can direct the interface query design by the requirements of the steps in those workflows

      - examples:

        - connecting a problem of 'too much structure' and solution of 'reduced structure' has a workflow involving steps like 'reduce variables', with requirements like 'variables', so the function or change interface can be applied to identify variables before executing that step in the workflow
        
        - connecting a problem & solution with a particular solution automation workflow also has input requirements, like 'break a problem into sub-problems' workflow, which requires that structure of variables (error/differences) are identified (to identify sub-problems), so applying the structural, function, or system interface is necessary to identify those structures which act as sub-problems
    
    - interaction structures allow interactions to develop but are different from interfaces/standards that specifically enable communication/comparison interaction types, despite interaction structures acting as a connecting structure which has structural similarities to communication, communication being the exchange of info that is interpretable & actionable to source/target
    
    - find equidistant point to information to start parallel interface queries from

  - assess fair use of copied code by:
    - how much effort it would have been to not copy (if its low-effort to not copy, there's no reason to copy)
    - ratio of value contributed by those lines (such as by the cost of bugs caused by distorting the line)
    - difficulty (cognitive distance) of inventing that product given what had been invented prior to that
    - ratio of total value that the code can generate (what other software can it generate, what can it automate)

  - anti-stupidity addiction counseling & meditation app, fixing addictions with incrementally slightly less evil replacements:
    - racism by convincing them there are non-racial groups that deserve hatred more ("hey Sharon, you're so right about the Jews, they absolutely cared about covering up their space weapons program enough to serve you that ad about fun outdoor non-genocide activities in your area, but you know who I hate more - wiccans with vegan pets" could be all it takes to wean Sharon off of her ethnic hatred so she can write manifestos about the behaviors she hates instead)
    - jealousy, by convincing them there are other people more deserving of jealousy, such as people who don't feel it bc they're the best ("my dearest Beatrice, rather than trying to criminalize your male coworkers complimenting someone else, dare I suggest developing intelligence so you can create any asset your heart desires" can fix Beatrice so she fulfills her destiny of being the recipient of increasingly & unnecessarily dramatic letters)
    - false hope that their new accessory, new dance move, brow grooming technique, or another just barely acceptable trait is giving everyone the jels ("have mercy Kevin, your false eyelashes are murdering our will to live, an asset that is literally necessary for life, which is technically murder" could inspire Kevin to dim his own light to let others shine for once, since absolutely everything is able to kill someone's will to live, if you just wish on a shooting star & believe & if theyre a total dweeb hurt by everything)
    - victim complex, convincing them there are actual problems, and that they have actual power to fix the illusory problems they think they're a victim of ("oh my god, are you ok? why is the red cross trying to help sex trafficking & genocide victims, when that cake just forced you to eat it?" could alert Pamela to the fact that she does in fact have superpowers instead of victimhood, powers like fixing her impulse control (the stronger she'll be to strangle us), or alternatively, her priorities (the better to crucify us with)
    - god complex, convincing them that they are not in fact allowed to murder other people who happen to exist, which they take as an intentional malicious reminder about their own mistakes ("no Lisa, someone else having something out of your intellectual price range is not justification for altering international human rights terminology just this once to justify what you did to them" could be the wake-up call Lisa needed to acknowledge her own limitations)
    - judging addiction, where they act like anyone trusts their judgment and thinks they're incapable of hypocrisy ("sure Chrissy, we'll punish all your victims youre jealous of who did nothing to you, we totally trust that you're not jealous of them, since you only told us to punish them bc you want the best for us, in your infinite wisdom, not bc you think that if we can't interact with them, we'll someday start to think you're great")
    - lying addiction, where they say stuff that is clearly false bc they think its "play pretend" time all the time and pointing out lies is "violating the social contract" in some way ("sure Chrissy, youre saying we should all have kids bc you think its the right thing to do, not bc you want every woman's body to be ruined" might trigger a thought in Chrissy's mind, like that she might not be great at hiding her intentions, which are a shade less complicated & interesting than she claims)
    - self-nominating sainthood of their own religion, where they act holy & concerned about their homicide victims so they make sure to check their email every day in case today is the day they need to act sad about their victims' deaths ("sure Martha, that bake sale almost ended all oppression & inequality - it's like you invented something that changed the world and made all problems trivial to solve, but slightly different in that your strategy didn't work bc people persecuted you for being better than them (at pretending decorating cookies is the only thing that has ever mattered)" might alert Martha to the possibility of other strategies to seem good, like actually being good)

  - finish poc & give other examples applying different workflows

  - apply concept of risk 

    - risk: adjacence to negative events (error types)
      - risk structures: 
        - cascading risk
        - compounding risk
        - interacting error types
          - adjacence of an error type to another error type
          - adjacence of input/output & other interaction formats enabling interaction

    - solutions to risk:
      - distributing errors or otherwise ensuring they cant interact
      - making sure if an error occurs, its at a dead-end trajectory where its side effects dont impact the system
      - distributing info sources to gather info on imminent risks (robot that can distribute a set of sensors to pick up signals it otherwise couldnt, like behind opaque objects)

    - when should errors be allowed to continue: 
      - when they dont impact system functionality, dont interact with other errors to form cascades/compounding errors, and provide useful signals of unhandled variance
    - when should motion be allowed in the direction of risk (risk of error types): 
      - when uncertainties exist between alternatives

    - flexible abstract/conditional/temporary error definitions to allow for beneficial errors & error-correcting errors
      - example: 
        - 'two wrongs make a right'
          - when a robot instructed to go in a direction is forced off its trajectory by the first error, it has to make another error to get back on track, if an error is defined as 'motion in any direction different from original planned direction'
          - a solution is a definition of error types that is:
            - abstract: any intentional motion that brings robot nearer to its goal is not an error
            - conditional: any motion to correct an external error is not itself an error
            - temporary: motion in a direction different from planned direction sequence is not an error in some temporary contexts
  
    - error type 'low-dimensionality' signals:
      - when motion approaches the solution metric (avoiding the error classification of not equaling the solution metric value), but never reaches it
        - example: 
          - vertical dimension: robot fell onto another level vertically but is still moving toward destination as planned
          - alternative agent/force dimension: robot fell onto truck and is moving toward planned destination temporarily
          - time/speed dimension: robot encountered barrier preventing it from reaching its planned destination in under the planned time limit

  - self-explaining AI test: able to identify metadata that align with its decision path, like:
    - thresholds
    - alternatives (selected & unselected based on thresholds)
    - testing points (gather info about relative value to threshold)
    - types/clusters
    - examples
    - statistics like average examples within a type

  - give example of selecting problem/solution format
    - examples: 
      - every problem can be framed as 'reducing solution space', but some problems are more adjacent to this format than other problems, such as:
        - 'find the one item in the set that matches the filter value', which is more adjacent to 'reduction' operation because it involves a solution output format of a lower quantity than the original quantity, specifically a quantity of one, which implies that the original quantity is greater than one, given that this is framed as a problem that is not solved yet
        - problems have many possible formats, so an initial problem to solve is 'reducing the solution space of possible formats to the one most adjacent format'
        - the correct format is important to find, bc some formats will make the problem trivial to solve or solvable with existing methods
      - as another example, a prediction function can be formatted as a problem of:
        - finding causal network of variables (root/direct cause in structures of inevitability, lack of cause in interchangeable alternates)
        - finding variable network connected with functions (apply 'randomize' to root cause variable, then apply 'specialize', then apply 'standardize')
        - finding variable structure network (boolean causing vertex variable causing spectrum variable)
        - mapping variables to influencing & interaction power (to influence & interact with other variables)
        - isolating & filtering variables in data set by impact/contribution, filtered by probability of coincidence (coincidental structural similarity between independent variables & actual causative variables, leading to secondary structural similarity in apparent relationship to dependent variable)
        - finding coefficients of variables in data set
        - standardizing data set to a subset of variables (like a vertex variable) so core/unit functions can be applied
        - inferring other variables not present in data set
        - allocating randomness to explain lack of predictive power of independent variables & changing prediction function state
        - finding the data set's distortion from a base/central/standard data set having those variables
        - finding the probability of a prediction function given a data set (or vice versa)
        - finding a line/cluster/point (or generalized structure) averaging the data set relationships
        - finding concepts & other interface objects in the data set (concepts like 'power' relevant to predictive/influential potential)
        - filtering data set by which data can be ignored (outliers, corrupt data, randomness, worst/best case, prior outdated data)
        - finding a statistic representing target solution info
          - does 'average' represent the relevant solution 'prediction function' that is best able to predict y across adjacent/derived/given data sets, or is there a better statistic, like:
            - 'weighted average'
            - 'subset average sequence'
            - 'emerging average given state data'
            - 'derived average given randomness injection'

  - math is a connecting interface of abstraction & structure bc it maps fundamental structures to abstractions
    - math describes info (stabilized structures) 
    - what structures have stabilized in the math interface, so math can be applied to describe stabilized structures of math?

  - finance: assess value of prior work by work that is still relatively valuable, incentivizing new work

  - alternative intent coordination & compatability of metrics

  - calculating interactivity by coordinating/adjacent/convertible structures

  - rather than learning (applying new info to update standard equalized or randomized structure), apply structural insight paths that frequently produce accurate task completion (in general like producing problem/solution format connection sequence, specifically like producing prediction function)

  - associate error types (with interface metadata like intents, causes, structures) with problem & solution types, to identify connections like:
    - what errors can be present in a solution that can still be considered successful
    - what errors are considered a problem or equal to the input problem when combined in a structure
    - iterate through possible interface definitions of problem/solution
      - problem :: solution 
        - general connecting function:
          - sub-optimal state :: more optimal state
        - specific problem/error type connecting functions:
          - state with errors :: state with fewer errors
          - state with unused resources :: state with fewer unused resources (unnecessary dimensions)
          - state with no possibility for change: state with possibility for change (randomness injection points, variance sources, dependencies)
          - distorted state (specific intent) :: undistorted state (center)
          - state where organization is a dependency source (too big to fail) :: state where organization is an efficiency source (solution provider)
          - specific solution for specific parameters/values :: abstract parameterized solution
          - mismatched format :: matching format
          - misaligned intents :: aligned intents
          - info dependency :: info generating function dependency
          - unknown cause :: set of possible causes of varying directness
          - state with inability to self-correct :: state with self-correcting function
          - state with inability to interact :: state with core functions to build interaction function & function to change interaction level
          - lack of chaos :: variance injection, variance source
            - when a system has no errors, that means its either not finding new variation (unlikely if capable of doing so), not capable of finding variation, or is not learning
              - inject errors to try to produce variation
              - apply function to build functionality to find/generate variation
              - apply errors/changes to learning functions to produce new learning functions
          - structure :: different structure
            - direction :: position
              - goals (result, impact, resource) :: flexibility (increase in function, increase in power)
          - missing structures (sub-type of opposite structures, sub-type of difference structures)
            - lack of structure :: unnecessary structure
          - sub-optimal solution :: improved solution
          - solution set :: optimal solution
          - decision options :: executed decision
          - lack of decision :: decision options
          - lack of power :: locally concentrated power
          - too much (concentrated, high density, unnecessary, unmanageable) power :: globally distributed power
      - apply error & problem types to generate other possible definitions of a problem & solution, allowing functions connecting them to be built/stored specifically for those types
      - apply system optimizations to all interface components 
        - example: 
          - apply 'have multiple variance sources' to 'variance sources' for intent 'distribute power' of input variance across sources
        - filter optimizations by contradicting intents that are identifiable as useful for functions connecting problem/solution structures
    - apply error types to interface component design/optimization
      - applying error type solutions to functions
        - 'avoiding dependencies'
        - 'avoiding traps leading to dead-end static states where variance injections cant change the system'
      - to avoid the associated error types:
        - 'missing dependencies', 'cost of generating dependencies'
        - 'lack of flexibility', 'lack of potential', 'lack of functionality'

  - apply definition of errors as structures of difference (what is not correct, meaning different from correct) to generate error types (structures of difference, like stacking variable permutations/distortions or generating new variables) and error patterns
    - create error types of ai using core combination generative function
      - includes generating error type structures (combination of error types)
    - identify error type patterns (when differences accrue in this pattern, an error of some type is likely to occur)
    - create ai algorithm that is different in some variable from error type algorithms to guarantee an algorithm without those known error types
    - identify interface queries (or ai algorithms) that generate error types to use as filters to differentiate & guide design of new queries algorithms
    
    - example of error type in structure: 
      - any distortion can be used as an asset, & every position has an error inherent to its structure
        - for example:
          - 'occupying the center position' has:
            - errors: having to do extra work to get to a position where it can handle less adjacent (outlier) problem types
            - advantages: its work is distributed among many positions in every direction (many positions are adjacent) so if the problem is solvable with an adjacent position, and encountered problem types vary a lot, the center has an advantage
            - 'a position in between most common error types' is another similar position that would have an advantage inherent to its structure, with the cost of having to do more work to get to a position where less adjacent error types are solvable, the less adjacent error types being more common than adjacent error types, but still less far than the average cost from other positionss
          - 'having the most power' has:
            - errors: intent of 'requiring dependency', inability to delegate, over-responsibility (imbalance in blame allocation), boredom
            - advantages: freedom in movement/change, ability to handle stressors, ability to make decisions that favor itself or its goals
      
      - how to derive the error type from this distortion structure 
        - distortion structure: 
          - 'too far in the direction of power centralization'
            - associated objects (inputs/outputs) to components (power)
              - with power centralization (power being at least an input to everything), other things are also centralized, like inputs/outputs/sub-processess of power (responsibility, decisions, dependency)
          - 'too central to reach outer positions quickly'
            - variables (cost, function, priority) in structures (paths) to similar objects (positions)
              - average cost to reach other positions may be lower than other positions, depending on density distribution or commonness of adjacent positions' associated error types, but cost to reach outer layers would be higher in the absence of efficient connecting functions
                - this error structure can generate other error structures:
                  - bc it cant reach outer positions quickly:
                    - it cant identify/handle external stressors quickly without building functionality to offset that error, like an alarm system to get info to the center faster
                    - it cant quickly generate new outer positions like it can generate new adjacent positionss

  - finish applying structures of concepts into algorithms
    - apply structure of time (state) into algorithms (network state algorithm)
    - apply structure of hypnosis (multi-interface alignment) to algorithm (hypnotized algorithm is static & cant learn, which is an error type)
    - apply meta structure to algorithms
      - an algorithm that cant see its own error types is one that cant:
        - change its perspective/position
        - change the variable creating the error type
        - receive negative feedback for errors
        - apply negative feedback to correct structure (like direction)
        - identify costs (indicating why its an error, as in what resource is lost)
        - structures that depend on the outputs of their distortion, becoming dependent on their distortion
        - structures that cant develop a function to correct the error (a power source that cant develop a power distribution/delegation function)
    - organize list of structures required for system optimization & make diagram & generative insight path & query
      - concepts
        - anti-chaos structures (organization)
        - lack of requirements (dependencies): an optimized system operates in a self-sustaining, self-improving way with as minimal requirements as possible with existing resources like functionality, and with decreasing requirements over time
        - multiple alternatives
          - example: having multiple definitions of cost avoid errors like 'lack of flexibility due to over-prioritization of avoiding costs like pain' and instead be able to sustain one cost type to reduce another cost type, for a duration like 'as needed' or 'while advantageous'
        - anti-complacence structures (checking for new error types that cant be measured with existing tools yet by always building new measurement tools)
        - other structures for optimizing systems
          - anti-complexity
            - apply filters to remove information that is repeated without value added
          - anti-trust
            - apply tests regularly to system components & structures of them, checking it for new variance sources & error types as well as known sources/types
          - anti-dependency
            - apply solutions to optimize system that increase similarity of components in the direction of independence, distributing functionality across components (like cross-training)
          - anti-static
            - add solutions that dont remove possibility of generating other solutions/error types (thereby reducing the variation the system can handle)
      - functions
        - apply error types to check a system for known optimizations (error types like 'structures that seem similar but are not')

  - use exclusively ai with known biases & error types so output can be corrected with the associated solution type
    - algorithms that dont have a mechanism to offset/correct biases from data can be used with a correcting function to improve output likelier to be an error type
    - this is an interim solution (algorithm/model + correcting logic of error types) while other algorithms are tested
    - every algorithm has limitations - those with known limitations can be an asset in some problem types (like to discover biases in data), while other algorithms with unknown limitations can be an asset in other problem types (like to add uncertainty or delegate responsibility for unfair decisions)

  - integrate related AI models within the same network of variables to derive functionality 
    - if you have an AI model to predict relationships between causally sequential variable sets A/B/C, D/E/F, and G/H/I, apply the connecting variable set models (A/B/C and G/H/I) to create a logical function to connect inputs/outputs of the D/E/F variable AI model (or a subset of variables), as a filter to streamline output of the model in response to changes to relevant to data & variables
      - this is an alternative to training a model on D/E/F in isolation, & updating it by training it on new data received
      - not only should it be able to identify the dependent variable value with accuracy, it should be able to fit into the network of relevant variables, connecting other related variable sets (A/B/C and G/H/I)
      - this may offer the possibility of skipping modeling D/E/F at all, if A/B/C is predictive of G/H/I on its own in the target range of accuracy, but at least will offer extra validation potential of D/E/F model inputs/outputs, and may provide core functions/structures (generalizers, thresholds, filters) to use in building the reduced stabilized logical form of the D/E/F model on-demand
      - answers the questions:
        - 'given A/B/C and G/H/I models, does D/E/F make sense'
        - 'can a reduced stable logical form of DEF be derived from ABC & GHI, and does this logical form match DEF, and to what degree'
        - 'why a variable is useful to predict another', not just 'if a variable is useful to predict another', since knowing 'why' will help the model adapt when that reason changes

  - apply insight path to solve problem of 'finding factors to produce number without using multiplication of every combination'
    - insight path: use filters to reduce solution space instead of generating solutions (such as by identifying metadata of solutions & applying combinations of those attributes)
    - problem: find factors of 28 without using multiplication of every combination (trial & error)
      - factors of 28: 1, 2, 4, 7, 14, 28
      - remove: 1, 2, 14
        - divide by integer unit 1, divide by 2 bc even, divide by co-factor of 2 which is half (select midpoint without multiplication))
      - the remaining candidates are: 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13
      - apply filters to solution space
        - apply similarity of value structures as a filter
          - adjacent items can be ruled out by proximity (for example, 13 couldnt be a candidate bc its too close to 14 to be a factor of such a small number)
            - the remaining candidates are: 3, 4, 5, 6, 7, 8, 9, 10, 11, 12
          - apply similarity (of adding factors to sequence) as a filter
            - test sequences for adjacent computations
          - apply similarity of components (factors) in definitions (numbers definable in terms of their factors) to find relevant structures
            - test primes which are relevant bc of their definition being definable in terms of the factor standard
        - apply output patterns as a filter:
          - multiples of 10 and 5 can be ruled out bc it doesnt end in zero or 5
            - the remaining candidates are: 3, 4, 6, 7, 8, 9, 11, 12
        - apply combination structure to produce solution format (multiplied pairs of factors)
          - pairs are a combination structure
          - the remaining factors can form pairs, which can also have filters applied
          - apply filters to pairs 
            - apply output requirements
              - metadata of the output, 28, includes that its an even number, so multiplied pairs must produce an even number
                - odd number x even number can produce an even number
                  - 3 x 4, 3 x 6, 3 x 8, etc
                - even number x even number can produce even number
                  - 4 x 4, 4 x 6, 4 x 8, etc
              - apply reduction tests (what could not be the solution)
                - apply tests to inputs
                  - inputs must be spaced according to the output number
                    - adjacent numbers are unlikely to produce the output number (as a multiplied pair) for an increasing output number
                  - structures of inequality (not equal to solution)
                    - too large
                    - too small
                    - not even
                  - identify threshold structures (values) of input structures (values, value pairs) that would produce one of these inequalities
                    - filter out inputs if they would produce an output that was too large to be 28
                      - 28 is quite a small number so pairs of numbers above a threshold value (3 x anything above 9, etc)
                      - some pairs are clearly too big to produce 28, without checking the product
                        - 11 x 12 is clearly too big, so can be removed from list of possible pairs

  - concept of attention in structures
    - mixed interim high-variation & high-similarity structures tend to maximize attention

  - examine conflating intent & requirement

  - insight path example

    - when generating solutions, identify:
      - context/case/condition that can filter it out
      - variables that can generate the most solutions
      - filters that can filter the most solutions
        - apply filters to solution space by solutions that are ruled out in fewest cases, best cases where solutions are less required or least probable cases
    
    - example: how to put shirt on underneath jacket without taking off jacket completely
      
      - alternative queries

        - identify sub-problem: 

          - find a format where sequence (shirt on top of jacket) can be changed into solution format (jacket on top of shirt)
            - identify adjacent format 'bunching into circle around neck' that allows changing sequence (which is on top) and transformation function into that format from origin format 'taking off sleeves'

        - apply adjacent formats to problem & solution formats
          - identify formats that have a sequence (stack, row) which is a structure implied in the solution format ('underneath')
            - apply functions to test if shirt can be transformed into one of those formats

        - generate adjacent functions (bunching) from core functions (move sleeve, lift, rotate) & try them to see if any useful structures emerge moving objects closer to solution formats/positions
      
        - generate default connecting function and apply structures of optimization (reusing functions, avoiding extra steps) to improve the default connecting function incrementally

        - identify tests that can filter out solutions

          - identify tests interacting with structures of variables (change types, potential, uncertainty) & constants (requirements, limits, definitions)
            - possibility test: 
              - interaction test:
                - in what ways can the shirt/jacket interact
                  - can the shirt occupy position (fit) under the jacket
            - requirement test: 
              - does the shirt/jacket have to stay in its current position/format
              - does every step of functions ('removal' function) have to be executed (can you just remove pieces, like the sleeves, without removing the whole thing)
            - change test: 
              - in what ways can the shirt/jacket be changed while remaining a shirt/jacket (bunching, removing sleeves) 
              - are these ways reversible (can it be put on after being taken off)

          - apply tests to reduce solution space
            - solution can involve variables:
              - position 
              - format
              - change functions (bunch, lift, remove)
              - components (sleeve)
              - interaction functions (stack in sequence)
            - solution must fulfill requirements
              - jacket must be in 'worn' position at all states
              - change functions cant change object identities (change jacket into shirt or into a not-jacket)
              - solution must reverse sequence of objects in stack structure)
            - any solution involving removing the jacket completely in a state, change functions that change object identities, and where solution format is not fulfilled are ruled out
            - other tests include:
              - minimize steps (did solution do any unnecessary steps)

  - alternate insight path direction types
    - generate solutions from problem statement using interface objects
      - core functions 
      - mixes/changes of previous or abstract solutions
      - insight paths (break problem down, trial & error, etc)
      - system structures
      - core structures (opposite, equal, adjacent)
      - function input/output chains
      - vertex variables
      - conceptual structures
    - apply solution format and reverse engineer solution
      - apply solution filters that are adjacently derivable from problem/solution metadata (most-reducing filters that rule out the most solutions)
    - apply both the generate solutions method & solution format method and connect them in the middle

  - example of deriving solutions (from https://www.quantamagazine.org/mathematicians-inch-closer-to-matrix-multiplication-goal-20210323/)

    - original solution (apply method to smaller matrices) applies:

      - core structures: 
        - meta (matrix of matrices)
        - subset (sub-matrices)
        - substitute (addition for multiplication)
      - core functions:
        - apply substitution method to subset once matrix is formatted as a matrix of matrices
          = apply(substitution_method(format(original_matrix, 'subset')))

    - how to generate other solutions

      - multiple queries to arrive at the same solution of 'finding adjacent interim values & re-using multiplication operation, in case where adjacent interim values exist in a matrix'
        - you can start with the target solution formats as your interface query filter (equating "problem format + operations = solution format")
          - a more efficient operation than multiply
          - a more efficient combination of operationss than 'multiply then add'
        - or you can start with applying interfaces, and iteratively focusing on & applying useful structures found for the solution (problem-reduction or problem-compartmentalization)
          - apply structures known to generate solutions to fulfill solution metrics (move toward solution position or reduce solution space or reduce problem)
            - apply core/adjacent/efficient/similar structures

      - apply structural interface

        - apply core structures of structural interface
          - apply structural similarity to structures of problem (including value)
            - similar values enable addition instead of multiplication (multiply 5 * 8 & subtract/add 8 instead of multiply 4 * 8 and 6 * 8)
            - if there are similar values in a matrix, and storage is allowed, this can reduce multiplication count (ignoring storage search)
          - apply adjacence structures
            - find values adjacent to matrix values to find similarities in computation requirements
          - apply similarity structures
            - find values in matrix having a common factor (base) and standardize operations involving those values
          - apply sequence structures
            - find sequences in multiplication operations & apply sequence operations rather than individual calculations
              - find numbers in even number sequence (common factor of 2) and reduce to addition of coefficients of powers of two
                - 3 * 5 + 2 * 6 + 2 * 4 =  3 * 5 + 2 * 2 * 3 + 2 * 2 * 2 = 3 * 5 + 3 (2^2) + 2 (2^2) = 3 * 5 + 5 (2^2)

      - apply function interface

        - find functions that convert multiplication to addition or other lower-cost problem
          - replace/substitute
            - identify when multiplication can be replaced by addition
              - addition can replace a multiplication, if an adjacent multiplication has already been done
            - convert numbers to efficient multipliers like powers of 10 that involve moving digits rather than multiplication

      - apply core interface

        - apply core functions (replace) & core structures (unit) to problem components (problem functions of multiply & add)
          - apply interface interface (standardize problem to interfaces of problem space)

          - apply system interface
            - apply system structures
              - apply efficiency structures
                - identify efficiency structures in problem 
                  - inefficient operation (multiply)
                  - efficient operation (add)

                - apply change interface
                  - connect an inefficient function (multiply) to an efficient function (add) to change inefficient function to efficient function
                    - define one problem function as a transformation of the other problem function
                      - define multiply in terms of add using core functions/structures or problem functions/structures
                        - apply replace to one unit of original multiplied values with an add operation until multiply is defined in terms of add (standardize to add interface)

                - apply efficiency structures
                  - apply efficiency structure 'apply one operation instead of multiple operations'
                    - identify when multiple multiply operations can be replaced with this type of adjacent multiply/add operation 
                      - identify when a multiplication operation can produce an interim value in between other values so the multiplication can be re-used for another value

          - apply structure interface
            - apply structural interface structures
              - apply filter structure
                - identify matrix cases where these operations are inefficient or unusable
                  - identify operations/information needed to determine inefficiency/unusability of this solution
                    - apply function to determine threshold value for matrix dimensions or metadata like value variability (if values are in a known range or have a known type): 
                      - 'if there are more than x adjacent values with an interim value in a matrix of size n x n, this method can save computation steps even with the determining operation'
                    - add average cost of determining operation to cost metric (computational complexity)

      - apply system interface

        - apply system structures
          - apply efficiency structures
            - apply efficiency structure of 'reusing existing resources'
              - identify what resources exist or are created in original solution (values output by multiplication & addition operations)
                - identify condition where these can be reused for other operations
                  - when other operations are adjacent
          - apply symmetry structures
            - apply symmetry structure of 'interim value one change unit away from multiple values - one being addable in the position of a coefficient'

  - add intent metadata to interface query examples 
    - 'find combine structures after applying system interface - to find connecting structures in problem/solution system'
    - the intent of a sub-query should be defined in terms defined on that interaction level, to avoid gaps in connecting structures across sub-queries, so that further sub-queries of the sub-query can connect to the original triggering interaction level intent
    - example: 
      - when solving a problem with an insight path like 'break problem into sub-problems', the sub-queries to solve each sub-problem should be defined in terms used by the insight path & problem statement
      - a sub-query to solve a sub-problem like 'reduce & isolate dimensions of problem' should be defined using the problem statement components & the insight path ('break' as the original function mapped to sub-functions 'reduce' and 'isolate'), so when it comes time to integrate sub-solutions into a solution, the corresponding opposite function to 'break(problem)' can be applied to 'integrate(solution)', using a version of 'integrate' such as a specific version of 'merge' that connects to the version of sub-functions of 'break' used

  - applying non-standard methods across systems

    - example:
      - applying concept of 'bias' used to fulfill intent of 'creating a truth filter'
        - bias is usually used to evaluate intentions of agents when interacting with other agents with some level of variance in agent identities
        - after abstracting intentions as decision/function triggers:
          - apply bias as a truth filter to determine non-agent change/function triggers
          - this can work bc even components without agency respond to incentives bc of their common tie to physics
          - bias also interacts with the concept of randomness & randomness can explain false info signals, which connects to problem-solving intent of identifying truth

    - queries to generate insight path to find useful structures to apply across systems, for an intent like 'truth filtering'

      - apply insight paths to generate insight paths

        - find structures with 'truth filtering' intent in solution (source) system
          - map system components across systems (map 'truth' in agent system to 'correct' in non-agent system, match 'intent' to 'incentive' bc non-agent systems always respond to incentives)
            - map connecting structures in source system to connecting structures in target system (what connects bias function in source system vs. corresponding connection in target system)

        - apply components of structures with 'truth filtering' intent across systems, to equalize problem (target) & solution (source) systems
          - apply metadata of 'truth-filtering' structures (bias) from agent source system to non-agent target system
            - apply bias/interface metadata (intent) to target system components
              - find intent ('reasons') for 'randomness' (find the change interactions producing false or temporary randomness in non-agent systems)

            - apply bias interface objects (intents/reasons to use biased rules) to target system components, due to commonness in intents across systems
              - bias intents/reasons: over-simplicity, lack of storage, lack of change type functions (update functionality)
              - 'if an info signal has bias intent signals (if its clearly caused by lack of storage), classify it as a potential false info signal (request from pathogen rather than from host cell, false electrical signal, illusion of an electron count)'

      - standard interface query 

        - apply structural interface
          - identify connections between structures in problem
            - problem: 'find true info in agent-based system interactions despite agent incentives to send false info & intentions/decisions to do so'
            - problem structures:
              - concepts: 'truth' (intention matches decision output = 'successful decision'), 'agency', 'incentive', 'intent', 'decision'
              - functions: 'interaction functions', 'decision functions'
              - other structures: 'decision function triggers', 'false info', 'true info'

        - apply combine function to conceptual interface
          - create combinations of abstracted versions of structures
            - problem: 'find true info in system interactions despite incentives to send false info & other sources of false info & change functions enabling that'
            - problem structures:
              - concepts: 'correct' (info implication matches its impact), 'incentive', 'change', 'randomness'
              - functions: 'interaction functions', 'change functions'
              - other structures: 'change function triggers', 'false info', 'true info'

        - apply connect function to abstract structures
          - find structures that connect abstract structures (randomness, false info, change/function triggers) without the specific attributes tying them to one system (agency)
            - test whether the connecting structures fit with the new system after removing attributes:
              - can bias be used to filter out false info or find true info in chemical interactions, despite elements not having agency, as an abstracted way to decompose randomness/noise or complex systems
                - for example, can an abstracted version of bias structures correctly model the integration of quantum physics with chemistry rules to explain some chemical phenomenon

  - finish core function structure example diagrams

  - vpn's & proxy servers to enable protection functions (viewing science research or medical info) by 
    - automatically finding countries with laws allowing those protected functions & routing requests from servers in those locations
    - finding a version of an app that has target features so user is directed to download for that versions

  - successful AI would identify multiple solutions as probably successful once variables of inequality were identified

    - query: identify 'value' as the vertex variable

    - query: identify input variables determining value: 
      - location
        - what is a low-method to change location: public transportation
        - what is a barrier to change location: visa, lack of info
      - proximity to supply chains
        - make an alternative supply chain between high-traffic suppliers/demands in other direction (across continent rather than across an ocean)
      - relevant cost ratios (cost of going somewhere, finding job, selling something, finding info)

    - query: find methods to transfer resources
      - temporary markets (tasks that will probably be automated within n years, markets for goods people probably wont want/need in n years, or only need once, or only while a law is applied that will be changed soon, or products that need a connecting product until theyre all invalidated by another product being built)
      - supply chains
      - transportation
      - delivery services

    - query: find relevant interfaces
      - laws
      - code
      - resource distribution
      - location

    - query: find solution methods
      - connect existing resources
      - apply multiple high-difference solutions, vary them to find subsets & versions that work

    - query: find lowest-cost combination of solutions
      - finding highest-value public transportation infrastructure to build (what routes would allow low-cost resource transfer for the most agents)
      - finding temp markets (delivery/resource-sharing/education services)
      - finding adjacent/existing law combinations to benefit the most low-income agents
      - finding adjacent/existing bugs or code loopholes to benefit the most low-income agents

    - query: organize info into a combination solution
      - example of a combination solution, integrating multiple relevant interfaces, solutions, covering a high ratio of input variables to vertex variable
        - 'investing in delivery businesses near planned supply chain routes offering a high-traffic alternative route, and relocation or transportation infrastructure to enable lower-cost market participation with subsidized education for delivery workers to help them get better jobs and leave their jobs open for immigrants'

  - consciousness as choice to move between neural nodes (rather than being directed) required:
    - the development of alternative node paths performing equal/similar functions, requiring:
      - the development of excess resources, delaying required decision time (making immediate decision unnecessary, avoiding a forced decision), requiring:
        - the existence & application of previous efficiencies & functions for alternative evaluation, energy storage, storage-checking, & energy requirement-identifying
    - the cause could be framed as structures such as an 'efficiency stack' or 'energy maintenance functions' or 'alternative options' or 'navigation/motion control' or 'lack of requirement/need'

  - example of applying interfaces to derive metadata about a component
    - apply causal interface to identify connecting function 'power is responsibility' (also an insight)
      - power can be defined in causal interface components as 'causative potential' (its the input reason for change in a system, including changes preventing changes)
        - given that it has structure 'change input', its also a source of other change types than intentionally triggering the correct function (errors, side effects, changes to errors)
        - changes to fix errors are related to the concept of 'responsibility' (definable as 'work that isnt incentivized but is necessary')
    - apply structural interface to identify connecting function 'power is responsibility'
        - 'aligning error & fix sources' also corrects the 'power source distribution imbalance' error, which is another way to derive this insight, using the structural interface (correct distribution imbalance with alignment)
        - identifying the 'similarity' (a core component of structural interface, applied during a standard application of interface) in the 'direction' structure, between power & side effects (including errors) as similar to the direction between power & fixes
        - identifying connecting functions positioning power as an input/required structure to fixing errors:
          - identifying that 'fixing functions' have an input trigger requirement like any other function, and function triggers therefore have power to fix errors
          - identifying that if something can generate a 'fixing function', it necessarily has power
          - identifying that if power is necessary to change a structure, by process of elimination, nothing else could fix an error

  - connecting function of math/logic 
    - a problem like the following is a logic problem ('find the logic connecting this input/output') that can respond to the general solution workflow (given a problem input format of a 'function' to check possible solutions with) of:
      - 'identify the unique correct solution in a solution set to a problem of equalizing the sides of this function' 
      - 'identify which solutions are not correct, reducing the set to a size of 1'
      - this can be converted to a math problem of:
        - iterating through solutions
        - checking each solution to see if it solves the problem ('equalizing both sides of a function')
        - removing it from the solution set if not
        - otherwise checking if the set of possible remaining solutions has a size of 1 yet to give a success signal
        - continuing iteration if not
      - the connection between these interfaces is in the structure of logic (math being structural information in core terms like numbers):
        - the set iteration has a 'sequence' (set, progression) structure
        - the remaining solution set size has a 'integer' (set, progression) structure
        - the success signal & the continuation condition has a '0/1' (core alternative) structure
        - the solution test has a 'function' and 'equal' structure (are both sides equal yet)
        - the remove operation has a 'subtraction' structure
        - the continue operation has a 'sequence' structure
        - the condition component has a 'direction' structure (change direction in logic network/tree) and 'multiple option' structure (a decision between differing & mutually exclusive options must be made)
        - the check/test operation has an 'equal' & 'inject' structure (inject variable values to see if both sides are equal)
        - the logic function has a 'directed network' or 'tree' structure (follow directed relationships between function components)
      - apply structural interface to connect logic & math structures:
        - 1. some of those structures have structural relationships which should be identified by applying interfaces, like structure (including components like the similarity concept)
          - similarity:
            - the similarity in structure between the solution set size & set iteration (a progression or sequence) is relevant, bc the iteration & the set size should:
              - move in opposite directions
              - equal the original set size when added
          - by applying the structural interface (with components like the concept of 'similarity'), the query can identify this relevance by checking if an adjacent connecting function between the similar structures exists & is relevant to the problem/solution
            - generate core functions & generate combinations of them, applying them to problem variables being examined for a connecting function (solution set size & set iteration)
            - filter by those applied core function combinations that move/change the problem (converted into a solution space, once identified) to be more similar or closer to the solution structure (solution set of size 1)
          - direction:
            - given the sequence & other direction-related components/attributes/structures of the problem, the input problem components & output solution structures can have a position structure applied
        - 2. given that the solution format is a 'set of size 1', and the input problem format is a 'set of size greater than 1', it can be derived that:
          - when executing problem-solving method, the method should include a step where:
            - an item(s) is removed from the set 
          - this connecting function between problem & solution format derives the solution requirement of the 'remove' operation (without being explicitly told to include that operation in the problem definition)
          - given the other structures involved (integers, iteration sequence), it can also be derived that the remove operation should apply a subtraction operation rather than another structure like division, which would introduce other less relevant & adjacent formats like non-integers
            - this applies problem-solving insight paths like 'adjacent solutions should be tested first in an absence of reasons to do otherwise', where reasons to do otherwise could be metrics like system complexity, info about adjacent solutions failing in that system, info about non-adjacent solutions succeeding in that system (info about non-adjacent solutions being optimal for a system metric)
          - interface query design should involve queries to check for inputs to a step given required sub-query tests for alternatives
            - before applying a step, apply its required sub-queries to test for its alternatives, like for an adjacent solution step, checking that alternative non-adjacent solution sub-queries have returned no contradictory info indicating an adjacent solution should not be applied

  - examine similarity (alignment/overlap) structures between 
    - different components (when an error type is an incentive, or a function used for other intents)
      - contradictory/opposite components

  - decision between selecting insight path/query for a problem & generating a new one is dependent on:
    - problem metadata (complexity, adjacent formats)
    - available info (whether metrics are capable of capturing relevant info)
    - input data set metadata (whether variables are output metrics, variance-covering metrics, proxy variables, etc)
      - different input/output relationships will imply different interface queries that will be useful
      - beyond that, other (interface analysis-identified) methods to design an interface query for a problem type
      
      - apply interface analysis to interface query design (system including interface components, query components, metrics) - apply interfaces to the problem of designing an interface query
        - examine what are the core functions, efficiencies, incentives, error types, etc of the interface query system, and check that they match what Ive identified
        - check if you can skip some interfaces, like when you start with an input containing mixed-interface (concepts, functions, intents) or cross-interface structures (structures that apply/generalize to or connect interfaces), such as when you can identify common terms in input component definitions that can be used to frame all relevant objects
          - once you standardize terms of component definitions, is there an interim sub-interface youve standardized components to, which can be used in place of a full interface query
        - example:
          - adjacent formats: 
            - problem is route optimization, problem format is network, solution format is network path, interface query should include function interface, bc function format is adjacent to finding a path on a network
          - intent alignment:
            - problem is over-complicated system, problem format is network, solution format is reduced-complexity system network, interface should include math & structure interfaces, to find & apply dimension-reducing functions (interfaces already contain functions that align with 'reduction' intent)
          - required inputs:
            - problem is 'find a relationship between functions for calculation optimization intent', solution format is 'connecting function', interface query should involve 'connecting' functions, which are a required input to solution format of a 'function to connect functions that optimizes calculation efficiency'
        - this can optimize for problem/solution metadata, as well as general problem-solving methods
          - optimize for problem type: interface query for 'missing information' problem type should include the 'similarity/difference' sub-interface on the 'structure' to identify 'opposite' structures like 'what is not there'
          - optimize for solution format: interface query for a problem with solution format 'prediction function' should include either causal, potential, change, or function or structure.network interface, all of which can generate a structure connecting the in/dependent variables
            - causal: organize variables with causal diagram having direction & check for predictive ability (identifying correlation, applying causal structures like moving/deactivating variables, using variable proxies or aggregate variables) to filter diagram for probable causation
            - potential: identify potential alternatives (variable sets not in data set, randomness explanation) and filter if possible, possibly leaving original data set as last remaining solution
            - change: identify variable change functions, and evaluate distorted data sets using those functions for alternate prediction functions, filtering by functions that are robustly predictive with more change conditions applied
            - function: index variables as functions (functions using variable combinations/subsets) to check for input/output connectivity potential between in/dependent variables
            - structure: organize the variables as a network to find relationships & if there is a relationship between in/dependent variables
          - optimize for general problem-solving methods: 
            - example: 
              - 'generate set of possible solutions & apply filters to reduce solution space'
                - the interface query should have a format that is filterable once it reaches the filter step of the general solution method
              - 'break problem into sub-problems & combine & merge sub-solutions'
                - the interface query should have a format that is combinable/mergeable once it reaches the combine/merge step of the general solution method
  
  - organize list of structures relevant for intents
    - for 'identify' intent, relevant structures include structures of difference (filters) and uniqueness (unique identifiers)
    - for 'connection' intents (identify/generate connection), a structure where components are only defined in terms of other components (by their relationships to other components), like a network or vector space
    - for 'differentiation' intents, a structure where the definition of difference is clear & applicable (can differentiate all different components)

  - grassroots citizen info tech & citizen journalism as an alternative to top-down govt law enforcement, where centralized/organized govt law enforcement is allocated when a metric threshold for citizen reports is reached
    - the correct position of govt is an automated tool usable by citizens to solve problems (prevent crime, enforce laws, exact justice)

  - example of multiple structural filters to reduce solution space
    - example of where a structural similarity could be used as an initial filter (in a dog vs. cat categorization algorithm)
      - find similarity to type 'dog' and type 'cat'
        - in cases where similarities point to equivalent probabilities for each category, apply additional filtering structures than similarities
          - apply base structures (random, core, common, etc)
            - apply path structures (how many steps from a base to produce a clear answer)
          - apply opposite structures (what is not a cat, what is not a dog)
          - apply filtering structures (both/neither) - (what are cats/dogs both or neither of)
          - apply structures of difference (what comes from a different origin/cause, like causes of evolving dog functions)
          - apply state/time structures (could this become a dog or could it have been a dog previously according to definitive attributes/functions)
          - apply variance structures (does this have variance from the cat base or following cat variance patterns)
          - apply agency/group structures (what groups do cats belong to or which groups are they found with)
          - apply system structures (what contexts normally go with 'cat')
          - apply change/distortion structures (what distortions are often applied to cats or dogs)
              - apply alternative path structures & network structure
                - how many different paths could this data produce a dog category? (how to get to 'dog' answer using that particular data)
                  - apply boundary structures in network (cat type path set or path region, dog type path set or path region)
                    - re-apply similarity structures to boundaries (is this within the cat path region)
                    - apply pattern structures (does this match cat path patterns)

  - usb that stores os/state/processes/files so when you take it out, all thats left is hardware, so secure portable destructible readonly sessions (or sessions allowing only one write at a time, from a user-facing process like a document editor, with regular approval requests to re-authenticate writes or session) can take place
  
  - organize examples of logic for functions (interface query design logic)

  - add functionality (or associated attributes) with components with base/core functions included, components which can be connected with user-defined functions
    - this can add functionality to products to reduce need for producing new versions
    - physical sensors can use communications tech with varying required internet infrastructure (beacons/bluetooth/radio) to integrate with data, computers, physical resources, building blocks of robots
    - physical components examples:
      - use a sensor added to non-electric or non-AI-driven vehicles, pedestrians, & other moving objects on roads (animals, robots) to detect other objects or sensors & help avoid crashes by attaching sensor output as input to steering mechanism with a steering component (interim tech while waiting on market capture of EV & AI vehicles)
        - can also be used to turn a cart or anything with wheels into a delivery robot, to reduce human traffic
        - this can turn the delivery market into a sensor coding market to add functionality/integrations to sensors & the robots or resources controlled by them
      - use a sensor (indicating position to lift away from) as input to another sensor (lifting sensor) with connecting function (fetch position to move away from, direct lift away from position, initiate lift)
        - add sensors with user-defined connecting functions & prioritized sensor functions
          - if a sensor on top of trash can has function "lift" and can take input like "heat motion in range", add user-defined connecting function to another sensor not on lid that the sensor on top can use as a reference point to find direction to move in (away from other sensor) 
    - code components/functions
      - user-defined connecting function like "query regularly for a function that can do this (publish, copy, export, search, build), and when found, add to querying component"
      - find connecting function like 'abstraction' to add functionality like 'handling other inputs' or attributes like 'flexibility' and distribute flexibility to other accessible components
      - hook a search function component up to input component (filters) using user-defined connecting functions (input filters to search on)
      - user-defined connecting function to connect components like core functions/scripts/metrics (when this event occurs in the sensory input function, send signal to trigger other function)
        - this is a way to abstract code (any function that can receive input data of that type) & code connections, delegating execution to code located with queries (find a function of this type or with this input/output) and modularize code as well as making it more connectible
    - task: identify the core functions/components that can generate required functionality for most user intents without introducing security flaws (making hacking devices less adjacently buildable than common legitimate use cases)

  - function-usage-intent::output or demand::supply combination/merging/building/matching functions (alternatively formatted as a solution-finding query for a problem or lack-resource matching function) as an alternative solution to ads

  - finish lists:
    - interface implementation problem type structures (suboptimal interface queries, incomplete definition routes, sub-optimal or mismatched formats) 
    - implementation variables as config options (different generation starting point/source of truth, equivalent/different voting influence in determining interface queries or system optimizations, different constants/derived info/functions, different default interfaces/definitions, etc)
    - core problem type structures (reduction, expansion, organization, matching, standardization, regulation, prediction/derivation (missing info), limit/change conflict resolution, error-to-resource conversion, optimization) & optimal formats & format structures for each
      - optimal optimization formats include network path-finding
      - optimal reduction/expansion formats include change type isolation as shape dimensions after structural assignment of problem attributes
      - optimal organization formats include layered networks & vertex variables

  - add to definitions
    - valid: having reason/logic/info supporting why it would be true, without higher-validity contradiction (info, more valid logic with higher support:contradiction ratio or higher applicability)
    - games (metric optimization, input/output link finding) 
      - games applying system context (metric selection) & function (function building) as a source of interfaces
    - important decisions as changes that are limited, clear & necessary (two options to change direction rather than 360 degrees of direction change options)
      - calculating decision metadata (optimal decision given intent, optimal decision across intents, least risky/causal decisions, alternative/equivalent decisions) can reduce the solution space of likely decisions
      - rather than predicting just using user decision history/incentives, predict using system incentives present in the decision-generating limits & indicated in decision metadata
    - integration/organization structures
      - give example of structures (like alignments, positions, directions) necessary to integrate structures of organization
        - organization structures are methods of organizing information (like neural architecture, information networks, OS)
        - as organization systems become integrated (like "applying an OS to machine learning networks to intercept/replace/validate bio system functionality"), their differences in structures like position and their organization & optimization structures will cause problems that are less predictable than other problems
          - example: applying a 'separation of functionality' structure to folders like occurs on an OS to a machine learning network structure may produce errors like 'separation of function & intent', which can be sub-optimal for tasks like 'identifying optimization metrics'
            - if this is further complicated by integration into a system as complex as the bio-system (for example by using that OS/network organization structure integration to filter & allocate sensory input information), it could cause less predictable errors, like misidentifying a bio system interface

  - add structural queries to insight paths
    - alignments present in security innovations (like alignment in inputs like keys)
    - source of rule development as structures of conflict between forced interactions like change causes & constant structures like limits
      - incomplete inevitability of interaction as a decision structure
    - other examples  
      - group device history authentication: authenticate credit card by proximity to cell phone & continuity applied to user usage history pattern

  - summary of advice

    - invest based on:

      - prioritizing real value signals: 
          - products that fulfill a need
          - companies that self-invest (prioritizing growth by re-investing in growth)
          - leaders that guide by example
          - independence: invest in independent value sources & independence-generating assets, become an independent self-supplier of your required inputs
          - understanding: invest in assets you understand
          - established/proven (long-term) value
          - efficiency: take responsibility for your required inputs

      - avoiding false value signals: 
        - incentives: high-reward/low-cost reasons to invest that may not align with real value or be sub-optimal dynamics that destroy value in a different position of the system 
          - speculation
          - investment banker advice
          - ceo status
          - short-term value changes

  - future value signals

    - problem metrics: current problem-solution ratio, problem to problems-solved ratio
      - solution metrics: 
        - efficiency: sustainability, reusability, cost-benefit ratio
        - independence: self-generatability, self-awareness/regulation/correction
    - efficiency-generating metrics: 
      - integration/organization/optimization/regulation/generalization
    - info distribution (across teams, departments, ranks, customers, other businesses)
    - info proxy derivation/acquisition/building
      - education
      - group membership (do they outsource some processing to groups, and which ones, and how do they evaluate groups like partners/clients/competition)
      - understanding
        - group dynamics (do they help toxic groups improve, do they have a self-destruct mechanism if they become a toxic group, etc)
        - markets (do they organize & optimize markets to achieve their goals quicker)
      - flexibility or learning potential (intelligence derivation/generation/acquisition/building)

  - give example of mathematized insight path 
    - standardize variables to math interface structures & values
      - apply type interface
        - identify types
          - standardize variables with types to differentiated clusters
          - apply difference definitions (like variable subsets) until type separations are clear
          - apply difference types until type separations are clear
      - apply structural interface
        - identify relative difference (difference from reference point, like origin node)
          - apply adjacent structures (vector or spectrum or loop) to variables having the concept of 'opposite'
      - apply causal interface
        - identify causal structures like direction
          - apply structures with direction to variables having causation in their connections
      - apply function interface
        - identify variables with input/output relationships to form path between structures on meaning interface
      - apply concept interface
        - remove randomness
          - compress variables with randomness injections to lower dimensional representations
      - apply meaning interface (using a structural relevance definition)
        - integrate variables in one structure to relate them
          - identify any vertex variables as the preferred variables to standardize other variables to
          - connect variables once formatted using adjacent/interim dimensions like topologies with variable subsets that can act as interfaces between connected formatted variables 
            (can capture info from input & output variables in the connection)

  - functionalize insight paths & integrate functions in optimized program with parameters to select function subset & structure for input problem
    - give parallel/perpendicular insight path examples, for insight paths that add info that the other is less/more likely to retrieve

  - diagram with error types
    - examples: 
      - over-structurization (specification) of an uncertainty/variable (assumption as fact, variable as constant)
      - over-correction of an error
      - over-prioritization
      - over-reduction (over-simplification)
      - over-variability (over-complication)
      - misidentification of minimum info to solve

  - new insight-fitting algorithm for error type avoidance
    - when a new discovery is made, apply insight paths formatted as questions to spot error types before they occur
      - could this violate any assumptions/requirements/dependencies we rely on for other tasks like calculations or applying systems of understanding
      - could this cause cascading (self-sustaining) errors
      - could this cause emergent errors, given other knowledge like probably interactive trends or rules
      - what are triggers of this? what can it trigger with certain interactions, how likely are those interactions

  - diagram with joke types
    - 'annoying when they bring up human rights in a conversation'
      - conversation system context
        - functions
          - change topic 
            - change topic structure (sequence)
              - introduce a topic (first time topic is included in conversation)
          - expected interaction functions
            - criticism of a behavior
              - 'conversation with dictator' system context
                - criticism of power abuse (law violation, specifically human rights violation, which are a related object to dictators)
                  - interpreted as right in the 'conversation with dictator' system context
                    - expected interaction in this context
                      - 'should bring up human rights to a criminal'
            - norms:
              - for low-stakes interactions & interaction errors (manners, annoyance, disrespect)
            - laws: 
              - for high-stake interactions & interaction errors (rights violations)
        - placing a norm (or related objects) in the place where a law (or related objects) would normally go:
          - 'its annoying when someone doesnt let you end the conversation'
            - 'its annoying when someone keeps going on & on about your previous conversations where you ordered deaths of a dissident'
              - 'its annoying when someone keeps going on & on about your previous conversations where you ordered deaths of a dissident for being annoying & then abruptly stops without explanation'
            - 'its rude when someone doesnt let you end a conversation with a laywer interrogating you for war crimes' 

  - organize examples in indexes

  - make efficiency map

  - bias structures: 
    - bias cycle: where specifically/partially false statements are falsely categorized as completely false, which triggers increase in distorted view of the group making the miscategorization error
      - saying a specifically/partially false negative thing about a group often has a partially true sentiment backing it (most people in any group do negative behaviors enough to trigger negative sentiments), so even if the specific negative thing is wrong, the sentiment might not be
      - the lack of acknowledgement of their own negative behaviors by the group saying the specifically/partially false statement also triggers the same response in the group making the miscategorization error (the group saying the specifically/partially false statement is doing a negative behavior, so the miscategorizing group has a negative sentiment about them, and often says specifically/partially false negative things about the group)
    - conflating stereotype ('false statement about a group') with 'a statement about a group that is more true of a higher ratio of that group than it is of other groups'
    - stupidity manifests as similar structures (fulfillment of low expectations) across groups in response to low expectations, leading to feedback loop

  - abstract risk insurance: guarantee a relative position (without specific currencies or amounts), such as 'resources giving a top 10% economic position'

  - finish applying structure to info components (memory components like personality, forgetting, compression, uniqueness, generation, learning, bias, and organization components like relevance, differences, type, abstraction, standards, randomness) to generate full set of neural net types
    - filter by usage intent

  - example of applying structure to components like technologies to find emergent trends

    - tech, standardized to common terms
      - movie: sensory info emotion triggers & info/abstract paths (stories)
      - video game: decision visualization
      - music: audio emotion triggers & info/pattern paths
      - ai: prediction/generation
      - ar: integrate visualizations with real sensory info
      - screen: visualization interface
      - video conferencing: visualization sharing
      - text voting: decision aggregation
      - drug: direct sensory info semotion trigger
      - brain-scanning tech: visualize memories & thought processes

    - multi-player video game voting: applying voting tech of viewers to influence video game tactics/resources/problems/outcomes/decisions
      - generative query: switch input of decisions to another decision-producing tool (audience voting vs. player/algorithm decisions), for randomness/customization/reality integration intents
    - user character customization: applying AI to generate characters of real people or characters from other games to play as other players in video game
      - generative query: switch input of character personality/story with another source of that info, for customization/reality integration intents
    - memory-generated vidoe game: apply ai & brain-scanning to generate a game based on memories
      - generative query: change experience level or skills required (use memory as a tool or test memory functionality), for testing/customization/reality integration intents
    - emotional/sensory alignment games: query for desired emotional path & map a game/video/audio/drug to produce or match that path
      - generative query: change content-creation direction & other variables, from story => emotions to emotions => structure applied to emotion-triggering tools
    - brain-development games: apply AI & brain-scanning to identify missing functionality in brains & generate game to develop that function
      - generative query: use output of game (learning) as input assumption for learning intents using games as intent-fulfillment resource

  - diagram for structures of emergence
    - example: 1-1 input/output relationship up an interaction layer, where extra resources that dont dissolve immediately on the higher interaction layer aggregate & form core structures like combinations, where interactions between combinations & sequences have different dynamics than the individual output interacting with other individual outputs
    - emergent functionality/attributes come from interaction structures (sequences & layers)

  - calculate deaths caused by products/companies by proportional contribution to deaths from slavery & pollution/plastic/additive/medicine & other chemicals

  - generate other interfaces with interface components (connection, requirement, structure, abstraction, set, independence)
    - intent: future direction with benefit to agency
    - cause: preceding inevitability requirement in sequential structure
    - function: structure of task structures (conditions, assignments, iterations) consistently connecting input & output
    - logic: function to connect information using info structures (definitions, inevitability, pattern-matching, exclusive/inclusive conditions, requirements, assumptions)
    - potential: structures like combinations not certainly excluded by requirements
    - change: difference in an attribute value, according to a base (time, relative change, change type)
    - abstraction: general pattern of a specific structure set
    - pattern: a set of connecting functions, often in a sequence structure
    - structure: connections & change of measurable change & difference types
    - information: specific description of a structure
    - math: description-connecting functions
    - system: structure of independence, often having boundary, function & other component structures, at a particular interaction level

  - platform to apply a portfolio of AI models to price a stock given private company data like available resources, internal analysis, & implementation plans & publish the ai-generated prices, with comparative historical pricing of other companies using similar data pre-ipo or valuation, as an offset to price pumping & other forms of misrepresentation

  - identify accidental & intentional govts/laws/markets, based on function metadata concepts
    - responsibility (restricting functionality to the functions that can & should handle, based on whether they caused the problem resolved by that functionality)
    - relevance (restricting info only to functions that need it)
    - structure (functionality gaps)
    - potential (analyzing future functionality and paths to those states)
    - integration (analyzing impact of intent/responsibility/optimizations of a function)
    - optimization (storing functionality needed, generating functionality where possible & where usage allows)
    - organization (indexing functionality in a way with specific side effects like limiting possibilities, and organization through queries & changes of functionality)

  - solution investing app
    - Is the cost (implementation/opportunity/mgmt/bug-fixing costs & lost previous work/equipment value) higher than the benefit?
      It's a technology if its problems solved/created ratio & relative feature value is high enough; if not, it's just a tax/debt
      We don't need to create problems to create jobs, we need guaranteed scientist jobs to handle existing science problems
      Companies/agencies should exist to manage/build solutions to existing science problems (pay this company $x to get y% of pollution removed from outdoor air)
      Taxes are a relatively inefficient & obscure way to allocate funds to these public good solutions, compared to product purchase payment plans
      People should be able to see what their taxes are funding & opt-out to invest in other products/companies
      - example: "avoid a local road that needs local govt budget to repair, to decide where to route funds to more important projects"
      There should be an app to opt-out of taxes if they agree not to use local resources those taxes pay for
      Companies can have efficiency & problem-solving scores (problem-solution, solution metrics, cost/benefit, hidden costs, time to solution, problem solved/created metrics) in a solution investing app to win investments from citizen investors to solve problems relevant to them
      - identify which companies/traders have the right market signals or info by retroactively analyzing historical data about which agents' investments turned out to be correct in the intended investing timeframe

  - apply intents & other interfaces to other decision (transaction) types (code transactions, financial transactions, legal transactions)
    - code transactions (user action like 'clicking a button' or 'running a script' indicates what intents, according to relevant system contexts, like applicable laws/protocols)
    - resource transactions (financial resource trades), to manage intents of a transaction - money is deposited on resource delivery, otherwise in pending state for x days agreed on by agents
    - legal transactions (allow laws to be passed having any of a limited list of approved intents)

  - finding necessary forms for an intent subject to indexed rules like laws, filling them out, and optionally filtering info by regulations cited in forms, including relevant regulations applied
    - involves automated calls/faxes to request/send forms, where processes/forms arent online, like how calls to find appointments are automated
      - involves functions to:
        - derive steps to complete the task in the right sequence/decision tree/flow chart structure (fill out form, fax form, consult legal consultant, review auto-filled input)
        - derive steps in the digitized process (log in, submit form, schedule appointment)
        - find & apply relevant regulations
        - find related forms
        - auto-fill forms
        - derive & execute steps to complete sub-tasks (like 'send a form' or 'schedule an appointment')
        - identify & highlight input that needs manual review
        - find people with expertise to guide manual review (legal consultant, govt employee)
        - digitize process (auto-import to workflow management tool or multi-step form component)

    - alternatively an automated process to digitize a process/form with necessary security, search functionality, and integration with other services/processes as digitization tool variabless
    - the tool should be able to submit user-permitted/submitted input to a preliminary process (like 'apply for a license' or 'submit voter registration') form (like a wsdl, other api spec, url with html form, or just a form pdf template with unfilled fields) and guess the values based on accessible inputs (user address info), then lookup any relevant regulations or related forms & fill out those forms or apply the regulations, and then return suggested output, with highlighting for missing fields or predicted fields with certainty below threshold that user or a legal consultant can manually review, and a list of remaining action items, which can be triggered if the user is ok with the output or updates the output, such as faxing/sending/printing/mailing the form on remote servers or using task-running apps to find a person willing to run the errand, or scheduling a call/appointment (like a dmv appointment).
    - this should also be applicable to software updates (submit a current request to a current wsdl, and find/apply relevant or recent govt regulation updates as well as web protocol updates to generate updated wsdl/request/response as well as request/response wrapper/handling functions, like updating new field names or request structures in codebase)


  - search of local product supplies across exchanges

  - diagram of alternate interfaces (information = combination of structure, potential, change or structure, cause or structure, system)

    - example of applying alternate interfaces with examples of advantages of each

      - the structure (position) of the component can be used to determine/differentiate its meaning
        - 'logy' and 'logi' as prefix/suffix
          - '-logy' as a study of the prefix
          - 'logi-' as a permutation of 'logic'

      - the usage context (sentences where they're used) can be used to determine intent
        - '-logy' used when 
          - discussing science & interactions between fields/topics or changes in a field/topic
        - 'logi-' used when 
          - discussing reasoning/rationality

      - intent can be used to determine meaning 
        - use '-logy' to describe a studying activity & topic
        - use 'logi-' to reference logic, its interactions & permutations

      - structural interface (differences in position) can be replaced with: 
        - intent (reason to use within a system usage context)
        - system interface (usage context to derive reason for usage), and fit to system (meaning)

      - applying different interface queries
        - apply system context to derive intent
        - apply structure (position) as an alternative to system context & intent
        - apply intent to derive usage & system context

  - examine temporary stabilized filter structures in ozone to push co2 out of atmospheric layers away from earth & forces to do that

  - reverse engineering structures like bet types & their ratios (ratio of types like random guessing, price-dependent algorithm bets, temporary bets in non-viable companies for profit beyond actual value, actual investments in innovation/businesses) to identify & filter out trading cycles to isolate unidentified bet types

  - examine calculation errors from one partitioning method vs. other methods, & a function to balance their contribution to error to select an optimal partitioning method for an accuracy level
    - a way around the discrete vs. continuous dichotomy is combinations: 
      - discrete counts of continuous compositions (overlaps, layers, components)

  - find meta-math structure: 
    - which would allow/incentivize/generate the changes in info functions/variables (change types: interaction, aggregation, structure-filling, gaps, convergence, similarities) of known math operations
    - is it a metric like efficient stability that allows info to develop into a measurable structure in the first place (possibly changeable interaction), or is it enforced by a system of a set of limits forcing info to interact those ways (definitively inevitable interaction)
    - 'the information amount/type/variance stored in this definition/structure can only take form in or interact with these other structures/to these degrees/in these spaces/on these interaction levels'
      - information = certainty = definition = structure
      - 'this certainty/structure can only interact with or be formatted in these certainties/structures'
      - can you calculate the set of math relationships more quickly by examining opposing structures of uncertainty/randomness, by applying operations to existing certainties, or by finding a common differentiating standard in between, like abstraction'

  - algorithm to generate variables in a system

    - development of a 'concept' in a system: an object begins aggregating changes (like functions/attributes) in such a way that it develops unique interactions that differ from those calculated by a simplistic summing of the interactions of its components
      - example: a system may develop a concept like a 'layer'
        - structural definition of a layer: a set of components that separates other components & their interactions, inside a containing boundary
          - this definition differentiates it from a boundary, limit, line, or container structure
        - the definition also has dimensions beyond a simple line
        - the layer may aggregate functionality, such as:
          - being stacked or combined to create larger layers or structures on top of a layer
          - forming a base for interactions to develop on, if its a vertically stacked layer
          - acting as a filter, if there are openings in the layer
        - so the layer is not only measurably different from similar structures, it may also have significantly different functionality, earning it a unique term (meaning it has developed into a 'concept' in the local system)

      - the variable of 'structure' can describe the layer & generate it, but it doesnt capture the full definition of the 'layer' concept
      - other variables are necessary to fully describe the layer, such as:
        - adjacent structures (line, container, limit, boundary)
        - core function (stack, combine, bridge, support)
        - adjacent functionality (filter, separating interaction layers)
        - default structure (vertical layer related to stacking function)
      - because it stabilizes into a useful unique component, the layer concept begins to act like a vertex variable and/or an interface, since it starts becoming causative of changes due to its stability (rather than just being the output of changes to similar structures or iterated core functions or aggregated variance)
      - concepts in a system can be local interfaces that are useful to use as standards for comparison
        - standardize to the 'layer' structural interface
        - standardize to the 'local system structural concept' interface

    - so you can generate the sequence of a set of variables for a system by which change type structures are stable enough to act like concepts/interfaces for a given stage subset in the sequence of system development

      - system metadata: invalidating/triggering/development conditions

    - you can also apply core structures to get change types (multiply a number by the structural concept of 'opposite' to get the 'sign/direction' variable)

    - variable definition: isolatable, measurable change type 

    - component generation: identify components of a system & generate possible change types that enable/optimize interactions between those components
      - core generation: identify core change types that can be combined to create other possible change types & generate other possible change types & filter
      - subset generation: identify subsets of a system's components that are sufficiently stable in functionality/attributes to interact with other subsets without invalidating the system
    - limit generation: identify limits of a system & generate possible change types that can develop within those limits & filter
      - reverse generation: generate required functionality in a system & derive possible variables that could produce it & filter
    - filter generation: identify & apply filters that determine variable development functions (like change combination, change metadata pattern, change coordination functions)
      - apply 'variable' definition filters: generate possible isolatable/measurable change types & filter
      - apply 'efficiency' definition filters: generate structures that would be efficient & check for components that could generate those structures
      - other example filters: 
        - are there resources to sustain this change type
        - does this change type contradict a system rule
        - is there a reason/intent/usage for this change type that is not fulfilled elsewhere (by metrics like adjacence to justify creating the functionality)
          - is there a system-invalidating force requiring a new change type
          - is there another position that could use similar functionality to existing functionality that is inaccessible in that position
        - is this change type adjacently buildable with system resources
        - is this change type probable
        - would this change type trigger changes that invalidate the system or reach stability
        - how would this change type interact with other change types
        - does the environment system change enough to justify developing another or extra change types
        

  - finding formulas: equate structures like:
    - concept: 
      - 'aesthetic': generating aesthetic formulas using simple/balanced/relevant structures
    - pattern:
      - generating formulas based on patterns & anti-patterns of other formulas
    - structure:
      - using limits that bound other formulas as assumptions to reduce solution space
      - finding vertex variables of formulas & applying variations to generate other formulas
      - https://www.vice.com/en/article/xgzkek/machines-are-inventing-new-math-weve-never-seen

  - authorized pick-ups/drop-offs by people in your social circle, extra keys for drop-off in lock boxes or cars, picking up packages from warehouses
    - https://www.vice.com/en/article/v7mnga/amazons-megacycle-shift-will-push-some-delivery-drivers-out-of-work

  - 'shared responsibility pools' as a form of insurance
    - anyone who uses a particular proxy/VPN/cryptocurrency accepts some responsibility for requests/transactions executed on that service in cases where the actual criminal cant be determined
    - feature where they can pay to prevent non-verified users from using the service or pay to use an 'invite-only' service

  - differentiate change (sequence of difference structures) vs. difference (non-equivalence on some metric) vs. variable (attribute capturing an isolatable change)

  - give example of alternative filters/routes & identifying optimal filter/route structure, as well as optimal starting point (origin), direction (target) & steps (queries) to generate them

    - the below 'reverse engineering' example uses the following filter query to determine relevance:
      - relevance = reverse(similarity => core => (combine, not) => adjacence)

    - but it could also use alternate filters such as: (substitute || (similarity, quantity) || test)
      - apply 'substitute' structure: find a metric that functions as an identifier, filter, approximator, predictor, or proxy
      - apply 'similarity' structure to 'quantity' attribute: find a metric value for a quantity of more than one unit
      - apply 'test' structure to problem system structure: find tests with output information containing the metric value

    - these alternative filter sets optimize for metrics like:
      - filter set metadata
      - optimizing for different interface metrics (variance degree, interaction layer, abstraction level)
      - having a particular structure (paths to connect source/destination) that uses available functions
      - maximizing a particular change or difference type for identification/accuracy-related intents
      - connecting difference types in different spaces (standardization)
      - interface structure-fitting (like 'intent alignment' or 'lack of contradictions')

    - these alternative filters have different metadata, like:
      - cost
      - variation sources (equivalence definition)
      - variance reduction (degree, type, pattern, potential)
      - requirements (like required information access)
      - path (in the filter network, & also possibly a path in the problem structure network)
      - interfaces, structures, & definitions used ('questions' asked by the query, 'alternatives' used as 'approximations')
    
    - these questions have the structure of a theorized directed connection/path in the problem system formatted as a network
      - the patterns of these questions in producing relevant info for a problem can be used as insight paths
      - alternatively, apply a general insight path of calculating which paths in the problem network have the sequence of input/output information that could produce the answering info to the query
        - formatting the system with structural interface metadata (such as info gaps, intents, incentives, equivalences, & vertex variables) will make these optimal query patterns more obvious

  - organize list of high-impact queries which can be used for finding optimal solutions manually now while building product

    - query: reverse engineering solution metric with core structures as filters to find relevant metric structures

      - problem statement: 'find individual unit metric value in a container having equivalent & different components, without a function to measure individual unit metric value, and given total container metric value & unit count'
        - find relevant structures of the metric
          - apply insight relevant to 'calculations': 'apply the same standards when calculating if possible'
            - apply concept of 'similarity'
              - find relevant structures having the same metric
                - find relevant structures to 'unit'
                  - apply core concepts/structures to problem system structures
                    - apply core structures of 'combination'
                      - relevant structure: set of units, having an aggregate metric, usable input to an averaging function
                    - apply core concept of 'opposite' or 'not equal' and the core concept of 'total' (the complete set of all components in container)
                      - relevant structure: set of non-unit components in container, having the same metric, usable input to a subtraction function
          - find most measurable structure (with greatest accuracy or fewest steps) out of the relevant structures having the same metric
        - find calculation relationship between adjacent proxy metric of relevant structure and original solution metric (individual unit metric value)
          - calculation relationship between sets of not-equal components and equal components to the individual unit metric:
            - calculation relationship: "subtract not-equal component set metric value from total value, and divide by unit count to find individual unit metric"
          - to find this relationship, execute the opposites/reversals of the operations to find the relevant structure metric values
            - 'subtract' is opposing function of 'combine'
              - 'combine' was executed to get the list of sets of components (not-equal components & equal components)
            - 'divide' is opposing function of 'combine'
              - 'combine' was executed to get the set of equal components, relative to the individual unit
            - these two combine operations were used to create a path from the individual unit to the set of total components in the container
            - they can also be applied in reverse to get from the given total container metric value to the individual unit metric value

  - use isolatability/inevitability/uniqueness as a structural foundation for interface conversion/generation logic

    - identify 'inevitable' definition routes that are unique which can be used as a default generation intent for the core data included for app functionality
      - example: a definition route that cant be used as a definition of balance & power, just one
      - unique intents are also a useful foundation structure for the intent interface

  - apply structures to error types
    - false equivalence structures:
      - 'lack of functionality' bc of root cause of 'lack of memory' or 'lack of functionality to build functionality' or 'lack of intent for that functionality'
      - the memory lack can look like a lack of ability, but its a false equivalence/similarity caused by a lack of an input resource, within a range of change potential where the memory lack & ability lack ranges overlap

  - apply structures to overlaps in definition routes
    - find the adjacent structure without contradictions, that doesnt resolve to either specific option, within the limits of both definition routes
      - lack/limit :: resource 
      - function :: resource 
        - resource-generating function :: resource
          - resource :: function

  - give example of mapping to structures & identifying contradictions its safe to ignore for applying a structure

  - identify structures (like contradictions & distortions from expected normal) as input to info type generation algorithm
  - examine which operations (rotate, connect, combine, shift) convert the base subset/limit functions represented by a neural network into the output prediction function
  - examine the distortion vector paths that adjacently decompose a data set into a prediction function from a base point/function set
  - algorithm to identify contradictions (of a statement formatted as a route between network nodes)
    - query for conditions that would make some input, component, or output of the statement function some structure of falsehood (invalid, impossible)
    - example:
      - query for intents that would require movement in different directions, 
      - query for causes or preceding/adjacent/interacting functions that would require development of functionality making some step in route impossible
  
  - give example of how to embed interface structures in neural networks (core functions, interaction layers, etc) to select different organization structures as components of the network (concepts like balance, functions/attributes like relevance/security, error type boundaries, abstraction levels, etc)
    - a granular intent structure like "differentiate => maximize => combine => compare => select" can map to a high-level intent like "voting"
    - these structural equivalences/similarities across interaction layers can be used to implement concepts like 'security' to neural networks, such as identifiable/possible error type structures as a boundary/limit (in the form of a threshold or weight-offsetting operation) across a metric calculated from an adjacent-node cross-layer sub-network (like 'function sequence' structures are often used in exploits)
  
  - consensus-building perspective algorithm (transform a structure in each perspective to a structure in the target perspective)
    - identify structure of attributes/functions/objects common to both perspectives (connecting function: 'function connecting power and distribution', 'function describing dictatorship dynamics')
      - identify interface objects within structures (change type in conneecting function: 'direction of power distribution', 'changes in identity & size of group in power')
        - identify similarities in interface objects within structures (similar change pattern in change type in connecting function: 'power favoring distribution', 'military coups after power abuses')
  
  - structural concept definition routes
    - nothing (lack) structures, as opposed to randomness (lack of differentiating info among possibilities)
    - opposite vs. lack (of common attributes/values, connections, similarities, spaces)
      - opposite requiring a potential for extreme values to occur in a structural possibility where difference can develop
    - thinking definition as 'applying structure to uncertainty'
    - reasonable (making sense) definition as 'fitting an existing structure, like a pattern, without invalidating contradictions' 

  - example algorithm to identify rules that violate a metric 
    - requirement like: 
      - 'dont exacerbate inequalities'
      - 'protect minorities on the disadvantaged side of an inequality'
      - 'identify advantaged side'
    - power structures: required or non-specific/universal resources (such as inputs to any function, like 'energy' or 'information')
    - inequality structures: differences in distribution of required resources
    - generate structures that would exacerbate inequality structures
      - assumptions in rules (lack of guaranteed potential to follow rule)
        - rule 'close malls after business hours'
          - rule structure: 'limiting supplies' (access to facility)
          - rule assumption: that they have alternative supplies
        - rule: 'fine for not wearing mask'
          - rule structure: 'requiring function' (purchase mask)
          - rule assumption: that they have inputs to a requirement
      - these assumptions would disproportionately increase inequality's disadvantages in distribution
      - 'disadvantaging rules/assumptions' can be distributed more evenly or to offset inequalities

  - identify semantic processing necessary to get good prediction results with existing algorithms & params
    - example: find the abstraction level or definitions necessary to get an approximation of system or conceptual analysis with a standard data set 
      - the approximation may leave out other analysis logic like alternative/combination analysis (to identify sets of alternate prediction functions, or causal/functional/priority/missing/type structures in the data set)
      - however it may find objects on an interface by including interface objects (include concept definition of agency/skill/decision in the titanic survival data set may identify concepts like 'education' as causative, given that a combination of agency/skill/decisions can be used to produce concept of 'education' = 'an agent making a decision to acquire a skill')
      - similarly, including structural definitions of 'relevance' may improve prediction results with standard algorithms, allowing output structures of relevance like 'semantic variable connections on the relevance level input to the algorithm', such as an 'explanation'
        - 'including' meaning 'standardizing to relevance structures, such as similarity/adjacence, inputs, interaction level, etc'
        - first you'd apply standard analysis to get a set of probable dependency graphs, with paths like:
          - gender => lifeboat access => survival rate
        - then you'd apply standardization to relevance structures to the dependency graphs
          - difference in functional position (gender roles) => difference in function (skills) => difference in usage (responsibility) => difference in resource access => 'survival' intent inputs => 'survival' intent fulfillment
        - the output would be an approximation of meaning, allowing explanations like 'being female (variable value) increased probability (ratio of outcome among possible alternatives) of being prioritized (randomness structures like starting position as well as the concept of agency in filter structure) for access to survival tools (type of 'lifeboat') bc of less agency/responsibility/skills'

  - add to decision points
    - when a method & data set can be determined to be capable of deriving the answer to a prediction function problem

  - questions that a computer may not be able to answer even with unlimited memory/computation capacity, without trial & error or other memory-based approach (simply storing methods that worked & incrementally building on that info) to determine system analysis methods
    - what calculations will prove to be optimal (faster/more accurate), before or during processing

  - examine subatomic superpositions as such a fast aggregation of time that each possibility is occurring simultaneously
    - if superpositions are a core physical structure of uncertainty, examine whether they can be used as a base for the optimal neural network structure, where core problem types are handled by subatomic particle type structures & other structures relevant to superpositions
    - applications
      - anti-structures: enforced lack of structure to preserve lack of structure development to ensure scale of operations
        - performing calculations in places with less gravity to speed them up and send them back to places with more gravity to get answers relatively quickly
    - questions
      - what combinations of velocity/time/scale produce equal positions/perspectives, and are there stable paths between them
        - what differences in potential emerge in different perspectives (differences in potential like reversibility)
        - how many different perspective types are there, and do they stabilize to a particular perspective in a vacuum
      - which change measurement syncs the best with time progression
      - which metadata (scale) are the best sources of randomness structures found in
      - which structures can store one-directional time (aka information)
        - where info is measurable, leaves signals, and processes are irreversible
        - is there a structure that can permanently store information (unchangeable information)
        - what structures of cause (inevitability, certainty, stability, equivalence) exist at subatomic scales
      - time speed factors: 
        - more interactions have to happen at larger scales
        - fewer things change at large scales
        - there are more randomness injection points at larger scales
        - change-resistance (stability) occurs more at larger scales
        - change measurability varies across scales

  - examine efficiencies from missing components
    - some functions are generated more quickly without a component, bc of the needs that the lack generates, which focuses generative processes on building alternate functions to fill the gap
    - this can be used as a way to predict what tasks the optimized network with missing components would be relatively good at
    - missing component metadata
      - how adjacently it can be learned/generated/invalidated/delegated/identified/borrowed
      - how likely it is to be learned/generated/invalidated/delegated/identified/borrowed
      - whether another missing component can be used instead
      - whether the system missing that component should be changed instead
      - whether a system having that component succeeds at the intent task (& fails at others currently fulfilled by the system missing that component)
    - example:
      - not having a function incentivizes:
        - identity: development of that function
        - abstraction: development of generalization of that function, parameterizing that function intent
        - alternate: development of a proxy or alternative or invalidating function, making the function itself unnecessary
        - cause: development of structure/function/attribute that invalidates the original requirement metadata (priority, intent, dependency structures), not just invalidating the function
        - alternate format: development of a structure/attribute that replaces the requirement for the function or allows the function to be generated as needed
        - derivation: developing a function to learn/derive/identify/borrow/cooperate functionality from external info, to generate functionality as needed
        - core: developing components capable of building all functions to generate functionality as needed
        - subset: developing components of that function so the function & other functions can be generated as needed
        - combination: development of a function capable of fulfilling that intent & other intents
        - distribution: distributing functionality-generating methods to all nodes requiring functions
        - organization: allocating gap requirements (uncertainties) to the gap in functionality (example: keep the gap so you can apply methods as a test to resolve the gap)

  - optimized network structure

    - the optimized network can be structured as versions for different intents like:
      - lowest-memory generator: the average network + distortion functions
      - relevant generator: the network nearest to the most useful versions of it
      - quick generator: the network with the components that can build other versions at lowest cost
      - core generator: the network with core components to build all other components
      - adjacent core generator: network with core components at an abstraction/interaction level where they are most adjacent (mid-level functions as opposed to granular functions or high-level agent-interaction functions or conceptual functions)

    - the optimized network (ark) has the interface components necessary to solve any problem, with no extra components
      - it has one of each parameter of required components (like definitions, bias/randomness/error structures, interfaces, core/change functions, etc) which provide enough functionality to decompose & fit all discoverable information into a system of understanding
        - for example, one example of each opposite end of a spectrum & the average in the center, or the average + distortion functions to generate the other possible values

    - can probably be adjacently derived from subatomic particle interactions, which implement the core objects of interfaces like cause & potential

  - primary variables of brain functionality:
    - connectivity/alignment
    - position/adjacence
    - structural integrity
    - available structures
    - circuits (closed/open)
    - sub-systems (optimized for a function)

    - for some problems, some aspects of the optimized network should be deactivated/inaccessible - give example of how to calculate the structures necessary to solve a problem structure

  - in a market where uncertainty & unexpectedly correct predictions (unlikely predictions) have value, high-value contradictions of assumptions (high price of low-valued stock) are an error type structure

  - stock market (predicting uncertain value) x gaming (low-stakes task completion in a system)
    - stock market tasks in a (legal, business) system
      - deriving value of legislation
      - predicting legislation
      - legislation (more static rules) competing with more dynamic rules
      - forming business structures to aggregate/delegate/distribute risk
    - predicting uncertain value in tasks
      - predicting which tasks will win a game
      - insuring against risk of players completing or not completing a task
    - feature request & prediction market
    - game plot/cheat code/successful strategy prediction market
    - prediction games
      - false signals, gathering/deriving info, identifying important variables, applying successful analysis rules
    - insurance & other risk & financial products in games
    - stock market games allowed by legislation to allow a degree of collusion/organization in prediction markets
    - organization & risk structures allowed in a particular game, for a level of difficulty/complexity
    - games accessed with performance in previous prediction games to find best predictors and assign them more complex problems, like predicting emergent trends in interactions of complex systems
    - business & stock markets as an info-trading game to get products/features/prices and other company byproducts (clean energy practices, mergers, etc)

  - apply anti-stupidity structures to neural network structure 
    - lack of learning functionality
      - inability to remember (identify relevant info quickly when new info isnt necessary)
      - inability to identify relevance structures (meaning, usefulness, direct causation)
      - inability to optimize (identify a quicker route to an insight, like an insight path)
      - inability to model structures (enough memory to store a different structure, ability to explore/change it like a visualization)
      - inability to simulate difference structures (contradictions, paradoxes, lack of similarity)
      - inability to direct thoughts (focus)
      - inability to forget sub-optimal/inaccurate rules (bias)
        - function to apply bias structures to a neural network structure
          - thinking benefits from bias removal
          - remove bias structures in neural networks to improve their thinking capacity
          - example
            - apply removal of 'simplicity' bias in a neural network structure
              - simplicity (specifically over-simplification) definition on structural interface: 
                - lossy lower-dimensional representation
                - low-cost representation with relatively reduced learning reward
              - the simplicity bias shows up in a neural network structure in many possible positions
                - for example, a pooling function, which has no reason to aggregate other than adjacence, which may not be an indicator of relevance
                  - find the structures that can build/derive/apply/store relevance and remove structures with artificial relevance
                - general default params also tend to store simplicity where it's not needed
            - apply removal of 'similarity' bias 
              - similarity bias structural definitionss
                - relatively adjacent in variable values according to a distance metric applicable & relevant to that variable
              - the similarity bias shows up when adjacent structures are given relevance/meaning that they may not actually be capable of storing/building/deriving, like subsets of inputs or clustering thresholds

  - neural network with anti-bias structures built in (a complexity structure, a difference structure, etc) to correct error types from common biases

  - function to convert article/listing/social posts into variables to enable queries (product with feature x in budget y that integrates with app z and has attribute independent)

  - document locked objects that are inputs to core objects (like functions & concepts)

    - core functions like 'change', with locked objects which should be generated as inputs to other functions and should not be removed bc they enable other rules & core objects
      - a 'check for errors' function
      - a concept of 'self-correction/optimization'

    - these locked objects can be used to generate rule-generating/deriving/finding structures, by forming an initial structure of locked objects and filling that structure with conditional & changeable structures
      - these rule-generating/deriving/finding structures can be used as solution automation workflows

  - difference vs. similarity
    - similarities between difference & similarity
      - distance metric
    - differences between difference & similarity
      - amount of info that needs to be stored for a complete accurate description ('what something is not' may require more info to be stored compared to 'what something is')
    - the position of difference between difference & similarity may be on non-opposite positions on a circle depicting routes to get from difference to similarity
      - this is bc a similarity is a degree of difference (low/zero difference) & so is a difference (higher degree of difference that can be measured or is observed as noticeably different compared to a similarity)
      - the structure may be a circle or other loop bc if you stack enough differences, eventually you may generate the original object
    - the conversion of difference into similarity is based on the concept of a threshold, where a difference acquires enough similarities to similarity to cross the threshold or vice versa
    - the gray area in between the two concepts & surrounding the symmetry of the threshold also conflates the differences between the two concepts, making the difference not a simple 'opposite'

    - example: spectrum structure
      - handles different cases like 'near low/high/average value' (like between 0 & 1), which have differences in adjacent change types to produce relevant objects (like an integer)
        - change types like 'small change to produce an integer', 'doubling to produce an integer', etc
      - the isolated relevant difference structure (without additional info) 
        - the average value, which has multiple difference types in adjacent change types
      - conditional relevant difference structures
        - if the nearest integer triggers other change types, the value near that integer has a relevant difference structure

    - example: position structure
      - similar positions will be near according to the distance metric, creating a radius of similarity, which results in emergent structures of a boundary, center & circle
      - different positions can be represented as a structure lacking a circle/boundary/center
      - the differences in similarity/difference structures have emergent effects & coordinate with different interface objects (like adjacent structures, change types, relevant objects, etc)
        - a lack of an object can be used like other gap structures are used (as a filter, container or template)
        - an object can be used as a component or other base object to use as an input

    - this is why differences are not just the 'opposite of similarities' - it leaves out information like:
      - similarities of varying relevance between similarity & difference (both use a distance metric)
      - the reason why a difference is used vs. a similarity (like 'filtering' intents)
      - emergent/adjacent/relevant structures of similarity & difference, embedded in different structures (position/spectrum)
      - info about the structure of difference (difference paths/stacks/layers/trajectories), which may vary in ways that similarities do not
        - this indicates the important point that similarities are insufficient to predict differences
      - if similarities were equivalent to differences, you could use similarities to derive all info, reduce all uncertainty & randomness, and solve all problems - which is not guaranteed
        - meaning 'derive structures outside of the universe, using info from inside the universe' 
      - similarities may have similarities to each other, more than similarities to differences
      - randomness has a similarity (in outcome probability), but is better than similarity as an input to generate difference structures like uncertainty

  - document uncertainty structures like randomness collisions & structures that produce certainty (combinations that stabilize)
    - randomness collisions generate structure
      - structure being the stabilized interaction of information (staying constant long enough to attain structure)
      - randomness being a lack of information (like a star or circle with equally likely directions of change)
        - where influences are equal enough in power to leave no clear priority of direction favoring one over the other
      - when an info lack interacts with an info lack, they may not generate another info lack, but a structure stable enough to organize them, depending on the angle/type of interaction and whether the info lacks are a similar or coordinating type

  - document interacting AI error types (as in financial price & crime prediction models)

  - standardization application to generate logic automation algorithm

    - iterate through interface objects (change type, problem type, assumptions, etc)
      - find interface objects in a problem space 
        - filter by relevance structures (like interaction directness/causation, such as change hubs)
          - apply problem structures related to relevant structures
            - apply organization structures (like a sequence of tests/queries) to problem structures

    - specific logic automation example
      - check for missing relevant info in info found with variables
        - change to add earlier window to mtime param bc its out of error window
      - find interaction type & change type in info metadata (filename, modification time relationship)
        - any logs changed in later would include logs modified earlier bc of lack of incrementing/rollover, so mtime increase is unnecessary
      - check assumptions for requirements
        - mtime param unnecessary bc most logs would be modified in original mtime param
      - check for relevant change-aggregation objects in structure (event objects in a sequence structure)
        - significant date (upgrade, reboot) was within original mtime param which could be a factor in error so mtime param is necessary

  - examine function topologies (structures & structure change metadata that can maintain a particular function)
    - document intent structures (like intent sets) associated with function topologies
    - even if a structure maintains a particular function, its other metadata like adjacent interaction/change types & intents may change with the structure change
    - intent topologies dont necessarily match metadata of function topologies
    - interaction of interface object topologies as a source of variance reduction

  - why structural analysis of components (like cell shape/surface) is insufficient as a predictor of functionality
    - it's missing info about:
      - components
        - other/possible components & their structures (other possible pathogens, foreign cell types, in other ratios/positions)
          - other/possible components with similar/contradictory shapes that might be interfering
            - like similar receptor/binding shapes that leave no room for the cell type being examined
        - internal cell components not measured or formed unless found in a particular environment context
      - change types
        - changes to the host system structure (like nerve damage)
        - changes to forces governing change (like motion, as blood flow) in the host system structure 
        - not measurable info
          - hidden non-structural variables (like blood flow/pressure, electrical effects, or prior exposure to nutrients like vitamin d triggering timers) or variable sets with similar net effects (activated lifecycle)
          - distortions commonly found in different cell types with same structure bc of different positions
          - functional implementation differences
            - different cell types have different method of achieving the same function using the same components, in a structure that varies within the data set but not enough to indicate different method
        - component interaction dynamics
          - interaction level
            - cells with same structure might operate on different interaction levels, given different position/system
          - structures of interaction object components
            - a cell with equivalent DNA might encounter 'jumping gene' functionality in one system position, where an equivalent cell in another position would not
          - determining interaction attributes/functions 
            - like how attributes like aggressiveness might be determined by missing info (indicating why one cell type would succeed at binding & another of a similar/equivalent structure would not)
        - limit/threshold dynamics
          - sample data might leave out variation in the form of determining cell type attributes like size above a threshold with emerging behaviors, or potential to change that attribute triggered by the environment
        - state dynamics
          - false equivalence: structure might be measured at two equivalent states across two different cell type lifecycles (like evolutionary paths or distortion patterns), giving illusion of equivalent structures
        - system dynamics
          - structural metadata (like position, which determines local system & adjacent cells/functionality)
          - invalidating functionality
            - system that deletes duplicates, where a particular cell type is handled second bc of some attribute (like size, indicating it needs to be broken down first), so its always found to be the duplicate & is deleted
          - functionality that is activated in environments & not obvious with structural analysis 
            - like a function that folds dna/proteins in a way that has more errors than other folding function in a particular environment
        - sequential dynamics
          - exposure to a pathogen might trigger a function in response to a cell type with a minor distortion that becomes determining in edge conditions

  - structure standardization (applying structure to structure to generate a particular structure/format)
    - translating structures into vectors
      - many vector structures can represent interface structures
      - example of selecting a vector structure to represent an interface structure on a particular interface, applying structure to indicate metadata about structures
        - example: causal loop
          - standard network structure translation: vectors to indicate direction of cause
          - relevant network structure translation: vectors of influence degree away from hub cause & other cause structures

  - variables of the network include structures emerging from or embedded in algorithms/structures

    - core structures
      - change types 
        - difference type 
      - agency types
      - cause types (influence/power of structures)
      - structures
        - sequence (embedded concept of 'time' in structural interface)
        - list (unique index)

    - structures applied to these core structures to generate conceptual structures in neural networks
      - alternative cause: change applied to causal structures at training & prediction time
      - organization: difference type index
      - agency/govt: decisions about change types to apply

        - structures applied to agency objects like decisions (such as subsets/alternates) & other conceptual structures (like time)

          - sub-decisions
            - structures of neural networks with delayed sub-decisions
              - conditionally activated cell structures with enough info to make a sub-decision
            - structures applied to decisions can generate networks with other decision structures than 'consensus voting'
              - govt structures/algorithms
                - organization structures are a structural version of govt (agent-based) decision-making
              - finding the level of 'agency' to apply to a network is possible with problem complexity identification
                - apply agency: delegating decisions to subsets/groups/layers of cells to delay change decisions to another point in time

          - alternative decisions
            - decisions are a 'selection/identification/filtering' problem about a possible change type (like direction) to consider/implement
            - structures of neural networks exploring alternative variable structures & alternate decisions rather than the stated problem decision or default variable structure (identify direct causation, filter out non-directly causative variables)
              - alternative decisions
                - finding root cause
                - solving a proxy problem

          - decision (change-filtering problem-solving) times

            - standard time points: training time, data gathering/processing/standardization time, decision/prediction time, re-training/update time, parameter selection/update time
              - sub time points: activation time, pooling time, aggregation time, filtering time

            - optional points where decisions can be injected
              - decisions: 
                - network-level decisions: continue learning, select prediction answer
                - structural decisions: change direction, identify threshold, ignore info
                - meta decisions: delegate/delay decisions, consider alternative decisions
              - time where decision is clear/final/starts to emerge
              - time where direction change decision is made
              - time where more info/time is identified as necessary
              - time where decision is identified as not answerable
              - time where alternatives are identified, assigned probability, filtered out
              - time where possible routes to an answer are identified (what structure of variable values like 'ranges' can produce a clear answer)
              - time where possible decisions remaining are identified (and conditional remaining decisions if a change is applied)
              - time to check for a structure in the difference type index
      
  - real vs. ai detection algorithm
    - variable count/size (under-complexity, fragmentation, lack of smoothness/curvature)
    - wrong context for a pattern
    - over-repetition
    - over-similarity to previous information (lacking expected change structures, like change trajectory & types)
    - no matching reason/intent/priority for deviations from archetypes/patterns
    - over-correction when integrating a variable
    - variables identified in isolation
    - most clearly/measurably different variables identified
    - structure organizing variable structures (randomness injection points, enforcement gaps, info imbalances)
    - over-simplistic or erroneous automated sub-components
    - improbable level of randomness
      - clear composition of core patterns
    - sources of randomness
      - errors are evenly distributed among more complex adjacent sub-components not expected to change as much

  - the most useful patterns will be:
    - cross-interface patterns: patterns linking interface objects
      - patterns of interface components that link all interfaces: 
        - error patterns
      - patterns of interface object links
        - change trajectories of randomness
    - system patterns: which unite other structures and form an interim structure in between meaning and an agent
    - core patterns & core interface components that can build other components
      - patterns in core interface components, like change/difference patterns

    - errors are a difference type in a specific structure (between expected/actual values) so theyre useful as example core problem signals
      - stacking errors may be a better way to frame problems than other interfaces
        - the level of randomness captured by the error structure
        - errors can function as limits as well as difference types building a problem structure

    - when testing different variable subsets, you can select a variable set split by structures like:
      - vertex variables
      - variables on interim interfaces where other variables aggregate (in bottlenecks or hubs)
      - difference interactions
        - difference type (homogeneous sets of difference types)
        - differences in different types (heterogeneous sets of difference types)
        - which difference type sets would identify the most errors or are the most different from other difference type sets
      - which difference types are the biggest variance-reducers when combined
      - which difference types have an attribute (common, relevance, similarity)

  - identify bias structures as output of operations on structures, or by missing structures that cause bias
    - bias is a filter that leaves out relevant information
    - 'facts without connection to meaning' is a biased priority (current state of truth) and a biased lack (ignoring potential truth & potential connections that change the meaning/position of facts)
      - example: if you just focus on data set facts, you miss other facts (contradictions, counterexamples, alternative conditional variables/functions), as well as opportunities to derive other facts from the data set (given the favorability of the data set to influential entities, we can derive a guess that other facts might imply a different conclusion), and the connections between the data set facts & other facts (other facts imply a different cause than the data set facts) as well as the meaning of those connections (why this data set was selected)

  - design an optimal sorting structure for general interface queries to apply to problems manually

  - starting points of filters that reduce the problem space
    - starting point of identifying all the assumption sets that it would be most problematic to get incorrect to prevent the worst error types
      - in the problem of 'predict cat vs. dog', the worst error types are:
        - an object from one category having all the features used to differentiate between categories, but with variable values of the other category (cat having dog features)
        - an object that is artificial identified as real (cat robot identified as a cat)
      - to predict these error types, certain concepts need to be inferred
        - the concept of 'agency' to design a machine that looks like an animal
        - the structure of 'false equivalence' to design situations where features would look like a category but not actually be that
    - starting point of identifying all the feature ranges where it would be impossible to give high-accuracy answers (ai-generated cat image vs. real image)
    - organizing these filters in a useful sorting structure (network, tree) can reduce the computations required to solve for a prediction function

  - predicting prediction function error types
    - false equivalence
      - similar routes to different answers
        - this implies similar patterns in variable structures & interactions across data groups
      - overlap
      - lack of differentiating variables in data set
    - false difference
      - merging/imminent similarity/equivalence
        - functions that can act on other functions to produce a false or real equivalence to another function
      - alternative routes to the same answer
        - identify all the alternative structures (routes, combinations, trees) to an answer between function components like variables, data sets/subsets, & neural net components like weight path patterns, and the differentiating factors & vertexes, then use that to implement a filtering structure to sort through them to rule out the most possible answers the quickest
      - alternative answer types
        - identify all the different variable/function combinations that could create the most differences in similar answers (such as different types or contexts like a separate function for outliers), and a filtering structure to apply these as variation-reduction functions
      - these filtering structures can act like interfaces, reducing variation in the possible answer set
    - equivalent combinations
      - alternative variable subsets that act as proxies to an answer
    - equivalent variable structures
      - find variable structures like functions that approximate other variable structures like variable networks

  - example of how to predict most interactive/causal concepts in a system

  - make diagram 

  - list interface selection (based on inputs like available APIs/data sets/definitions)

  - the problem is the solution in a different format, or a piece of the solution (problem being a sub-optimal state to optimize, or a difference that shouldnt occur, and the solution being a set of constraints forming boundaries, or an optimal structure to construct)
    - filling problem
      - missing info problem: the solution format is the complete structure
      - optimization problem: the solution format is the variables/system organized to comply with/fulfill the metric to optimize 
      - aggregation problem: the solution format is the aggregation method to form a structure (like combining core functions to get a function for an intent)
    - limit problem
      - constraint problem: the solution format is the removal/invalidation of that constraint
    - reduction/decomposition problem
      - complexity reduction problem: the solution format is the set of variables that reduces complexity of the problem
      - randomness reduction problem: the solution format is the set of variables that can replicate a semblance of randomness
      - problematic structure: the solution format is reducing the structure (identifying variables & invalidating those variables)
    - organization/mapping problem: the solution format is the set of relevant components in the right structure (positioning & connecting them)
      - conflict problem: the solution format is positioning the conflicting problematic vectors so they dont intersect
      - balancing problem: the solution format is the distribution of resources nearest to a balanced state (subset of matching problem, by matching distribution across positions)
      - combination problem: the solution format is the set of components in a combination structure that doesnt contradict combination rules (components fit together, like 'finding a system where a function can execute')
        - connecting problem: the solution format is the set of functions that connect the components, in the position where they act as connectors
    - finding problem
      - discovery (insight-finding) problem: the solution format is the set of generative/distortion/core functions or the set of filters to find the insight
      - route-finding problem: the solution format is the route between two points that doesnt contradict any solution constraints and/or optimizes a solution metric
    - other solution formats would be for adjacent/causal problems, solution formats that invalidate solving the problem, etc

  - to generate solution automation workflows:
    - combine problem types
      - a reduction/decomposition problem + a filling/aggregation problem = the solution automation workflow 'break a problem into sub-problems, solve sub-problems, aggregate sub-solutions'
    - combine structures & connect structure combinations by problem types
      - the structure combination of 'a sequence injected in a network' is a structure matching a 'route finding problem', so apply solution structures that find a route in a network, such as filters using metrics or rules that can filter routes by which routes dont contradict rules
        - the solution automation workflow is 'find structures relevant to resolving problem structures like inequalities in other structures' (inequalities like the difference between start/end positions)
        - the workflow matches 'sequence in a network' with 'route filtering structures', connected by the problem format 'find a route'
    - combine structures & core functions
      - the structure of the core function sequence(find, apply, build, filter) = matches solution automation workflows like 'find components which, when this function is applied, can construct this structure, complying with these solution metric filters'
    - combine components of solution automation workflows (functions, queries, interfaces, problems/solutions, structures) that have a valid input/output sequence

  - problem objects: solution constraints/metrics, problem space variables, available functions, useful formats/structures

  - how to find variables in a problem statement
    - find isolatable change types
      - if the problem is 'predict movement of object', this means: 'find change in possible orthogonal directions'
        - filter out redundant variables (like if variable A/B + randomness constant can be replaced with variable C + another randomness constant)
        - filter out variables or variable structures like combinations that look like randomness to leave sets of variable/s
          - find prediction function for variables with randomness excluded
          - apply degree of randomness with randomness accretion patterns & interaction structures (like other objects on interaction layers) to prediction functions once variable dependencies are described, to generate prediction function set or prediction function with distortion vectors for possible ranges, then test on data 
    - variable sets that cant be ruled out can be considered sub-problems to solve ('rule out this variable set') in addition to the original problem of 'finding a prediction function'

    - concept-structure 'find prediction function' interface query
      - find solution filters
        - find range of error allowed for solution
      - convert to problem interface
        - predict missing info 'future state of variables' with input 'past information'
        - standardize to structural interface
          - find vertex concepts
            - 'find prediction function' using past information involves:
              - risk structures like: possibility that an unknown structure is causative
              - randomness structures like: possibility that known structures will be distorted by randomness
              - change structures like: possibility that known structures will change & info needs to be found/derived to update variables
            - combine risk structures, randomness structures, & change structures
              - filter which combinations match data
                - filter which combinations match data within range required by solution filter

  - general interface query example for 'find prediction function'

    - change: find highest change problem variables in problem statement
      - structure: find combinations/subsets of variables
      - cause: find dependency structure of variable subsets
        - function: find input/output sequences of variable subsets
        - structure: filter the sequences by whichever sequences link the source/target structure
          - problem: solve sub-problems of organizing variable subsets
          - structure: aggregate sub-problem solutions

  - specific interface query example for 'find prediction function'

      - change: find highest change problem variables in problem statement
          - which probability distribution it is
          - variable values given
          - whether alternate probability distributions can be ruled out using constraints/assumptions/parameters/change types & other info of problem
          - sub-problems
          - sub-problem structure (organizing the sub-problems)

        - structure: find subsets of variables
          - example problem variable subsets:
            - missing info + variables values given + sub-problems
            - probability distribution + variable values given + other problems or problem patterns

        - cause: find dependency structure of variable subsets
            - missing info + variables values given + sub-problems
              - with the missing info & variable values given, you may be able to infer the probability distribution (though not always if the problem statement is ambiguous) and derive the sub-problems to solve
            - probability distribution + variable values given + other problems or problem patterns
              - from the probability distribution & variable values given & other problems, you may be able to infer what the missing info is given questions usually asked with that distribution

          - function: find input/output sequence of variable subsets

          - structure: filter the sequences by whichever sequences link the source/target structure (variable values, probability distribution & missing info, 'probability of event')

            - problem: 'predict probability of event A given event B & some parameter/condition C'
              - sub-problems
                - identify problem metadata (probability distribution, variables & values) in problem statement
                  - identify missing info (specific problem to solve, like 'find the missing info that is a probability of a specific event')
                - identify alternate interpretations of problem
                  - filter alternate interpretations (to likeliest or the interpretation with no contradictions)
                    - match variables & values in problem with parameters of the probability distribution or relevant functions
                      - filter functions to functions with output type 'probability'
                        - filter functions to functions with specific output probability matching missing info
              - aggregate sub-problem solutions
                - missing info:
                  - apply variable values to relevant functions to generate missing info (specific output probability)

  - identify economic cycles not integrated enough with other economic structures so as to be considered essential
    - debts to entities who dont provide essential inputs or inputs further up the chain with x degree of distance from essential resource suppliers

  - add to insight path & solution automation workflow indexes
    - find an example & generalize
      - find core/unit objects, find example using those objects, & generalize
    - find an example & counterexample & connect them
    - execute a problem-reduction function/structure/question sequence
    - execute a solution-space reduction sequence before solving for remainder problem
    - run query to find interacting interface structures, then apply solutions for that specific problem space's interface network
    - identify vertex variables first & approximate
    - identify problem types & corresponding solution aggregation method for that set of types
    - identify alternative problems to solve (like whether to solve for organize, format, select, re-use, derive, discover, build, diversify, optimize, distort, or combine problems/solutions) & apply problem selection method, then solve

  - examine structures of trend convergence 

    - trends 
      
      - micro internet markets
      - micro/specific app favor markets
      - violent power transitions
      - competitor/competition bans/taxing
      - currency/wi-fi competition & dictators as a source of stability
      - anti-democratic activity as a specific case of anti-trust activity
      - investment in job creation/antiquated tech subsidies
      - customer product lock-in
      - dependent product price-raising
      - drug discovery automation
      - all-service companies
      - info derivation tools
      - temporary/sequential info markets as a social mobility/equalizing tool
      - delegation of high-cost/low-interest problems to AI
      - ending resource inequalities (tech, energy, internet)
      - hacking targets (democracies, big consumer markets like traders/gamers)
      - labor trends of balance between priorities (organization/innovation/optimization/integration/cooperation/research)

    - structures

      - cascading errors
        - AI is applied iteratively to tasks that people dont want to pay attention to bc they assume lack of relevant or changing variation, which may include monitoring AI errors or designing AI tests

      - interacting trend trajectories
        - price manipulation for investments in systemic price reduction (ending resource inequalities necessitating competition for moats)
        - markets for info, decisions, risks, intelligence, potential, justice, laws, independence, problems/solutions, customization, organization
        - competing prediction/computation tools: stats, system analysis, quantum tech, AI-optimized processing units
        - AI as an error-correction tool for quantum tech
        - checks & balances through competing evaluation tools: 
          - science experiment automation, automated testing tools, AI, quantum computing, system analysis, stats
        - evaluation/info-derivation/prediction/computation tools as components of a system building understanding
        - competing task runners: AI, robots, & gig workers
        - contact-reduction & independence tools like 3d printing
        - organization tools, encryption & dictator overthrow-planning/subversion, consensus-building, or dictator-manipulation
        - organization of competition in a problem market, for important optimizations only
        - market selection/optimization/automation

  - make interface query output diagram

  - organize examples
    - label examples so they can be queried more structurally
    - query for logic in examples when implementing functions
    - give structural query example diagram for GANs + image compression problem

  - generate default function list

  - add mapping for data sci use cases => tools

  - function to translate interface query logic into interface language (combination of core functions (find/build) & other core components)

  - de-duplicate logic
    - organize interface analysis logic definitions
      - organize functions in problem/interface definitions, before organizing functions in implementations/*
    - integrate problem_solving_matching.md
    - integrate find/apply/build/derive logic from system_analysis/ & maps/defs*.json
    - separate interface analysis logic into implementation/functions (functions dont need unique info)
    - add functions from workflows & analysis (to do list, questions answered, problems solved, interface definition & functions) as files in functions/ folder
      - organize into primary core functions & list sample parameters (like objects to identify for the identify function)

    - integrate rules from other diagrams not included in patent applications to relevant documents
        [0010] Example embodiments will be described and explained with additional specificity and detail through the use of the accompanying drawings. 
        [0011] FIG. 1. 'User Interface Module' illustrates a diagram of a user interface that can accept user input about a problem & program configuration. 
        [0012] Fig. 2. Interface Analysis Module 140 is a diagram of example components (such as functions & constants) of a program to automatically apply information formats to achieve an input intent. 
        [0013] Fig. 3. Machine learning system 120 is a diagram of an example wrapper component that would call a machine learning system to predict a variable. 
        [0014] Fig. 4. API finding/calling system 130 is a diagram of an example wrapper component that would call an API finding/calling system to retrieve data. 
        [0015] FIG. 5. 'Structure Application Function - Apply Function' illustrates applying a structure to another structure. 
        [0016] FIG. 6. 'Problem space visualization' illustrates an example visualization of a problem space. 
        [0017] FIG. 7. 'Network of related problems' illustrates an example of a network of related problems. 
        [0018] FIG. 8. 'Problem Types' illustrates a set of common problem types formatted as information or structural problems. 
        [0019] FIG. 9. 'Problem formats, with matching solution formats of problem formats' illustrates an example of various problem formats & solution formats that match them. 
        [0020] FIG. 10. 'Problem-solution structure-matching: apply a solution function to a structure containing the problem to find specific solution structures for that problem' illustrates an example of matching a problem with a solution. 
        [0021] FIG. 11. 'Finding alternate solution formats that fulfill different metrics' illustrates an example of selecting a solution format that fulfills a solution metric. 
        [0022] FIG. 12. 'Network of problem sub-problems, breaking a problem into components problems' illustrates an example of breaking a problem into a set of sub-problems, which once solved, can be aggregated with a solution-aggregation method as shown. 
        [0023] FIG. 13. 'Causal structure-matching' illustrates a method of matching causal structures to a variable set. 
        [0024] FIG. 14. 'Design Interface Query' illustrates a method of assembling input information into structural meaning relevant to the input intent, using a structure containing information formats. 
        [0025] FIG. 15. 'Concept definition network' illustrates a network of related concepts. 
        [0026] FIG. 16. 'Alternate definition routes' illustrates a set of definition routes for a concept. 
        [0027] FIG. 17. 'Match structure for a definition of a concept' illustrates matching a structure to a concept. 
        [0028] FIG. 18. 'Intent-matching' illustrates matching intent to structure & vice versa. 
        [0029] FIG. 19. 'Insight path application' illustrates insight path examples and an example of applying an insight path. 
        [0030] FIG. 20. 'Interface conversion & matching' illustrates an example of selecting an interface to traverse. 
        [0031] FIG. 21. 'Interface & traversal diagram' illustrates an example of a diagram indicating an example interface, & a diagram indicating which interfaces to traverse in what sequence (forming an interface query). 
        [0032] Fig. 22 is a diagram of a process that describes the general workflow for implementing interface analysis. 
        [0033] Fig. 23 is a diagram of an example usage of the system. 
        [0034] Fig. 24 is a diagram of an example environment in which systems and/or methods, described herein, may be implemented, including interface analysis module 220 in FIG. 22. 
        [0035] Fig. 25 is a diagram of example components of one or more devices of FIG. 22. 
        [0006] Figs. 1A - 1J contain diagrams of an overview of an example implementation 100 described herein. 
        [0007] Fig. 1A User Interaction Module 110 is a diagram of an example user interface implementation to gather input about a problem & program configuration for Solution Automation Module 140.
        [0008] Fig. 1B Solution Automation Module 140 is a diagram of example components (such as functions & constants) of a program to automatically find/derive/generate a solution for a problem, to implement the general execution workflow of Fig. 4. 
        [0009] Fig. 1C Machine learning system 120 is a diagram of an example wrapper component that would call a machine learning system to predict a variable. 
        [0010] Fig. 1D API finding/calling system 130 is a diagram of an example wrapper component that would call an API finding/calling system to retrieve data. 
        [0011] Fig. 1E Solution Output 150 is a diagram of an example output of the process in Fig. 4 that could be displayed & edited in the User Interaction Module 110. 
        [0012] Figs. 1F - 1I contain diagrams of an example problem-solving automation workflow (such as problem space structurization (formatted as filters/limits/functions/networks/vectors)) detailing a particular interface traversal format sequence that can be used to solve most problems. 
        [0013] Fig. 1F Finding matches between problem & interface components is a diagram of an example implementation of step 404 - 406 of the process of Fig. 4 (converting a problem to an interface, mapping between components of the problem & interface). 
        [0014] Fig. 1G Applying matching interface components to relevant problem system components is a diagram of an example implementation of step 407 of the process of Fig. 4 (applying matching mapped objects from the interface to the problem system). 
        [0015] Fig. 1H Applying solution metric structures to solution structures is a diagram of an example implementation of step 408 of the process of Fig. 4 (applying solution metric structures to solution structures). 
        [0016] Fig. 1I Example Object Definition Structures is a diagram of example structures forming the definition routes of an example system object on the structural interface. An example of a definition route is documented here: https://github.com/outdreamer/build-a-cure/blob/52c3461fdd3ff38284b63f8c2e71542f415d88d9/find_existing_solutions/system_analysis/maps/definition_routes.json 
        [0017] Fig. 1J is a diagram of an example usage of the system. 
        [0018] Fig. 2 is a diagram of an example environment in which systems and/or methods, described herein, may be implemented, including solution automation module 220 in FIG. 2 which refers to solution automation module 140 in FIG. 1. 
        [0019] Fig. 3 is a diagram of example components of one or more devices of FIG. 2. 
        [0020] Fig. 4 General Execution Workflow is an overview of an example process 400 for implementing problem-solving automation workflows in steps 402 - 410, from initial problem formatting to solution matching to solution application & analysis. 
          
  - using set theory in query operations:
    - edges as core organizing/formatting operations (find/apply) & interfaces (connecting/explanatory concepts/functions)
    https://en.wikipedia.org/wiki/Hypergraph


## examples

  - example: to identify false information across user requests:
    - example of applying intent interface: 
      - check with intent provider (site) if a request for an intent (request password) was just made, to validate messages
    - example of applying pattern interface: 
      - check if user access patterns (like 'navigate to site, then check email for site password reset') match the intent of a message

  - example of permuting assumption: "reports of power consumption have to be exact measurements" (platypus)
    - a temperature monitor sensitive to a hundredth of a degree might provide similar but non-specific power reporting for important/extreme usage patterns without revealing such specific information as that which could infer exact operations being done, bc the interval of temperature measurements allows for greater variation in calculations that could explain it

  - finish dilemma problem type example formats
  
  - query examples for use cases like:
    - lack of information stored (match problem of type 'information lack' with interface query 'check pattern interface for similar patterns')
    - query problem breakdown & integration diagram
    - calculating various different problem breakdown strategies first before executing normal query-building logic for each
  
  - give example of generating problem types by applying structure
    - for instance, a common problem type is a mismatch/imbalance
      - by applying the 'mismatch' to the cost/benefit relationship, you get an 'inefficiency' problem type, which can be defined as a mismatch/imbalance between the cost & benefit, favoring the cost side (the negative object out of (cost, benefit), associated with problems)
  
  - add examples of system/object/rule/type change patterns
  
  - include example workflows with example problems
    - include example of how to generate other workflows (different starting/ending points & trajectories)


## diagram
  
    - add diagram for intent-matching
    - add structures to diagram: interface overflow (to sub-interfaces), interface foundation
    - diagram for workflow 1: 
      - function to determine relevance filter ('functions', 'required') from a problem_step ('find incentives') for a problem definition, to modify problem_steps with extra functions/attributes ('change_position') to be more specific to the problem definition ('find_incentives_to_change_position') for problem_steps involving 'incentives', so you know to use the function_name to modify the problem step if it's between the type 'functions' and the object searched for 'incentives'
    - add conceptual math interface query diagram
      - use lattice multiplication as standard example, other than core operations (add/multiply mapped to language, concepts like irreversibility/asymmetry mapped to math)
    - interface conversion, matching, starting point selection (applying structure, checking if relevant information is found)
    - diagram to document sub-functions of core functions with distortions
    - make diagram for dimension links higher than 3d that are depictable in the same network space
      - should show variables that impact other variables, the change rates of these relationships
      - overall impact should be calculatable from these relationships
      - should show similar movements for correlated variables
      - should show skippable/derivable variables (variables that can be resolved later than they normally are)
      - should show meta forces for overall trends in change rules (direction of combined variable forces)
      - should show limits of measurability & threshold metrics
    - structurize (apply structure to) definitions of objects specific to interfaces
      - example: info asymmetry is associated with an info loss in a particular direction between info types/formats, rather than just an info imbalance or mismatch
      - diagrams for specific concepts, core functions, concept operations (combine, collide, connect, merge, apply), ethical shapes
        - variable accretion patterns (how an object becomes influenced by a new variable, complex system interaction patterns, etc)
        - make diagram of potential matrix to display the concept
          - map parameter sets to potential matrix shapes 
        - finish diagrams for cause (shapes & ambiguity), concept (evolution of concepts, networks, distortion functions)
        - diagram for argument
      - make a system layer diagram for each interface to allow specification of core interfaces & other interface layers (interface interface)
        - make a system layer diagram for structures to include layers of structures 
          (beyond core structures like curves, to include n-degree structures like a wave, as well as semantic output structures like a key, crossing the layer that generates info structures like an insight, a probability, etc)

    - map variable structures to prediction potential for problem types, given ratio of equivalent alternate signals

    - calculating solvability limit of a problem, without being given the answer
      - example: standard 'psychic' magic trick like guessing number of fingers held behind back, or which number people will choose
        - connected structural info:
          - when they choose the number
            - physical motion rules
              - how arms/joints move 
              - how their eyes move (indicating remembering or creative process or a local distraction or another input)
          - default input rules
            - hand motion dynamics, like how fingers interact & which motion types are favored/prioritized/likelier
        - general rules
          - alternative selection rules
            - how people make decisions from a set of similar alternatives (familiarity, understandability, simplicity, standard vs. non-standard choices)
          - intent rules
            - agent intents (trying to surprise the magician by subverting expectations of their choice)
        - related variables
          - attention
      - limits of solvability occur with non-interchangeable (not equal) alternatives that can't be distinguished with the given info, without being given the info of the answer (or info that makes it identifiable or possible to filter/reduce other options)
        - indicates that the interaction of the available variable info: 
          - is too low-dimensional
          - includes info about too distant/indirect variables/rules
          - includes info that cant capture/derive approximations/actual values of the variation/patterns of the output variable or its proxy variable
          - doesnt have a vertex variable or connectable interfaces/variables

        - there may be some combination of movement, rule selection, default config, attention & memory that produces difference choices without giving clear info signaling this difference (limit of solvability is reached)

    - optimizability of a problem, given resource limits (market, time, info about alternative, related, & interactive products)
      - buttons vs. configuration (headphones with buttons)
        - variables
          - hardware
          - alternative/related/interactive products
          - usage patterns
          - sound functions (play, skip, switch to voice commands, reduce noise, highlight bass, use more capacity to clarify sound quality, change relative volume, predict lost sound)
          - buttons
          - attachability/detachability/migratability
          - compartmentalization/isolatability
          - buildability
          - configuration options
          - simplicity
          - memorizability
          - adaptability
          - app
          - higher-variation alternative interfaces
            - sound input/output (alternative input to a button)
          - probability (commonness of a usage pattern)
          - demand (need for a button, configuration, usage pattern, or a function)
          - variable structures (combination of variables, like a particular set of variables or a set of interaction rules between variables)

        - implementations
          - find common usage patterns & assign to buttons
            - buttons for common functions
          - find memorable button structures & assign to common usage functions
            - find memorable combinations & sequences, like double-click of a button, or a button combination click, and assign to common usage functions
          - inject crucial high-variation function in higher-variation interface
            - configurable button functions (configure options of how buttons connect to functions), using an app (higher-variation interface, allowing more buttons)
          - inject crucial high-variation function into a button
            - configuration button (configure options of how buttons connect to functions), by clicking a config button
          - embedded menus in buttons
            - access menu (list of functions) with a button or button structure (combination, sequence)
          - alternate input with higher-variation potential
            - voice commands rather than or in combination with buttons
          - allow buttons to be attached like legos
          - allow buttons/functions to be coded & switched out to do any function the hardware (or connected hardware) can support, including functions from other alternative products
          - integrate with existing hardware like glasses/hat/shirt (use materials to conduct sound, attach speakers/microphones to glasses rather than having wires, attach buttons to glasses)
          - allow each alternative to be selected so they can choose which config/button/sound interaction rules to apply to those variables

        - optimized mathematized implementation for intent (simplicity, highest features given simplicity, maximized features)
          
          - simplicity: assign common (high-probability) functions to buttons & simple button structures (low-dimensional buttons & button structures)
            - variables: button count, button function, button structure (combination, set, sequence), function probability, simplicity

          - highest feature count, given filter of 'simplest implementation': highest number of functions possible to implement simply (low-dimensional memorization)
            - variables: function count, memorization, simplicity, abstraction (type), button usage structure (scale like repeated clicks of a button, sequence like buttons clicked in sequence)
            - variable interaction rules:
              - 'when function count increases absolutely (all other variables being equal), memorization decreases'
              - 'when count increases but is organized simply (like accessing functions organized by type or scale with successive button clicks), memorization is constant'
            - variable structure: 
              - intersection of independent variable changes (function count & memorization)
              - alignment of simplicity & memorization changes
              - alignment of abstraction (type) & simplicity changes
              - substitution of proxy variables (substitute more measurable variable like simplicity for memorizability)
              - substitution of more measurable variables
                - substitute simplicity-filtering rules to identify complexity rather than using complexity identification rules
                - substitute similarity-filtering rules (what something is) to identify similarity than difference identification rules (what something is not)

            - optimized variable structure: 

              - maximized 
                - parameterization of variables that change on similar input
                - intersection of variables to optimize (intersection of highest function count and highest simplicity)
                - alignment of related variables (aligning memorizability & simplicity) that should be similar
                - opposition of variables that should be different
                - compression/merging/selection of variables that act interchangeably

              - structure application
                - sequence structure applied to causative variation (input/output)
                - topology structure applied where changes in variable values of a variable set can be mapped to distance (different changes do not produce equal points)
          
          - maximized features: use highest-variation interface as input to generate temporary/editable config (app configuring which implementation to apply, which custom functions to use, which hardware to combine when ordering/updating)
            - variables: config input (voice, button), variable variation, config adaptability, config source (custom user-defined function, open source/multi-vendor libraries)

        - how to generate optimized mathematized implementations for intents
          - apply structural definitions of components (rules, variables, intents, concepts)
          - find interface where these structural definitions of components can be depicted according to their variation (dimensionality), interactions (substitutability, causation), & metadata (accuracy)
            - interface where variable structures (constant, sequence, input) and function structures (interactions/alignments) can be found & connected as needed

    - vertex variable structures
      - quantum physics, prediction/derivation tools, building automation tools, testing tools, learning/adaptation tools, system rules, computation power are all vertex variables of information
      - which structure (sequence, network, set, or cycle) of vertex variables is most efficient

    - add to markets:
      - examine net effect of competition on markets
        - does allowing companies to fail have a net negative effect thats biggest on the risk (insurance/debt) industries, organization/analysis (ratings, group investing), info markets, govt industry, or is the negative effect exported to other countries or used as an input (legal precedent, example to use for future legislation) or do negative effects continue to be transferred to end-nodes of debt chains (those oppressed by stacked inequalities)

    - add to internet optimizations: add local data backup centers to cache copies of critical data just like backup electricity generators to methods of recovering or rebuilding crashed systems with alternate data sources

    - add to govt:
      - in order to prevent "info genocide", where groups that dont adapt quickly (bc of lack of group self-awarenesss & resulting adaptation, or alternatively a lack of lighter/temporary/optional connections allowing competition in a free market of group membership, requiring adaptation in group market to survive) are eliminated by groups with better organization/cooperation/adaptation/self-awareness or freedom given to members, you can:
        - distribute power limits to prevent any group from acquiring more power than other groups (more resources in the form of info, tech, land, supply chain dominance, market share, innovation advantage, organization)
        - distribute power artificially to offset naturally occurring imbalances from randomness
        - delegate power to tech automation
        - prevent power from accruing (prevent info from being storable/transferrable)
        - help all groups achieve self-awareness & independence and distribute it to members

    - finish core component metadata
      - identify any missing attributes/functions that cant be reduced further
        - example: 
          - everything that exists (has structure, either implied or verified) has an opposite/different version, so 'opposite' is a core attribute
          - everything that has structure can be verified to some degree, so 'verifiability' is a core attribute

    - add to science: 
      - examine whether imaginary numbers are a structure of uncertainty (bc of structural unverifiability), producing reversals of info with application of self-similarity (like time reversals) when interacting with other systems like physics
      - example of aligning & standardizing to interfaces to identify optimal methods to solve a problem:
        - use of non-structural information (randomness, imaginary numbers, & other sources of uncertainty) as an input for intents like 'exploring rules of high-variation structures like energy, info, or space-time'
        - example accidental application of this: https://phys.org/news/2021-03-imaginary-quantum-resource-theory.html
      - examine peak/valley of space-time wave as flow between energy (info) storage/usage types, possibly acting as opposite ends of a spectrum representing a trade-off (on liquidity, direction & other attributes)
      - examine magnetic/gravitational/electrical/other forces determining relationships of space-times
        - forces allowing:
          - non-adjacent & multi-directional space-times to be connected
          - space-times that arent efficient enough to be used as input to be left behind/dissolved rather than crystallized/connected
        - finding structures of lack where forces arent triggered bc there arent particles or other structures of energy there, to allow for uncertainties to develop & be used as input

    - add to bio system analysis: examine what types/positions of intentionally triggerable DNA damage can be used as a way to produce pathogen DNA sections at scale to generate immunity-producing immune response (mutation stack/factory)
      - structural (DNA/cell) damage/change triggers: mutations from toxins, exercise, animal pathogens, enzymes, competition for energy, & other types of DNA mutations
      - structural (DNA/cell) division/growth triggers: hormones, extra energy (energy imbalance based on need)
      - structural (DNA/cell) repair/recovery/regulation triggers: like bypassing digestion with amino acids to help recovery, timing cycles & alignment
      - other sources of structural change/standardization: 
        - gaps in cell types/attributes
        - sets of attributes
        - clusters of attribute changes/values
        - structural interaction levels
      - once you identify a section of existing DNA/RNA that, when isolated & edited, would produce a useful section of pathogen DNA to trigger immunity with minimal & possible editing (adjacent change) given exposure to a chemical or other stressor:
        - identify position in bio system where this change could be triggered by exposure in such a way that production of the DNA section is scalable (like in a cell that is about to be copied or a root/memory/template/stem cell) & where immune response would be quick
        - identify filter/containing structures necessary to give exposure/immune responses the time they need
        - identify type of exposure possible (injection, nanoparticle) and assemble components
      - identify evolution triggers (like cycles & variation of stressors) & use as a supply of change requests to trigger adaptive mutations for pathogens or pathogen types

    - example of alternate variable sets for predicting motion

      - exercise variables:
        - info (about optimizations, possibilities, rules, metrics)
          - attention/memory to focus on, remember & apply info
        - patterns
        - structures
          - point (metric threshold values, change points, decision points)
          - sequence: 
          - combination: multiple variables to make a decision
          - limits: time limits, energy limits
          - context
            - health
            - energy
            - environment
              - landmarks
              - agents
              - interactions/events
        - time
          - time structures (alternation, number of seconds, continuity of pattern applied)
        - functions 
          - core functions (test, start/stop, switch, remember, identify)
          - interaction level functions (decide when to speed up, plan decision points)
        - concepts
          - energy
          - agency
          - intent
            - exercise intents: recover, rest, test/find limit, test function, switch energy sources, apply info, identify landmark, align with music
            - other intents: what to do after workout, scheduling limits to work around, listen to new music, listen to music limited number of times

      - variable sets that can predict motion:
        - time cant be used on its own bc usage patterns may offer the illusion of equivalent alternatives that are actually different
          - example: pattern 'a-b-c' may occur just as often as 'a-b-d' without any distinguishable signals using available time info, so other interfaces need to be applied to predict c or d, such as contextual/intent probabilities, or patterns like intent patterns or change patterns 
        - agency rules
          - agents have known intents, which interact in a known way
        - interaction rules
          - energy, time, agents, & health interact in this way
        - energy rules
          - 'energy can be used to produce energy in other formats'
          - 'stored energy can replace agent prioritization'
          - 'excess energy can have these outputs when used optimally'
          - 'energy efficiency increases with usage'
          - 'high variation in usage increases energy coordination & distribution'
          - 'brain & muscle energy are related, in a pseudo-tradeoff'
          - 'high variation in energy usage can offset energy plateaus'
        - variable interaction patterns
          - 'using n number of variables to make a decision only occurs once out of every x decisions'
          - 'applying previously applied variable interaction rules is most common'
          - 'excess energy results in higher variability of variable interactions'
        - concepts
          - concepts & concept structures (concept set including 'energy' or 'health') can predict independently of other variables bc theyre a low-dimensional (conceptual dimension) representation of high variation (motion)

# content/config

    - import insight history data to identify insight paths (info insight paths like 'lie => joke => distortion => insight', system insight paths like 'three core functions + combine function with this definition + n distortions to nearest hub')
    - define default & core objects necessary for system to function (out of the box, rather than minimal config necessary to derive other system components & assemble)
      - add default functions to solve common problem types
      - alternate utility function implementations have variation potential in the exact operations used to achieve the function intents, but there are requirements in which definitions these functions use because they are inherent to the system. For example, the embodiment may use a specific definition of an attribute (standardized to a set of filters) in order to build the attribute-identification function using a set of filters - but the general attribute definition is still partially determined in its initial version by requirements specified in the documentation, such as a set of core attribute types (input, output, function parameter, abstract, descriptive, identifying, differentiating, variable, constant), the definition of a function, and the definition of conversion functions between standard formats.
    - document time structures (concave time explaining compounding similarities up to a point of maximum concavity, a structure that can separate from the other space-times)
    - systematize your definitions of info objects, to include analysis that produces relationships of core objects like opposites to their relevant forms (anti-symmetry) in addition to permuted object states (asymmetry), such as an anti-strategy, anti-information, anti-pattern
      - organize certainty (info) vs. uncertainty objects (potential, risk, probability)
      - make doc to store insight paths, counterintuitive functions, hidden costs, counterexamples, phase shift triggers
      - add technicality, synchronization, bias, counterintuition, & certainty objects leading to inevitable collisions
        - the collision of compounding forces producing a phase shift
        - lack of attention in one driver and false panic in a second driver leading to a car crash given the bases where their processes originate
      - define alignment on interfaces (compounding, coordinating, parallel, similar, etc)
      - start with these info object transforms that filter the most info: opposite, invalidating, symmetric, core, aligning, boundary-breaking, phase shift activating, structure stabilizing, constant changing, converging
      - add core info objects (core strategies, core assumptions) so you can make a network of graphs for a system
    - concept analysis:
      - how new concepts (gaps in network rules) evolve once structure is applied to prior concepts 
    - interface analysis:
      - limitations of interfaces & how to derive them
      - how rules develop on stability & how foundations are connected & destroyed
      - explainability as a space limited by derivable attributes from data set & cross-system similarity
      - vertex definition & give examples (as an intersection/combination of interface variables, such as determining/description(compressing)/generative/causative/derivation variables), around which change develops
    - change analysis:
      - generated object change types
        - constant to variable
        - variable to removal of assumption in variable type/data type

    - examine implementing your solution type (constructing structures (made of boundary/filter/resource sets) to produce substances like antibodies, using bio system stressors)
    
    - merge definitions into docs/tasks/implementation/constants/definitions.json
      - add to definitions.json
        - meaning
          - structure:
            - relevance (to a position/intent)
              - importance
                - utility value
                - uniqueness
              - similarity
            - system fit
              - understanding

    - clarify/resolve terms that can be conflated: 
      - shape/structure
      - rule/test/metric/limit/threshold/boundary/state change/phase shift
      - intent/priority/motivation/incentive
      - method/function/rule/pattern (pattern is a sequence of specific objects)
      - path/route/trajectory/traversal/order/list/sequence
      - object/entity/item/component
      - type/class/category/group/subset
      - closed/isolated/independence/unique/orthogonal
      - model/perspective/filter
      - standard/interface/index/symmetry
      - dimension/variable/axis
      - space/system/context
      - perspective/filter/standard/index & relationship to variables/operations on the interface
      - filter vs. rule is a similar question to attribute vs. rule - sometimes one format is better based on the info you have, sometimes its worth it to transform the format
        - interface network: a set of standardizing filters applicable to format information in way that it can be analyzed with interface-specific logic, 
        - a query of the interface network may also be a problem-solving automation workflow, if problems can be solved with the format sequence indicated by the interface traversal

        -  For a prediction function problem, the solution space is the range of likely prediction functions. 
        - The problem space is the route between independent variables and the dependent variable on a network - it can also be framed as the route between common prediction function terms for a data set like the input data set, and the prediction function. The original problem structure is also depicted as a subset of this problem space visualization.
        - The solution function can be a route on the problem space if the problem space is formatted as a network, for example.
        
        - interface: a useful standard for comparison consisting of the filtering object's definition routes, conversion function, core functions, objects, & attributes, and related objects like patterns & metadata specific to the interface. Abstract interfaces include cause, concept, structure, etc, whereas specific interfaces are other foundations where change develops in a clearly defined range that can be found in specific systems. The traversal of an interface implies finding a map between objects, functions, & attributes inherent to that interface to the problem objects, functions, & attributes. The application of an interface is an operation in an interface combination, mapping, injection, or other operation. 
        - function: a set of inputs, a sequential list of rules or rule lists (logic tree) applied to the input set, an output set created by changes to the input set executed by the sequential rule list, optionally including side effects from execution. A function may be formatted in a different way, such as a list of attributes, filters, or a network. The function definition includes: 
            - attributes: 
                - alignment: enforced/optional, core, required, emergent/output (built from core functions, with or without associated intent) 
                - interaction: cooperative/conflicting 
                - intent: generative, filtering, grouping, organization/delegation/ distribution/matching/grouping/filtering, classification, differentiation/ transformation 
                - scope: use case, context, range, host system 
                - related objects (like host spaces/systems & object positions in those) 
                - types: 
                    - core functions 
                    - meta (rule-modification/generation rules) 
                    - attribute rules (state, scope) 
                    - interaction rules (competition, binding, combination, sharing, collaboration, intersection, conflict resolution, trade rules) 
                    - assessment rules (metric, difference, definition, validation) 
                    - processing rules 
                    - change rules (update, distortion, maintenance, adjacency, conversion) 
                    - filtering rules (find, identify, define, alternate, organize, learn) - matching rules (fitting a structure, filling a structures) 
                    - application rules (inject, embed, apply) 
                    - derivation rules (structure, navigate, abstract) 
                    - decision rules (prioritize, select, compare) 
                    - formatting rules (standardize, isolate, cluster) 
                    - destruction rules (replace, invalidate, neutralize, remove, merge, de- duplicate) 
                    - government rules (monitor, correct, enforce, maintain, stabilize) 
                    - system rules (incentives, variance handling, optimization) 
                    - interface rules (change, intent, type, pattern, concept) 
                    - info rules (problem, strategy, insight, game, perspective) 
                    - variance (injection, leaks, combination, replacement, causal direction, uncertainty, risk, potential, probability, prediction) rules 
                    - information handling (storage, versioning, replacement, merging, monitoring, indexing, communication, interpretation, processing) 
                    - solution rules (variance/stressor/error detection, tracing, identification & handler) 
                    - structure rules (gap, boundary, system, limit, hub, object, link, network, filter)     
        - intent: any of an abstract goal with direction, a reason to use something, or an output such as the intended result or an unintended side effect, which can be an abstract priority like fairness, a concrete goal, and can have a starting & ending position. 
        - game: a set of intents/alternatives/limits/incentives/exploits/rules/risk & a definition of distance from intent fulfillment (position), usually resulting in the resolution of a clearly optimal route. The game definition includes: 
            - a game is a type of system & a mixed set, which can exist as a component of a system 
            - games can have many different structures like: 
                - a directed graph with a vector set representing possible agent intents/ functions/resources 
                - a system of nodes & links where agents need function input resources to traverse 
                - a decision tree where certain tree info becomes accessible only at certain nodes (adding uncertainty/risk) 
                - a set of trade options between nodes with different info change/update rules in a system to optimize a resource/trade/market metric 
        - concept: set of unique objects remaining once objects from other spaces have their context & contradictory attributes removed. For example: power is the object left when objects implementing it: resources => energy => input => potential) have their context removed, navigating up the abstraction stack from: 
            - the info layer (resources & energy), removing their contextual attributes/rules - to the abstract structural layer (input) 
            - to the abstract layer (potential, which is a related concept of power) 
            - so that the final object is defined in terms of other abstract objects on the top layer 
        - problem: may include any context or condition that causes a negative position or state determined by a metric for an agent in a system. The problem definition includes problem types like dependencies, leaks (variance, resource/info),  injection (assumptions/variance/control/randomness), mismatches, conflicts, imbalances, inefficiencies, incorrect metric, misidentification, gaps, limits, side effects: whether it's a closed system or leaks variance (function side effect example: before execution: pre-computing, during: memory access/overflow, after: process re-starting), specific problems like an enforcement gap (should have enforced rule but did not), an unintended use (involves integrated third party tech not under review), a malicious alternative route to get same output, a legitimate/alternative route to get malicious output. 
        - problem space: context relevant to a problem; the containing system(s) of a problem that may include related problems 
        - solution: may include any combination of events, methods, or steps that reduces the negative position or state for the specified agent. The solution definition includes solution types: 
            - solution-metadata solution: evaluating & comparing solution metadata for solution selection 
            - problem-metadata solution: evaluating problem metadata to evaluate metrics like problem-solving postponement 
            - generative solution: solution that generates solutions 
            - solution framework: provides starting point & structures for solutions to be built in/with 
            - problem decomposer: solution that reduces a problem's root causative (as opposed to just varying) parameters 
            - solution automator: solution that automates solutions of a type 
            - interim solution: clearly suboptimal solution while optimal alternative is built 
            - solution query constructor: solution that builds new solutions out of known solution types (existing structural solutions or core functions) 
            - structure-finding solution: solution that assigns a structure to information 
            - structure-fitting solution: solution that matches the gaps/limits in a problem structure to neutralize them 
        - solution space: set of possible solutions in a problem space, which may be reduced by applying interface traversals like solution space-reducing insight paths 
        - network: standard network graph containing nodes (sometimes called vertices) & linking functions (sometimes called edges), with or without node attributes, function relationship determining function shape, and direction as an indicator of intent or another metric. 

        - component: functions/attributes/types/objects/systems 
        - input information: can refer to original information input to the initial interface traversal, or traversal output information that has been converted, enhanced, formatted, or otherwise altered in a prior interface traversal, stored as a possible version of the original input information, and sent as input to another interface traversal 
        - interface: 
        - function: a set of inputs, a sequential list of rules or rule lists (logic tree) applied to the input set, an output set created by changes to the input set executed by the sequential rule list, optionally including side effects from execution. A function may be formatted in a different way, such as a list of attributes, filters, or a network. 
        - intent: any of an abstract goal with direction, a reason to use something, or an output such as the intended result or an unintended side effect, which can be an abstract priority like fairness, a concrete goal, and can have a starting & ending position. 
        - concept: set of unique objects remaining once objects from other spaces have their context & contradictory attributes removed.  
        - network: standard network graph containing nodes (sometimes called vertices) & linking functions (sometimes called edges), with or without node attributes, function relationship determining function shape, and direction as an indicator of intent or another metric.  

       - info conceptual relationships:
          priority = direction
          observation = insight = function = result = relationship
          conclusion = ordered_list(observations) + guess = coefficients + bias
          strategy = ordered_list(insights)
          strategy = insight + context
          problem = (combination of intents having different priorities) or (an resource distribution imbalance)
          intent = strategy + priority
          solution = (combination of strategies operating on variables with insight functions that reduce dimensions of problem (function-combination) or (resource-imbalance))
          type = combination(attributes)
          intents = function outputs, including unintended/emergent/unforeseen side effects (target/avoid)
          roles = functions
          relationships = treatments, intents, functions, insights, strategies, mechanisms, patterns, systems
          components = compounds, symptoms, treatments, metrics, conditions, stressors, types, variables

    - update links

    - integrate archive_notes/finder_info/functions
      Terms:
      - objects: a data set, function set, attribute set, class definition, type hierarchy
      - attribute value: value held by the attribute like True/False
      - attribute property: conceptual metadata property of the attribute like unique, identifier, static, etc
      - decisions:
        - choosing to execute one section of code over another; 
        - for example a conditional statement, design patterns, emergent usage/behavior 
          of user/system, bugs, assumptions, & possible input values are decisions since
          they may result in calling different code
      - relationship types: sub-type, causal factor, cooperating equal, different version
      - strategy: rule used to make decisions, possibly for a particular context
      - solution: strategy implemented for a particular context & problem type