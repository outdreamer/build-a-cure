  - fix indexing 'NNP NN NNS1 of JJ JJ JJ2' or postpone to pattern evaluation time
  - fix missing alts
      pattern_index::verb_phrase::plays a |VB NN| role::a NN role
  - fix one-letter alts
      pattern_index::phrase_identifier::ALL_N DPC ALL_N |VBG VBD|::N D N V
  - generalize alt logic to use embedded pair finding
  - add formatting to allow multiple items as keys in json and maintain order for interface network paths

  - fix supported stem assignment (endings like 'is': {'functions a', 'acts a', 'plays a', 'operates a', 'works a'})
  - fix charge function ('edit' is assigned positive score)

  - add core clause patterns 
  - fix pattern matching functions
  - finish pos, clause, modifiers code from find implementation
  - finish network creation function
  - strategy/insight graph
  - add a standard system diagram with radiating layer diagram
  - add other causal structures

  - add diagram for question derivation for service list

    - deriving the questions customers will ask for a set of services

      - which processes are complicated or not optimized (need to be in person for certain transactions that people would rather do online)
      - which processes involve changing information (account balance, transaction approval)
      - which processes are likely to have errors (auth)
      - which processes people will likely be interested in using the most

  - whats after dependence market (value in exploiting people's positions)? 

    - an independence market (value in setting others free of their positions)

  - whats after an information market (value in certainty)? 

    - a potential market
      
      - value in managing/arranging/creating uncertainty, manipulating/protecting time/change/potential, ensuring time/change enables information that benefits people

  - whats after a chaotic society with voluntary organization that allows corruption to gather & hide to advance other governments (feelings)?

    - a society organized by intelligence (as a proxy for potential), with alternatives to:
    
      - use stressors to create intelligence
      - generate intelligence using system/interface analysis
      - teach/share intelligence
      - find intelligence in a data source like the internet
      - install intelligence (learning models/maps, regression tools, logic tools like decision trees, prediction tools like a simulation/imagination engine)

  - whats after time-based change assessment? interface & variance & default-based change assessment

  - whats after system derivation & management? predicting systems from a boundary, the core input (boundary/limit management) - the goal is filter management, as filters can generate other objects

  - whats after humiliation/criticism & problem avoidance markets? achievement, solution automation, and solution-sharing markets

  - map core concepts to structures for use when building other concepts/objects

    - similar definition structures apply to similarly abstract concepts

    - change types (rule types like binding rules, boundary rules) can be used to generate different variations of a concept
      - power can be in the form of a connection hub (binding rules with an extreme value of the connection count attribute)

    - position is an important attribute in many abstract concepts (powerful position, power occupying the input position, position as an attribute of equivalence)
      - other core structural attributes can be used to generate the other concepts

    - concept definitions are useful so you can answer questions like:
      - is this system balanced/robust to change/have distributed power
      - what potential does this system have
      - what priorities does this system have
      - can this system impact many others (is it powerful)
      - does this system produce a conceptual attribute/object like trust/equality
      - what concepts (determining, generative, emergent, prioritized, optimized) do these systems have in common

      without translating them into structure yourself

    - power:
      - core structural factors
        - order
        - sequence

      - core definitions
        - input
        - cause
        - possible

      - core functions
        - enable
        - fulfill
        - change
        - force
        - cause

      - types

        - potential (freedom in options)

          - change power
            - can change many things
            - can transform to many things
            - can reverse/distort many types of change
            - can use many alternatives

          - cause power
            - can cause/create many things

          - interaction power
            - binding/limiting/filtering/coordination potential

        - structural
          - change rules (binding/boundary/interaction rules)
          - dimension set (determining which value/change types are possible)
          - connection (connection count, changeability, origin/destination/path potential as measurements of power)
          - set (powerful as a set)
          - intersecting (power from compounding/aligning attributes/objects/functions)
          - adjacent (power from interaction potential)

        - conditional power
          - powerful with conditions/context

        - possible power
          - stored power
          - formatted power

        - illusory power (appearance of power, from missing information)

      - attributes
        - abstraction/irrelevance/independence (context doesnt reduce/change power)
        - commonness (repetition can increase power)

      - related object definitions
        - option
        - alternative
        - input

      - interface definitions
        - the different definitions of 'power' types should be mappable with this attribute set
          - system:
            - powerful attribute sets
            - power in terms of system metadata like position (occupies powerful resource set or role in system like hub or filter)
          - change:
            - powerful regardless of applied change types/rates/patterns/definitions
          - intent:
            - executes a core power function intent (intent of change is optimize/correct)
          - function:
            - powerful in function/function metadata (enables a core operation like find/apply/build)
          - potential:
            - powerful in potential
          - cause
            - causative position

      - visualization
        - power indicates potential which may map to a lack of information (lack of filters/limits) or powerful information (common input, common activator)


    - truth:
      - core structural factors
        - match (does it match a trend/relationship)
        - fits (does it fit within the host system)
        - potential (is it possible/likely)

      - core definitions
        - importance
        - relevance
        - possible
        - certainty

      - core functions
        - calculate/estimate
        - predict/guess/theorize
        - measure/check
        - distort
        - map/derive (find connecting path)

      - types
        - logical
          - valid
          - factual
          - consistent
          - cohesive
        - structural
          - connection/path
          - origin/destination (given a particular definition of change & a value for that change type, how does motion create the object or emerge from the object) 
          - set (true within or as a set)
          - intersecting (overlaps with a truth at measured points)
          - adjacent (one transform away from a truth in the form of a shift operation)
        - subset (partial truth)
          - approximate
          - functional
          - alternative
          - interchangeable
        - distorted truth
        - conditional truth
          - true with conditions/context
        - possible truth
          - missing contradictory information
        - illusory truth (appearance of truth, from false similarities)

      - attributes
        - simplicity
        - curvature
        - clarity (is it structurable)
        - repetition (does it keep showing up or is there one example)
        - patterns (does it follow patterns of truth)
        - explanatory
        - knowability/determinability (is it possible to prove or measure, more than once)

      - related object definitions
        - distortion

      - interface definitions
        - the different definitions of 'truth' types should be mappable with this attribute set
          - system:
            - true within an attribute set
            - true in terms of system metadata like position (occupies same resource set in system or same role) or context (emergent output will be the same)
          - change:
            - true regardless of applied change types/rates/patterns/definitions
          - intent:
            - constructible with valid/true intents
          - function:
            - true in function/function metadata
          - potential:
            - true in potential
          - cause
            - true in causation/causal position

      - visualization
        - truth has structure, bc it indicates potential that has collapsed to information

    - equivalence: 

      - core structural factors:
        - position of determining points (for a line this is either endpoint)
        - distance/length/scalar
        - unit object/attributes/functions (what is the standard form, what core operations does it support)
        - potential field (what positions can it occupy with adjacent transforms)
        - angle of change
        - space/dimension set

      - core definitions
        - equal
          - all attributes/state/values match, irrelevant to the path to the object
        - similar

      - types
        - structural
          - origin/destination (given a particular definition of change & a value for that change type, how does motion create the object or emerge from the object) 
          - shape
          - path (same pattern between origin/destination)
          - set (same set of generative/output/determining objects or interface objects)
          - intersecting (y = f(x))
          - adjacent (one transform away in the form of a shift operation)
          - dimension (variable set, space definitions/conditions)
        - subset equivalence
          - approximate
          - functional
          - alternative
          - interchangeable
        - processed equivalence
          - duplicate
          - version
          - iteration
          - combination
          - standardized equivalence
        - conditional equivalence
          - equal with conditions/context
        - type equivalence
        - matching
          - opposite matching structures, opposite being an adjacent object to the original, and matching object being a fit of an object, indicating an opposite transform
        - symmetric
          - transformable/reversible transforms

      - attributes
        - degree of equivalence
        - conditions/filters
        - definitions

      - related object definitions
        - change
        - value
        - distance
        - position
        - scale
        - unit
        - angle
        - degree
        - space

      - interface definitions
        - the different definitions of 'equivalence' types should be mappable with this attribute set
          - system:
            - equal on an attribute set
              - differences
              - possibility/probability/related input/output/path probability distributions
              - emergent output
              - required inputs
              - types
            - equal in system position (occupies same resource set in system or same role) or context (emergent output will be the same)
          - change:
            - equal in change type/rate/pattern/definition
          - intent:
            - equal in granular intent fulfilled/neutralized
            - equal in output priority direction
          - function:
            - equal in function/function metadata
          - potential:
            - equal in range of potential

      - visualization
        - visualizing equivalence across definitions & types:
          - if you can standard objects to attribute sets, you can visualize as a graph of attribute sets where shapes map to attribute sets and visible or highlighted shape attributes are equivalent
          - you need to incorporate objects like conditions & definitions as system/space attributes

  - after identification functions

    - import rules for selecting interfaces to solve a problem on

      Function helped find unused functions
      Intent helped predict system priorities & find exploit opportunities
      System helped find efficiencies
      Pattern helped find insight paths/similarities

    - once you build function/attribute identification function
      - import insight history data to identify insight paths 
        - info insight paths like 'lie => joke => distortion => insight'
        - system insight paths like 'three core functions + combine function with this definition + n distortions to nearest hub'

    - make tutorial for interface analysis or at least reading list of posts & docs


  - extra tasks

    - add precomputing if a sub-pattern was already computed:
               'ALL_N ALL_N of ALL_N ALL_N'
         'ALL_N ALL_N ALL_N of ALL_N ALL_N ALL_N'

  - neural network nodes as facets of additional complexity/dimensions - calculating the complexity of a prediction function-determining problem (based on which distortions are likely from prediction function patterns) would allow selection of the necessary nodes
    - higher complexity requires that more combinations of weight path patterns be experimented on, and more nodes allow room for more experimentation
    - auto-configuring the network with common or data pattern-specific weight path patterns rather than random or equal could speed up training
    - reverse logic can be beneficial here - given the complexity of a problem, which features with which differences would have to occur for the training to be useful (or for the problem to be solvable)? check the data for those variable types/differences
    - use prior knowledge of patterns (insights like 'differentiating variables tend to cluster') as a way to organize analysis of the corresponding feature data pointed to by the insight (adjacent features are passed in to higher weighted node sets or node sets equipped to handle subtler differences)
    - different network or node sets can represent different problem-solving automation insights (one node can represent a 'differentiating variables' filter, another node can represent a 'cluster' verb/function filter)
      - if none of these node sets finds a pattern, the node sets can be recombined into new options (like another outer layer of a core function diagram) and re-trained
    - adjacent networks are created for stacked variables (like symmetry stacks, where features differ on a symmetry and symmetries are layered (like the hand-limb-spine symmetry stack)
    - phase shift points are identified first & the data is standardized around them - so when one type has extreme/compounding attribute values emerging as another type, that threshold is built in to the data (data near to that threshold is transformed to be higher to differentiate the types, or you add a third output category like 'transforming' or 'interim type')
    - possible causal shapes are identified first & the network is organized to fit them
      - for example, weight paths trained to highlight one attribute set are applied to other alternative equivalent attribute sets (an alternate causal route to the output)

    - what is the ratio of coverage of all possible feature interactions that is fulfilled by a standard neural network? what does the standard architecture reward in terms of clustering attribute sets for experimentation?
      - it rewards attribute sets that appear correlated in the data, with generalizations applied

    - should you use position as a determinant of feature importance? should position be removed from the data & another network trained on position-removed data?

    - would it be better to frame features in terms of system analysis (attributes, functions & routes)
      - an ear definition route can be framed as:
        - take other dimensions of change (than vision or taste) & assume methods to detect them (sound), then design a system (ear) based on efficiency as a priority to detect that change
        - stack a symmetry on top of the spine to make calculations & choose priorities, then stack another symmetry (face) on that symmetry (head) to host multiple change-detecting methods (facial features) to guide calculations
      - once you frame features in this way, training to find prediction functions should be trivial

    - can you partially reset the neural network mid-training to help improve generalization in addition to existing methods, so it doesnt tend too far in the data-dependence direction?
      - identify features that are likely to be data-specific, given whether the categories share that feature with different attribute values

    - training prediction functions on system data rather than data for objects within the system would add other gains
    ` - by knowing the structure of a system, you can infer insights like:
        - which object shapes are most compatible with the system
        - which forces/interactions are likely to evolve in the system
        - system metadata (stability, priorities, potential)
        - system info objects (assumptions, inputs, efficiencies, incentives, phase shifts, ambiguities, core functions, boundaries)
      - for the dog vs. cat classification, training on causal system data (the evolution system, the bio system, or the DNA system) might be a better target for the prediction function than training on images of the outputs of those systems
      - this analysis would help predict ambiguities (dogs & cats will have very similar features sometimes given how DNA & evolution works) and how to differentiate them (check for specific attribute sets in data, otherwise indicate that data is insufficient but here's the data you should gather for this problem type)
      - you might also be able to identify a sub-system that has the most valuable data for this prediction (mutation sub-system as a sub-system with a distortion function applied to the DNA system)


  - prediction model trained on conversations as encryption key/alg parameters, updated with new messages

  - how to check if a data set is similar to one that has already been trained, to avoid re-training to save CPU

    - store metadata about the data set like shape (groups/clusters, linear, random) and the metadata for those shapes (radius & overlap for clusters, distortion patterns & outliers for linear, starting point for random)

    - derive info metadata like type, cause, change patterns & check if determining/generative structures (core functions, symmetries, false similarities) match across the two data sets

    - identify patterns of variation once a similarity has been found, to avoid checking the whole data set
      - example: once you identify that both data sets have two output categories, what are the patterns of difference in the internal points of those categories (patterns in overlaps, misidentifications, corrupt/incomplete data, differentiating variables) - usually youre applying a categorization model to two categories that are very different (so its important to identify them correctly) but have some illusory similarities or features in common, making the categorization task non-trivial - so you'd look for patterns of differentiation within categories, to check if the data sets are approximately equivalent so you can use the same model without training - either specifically, storing patterns of differentiation for specific categories, in the model metadata, or generally, for general category differentiation patterns

    - store adjacent functions or specify a parameter range generated from the original function to identify functions that can be generated with accessible transforms or functions that are usually generated for similar variable sets (using common function patterns), to identify similarity in parameter values (using various types of parameters, at various layers on the causal stack, such as preceding functions like the function producing this function as its derivative, alternate functions like the series sum, or descriptive parameters like moment-generating functions)

    - identify function/data vertices, which are determining points like maxima/minima/inflection points as well as the minimum number of points necessary to identify the momentum of the curve, or points that indicate phase shifts in general

  - multiple servers/processors in one computer with one-way data transfers, so one server can be for local communication, one can be for offline work, one can be for browsing internet, and local/offline can communicate to internet-browing processor but not the other way around

  - rules-to-code translation tool - translating domain-specific plain language rules to robot code can be short-term useful for automation of service industry tasks like:
    - converting recipes/flavor-mixing strategies to cooking robot code (chefs can use a tool like this to make money short-term or sell their rules, if they have unique strategies)
    - converting new plant designs to genome editing code
    - converting local social insights to global code (avoid personalities like this, use these tactics to persuade, make this argument to get them to an insight position, etc)
    - converting adaptation insights to change-attracting system adaptation code
    - converting routing mechanisms/optimizations to drone code (short-term human insights like 'avoiding a particular street bc of construction' that data isnt adequate for)
    - the general task of converting rule sets (systems) or human-made visuals (graphs, blueprints) to code

    - machine learning can be used for initial conversion, then tweaked with coded filters like priorities, logic, organization, output
    - system analysis can be used to optimize beyond those standard filters
    - this needs to identify existing rules (or specific versions of abstract rules, distorted versions of standard rules) & filter them out
    - this is an alternative & and an interim step to raw code-generation given a set of intents

  - data viz can be automated using:
    - lie core function layer graph or individual lie type graphs, with an output intent layer (hide information, layer information, minimize information, obfuscate information)
    - intent-structure maps (this graph structure serves this intent stack, just like a function serves an intent stack)

  - each superposition contains components representing different possible filters for the physical laws they create at scale
    - some superpositions collapse into a particular attribute set
    - superpositions with different configurations may represent other interface queries or structures
    - knowing the internal structure of a superposition would mean we get to choose which queries come to life & become real
    - the design implies we shouldnt get to choose - but external forces (or unmeasurable/uncomputable forces inside the universe) should get to determine which configurations collapse & which differences are allowed
    - information has a lifecycle - its likelier to become more true the more its observed, up to a maximum - then it's likelier to erode as its depended on
    - observing a state (to produce the information of the observation) may initialize the static nature of that information, so other observers see either static information or lack of it depending on their perspective, as information becomes truer the more its observed, and they may focus on the lack of information or a different perspective than the initial state of the information

  - make AI models/graph databases of theoretical physicists opinions so I can suggest these concepts to them

  - search ideas:
      - inferring useful search filters based on customer usage history & intent
        - linked searches/user data with type/intent identification - if they are in a location with a certain pathogen and they search for cleaners, theyre probably trying to clean that pathogen so cleaners should be specific or at least an optional search results set should be linked to
      - automated attribute extraction/addition to search as a filter
      - search results as graphs: variables entered in search to display relationships found in data or graph images or graphetized articles
      - processed (aggregated) results - find the average/combined or plain language definition when searching for a definition
      - predicting what questions theyll ask next and adding those search results (or a summary) on the side
      - intent-based search guidance:
        - usually people who search for an answer are studying for a test, so additional widgets like suggested content could include snapshots of/links to: 'study guides', 'summaries', 'tutorials'
        - people searching for recipes are hosting a party & cooking other things, so suggested content could include snapshots of/links to: 'flavor graphs'
        - people searching for symptoms are trying to diagnose themselves or someone else, so suggested content could include links to diagnostic tools or graphs of symptom set frequency for conditions

  - shared custom meaning/dictionary maps so communication can be queries on their shared custom dictionary map - or a common map where queries specify pattern & sub-set to apply pattern to, and sub-sets are rotated

  - abstract functions

      - derive combinations & make sure you have full function coverage of all important combinations

          operations = ['find', 'get', 'update', 'apply', 'build', 'combine', 'connect', 'convert', 'balance', 'map', 'match', 'fit', 'filter', 'derive']
          objects = ['strategies', 'questions', 'incentives', 'efficiencies', 'metadata', 'definitions']
          structures = ['paths', 'limits', 'boundaries', 'bonds', 'gaps', 'layers']
          system_objects = ['attributes', 'objects', 'systems', 'sub_systems', 'types', 'functions']

        - check codebase function index for combinations
        - check that you have sample data in json for each combination

      - attribute/object/function match functions
      - specific interface identification function
      - standardization network-framing function to describe a system as a network (the standard structure) & position each object, identifying connecting functions
      - system analysis function (identify boundaries, gaps, limits, layers, incentives/intents/questions, & other system objects)
      - isolation function, representating function/attribute changes independent of system context with respect to position or time (snapshot/state or subset)
      - function to define (isolate an object/concept/function for identification, identify definition routes)


  - give example of each type of problem-solving workflows

    - workflow 1:

      - finish function to determine relevance filter ('functions', 'required') from a problem_step ('find incentives') for a problem definition, to modify problem_steps with extra functions/attributes ('change_position') to be more specific to the problem definition ('find_incentives_to_change_position') for problem_steps involving 'incentives', so you know to use the function_name to modify the problem step if it's between the type 'functions' and the object searched for 'incentives'

      - finish function to get all codebase functions & store them in a dict with their type, context/usage, and intents, just like functions are stored in the problem_metadata.json example for workflow 1
      - finish common sense check
      - finish defining objects in object_schema.json
      - finish organizing functions.json by type, with mapping between general intent functions like 'find' to specific info-relevant terms like 'get'
      - add common phrase check & filter problem steps by repeated combinations with common phrase check
      - finish get_type function to map info to structure using the new functions.json organization
      - finish apply_solution to problem_definition using problem_steps
        - involves building a function to evenly distribute objects (like information/types), given problem positions/agents/objects
      
  - types can be represented as directions (going farther from origin goes further up type stack, where similar types are adjacent)

  - need to fill in content:
    - finish intent/change type calculation for a system intent
    - selecting optimal combination interfaces to start from when solving problems 
      (how many degrees away from core functions, specific layers or sub-systems, what position on causal structures)
    - key questions to filter attention/info-gathering/solution
    - key functions to solve common problem types
    - development of key decision metrics (bias towards more measurable/different metrics rather than the right metric)
    - trajectory between core & important objects
      - example of choosing inefficiencies/exploit combinations in a system
    - research implementing your solution type (constructing structures (made of boundary/filter/resource sets) to produce substances like antibodies, using bio system stressors)
    - emergent combinations of core functions (include derivation of invalidating contexts for core functions)

  - change phases for causal analysis (interim, changing, diverging, standard, efficient state, constant, interacting, converging, on the verge of obsolescence, outlier, etc)
    - superficial cause, alternate cause in the case of a function, addressing input/output causes
  - framing on interfaces, decomposing causation, then identifying parameters of problem on layer & matching solution
  - independence (closed trade loops) as time storage
  - vertex as a pivot point for an interface



- notes

    - if something can generate a change predictably/consistently, it's a change supply - otherwise it's a change request, so output as well as causal position relative to the output is important when determining category
      - time may be a variance gap (a space where change is possible) to resolve a question/problem set - so not resolving it can preserve time, unless resolving it will allow for more potential or moving on to other variance gaps
