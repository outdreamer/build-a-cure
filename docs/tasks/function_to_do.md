# to do

  - yes, there's more torture, every day Im on my diet Im going to torment you with my achievements

  - merge examples in implementation_examples from various patents
    - identify any examples missing from patents in docs/tasks by diffing patent spec texts
    - organize function implementations, to do list, examples, logic

  - examine alternate interpretations of incompleteness theorem statements & implications:
    - 'has to use terms outside of the definition to define the object'
    - 'the definition has to exist in a system where contradictions to it can exist'
    - 'are there non-mathematical structures (cannot be standardized to math interface) that are necessary for describing math structures'
    - what about recursive definitions with no halting condition defined or triggered
    - apply the definition of absolute/contextual/alternate/potential meaning to this problem
    - finding a system to define/prove the rule set is required if the rule set has a false paradox/contradiction in its definition, which is resolved with conditional logic

    - add to math mapping:
      - math structures of 'certainty' ('inevitability', 'requirement')
      - math-logic structures of 'reference' ('self-reference' as 'recursion', 'identity or unit reference' as 'variable label', 'combinations of multiple reference sets' as 'nested loops', 'contradiction' as 'opposite' structures like 'booleans' or 'difference' structures in 'continuous variable values')
      - connect 'complexity' and 'reference' concepts using math structures ('self-reference' adds complexity x, 'iterate reference sets' adds complexity y)

  - add to science
    - trigger immunity by applying the abstract vaccine (vaccine for that pathogen type or general variant or identifiable features of the pathogen, or for a more demanding variant of the illness that also produces antibodies active against the original target)
      - this can build immunity to one illness that belongs to a family of types
    - how do you trigger the evolution of the illness inside the host, so that immunity can be built to variants of it, by triggering immunity processes after variants are allowed to develop?
      - can environments be relatively isolated inside the body so that a pathogen can evolve in those environments, like injecting it in a tumor, and then applying immunity triggers once variants are allowed to develop, so that more general immunity is achieved
    - what triggers of dna repair can be triggered after dna is damaged, and can a higher ratio/priority of dna repair processes prevent promotion of a pathogen that damages dna

  - add to govt

    - debt repayment avoidance: 
      - if everyone but a lender moves to a volatile currency, debts to that lender may go unpaid - this is a risk for nations that fund a lot of debt, with debtors using crypto market & adoption of a currency as a way to avoid repaying debts, and driving down the price of assets artificially, as if there's only one person who possesses a currency and no one will accept or trade using that currency, their currency becomes worthless

    - give example of how to connect perspectives across conflicts in a way that doesnt invalidate either side - if you dont connect them, they will continue
    
    - how to make a group question itself
      - convince them of their own stupidity, which is total lack of power, so they understand they dont deserve power, and accidentally start sabotaging themselves bc they wont believe in themselves anymore
        - the taliban are so stupid they cant identify that people will realize that if they take over a community that was peaceful, they dont have good intentions like 'bringing peace'
        - they also are so stupid they couldnt realize that the us was not trying to hurt them, so all their anti-us messaging was invalid & would be proven false thereby sabotaging their reputation, and if they were smart they would have worked with the us rather than demonizing the entire govt bc of a few rogue soldiers & some prior bad decisions like working with Russia's govt, which actually does want to cause harm bc they depend on war for money, but all they can do is think in terms of simple types like 'Americans', 'women', 'enemy', & 'soldier' instead of 'individuals', and theyre far more similar to their perceived enemies than they want to admit bc it doesnt make them feel good/powerful, but there are neutral or better ways to feel good/powerful (solving problems for people, becoming good at something like comedy/writing/math/games, sharing solutions, building understanding, targeting other criminals instead of innocent people, or just neutral activities like playing games)
        - if they were good & smart they would have solutions and wouldnt need to use guns or lies to convince people to let them govern
        - people who need to use tools of power (weapons, lies, & other cheap tricks) arent powerful (theyre stupid)
        - the taliban are burdens who only cause problems & need help becoming smart, not smart people who should be governing
        - the taliban are easily fooled by the same psychological tricks they use on other people - try to scam them and see how easily they fall for it, just like they fell for the scam that is their ideology, which they swallowed without question or thought despite being very obviously false, bc like all religions it makes unprovable claims that dont match reality & are carefully designed to not be testable/provable, whereas if their ideology was true, it would be based on science/math & other truth-deriving methods, which could be used to derive their ideology & no other ideology would pass their system's tests
        - they dont understand that logic is all it takes to disagree with them, let alone information/facts, common sense about human rights, etc
          - if the us govt was really evil, the us govt would have murdered everyone who wasnt a us citizen by now, instead the govt usually tries to save people being oppressed in other countries & usually succeeds, except in rare cases of corruption which we try to fix quickly
        - they dont understand that anyone can use cheap tricks to fool people, people arent difficult to fool, so anyone else could fool their victims as well, its not an achievement, so it shouldnt make them feel special/powerful
        - if a shape (like the shape of a gender or american) is all it takes to make them angry, they are the powerless ones who are easily triggered/controlled
        - their leaders/prophets are limited by their lack of intelligence 
          - the quran is not absolute truth, even if some of its statements make them feel good/powerful/valid bc it portrays men as superior, bc it was written by a defective mind incapable of logic
          - men are just a product of genes/hormones/nutrients like women are
            - most people are stupid, and stupid people are creepy, but stupid people can learn, they just need to practice thinking and get over the pain threshold that they experience whenever they think, which is all thats holding them back
          - 'blasphemy' is just noticing an error, which is a good thing because it allows someone to improve their system, and they should be happy that someone communicated the error to them, because that means the person thinks theyre capable of fixing the error, rather than abandoning them to failure
          - the way to handle a negative truth is not to destroy truth-tellers but to be better at handling & telling the truth than they are, or better than them in some other way like solving problems better
            - if they were funnier/smarter than women, they would be able to control women without guns, but instead they have to kill & threaten to control people
        - just bc someone looks like a woman, doesnt mean they think like the stereotypical woman, and it doesnt mean they have the same hormones or desires, bc hormones are just one variable & hormones do not control the brain, which cannot be seen without technology - the brain can control chemicals like hormones (you can see this by studying smart people, who tend to have lower sex drives & greater self control in general, but this is a controllable learnable skill, rather than an uncontrollable genetic/hormonal behavior)
        - they cant evaluate meaning (how something fits into a system), such as the meaning of their actions, given the fit of the output/impact of their action in the global system (the world), which is to cause ridicule for being so afraid of women's freedom that they have to attack it, rather than the fear they wanted to cause
          - if they really wanted to cause fear, they would become smart bc normal people are scared of smart people because they dont understand us because we're different and they dont like things that are different
        - rather than attacking people with violence, they can attack people with insults & comedy
          - if they want to feel powerful over a woman, all they have to do is make fun of her, and that should be enough for them & they shouldnt need to commit violence - if its not, their brains are defective & cant come up with a good joke
            - they should be aware of how insecure women are, how controlled by their needs they are just like men are, & how women oppress/attack themselves, and they wouldnt be threatened by their freedom anymore
              - they should how much women copy me to avoid work even though women know that copying me hurts innocent people I could help, and how much women attack me just bc I dont compliment them enough to fix their insecurities, theyre all afraid of being replaced & are willing to kill innocent people to avoid being replaced, and they'll never be afraid of or impressed by women again once they know how creepy they are & how many crimes theyll do to avoid work or the slightest inconvenience - this is one of the reasons smart people have a low sex drive, bc most people are stupid & stupid people are creepy criminals who will do anything to avoid work (unless you give them plenty of time & education to learn how to think, because every structural problem is fixable, and stupidity is just another structural problem)
            - people exist to realize their potential to help people & solve problems, people dont exist to be controlled, so smart people understand that the point of life is empowering people to realize their potential
        - the taliban dont understand that they are attacking the hormones & stupidity of other people, and that these things also control the taliban's leaders & prophets & supporters
        - the way to handle excessive hormones & stupidity is science such as thinking/medicine/education, not killing
          - if theyre afraid of women's sexual power, the way to handle that is by empowering men to control their sex drives, which would mean they dont need to control women & women will have less power over them
            - the more power they have other themselves (the smarter they are), the less other people can control them & the less they will want power
        - they should try teaching my system to women and see if they can learn, which if theyre not malnourished or brain damaged, they will be able to learn, and will become smarter
        - if the taliban was already smart, they would have come up with my invention, clearly I need to teach them how to be smart
          - if their leaders were smart, they would be teaching their followers how to be smart, but theyre not
        - they shouldnt see themselves as a cohesive unit, but rather a cohesive system of components that can be replaced/upgraded that is constantly improving 
          - if they do see themselves as a unit, they will feel bad whenever they make any error or bad decision at all
        - if they ever got true power, which is where people depend on them, they wouldnt want it anymore, so I know theyre not smart because of their aggressive pursuit of power, when if they were really smart, they would be trying to empower other people who need/deserve it, like oppressed people
        - the taliban are so stupid they dont understand they were tricked into making war by the Russian govt & other entities that benefit/profit from war
        - they dont understand basic social dynamics like projecting, because they keep projecting the fact the they are controlled by hormones on to women (both men & women are controlled by hormones, people dont define themselves as a gender and arent controlled by hormones/stupidity), because theyre so stupid that all they can think of is stuff that has happened to them, they cant figure anything out unless its happened to them, so they can retrieve it from their memory, which is just a rules database, not real intelligence capable of adaptation & problem-solving
          - if they copy people's sub-optimal solutions instead of inventing more optimal solutions, thats another way to tell that theyre stupid
        - their victims arent worthless by default, they are capable of learning & becoming smart enough to invalidate learning, if they have enough time to think like I did - if they did have time to think without being attacked or afraid (forced to think about only one thing - survival), they would have developed intelligence
        - their definition of power is flawed
          - they think power is 'ability to abuse without being abused back', which is just luck & lack of structures designed to punish every criminal, so really they are worshipping random (equal probability of outcomes), a far simpler concept, not power
            - their requirement of total loyalty is also flawed
              - if they were good & adding value to people's lives, people would want to be a part of their community, but theyre not so they have to try to force people to be loyal
          - the definition route of power (in relation to social interactions, on the 'social interface') is actually 'being depended on', which very few people would want once they knew that
          - if we punished every criminal immediately as soon as they do a crime, or prevent all crime by identifying intent by brain scans, thats just manually assigning rewards/costs & using rewards/costs to force behavior, so there is no meaning of good behavior, because we created a reward for being good so good actions might just be to get the reward, rather than just to do the right thing or help people realize their potential
            - rather than doing this, we should distribute intelligence, so that everyone can make good decisions & solve problems without attacking innocent people, and without being totally controlled by rewards/costs, because smart people can justify doing the right thing with logic/potential or other methods, even when there is no good feeling associated with it (or bad feeling associated with not doing it)
            - if we can push humanity over the rainbow of the intelligence deficit, society will be efficient & problems will be solved much more quickly
              - once that happens, people can build interesting new games & tools like gene-editing tech to add superpowers and we may even attract aliens who need our technology
        - if they were smart, they would have designed an organized, efficient society that was solving problems better than other societies, and theyre not
          - they didnt even come up with machine learning, which is just a way to update variable interactions by filtering out or de-prioritizing variable interactions that dont work, which most people should be able to come up with once they can think
        - if they dont believe me about how hormones control them, they should take libido-altering drugs to see how differently they think, and to identify that they are subject to hormonal control, just like their victims are

    - examples of connecting/equating opposing/conflicting perspectives

      - temporary alliances in war zones can be built against conflict suppliers (surveillance/conspiracy tech, US & Russia providing propaganda, weapons & other fear-based/promoting tools)
        - if theyd just given them cell phones & internet, they might have gotten addicted to gaming/arguing/insulting people online instead of killing people
      - criminals & victims can be connected with the intent to repair damage & reduce conditions leading to crime
      - incels & woman can be connected by intent to reduce conditions leading to sex crimes (social skills, lie identification techniques, lack of understanding about attraction/love, lack of science tools)
      - if either side was right, they would be able to come up with solutions & solve the conflict themselves, but theyre equals because neither side has won, so theyre similar in their inability to solve problems
      - if either side has to win by having more resources like more weapons/funding, they didnt really win, bc anyone could win with more resources, so theyre equivalent to their victims
      - if either side was right, they would be able to convince the other side that they were right

    - the 'physics' perspective can unite other perspectives in a testable way that can create trust in the perspective locally, bc these dynamics can be proved with individual local testing

        - apply conflict resolution rules like the following, which can be used by individuals to check that these rules work to reduce conflict/conspiracies/cults

          - apply social/info rules   
            - offering common enemies to unite them
            - delivering facts from trusted insiders
            - show how to take social power away from a side if they abuse their power (comedy, pity/empathy, exaggerated over-dignifying the other side) by offering them help, by teaching them what it means to abuse their power & how to fix it, or by teaching them how to have self-esteem/self-respect so they dont need to abuse their power
            - show how to forget (not forgive, which is asking too much, places a burden on victims, & repeats the crime) in case people want to move on from conflict (distractions, memory/focus management tools, alternate reality-enabling info tech, alternate perspectives like 'future' or 'abstract' or 'interface' or 'random/probability' or 'change' or 'solution/computational complexity' or 'tech/ai/robot' or 'multi-sentient species (animals, alien)' or 'multi-religion/heaven' or 'multi-verse/hologram universe/simulation/game, universe as game/test levels' or 'potential/quantum physics' perspective)
            - inspiring by example rather than by using force
            - showing people how to be happy without abusing their power (distributing power, solving problems), even if theyre selfish (they get more power if they distribute power/solve problems the best, not less power)
            - show people how to not condemn the other side completely, which invalidates other conflict resolution rules (show them probability of behaviors of the other side, given the context, such as limits like their available resources & experiences & stupidity), and focus on fixing the other side with understanding/science
            - show people how to get justice without starting new conflicts (try to make sure criminals have a way to contribute to society & invalidate conditions leading to crime, distribute justice as possible among victims of a crime type rather than of a specific criminal who may be unpunishable bc theyre dead)
            - show people how to humanize themselves to the other side (showing intelligence/potential & ability to empathize with & help their oppressors)
            - list the thoughts that their enemies have to understand that theyre similar in how they think and see how they arrive at their conclusions/decisions
            - show oppressors how theyre only able to justify their actions if they ignore the similarities to their victims & carefully avoid applying any logical analysis to themselves to reach self-awareness, as theyre often similarly stupid to their victims
            - show people how to manage emotions like disgust/anger without committing violence, by reducing it to a scientific process or reducing the method used to trigger these emotions

          - apply logical rules to identify flaws in belief systems
            - identifying that their beliefs are unprovable just like other religious beliefs but they choose to believe because it feels good to them, but those good feelings can be produced by other methods and are generated by physics & chemistry, not gods, and are subject to change & randomness
            - their beliefs & gods would be valid if they could be used to solve any problem but they cant, so they have to ignore facts to believe in their system
            - if their system was correct/valid, there would be no counterpoints or counterexamples contradicting it
            - identifying that cult members & conspiracy theorists are gullible & arent identifying facts, like the 'lack of incentives to execute the theorized plan', or lies like that 'a cult leader is a deity'
            - if their beliefs were correct, they would be able to control or understand all other systems like physics/math

  - since neural network nodes are in between function inputs & outputs, they can be used to represent any structure of 'contribution' (like 'change potential') that could change the output
    - this means all the possible structures that could build/derive/find the output (or equivalent operations like 'filter out what the output is not') can be represented by neural network nodes, structures of them & their interactions
      - this includes:
        - interim states in converting inputs to outputs
        - type identification functions
        - prediction function subsets/alternates/bases
        - solution component/input/describer/requirement templates
        - change types & change structures like sequences, as variable structures
        - prediction components/input/describer/requirements
    - this means the number of network nodes (possible contributing features) & layers (interim conversions to create/filter contribution versions, like an aggregation layer to aggregate features into a different interaction layer) can be derived by determining the features that could possibly contribute to the answer (prediction function) and the conversions (layers) required to create them from the inputs 

  - example of structures of usefulness in a problem space, like identifying useful functions & function structures
    - reusability: can be used as inputs to many other functions
    - successful: fulfills solution metric such as 'reaches destination node'

  - 'ml & a search form apply filters too, so everyone would eventually have invented interface analysis'
    - first of all, someone other than these people invented ml, bc the creators of ml are dead, but luckily someone explained their invention to these people, who now pretend to be smart
    - secondly, ml applies filters of neural network nodes to filter out info that doesnt change the output, which is a very specific function relying on a very specific insight that doesnt automate problem-solving bc think of a case where 'the change in output wouldnt be possible from the input data' which is all the ml can handle, my invention applies filters in both an abstract & structural way to connect various important variables like causes/intent/potential/change in a way that allows these objects to be connected to create meaning
    - ml cant evaluate meaning, it can only tell if one variable changes another
    - my invention can evaluate meaning, such as whether the output of a query is relevant to the general problem-solving intent, if it contradicts another solution, if it solves another problem, if it creates another problem, etc

  - how do you build a contradiction-identifying function:
    - apply structures of contradictions:
      - example: 
        - a function that diverges from the original function in an opposite/different direction is a 'contradiction' of the original function (though it could also be an 'error' structure if misinformed or otherwise falsely constructed)
        - an outlier of a function is another contradiction structure
        - true/false difference: an assumption that something is true, like that something is required, can be proved false by finding another way to generate the output without the requirement
        - identifying whether an assumption or other solution component is 'relevant/meaningful' is another source of finding contradiction structures - if something is misapplied in the wrong context where it doesnt fit, the conclusions based on it are likelier to be false
          - how to tell if an assumption doesnt fit
            - if it contradicts a requirement
            - if its inputs/outputs dont fit with the inputs/outputs of the functions its connected with
            - if it interacts with other components of the system where its injected in ways that change their assumptions/requirements/inputs/outputs
        - identifying whether you can change the system to match/contradict the assertion is another source of finding contradictions
          - it may not be true where its applied, but can it be made true with available operations in a similar position/structure or within the same system?
          - there are degrees of contradiction, and an 'absolute contradiction' where an assertion is totally impossible is relatively rare compared to partially false assertions, based on provability/verifiability, bc someone is less likely to make a totally impossible claim (that could never be true in any system) than a partially false claim (that could be true with adjacent changes)
            - structures of contradictions like partial contradictions are useful in determining the degree of contradiction
        - opposite structures of contradictions (consistencies/similarities) are another useful source of contradiction structures by definition, as well as inputs to contradiction structures like variables & maximally different objects from those in the assertion (to maximize the chance of finding a contradiction)
    - find a space where these contradiction structures are clearly identifiable and differentiable from non-contradiction (confirmation/matching/similarity) structures
      - an object node network might make the 'diverging function' structure obviously different from a converging function
      - a function that cannot generate the outlier from the inputs/components/assumptions of the function would make it obvious that the outlier is a real contradiction
      - a function network can identify alternate routes to an output, to prove that a required input is not actually required, given the definition of a 'requirement' (an input necessary to generate the output)
    - these methods apply interface structures like definitions, inputs/outputs, differences, & alternates to identify/exaggerate differences in the 'asserted true' & 'actual false' structures to make the asserted/actual structures obviously different

  - add to solution automation workflows

    - integrate other sources of certainty than 'an update function' with a solution, so it applies:
      
      - rules of optimization & certainty-development like:
        - build from understanding (meaning interface) first, rather than building to get understanding
          - new information & rules discovered should usually sync with previous information & rules & other interface structures, as there is rarely a discovery that requires a change to many/all known rules, and these discoveries follow patterns & rules as well (like that 'changes develop on interfaces that can contain them' and 'changes follow change patterns/rules & comply with other change interface structures like change components')
        - build in learning/update functionality at every point to allow injection of new information to correct understanding & understanding-derivation/generation/finding methods
        - build in functionality to assess the comparable reasons a solution/prediction/answer is right/wrong
          - this is to filter out solutions that have more reasons it could be wrong compared to other solutions or compared to reasons it could be right (like being 'possible' with known rules)
        - build in functionality to evaluate the probable error types & cost of those error types for any given solution, compared to other solutions, and rules to minimize error costs (like generalization, updating quickly with new information, identifying the reasons for contradictory/confirming information, identifying sources of bias & other error types, etc)
        - build in functionality to evaluate solvability & resources required to solve a problem before solving it, compared to its possible solution's value
      
      - so that the system can do other operations than just 'make a prediction of a variable interaction function', like:
        - predicting updates to their prediction function
          - by understanding that the differences in data sets & prediction functions follow patterns with varying success, and that ml is a pattern-identification tool (creating an 'intent' or 'input/output' or 'structural' match between the problem structure & a solution to resolve it), ml can be applied to predict how a particular algorithm will create a prediction function from a data set, given patterns of differences between inputs/outputs of a particular algorithm/parameters/network applied to a data set, and given how ml is applied in similar cases like with complex data sets of many variables that vary on subtle differences that humans are often unable to identify, or on understanding that humans havent built in that domain yet
          - by understanding that the types of problems humans find difficult is likely to mimic the meaning & understanding & other interface components of other systems, ml can be applied to check for patterns of those structures first as an initial filter
        - predicting error types & costs of a solution
          - understanding can generate error types of a prediction function like specificity, and these error types can be integrated into a solution with more certainty sources & types built in
        - predicting solution structures, variables, & optimal implementations
          - understanding can identify: 
            - patterns in prediction functions (which make them easier to check/generate/find/derive)
            - the standard attributes & structures of prediction functions, which can act as solution filters/structures, and the reasons why these are standard solution attributes/structures (an average is a standard solution structure of a prediction function because of the cross-system insight rule that 'variation typically occurs around a symmetry, like an average')
              - continuity, conditional subsets/alternates, averages, specificity, & probability distribution
            - interface structures of prediction functions
              - sub-interfaces, assumptions, examples, contradictions, errors, causes, concepts, intents, logic, & probable changes
            - solution structures like topologies of solution variables that have peaks at optimal solutions, or solution components/inputs/requirements
        - identifying whether a solution confirms/contradicts existing structures & other sources of certainty, to identify probability of being correct (like whether it has known contradictions or flawed assumptions or uses logical fallacies or implications rather than information)
        - check that predictions/solutions from various certainty sources match up with each other, as an additional certainty structure, given that other certainty sources are integrated & can be applied, either as solution generators/filters/verifiers
        - identify that the host system where a prediction function is being injected can be used to verify the prediction function, so if that information is available it can be used instead of the original data set

      - this applies interface analysis & specifically the understanding/meaning interface to a specific solution (as opposed to building a general solution-finding/deriving/generating/applying system implementing interface analysis)
        - so a prediction function for example would have:
          - an 'information injection' structure to integrate new information like 'online learning')
          - functionality to split itself into subset functions or alternate functions when it identifies that different functions are required
          - a prediction function to predict its own changes
          - a reason (intent/cause/meaning) for its own structures, like that it complies with other known rules or fits into other known systems
          - a set of interface structures that it uses as understanding of the variable system, like variable types, variable interaction types, and variable structures like combinations/alternates, and may apply other methods like ml or concepts like randomness/outliers/change to reduce any remaining uncertainties

    - apply other changes/differences (like 'opposites' or 'gaps') (or patterns/generative functions of changes/differences) of error or error-relevant (like 'sub-optimal solution' structures) to identify what is not a solution & differentiate from it to find solutions
      - finding the 'gaps' in sub-optimal solutions identifies spaces where optimal solutions can exist by definition
      - this provides a different target when solving a problem (a solution filter that enables adjacently identifying multiple optimal solutions, once applied) than aiming for the solution itself

    - apply specific useful system concepts like 'democracy' or 'freedom' to problem systems given their usefulness for general problem-solving intents like 'organization' (of a neural net architecture or another problem system)
      - example: apply it to the ml system to generate networks where a node can do whatever processing it wants, as long as its output doesnt contradict another node's processing functions and its output contributes to the global intent and doesnt require more default processing inputs/functions than other nodes
      - these useful structures can be found with connection sequences to problem-solving intents
        - example: for the problem-solving intent of 'organization', the connecting structure would be 'government', which organizes interface structures of a complex 'society' system such as 'info' like laws & 'functions' like law enforcement & defense
          - other structures that 'organize' interface structures could also be possible connecting structures to useful system-specific or abstract concepts related to those organizing structures
          - the same applies for other problem-solving intents than 'organization'

    - find/generate/derive & apply structures where they are determined to be useful by some structure of usefulness
      - the structures are known to fulfill an intent, given the known intent of the structure
      - the structures are/generate/derive/identify either inputs to subsequent functions, or outputs of the function itself
      - example: the combination of 'freedom' & 'interactions' is a useful structures to 'generate difference' to fulfill intents like 'resolve uncertainties' bc of the structure of usefulness 'find/generate/derive inputs' (to the 'resolve uncertainties' intent, bc if you have a way to generate differences, you can test if these differences explain the uncertain/complex system, so 'differences' are an input to that intent and anything that generates an input to that intent is a useful structure)

    - apply error structures to known standard/sub-optimal solutions to account for & correct probable errors in known standard/sub-optimal solutions (which can be bad guesses/approximations, so hardly a solution but still a useful origin structure to base changes on)

      - apply error & change structures to error structures to generate solution structures (the opposite of an error is a solution)
    
      - apply variable & error structures to interaction structures (like between input variables, interim variables like weights, or processing functions) to account for errors in their interactions, generate possible interaction structures connecting them, and generate alternative structures to filter out
        - weights are an interim structure that can be an input to the solution so they can be treated as variables to analyze on a secondary basis
        - example: 
          - apply rules of variable interactions like 'high variation variables are likely to be found together or in a connection structure, unless some distribution structure occurs between them to distribute variance' to weights, inputs, network layers, etc
          - apply error structures like 'opposite' of the correct value to existing variable values to generate altnrnates to filter or form an alternative base
      - apply change & error structures to structures of standards to generate possible solution structures if the structures of standards dont fulfill solution metrics
  
    - apply difference patterns in how solutions change with different info to:
      - identify whether the minimum info to reach a optimal solution is available
      - generate changes to existing solutions to test if their change patterns match changes in solutions with different info in a way that leads to an optimal solution

    - apply connection structures to connect useful structures so other useful structures can be found/derived/generated from input useful structures which may be more adjacnet to the problem space structures
      - example: find connection/difference patterns between structures like the following, so that either can be generated from the other:
        - requirements & vertex variables
        - efficiencies & ambiguities
        - expectations/predictions & outcomes
        - problem/solution structures
        - filter input/output (solution space & solution)
        - solution components & solutions
        - connection structures/interim states & problem/solution structures
          - applying pattern & difference-identification algorithms to interim states between problems/solutions generated by other solution functions to identify patterns that solutions typically take and connections between interim states to reduce the time to generate interim states & solutions (like weight path patterns in an ml network are an 'interim state' of the input data while its being converted to a solution), and so these interim states & solutions can be generated without the original tools used to generate them (for the same problem or other problems)

    - find combination of interface structures (like change types/sequences) that creates a topology of solution optimization that can be navigated with methods of finding global minima/maxima
      - generalization: apply other interface structures that can solve a relevant problem (like 'finding a maximum value') once info is formatted in a particular structure (like a function or topology), and convert problem system info (like possible solutions) to that input structure, optionally using interface structures (like solution filters to reduce possible solutions), as a way of applying the 'input-output sequence' structure using useful functions on various interfaces
      - example: cluster analysis to 'identify similar solutions of different types' is another example

    - different origin/inputs/data can produce a solution faster, even invalidating any update/learning process (if you have the right info, a solution may be obvious and no learning is required, as your data is optimized so no learning optimization is required)

      - apply data optimizations (pre-processing) to find/create optimal data to reduce learning requirements
        - prioritize rules that generate or form a basis for other rules, like physics rules, so any info that reveals physics rules is highly prioritized
        - this includes rules like 'apply change patterns & other change structures to data that typically produce solutions'

      - understanding based on meaning can also reduce learning requirements
        - applying interface analysis to identify useful interface structures like 'probable variable interaction structures' or 'approximate variable interaction structures' improves the probability of a successful variable interaction prediction function, which is based on the insights that reflect understanding & reduce the requirement to learn a new function with no understanding, which interacts with the meaning of these structures
          - 'probability is a relevant concept to prediction'
          - 'variable interactions follow patterns so they have corresponding probabilities'
          - 'approximation is a relevant concept to prediction'
          - 'approximations are related to predictions in the form of "minimal difference"'

      - anything that can find/derive/generate understanding or find/generate/derive optimized data can also act as an alternative to learning, understanding, or optimized data, with varying success rates
        - for example, a known solution database may act as an alternative
        - a rules database of known interactions may act as an alternative to a meaning-deriving/finding/applying/generating function
      
      - these are static alternatives so are likelier to be sub-optimal, so other structures of optimization (like generalization, or generating causes/inputs rather than original components) should be applied
        - an alternative that can update itself is by definition likelier to be optimal
          - an alternative that can find/generate/derive/apply itself is likelier to be optimal
            - an alternative that can find/generate/derive/apply components/inputs of itself is likelier to be optimal
              - an alternative that can find/generate/derive/apply components/inputs of anything (including other alternatives) is likelier to be optimal
                - an alternative that can find/generate/derive/apply components/inputs of find/generate/derive/apply functions is likelier to be optimal

      - why is it important to have alternatives to learning functions/structures?
        - bc learning structures usually use some form of reward/cost to identify information that is more valuable, given information that was previously valuable
        - but rewards/costs that are specific to solving one particular problem is how bias develops, so learning functions will inevitably produce errors some of the time

      - this is why its important to have other interface structures in place, which can identify alternatives that could contradict/neutralize/prevent bias from developing
        - this interface structure can be another function network, or a parameterized bias-assignment function that changes learning costs/rewards to avoid bias development
        - other examples:
          - change-function interface structures: function network with updated weights to indicate prioritized useful info (ml)
          - cause-function interface structures: cause-identifying/generating/deriving function, generative functions in general
          - potential-structure-logic-function interface structure: applying logic to rules about what is possible to create filter functions to filter out impossibilities in the solution space of possible solution functions

    - apply differences in problem structures as a filter of solutions (solutions to different problems have to have a correlated level of difference in the solution)

  - apply useful structures like 'alternative routes' to the ml network problem space by substituting other useful functions that accomplish the same intent
    
    - example: rather than 'feature reduction', apply 'compression' or 'summary metrics' ('average' or other common values or probability distribution) or 'cause/intent/type identification patterns' which also has a high input/output ratio, but retains the input info, which is an important input of other functions eventually applied to the output

    - example of injecting structures of relevance like certainty structures into ml neural networks
      - example of injecting structures of 'obviousness' into a network would be highlighting features manually during pre-processing

    - example of how to identify protein-folding as an important variable in illness: structures of variance (high-variation) and structures of relevance (adjacent to the processes used by various illnesses)
      - this is bc of the:
        - 'structural similarity' between input/output of the function, which is similar in the complexity of the input protein structure & the output illnesses
        - insight that 'variation doesnt just disappear in a system, it has to be routed somewhere, so if there is complexity or variation in inputs, the outputs will reflect that attribute as well, unless a complexity/variation-reducing function like a delete or cause/intent/type identification or compression function is applied between them'

  - main variables of ml:
    - data
    - data pre-processing & standardizations applied
    - data format
    - data processing functions in network
    - network constants
      - node count, node layer types, node layer type count
      - activation function & thresholds
      - loss function (how a prediction error is determined)
    - network learning algorithm (how weights are updated, like backpropagation)
    - network learning algorithm parameters
    - interface structures embedded in ml, either explicitly like 'feature subsets' or implicitly as emergent structures like 'merged feature structures'
    - general combination types of these network structures
      - supervised, unsupervised, reinforcement

    - you generate a new ml algorithm/network when you have a theory about the structures of relevance (feature position, feature structures, feature interaction structures) for a particular data/problem type
      - these relevance structures can be used as variables to generate new ml algorithms/networks
      - interface structures that are frequently useful or specifically useful for a problem type can also be injected as variable values, instead of selecting these variable values by other methods

    - out of all the interface structures, a given network structure set can identify a subset of them better than other algorithms
      - difference types, interaction types, function types

    - given the variance & interactions in the data set, only so much info can be derived from it
      - additional restrictions are added by other network structures, like the algorithm
        - meaning a subset of the 'possible info derivable from a data set' is derivable with a particular algorithm
        - in addition to this subset:
          - a set of errors can be created and added to the 'derived info' as legitimate info by the network structures
          - a set of errors can be identified by the network structures
        - the network constants, learning algorithm, & other network structures allow other errors/info to be derived/identified/created
      - these structures combined may interact in a way that allows progress toward the optimization metric, or neutralizes/contradicts progress
    
    - applying interface analysis to network structures
      - the interface structures of a set of network structures may not align
        - for example, a set of network structures standardized to the logic interface may not allow logical connections between the structures once converted
          - including logical structures like assumptions, connections, conclusions, contradictions
        - applying the concept of 'adjacence' as a source of 'relevance' with intent to 'identify causation' may not make logical sense if the output of that operation is later 'randomized' in some way, so these operations would have to be reversed
          - example: if randomness in the form of 'noise' is added to the data set to augment it for robust training, and then 'adjacence' is added as a source of inputs to the 'identify causation' intent, that wouldnt create a 'logical contradiction' structure
          - the original sequence cant use logical structures to reach the target structure, indicating that a particular set of network structures isnt a useful combination
      - a useful solution space-filtering structure is 'if it doesnt make sense with the laws of physics (connectible on the physics interface), its not a good idea'
        - example: 
          - aligning 'attention' in the form of extra computation power with 'complexity' makes sense with the laws of physics, even if its not an optimal complexity-reduction method (such as reducing a complex structure on other interaction layers that compress features efficiently)
          - if its more costly to create, its less likely to be true/real
            - high-variance functions connecting exact data set points are less likely than a general function with fewer changes/terms
            - functions of one variable with many terms of that variable are less likely than functions of one variable with few terms of that variable, just bc of the improbability of a variable having a causal relationship to another variable without any other variable interactions in isolation
          - if x's causal variables arent correlated with y, a causal relationship between x and y is not established, bc of how the 'sequence' structure works

    - the problem with incremental weight adjustments typically found in network learning states is that they might miss non-adjacent weight adjustments that would be useful

      - an algorithm that looks for minima with incremental steps and encounters only costs in one particular direction, resulting in a deactivated node or deactivated node set barrier, would fail to identify a minima in some functions with more complexity
        - this is why its important to generate maximally different bases of the possible solution space, and start with those as initial inputs
        - in a network structure, this could be fixed by adding nodes where a deactivation occurs, optionally according to known 'deactivation followed by epiphany' patterns or up to a general threshold of added nodes, to avoid failing to explore a useful high-cost direction, that may not occur in the data set but may be possible given the problem space & variables
        - its also useful to calculate possible stabilized solution & error structures in the problem space to:
          - identify error types of failing to identify these stabilized solution structures
          - design an algorithm that can find these stabilized structures efficiently in the data set, if they exist
          - example: 
            - generating all the possible sub-species (stabilized solution structures) given a set of required & probable features of the general species (as well as other important variables like costs the species sustains & their interactions with environment & other species), so that if a deactivation in one direction occurs but there is a sub-species possibility in that direction, the algorithm adds nodes/iterations as needed to pursue that possibility in the data set, and retains a nonzero weight or activation path in that direction just in case, even if no examples of it are found in the data set, once the direction of a particular sub-species is identified in the network structure

      - the whole point of 'weights' is to 'highlight features that are useful' (after identifying & removing irrelevant variables like 'orientation' in the 'image-categorization' problem)
        - the corresponding error type for a learning method is:
          - 'if a learning method of weight update cant update weights in the structure that matches the actual prediction function'
        - other error types:
          - 'if a useful feature structure like a combination cant be identified by the network structures like the network feature/weight processing functions, the weights cant reflect the right feature structure to use in the prediction function'
            - its important to identify what feature structures a network structure set can identify & use
            - you can also derive what feature structures would be useful to solve a particular problem & start with those as an initial structure when generating/identifying an algorithm to create those useful feature structures
              - example: 
                - for the 'image-categorization' problem, derive that 'features of varying size/position' (leading to 'subset' feature structures) might be a useful feature structure to generate in the network & assign weights based on this feature structure
                - for the general 'find a prediction function' problem, derive that 'function-subset functions, variable subset functions, base functions, or data point subset-functions' might be useful feature structures to allocate training computation to

      - identifying irrelevant variables for a problem type
        - variable structures & interactions likely to indicate an irrelevant variable
          - variables whose changes dont correlate with changes in the other variables, so they might as well be converted to a constant
          - variables whose changes/interactions can be compressed in a type variable, where the type variable is sufficiently predictive & variations within that type are irrelevant

  - generating a function set to fulfill an intent can encounter errors after generation time, so injecting 'self-awareness according to responsibility/ability' is a useful structure to apply at the function level
    - a function that can access usage info (the outputs of functions triggering it and the inputs of functions using its output) can identify before an error is encountered if it will produce an error for the functions using it
    - allocating awareness of context in the form of triggering function info based on a function's ability to identify & correct errors is useful at that interaction level
    - cross-interaction level interaction awareness is also useful in addition to intra-interface interaction context awareness, so an application can identify which functions that use it may encounter errors given their input/output structures
    - this is useful in ml at the function & node-level, where a function/node can identify when error structure patterns (like 'deactivation barriers to valuable feature values' or 'incorrect feature structure to identify all possible solution structures') are occurring in adjacent functions/nodes & modify its params like thresholds to account for that imminent error before it occurs
      - this adds error-correcting functionality to the network on certain interaction layers, which can create other error types if the error-corrections aggregate into an error, so network-level error-corrections can be built to handle this emerging error structure from lower layers
  
  - add to error-finding methods
    - identifying & generating known useful structures like 'symmetries', 'variables', 'subsets', 'interchangeable alternatives', 'maximally different inputs' & 'bases' & 'type/phase shift thresholds'
      - identifying & generating combination structures like 'maximally different inputs around bases'
    - identifying gaps in known useful structures explaining data points (where data points arent explained by those known structures) & generating inputs in those gaps other than those data points

  - add to ml

    - identify interface structures that can generate a particular algorithm or interface query
      
      - for the example problem 'sum a sequence', multiple interface queries to solve it include:
        
        1. identify minimum info to fulfill the following sub-intents & execute the steps to identify that minimum info for those sub-intents
          - identify various standard sequences like 'random' and 'sum of n * increasingly negative exponent n' or 'sum of factorials' or 'sum of n / (n - 1)'
          - identify distortions of these standard sequences
          - identify efficient method to filter these sequences
        
        2. generate a probability distribution or generative function from a subset & extend that as an assumption for the remainder of the sequence
        
        3. identify filters differentiating a subset of the sequence from other sequences ('opposite' structure, or 'what the sequence is definitely not') & apply to the sums of those sequences (the sum is 'definitely not the sum of the other sequences')
      
      - for each of these queries, the associated priorities/assumptions & other interface structures that allow that interface query to be seen as an optimal solution include:
        
        1. interface structures associated with 'identify minimum info to identify if a sequence is n distortions from a base sequence'
          - assumptions
            - an assumption that the 'minimum info to identify base/distortions is more computable than iterating through the sequence' and that the 'sequence is an adjacent distortion from a known base sequence'
          - errors
            - if a sequence is n distortions from a base sequence and the algorithm only checks n - 1 distortions from a base sequence, this will fail
          - error-handlers (to integrate with interface query or other solution)
            - the base sequences have to have a high coverage ratio, so that its unlikely that the distortions checked will not find the sequence if its a non-trivial number of distortions from a base sequence
        
        2. interface structures associated with 'identify generative/descriptive function of a sequence subset & apply to remainder of sequence to get summary metrics'
          - assumptions
            - that a selected subset is representative of the sequence (that it has a degree & patterns of variation reflected in the complete sequence)
          - errors
            - that the selected subset is not representative of the complete sequence
          - error-handlers (to integrate with interface query or other solution)
            - identify differences between a subset & a sequence that could make a subset non-representative, identify tests for those differences, & apply them as a filter for subsets
        
        3. interface structures associated with 'identify filters differentiating a subset of the sequence from other sequences to apply as a filter of the summary metrics'
          - assumptions
            - that the differences in a sequence are reflected in its summary metrics as well
          - errors
            - that the differences in a sequence are not reflected in its summary metrics (sums/averages can involve neutralizing terms, allowing for significant differences in sequences)
          - error-handlers (to integrate with interface query or other solution)
            - identify patterns of neutralization and tests for these patterns to identify if this solution can be effective, or if there is a subset-selection method that can offset this error (select a subset without a ratio or other structure of neutralizing structures that is not reflected in the complete sequence)

      - this is useful in determining the error types likely with a solution interface query, bc each perspective has known (or adjacently derivable) error types associated
        - this is relevant bc the idea of 'specialization' in generating algorithms for a particular intent/case, or an 'algorithm network' where problems are solved by allocating them to a network of specialized algorithms for various intents/cases, may be a standard structure, but its also extremely inefficient, involving:
          - a high degree of computation
          - dependence on assumptions about prior static solutions continuing to work
          - forced categorization of intents/cases into being handled by one of the algorithms that exists, rather than creating a new solution or adapting solution
        - whats important instead of relying on a static & computation-heavy 'algorithm or ml model network where each network is selected for a case/intent' is to identify/generate/derive the right perspective for a task, so that the associated generated interface queries or other solutions to solve a task with that perspective can be designed with error types of that perspective integrated with the solution
          - where the 'perspective' refers to the associated 'combination of interface structures', like priorities/assumptions/errors, that are associated with an interface query or other solution structure to solve a problem (associated in a 'generative' or 'descriptive' or 'emergent' connection structure)
        - this extends the interface structures used as input to a solution (beyond 'context' and 'intent') to a comprehensive set of interface structures, and connects these input interface structures with the solution in a way that allows generation of more optimal solution structures built from meaning/understanding rather than granular structure-fitting of a problem & solution algorithm that was previously found almost by accident to perform acceptably (or at least better than other known solutions at the time of testing/deployment), a structure-fitting that was constructed from the core layer of available functions without an injected sense of 'meaning' (cross-interface fit)
          - the 'granular structure-fitting of a problem & solution built from the core layer of available functions' method ignores error structures:
            - the input data allows this solution to seem acceptable for those inputs, but the input data is actually corrupted, specific to an unidentified type/other variable, reflects noise, is unlikely, reflects a disproportionate ratio of outliers, or is about to change/has changed by the time the solution is built/tested/deployed
            - the resulting solution algorithm is only effective for some subsets of the input data but cant handle (throws errors for) other subsets
            - the resulting solution algorithm works in general for most problems within a certain range of problem attributes like complexity, but cant handle problems outside of that range, like the problem the original problem is about to convert into, rendering the initial solution ineffective
            - the resulting solution algorithm identifies generally useful objects like certain difference or contradiction types, but it cant handle types of differences or other useful structures that it didnt identify in the input data
          - the first solution found with this granular structure-fitting method of 'apply available functions to form a solution that solves the problem in a slightly better way on some metric than other solutions' is also unlikely to be the optimal solution
            - however the patterns of differences between an initial solution found with this method & an optimal solution to the problem can be applied as a default conversion to any solution found with specific granular problem-solution structure-fitting
              - optimization functions that apply structures of optimization like 'structures of generalization' can be applied to make an initial solution found with available functions likelier to be optimal, to reflect differences between sub-optimal & optimal solutions like 'specificity'
              - other optimization structures include 'unique structures' (to avoid 'repetition' of functionality) and 'integration structures' (to avoid 'acontextual solutions' without a meaningful fit with other solutions that isnt just a case/intent routing function)
        - this offers a 'solution network & "solution network-routing function" generator & integrator (fitter)' as an alternative to a static & computation-heavy 'specialized algorithm/model solution network' as a general problem-solving structure
          - rather than distributing optimizations to nodes in the algorithm network (where each node optimizes a subset of optimization metrics), general optimization metrics can be applied & fulfilled dynamically by generated perspectives (with associated interface structures like interaction functions, core functions, error types, problems solvable, solutions generatable, assumptions, priorities, etc), perspective-solution connections & solution structures, so that errors of a perspective associated with a solution are built in to the solution, and when new errors are identified, the perspective/solution can be re-generated/integrated
          - a variant of this is 'generating the maximally different useful perspectives & associated solutions', rather than building a 'network of solution algorithms composed manually of available functions' from a granular level or generating perspectives & associated solutions dynamically for a problem/problem-solving intent
          - the perspectives that would be useful for solving a problem can be identified/generated as an input to identifying solutions, as a way of avoiding perspectives lacking the combinations of required structures like 'complexity' & 'functionality' required to generate solutions for a problem (such as the static granular perspective leading to a 'specialized algorithm network built for known specific cases/intents')
          - other alternatives to the 'specialized algorithm network built for known specific cases/intents' solution include:
            - generating the 'probable useful intents' (including core intents like 'differentiate', 'identify', 'connect', etc) & building a network of those rather than a set of specific intents constructed manually that would likely have errors like 'overlapping functionality'
              - this has a generative perspective that allows for the 'generalization', 'composability', 'interactivity', & 'flexibility' of intents as inherent to the definition of 'intent', making it more optimal than the 'specialized algorithm network' bc of its integration of the meaning of 'intent', which prevents by default errors of 'intent mismatches'
              - other perspectives integrating other combinations of definitions & interface structures would avoid other error types
              - some error types are higher priority to avoid in different contexts, so a routing function that connects solutions or solution-generating methods based on 'error type avoidance prioritization' in addition to other metrics like 'problem' and 'optimization/solution metrics' and other interface structures like 'assumptions' is a more effective way of connecting solutions/solution-generating methods in a solution/solution-generator network
              - a useful format to structure this is a 'perspective network, where perspectives generate interface queries & other solution-generators'
                - this is a network of perspectives (as 'cross-interface structures with varying levels of structure/abstraction'), rather than an interface network on which queries can be run
                - a network of interface queries to generate this network of perspectives is another useful relevant structure
              - occasionally a perspective in a perspective network may be associated with a granular solution rather than solution generators, like how the above perspectives can be associated with specific solutions or solution-generating interface queries

  - add to science
    - time crystals are an example of a 'certainty' structure created by 'requirements'
      - there is no other possibility allowed by the 'interactions of those structures' which create 'requirements', so the behavior of the time crystal is 'inevitable', or 'required'
    - a quantum superposition is a 'lack of information' or 'semi-information' as in a 'lack of efficient stability' (or a mismatch between a structure and the space its observed in vs. the space its clearly defined in, like how imaginary numbers are partially structural in euclidean space in their reference to 1 & square roots and negatives) so that the interaction with the 'observation' function gives the not-information or semi-information of the superposition the efficiency/energy it needs to stabilize, like a template being filled with variable values according to a query, where the template represents 'partial/semi-information' and the values injected into it crystallize it into a more certain form
    - energy transfer from observer to observed in the form of certainty/structure allowing resolution of observed 'lack of structure' (as a variable) into a 'structure' (value)
    - how could you derive that 'light collisions generate matter':
      - core structures of physics interface include:
        - 'light'
        - 'matter'
      - core structures of structure interface include:
        - 'combinations'
      - core structures of system interface include:
        - unit
          - it makes sense that: 
            - 'fundamental structures are created from other fundamental structures'
            - alternatives/contradictions to this include rules like:
              - 'structures that seem fundamental may just be dissipated side effects of more complex object interactions rather than core components of a system'
        - interactivity
          - if 'the universe has boundaries preventing light from being emitted or destroyed in some form (like through black holes or other objects without complete understanding)',
            - it makes sense that 'light combined with interactivity & boundary structures leads to common light interactions, leading to a required handler making the output of light interactions useful'
              - 'light would be forced (required) to frequently interact with itself by being rebounded by universal boundaries'
                - a 'required common interaction' like this would be 'probably useful' in some way, or 'required to be useful' by its commonness
                  - its output, whatever that might be, would have to be handled or the side effects would cascade and cause errors
                  - you could frame 'matter' as an 'error', as in an error in the form of an 'imbalance' between antimatter & matter
      - either of these structures could connect 'light' with 'matter' with different routes

  - add to math mapping:

    - formatting primes as the 'gaps in the various configurations of a four-sided shape; (produced by multiplying two integers) is a useful way to see patterns in the gaps, which are also 'intersections of gaps in various configurations of other factor sets within a range' which form structures similar to 'concentric circles'
      - finding the circular (repeated) structure in the riemann zeta function is probably relevant to the zeros & the primes, which may also correlate with these 'concentric circles' formed by configurations of four-sided shapes in factor range sets
        - this is also relevant bc the riemann zeta function looks like a connecting function providing the conversion between the structures of infinite connected change/curvature/continuity & finite unit change/constants/discreteness
        - this unites a lot of opposing conceptual structures which is why one reason its an important structure
        - another reason its important is that it involves a core structure (combination, as in 'sum') of core structures (units, as in 'integers' and 'constants') having a core operation ('inverse', to connect them with the imaginary unit, and 'exponential change', as in 'continuous infinitesimally small changing (curved) change') applied, and core structures/operations are usually important by default as input/component structures
        - you could predict that this function would connect change types 'infinite connected change/curvature/continuity' & 'finite unit change/constants/discreteness' by identifying that unit structures of these change types occur in the function
        - related questions
          - what relationship does this function have with adjacent variants of it (non-inverse, fractional powers or constants, different number types, different starting position, etc)
          - what emergent structures exist from the sum of these numbers (the maximum value that can be created with the 'addition' operation from the sequence values in this pattern)
          - how to frame this as a different operation type like multiplication rather than addition (area calculation rather than a sum of values indicated by a line), so that a multiplication problem can be formatted as sums of sequences like in some integration methods
            - variant: how to format the prime identification problem as a sum of sequences
      - the 'various configurations of a four-sided shape produced by two integer factors' is a useful way of producing curvature in their 'integrated tangents'

    - this is just applying any of the useful interface structures: 'change types' or 'generative structures' or 'symmetries', using 'change types that dont break a symmetry' as a 'filter' structure of relevance
      - https://www.quantamagazine.org/how-galois-groups-used-polynomial-symmetries-to-reshape-math-20210803/

    - apply 'physics' (energy) interface, 'function' interface (composability), 'logic' interface (requirement), 'system' interface (scalability, interactivity), and 'concept' interface (certainty) to 'math' interface

      - information is a form of certainty that has energy from various sources

        - energy sources include:

          - interactivity
            - if a structure is more interactive with other structures, its likelier to be required and can get energy from those other structures, although its also likely to produce energy if its highly interactive and may be self-sustaining
          - similarity
            - similar change types are likelier to be found near similar change types 
              - one of the reasons for continuity in a function with curvature
          - efficiency
            - if a structures has more useful attributes (like -1, 0, 1, 2, and 10), meaning it 'stores more information' than other structures with the same type of structure (differing just on 'position'), it is likelier to appear in more interaction rules (have higher interactivity)
            - reusability
            - scalability
              - if a generative function can efficiently create an infinite sequence or convert a non-infinite sequence into an infinite sequence or conduct infinity operations, it is optimally efficient bc thats the largest scale
          - requirement
            - if a structure exists by requirement, it gets its energy by default from the structures requiring it, so any energy it produces is a net gain
          - composability (unit that interacts with other units)
            - units interact with every other structure in an interaction space & with other units in the unit (type) space
          - standards
            - interaction with y = 0 (zeros) provides info about the behavior of a function in its default/standard form, which can be used to describe it efficiently

        - other structures are structures of energy distribution/storage
          - equivalent or similar alternates can share the energy distributed from another energy source and distribute it using an efficient method like a rotation function, or store it in its components (roots of unity)
          - given that infinity is a 'sequence that doesnt end', the opposite of infinity is a 'sequence that ends'
            - the structures that are semi or almost infinite indicate the boundary between non/infinite sequences
            - a similar structure is a closed vs. open shape, or a wave (or another function defined for all values of x) vs. a line with endpoints if infinities can be mapped to a regular repeating pattern by some parameter
          - what structure store energy the most (consumes energy in the form of inputs/information but never pays off, or just stores it), like a black hole, such as a set of numbers like the real numbers
            - self-sustaining structures like recursion structures
            - the opposite structures 'self-invalidating' or 'neutralizing' structures are useful in determining less efficient structures
            - the related 'boundary structures' (halting conditions, boundaries between in/finite sequences) are useful as filtering structures, so may be more efficient depending on the energy intent (energy storage, categorization, vs. energy conservation or 'cost minimization')
          - what is the 'qft' structure or 'meaning' interface of other structures than infinite sequences
            - what is the summary or integration structure that describes or forms a base or average metric of other structures
              - given that all interfaces have rules, and physics is the generative rule system, is physics a more effective 'meaning' interface than the meaning ('meta' or 'interface') interface, and for which intents
          - what structures correspond to pi (which represents the 'cost of creating a unit shape from a unit line, converted into an area (cost of applying the line to itself), by applying the default symmetry operation of rotation, as in "applying all possible changes within the symmetry"'), which is a particularly useful number (with high energy output)

        - example of combination structures that form other useful structures
          - an interactivity combined with orthogonal (applying 'maximal differences' as a source of independence) symmetries form the basis of default useful structures like 'rotation' functions (by connecting endpoints of the symmetries)

        - some structures make these energy sources useful for other intents requiring numerical structures, like:
          - ambiguities providing a source of obscurity/protection
          - uniqueness like prime numbers as a source of reference points

        - semi-information is:
          - 'allowed by the definition of a space'
          - 'has interaction rules in that definition, with its own type and with other numerical types'

          - imaginary units are useful in that they form an opposite structure of standard units, which can be useful for forming a spectrum or a limit filter structure

          - quantum superpositions may take the form of:
            - possibility interaction space: 
              - topologies (attribute value combination spaces)
            - mixed certainty/uncertainty: 
              - attribute set cross-sections (combinations of interfaces/types/variable structures & attribute value structures)
            - uncertainty addition: 
              - variable injections (allowing variance of a constant)

        - these energy structures follow other energy patterns, like energy dissipation/distribution & energy storage
          - if a number has more energy than its structure can contain, it may distribute it into a system of related numbers, like components or variants
          - this implies there are input/output sequences in terms of energy flow across math structures, which may reveal intent structures of matter, just like how some math structures have useful outputs for human intents like 'security' by 'obscuring inputs'

        - math structures are a form of energy that is useful/interactive/similar/efficient/required/composable/standard in some way, so it acquires & maintains a stable structure
    
    - similarity of structures like data type in connections (like how 'absolute references' are a possible connecting structure between absolute structures like 'infinities' and referential structures like 'ratios' or 'constants')
    - imaginary unit as a unit of non-definition, or non-structural definition (requires a different axis to portray bc its not defined in the original axes), or a unit of isolation/independence from a dimension set
    - numbers as symmetries given the adjacent change types available to them, and math as the interaction space of these symmetries, where values that dont have structure can exist between the interactions of these symmetries but only while they can maintain a lack of structure & interaction
      - math helps with calculatable problems
      - there are limits to what math can calculate, bc there are limits to the interaction space of number types
        - like how 'integers' and 'infinities' can interact bc you can have 'multiple infinities' and 'infinite integers' and so on, but not every number or number type/attribute can interact like this, in such a clearly defined (structural) way
        - for example, the 'root of negative infinity' is less structural than the 'root of infinity', and the 'root of infinity' is less structural than the 'unit of infinity', bc the less structural interactions require special definitions/limits creating spaces in which they can exist, rather than having structure in a high ratio of spaces
      - relevant questions
        - can you extend the direction of reductions in structure in the interaction space to determine the limits of structure
          - does it reach zero structure at its limit
          - does it extend indefinitely by definition, bc of the lack of structure and a corresponding lack of limits enforcing structure to exist/develop
          - at what point do the requirements of the 'special definition' prevent any structure from being defined?
          - is the interaction space continuous (does it form a topology)
      - some questions (lack of structure) cannot be adjacently resolved by any combination of known structures (such as ambiguities that are more efficient/stable than any structure that can deconstruct them or any input/output structure)
        - is this an absolute limit or does it result from logical flaws, lack of info, or other errors
        - example: calculate interactions of infinite sequences (like in quantum field theories) with perfect accuracy in less than x time type
          - in order to calculate these interactions, the following would be necessary:
            - rules about sequence interactions (why one sequence is adjacent to another, if there is such a rule, and how they can be combined)
            - info about the probability distribution or function that could generate the sequences
            - rules about combining infinities, where each infinite sequence could be defined as an object in the space of infinities
            - rules about interactions in a space that compresses to other relevant info about the sequences
              - meaning alternative compressions than the value/sequence attributes of generative functions, progressions or probability distributions
              - rules about other interface structures than the values or the values' attributes themselves, such as structures like:
                - requirements/opposites & other standard structures
                - alternative equivalents
                - approximations/probabilities
                - difference topologies (rather than infinite sequence value topologies)
                - rules about the system where these sequences could exist rather than the values that exist in the system
                - the structures (of differences & patterns & other useful structures) in the sequences/values that would be impossible, possible, adjacent to calculate, and adjacent functions to determine structures of calculation impossibilities/possibilities/adjacencies in the original topology
                  - finding the structures of sequence sums that would be adjacently calculatable creates a test that can filter out structures that are not adjacently calculatable, like finding local minima using gradient descent
                    - this is an example of why other system structures than input/output sequences/networks are useful, such as the 'structural similarity' of 'gradient descent' and 'filters of structures of adjacent calculation potentials'
          - if there is an ambiguity that defies calculation (without checking every value), it means there is a certainty in its structure of uncertainty (like an ambiguity between a 'random generative function' and an 'input structure that generates output that seems random') that can be relied on to calculate its attributes (like requirements/limits, such as 'equal distribution of outcomes as trials n increases'), if not its actual output values at a given input
        - in terms of graphing these structures, a space where every variable can be reduced to a spectrum with opposites indicating a difference type would be able to visualize these structures
          - a variable indicating additional operations added to a function, where each operation has an opposite, and each operation adds a change type, still doesnt have an inherent ordinal value indicating how far from the origin it should be, unless there's a clear association in increases in complexity/potential between change types added by each operation
          - the definitions of 'ambiguity', 'inevitability', 'intent', 'output', and other structures may allow visualization in a mathematical space without using vectors or networks, if their definitions can be mapped with opposites & incremental increases in an attribute with an inherent ordinal ranking and a zero value having no structure
            - ambiguity: 'lack of differences in alternatives (different options)'
            - inevitability: 'lack of alternatives (different options)'
            - requirement: 'exclusive trigger'
            - output: 'subsequent causal node'
          - if 'lack' can be mapped to the negative direction, the other structures can be mapped to structures of certainty (largely in the form of logical sequences & variables as useful axes)
            - 'output' creates a certainty in the form of a sequence (as in a 'guaranteed product of the cause')
            - 'requirement' is a 'certain input' if another structure occurs (a function is triggered), which is also a sequential structure
            - 'ambiguity' is a lack of 'structure indicating difference between different objects', meaning the differences cant be mapped in the current space, indicating the structure of a sequence of a 'conversion' (like 'adding a variable') before the difference can be mapped, which makes sense bc it offset the 'lack of structure indicating difference' (a variable), and if change types (variables) are mapped in this space, this can be structured as a function converting the current or previous change type value to the required change type value

  - example of applying interface components to solve problem of 'generating arguments to make a point'
    - change variable 'location of power':
    - this company is welcome to build their own app store with their own phones or team up with their coalition to do so
    - add variable 'time sequence' to 'location of power' change:
      - if this company operates an app store someday, they will set rules to benefit themselves too, just like theyve done in the past
    - offer an alternative to charging app store rate
      - is there a one-click button to migrate from this company to the monopoly that could replace any difference in taxes on this company
    - apply conceptual definition filter 'does concept of persecution (and related components of the definition like focus) apply to the behavior (does behavior have a specific target that is the focus of persecution)'
    - are the monopoly's rules applied exclusively to this company? if not, it's not anti-competitive behavior
    - apply intent filter
    - is this company's mission nobler than the monopoly's
    - apply system cost-benefit analysis
    - what features were improved bc this company exists? are those features worth anything or required needs? did they develop those features better than competitors?
    - if this company is just charging rent on a catalog, are they adding value to the market, so they should be allowed to dictate the market at all?
    - what products/features would the monopoly develop if they didnt have to pay a fine, and what are those features worth, and are those features required?
    - apply logical fallacy filters
    - apply 'hypocrisy' filter
      - apply 'anti-competitive' conceptual definition structures & test if these structures fit the opponent
      - does this company plan on raising prices at some point or will they keep prices low even if the app store rate holds? are they only keeping prices low to dominate the market & plan on raising prices later? isnt that anti-competitive behavior?
      - if they are so concerned about anti-competitive behavior, why arent they trying to compete by building their own app store? isnt there a risk that the apple app store is sub-optimal and needs to be improved with competition from this company

  - organize examples of logic for functions (interface query design logic)
    - document default static config objects that are inputs to core objects (like functions & concepts)
      - core functions like 'change', with locked objects which should be generated as inputs to other functions and should not be removed bc they enable other rules & core objects
        - a 'check for errors' function
        - a concept of 'self-correction/optimization'
      - these locked objects can be used to generate rule-generating/deriving/finding structures, by forming an initial structure of locked objects and filling that structure with conditional & changeable structures
        - these rule-generating/deriving/finding structures can be used as solution automation workflows
    - design an optimal sorting structure for general interface queries to apply to problems manually
    - example of how to predict most interactive/causal concepts in a system
    - list interface selection (based on inputs like available APIs/data sets/definitions)
    - problem interface structures: solution constraints/metrics, problem space variables, available functions, useful formats/structures
    - function to translate interface query logic into interface language (combination of core functions (find/build) & other core components)
    - function-usage-intent::output or demand::supply combination/merging/building/matching functions (alternatively formatted as a solution-finding query for a problem or lack-resource matching function) as an alternative solution to ads
    - decision points (required/optional resolution of variables to constants, as in selecting a variable value)
      - identify when a method & data set can be identifyd to be capable of deriving the answer to a prediction function problem
    - alternative intent coordination & compatability of metrics
      - calculating interactivity by coordinating/adjacent/convertible structures

  - add structural queries to insight paths
    - alignments present in security innovations (like alignment in inputs like keys)
    - source of rule development as structures of conflict between forced interactions like change causes & constant structures like limits
      - incomplete inevitability of interaction as a decision structure
    - other examples  
      - group device history authentication: authenticate credit card by proximity to cell phone & continuity applied to user usage history pattern
    - functionalize insight paths & integrate functions in optimized program with parameters to select function subset & structure for input problem

  - create configuration for 2-3 list items every day
      - create compilation script to compile code/config into a network graph on every change
        - add support for equivalent synonyms
        - add conversion to standard vocab
      - write some default interface queries to use until logic is written
      - finish lists:
        - most valuable interface queries & workflows
          - find the sets of differences/dependencies/formats/errors & other useful structures that are the most valuable in a particular structure like a sequence to solve a problem
        - useful perspectives
          - useful to think of prediction functions as generative functions to select the variable interactions that are most likely
        - interface component definition routes
        - ai structures with supported intents & solution success causes
        - solution filters to apply in functions
        - useful interface components
          - aligning/balancing structures, to solve problems like 'a balance position of structures producing errors when unbalanced'
          - questions formatted as a disconnection between components like causal positions, paths, directions
        - subset indexes of an interface useful for solving most problems (structure indexed by metadata like problems solvable, fitting systems, interactive structures, supported intents)

  - add to conceptual math examples
    - example of a conceptual math operation that builds a boundary structure leaving an inevitability of a matching concept (numbers) filling the structure
      - the concepts of 'missing', 'multiple/more', 'unit', 'type', 'identifiable as similar/equal/different' and 'difference in amount' allow for/require/build the concept of 'numbers'
      - also functions like 'compare' or 'reduce' or 'expand' require the concept of 'numbers' when comparing objects of that data type or objects having a quantifiable attribute

  - interim functions to build:
    - derive definition routes on various interfaces from a definition
    - apply a standard format (function/attribute/object) to an input
    - derive function intent stack
    - identify error types
    - identify structures of cause
    - identify variable types & structures
    - consolidate repo, remove repeated content, merge similar functions

   - finish processes:
      - finish interface analysis of physics to identify other useful components like efficiencies, incentives, trade-offs, closed systems
      - finish applying interface components (like abstract) to useful interface components
        - core interaction functions of core interaction functions
      - create interface queries of finding useful interface component filters

  - example of structural version of solution difference from original solution: 
        - this is like using a pair of connected lines at different angles to connect two points (multiplying alternate multiplier pairs to create a product), where summing the line lengths produces an equivalence, so different solutions would look like differently angled triangles connecting the two points
        - https://www.popularmechanics.com/science/math/a30152083/solve-quadratic-equations

  - add to useful structures
    - identify what is the system structure format where the maximum number of interface queries can be executed structurally, with minimal conversions required? is it a merged format of variable/function/concept/cause network graphs, or system state networks, or a set of variable subset graphs, or differences visualized as vectors, or input-output sequence visualizations, or a network with all identifiable interface components visualized
    - identify useful perspectives

  - document useful component/sub-structures of interface queries (interface components, interaction rules, cross-interface interactions, generative functions)

  - ml explanation of finding coefficients of prediction function by applying distortions to coefficients & ruling out distortions that dont contribute to prediction accuracy
    - can be optimized with reductions like:
      - 'calculating the most different distortions that will reduce possible values the quickest & applying those distortions'
  
  - proof/determination structures
      - what makes something possible to identify/calculate
        - a solution structure where the solution metric is clearly defined (structural or having other structures of certainty like consistency or inevitability or requirement)
          - checking a path to see if it includes a node twice is clearly defined (it uses the structure of 'node visit counts' in the 'path' solution structure)
      - what makes something difficult to prove
        - where there are ambiguities (lack of certainty/structure/definition) between the input parameters & the output function value
          - ambiguities such as where multiple inputs produce the same output, like how different x-values can produce the same y value on a wave function
      - useful proof structures
        - apply possible components to create an absolute or scalable definition include components framed in terms of interactions with other components that can be used with a consistent measurement (like a stable structure across interfaces or dimension sets) & can also scale (boundaries), rather than framing them in terms that can have different meanings at different parameters (closed, hollow)

    - add to causation variables
      - ability to change (if a variable cant be changed, it is less causative for problem-solving intents)

  - examples of identifying vertex variables
      - general vertex variables: topic, origin/destination, reason/cause/point/intent, errors, variables, types
      - comedy vertex variables: sincerity, stupidity, stakes, tension-resolution/expectation-subverting pattern variation
      - music vertex variables: tone, tension-resolution/expectation-subverting pattern variation, lyrics
      - optimization metric vertex variables: solution metric patterns (what other solutions optimize for, to identify optimization metrics to apply)

  - when is it optimal to store a mixed structure of varying specificity (like a type, intent, cause & a specific example)
      - when there are potential uncertainties to resolve, like whether the example represents a new error, type, or variable, bc the example doesnt fit known structures

  - all primary interfaces can act like the problem-solving interface (start solving problem from the concept or structure interface and integrate all info back into that interface & frame the solution in terms of that interface) but the meaning interface (the interface interface) is the most powerful

  - visualizing higher dimensions with changes in a network of visualizable variable subsets like:

    - dimension subsets: displaying dimension subsets in groups of sizes that are already visualizable (from 1 - 4 dimensions), where orthogonality is preserved across the network of subsets

    - dimension groups: grouping similar dimensional changes into a change type across a dimension subset, to visualize the change types
    
    - relevant (robust) dimensions
      - dimension invalidations: grouping invalidating/neutralizing change types
      - causative dimensions: just visualizing higher-impact/causal dimensions
      - vertex dimensions: graphing variables as differences from vertex variables
    
    - mapping dimensions: group value sets as other structures like points or networks in a space where change types like 'continuous change' are supported
      - embedded dimensions: dimensions graphed visually using extra dimensions as parameters
      - base dimensions: dimensions standardized to a base and graphed in alignment, like multiple functions on a graph with a common base
      - mapping other dimension metadata: interactions between dimensions units/change types/limits/definitions are graphed
      - abstract dimensions: abstract value structures are graphed ('a point on a line') instead of specific values in a dimension
      - constant dimensions: adjacent limiting constant dimensions are graphed instead of dimensions of change

    - dimension interactions: 
      - interactive dimensions: dimensions that interact are condensed into an input/output of the interaction structure, and input/output dimensions are graphed instead
      - dimension interaction structures: structures of interactions between variables (like direction/circuits/networks) are preserved, where values may be lost
        - dimensional difference: difference between dimensions is graphed instead of different dimensions & values, where dimension values are structures associated with graphed dimension interactions
        - conversion requirements: conversion requirements to a visualizable shape are graphed instead of actual dimensions/values
        - interaction structures between value structures like positions on dimensions when graphed as a standardized shape
          - example: with dimensions formatted as a standardized form like 'lines of equal length, & values as points on these lines', the interaction structure would be the lines connecting the points on the dimension lines, when arranged in any order

    - example of resolving a conflict between structure/limits using a structural similarity between a structure (gradient of function) & its container/limits (gradient of constraints)
      - https://en.wikipedia.org/wiki/Lagrange_multiplier
      - also an example of a solution space (the whole function is the solution space of possible minima/maxima) and a filter applied to it (constraint)

    - make a function network of math domains (inputs/outputs of geometry, algebra, calculus that align)

    - resolve definitions of components so you can finish organizing useful structures like combinations of concepts such as "format sequence", "solution automation workflow", "insight path", "reverse-engineer solution from problem requirements or opposite structures", "connect problem & solution"

    - value isnt created/lost by companies in the timespan of hype/short cycles, so stock market price swings aren't reflective of reality from a macro perspective
      - it takes years to build value, it doesn't happen overnight, excluding almost magical insights that create cascading efficiencies like my system
      - losing value also happens slowly, excluding extreme natural disasters, like the value of a community still being relatively high despite shared losses, bc of social network effects & organization/coordination effects
      - value can be calculated differently, using metadata like the lifetime & total possible value of a product
        - what is the total supply of the product inputs (fossil fuels)
        - what is the usability lifetime
        - what are the costs
        - what are the product intent alternatives (can intents fulfilled by the product be fulfilled by other products)
        - what is purchased with revenue from a product (research, insights, other more valuable resellable products, etc)
        - if this pricing method is applied to fossil fuels, oil companies would be paying people to use them

    - identify filters for definition routes

    - database polling/prompting user for update & predicting updates or searching for & receiving user-approved updates from other services, rather than being a passive receiver of input from user
      - based on local usage/change patterns or integrated usage patterns to identify expected transactions with other services
        - once a credit card is marked as lost in a banking service, a change of credit card numbers is expected by other services which can poll for updates to this flag
          - user option like 'yes, allow other approved databases containing my address poll to collect this update'
        - once a renters insurance policy is changed in an insurance service, a change of address might be expected by other services

    - make list of variable structure variables measured by algorithms & why they are measured by a network algorithm

  - make diagram of absolute reference connections with metadata structures like networks/paths
  - identify core graph variables (definition of adjacence/difference, connectivity, dimensions, info storage methods, interactivity of structures like sequences)
  - crypto as community consensus, where a decision can have value if backed by a community

    - examine connection between fractals, sequences, averages, origins, multipliers (self, as in power), & circles
      - fractals as a relevant structure for adding sequences of fractions (adding numbers similar to itself on a smaller scale, infinitely) as a way of producing inputs to circles created by transforming a fractal spiral, where the origin is the original number as a base for applying increasingly smaller scales
      - the set of points forming roots of an infinite negative number sequence (roots of unity, rather than roots of any negative integer number) as a way of producing a circle because of their common distance to their average (center) forming the radius
      - lines of equal length having a common average point (center/origin)
      - fractals & infinite sequences as a way of calculating area under a continuous line (increasing small subsets of structures with area calculatable with multiplication of x & y)
      - what continuous line segments would have an area equal to a circle of relevant proportions?

    - how to identify the killer counterpoint
      - point: 'election fraud claims'
        - counterpoint: you dont think the other party has members?
          - followup points:
            - how are they organizing? why do you even hear about them?
            - who's benefitting from investing massive funding in creating a false illusion, if its all fake?
            - why wouldnt they choose cheaper methods of doing so than an elaborate illusion?
      - identify most extreme false assumption of point by identify causes of the output metric (vote count)
        - the primary/basic false assumption of the point is that the other side doesnt have votes that are comparable in quantity
          - possible causes (inputs) of 'not having similar vote count' include:
            - not going to vote
            - not being able to vote
            - not having members to vote
          - the most extreme false assumption is the most extreme cause of 'not having similar vote count' (that the other side doesnt even have members (input of votes) that are comparable in quantity, let alone votes (output))
      - identify 'incentives', 'side effects of party/member/vote existence & size' as other relevant concepts to generate followup questions

    - the work of 'stealing my work' and 'pretending not to' doesnt produce efficient brains with a good grasp of concepts like 'meaning', so it emerges as an inefficiency bc the type of brains developed wont be good at finding solutions to output problems generated by granularly copying/pasting to a specific problem/solution & hoping it works better
            - 'understanding my work' is a better goal if you cant look away bc youre not done experiencing awe
              - once you understand it, you wont need to watch me work, youll be able to generate my work
  
  - apply nn to derive error types & bias incorporated into other nn data/parameters/algorithms/structures/models    


## general

  - integrate objects/.md text with interface implementations

  - apply to structures

    - concept of attention in structures
      - mixed interim high-variation & high-similarity structures tend to maximize attention
    
    - examine error type of conflating intent & requirement
    
    - consciousness as choice to move between neural nodes (rather than being directed) required:
      - the development of alternative node paths performing equal/similar functions, requiring:
        - the development of excess resources, delaying required decision time (making immediate decision unnecessary, avoiding a forced decision), requiring:
          - the existence & application of previous efficiencies & functions for alternative evaluation, energy storage, storage-checking, & energy requirement-identifying
      - the cause could be framed as structures such as an 'efficiency stack' or 'energy maintenance functions' or 'alternative options' or 'navigation/motion control' or 'lack of requirement/need'
    
    - examine similarity (alignment/overlap) structures between: 
      - extremely different components (when an error type is an incentive or a function used for other intents) 
        - when the solution format of some problem has similarities to the error type, like when you need randomness so errors generating randomness are a possible function to use for that intent
        - contradictory/opposite components (have some metric in common, with opposite values)

  - finish organizing lists of examples, functions, info objects (insight paths, definitions, questions), components for configuration

    - organize examples

      - label examples so they can be queried more structurally
      - query for logic in examples when implementing functions
      - organize examples of useful structures & questions in index.md
        - identify useful questions in notes
        - check reduced language components for any other useful functions (what terms cant be adjacently, clearly & accurately framed in terms youve defined) for completeness


## examples


  - examine the distortion vector paths that adjacently decompose a data set into a prediction function from a base point/function set

  - give example of mapping to structures & identifying contradictions its safe to ignore for applying a structure

  - example of permuting assumption: "reports of power consumption have to be exact measurements" 
    - a temperature monitor sensitive to a hundredth of a degree might provide similar but non-specific power reporting for important/extreme usage patterns without revealing such specific information as that which could infer exact operations being done, bc the interval of temperature measurements allows for greater variation in calculations that could explain it
  
  - query examples for use cases like:
    - lack of information stored (match problem of type 'information lack' with interface query 'check pattern interface for similar patterns')
    - query problem breakdown & integration diagram
    - calculating various different problem breakdown strategies first before executing normal query-building logic for each
  
  - add examples of system/object/rule/type change patterns
  
  - include example workflows with example problems
    - include example of how to generate other workflows (different starting/ending points & trajectories)

  - example of using set theory in query operations:
    - edges as core organizing/formatting operations (find/apply) & interfaces (connecting/explanatory concepts/functions)
    https://en.wikipedia.org/wiki/Hypergraph

## diagram
  
  - diagram with error types
  - diagram of the network of formats
  - make efficiency map
  - diagram of alternate interfaces (information = combination of structure, potential, change or structure, cause or structure, system)
  
  - give structural query example diagram for GANs + image compression problem

  - chart type: overlaying multiple 2-dimension variable comparisons to identify common shapes of variable connections (density of points added with a visible attribute like more opacity)

  - finish core function structure example diagrams

  - diagram with joke types
    - 'annoying when they bring up human rights in a conversation'
      - conversation system context
        - functions
          - change topic 
            - change topic structure (sequence)
              - introduce a topic (first time topic is included in conversation)
          - expected interaction functions
            - criticism of a behavior
              - 'conversation with dictator' system context
                - criticism of power abuse (law violation, specifically human rights violation, which are a related object to dictators)
                  - interpreted as right in the 'conversation with dictator' system context
                    - expected interaction in this context
                      - 'should bring up human rights to a criminal'
            - norms:
              - for low-stakes interactions & interaction errors (manners, annoyance, disrespect)
            - laws: 
              - for high-stake interactions & interaction errors (rights violations)
        - placing a norm (or related objects) in the place where a law (or related objects) would normally go:
          - 'its annoying when someone doesnt let you end the conversation'
            - 'its annoying when someone keeps going on & on about your previous conversations where you ordered deaths of a dissident'
              - 'its annoying when someone keeps going on & on about your previous conversations where you ordered deaths of a dissident for being annoying & then abruptly stops without explanation'
            - 'its rude when someone doesnt let you end a conversation with a laywer interrogating you for war crimes' 

  - diagram for structures of emergence
    - example: 1-1 input/output relationship up an interaction layer, where extra resources that dont dissolve immediately on the higher interaction layer aggregate & form core structures like combinations, where interactions between combinations & sequences have different dynamics than the individual output interacting with other individual outputs
    - emergent functionality/attributes come from interaction structures (sequences & layers)

    - add diagram for intent-matching
    - add structures to diagram: interface overflow (to sub-interfaces), interface foundation
    - diagram for workflow 1: 
      - function to identify relevance filter ('functions', 'required') from a problem_step ('find incentives') for a problem definition, to modify problem_steps with extra functions/attributes ('change_position') to be more specific to the problem definition ('find_incentives_to_change_position') for problem_steps involving 'incentives', so you know to use the function_name to modify the problem step if it's between the type 'functions' and the object searched for 'incentives'
    - add conceptual math interface query diagram
      - use lattice multiplication as standard example, other than core operations (add/multiply mapped to language, concepts like irreversibility/asymmetry mapped to math)
    - interface conversion, matching, starting point selection (applying structure, checking if relevant information is found)
    - diagram to document sub-functions of core functions with distortions
    - make diagram for dimension links higher than 3d that are depictable in the same network space
      - should show variables that impact other variables, the change rates of these relationships
      - overall impact should be calculatable from these relationships
      - should show similar movements for correlated variables
      - should show skippable/derivable variables (variables that can be resolved later than they normally are)
      - should show meta forces for overall trends in change rules (direction of combined variable forces)
      - should show limits of measurability & threshold metrics

    - diagrams for specific concepts, core functions, concept operations (combine, collide, connect, merge, apply), ethical shapes
        - variable accretion patterns (how an object becomes influenced by a new variable, complex system interaction patterns, etc)
        - make diagram of potential matrix to display the concept
          - map parameter sets to potential matrix shapes 
        - finish diagrams for cause (shapes & ambiguity), concept (evolution of concepts, networks, distortion functions)
        - diagram for argument
      - make a system layer diagram for each interface to allow specification of core interfaces & other interface layers (interface interface)
        - make a system layer diagram for structures to include layers of structures 
          (beyond core structures like curves, to include n-degree structures like a wave, as well as semantic output structures like a key, crossing the layer that generates info structures like an insight, a probability, etc)

    - map variable structures to prediction potential for problem types, given ratio of equivalent alternate signals

    - vertex variable structures
      - quantum physics, prediction/derivation tools, build automation tools, testing tools, learning/adaptation tools, system rules, computation power are all vertex variables of information, since they can generate/derive/find information
        - which structure (sequence, network, set, or cycle) of vertex variables is most efficient

    - core component attributes: identify any missing attributes/functions that cant be reduced further

# content/config

    - import insight history data to identify insight paths (info insight paths like 'lie => joke => distortion => insight', system insight paths like 'three core functions + combine function with this definition + n distortions to nearest hub')
    - define default & core objects necessary for system to function (out of the box, rather than minimal config necessary to derive other system components & assemble)
      - add default functions to solve common problem types
      - alternate utility function implementations have variation potential in the exact operations used to achieve the function intents, but there are requirements in which definitions these functions use because they are inherent to the system. For example, the embodiment may use a specific definition of an attribute (standardized to a set of filters) in order to build the attribute-identification function using a set of filters - but the general attribute definition is still partially identifyd in its initial version by requirements specified in the documentation, such as a set of core attribute types (input, output, function parameter, abstract, descriptive, identifying, differentiating, variable, constant), the definition of a function, and the definition of conversion functions between standard formats.
    - document time structures (concave time explaining compounding similarities up to a point of maximum concavity, a structure that can separate from the other space-times)
    
    - systematize definitions of info objects, to include analysis that produces relationships of core objects like opposites to their relevant forms (anti-symmetry) in addition to permuted object states (asymmetry), such as an anti-strategy, anti-information, anti-pattern
      - organize certainty (info) vs. uncertainty objects (potential, risk, probability)
      - make doc to store insight paths, counterintuitive functions, hidden costs, counterexamples, phase shift triggers
      - add technicality, synchronization, bias, counterintuition, & certainty objects leading to inevitable collisions
        - error of the collision of compounding forces producing a phase shift
        - lack of attention in one driver and false panic in a second driver leading to a car crash given the bases where their processes originate
      - define alignment on interfaces (compounding, coordinating, parallel, similar, etc)
      - add core info objects (core strategies, core assumptions) so you can make a network of graphs for a system
    - concept analysis:
      - how new concepts (gaps in network rules) evolve once structure is applied to prior concepts 
    - interface analysis:
      - limitations of interfaces & how to derive them
      - how rules develop on stability & how foundations are connected & destroyed
      - explainability as a space limited by derivable attributes from data set & cross-system similarity
      - vertex definition & give examples (as an intersection/combination of interface variables, such as determining/description(compressing)/generative/causative/derivation variables), around which change develops
    - change analysis:
      - generated object change types
        - constant to variable
        - variable to removal of assumption in variable type/data type
    - examine implementing your solution type (constructing structures (made of boundary/filter/resource sets) to produce substances like antibodies, using bio system stressors)
    - resolve & merge definitions into docs/tasks/implementation/constants/definitions.json
    - update links
    - integrate archive_notes/finder_info/functions
    - de-duplicate logic
      - organize interface analysis logic definitions
        - organize functions in problem/interface definitions, before organizing functions in implementations/*
      - integrate problem_solving_matching.md
      - integrate find/apply/build/derive logic from system_analysis/ & maps/defs*.json
      - separate interface analysis logic into implementation/functions (functions dont need unique info)
      - add functions from workflows & analysis (to do list, questions answered, problems solved, interface definition & functions) as files in functions/ folder
        - organize into primary core functions & list sample parameters (like objects to identify for the identify function)
      - integrate rules from diagrams in patent applications to relevant documents
            

- examples
      - joke mapping insight paths
        - unlikely hypothetical
          - several degrees of assumption chains to generate an unlikely hypothetical (sequence of assumptions from a starting assumption/premise, generating a background story/context)
            - serious + petty + important: 'none of us can figure out why he tucks in his tie'
              - implies that the problem was so serious that a discussion happened to investigate & research it to fix it
              - implies that no one is allowed to ask him or has the courage to ask him directly, implying he's powerful in some way & cannot be questioned, which implies these are his subordinates who are not doing work in order to discuss this, which implies this could cause their work arrangement to be invalid 
            - 'none of us ever figured out why he tucked in his tie'
              - repeated + important: implies that the discussion was repeated bc it was such an important matter
            - 'none of our lawyers or R&D staff ever figured out why he tucked in his tie'
              - important: adds another level of importance in that they hired an expensive legal team to investigate the matter for liability/indemnity/litigation potential as if it made the company look so bad they had to hire a legal team
            - "the tie-tucking survived ex-girlfriends' interventions"
              - important: it has ruined multiple relationships with people who cared about him who tried to stop him from doing this to himself
              - briefer than previous version & uses more evocative verb
              - different: 
                - add assumption: assumption that the audience is rooting for the tie-tucking to continue
                - add concept of 'agency': attributing personhood to the tie-tucking, which is fighting for its life against cruel monsters
            - 'even after being accused of being a double-tucker who tucks his tie but not his shirt, he persisted'
              - important + petty + similar: fashion is a petty thing to care about this much, and a special jargon term 'double-tucker' implies a whole community or sub-culture based on or caring about this issue or related issues, which he has caused controversy in, with added importance by association from term 'double-agent', typically reserved for high-stakes situations like foreign wars, as if he's betraying someone or his heritage or group or people who rooted for him, and rhymes with a curse word
            - 'the mysterious tie-tucker left the board of directors' 
              - important + reduced: condensing the entire story into a brief structure like a nickname and casually referencing it despite the importance implied in a problem that generates a nickname
        - topics
          - conspiracy theory (a muffling device to prevent the Chinese from listening to his balls chafe for blackmail material)
          - cults/ex's (definitely worshiping the wrong things)
        - total opposite: 'your fatal flaw wasn't so much all the excessive drunk online shopping purchases at the police store & the corporate sabotage so much as the curtains from korea that spied on us & posted our arguments to porn sites' (it was absolutely the excessive spending at the police store)
        - changing definitions to very different alternate definition
          - 'tucked in his tie' or 'used unnecessary protection' or 'packed heat'
        - resolving awkwardness
          - 'i dont hate your dick pics (introduces the problem, 'uh oh does she hate my dick pics'), I just think theyre (foreshadowing something that seems like a difference but is actually similar) more optimal when directed at enthusiasts (or professionals), such as doctors/researchers, who might appreciate it more than I ever could (optional: from a curiousity perspective)'
        - mixed contexts/styles
          - talking about an unimportant matter in the same terms used for important matters
            - 'his parents couldnt deal with the idea of confrontation so they gently let him lose touch rather than disowning him outright' 
        - adding relevant structures of meaning like:
          - aligning layers like double entendres
            - calling him a 'magician' bc they have a function of 'hiding scarves' which is similar to 'tucking a tie' bc 'tucking' has a related output of 'hiding'
        - defeating the purpose/self-defeating
          - listing all the manipulations youre using, while using them, to the target 
        - false dignity/over-generousity
          - calling him a 'international man of mystery' bc its a very dignified way to portray having mysterious fashion habits that defy analysis from subject matter experts
          - 'he must be doing it to scare away women bc theyre always chasing him'
        - injecting stupidity/extremes
          - he thought it would act like a talisman to protect him from rape or unwanted pregnancy
          - he was told by a foreign holy man (has association with 'wise foreigner' stereotype) that it would protect him from fertility like a 'cosmic condom' (repetition, catchy) 
        - removing a point/agency (its not an intent, he just had the clingiest underwear/reproductive organs known to mankind)
        - fitting with existing systems without obvious contradictions
          - 'using existing phrases in a new way with minimal distortions' is surprising bc its unlikely to find a new distortion of an existing component that someone hasnt tried, so the simpler the better for this type
