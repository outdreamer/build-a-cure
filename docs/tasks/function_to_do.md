# to do

  - finish processes:
      
      - finish applying systematization of solution automation
      
      - finish interface analysis of physics & other interfaces to identify other useful components like efficiencies, incentives, trade-offs, closed systems
      
      - finish config
        - add useful structures & questions from index.md to systematize_solution_automation.md
        - for each workflow involving useful structures, indicate an actual interface query example connecting the workflow with the example
        - useful structures
            - identify filters for useful structures like definition routes
            - the system structure format where the maximum number of interface queries can be executed structurally, with minimal conversions required? is it a merged format of variable/function/concept/cause network graphs, or system state networks, or a set of variable subset graphs, or differences visualized as vectors, or input-output sequence visualizations, or a network with all identifiable interface components visualized
            - interface queries optimizing finding useful interface component filters
            - useful perspectives/specific interfaces
              - useful to think of prediction functions as generative functions to select the variable interactions that are most likely
            - useful solution filters to apply in functions
            - aligning/balancing structures, to solve problems like 'a balance position of structures producing errors when unbalanced'
            - questions formatted as a disconnection between components like causal positions, paths, directions
            - subset indexes of an interface useful for solving most problems (structure indexed by metadata like problems solvable, fitting systems, interactive structures, supported intents)
            - ml structures with supported intents & solution success causes
            - most valuable interface queries & workflows
              - find the sets of differences/dependencies/formats/errors & other useful structures that are the most valuable in a particular structure like a sequence to solve a problem
                    - interface component definition routes
            - useful component/sub-structures of interface queries (interface components, interaction rules, cross-interface interactions, generative functions)
            - useful interface components (like abstract) of useful interface components
              - core interaction functions of core interaction functions
          - creating useful structures
            - organize automating useful structures like combinations of concepts such as "format sequence", "solution automation workflow", "insight path", "reverse-engineer solution from problem requirements or opposite structures", "connect problem & solution"
            - convert structural queries to insight paths
              - alignments present in security innovations (like alignment in inputs like keys)
              - source of rule development as structures of conflict between forced interactions like change causes & constant structures like limits
                - incomplete inevitability of interaction as a decision structure
              - group device history authentication: authenticate credit card by proximity to cell phone & continuity applied to user usage history pattern
            - functionalize insight paths & integrate functions in optimized program with parameters to select function subset & structure for input problem
        - default config
          - write some default interface queries to use until logic is written
      
      - finish scripts
        - create compilation script to compile code/config into a network graph on every change
          - add support for standardizing equivalent synonyms
            - add conversion to standard vocab

  - integrate logic
      - integrate objects/.md text with interface implementations
      - integrate archive_notes/finder_info/functions
      - organize interface analysis logic definitions
        - organize functions in problem/interface definitions, before organizing functions in implementations/*
      - integrate problem_solving_matching.md
      - integrate find/apply/build/derive logic from system_analysis/ & maps/defs.json
      - separate interface analysis logic into implementation/functions (functions dont need unique info)
      - add functions from workflows & analysis (to do list, questions answered, problems solved, interface definition & functions) as files in functions/ folder
        - organize into primary core functions & list sample parameters (like objects to identify for the identify function)
      - integrate rules from diagrams in patent applications to relevant documents
      - organize function logic (interface query design logic)
        - document default static config objects that are inputs to core objects (like functions & concepts)
          - core functions like 'change', with locked objects which should be generated as inputs to other functions and should not be removed bc they enable other rules & core objects
            - a 'check for errors' function
            - a concept of 'self-correction/optimization'
          - these locked objects can be used to generate rule-generating/deriving/finding structures, by forming an initial structure of locked objects and filling that structure with conditional & changeable structures
            - these rule-generating/deriving/finding structures can be used as solution automation workflows
        - design an optimal sorting structure for general interface queries to apply to problems manually
        - list interface selection (based on inputs like available APIs/data sets/definitions)
        - problem interface structures: solution constraints/metrics, problem space variables, available functions, useful formats/structures
        - function to translate interface query logic into interface language (combination of core functions (find/build) & other core components)
        - function-usage-intent::output or demand::supply combination/merging/building/matching functions (alternatively formatted as a solution-finding query for a problem or lack-resource matching function) as an alternative solution to ads
        - decision points (required/optional resolution of variables to constants, as in selecting a variable value)
          - identify when a method & data set can be identifyd to be capable of deriving the answer to a prediction function problem
        - alternative intent coordination & compatability of metrics
          - calculating interactivity by coordinating/adjacent/convertible structures
        - check reduced language components for any other useful functions (what terms cant be adjacently, clearly & accurately framed in terms youve defined) for completeness

  - integrate examples

      - index examples so they can be queried more structurally when implementing functions

      - move examples from:
        
        - drinkme/examples_from_faq.md
          - check other examples of high-value use cases (other than identifying important concepts) from faq:
            - identifying the important base to frame changes on (identifying new interfaces)
            - identifying the right interaction level to focus on (identifying the change-maximizing layer of a system to examine a particular relationship)
            - identifying the right perspective to filter with (like 'identifying whether the legal/scientific/progressive perspective is most useful for an intent')
            - identifying the right context/position for an object (derive context when it's missing or fit an object to a system)
            - identifying the most causative function set (like identifying core functions, or the most misused functionss, or the most change-causing functions)
            - identifying important differentiating types (like function types indexed by intent & structure types, like boundary/change functions)        
        
        - patent implementation_examples
          - identify any examples missing from patents in docs/tasks once examples are organized

        - specific examples from specific_problem_analysis
          - example of permuting assumption: "reports of power consumption have to be exact measurements" 
            - a temperature monitor sensitive to a hundredth of a degree might provide similar but non-specific power reporting for important/extreme usage patterns without revealing such specific information as that which could infer exact operations being done, bc the interval of temperature measurements allows for greater variation in calculations that could explain it
          - example of using set theory in query operations:
            - edges as core organizing/formatting operations (find/apply) & interfaces (connecting/explanatory concepts/functions)
              - https://en.wikipedia.org/wiki/Hypergraph
          - example of structural version of solution difference from original solution: 
              - this is like using a pair of connected lines at different angles to connect two points (multiplying alternate multiplier pairs to create a product), where summing the line lengths produces an equivalence, so different solutions would look like differently angled triangles connecting the two points
                - https://www.popularmechanics.com/science/math/a30152083/solve-quadratic-equations
          - examples of identifying vertex variables
              - general vertex variables: topic, origin/destination, reason/cause/point/intent, errors, variables, types
              - comedy vertex variables: sincerity, stupidity, stakes, tension-resolution/expectation-subverting pattern variation
              - music vertex variables: tone, tension-resolution/expectation-subverting pattern variation, lyrics
              - optimization metric vertex variables: solution metric patterns (what other solutions optimize for, to identify optimization metrics to apply)
          - example of resolving a conflict between structure/limits using a structural similarity between a structure (gradient of function) & its container/limits (gradient of constraints)
              - https://en.wikipedia.org/wiki/Lagrange_multiplier
              - also an example of a solution space (the whole function is the solution space of possible minima/maxima) and a filter applied to it (constraint)


## examples

  - why problems will always have new variables determining how solution automation workflows can be generated
    - problems are a 'lack of available structures that can fix the problematic structures', requiring new structures to be found, built, or derived to solve those problems
    - by definition, problems will always change from previous problems
    - therefore problems are a structural source of new change types
    - new change types map to new variables describing those change types
    - new variables may not be adjacently buildable, derivable, or findable with existing structures, leading to a requirement to find/build/derive new structures to structure those new problem variables
    - this may lead to new values of existing variables like new problem-solving intents or new problem/solution core interaction functions, or other new structures that may be representable as new variables, bc they exist on another interface where problems & solutions interact, such as new variables regarding a new function type that can be used for problem/solution interactions
    - even if a problem is static, there will always be new ways to solve it because of the inherent variation captured in the concept of 'problem', and these solution-finding methods will always have new interfaces or other structures which can be used to frame them
    - there will always be new structures that are adjacent to available tools to fulfill interaction functions (like 'connect') between given new change types generating new interaction functions, and new interaction levels to reduce a system to in terms of finding core components of the system, new structures of errors from new change types, and possibly new primary interfaces created by new interactive change types and new useful structures to connect other new structures

  - why understanding is usually a better option than guessing methods like prediction function-finding functions
    - understanding can be used more effectively to derive a 'guessing method' than the opposite
    - understanding is more generally applicable & can fulfill more functions than finding/deriving/generating a particular 'guessing method'
    - understanding is less dependent on data such as context data
    - understanding has compounding value, where guessing methods usually offer temporary & otherwise conditional value while & where the original prediction function is accurate
    - understanding can find/derive/generate error structures of a particular guessing method
    - understanding is better at guessing than most guessing methods in most cases (except where a guessing method handles new contradictory information, a new problem type, a new change type, etc better than existing understanding)
    - understanding, learning, simulation, organization, prediction, testing are interchangeable functions with varying advantages but generally can replace each other

  - add to govt
    - democracy errors include:
      - alternate illegitimate routes to power like buying info goods like opinions, usage & user bases & user compliance/dependence, & artificially manipulating prices with information market manipulation, voter free time, job market info
    - 'excess regulation' errors include:
      - punishing successful solution-finders (by de-funding through taxes on profits)
      - regulations that 'take away or invalidate property or otherwise violate property rights' prevent people from benefitting from their labor
    - 'lack of regulation' errors include:
      - rewards those who are best at 'finding loopholes to exploit' rather than best at 'solving problems'
      - exploitation without accountability
        - companies that exploit customers can liquidate their remaining assets to build solutions for those customers
          - example: colleges that charge exploitative fees can liquidate assets to build useful AI models for banks that gave student loans
      - making people fight to survive (without extreme advantages like 'information' or its proxies like 'education to derive information' or 'internet access to find information') so they frequently see crime as the best option likeliest to help them survive
        - solution: creating 'organization' to make basic goods affordable, or 'artificially creating jobs' instead of organizing
        - example of an 'organization' structure:
          - if a supply chain is optimized (where 'optimal states' include such states as 'local supplies are distributed locally (such as by 3d printers that use cheap resources like sun/air/dirt as input'), basic goods will be cheaper
          - this is a case where 'labor & other resources' are 'organized' into an optimal 'supply chain' structure
        - other examples of 'organization' structures include 'location', 'language', 'communities', 'info tech' (specifically 'databases/apps', 'internet access/search', 'cell phones'), all of which can increase the probability of other organization structures occurring, like 'markets' and 'coordination' and 'companies'
        - these can be created by creating structures of 'interactivity' that enable organization structures (having a 'language in common' or 'good translation tools' allows 'interactivity' to enable 'markets')
      - lack of education or its interchangeables, which results in other errors like 'not identifying interchangeable inputs to requirements (basic goods)' (such as how 'independence', 'power', 'love', 'happiness', 'understanding', 'intelligence', 'information', 'solutions', 'potential' can be interchangeable alternates as they can be different formats of the same structure which is 'freedom' as in 'ability to adapt, as in self-correct')
    - companies exist to solve customers' problems, so if there's a problem people can & are willing to pay to solve, companies will solve it, so all thats required is to tell them what consumers want at what prices and what costs are preventing govt budget from being optimized for solving unprofitable problems, & the products will be built & tech will be built to reduce those costs, rather than taxing to create govt agencies to slowly & inadequately prevent the same group or corporate loophole-exploitations that can be identified by algorithms, exploitations which are symptoms of an inefficient system that hasnt invested in reducing its primary costs like 'health care' or 'form processing' yet, rather than unique exploitations that requires budget to solve inadequately when reducing primary costs would invalidate the need for these exploitations by group or corporate entities
      - example: 'tracking assets' is only necessary if assets are scarce & unique, and 'regulating the health insurance industry' is only necessary if health care costs are unpredictable, extremely variable, & subject to randomness, which they dont have to be if costs are reduced 
    - 'actively preventing their money from ending poverty' (with tax avoidance, not trying to end poverty, not trying to be sufficiently different from people who failed to end poverty, etc) is a sub-optimal (inefficient) use of money, proving that people who could end poverty (either individually or as a small group) without negatively impacting their own lives and dont do so, are people who cant use their own money in an optimal way, indicating that their success was almost a complete accident plus a little hard work such as 'filling out forms' or 'making a pitch to investors' or 'hiring people'
      - if their success wasnt an accident, they will be able to replicate it bc theyll understand how to become successful and will be happy to share that knowledge and will have rules they follow to achieve that goal (creating huge value out of almost nothing) and will be happy to do so for a government or other entity trying to solve a big problem, otherwise their success was an accident and they dont deserve to keep the outputs of that success (just like how I claim I can find a new way to automate problem-solving in a few minutes and I regularly prove the legitimacy of that claim of my value, they also need to prove they deserve their claimed value on a regular basis so it's believable, by demonstrating how they didnt need luck but rather their own work/intelligence to make good decisions that few others would have made in the same position, indicating that these are their identities differentiating them from other people, rather than an out-of-character decision, and teaching other people how to create huge value out of almost nothing, and making sure other people succeed at doing so, and regularly executing their process to create huge value out of almost nothing to solve important problems)
      - poverty negatively impacts these individuals who could easily end poverty, given that theyre constantly harassed about it by similarly selfish & stupid people, but they try to maintain poverty instead bc its cruel and they prefer cruelty
      - these individuals dont try to teach people how to use their money efficiently or figure out how to do so themselves (teach them to 'apply interface analysis' or even just how to apply one interface such as logic), which would solve the problem of 'their money being used inefficiently', so its possible they dont want to solve that problem
      - these individuals are not being offered protection by some powerful entity like a government if they do give most of their money away, after being criticized for many years, which has made them afraid of non-rich people, and should be offered things like some immunity & protection by governments & then they need to see that promise honored so that someone is safer after giving their money away rather than more vulnerable - there have to be rewards in terms of things they care about, such as more customers/profits - if they get richer by ending poverty bc everyone wants to buy the products made by a company that ended poverty, thats a sufficient reason to justify doing so, so a company that struggles to make advances in technology could get customers another way, by ending poverty which would improve their brand beyond any possible attack
      - if you told these companies that their customers/users really want to end poverty or really want cheaper medicine (which would indirectly end poverty) or really want to buy & donate cheap phones to poor people so every person at least has a cell phone so they can find the resources they need, these companies would find a way to create that product (the product being cheaper medicine, an easy way to buy phones for poor people, etc) & offer it, then market that product until it was used & successful at fulfilling the demand from consumers/users, bc thats the whole reason companies exist (to create value for customers), and if 'cheaper medicine' has value to consumers, they will create it - the tech companies are in the best position to do that so tell tech companies that people really want to end poverty and theyll find ways to create that value if the market exists (if enough consumers want it and are willing to help them pay or otherwise create value for it)
      - 'open-source accounting' takes places every time you buy something on amazon and you see the shipping costs added to your total, which you can compare with the comparable usps shipping costs & cost of finding the seller/product without Amazon, which I notice people who could individually end poverty by buying enough phones/water/food/medicine/generators to end poverty are not doing, possibly to avoid giving Bezos/Amazon any credit, so lack of transparency in how charities use money isnt good enough because theres another charity offering full transparency that ensures donations get to the right person, which is Amazon - all thats missing are the addresses to send these packages to and a coordinated effort (of large groups with small resources or a small group with large resources or both) to buy & activate those transactions
    - changes that can improve markets, laws, & govt interactions
      - 'spend govt tax money or excess corporate profits on hiring people to solve large govt/community/company organizational & social problems that add permanent value and are one-time problems (where once the solution is found, it can be deployed repeatedly and new problems can be solved instead) problems such as:
        - reduce their costs (automate tasks), unpaid debts, sub-optimal markets that are over/under-monopolized, find low-cost, high-value products that can be re-sold without modification (ad space, subscriptions/queries of an API, charger cables/headphones/cases, t-shirts/nft's/posters, medicines) or products that have a large probable user base to generate research funding for companies/govts, fix supply chains, high tax rates, allocating basic goods quickly at low cost, disincentivizing crime, fixing mental health problems, reducing unimportant jobs like re-building the same apps repeatedly & allocating labor to more important tasks, ending tribal feuds by similarizing ('humanizing') sides & physical separation, customized education for neurodiverse learners with different experience/interests, enabling vulnerable groups to report crimes or request help with cell phones, inventing & building crime prevention technology, over/under-spending, mismatched cause/responsibilities, unequal laws/opportunities, sub-optimal technology like biased algorithms, repeated work like govt websites, expensive medicine, deploying solutions once a decision is made about which solution to apply, design experiments to find solutions faster, coming up with new ideas to solve problems, lack of oversight/transparency, fixing corruption, preventing dictatorships & rent-seeking
        - alternatively, hire them to automate solving those problems (my invention), such as automating 'system analysis' to identify low-cost optimizations that wont cause errors in a system, automating 'interface analysis' to come up with new ideas to solve problems
      - 'temporary transparent directed costs' like 'temporary subscriptions' or 'temporary price hikes' that a company charges customers who opt-in to pay slightly more temporarily for particular purposes like 'researching a medical problem' or 'using clean energy' or 'funding transportation of supplies where needed', where the company offers a way to check how theyre using the money & states specifically what the extra money is for & specifically how theyll use it
      - 'multi-purpose investment funds' where people who invest are guaranteed returns in multiple possible formats, so if the fund doesnt fulfill an intent like 'make x% ROI', it can guarantee that it will fulfill another intent of the fund, like 'increasing clean energy use' or 'increasing quality of life for vulnerable groups', so either way investors get some value type that they care about
      - 'finding interim "state-connecting" solutions' that solve problems 'temporarily' or 'emergently when applied at scale or under other conditions', such as how covering/reducing 'transportation costs to high job-density areas or cell phones costs or basic good (medicine, clean water) costs or "valuable idea"-inventing costs (computers, machine-learning, encryption, dictionary/encyclopedia, solution/rules database, quantum computing, batteries, solar power, math, clean energy, search engine, sorting algorithm, bitcoin, interface analysis)' is an interim solution to 'ending economic opportunity inequality' that when applied at scale might temporarily end economic inequality, & once other solutions are found to improve job distribution in general, it might complete the task if the previous state is the 'state after applying the interim solution', rather than applying it starting from the 'worst case scenario'

  - add to tech
    - 'download/generate & delete app' process for each use as a preventative measure against malware that checks for an app thats installed before exploiting a vuln in the app
    - add concept of 'value overflow to adjacent nodes', 'local conditional deactivation', and 'local feedback' to nn
    - depending on propagation/routing function, a neural network node can contribute multiple features (influences) (in deactivating multiple weight paths) on the final set of generated coefficient sets for each iteration during training
    - examine other weight update functions, like those having an oscillating sequence that converges around a value rather than those having incremental updates in one direction
    - related problem: as weight updates occur, predict the remaining weight updates (what is the likelihood that weight updates follow one pattern over another or end at one value over another) so that some weight updates can be skipped
      - applying 'find prediction function' to various inputs/outputs in a particular solution-finding method like neural networks for the general 'find prediction function' problem format can be useful in some positions
    - add example of a 'generate variants' or 'generate possibilities' function by applying variables to a particular structure, to generate variations of functions including workflows, queries, or problem-solving functions like 'find structure in a structure'
      - 'structure x' can be replaced by 'alternative structures' like 'find generative functions of x' or 'find alternate routes to x or its outputs' or 'find generative functions of requirements & other inputs of x' or 'find variables of x and alternate variants of x' or 'find interchangeables of x' or 'find invalidating structures of x' with varying degrees of success according to various solution metrics
    - identify the 'reason why something works' (solution success cause) formatted as the 'structure of change applied to inputs/outputs that enables a later output to be optimized by some metric' and 'why the structure of change enables that or does so better than alternatives'
    - which format is optimal to standardize to when implementing function
      - using 'vectors' to represent everything require a function to map non-numerical values to vectors, which will normally either lose information or encode it inefficiently, storing more information than necessary, and not storing it in a way that can be compared with other numericalized non-numerical values in a meaningful way by default, without referring to the original non-numerical variable values
      - using 'inputs/outputs' to represent everything is more likely to avoid losing information, but does represent everything as a sequence structure, where sometimes sequences arent implicitly described, such as in a set, which would require a description in the form of a 'generative sequence' or an artifical order imposed on the sequence

  - add to science
    - roots to provide a core structure of alternatives as opposed to a requirement through one option
      - https://www.livescience.com/imaginary-numbers-needed-to-describe-reality
    - 'metformin and a low carb diet' reduce 'serum vitamin b' which can be used as inputs to 'cancerous growth'
    - check how folding relates to growth patterns like exponential growth, learning & information storage, and related structures like waves
    - the compromise structure is a network structure of 'multiple solutions' rather than one: require anti-vaxxers to self-isolate from people until the vaccine is distributed
    - life forms as 'structures of potential' (as in what changes a structure can cause that is different from what other structures can cause)
    - prioritize 'scouting nanobots' to clean out chemicals as a prevention measure
    - structures of invalidity like 'hiding health data avoids the solution to the health problem, which would invalidate the reason to hide the health data' and 'funding a prevention measure rather than a cure, which would invalidate the reason to fund prevention measures'
    - general relativity: does gravity exert limits on the change types/rates possible so that time cant pass as quickly/slowly depending on gravity, enforcing a ratio of similarity between states to conserve energy in higher gravity environments?
      - how do you direct gravity to restrict time in one position so it can occur in another
      - how to manage risk of 'matter-forming cascades' using technologies to form matter that can restrict time in some positions from gravitational effects
      - what changes occur if you decrease gravity of black holes like by positioning other black holes so they can exert gravity on the other
      - re-define time as 'change potential' so its clearer whether an increase in time means 'slower change, meaning more space-times or more opportunities for change' or whether an increase in time means 'faster change bc time is moving faster' or 'faster change of change types as in new change types' or 'relative change rate/types' or as 'unresolved uncertainties' or 'calculatabilities' as a measure of what hasnt been calculated or isnt calculatable in a space time given its relative change rate compared to other space times that will solve it faster and prevent change in other space times
        - time as 'structure', as in structures that are allowed to be 'efficient/interactive/powerful/organized/accumulated/irreversible/robust/derivable/measurable/inevitable/required/defined/sequential/maintainable'
        - observers have 'potential' in the form of functions like 'applying changes to matter', adding a variable to the 'position' attribute, obeying possible rules like:
          - connected structures (like observer/particle) exchange structures until they are equal (energy transfer of change potential between observer and particle)
      - whether 'time' has an ending, or a high probability of an ending, as determined by the configuration of space-time distortions in this universe, indicating theres no way to ultimately guarantee the preservation of time, as defined in the form of gravitational variations that ensure a space-time can slow down enough to be measured or distorted by high-potential or high-change sources like humans, or whether the current configuration guarantees that black holes will continue sucking up time until there's none left, or whether the current configuration has a high probability that change types allowed by lower-gravity areas will inevitably intersect in a way that creates a change type that will invalidate conditions necessary for space & time
        - what ratio of these scenarios are likely, possible, and involve risks or guarantees?
      - how time in one universe interacts with time in another - whether constants created by gravity and variables created by lack of it can interact with other constants/variables in adjacent, aggregate/net, reflective, or otherwise interactive universes
      - whether the similarities acting as inputs to standard gravity have corrollaries in the similarities acting as inputs to quantum gravity
      - what gravitational impact on time exists on the quantum scale - the farther away from standard scales a particle or other structure is, gravity applies less and time moves faster?
      - does structure/matter (order) act like an opposite structure of time (disorder) so destroying matter can create time in a particular position
      - can you navigate to other space times by increasing entropy to the entropy levels adjacently surrounding the target space-time (after which any matter navigating there reduces entropy to the target space-time)
      - does that mean you have to destroy matter (order) in order to power time-travel - would that invalidate motion in certain time-directions, as increasing entropy tends to cascade and create irreversibilities
      - is there a possible energy configuration that can increase entropy with enough precision that it can reach the target level
      - are constants like irreversibilities more similar to the definition of order or is the initial lowest-entropy state of matter closer to the definition of order
      - how do different structures that can create order (time crystals, black holes, other semi-closed or independent systems) interact - are they interchangeable in any way
    - uninhibited cell division
      - a process of applying regular damage of cell types successfully handled by immune system, rotating between locations & systems, to make sure the immune system is being sent to these components on a regular basis
      - alternate methods of increasing blood flow/circulation to every cell & distributing energy in the form of heat through exercise/sweat by increased connectivity/circulation
      - alternate methods of increasing cell replacement rate to prevent proliferation of dna mutations
      - rotate periods of inflammation, immune activation, scar formation, cell lifecycle triggering & stem cell differentiation, fasting/recovery, by triggering these processes in a way that avoids excess or compounding tissue like in fibrosis or excessive scar formation
      - forming or adding antigens on cells with sub-optimal configurations (like rb-binding, p53-deactivations or oncogene activation or insertion, or malignant transformations) as vaccine targets & to trigger creation of cells with correct configuration
      - 'aggressive regulator structures' as opponents to 'aggressive cell division structures', given that dna has built-in vulns that allow cancerous mutations to develop easily, what is the dna configuration that would aggressively prevent these mutations, and what is the cost of this being mis-applied to one in every x normal cells as opposed to one in every x cancer cells
      - 'variation-generating structures' to ensure a variety of dna configurations in a location in case cell division occurs so theres always a supply of active p53 copies nearby
      - directing fibrosis and other constraints to induce apoptosis in tumors
      - engineer nanobots to generate uv light & other p53/rb-activators where cell communication is hindered in ways that indicate cancer
      - activating genes or proteins inhibiting s-phase or death phase of cell cycle once the cell is malignantly transformed (is missing a 'protogen' as opposed to having an 'antigen', an 'opposite' vaccine target)
      - preventing degradation of pain or other immune signals by repeated exposure to triggers like heat
      - decreasing cell cycle timing in susceptible or vulnerable cells to reduce chance of mutation persistence after cell division
      - injecting any cancerous regions found with pro-health mutations or immune cell formation to offset ratio of healthy or immune cells & malignant cells produced by cancerous processes
      - moving cells with useful functions like 'destroying aggregates' to other positions where that function would be useful in 'preventing unrestricted cell division'
      - fixing 'illnesses that modify genes' is a problem of 'fixing genetic mutations', as genetic mutations can occur that mimic the effects of illnesses that modify genes, so prevention is not the most powerful problem-solving intent
      - fasting to direct energy toward 'replacing (killing & growing) cells' instead of just 'growing cells'
      - processes that increase metabolism (like exercise) as a way of fighting cancer, since metabolism is a regulator of the cell life cycle, and cell life cycle disregulation is a promoter of cancerous growth (more cells created than dying when cell division isnt necessary or otherwise useful)
        - look for other cell life cycle disrupters (triggers of cell division like chemicals causing specific mutations) & regulators (triggers of cell death like 'plant immune proteins')
        - look for 'immune states (including specific immune responses) that change metabolism temporarily' that could be used as anti-cancer tools, triggered by brain processes (memory) or other inputs to immune state changes (pathogen exposure, exercise) - 'addressing the systemic issue using other systems bc theyre on the same interaction level'
        - serotonin & other feedback mechanisms that act as signals to processes like cell life cycle regulators
      - useful questions
        - what is the difference between structures having problematic differences (pathogens for which immune response is always/adjacently constructed & those that are not) & what can resolve these differences (what changes can be applied to convert one into the other)

  - add to definitions
    - solution-finding method can mean a solution-finding method on any interaction level, such as a domain-specific problem space (like how 'regression' is a solution-finding method in the 'find a prediction function' problem space), or a problem-solution interaction level (like how a solution automation workflow or interface query is a solution-finding method), or interim interaction levels (like how a 'find connecting input/output sequence' is a solution-finding method on the function interface)
      - if the problem is (or can be) to 'find a solution-finding method', and can be applied to solve the original problem, these will also be called 'solutions' to that problem
    - pattern: similarity created by repetition
    - a 'problem' could be defined in various formats including:
      - difference between the initial problematic sub-optimal state and the target more optimal solution state
      - lack of functions preventing the problem or its causes, or lack of error-correcting or error-containing/isolating functions, or lack of variable-handling functions
    - where a 'solution' would be a structure that fulfills the solution/problem-solving requirements, and a 'solution-finding method' would be the structure that found/derived/generated the solution, optionally using the problem or problem attributes like problem type as an input (or using another solution, or the problem space as an input)


  - add to examples

      - example of logical fallacies like the rarity of simple rules, as everything is a simple transformation away from something else
        - such simple rules often reflect other simple rules like 'a simple rule is representable with more complex structures (like an infinite sequence)'
        - 'simple rules using core structures that can be replaced with other simple rules' is a rare structure
        - some simple rules are required to exist (everything cant be complicated or there would be no adjacent transforms possible)
        - in the iteration of all possible rules with incremental changes applied to generate new possible rules, inevitably the iteration will encounter simple rules that involve units at various points in the iteration
        - https://mapehe.github.io/math-problem/index.html

      - examples of simple interface queries that can find/derive/generate useful info
        - apply 'multiple' to constant structures like 'threshold counts'
        - examine lower-dimensional structures like boundaries as structures of 'simplification'
        - apply 'opposite' to 'sequence' structure like 'discrete to continuous structures' (where the continuous structures are the inputs)

      - example of why a structure is more useful than another which may be equivalent in some ways
        - an 'input-output sequence' (a 'state sequence') is more useful for things like 'connecting specific inputs/outputs' than 'a function database' bc 'inputs/outputs' are specific data/examples of cases where a function is applied making them useful for connecting specific problem inputs/solution outputs, and a function database takes less memory than an 'input-output sequence' database given that inputs/outputs will vary more than functions, whereas functions are more useful for 'finding connection functions quickly'

      - write example implementations of trial & error (various sorting methods, identifying solution requirements, testing function, identify solution variables like position/state that generate additional solutions, identifying solution space given solution variables, various solution automation workflows & interface queries applied), as an example of translating solution automation workflow into code that can be applied to example problem definitions
        - include example of creating all possible variable values, variable combinations & other structures, and other interface variables such as assumptions/context

      - solution automation workflow metadata, like for trial & error:
        - assume the problem space is equal to the solution space (rather than a normal workflow, which might filter the problem space of all possibilities to identify the solution space containing possible/probable solutions)
        - required functions: identity possible solutions, iterate through possible solutions, test possible solution
        - involves fewest changes to the problem space or problem structure

      - whether a solution or solution-finding method like an interface query is formatted as a set of requirements/sub-queries/sub-problems, change/function sequences, state sequences, input/output sequences, filter/limit/boundary sequences, format sequences, added structures, variable/function networks, parameterized graph traversals, other interface structure sequences like pattern or causal sequences, or other structures depends on adjacent structures and the requirements of the solution automation workflow & interface query applied
        - similarly, whether a solution automation workflow is applied to various problem/solution structures - such as to connect the problem-space and the problem ('differentiate a problem space so that the problem is easily solved by generating adjacent required functions') or the problem & a solution ('convert the origin problem state into the target solution state') or a standard solution & an improved solution ('apply changes to an existing solution until its improved') or a general solution intent ('find a line connecting subset averages of this data set') and a specific solution implementing it (code implementing a particular definition of 'subset averages') - depends on the problem/solution structures required by that workflow

      - example interface query of formats, requirements, metrics, variables & filters applied to tetris problem/solution structures:
        - select solution format as 'set of states (moves & rotations) of a block', 'set of final positions of a block', 'set of possible value of other solutions not used (options invalidated or opportunities closed by a solution)', 'set of possible positions of all blocks once games is over'
        - for the 'set of final positions of a block' solution format, the solution space is:
          - 'set of available final positions that a block can be fit to (in general rather than for a specific block)'
          - apply filter by 'specifying values of variables' (such as which block is referred to, in which position) to the above solution space
            - 'set of available final positions that a specific block (currently selected block, in its current motion trajectory & rotation) can be fit to'
            - optionally apply variables (moves and rotations) to expand or filter the solution space depending on what filters were previously applied
              - 'set of available final positions that the currently selected block can be fit to, once all possible changes (rotations and moves) are applied'
            - optionally apply filters in the form of solution metrics of other solution formats (impact on other future block positions rather than just the position of the current block)
              - 'set of available final positions that the currently selected block can be fit to, once all possible changes (rotations) are applied, and once impact on solution spaces of next incoming blocks is identified & optimized'

      - example of various implementations with 'user-submitted visit purpose' data and 'cellphone location' data, and requirement to "create an app to predict a user's wait time"
        - identify relevant intents to problem-solving intent to fulfill the solution requirement using the specific available input data
          - intents such as 'predict user wait times', 'import data', 'identify relevant data', 'predict end time of appointment', 'predict conditional average time taken by appointment', etc
        - apply function to find relevant data (out of existing available data, such as user location and user visit purpose) for intents relevant to the solution requirement 
          - 'location' data is relevant for importing 'actual appointment start/end times', 'user arrival times', as well as determining the 'order & number of people in the wait list' data, and may also be used for predicting 'appointment end time nearer to the end of the visit' data
          - 'visit purpose' data is relevant for finding 'static average appointment times' for a 'visit purpose type' or 'conditional average appointment times' data for various conditions like 'business' and 'hour of the day' which may be variables that change 'average appointment times'
          - 'audio' data could predict 'real-time updates to appointment end times'
          - these have differing accuracy for various intents: 'location' data is a proxy for wait list & appointment time data, whereas audio data could be a more exact representation, and location can exactly determine variables like arrival time
        - identify relevant structures to the solution requirement ('predict wait time') & determine variables of relevant structures
          - appointment times (predicted time before the appointment end, and actual time once known)
          - wait list
          - alternate relevant structures could include:
            - 'appointment complexity or urgency' which would mean the assigned staff should be considered unavailable for x period of time or even closing the office and routing users elsewhere
          - determine variables of the relevant structures to the solution requirement, like how 'business', 'hour of the day', 'scheduled appointments', 'urgent purpose visit', 'canceled appointments' may be relevant to the wait list as well as appointment times
          - apply variables of the wait list & appointment times to generate a set of possible/probable cases, and filter by whether available data can detect those cases (detect a chatty patient or a higher ratio of urgent cases)
        - determine variables of the solution requirement structures (user's wait time) (how many people in wait list, available staff, predicted wait times of preceding appointments, probability of more urgent cases arriving)
        - determine differences between variables of relevant structures of solution requirement (wait list, appointment times) and the solution requirement structure itself (user wait time)
          - which 'wait list' variables ('business', 'hour of the day', 'scheduled appointments', 'urgent purpose visit', 'canceled appointments') and 'appointment time' ('purpose visit', 'business', 'hour of the day') variables can change or connect to 'user wait time' variables (such as 'position in wait list', 'urgency of purpose visit', 'arrival time')
        - different implementation methods
          - predict any relevant interim variables like 'appointment durations' and 'wait list changes' as a way of finding inputs to predicting the original variable 'user wait time'
            - predict using conditional/average predictions of appointment times or sequential-data neural networks trained on location or wait list change data, based on prior data patterns
          - calculate exact wait list, arrival, start & end times of appointments and predict the biggest uncertainty (probable appointment duration) as inputs to a user's wait time
          - predict highest variation variables like 'purpose visit complexity' as an input to relevant interim structures like 'appointment duration' and plug into a standard calculation of wait time based on arrival/start times & wait list attributes
          - select one variable like 'audio' or 'location' data and apply sequential-data neural networks
          - find most explanatory features of as many variables as there are variables available & integrate them into a prediction function
            - 'co-occurrence' or 'adjacence' of variables like 'high urgency user location adjacent to another high urgency user' indicating a higher wait time for users after them
            - integrate with derived user routing information such as 'user prioritization' to determine what changes to location/audio/visit purpose will impact user prioritization (and position in wait list)
          - other various implementations using different inputs/outputs (predict original solution requirement variable from available data or data derived from available data)
            - use existing solution-finding method (regression, machine learning, etc) and apply "different input/output variables" to prediction function
              - 'given existing wait list & purpose visit data of other users, predict user wait time'
              - 'given location patterns of other users before current user, predict current user wait time'
              - 'given arrival & exit patterns of users in general, predict current user wait time'
              - 'given values of high-variation variables like purpose visit complexity & staff skill set & staff fatigue, predict user wait time'
            - use existing solutions and apply "different prediction function inputs/outputs" (predict changes to standard prediction function borrowed from other existing solution code base or model, based on differences in problem definitions & requirements & data)
              - 'identify problem type (queues, timing/duration predictions), generalize & apply generalized functions from existing solutions for that problem type to available data'
          - the above implementations require different functions to be found/derived/generated & applied in various interface queries (the above workflow requires a function to derive 'purpose visit complexity' from 'purpose visit', possibly using prior purpose visit & appointment time data)

    - example of deriving the neural network structure
      - deriving the neural network structure by applying useful insights relevant for problem-solving intents like 'generate the solution' (as opposed to finding it) - insights such as that 'a structure (like a function) can usually be generated from structures similar to itself (a function) or structures of the same type (a function) or structures on its interaction layer (other functions, variables)' an insight which is derivable from math structures such as 'requirements involving the output data type of a particular operation like addition' - to derive the solution structure of a 'directed function network' as a useful structure to use as input to fulfill the intent of "creating a function to solve the 'find a prediction function' problem", given that functions can be formatted as 'sequences and combinations of other functions' and a 'directed/tree function combination network' could 'apply multiple combinations of functions' or 'apply functions in multiple sequences', which can 'build a function' so are useful for 'building a prediction function'
      - find a prediction function
        - build a prediction function
          - build a 'directed function network' to build 'function combinations' and 'function sequences' that could 'build the prediction function'
            - identify change structures (such as weight update or routing functions) of function combinations/sequences that would allow maximal or otherwise useful differentiation/reduction of function combinations/sequences
          - identify core change structure as a 'function performing a sum of weighted input coefficient changes' to create the useful change structure 'different prediction function coefficient sets' that would solve the problem of 'sorting/filtering possible solutions in the form of prediction function coefficient sets', where this change structure ('function parameter changes') happens to be the only change structure allowed in the problem space, so by definition the solution format for the 'find a prediction function' is a 'set of constant values for prediction function parameters applied to inputs', and the way to generate those coefficients involves applying changes to an initial set of coefficients, and the changes involve structures like combinations of core changes like 'summing weighted inputs' where inputs would be 'previously summed weighted inputs' or the 'original inputs', to generate 'different sets of coefficients' to input to a 'solution metric testing function'
            - then identify structures that could produce these change structures

  - add to useful structures

      - the idea of a 'consciousness' and 'personality' as a useful interim structure connecting a 'brain' and the 'intents' of the 'bio-system containing the brain', acting as the controller of the brain to serve requests made by the 'bio-system' in fulfilling its intents, but may be a false structure where the actual functional structure is a 'router' or 'platform' for these connections to be made, with no agency, where the 'consciousness' is the 'output' or 'input' rather than a 'connecting structure', where its likely to be all of the above bc it exerts similar interchangeable control as the bio-system intents, general system incentives, and brain structures, indicating a 'mutually dependent' structure

      - why are some structures more useful than others? 
        - bc they are more adjacent or directly fulfill definitions of usefulness, such as 'power' in the form of 'inputs' (which control other structures by activating some functions) or 'controls of other structures' in general
        - example: why is heat a useful structure? bc it is an 'input' ('power' structure) to (as in 'creates' or 'increases') the attribute of 'interactivity' which is a very useful structure for intents like 'connect' and 'change', acting like adjacent power structures of 'heat' (like 'energy'/'electricity') for other dependent structures to function

      - functions representing core operations of generally useful functions
        - example: 
          - for the 'find' function, useful functions include 'check if structures are equal/similar' 
          - for the 'build' function, useful functions include 'check if structures are core/interactive/coordinating/opposite/fitting'
          - for the 'apply' function, useful functions include 'identify variables in a structure' and 'apply different value of a variable to a structure, as specified by this change function' and 'check if structures are different after applying change function'
        - these specifically relevant functions would allow adjacent implementations of the general functions like 'find'
      - functions whose outputs can be their inputs, like iteratable functions, where the inputs/outputs are relevant to or are useful structures
        - where problem-solving structures or methods can be iterated, apply them iteratively where complexity isnt reduced by prior iterations
          - 'find solution-finding methods of solution-finding methods of solution-finding methods'
          - 'find structures in structures in structures'
          - 'approximate apaproximations of approximations'
          - 'predict prediction function of prediction function inputs'
          - 'alternate alternatives of alternatives'
      - apply sources of ambiguity as possible interfaces and variables
        - if an ambiguity is maintained, the measurement/metrics of the ambiguous alternatives are wrong, or the assumption that they were similar alternatives is wrong, or the assumption that a selection should be made is wrong, or the assumption that either is correct (as opposed to neither) is wrong, or the question being asked about their similarities/differences in metrics is wrong, or the optimal alternative is a combination of alternatives or different option from either alternative, or its a structure of randomness or equivalence, or change types are involved which are not measurable or which dont have a measurable impact in that system/interface, or it can support enough change types of a unique conceptual foundation that it qualifies as an interface
        - if it doesnt resolve, it can be a set of interchangeables forming an interaction layer, which can act as an interface, like how 'variables' & 'state' are both structures of change that would seem like ambiguous alternatives under the wrong measurements
      - some specific functions like 'learn (as in update/improve on a metric)' as opposed to general function (like 'change') are more useful than others, bc of their alignment with & relevance to problem-solving intents like 'improve an existing solution'
        - existing solutions always exist or can be adjacently derived, such as 'try every combination or possibility', and these can be used as a standard or base solution to apply improvements to
        - 'updating to improve a metric' can take the form of 'moving/changing a structure toward a solution position/structure' having a 'state sequence'
        - there may be multiple solutions in the space in which case determining the most adjacent solution, ambiguous alternative solutions, solutions for different optimization metrics, and the most optimal solution for the original metric are relevant sub-queries, and only framing it in terms of 'connecting two states' is not useful as it will miss the other possibly optimal states to aim for
        - 'build a learning function' is more useful than a specific 'learn a function or pattern' function with data & a metric to optimize as its inputs, but requires interface analysis to identify concepts like 'deactivation' & structures like 'alternate coefficient sets', 'incremental changes', 'accuracy feedback', and a 'stopping point' as useful structures for a learning function
        - a function fulfilling a problem-solving intent like 'improve an existing solution' would ideally have multiple implementations for various fundamental solution automation workflows, workflows like 'connect problem/solution states' or 'apply solution structures to the original problem structure' or 'apply differences to the problem or errors' or 'apply differences to sub-optimal solutions' or 'generate solution from problem space or solution space components' or 'apply filters to the solution space' or 'apply useful structures to the problem'
          - the 'improve function on a metric' function would have:
            - a 'connect problem/solution states' implementation, where it would start with the data or a standard solution and connect it to a more optimal solution, such as through iterative state updates
            - a 'build solution out of components' implementation, where it would start by identifying available components in the problem space or required components of the solution and aim at building those & then combining them in a way that fulfills the original solution structure
            - a 'apply solution structures to the problem structure' implementation, where it would start by identifying solution structures such as a data set point that should intersect with the prediction function or a solution metric to fulfill like a level of accuracy to aim above, and then applying those to the problem structure, which might be a standard sub-optimal solution or the data set
            - a 'apply filters to the solution space' implementation, where it would start with the solution space, which might be a set of functions that intersect with or represent at least some subset of the data set, and iteratively filter out solutions based on a solution metric
          - these various implementations would allow the solution to be found in various situations like different available inputs
      - solution (prediction function) metadata should include attributes like:
        - which data points from the original data set are prioritized by the solution given their better represented in the function, and if other points are prioritized, how much does the solution change, and why are those points prioritized
        - which data points will fail with this solution and by how much
        - which representative examples of the original data set will succeed/fail 
        - why this solution was selected having been constructed this way, given the infinite equally accurate functions available (to use a standard representation metric, to generalize across other data sets, to retain a percentage of original data set information, to account for other latent variables)
        - how much randomness and how much variation in how many alternate data sets this solution will handle
        - in what thresholds in what cases this solution will become clearly sub-optimal, and which solutions will be optimal after those thresholds are reached (format of a 'network of alternate conditional functions')
        - how likely it is that there is an alternate solution that is interchangeable in terms of robustness across some variation in different data sets, similar fulfillment of solution metrics, or similar function metadata like a similar generative/descriptive function
        - how much & what quality of the original data set information is retained (quality of information like its average)
        - why variable interactions are justified & used in the solution
          - example: if two variables are usually around 1.67 times the other (like an animal's height & width), why would that be the case:
            - useful functions for the animal like 'speed and balance when moving' could be inferred from that data set if variable label information is retained and has access to a scientific/language dictionary
            - structural concepts like 'optimal ratios' would be noticed across various problems with different data sets once it was repeated enough, which could be identified through a repeated pattern or derived through filtering out alternatives like 1:1 or 1:2 ratios, which are less common because it's uncommon to find integer relationships in nature, it's less likely that one change unit of each variable type is likely to be optimal, and it's less likely that an excess of one is optimal for a unit of the other, given how related variables in a type or interface have some similarity given their common or related causes such as genes & functions, or derived through useful structures like 'comparable but not equal quantities' often found in 'related variables with a trade-off structure while an uncertainty in the form of an optimal ratio is being resolved'
      - filters of solution automation workflow implementations (interface queries) are particularly useful bc there are many ways to implement trial & error ('try every possible solution'), even though it would seem like there is one way to implement it, there are other variables of the implementation, such as the order in which every possible solution is tried, as well as how to identify what is a solution (a combination of some items in a set, or an item in a set, or a sequence of items in a set, etc) and the full set of the solution space, and not every interface query or solution automation workflow that the query implements will produce the same answer for these variables, although some of the answers are requirements by definition rather than variables, like how applying 'trial & error' to find the right block in tetris would have a 'correct' solution space of the 'set of available positions that a block can be fit to (in general rather than for a specific block)'
      - identify absolute rules as useful structures in the form of reliable functions to fulfill their original intents by those rules which apply absolutely in all cases, such as functions whose output cant be changed from a solution to an error or sub-optimality for differences in inputs
      - differentiative definitions of error structures like those determining sub-optimality, neutral structure & error structure
      - error-correcting structure requirements for error structures such as:
        - 'using only internal components of the structure in error to generate a solution to that error' for an error that occurs in one position
        - 'removing structures added with no or mismatched intent alignment for structures in error having no associated intent'
          - example: in various 'pooling' functions, one option may be selected at random (for no reason other than to generalize) when there could be a reason to select one option above another (it fulfills an intent like 'increasing solution accuracy' or 'reduces mismatch error between intended and actual functions')
      - add to other useful functions on the interim interaction layer which include 'invalidate' & 'enable'

    - add to error structures
      - structures of volatility where 'small input changes can drastically change outputs' are useful for 'differentiation' and 'comparison' intents but not 'connecting' or 'equalizing' intents, as an opposite structure of 'ambiguity'
      - structures of invalidity such as where 'an area produces a level of error in a prediction function that invalidates fulfillment of a required solution metric' for the 'find a prediction function' problem
      - when 'alternatives' are actually 'mutual dependents'
      - when an 'abstraction mix' is useful: 
        - all variables are related to all other variables, so removing variables is inherently an error structure in the form of removing information that could be used to predict the output variable, but some errors are un/acceptable and some are useful, such as how the errors contributed by an over-predicting function and an under-predicting function can offset/neutralize each other, so an 'average' error structure (removing some data points/variables) is one of the 'useful/optimal' error structures, rather than one of the 'sub-optimal error structures' that can be neutralized by a similarly probable error in the form of its 'opposite error structure'
        - there may be 'contributing variables' in all possible alternate formats of a particular input set (in its original state, after change combinations are applied, after being filtered, as described by summary stats like averages, in identifying/generative function form, etc)
          - example: a 'set of variable values' or a 'set of values of variables' or a 'variable value' may be just as useful on their own, as 'change combination & filter functions applied to multiple data points', so the combination of both the 'data point' or 'variable value' and the 'variable created from those values by a change combination function' should both be handled as inputs by a solution-finding method, rather than removing prior values and replacing them with new calculated values, which is what a neural network does in forcing a linear direction of operations to create the output prediction value, rather than including the original inputs as optional inputs for each node layer/unit, implying that the original inputs cant exist on the same interaction layer as a variable created from those inputs, which is not an absolute truth that should be enforced by the network structure
          - complex structures like cases where an original input is one possible input of a variable created with change combinations, but other inputs exist and show up in other original inputs, so calculating the variable just from the original input and removing the original input is not optimal in cases where the other original inputs should be used and the original input can co-exist unchanged with the 'changed' variable ('coexist as output variable-influencers'), but using the original input instead of the other original inputs might remove the original input's influence on other processes when its in different states (other change combinations are applied), rather than applying an identity weight (weight of 1) to preserve original inputs whose impact can be replaced by changes to other variables that dont impact other processes
          - this is an example of a case where 'keeping data in different formats' (such as 'changed' & 'unchanged' inputs) is optimal

    - variable interaction rules
      - standardizing variables of a data set to the same interface increases the power of those interface structures so that only one interface needs to be applied
        - as opposed to variables including cross-interface structures, like 'type variables' and 'core variables' which should be standardized to one or the other, rather than including both formats
      - variables follow similar interaction patterns such as 'building the next interaction layer by creating a new set of interchangeable alternates', and while these variable interactions are being identified in available & updated data sets, alternate solution functions should be maintained to avoid an error structure of 'prematurely selecting a solution' as a subtype of the error of 'hard-coding a variable value', which may also be an 'irreversibility' error structure, so that a pattern of change in a particular direction can be identified (which could vary as the system having these variables could be moving towards or away from forming an interaction layer or other structures), as most functions will be better formatted as a function network than as one function of input data set variables, which not only handles 'missing information' and 'ambiguity' error structures, but also handles 'predicting variable interaction structures' as 'finding the direction of change of the variable system' is useful input information to the 'predict variable interaction structures' problem, as variable interaction structures may change in predictable ways given change patterns
        - 'finding alternate/component/generative functions' and 'finding system change pattern update functions' are intents supported by the 'function network' or 'parameterized/generative function' structure, as opposed to the single function structure as the solution format
      - allowing some errors to avoid over-reducing a function inputs, when solutions may have a false similarity with error structures, so that reducing/preventing known errors also prevents useful signals from being handled, such as other errors or solution structures that resemble errors, as an optimal 'approximation' (fuzzy) solution that can be changed within a certain range to cover possible alternates
      - why is a 'variable network' less optimal than a '"change combination" function network' where many 'scalars' (weights) as functions are applied to many 'change combinations' (node inputs & input sums) as functions
        - a 'network' is sub-optimal in general where its useful to compare 'uses/queries of the network', in which case 'sequences of steps (where steps represent objects, variables, states, functions, etc)' are more optimal structures that allow comparison of step patterns and step sequences, whereas the network would format these 'uses/queries of the network' as routes between network nodes, which are not as clear for comparison intents
        - the variable network could represent different change types/combinations as queries/routes of the variable network, but this wouldnt allow clear description of inputs/outputs of various routes as separate from other queries using the same variables in a slightly different route (change combinations are created by flows through or uses of the network where 'change combinations' are represented by 'variable node sequences linked by change-generating functions'), as opposed to a structure supporting separation of change combinations (creating different nodes & layers to temporarily store 'change combination state' applied to possibly useful change combinations represented by sequences of weight paths overlapping at the end node to create multiple different trees creating different coefficient functions, so different change types/combinations are represented in a way that allows them to be structurally separable from & comparable to other change types/combinations in a standardized way (comparing relative trees of inputs moving in the same direction, as opposed to comparing unrestricted or randomly assigned network routes))
        - possible structures to use as 'function' structures in the 'neural network' problem space system
          - explicit structures
            - node-weight units
            - feature organizing functions (position or other similarity of features as a structure of relevance), weight initializing/update functions, input routing/combination functions, weight application functions, etc
          - adjacently implied structures
            - node-weight structures such as node-weight sequences (paths)
            - overlapping weight sequences (trees)
          - interim inferrable structures
            - node-weight structure neutralization/magnification structures
            - weight deactivation, near-deactivation, & activation node-weight unit sub-networks
            - node-weight weighting/prioritization functions (structures allowing repeated node-weight unit values increasing the priority of that node-weight unit value)
            - error structures like ambiguities, volatilities, improbabilities
        - possible structures to use as 'variables' structures
          - explicit structures
            - 'weights, weight change, & node output values' are the default variable structures specified by the network
          - implicit structures
            - the 'net impact of weight sequences' is an implied variable
            - 'change combination' structures including parameters of 'weights' as a sequential set and 'weight paths' as a sequential set that is sequentially updated, converging on a final 'weight path' sequential set or a final 'weight' value to find variants of the prediction function inputs that are likelier to be useful
            - 'change unit' structures such as where a node/weight unit combination can produce a 'big change' as opposed to a 'small change', a 'cross-threshold change', a 'cross-output differentiating (category) change'
        - other possible useful neural network structures
          - 'consolidation' or 'filtering/selection' nodes randomly applied in deep learning layers to cover cases where increasing the dimensionality of a weight coefficient set is not optimal and sufficient dimensionality has been reached, like when a 'type' variable doesnt usually need more than x changes applied to inputs (given allowed weight update structures) to be identified, as in cases where the 'type' variable is adjacently computable with inputs, at which point exploring further changes to that variable would be sub-optimal, but the type variable is predicted at row 5 in a network with 10 layers, and the type variable is more useful in predicting final values without further changes (further changes that result in higher prediction inaccuracy), and the type variable produced by those changes to inputs overlaps with another variable in a similar position, so preventing further changes to that variable would be the right move for the 'type' variable but the wrong move for the other variable produced by those changes to inputs, in cases where the data set indicates a different category or phase or interaction level of the variables of the data set, so randomly consolidating that node's output (the node where the type variable is accurately identified) & preventing further changes (instead propagating it as a constant that is not modified by subsequent weight changes) might preserve the information of two variables identified by that weight path
            - structural similarities like this weight path covering predictions of two variables with similar change types that occasionally occupy the same input because the data wasnt separated by these different variable sets or standardized so the alternate variable would be in a different position can identify optimizations to rules limiting network weights & weight updates
              - general network update rules can be injected in a way to prevent network-wide impact that individual nodes may not always be able to calculate (how would a node identify that its optimally encapsulating a type variable or multiple variables, invalidating further changes, without feedback at the network level optionally at each iteration)
            - assumptions are built into the 'node layer' structure such as assuming the 'same number of change types should be applied to the input feature vector' or that 'just-over/under-threshold values can be ignored a percentage of the time as the threshold is likely to introduce an inaccurate barrier separating values, for some proportion of the time'
            - deciding where to host changes is an important decision in aligning 'certainty of a change' with the 'enforcement of that change' (such as 'hard-coding variable change rules in pre-processing functions' because 'those pre-processing changes are more certainly correct so they are injected as assumptions'), whether to host changes in:
              - the default structures of the data set input features & weights & the weight update/propagation functions
              - the implicit/emergent/interim structures such as the 'change structures supported by the weight update/propagation functions', functions limiting network-wide or sub-network iteration changes, functions limiting net impact on 'variation of weights tested by the algorithm', functions that have an output of decreasing required training iterations, etc) 
          - a useful graph would be a 'prediction function change impact map' indicating how structures of explicit/implicit change structures such as 'change types supporting differentiation across ambiguous alternate predicted values' and 'change sequences supporting multiple variable interaction structures' and 'change sequences supporting various difference-resolution functions', so that some areas of the map would indicate a particular alternate prediction function or function having a particular 'type' or 'solution metric range' would be predicted, and if areas indicating different prediction functions are equal in area, then multiple conditional functions may exist, or the functions should be required to be weighted if theyre not equally or more optimal than combinations of them, or an ambiguous alternate may be unresolved in which case more info is necessary to select a prediction function
            - impact of neural network structure, algorithm, initial weights, & other parameters on the change types supported by the network according to one data set can be combined with the same graph for other data sets to create a 'net or weighted impact' or 'conditional impact' graph across different data sets that point to the same general prediction function (or parameterizations of it, or alternate/subset/combination functions) given the priority/weight represented by each data set
            - other useful attributes can be identified for the network, such as 'common core functions' of 'alternate optimal prediction functions' identified by the network
            - one prediction function might actually be a 'subset function', given some 'variable invalidating' structures present in the trained model for that data set that effectively removes some variables
            - other 'prediction function types' are relevant as other possibilities, indicating that a set of alternate prediction functions should be identified & their actual function type should be identified so they can be combined or kept separate in a function network or parameterized function representing the alternative optimals to improve the chances of finding the actual prediction function which may be a combination of those alternate functions
            - deriving the set of 'probable alternate optimal prediction functions' is useful as a way to connect the solution format with a more adjacent structure (the set of 'probable alternate prediction functions') so neural network parameters can be initialized with or calculated to have parameters likelier to converge to those 'probable alternate prediction functions' and identify the correct function(s) faster
              - in other words, find 'probable alternate prediction functions n change types away from the initial functions represented by initial network parameters if that would differentiate the alternate prediction functions faster, or n change types away from a combination of these probable prediction functions if a combination of them is expected to be useful' and then apply a neural network 'supporting those change types/amounts'
              - other functions would be useful such as 'probable median functions between alternate sub-optimal prediction functions' and 'probable weight functions prioritizing probable component functions of the probable prediction function(s)'
            - finding which input-output structures are unique in a set of 'possibly useful functions' is useful in filtering the set of functions required for a complete solution (involving the 'alternate sets of unique input-output structures')
          - other change types than the defaults supported by the network, which would be useful in various cases, like cases where two variables are probably related/correlated (but not clearly similar/different such as equal or opposite of each other), in which case when the value of one variable is calculated, the other is not directly determined by that value, so a network that has a net impact of finding an equal/opposite correlation between those variables can be ruled out
            - other variable interaction structures can be applied to make sure the network tries changes corresponding to all possible/probable variable interaction structures
            - variable correlations can have a corresponding structure of 'variables adjacently connected' such as a 'variable causing another variable with n equivalent linear operations (being n degrees of separation away)' which should be reflected in the possible/probable/guaranteed/required/optimal/implicit/explicit change types allowed between those nodes if theyre on different layers
            - nodes that represent randomness injections by randomizing their output can also offset some assumptions about change types that the network is required to support in order to converge to the correct coefficients quickly
            - identify change structures like 'weight update sequences' and 'weight paths' that cover many change types by default, like where a coefficient change can be produced by many structures of change types
            - identify change structures that cant be guaranteed or probably/adjacently produced by a network and/or data set and add their impact to the outputs during training, if theyre likely to be relevant given their input/output-connecting capacity (variables that cant be probably/adjacently derived by the inputs & neural network structures/parameters)
        - 'run-time deactivation' functionality can be built by:
          - integrating other information from prior predictions/feedback (integrating 'adjacent output values' as a filter of which nodes should be deactivated if they are producing values that will produce extreme differences between adjacent output values and the output value currently being calculated by the iteration, given weights available in the next layers that could change an extremely different value)
          - integrating requirement info, like which 'sums of change combinations' are required to generate the differences that are useful in selecting a solution
        - whichever change structures can be guaranteed to be produced by variables like weight paths can identify the error structures that those structures miss
          - if a network parameter set can produce change structures 'having a certain degree of difference', it has an error structure of 'missing the possible solutions between those different values'
          - if a network parameter set can produce change structures of 'extreme change', it has an error structure of 'producing values that may not be adjacently convertible into target outputs' such as where 'a particular node output is not convertible to the accurate output value, given the following available weights/nodes'
        - change structures such as 'incremental changes' are derivable as useful to building an accurate prediction function that involves improving a standard or base solution which is already near its optimal value

    - add to nn error structures
      - backpropagation cant identify error structures such as 'previously barely deactivated nodes' that would have contributed with slightly different inputs (ability to recall prior deactivation values & decide whether to re-activate a node, possibly reverting to a previous training state with less information but excluding fewer useful nodes)
        - if a change to inputs required to avoid deactivation or optimize weights according to some optimization structure (like creating weights that 'maximize differences in outputs') doesnt contradict the output of other weight paths, deactivating a particular node can be avoided or weights can be optimized if its barely under the threshold (which moves the error to a new threshold, the threshold value to determine whether to apply the original threshold value)
        - examine effects of 'weight-swapping' across structures of relevance like 'adjacent nodes' if node position indicates initial or emergent similarity in weight path or 'adjacent weight values in a node layer' to test if similar but not equal weight updates apply to the possibly similar weighted feature sets that created those similar weights or are otherwise useful in generating errors (such as how values relevant by a similarity such as adjacence should be or remain similar)
      - cant identify its own possible error structures, such as assumptions/biases or the cause for its errors like 'missing information', or change its predictions given its error structures to correct them or at least account for them
        - cant identify why its not being used (has error structures for a particular problem), which is bc its a function network, rather than a data store of 'user intents' and 'user queries', and 'functionality to infer this related data to improve itself' is not built-in to neural networks by default
          - by default, neural networks optimize for 'finding prediction (variable interaction) functions of the original data set', not 'creating an optimal solution-finding method for finding variable interactions in a data set' or 'inferring related data to improving itself' or related intents, integrating 'local specific optimization structures for a particular solution metric' (to find prediction function for original data set) rather than 'generally useful optimization structures' or 'optimization structures specific to problem attributes like problem type'
          - a target explanation to generate is: 
            - 'given my understanding of the concept of "differences" applied to identify different variable values such as a "category" for a particular input in the data set, and given my ability to identify differences & my ability to derive output features from input feature change combinations, I think these inputs map to these outputs with x% accuracy, which should be adjusted for my inability to correct "ambiguity resolution" errors in contexts such as "unexpected new inputs" which differ from the ambiguity structures I can resolve, because the differences I cant resolve are less obvious'
            - this explanation involves the network's abilities (functions) & decisions (solution filters/selections), the error structures the network can infer as possible given its functionality, the cause of those error structures, the interface structures like 'concepts' and 'assumptions' involved in its functions & decisions, & the cause of its input-output mapping decisions, and how the network (& its probable usage intents, given its abilities) relate to problems including problem attributes like 'obviousness of a solution'
      - if the output isnt exclusively a function of structures (like combinations/subsets) applied to inputs, it wont identify those structures that its missing
        - if the neural network 'differentiates' too early (by commiting to a particular structure as a 'certain' useful structure for generating the output, like stem/blast cells differentiating too early or certain pre-processing rules applied to data that shouldnt be assumed constants), that differentiation cant necessarily be reversed later by emergent effects, so differentiation should happen on subsequent layers rather than immediately after inputs
        - this would occur if there are multiple input-output sequences of different structures leading to outputs that coordinate (rather than providing alternatives)

      - over-prioritization of correcting known errors, such as 'filling in missing information', which occur frequently in language data sets but not frequently in highly structured language data sets like math word problems
        - example: the model thought that John and Ali should have every attribute/function the same, including an attribute such as 'number of classes' and 'total capacity of classes' and the problem should have similar structures as the solution so the text 'combined capacity of classes' was assumed to be required, because over-prioritization of a priority like 'equivalence' fulfilled by 'inferring missing information' can create errors like 'forcing equivalence of different variables that dont need to be equal'
          - https://openai.com/blog/grade-school-math/
          - these 'mental models' of networks involve various functions, errors, alternatives, priorities, concepts & other interface structures, including functions as 'components of the concept of intelligence'
          - these 'mental models' of networks have some alternative structures, where the reason for an error/success could involve multiple different interface structures
          - they also have some 'neutralization' structures where a particular function would 'invalidate' an error structure
          - these are not direct or exact mappings from the neural network parameters to interface structures, but rather these interface structures offer an understanding of the emergent effects of the model when applied to a particular input, that may be a functional understanding that can be changed with later output indicating other interface structures
          - on the 'agent' interaction level, the corresponding object is a 'personality', such as a network that is particular averse to learning or new information, over-simplifies everything, more rational/logical than other networks, more stubborn in its resistance to change, more interested in the truth than other networks, lacking in self-awareness in not correcting its own errors, more intelligent in having more abilities (like an imagination/simulation function) or having more experience or better at avoiding errors than other networks, etc 
          
          - a 'mental model' of the neural network from example 1.1 can be inferred as:
            - having the ability to identify the correct 'missing information' error to fix ("capacity of John's multiple classes" and "total capacity of John's multiple classes"), given its ability to identify when the correct variables are different (number of classes, which determines whether a connection function relating their class capacity needs to be adjusted (for multiple classes), or is equivalent), which differences need to be resolved in which sequences (calculate John's individual (unit) class capacity before calculating total capacity of John's multiple classes), and which variable differences are relevant (they dont need to have the same number of classes or class capacity, but John's class capacity needs to be calculated just like Ali's class capacity was given before total capacity can be calculated, and their total capacities need to be calculated the same way before the values can be added to determine the overall total)
            - having the ability to identify when an object is relevant to adjacent sentence's objects in the sentence sequence
            - having the ability to identify related or equivalent objects ("Ali's total class capacity is given, so John's must be calculated, as John's total class capacity is an input to the solution format of 'overall total capacity'")
            - having the ability to sort information in order of factuality, prioritizing factual information first to start applying possible connection/change functions to facts first, identifying that assumptions need to be based on facts
            - having the ability to identify membership 'class of John', 'class of school' to infer object connections
            - having the ability to identify the solution format of 'combined capacity of all classes in all schools', thereby connecting the given & adjacently inferrable information 'class capacity' and 'class count' with the requested solution format 'combined capacity of schools', inferring the connection between 'class' & 'school' in the process
            - understanding of how to build features from sequential operations applied to input features, like how 'class capacity' and 'class count' are inputs to a 'school count', and how 'school counts' are inputs to 'combined count'
            - possibly a concept of problem-solving workflows such as 'build possibilities from facts', 'build outputs from sequences of functions applied to inputs', 'connect original inputs with target solution output', 'identify differences/errors to resolve/connect/equalize and only connect those differences/errors', 'identify requirements & fulfill requirements using inputs', if the neural network can store the abstraction of the 'workflow' rather than generating the workflow implicitly in solving a problem

          - a 'mental model' of the neural network from example 1.2 can be inferred as:
            - an inability to identify when a value has already been calculated, in assuming an inherent required difference between 'total capacity of a school' and 'class capacity', in the case where 'class count' is one
            - an inability to infer alternate contexts, such as the case where 'class count' is one, making the 'total school capacity' and 'class capacity' variables potentially equal
            - an inability to identify that there is more than one school object, despite the differences in school descriptions
            - an inability to infer an error of a 'false difference' or avoid this error, in identifying 'total capacity of a school' and 'combined capacity' as necessarily different when the 'combined capacity' was already calculated because of the assumption that there is only one school so all schools should be grouped in the 'combined capacity' calculation, and if this error had been avoided, the other error wouldn't have been a problem, as 'differentiation of schools' isnt a necessary input to 'add the individual class capacities'
          
          - a 'mental model' of the neural network from example 1.3 can be inferred as:
            - having the ability to correctly identify 'total capacity of John's classes' as a relevant interim variable value to calculate before calculating 'combined capacity of all classes', but has an error in identifying the correct calculation to do so, instead performing the calculation to determine the individual capacity of John's classes and conflating that output with the output of the next required calculation in the sequence, 'total capacity of John's class', instead of finding the 'total capacity of John's classes', which is the similar comparable variable value to add to the 'total capacity of Ali's classes'
            - having an inability to identify whether an operation has been done, is required, or in what sequence the operations should be executed, producing errors in 'missing operations' and 'incorrect operation-output connections'
            - having an inability to identify the inequality in the class count used to calculate John's total class capacity compared to his actual class count as relevant or as an error to fix
            - having an inability to identify the difference in the variable 'effective coverage of class count applied', rather than inferring the concept of 'all' from the 'total capacity' given for all of Ali's classes, indicating an inability to infer the concept of 'all' or inability to infer the a class count of 'one' can be equivalent to the class count of 'all classes', or an inability to infer that not every value should be equal, and John's class count can differ from Ali's class count, rather than calculating the corresponding value of one of John's classes for comparison to one of Ali's classes in the addition operation to find the 'combined capacity'
            - having an inability to identify the right format to input into 'combined capacity' addition operation as 'all class counts of each teacher' or 'all class counts of all classes' rather than 'class count for one class for each teacher', which is comparable as a unit capacity of each class (as if John and Ali are types & the intent is to get a unit class capacity count for each type), but not comparable as relevant for adding all class capacity values, which is the point of the problem, so inferring the concept of 'all' classes in the inputs and the target solution output would have avoided this error

          - a 'mental model' of the neural network from example 1.4 can be inferred as:
            - having a 'priority' of simplicity leading to an overly simple 'concept' of errors as 'differences', limited to a particular 'example' error structure that it knows how to handle which is 'missing information, where "missing" is determined by inequality'
            - a 'priority' of preserving 'structures' like similarities, without evaluating usefulness
            - a 'priority' of 'equalizing' variables to create similarities, without evaluating whether a difference is useful, such as for a 'comparison' intent
            - having minimal 'understanding' of usefulness, and an 'inability' to imagine other contexts like different problems to solve such as 'compare'
            - an 'inability' to correct its errors, identify the cause of errors, identify other errors that are more high-priority to avoid such as 'removing information about a difference', an error which the neural network creates by trying to fix the 'missing information of the inequality present in "one object's variable not being populated when the other object's variable is populated"' and in trying to fix this definition of 'missing information', it creates a 'missing information' error by removing the information that differentiated the two objects, which is the actual 'missing information' structure the model should have avoided
            - never having encountered the 'experience' of negative feedback for errors in its error-correction method
            - given these priorities, examples, experiences, concepts & other components, this neural network can produce errors when applied to language data that is highly structured for a particular purpose, simply applying any equalizing method it can, without evaluating whether all variables of the problem/solution should be equalized to connect the problem/solution state or if a subset of variables are useful for doing so

      - the error structure of a 'gap in the 'input-output sequence' of a possible function of inputs predicting the output can prevent almost useful sequences from contributing to output, if a node is deactivated to create the gap

    - add to solution filters
      - is it a 'possible' structure:
        - are its inputs possible
        - is a system that could sustain it possible
      - most solutions are better when multiple interfaces are applied, to avoid errors like 'missing a variable bc the current system hadnt made the variable isolatable from randomness yet, but the causal interface indicated a variable would soon emerge', so applying the 'function/system/cause' interfaces would be better than just applying the 'function' interface, as interfaces can fill in the gaps in information if they exist on one interface (such as where 'not all functions are found/generated/derived')

    - representing extra dimensions as 'queries of interfaces' or 'queries on a network' where nodes are endpoints (that can be connected to other points with queries)

    - add to ml definition of a 'solution-finding method' as it finds the solution 'neural network configuration' (prediction function) by a 'sequence of weight changes'

    - examine 'locally same, globally different' as a structure of 'interchangeable alternates' on 'interaction levels'
    
    - standardize primary functions in terms of common functions
      - 'derive' mapping to 'find cause', 'build' mapping to 'find combination', apply mapping to 'find changes', and 'find' mapping to 'find filters'

    - example of possible usefulness of a structure
      - an example structure like '1/2' can be useful for the following structural intents
        - equivalence of two values (two halves), equivalence being a structure of randomness, which when applied to 'interactivity of values' can produce other structures of randomness like 'complex systems with many variables/structures having high interactivity'
        - a default structure of a 'comparison' and 'a randomizing division (1/2 slope line) of an interaction space (like the area created by multiplying x & y)' between a default unit structure '1' and a default unit interactivity structure '2' when applied to dimensions (as an exponent)

  - index particularly useful structures by their interface attributes like 'cause' to find the interface structures associated with particularly useful structures to apply by default when selecting useful structures to apply in a workflow
    - the 'minimal set' of 'maximally different' structures is more useful than the set of 'maximally different' structures on its own
      - find the 'cause' of why some structures of useful structures (like the 'combination' of 'injected' useful structures such as a 'minimal set' of 'maximally different' structures) are more useful than others (like a 'sequence' of 'maximally different' structures) - the minimal set of maximally different structures creates another useful structure like 'archetypes' or 'core structures' or 'interchangeable alternates' bc of the way that 'minimal set' and 'maximally different' are interactive
      - 'minimal' & 'core' and 'maximally different' & 'unique' are components/users/alternates/attributes of each other's definitions

  - add to useful structures
    - core math structures (point, line, path, network, factors) and their counterparts when applied to other interfaces (object, connection, function, system, components) as a way of finding important (as in 'core') structures in another interface

  - neural network optimizations through applying interface structures
      
      - why its important to build neural networks that can derive insights, even if the insights are certain and can be applied as preprocessing functions rather than in the neural network

        - neural networks are for resolving uncertainties to find rules in the problem system, but insights enable this process to be optimized, and the uncertainty of 'finding insights to optimize neural networks' or the uncertainty of 'selecting which insights to find/apply' are likelier to be more valuable uncertainties to resolve than the original problem

        - example insight: different label ratios create a higher likelihood of predicting the more common label bc networks dont have a structure to handle this, like an input-representation-ratio metric included as inputs by a preprocessing script or derived by the network itself, such as through a function that calculates what percent of the dataset contains that label or a structure that emergently calculates the same (like a function that amounts to evaluating if the ratio of a predicted label is too similar to the ratio of that label in the training data set, beyond what the test set implies), (preprocessing functions like a commonness type evaluation function to evaluate what the source of commonness is and whether that source is legitimate to influence the weight changes toward a particular output or a standardizing function that removes various illegitimate sources of commonness), and then adds it as a feature after a layer containing these functions, which would likely discover such associations as false and being attributable to data ratios rather than meaningful connections that should be included in the output prediction function
      
          - this type of insight can be injected into networks by force (designing the network to apply the insight), but it can also be discovered by particular neural network structures (emergent functions), and given that new prediction function-correcting insights will always be in-demand and should be considered part of the responsibility of a neural network, given their potential to discover them if used & built correctly

      - maximizing the number of functions supported by a network (varying constants like weight sign to allow different operations) vs. maximizing the number of emergent/conditional/compression/other function types supported by the network vs. maximizing the number of error types avoided by a network in a variety of data cases vs. covering the functions necessary to cover the functions required to quickly converge to most prediction functions is a useful set of limits to apply when filtering possible networks

      - given that a network learns one representation of a prediction function, it cant learn multiple representations unless it retains 'alternate conditional' neural network nodes/layers/paths that allow other structures to be considered in case a hidden pattern emerges in another data set (like what appears to be a parabola can be produced by multiple polynomial structures) - restricting ml networks to the 'most efficient compression' of a network to represent the found prediction function would remove its learning potential for these possible alternate structures
        - these 'functional similarities' can be encoded in the network if it evaluates which decisions are the result of possible emergent functions in the network that could have 'alternate conditional functions' also explaining that decision, and retains multiple alternate weight sets
      
      - other types of operations a network can support include interface operations
        - ideally all of these operations would be supported by a network, with various structures like 'injecting interface structures' & 'allowing communication with other networks or generation of networks'
        - example: 
          - can a network discover the right variable interface level at which most of the variation occurs (variables like position/angles for facial recognition) & retain that variation
          - does it support interface operations like:
            - merging the high-variation variable interface with the interface containing the specific functions creating that variation or the standard functions of the network (aggregation, pooling)
            - removing interface structures like the 'requirement' structure to identify concepts like 'play' (unrequired activity without a required point/intent like 'survival'), this concept being useful for optimizing resources to exercise/learn when not used for other intents, which is a useful system design insight that networks should integrate
            - identifying useful concepts like the following, and identify the set of concepts that are useful in optimizing the network system:
              - 'sharing' which allow it to coordinate with other networks to optimize various metrics (this concept having requirements in the form of other input concepts like 'other networks' which requires the concept of a 'network' for the network to be able to identify this concept), and apply these concepts to its own structure ('learn a more optimal network structure' such as by including them in the input or by changing its structure/params to consistently create these concepts) to enable learning other concepts
              - 'helping' which allows it to identify extremes such as 'fighting other networks' as sub-optimal and identify when another network is not helping it but rather exploiting it so it can defend itself against exploits/hijacking
          - can it support finding operations like the 'attribute removal' and 'attribute generalization' to identify concepts like 'type'
          - can it support finding useful structures like similar structures across interfaces for intents like:
            - a 'priority' -> 'hierarchy' -> 'tree' -> 'overlapping sequences with fixed end point' -> 'rotation with fixed end point' -> 'circle' -> 'symmetry about the center' -> 'direction' -> 'priority' connection structure for intents like 'find a structure to model a priority or hierarchy'
            - a 'combination into one output' -> 'tree' + 'overlap' -> 'network' + 'select/summarize/filter' -> 'neural network' connection structure for intents like 'find structure to integrate multiple outputs of trees'
          - can it support finding common distortion functions of features like 'corruption around a symmetry'
          - can it support identifying useful structures as particularly powerful structures applicable across neural networks/intents/problems, and finding alternative optimal paths to generate these useful structures like the following paths (which are solution automation workflows), if these concepts/structures like 'useful structures' and 'insights' are injected as inputs or built in to the network structure emergently by creating a network that priorities creating/finding/applying useful structures or by other function sets amounting to the same
            - apply useful structures like input/output sequences to problem-solving structures like problem-solving intent functions
              - find structures like 'similar' that implement/enable or are inputs to core problem-solving intent functions like 'connect', such as finding structures with similarities that may be relevant/useful in 'connecting the problem/solution'
            - find alternate structures of 'usefulness', like structures of 'obviousness', such as structures which make clusters linearly separable or highlight differences by maximizing them, & developing functions to find these structures
            - start by solving a unit case & generalize
              - start by solving for a function to find functions to connect input/output pairs that are connectible with adjacent/simple transforms, and find a function to generalize unit cases (generalize by 'removing specific problem attribute values such as inputs or requirements, like the number of inputs') and apply this function to the first function
            - specify inputs & outputs
              - specify solution outputs to restrict possible outputs: specify a range/area of functions for a network to find a prediction function within, which has a particular error rate across data subsets, without varying on any function subsets that are more certain/calculatable
                - if the outputs are more restricted in this way, the network can identify optimizations for those specific outputs like 'calculate the impact of a de/activation decision and if it contributes significantly to violating the specific solution function error range with other decisions being made in this iteration or likely to be made in future iterations at that point, prevent it'
            - identify alternate function sets
              - identify alternate functions (like a set of useful testing functions, a change function, and a selection function as an alternative function set to the functions of a neural net) to fulfill a problem-solving function like 'connect the problem inputs & the solution output'
            - apply insights from other systems 
              - identify alternate systems (like physics) with the complexity to delegate functions like 'generating new change types' to that system, or alternatively identify a system that is the source of problems solved by neural networks, or alternatively generate a system that could be the source of problems solved by neural networks, in order to use that system info to optimize neural networks & derive intent/meaning/usage of neural networks, derive other networks & coordinate with other networks, and develop self-awareness in that system
            - identify useful structures for core standard intents like 'identification' (such as 'identifying whether something has changed') or core specific neural network required intents like 'identify change' or 'identify change contributed by a unit (node)'
              - identify the concept of a 'derivative' by identifying the following, and identifying variables to compare it to (previous success, time, other input variables)
                - the useful structure of the combination of a 'change' and a 'comparison' to derive a 'change rate' concept by identifying the relative low value of change information when not compared to another type of change ('the meaning of a change, compared to what')
                - the requirement to identify change to test if a weight update improved the solution ('meaning of a change, compared to the previous solution success')
            - apply general solution metrics as filters of possible solution components & further inputs to derive problem structures
              - apply general solution metrics like accuracy (with possible input solution components like 'specific equivalence' with possible inputs like 'equivalent structures')
            - identify rules & inject those rules as prior knowledge: 
              - identify insights & applying insight operations like 'variables usually vary more than a particular subset of the data set' + 'real systems typically have many variables leading to noise' = 'variables should be corrupted to generate a more realistic data set'
            - change problem-solving intent (like 'find structures to prevent errors' rather than 'find solutions')
              - rather than finding/generating solutions & checking if a particular solution has errors, identify possible errors (differences from solutions) & reasons why (causes) an error might occur ('over-prioritization of a priority, like simplicity') and check for & prevent these causal structures of errors by generating counter-structures to correct it without causing other errors 
            - trial & error: 
              - alternatively, apply every possible change to every possible variable to generate every possible variable structure and then select for those which perform better & repeat this process for every task
            - self-awareness/modification or awareness of neural network structures like usage structures (like 'training structures')
              - make one training iteration aware of another by allowing a network to keep track of other training data sets & contexts & outputs it's trained with, to identify the reason training is more successful (difference in data sets making one quicker to converge than another) in order to identify differences to apply (change a data set by adding noise to account for these differences in data sets) and identify possible operations to try that could identify useful structures (like combine the data sets, select subsets, combine training input/output in a sequence of training iterations, etc)
          - can it support identifying useful high-level & meaning-adjacent intents like 'standardize' and 'organize' as particularly useful functions to create and apply across intents/problems
            - identifying that its 'operations are more effective once a standard is applied to inputs' or 'operations like comparison is more effective when inputs are more similar in some ways and more different in others' can be an input to identifying 'standardize' as a useful function
              - alternatively, a function that can identify when structures have the same input/output like 'operations are more effective once a standard is applied to inputs' or 'operations like comparison is more effective when inputs are more similar in some ways and more different in others'
            - identifying the insight that 'some structures are more useful in specific positions' or 'structure adds value when variation is known and randomness adds value in discovering new change structures' can be an input to identifying useful connecting structures like 'networks' and identifying useful implementing functions like 'organize' of those connecting structures, or in identifying useful structures for improving prediction functions rapidly
              - alternatively, identifying that functions which support multiple cross-interface intents like these are inherently more important/useful and should be prioritized & developed by the network
          - can it support other variable operations than aggregation (of small features into big features) such as 'selecting' (between alternates) or 'changing' (creating alternates), or 'breaking the original output into components' (reverse direction of aggregation) or 'generating all feature structures like combinations and filtering them' or 'switching & mixing abstraction & interaction levels'
          - can it support finding contexts where applying structures like 'opposite' (such as by negating a sign of a network structure or executing a filter instead of a combination operation) is useful for quick or accurate convergence, for finding maximally different functions, for finding alternate conditional functions, for handling errors of 'falsehood' where the trend in a particular direction is an error to correct by negating it, or other intents relevant to 'finding a prediction function' that could be fulfilled by neural networks if informed by interface structures
          - in addition to interface structures like interaction levels of related objects in a network, does it support other useful structures like opposite structures (input variables & output requirements, problem/solution, difference/similarity, component/whole, generate/filter) which provide a network of useful boundaries to base solutions within, as few extremes/absolutes apply in real systems (except in for example cases like booleans) and therefore these opposites act like bounds
          - can it support finding tasks that would capture the functions of or generalize many tasks, like abstract tasks or other interface tasks, or standard problem format tasks like 'sorting', 'filtering', 'building', 'fitting/matching', 'connecting', 'combining'
            - meaning 'would a network trained specifically to perform a particular interface operation be better at tasks in general than a standard neural network'
            - is there a task or set of tasks that generalizes most tasks better than other tasks, and which is it (is there a set of interface operations which, when supported by a network, can solve most 'find a prediction function' problems)
            - is this the reason networks that learn functions like compress/expand (encode/decode) or translate perform better than standard neural networks, because these functions are similar to interface functions (like 'standardize to a (network)/interface') or core functions like 'change a (language) network into another network'
          - can it support applying other solution automation workflows like 'reverse engineering a solution' such as by first 'finding out what structures could influence the target structure, then checking if any of the inputs are those influential structures' (a 'function output' or 'function intent'-based interface query which first generates candidates for solutions using solution requirements before checking if each input is one of these candidates), rather than a standard workflow such as 'check if each input influences the target structure' (which iterates through all possible inputs and checks each one for equivalence to a solution)
          - whether it can identify structures of optimization such as structure to optimize for the number of data points predicted
            - example: instead of predicting one particular data point in a set of adjacent or otherwise related data points (adjacently converted into each other using minimal probable available operations in the input problem space), try to predict that data point earlier in the network and then apply distortions to it to optimize predicting data points generated by distortions of that data point, to optimize predicting more data points accurately than to predict each data point accurately
          - whether the network can infer the input problem space, such as the network of variables, the system in which this network occurs, the functions that take some subset of the variables as inputs or creates subsets of variables, etc
          - whether the network can identify opportunities for optimization like:
            - developing a 'function to identify adjacency/groups and predict one point from an adjacent point or a point in a similar group' rather than 'finding prediction function coefficients of input features', which may be useful for local optimizations as opposed to the standard function developed by a neural net
            - identifying when a subset function is better than a combination function to identify features, such as when a 'surrounding structure like a container' is relevant to identifying a medical problem like 'structural damage', to identify that the 'subset of the damage' that has a similarity to the surrounding structure is the important place to use a subset when looking for interface structures like similarity that can indicate relevance, to identify other relevant features like where the damage can not be and where the damage may hit next
            - identifying 'attachable sensory/function simulation networks' as a more effective/efficient way to gain feedback than 'labeled data with supervised learning' and creating & attaching these networks to avoid over-focusing on a priority/metric/structure/task and identifying the tasks these sensory/function simulation networks should perform in order to gain the most useful feedback (for example identifying that a 'robotic arm used to put blocks in a pattern' is a useful system & task to create a 'sensory vision network' and a 'functional arm network' for in order to optimize a network used to 'rank features in a particular sequence' (like a sequence of blocks))
          - whether the network can identify sub-optimal structures, like whether a neural net that aggregates features is only useful for image recognition bc adjacent features in an image tend to combine to create larger features in reality, so by accident, the aggregation function aligns with this real function applied to features in reality
          - whether the network can identify structures to counter these sub-optimal structures (correct sub-optimal structures rather than purely optimize structures) such as phase shifts/thresholds between features that should not be combined with adjacent features (detecting the limit/boundary of feature groups that dont always appear adjacently in real life)
            - example: identifying these boundaries would be a clear counter-structure to the structure of combining adjacent features which would be sub-optimal in situations where the adjacent features dont always appear together, which is a possibility the network should be able to generate & create a counter-structure for
          - whether the network can identify insights about its own structure like 'a network is insufficient to describe its own interactions with other networks'
            - 'inputs' like injecting interface structures like concepts like 'networks' into a neural net along with the data set input features might produce this functionality
            - 'network structures' like 'giving every node the ability to connect with every other node', 'conducting regular global communications', 'abstracting features', 'allowing the network to generate other networks' could also amount to the same functionality

      - how to translate net/emergent operations of a network like 'aggregating small into big features' into variants of those operations like 'opposite of aggregating small into big features (decompose big into small features)' 
          - meaning, how to translate 'reverse' or 'opposite' of a network's emergent/net/explicit function ('aggregate') into a network configuration, in other words, how to implement 'semantic relevant automatic generation of neural networks'
            - such as by keeping the first 'combine input features' layers to create as many of the big features as possible, but then splitting & filtering component features after that, to align the input/output data types by adding an opposite operation of 'aggregate' (decompose) once the input (big features) is created
            - for intents like 'implementing a neural net for each basic core structural operation like combine/aggregate, find/filter, decompose, connect, reduce' by auto-generating each neural net config for these operations with interface queries, or intents like 'changing the position of features or filters emerging from a network to maximize filtering or optimize another function like aggregation, such as by changing feature position to maximize explainability coverage of earlier features similar to decision trees'
          - interface query to implement this 'find opposite function' operation
            - find opposite of function 'aggregate small features into big features'
              - 'decompose big features into small features'
            - find requirements of opposite function
              - 'big features' are input requirements
                - create input of target function 'opposite of aggregate small features into big features' ('decompose big features into small features')
                  - check for existing structures fulfilling requirement (first aggregation layers)
                    - keep first aggregation layers creating big features
              - 'small features' are output requirements
                - convert big feature inputs to small feature outputs
                  - add decompose operation
                    - add feature split/filter operations to create small features

    - write example of code as a set of parallel sequences

  - add to solution automation workflows

    - identify problem/solution attributes like 'difficulty' that are especially useful in determining probable related structures like 'solution structures' of solutions related to that problem as these attributes are particularly useful through being particularly differentiating of relevant structures, such as structures of understanding
      - example: identify common structures of 'hard problems', like where structures of understanding (structures like 'interfaces', 'interaction levels', 'important concepts', or 'sub-systems') are missing, making it falsely appear difficult
        - for example, the p=np problem falsely appears difficult because people arent working with the concepts or interaction levels that would make the problem trivial to solve (for example, the classification of problems is incomplete, meaning the interaction level of problem types relevant to that problem is incomplete, and the mapping between problem type & absolute computation complexity (meaning calculated across all equivalently optimal computation methods) is similarly incomplete or a method to infer these mappings logically is not available
        - therefore, given that we know that the structures of understanding we have are inadequate to solve that problem, these known structures must be different from these concepts or interaction levels, so applying differences to these known concepts or interaction levels would likely generate the structures of understanding necessary to solve that problem
        - if its not true that the structures of understanding we have are inadequate to solve this problem, then all possible combinations of these known structures of understanding have not been applied, and solution automation workflows can be applied to these known structures to test each combination or filter all combinations for the more probably successful combinations using some workflow that filters solutions or some other workflow more efficient & effective than trial & error
        - for example, concepts like 'recursion', 'self-reference', 'completeness', 'closed system', 'ratio of required computation to computability', and other concepts relevant to this problems may be inadequate to understand the problem
          - other concepts can be generated to combine with these known concepts in new ways
          - given the definition routes of these concepts:
            - recursion: function self-reference
            - self-reference: function to differentiate internal from external structures (identify a boundary between internal/external structures)
            - completeness: fulfillment of a set definition
            - closed system: system that does not interact with other systems
            - ratio of required computation to computability: physical resource limitations
          - they can be combined or combined with other concepts to generate new concepts:
            - an existing combination is the combination of 'infinite' with the concept of 'recursion' to create an 'infinite recursion' error
            - a combination of these known concepts would be an 'incomplete closed system' which eventually degrades into chaos as it cannot sustain its independence from other systems
            - a new conceptual combination might be a 'universe' combined with a 'ratio of required computation to computability' to create the concepts of 'computing a plausible universe where this computation is not required', or a 'universe-simulation machine to compute this required computation' or 'high variation or highly chaotic systems (like black holes and quantum particles) where this computation is likelier to be computable' or the 'limits of entropy reduction in creating a machine capable of such organization as that which would be able to compute the computation, applying "structures of organization" as an alternative metric to complexity when evaluating computability of a computation' (can a machine be created that is capable of such organization that the computation is more computable)

      - differentiation: this workflow finds attributes that are particularly useful for the intent of 'differentiation of usefulness of structures' which is useful for problem-solving intents like 'finding useful structures'

    - identify useful structures like 'states adjacent to solutions which include info that makes calculating the remaining info trivial' that are useful alternative problem/solution structures like 'proxy solution targets' to replace those problem/solution structures in other problem/solution structures like solution automation workflows/interface queries in order to fulfill the problem-solving intent of 'generate new solution automation workflows' or 'generate new solutinos'
      - example: proximity to solved problems or states which are adjacent to solutions (states like 'remaining information is adjacently determinable from existing information')
      - differentiation: this workflow involves finding alternative structures (like 'proxies' and 'solution-determined states') to replace other structures in solution automation workflows to generate new solution automation workflows or other problem/solution structures

    - identify identifying/determining/generative variables of useful structures (like opposites, spectrums, networks) of useful structures that are found to be common across problem-solving structures to fulfill problem-solving intents like 'find/derive/generate new solution automation workflows' and 'finding useful structures to adjacently fulfill problem-solving intents'
      - example: 
        - dichotomies/opposites like the dichotomy between 'variables/requirements' and 'generative/limiting functions' are particularly useful in problem-solving
        - networks like 'equivalent alternate interchangeable functions' or 'functions forming interaction layers' are particularly useful in problem-solving
        - interfaces like 'structures that can support high variation' are particularly useful in problem-solving
      - the interface variables of these structures are:
        - corresponding math structures of each structure (an 'opposite' structure might have a corresponding math structure of a 'decision tree fork' if they are mutually exclusive or a 'spectrum' if its a continuum)
        - structural variables
          - an 'opposite' structure can vary in terms of its exclusivity, continuity & other variables
        - other interface variables
          - a structure can vary in terms of its usefulness (composability, coordination with other structures, uniqueness, commonness of usage intents for which the structure is optimal)
        - what interface structures can be used to construct them (a 'network' is a 'set of connection pairs', an 'interface' is a 'change-supporting structure')
        - what intents theyre adjacently useful for (an interface is adjacently useful for 'compare intents', a 'network' is useful for 'finding connecting paths' and 'finding similarities/differences between network nodes')
        - what structures they can adjacently be converted into (an interface can be adjacently converted into a 'network of networks')
      - this workflow finds interface structures ('variables') fulfilling useful intents ('identify', 'generate') of interface structures (useful structural interface structures like 'opposites') applied to problem/solution structures ('variables/requirements') in order to fulfill problem-solving intents like 'find/derive/generate new solution automation workflows', and applies optimized variable values as filters (such as favoring core structures) to find probably useful structures
      - this workflow fulfills the problem-solving intent of 'finding useful structures to adjacently fulfill problem-solving intents' (like 'variables/requirements') to fulfill other problem-solving intents (like 'connect problem/solution') which those useful structures can adjacently fulfill

    - find/derive/apply limiting structures like 'requirements of problem/solution structures' to fulfill problem-solving intents like 'filter structures that can fulfill other problem-solving intents' (problem-solving intents like 'connect problem/solution')
      - example: 
        - find useful structures like core functions that can fulfill common problem-solving intents (like 'connect', 'reduce', 'break' applied to problem/solution structures) as initial/core components of solutions in the problem system to filter the set of possible components that can be used in solutions, as functions that can be combined to fulfill these common problem-solving intents are likelier to be legitimate/probable/realistic than other functions, as these intents are known to be fulfillable with any problem, so a solution is likely to adjacently use functions that can be combined to fulfill these common problem-solving intents
          - meaning: any solution whose core functions cant be used to build a solution using another solution automation workflow is unlikely to be an optimal solution bc its unrealistic to find a system where one workflow can be applied and another cant
        - find input/output sequences of problem-solving intents like 'find useful structures for problem-solving intents' to fulfill problem-solving intents like 'generate new solution automation workflows' as a way of fulfilling problem-solving intents like 'generating new solution automation workflows', since 'input-output sequences' of problem-solving intents are a requirement of that particular problem/solution structure
      - this is related to applying interface structures to other interfaces, applying the requirement that if a structure is actually compliant with the definition of an interface, it will have a corresponding structure as that found on other interfaces
        - the difference is that the requirement is being applied to interchangeables like functions that can fulfill problem-solving intents, rather than interchangeables like interfaces
      - workflow differences: this workflow uses useful functions like 'applying similar structures' ('limiting structures' to fulfill similar intents as limiting such as 'filtering', given the structural similarity between limits/filters), applying it to problem-solving structures like 'interchangeable problem-solving intent functions'

    - identify probably useful structures (like common structures) of problem-solving structures (like solutions or solution automation workflows) that can be found/derived/generated first given their probable usefulness for other problem-solving structures
      - example: the following workflows to solve a problem like 'find a prediction function' would have some structures in common, such as a 'function that is changed until its the solution function', 'solution components', and 'variables', and the same can be said of other problem-solving structures like solutions or workflow implementations
        - identify variables of problem space and change until solution function is found
        - identify previous solutions to similar problems and change previous solutions until original problem is solved
        - identify requirements of solution and identify structures that can fulfill those requirements as possible solution components, combining them until a solution is found
        - identify structures that can be combined like interactive, connected, & probable structures and apply them in combinations as solution components until a solution is found
        - identify variables that solve subsets of the problem or sub-problems and combine those variables in the way that those subsets or sub-problems interact
      - solving for the common structures (the relevant variables or a filtered list of variables to test, the solution components, and a base function that can be adjacently changed to create the solution function are useful structures to find/derive/build first, after which other workflows can be applied to filter those structures
      - related workflows:
        - these workflows can be applied simultaneously and can inform the other workflows (when a 'trial & error' workflow identifies a sub-optimal solution, that can be filtered out in other workflows being applied simultaneously)
        - this set of coordinating workflows can be derived, as workflows that are probably capable of producing information that would assist the other workflows in the set in deriving information

    - identify structures that can support variation (like solution automation workflows & their implementations) without degrading their structure as initial prioritized targets for variation (like workflow implementation variables) to apply when generating new problem-solving structures from a base/origin problem-solving structure (like solution automation workflows & their implementations), as indicated in the next solution automation workflow
      - identify that solution automation workflows can support high variation without violating their definition, so applying changes to these workflows is likely to produce other valid workflows
      - example: 'break a problem into sub-problems & merge sub-solutions' is a workflow that can act like an interface bc it can be implemented in many different ways without violating its definition
        - variables of this workflow's implementation include:
          - the sequence of sub-problems, the sub-problems themselves, the breaking/merging functions, the sub-problem solving functions
        - identifying variables of a workflow's implementation are a useful step in filtering the implementations as solutions to the 'select a workflow implementation' problem-solving intent which is solved by the interface query
        - this workflow includes the step of 'identifying & applying variables of a workflow's implementation to generate solutions to the "select a workflow implementation" problem-solving intent & then filtering those implementations to design the interface query that implements the implementation of the workflow'
        - given these variables, the workflow itself can be adjusted, not just its implementation
          - because some sub-problems may be more important to solve than others, it's possible that 'selecting one sub-problem to start when solving sub-problems' or 'selecting one sub-problem to solve to solve the original problem' are alternate workflows, if enough variation is applied to make these steps similarly useful as the original step
        - to make these steps similarly useful as the original step, applied functions/structures need to have a reason why theyre applied (a reason why applying them would be useful, such as a reason why it would make the step similar to the original step)
        - example: 'abstracting the sub-problem' would make the sub-problem more useful bc a general solution is more potentially useful for 'solving other/related problems' and for 'deriving abstract insights', both of which are useful problem-solving intents that may be fulfilled by 'abstracting the sub-problem'
        - applying such functions as 'abstracting the sub-problem' might make the new step 'solve a particularly important sub-problem' similarly useful as the original step 'solve all sub-problems'

    - identify structures of problem/solutions (like a sub-problem) that can be used in place of other structures (like all sub-problems) if enough variation is applied to make it similarly useful for a particular solution automation workflow using the original structure (like 'break problem into sub-problems') to generate other solution automation workflows (like 'identify & solve a particularly important sub-problem to make the other sub-problems trivial/solved/irrelevant')
      - examples of functions/variables to apply to the sub-problem to make it similarly useful as all sub-problems:
        - abstracting the sub-problem
        - identifying the most important sub-problem, such as an input sub-problem required to solve other sub-problems or an otherwise particularly required/causative/influential/enabling sub-problem
        - identifying the sub-problem type to identify the types that other sub-problems are less likely to be, as if theyre different from the other sub-problem, theyre likelier to differ in attributes like problem type as well
      - generative function of this workflow: apply 'change' structures to the original solution automation workflow ('break a problem into subproblems & merge sub-solutions')

    - identify variables of structures of state changes between problem/solution which form solution automation workflows that are useful such as a 'linear connection' (find interim states to connect problem input & solution output) and 'angular connection' (find alternate system to convert to & back from) and 'parallel connection' (convert both problem/solution to different alternate system and solve problem there) and 'extended linear connection' (find cause of problem inputs or cause of solution outputs) and 'connective lines' (find solution components or find interactive components) or 'filtering solutions by reducing items in a set to a single output' (joining sequences with a fixed endpoint & traversing/reducing items, like a tree) or 'injecting different/additional inputs/outputs or variables' (increasing/reducing the problem/solution dimensions) and apply these variables to generate new solution automation workflows

    - identify useful structures (like connection between structures) to fulfill intents like 'find missing information about a full structure' which combine problem-solving intents like 'find missing information' and specific problem contexts 'the solution structure is the remaining structure to find the full structure' in a combination ('find missing information about a full structure') to use these useful structures to fulfill those mixed/merged/combined intents
      - example: identify sub-structures (an arc) that can imply another structure (a circle) as a way of connecting structures that are particularly useful to connect, for cases where info about the full structure is missing, to fulfill problem-solving intents like 'find missing information' if the problem is formatted as a 'missing information' problem format, specifically for the format where the 'missing information' is the remaining structure of the full structure
      - a generalization of this is to find these structural connections that are particularly useful, and find the versions of them in different formats (like when finding 'information about a full structure', in which case information about connections between partial/full structures is particularly useful)

    - identify structures that can be used as filters of solution structures like solution components, based on insights connecting relevant structures like solutions/errors ('solutions should be as different as possible from errors')
      - example: an 'optimal/good' solution function range can be built with a set of functions/variables, but if one of those functions/variables can convert a solution in that range to a bad solution with minimal effort (as in a probable change or adjacent/linear combination), those functions/variables are less likely to be the correct set, bc a system that is observable (such as the system described by the variables) is usually more stable & therefore consistent over time & robust to change

    - identify structures ('function sets') with the same useful metrics ('differences producible with that structure') that can be used to connect useful structures like 'different abstract layers of a problem-solving intent implementation' or 'connect inputs/outputs'
      - example: identify function sets that produce the same difference types/degrees/levels/structures and use these as default alternate abstraction levels of an implementation of a problem-solving intent (the different layers & sub-queries of an interface query)

    - identify functions like 'combine' that fulfill core structural insights like 'small components can usually be combined to build larger components' (with variants like 'small components can be combined to build larger components, once specific functions are applied at various points in between applications of the combination function') to determine how input/output formats can be connected and what structures the inputs/outputs can be
      - example: a 'function network of function networks' can be used to represent a 'two-step combination of components', where the components are formatted as inputs to the second-degree functions in the 'function network of function networks', and the outputs are the outputs of the largest interaction layer (the first-degree 'function network'), which is how smaller components can be combined to build larger components
        - another example is where multiple sets are the input and a single object is the output, either through a 'merge' function (like 'average') or a 'select' function (like 'test/filter'), which fulfills core structural insights like 'multiple objects can be merged into one object through functions like averaging functions' or 'a set of multiple objects can be filtered to a set of one object'
        - these functions ('combine', 'merge', 'select') also determine what format the inputs/outputs can be (inputs to functions in the function network, objects in a set, a set of similar sets), so if the problem definition states what format the input problem & output solution are, a default structure generated by these core structural insights can be derived to connect the problem input & solution output or an adjacent transform of them to use these generated structures, depending on the likelihood of their applicability to the problem and what structures are available (whether functions that can be used in the function network are known or should be populated with standard/core/common functions)
      - these structures generated by these core structural insights can be modified according to the problem, such as by adding more layers to the function network if the inputs are very different from the outputs (and therefore unlikely to be fulfilled by the default function network), adding other networks, etc, where a default structure is insufficient, either iteratively using adjacent transforms or by running multiple parallel processes to evaluate maximally different versions of the default structure such as an example of each common solution type (common types of multi-layer function networks)
      - matching the difference between the problem/solution with the difference allowed by the structures connecting them is an example of applying the intent interface, to find differentiating structures to fulfill an intent like 'differentiate the problem/solution with these difference types/degrees/levels' (a 'function network' being a structure that fulfills specific intents such as 'combine inputs with these functions to differentiate with the same degree of difference as the known inputs/outputs' in fulfillment of general intents like 'differentiate inputs/outputs using available functions')

    - identify relevant structures of a structure to fulfill useful common intents like 'identifying whether a structure is complete' or 'identifying structures that change another structure' and 'formatting a structure in its complete form' which are adjacently useful for problem-solving intents like 'identify missing information' and 'identify inputs/cause' and 'identify the fit of a structure in a system context'
      - example: a network has relevant structures like nodes/functions, network states, nodes represented as functions or functions represented as structures of common core functions ('common core' meaning on a different interaction level that can generate the original function set), structures like combinations of nodes or common structures, and queries on the network as well as the associated input/output pairs & input/output differences associated with those queries, without which its definition is incomplete, not having the 'intent' or 'meaning' interface fully applied to its core structure of the network
        - similarly, a tree has relevant structures like variance injection points, where a logical tree representing a function like a script can be interrupted by another logical tree like a process manager function or by a logical tree representing electricity or system usage/wear, so representing functions like a logic tree in isolation of other relevant structures is likely to cause misinterpretation of that function

    - identify info adjacencies by mapping interface structures to other interface structures which can be mapped to useful structures like error structures as a filter to find changes to apply to a suboptimal solution
      - example: derive alternate complexity structures (like a term with a zero coefficient in some cases and a non-zero coefficient in others, which could be created by structures like neutralizing/antagonistic structures) to explain errors associated with those structures (like mismatch errors)

    - identify change structures like change potentials and change interaction potentials to filter set of solution components like functions/variables, to filter inputs by rules like 'whether the components are capable of producing the change types necessary to generate the outputs in a way that fulfills known common/probable system dynamics/intents like how more stable systems tend to be simpler with robust randomness handlers'
      - example: derive the possible changes ('increase', 'weight', 'reverse', 'rotate') that a possible structure (a possible/probable input function/variable, like pixels or pixel groups or pixel difference groups) can create across various systems (like 'changes in similarity to adjacent pixels'), and whether those systems or changes are reflected in other structures (could other variables be explained by that increase/weight/rotation/reversal, are there anti-structures invalidating its changes), to determine the probability of a particular possible structure (function/variable) as a component of the solution, weighted by the likelihood of the systems/components that allow it to be a component of the solution

    - apply useful structures like interface structures ('cause') of useful structures ('difference') for problem-solving intents that are fulfilled by those useful structures ('apply differences to generate alternate inputs' to 'get more input data')
      - example: apply 'causes of difference' to generate alternate inputs that are probable to use as additional inputs (such as 'how a particular size of a component may produce a more smooth surface as it may interact with more components bc there are more components at that size it is likely to encounter' which applies the 'cause of difference' of an 'interaction level')
      - this is similar to the 'apply error distortions to generate alternate inputs given various probable reasons for errors (like randomness or data corruption error or sampling error)' workflow, but rather than applying differences according to error types, apply differences according to probable causes of differences to generate alternate inputs

    - apply useful structures like differences/similarities of problem-solving objects like problems/solutions to generate relevant structures like probable inputs/outputs given either inputs or outputs to fulfill problem-solving intents like 'derive the inputs/outputs that are missing'
      - example: if a function is supposed to drastically change the inputs (a high input/output difference degree), generate maximally different outputs according to the inputs and use these maximally different outputs as a default solution space to filter, (if the function is supposed to connect the inputs/outputs using as few changes as possible, meaning the inputs/outputs are supposed to be similar, do the opposite by generating outputs that are very similar to the inputs, and use these outputs as the default solution space to filter)

    - apply 'implied prevention structures' derived from probable solution components as filters of possible remaining solution components and weight these remaining inferred solution components according to the certainty of the probable components
      - example: if a subset of a function is probably a particular line and another subset is probably another particular line, and they form an implication of the missing subset between them (like how '\ /' implies '\_/'), these structures act like a 'prevention' structure that filters out the implied _ section at a specific probability, which when combined with other probable solution structures & the structures they prevent/filter out, can be used to infer probable remaining structures, especially where probable remaining structures are repeated across many implied structures by probable solution structures, to identify which structures are the most prevented, most implied, most required, and/or most allowed as a determinant of the most 'probable' solution components, to use as a solution filter based on probability

    - apply useful structures like 'approximate' and 'range' which combine to form other useful structures like 'probability' to existing solutions like 'averaging existing solutions' to fulfill generated associated problem-solving intents like 'find a range of probable solutions rather than one optimal solutions' (which itself is generated by applying a change 'multiple' to the 'solution' structure and a change 'probable' to the 'optimal solution' structure to standard problem-solving intents like 'find an optimal solution in a solution space')
      - example: values for a predicted output variable that overlap with the most ranges of solutions within a range of solutions having a particular accuracy rate
        - if the set of solution functions of solution type x within accuracy range 10% overlaps in a particular area with the set of solution functions of solution type y within accuracy range 10%, that area should be prioritized as the actual solution area (where individual solutions in the solution area are solutions in the set described by that area)
      - this also involves applying useful structures (like 'probability') to generate new problem-solving intents that can be fulfilled by applying relevant associated structures ('range', 'approximate') to the problem space

    - identify useful structures like 'information adjacencies' in solution automation workflows ('sequences of steps') that can capture variation that is unlikely to be captured in a standard solution workflow or 'step sequence' given available info which may be handled by alternate functions better than the original workflow function
      - example: 'predict pieces of the solution' and then 'predict the solution from those pieces' instead of 'predict the solution' bc the inputs are unlikely to contain all of the info required to generate the exact solution, but either input of the two sub-functions is likely to be able to be used to generate/predict the output of that function, compared to the input/output information difference of the original function

    - identify useful structures like 'sequences' of 'information adjacencies' based on structures like 'limits' & 'filters' and concepts like 'completeness' which produce useful structures like 'requirements' and use to derive information required for a solution
      - adjacencies include:
        - useful structures like a "combination of an information structures (like insights) + randomness" to account for possible changes to that known structure, where the insight is the adjacent information needed to calculate the target information of the actual version of the insight in a particular system, since another insight is that 'few insights are absolute and they often have variants depending on context'
        - a limit that creates a requirement for the complementary/remaining structures (like one-hot encoding or the last remaining piece of an almost complete puzzle), where the complementary structure is the adjacent information needed to calculate the target information
          - structures like a complete set can be used to infer or guess the remaining items in a set, similar to how sequential patterns can be used to guess missing items in the sequence
        - an object that differs is likely to be a source of variables, and a set of variables are likely to be a source of solutions in a system, so identifying objects that differ is the adjacent information needed to calculate the variables, which are the adjacent information needed to calculate the solutions in a system
        - structures that create adjacencies include interfaces, so identifying the interfaces in a system makes other calculations more adjacent, so the interface acts like an information adjacency, where a sequence of interfaces can act like a sequential information adjacency leading to the target information
      - solution success cause: this works bc it applies the insight that 'all information is connected', so iteratively calculating sequences of adjacent information can lead to the target information or at least the requirements of the target information that can define it further than its original description in the problem statement, similar to calculating functions earlier/later in a sequence to determine the missing function in the sequence

    - identify any differences that could be errors (such as ambiguities or missing information) that can be resolved without violating a solution metric & apply difference-resolution methods to resolve those differences, merging these methods & their results as needed to fulfill such optimization as 'preserve the original system, avoiding excessive changes where possible'  and/or fulfilling problem-solving intents like 'avoid violating a solution metric'
      - example: often an ambiguity may be resolved by applying filters or functions that leave one possibility, creating a requirement in that one possibility, and invaliding further application of functions to resolve the ambiguity as the requirement of the one possibility has been determined already
      - this applies a general workflow of 'identifying structures that any problem can be formatted as, such as differences which are a core error structure' and specifically applies the value of structures likely to already exist or be adjacently derivable in relation to those structures such as difference-resolution methods like 'connecting a problem state with a solution state'

    - apply requirements to solution-finding methods (including interface queries, solution automation workflows, specific solution-finding methods, etc) by finding/deriving/building structures of the solution-finding methods
      - example: for a problem that can be identified as having a high probability of a complex solution given the simple solutions filtered out (by another workflow/query or by existing failed solution data), apply solution-finding methods that are complex and/or capable of storing/creating complex structures which are likelier to be relevant to the actual structures of the solution

    - apply useful structures like 'changes' to find/build/derive useful structures like 'interaction rules' of meaningful structures like 'definitions' which are meaningful in providing structures like 'limits/requirements/interfaces' to apply changes to, to fulfill problem-solving intents like 'apply changes to a standard solution to improve the solution'
      - example: apply 'randomness' to generate functions that are random (rather than meaningful) as a way of generating 'not solution' structures, given that 'randomness' is an opposite structure of 'meaning'
        - similarly 'organization' structures (like 'sequences' which by definition have inherent order) can be a structure useful for generating solutions, given that solutions are by definition not random
          - a solution to the 'find a prediction function' problem cant be a random function of inputs, so it will inevitably be different from 'random functions of inputs'
          - in the case where the 'random function of inputs' has an overlap with the 'actual prediction function', it is bc random structures can by definition create every possible structure (possible solution), not bc it magically found the solution, as a random function has no way of differentiating significant variables/coefficients and predicting whether it is likely to have found a good solution before testing it
        - applying random structures to 'interim structures' like sub-optimal solutions, solution components/requirements, & available/adjacent functions is likelier to produce a solution than applying randomness to the original inputs bc these structures are by definition nearer to the solution than the original inputs and 'random changes' are likelier to be useful in randomly generating the solution itself
        - this means a solution structure should be composed primarily of structures for a particular reason (like that they are core structures or structures which are likely to interact) rather than for no reason (like randomness)
        - randomness can be applied to sub-optimal solutions once the primary meaning structures have been applied, as a way to temper error structures like 'lack of information' or 'incorrect input information'
      - this workflow involves applying definitions to find the correct position to apply a structure in (apply randomness to adjust a standard solution to handle error structures, after applying organization structures to create a standard solution)
        - it also applies the 'differences in variable structures' that can create a 'difference in solution/error structures' (structures of volatility in crossing the threshold between the definitions, structures of robustness in finding interchangeable alternate variable subsets maintaining a state, etc)
      - simplification: this workflow identifies differences in useful structures like definitions (specifically interface structure definitions, like the definition of meaning) that can be used to infer structures from other structures
      - generalization: this uses the variables/functions of definitions to infer rules that can be applied to fulfill problem-solving intents related to definitions, like 'finding the correct position to apply a structure like a definition', using insights about definitions like that their structure provides 'organization' structures which can be used as a foundation/limit to apply changes to, similar to how a standard solution structure can be used to apply differences to in order to improve the solution

    - apply the structure of the 'inject' function as a way to find/build/derive a solution, as a core (explicit structural) interaction function (like 'connect', 'reduce', 'combine', 'change' or 'filter')
      - example: 
        - a 'neural network' structure is a 'sequence of changes applied to (a sequence of changes applied to inputs to connect inputs/output vars) to connect input standard solution to output improved solution'
          - where 'sequences' are a standard structure used to create 'connections', so 'sequences' are by definition relevant for solving a 'connect' problem (like 'connect inputs/outputs' or 'connect states')
            - where 'sequences of changes' are used to create 'connections' between specifically 'different' structures
        - injecting iterative structures of 'random mappings' is a way to add 'randomness' to a structure
        - injecting a 'query for a solution' in place of a 'solution' is a way to add 'variation' to a solution structure (like an interface query with sub-queries to 'find info' or 'find a sub-solution to a sub-problem')
        - injecting 'variation' in structures like functions/inputs adds 'generalization' to an output structure
      - this 'inject' function works bc 'injecting' a useful structure like a 'variable' adds a structure that can handle more complexity, making a solution more robust
      - generalization: the general version of this is to find/build/derive functions that can connect various formats (like 'injected sequences') to fulfill core interaction functions (like 'connect') in problem-solving intents (like 'connect problem-solution') of a solution-finding method (like a 'neural network') for a problem format (like 'find a prediction function'), and use those functions as alternative problem-solving functions

    - apply useful structures like 'solution automation workflows' or interface structures like 'variables' having outputs in common to create interchangeable useful structures like 'new/different solution automation workflows' or 'changes to solution automation workflows' to fulfill the problem-solving intent of 'improving an existing solution structure'

      - examples of solution automation workflows that may have similar output solutions:

        - changing the definition of a solution as a 'structure without certain/definitive error structures' 
          - example: a function that does not produce outputs in the 'known error ranges/areas'

        - applying complementary structures like errors/solutions to create requirements of what is not a solution
          - example: known error structures can be used to determine what is not a solution and what is possibly a solution

      - examples of solution automation workflows with high probability of different output solutions:

        - applying contextual structures like 'an error, in the case where the function has this many other errors'
          - example: some functions would only count as sub-optimal if they produce errors above a certain threshold, so an individual error is only an error if other errors also exist

        - applying solution structures to filter solution space 
          - example: 'incremental changes' are part of the definition of a standard prediction function for a data set, so a solution should have 'incremental changes' (ruling out step functions, hyperbolic functions, etc)

        - applying differences to structures that can be used to derive solution structures like solutions or solution automation workflows
          - example: if a subset of the data set can be used to derive the prediction function, apply differences to the subset to find out the limits of what other subsets can be used to adjacently derive the prediction function (like a certain level of difference & pattern-compliance across subsets that can be used to derive it)

      - the 'different-output' solution automation workflows have differences in standard structures like different functions/inputs/outputs
      - the 'similar-output' solution automation workflows have similarities in functions/inputs/outputs
      - if a solution is sub-optimal like a standard solution may be, applying differences to the functions/inputs/outputs of the solution automation workflow may create a more optimal solution automation workflow to apply, once the set & interface of solution automation workflows that can generate the standard solution is derived/found/built so changes can be applied to it
        - related requirements filter which similarities/differences are meaningful
          - requirements like 'identify in what position a definition should be applied'
            - when a difference is an 'error structure' or a 'sub-optimality structure', given that a difference can be either (errors dont have to be in very different positions, they can be adjacent to solutions), as a 'difference' isnt equal to an 'error' by definition, so it can only conditionally be applied to generate/find/derive errors
      
      - different workflows can produce the same solution if they are similar in their error structures or solution metrics like accuracy, even with very different workflow functions/inputs/outputs
        - applying workflows to workflows, like the following is a way to derive more optimal workflows:
          - 'starting from the solution metric of a workflow like accuracy & reverse-engineering the workflow applied to create that output'
          - 'applying variables to sub-optimal workflows to create more optimal workflows'

        - this is bc there are 'ranges/areas of optimality' in the solution 'workflows' to the problem of 'find an optimal solution automation workflow' just like there are very different solution 'prediction functions' having the same 'solution metric value' of the same 'level of error' for the problem of 'find a prediction function'
          - there are workflows that are similar in some variables (functions) and different in others (inputs/outputs), where the similarities may be meaningful (related in cause) or meaningless (random coincidence or common/required structure)
          - these similarities/differences can identify 'volatility' or 'threshold' points where the outputs start to vary disproportionately relative to the outputs of adjacent inputs
          - at what point does a 'cause' become a structure of meaning (direct, unique, functional, unambiguous, variability, limited alternatives, intent, usage) or a structure of randomness (core, common, required, constant, ambiguous, or complex as in 'not understood')
            - what structures can change one or more of these variables across a threshold (solution structures), to overcome alternative structures of those variables (other structures that keep it a solution as opposed to crossing a threshold)
            - this is a question of the meaning of a 'definition' of complementary opposing structures (like solutions/errors)
              - structural distance: what differences in components/shapes/connections/inputs/outputs do the solution/errors have
              - change distance ('conversion potential'): how many variables of a solution need to be changed to change it to an error & to what degree
              - solution definition variables: how flexible is the solution metric, allowing what variables to be changed while still qualifying as a solution
              - interchangeable alternates: what variable subsets can replace other variable subsets (how robust is a solution to being changed into an error)
              - determining capacity (identification requirements): what ratio of variable values is required to determine if a structure is a solution/error (required identifying attributes)
                - ambiguities: are there overlaps or unresolvable structures of neutrality in solution/error structures
              - generative capacity: can a variable subset find/derive/generate the other variables, and what variable subsets can be either solutions/errors
                - cause: 
                  - what similarities in cause do the solutions/errors have, given that they exist in the same system
                  - what differences can by definition cause a solution/error even in isolation of a ratio/type/degree of other differences (powerful/robust solution/error causes)

        - the structures inherent to these workflows (the 'solution structure' and the 'change structure', respectively) indicate structures that can be applied which can have the same impact on the output
          - a 'change' structure can have the same impact as applying a 'solution automation workflow' to a 'solution automation workflow', bc a solution automation workflow is a powerful source of change as well as a change-resolver (difference-reducer)

      - simplification: this workflow involves finding useful problem-solving structures with similar impact/outputs and applying those as alternative structures to generate new solution automation workflows, for the problem-solving intent of 'improving an existing solution structure like a solution automation workflow'


    - useful structures like 'rule set' interactions can be used to find/derive/build problem/solution structures like problem/solution components/variables/examples/filters, problem space systems for simulating solutions, & solution automation workflows
      - various systems as 'isolatable/closed rule sets' act like interfaces in the brain
        - sub/unconscious
          - 'learned rules' like memory associations
          - 'inherent rules' like requirements
          - 'emergent rules'
            - lack of contradictory structures (if a system survives, its likely to optimize on some metric involving the avoidance of destructive contradictory structures)
              - there is no inherent 'rule set'-controlling rule that corrects the whole set of rule sets, except for an emergent rule set in the form of structural survival (energy rules & other physics rules) 
            - interaction levels that emerge between rule sets
            - emergent limits on functionality like 'picturing higher-dimensional spaces' (these limits being created by the interactions of other rules) or 'thinking of unknowns that can or cant be known' or 'evaluating uncalculatable uncertainties'
            - emergent priorities like 'simplicity' given input priorities like 'minimize cost', leading to emergent rules like 'biased rules'
          - these would include the 'default structures', like 'core structures'
        - consciousness (rules enabling/describing the control of other rules)
          - 'rule-update/learning rules'
          - 'rule-testing rules'
          - 'rule-storage/retrieval/deletion rules'
          - 'rule-activating rules'
          - 'variable-handling rules'
          - 'abstraction rules to enable additional layers of rule-controlling rules'
          - 'logic rules', 'filter rules', 'change rules', 'info rules', 'derivation rules', and other interface structure rules 
          - these include 'adaptive structures', like structures to improve a default/standard solution
      - other interfaces (also formattable as rule sets or systems) emerge bc of these 'rule set' interactions
        - error structures
          - imbalances in rule attributes like 'rule importance' and 'rule functions'
            - a 'rule-deletion rule' can destroy useful rules, even if its primary function & usage of that function is to remove useless rules
          - lack of consciousness/ability (limit structure)
            - these rules may interact to have blind spots and other error structures, where one rule (prioritize positive thinking) may invalidate updating another specific rule type (self-evaluation rules)
          - a consciousness that depends on a rule may not be able to evaluate that rule objectively
          - some rule types can disable conscious/adaptive rules
            - 'emergent rules' like 'error (bias) rules' can disable conscious rules from being applied
        - structure interface
          - formatting other structures like attributes & systems as rules to standardize input so rules can be organized
          - structures (like combinations, filters, gaps) of rule sets
            - learned & inherent & emergent rules might all seem like alternate rule sets or one combined rule set building or representing sub/consciousness depending on how much consciousness is enabled
        - usage interface
          - conscious/adaptive rule interactions are limited by the degree/type of consciousness applied
        - system interface
          - a system providing/prioritizing access to some rules can have a scalable effect like an activation effect on other rules (like useful questions to speed up learning)
        - meaning interface
          - organization rules of rules
          - interaction rules of rules
        - some of these systems are likely to develop, to develop in parallel, to replace other systems, and/or likely to develop similar/equivalent functionality as other systems
      - placing filters between rule sets enables the injection of rules to apply these rule sets, which allows the development of structures like non-linearity, potential, flexibility, alternatives & abstraction layers around rule sets
        - 'apply this rule set when this condition is met (the condition rule is applied as a filter, so the rule set acts like an option), rather than by default in all cases (the rule set acts like a requirement)'
        - these structures emerge as 'freedom' or 'consciousness', allowing 'variables' to be represented in which rule sets are applied
        - the 'line' representing a 'difference' between un/sub/conscious may be a line on a spectrum of consciousness, or a line separating separable systems of rule sets, or an emergent structure that can take multiple alternate/combined forms, where its variation (reflected in its 'energy' or 'information') can be distributed across alternates or centralized in one alternate
          - a structure allowing a 'requirement' to be converted into an 'optional alternate' (applying a variable) is useful in adding attributes of consciousness, if that variable can be handled given the other rules existing, as in if there are limits on this structure to add variables, so variation can be handled at a survivable pace
      - these structures can be used to derive solution automation workflows, solutions, or other problem/solution structures (such as 'adding a variable' to a workflow, 'adding a filter to create a variable' of a workflow, 'applying rule-update rules' to improve a workflow, 'applying abstraction rules' to abstract a workflow, 'adding rule set state storage' to allow deriving examples/intent/common attributes/errors/variables of other possible rule set systems, 'adding extra resources for simulating/testing structures' to allow storing a mock system as a sub-system for testing solutions before applying them, etc)
      - simplication: this workflow finds/builds/derives/applies useful formats like 'rule sets' (systems, functions) as useful structures like components/inputs of a solution automatically, to find specific rule sets/functions that can relevantly simulate a problem space system in which a solution can be found adjacently
        - a 'complex' rule set (like a 'neural network structure' as a 'complexity-reduction' or 'complexity-handling' structure) may handle enough complexity to identify 'complex variable interactions' in an input problem like 'find a prediction function'
        - a rule set with 'minimal error structures' like a 'general error-correcting method' or a 'general solution-finding method' may be adjacent enough to the input problem state that it can adjacently be converted/fit into the rule set
        - converting interface structures from other interfaces (change, cause) to a standard format like 'functions' is useful for identifying other useful structures in a problem space including the 'find a prediction function' problem and the 'neural network' solution-finding method, useful structures like:
          - overlaps of functions that can generate an example input like a data point
          - functions that can map a set of input/output example vectors to a general (connecting) coefficient vector
        - this is useful bc primary interfaces (like 'system', as in a 'rule set', or 'change', as in an 'input/output difference') can be represented on other primary interfaces (a 'function set' or 'function having an input-output connection')
      - workflow fit: this is related to workflows where an interface is found/built/derived that would make solving the problem trivial, but specifically uses 'interchangeable alternate' formats like functions which can reflect high variation in interactions and specifically applies them to problem/solution structures by applying useful cross-interface structures like 'attribute-system connections' or 'attribute-structure connections'

    - apply structures like 'randomness' which have a connected useful attribute like 'complexity' ('randomness' having the output of adding attributes/functions like 'adding complexity', 'adding generality' and 'offsetting an imbalance in randomness') to create useful differences in problem/solution structures that are likelier to solve problems in 'problem space systems' having those attributes (more complex systems)
      - this workflow applies useful structures (like cross-interface 'structure-concept' connections) to optimize interaction functions specifically between problem/solution structures, so they are likelier to reflect true connection functions (insights)
        - the more similar a problem space system or solution is to reality, the better it can be used to solve real problems
        - randomness is a 'shortcut' to complexity, not equivalent to complexity, but it is easier to add than a set of insights about the system

    - find interface structures that are useful for problem-solving intents like 'reduce computations required to solve a problem' such as 'interaction levels' by applying 'solution filters' (like 'attributes of interaction levels') to the solution space of 'all possible useful interface structures', in order to find useful interface structures like 'interface queries that solve multiple problems' 
      - attributes of interaction levels include 'a level of specificity/abstraction that doesnt reduce the supported complexity'
        - attributes of useful interaction levels include 'having a reduced set of structures to reduce the required structures to represent other structures'
      - applying solution automation workflows to solve the problem of 'finding optimal interaction levels to run interface queries on, given their definitions or attributes of useful interaction levels' would also solve the problem of 'filtering the solution space' and 'reduce computations required to solve a problem'
      - these queries are 'interchangeable alternates' generating another 'useful interaction level' of 'interface queries that solve multiple problems' which are likelier to be useful than other interface queries

    - apply interface structures like 'attributes' of problem/solution structures like 'solution automation workflows' to fulfill problem-solving intents like 'connect' applied to other problem/solution structures like 'specific solution-finding methods like "regression" for a particular problem format like "find a prediction function"'
      - example: for the 'find a prediction function' problem, this would take the form of finding interface structures like 'interim solutions' between other solution states created by applying different solution automation workflows, solution-finding methods, interface queries, and other problem/solution structures, then finding 'connections' between those solution structures
        - for example, finding the connection between a standard constant regression line, a set of connected subset lines produced by tangents of input vertexes (important data set points determining or representing the data set), and a solution produced by applying a different error-minimizing metric, different 'average' representation definition, or a function-adjusting method like a generalization or regularization method
        - each of these solutions would be produced by different solution automation workflows, interface queries or other problem/solution structures, and each of these solutions is connectible to the other, and the connections between solutions reflect the parameters of the variables of the problem/solution structures applied to generate them, indicating a 'solution-connection path' that is findable & applicable to find solutions from other solutions, to find a particular solution having specific attributes of the problem/solution structures applied and possibly also specific attributes of the solution itself such as solution success probability or a particular solution metric value
        - for example, the interface structures to produce a 'standard solution' may involve applying structures such as a 'standard/default/unit definition' of the 'input requirements', and the structures required to produce a more optimal solution may be 'connections between determining/generative/causative inputs'
          - a 'connection function' between these two sets of interface structures would include:
            - 'apply a definition of "relevant" inputs'
            - 'fulfill problem-solving intents like "connect problem input data points & solution output prediction function" for relevant inputs'
          - this 'connection function' can be applied to generate other more optimal solutions from a standard solution
          - a 'generative function' of a solution would include:
             - 'generate a standard solution by applying standard interface structures or a standard solution automation workflow'
             - 'merge solution automation workflows to reduce the work of applying multiple workflows (such as where steps executed during iterations of solutions are executed simultaneously to reduce iterations)'
          - this can be used to generate solutions having particular solution metric values, like 'higher accuracy' to reliably generate standard solutions & improvements to a particular solution
          - these 'solution-generating' and 'solution-connecting' functions can be applied to fulfill problem-solving intents like 'connect input-output sequences' applied to solution structures, applied specifically to 'connect solutions of varying optimality'
      - workflow requirements: this workflow involves pre-calculating some structures (like 'solution-connecting functions of solutions produced by solution automation workflows') to reduce future required computations ('a full interface query', as opposed to a 'solution interface query involving only pre-calculated structures'), by finding the structures that would be most valuable to pre-calculate, out of all the structures that follow rules & can be calculated, like 'connections between standard & optimal solutions' or 'connections between optimal solutions'

    - apply useful structures like 'structural similarities on interaction levels' to problem/solution structures like 'interface queries' to fulfill problem-solving intents like 'find/derive/generate more optimal solutions from standard solutions'
      - example: 
        - for the problem of 'find equivalence between structures', interface queries to solve the problem would include:
          - 'define equivalence as similarity of determining structures like interchangeable usage contexts or input-output sequences'
            - 'check different inputs/outputs of structures to compare, and assess their difference by differences between input-output sequences'
          - 'define equivalence as zero work required to equalize structures'
            - 'create the target solution structure (a difference) or an adjacent transform of it (equivalence), and apply the 'differences required to create it' as an identifying attribute of their difference'
              - 'create an equivalence & assess differences required to create it'
                - 'reduce differences between input structures to compare, then compare the input structures once differences have been reduced, assessing similarity by how many & what type of differences needed to be changed in order to create an equivalence'
          - 'define equivalence as similarity of values of differentiating attributes'
            - 'check each attribute of input structures to compare, and apply the comparison function to check for similarities/differences in each attribute'
        - interface structures like 'connections' between various interface structures such as 'interaction levels' of these interface queries follow rules which can be automated
          - for example, the differences in the 'top' or 'initial' interaction levels of the queries of each interface query (the lines including 'define') can be connected by applying different definitions of 'equivalence' or other variables (like 'direction') to the problem
          - the differences in the 'secondary' interaction levels of the queries are connectible in their usage of specific problem space structures (like inputs/outputs or differences of structures to compare) implementing those definition/direction variables
          - different interface queries can connect these interface queries on different interface structures like interaction levels using similarities, like how a function to 'find differentiating attributes of inputs' is similar to a function to 'compare inputs'
          - these interface query interactions such as 'connections' can fulfill problem-solving intents like 'switch to another interface query that is more optimal (as in likelier or known to produce a more optimal solution)' or 'test multiple interface queries'
        - general interface queries to connect interface queries include:
          - 'apply interface queries that can fulfill a solution metric like "preserve complexity of inputs in outputs" to avoid error structures of a standard solution like "loss of relevant input info"'
          - 'merge interface queries requiring the same functions or inputs to maximize probability of a query finding an optimal solution'
      - solution success cause: this works bc connecting two high-variation 'problem-solving' interfaces like the 'interface query' interface and the 'solution' interface or the 'solution-finding method' interface creates a new interaction level where these structures can be found/derived/generated from each other since theyre connected/comparable & otherwise operable on that interaction level
      - generalization: fulfilling problem-solving intents like 'connect problem/solution' or core interaction functions of problem/solution structures like 'connect' applied to problem/solution structures is likely to generate useful structures, like 'interface queries to find optimal interface queries to solve a problem, given a standard solution & the derived interface queries used to create it'
        - implicitly, the workflow 'derive interface queries that can be used to find/derive/generate a known solution, find structures to optimize these interface queries to find more optimal variants, and apply differences to these optimized interface queries to find more optimal solutions, and apply differences like abstractions/specifications to these interface queries to find variants of them like solution automation workflows derivable from the queries' is referenced in this workflow

    - find the variables of change that would determine a solution using a particular solution-finding method in some optimal way & apply those to the solution-finding method
      - there are two values of the 'direction' variable in which a straight line (standard solution) can vary (if the accurate solution prediction function is a parabola with one peak) in order to test whether the positive/negative change is the right direction of change, but there are multiple values of the 'position' variable in which the straight line can vary
      - it is trivial to configure a neural network to support testing these changes in an optimal way, such as using 'maximal differences' to find a function of 'big-change contributing variable', then 'minor differences' to adjust the possible alternate functions produced by the previous change type

    - identify useful structures like 'sequences' of useful structures like 'change types' that would find a solution optimally in some solution metric when applied in some position of a particular problem/solution structure (like a 'solution-finding method') in a particular problem/solution format
      - example:
        - useful change types include the following:
          - applying a set of changes in opposing directions & then checking for improvement in the solution metric (like 'difference from the optimal position of a miminized error') 
          - applying big changes initially to differentiate solutions and then small changes after the initial change to check for adjacent solutions to initially differentiated solutions
        - a 'sequence of these change types' applied to a useful structure for the 'neural network' solution-finding method format in the position of 'weight updates' might be more useful than applying them in randomly selected variables
        - 'applying big changes initially, then small changes' would also be useful for identifying highly causative or predictive variables quickly in a neural network (as in how some variables explain more change than other variables) and by delaying the application of 'small change-contributing variables' until later, we can avoid randomly or generally applicable functions to adjust the function of 'big-change contributing variables'
          - first identify a standard solution composed of 'big-change contributing variables', then adjust the standard solution using 'small-change contributing variables', which is a specific implementation of a solution automation workflow involving 'applying changes to a standard solution' applied to the 'solution-finding method format' of a 'neural network'
        - configuring a neural network or another solution-finding method to enable testing these structures of change structures is trivial, just like configuring a network to support 'combination' change types is trivial, or configuring a network to support various weight-update types/degrees is trivial, and configuring a network to support all possible input subsets & n maximally different coefficient sets is trivial (like with a 'difference-maximizing weight initialization function'), a configuring a neural network to support change types that vary within ranges/positions of determined optimals (by applying optimization structures, solution structures, anti-error structures, etc)
          - testing every possible useful neural network that uses the structures referenced in solution automation workflows is another adjacent solution
      - a variant of this is to apply 'common' useful change types to create a solution
        - apply changes to core variables like the 'count' of a variable value
        - apply a 'position' change type to the input variables of a neural network to represent variables emergently (as an additional variable to apply to the 'neural network' solution-finding method) when they can be constructed by other variables
        - apply the 'opposite' of or 'differences' to the 'negative' structure (error structures, sub-optimal solutions, neutral structures)
        - differences in optimality of solutions & converting one to the other by 'optimization potential' of one solution
        - common attributes & other structures of successful solutions and successful solution structures such as 'successful solution types', like 'solutions that optimize for multiple metrics'
        - paths to connect different solutions & inputs to workflows/queries that create 'common solutions (solutions to multiple problems)'

    - apply 'interim structures' of a particular solution automation workflow, optionally in a particular problem format, and/or using a particular core interaction function associated with the workflow to 'problem/solution structures' to find structures linking the workflow with filtered interface queries such as for 'related useful structures' more directly, as the 'interim structures' are inputs or adjacent structures to the useful structures & other query objects, which can be derived once the 'interim structures' are found/derived/generated
      - 'connecting attributes of a problem structure with attributes of a solution structure' is how to fulfill the 'connect problem/solution' problem-solving intent using a 'connection sequence' structure in or created by a workflow/query
      - 'connecting problem state with solution (or at least more optimal or neutral/not-error) state' in a 'state sequence (composed of changes, change units like functions, change outputs or their impact on previous objects in the sequence like state)' in or created by a workflow/query is how to fulfill the 'connect problem/solution' problem-solving intent
      - 'connecting problem space with solution space' and 'connecting solution space with possible solution set' and 'connecting possible solution set with optimal solution subset' are ways to fulfill the 'connect problem/solution' problem-solving intent using a 'connection sequence (composed of filters)'
      - 'connecting problem space system with the optimized problem space system without the problem' is a 'state sequence' in or created by a workflow/query that fulfills the 'connect problem/solution' problem-solving intent
      - 'connecting problem with the problem cause, and connecting the problem cause with a solution to the cause' is a 'set of state sequences' in or created by a workflow/query that fulfills the 'connect problem/solution' problem-solving intent
      - 'connecting problem with all possible optimal solutions' is a 'connection network' structure or a 'set of connection sequences' structure (if there is more than one optimal solution)
      - there is always a way to format a problem as a 'find a connecting structure' problem (connecting problem to a related/causal problem or connecting solutions or connect problem/solution or multiple included versions of these), 'connect' being a 'core interaction function' that is implemented in a problem-solving intent ('connect problem/solution') as a way of implementing a useful structure ('change one state to a more optimal state')
      - different problem/solution structures are involved in these examples bc different problem/solution structures are used in different workflows/queries to fulfill the 'connect' core interaction function, but the problem/solution structures allowed are determined as adjacent (and possibly limited) by the problem format:
        - 'find a structure that fits another structure' can connect some 'attributes' of the problem to the 'attributes' of the solution structure, so 'attributes' of the structures are particularly relevant for fulfilling the 'connect' core interaction function for this problem format
        - 'convert problem state into solution state' or 'move problem state toward solution state' can connect the 'position' of the problem/solution indicating their 'similarity' using 'changes' applied to the sequence object (state, format, etc), so position/similarity/changes are particularly relevant for fulfilling the 'connect' core interaction function for this problem format
        - 'filter possible solution set' can connect a 'set of solutions' with a 'filtered subset of solutions', so 'solutions & solution attributes like possibility/probability/relevance' are particularly relevant for fulfilling the 'connect' core interaction function for this problem format
      - these structures can be stored as 'useful structures to apply when a particular core interaction function & problem-solving intent combination is applied to particular problem/solution structures', and they can be found/derived/generated by the functions that can connect a problem with the resulting useful structures (like 'connecting problem space system with the optimized problem space system without the problem'), at which point the structure is specific enough to be useful & invalidate the requirement for finding the problem/solution structures to fulfill the 'connect' core interaction function for, and other related useful structures can be applied to apply the useful structure, such as 'input-output sequences' being a relevant useful structure to 'connection sequences', 'state sequences', and other sequence types
      - these can be called 'interim useful structures, given variables like problem/solution structures, core interaction function, and problem format' which connect a solution automation workflow and help filter the set of possible interface queries

    - identify the probability of solution automation workflows to identify different/similar solutions & prioritize structures of solutions like 'attributes of successful solutions like solutions to multiple problems'
      - example: 'trial & error', 'break a problem into sub-problems', and 'apply useful structures to find interactive structures to connect problem/solution' may produce similar/different solutions
      - a variant of this is 'finding solutions that are commonly produced by solution automation workflows for a particular problem format and applying those as a pre-filtered solution space to initialize the search for the more optimal solutions' (reversing the logical direction of the workflow, or applying workflows from both directions (optimizing/finding solutions first given solution patterns, or optimizing/finding solutions first then applying standard problem/solution connection functions))

    - identify structures that are likelier to be useful in a particular solution-finding method (like 'neural network configuration') in a particular problem format (like 'find a prediction function') as a way of 'finding useful/error/solution structures' for problems in general
      - example: for the 'configure neural network as a solution-finding method' problem applied to the 'find prediction function' problem format, how to introduce variation in the 'prioritized structures' of a neural network (like 'combinations' and 'subsets') to cover testing of more possible useful change structures in the network, without reducing the variation in weights applied to other structures (like 'combinations')
        - problem:
          - if you add 'subset' structures to a standard dense network that applies 'combinations' by default, you get sequences like this, which reduces the number of different possible weights applied to the 'combination' structure
            - subset(all layer outputs) as input for 1/2 of the next layer nodes
            - combination(all layer outputs) as input for 1/2 of the next layer nodes
        - possible solution space:
          - you can apply 'unit' structures ('components of original features' or 'original features') that are likely to build the target structures ('combinations', 'subsets')
            - subset_and_combination_components(all layer outputs) as input for all of the next layer nodes
          - you can apply 'all possible versions of the subset/combination interactions' to each node output
            - all_subset_combination_interaction_functions(subset_and_combination(all_layer_outputs)) as input for all of the next layer nodes
          - you can apply difference-maximization or another function to make 1/2 or another ratio of nodes sufficient for finding 'probable' rather than 'all' structures of combinations
            - 'maximize_differences(combinations(all_layer_outputs))'
          - you can derive which weight changes would cover the most functions applied to inputs & update weights in the direction of 'maximum coverage'
            - 'identify_common_weight_changes_from_different_functions(all_layer_outputs)'
        - any choices other than default choices introduce a perspective that may reduce the chance of finding certain structures, but the same applies to the default parameters
        - 'maximally different' networks that support 'finding the most possible interaction functions' in data can be derived by which network configurations are im/probable or required by a structure change
        - the 'weight' variable acts like a high-variation network parameter, but it also acts like a limiting filter on what changes are possible, even when 'high variation in functions' is applied to outputs of a particular layer
          - this is a useful variable type to identify, bc it has that ambiguity built-in and can determine the success of the solution-finding method, even when other parameters are optimized for success
          - it is a 'cross-interface' variable in that it can act like a pro-change (expansion) or anti-change (filter) structure, regardless of other change structures, making it very powerful
        - these functions applied to layer outputs (which can be formatted as 'weight changes that maximize difference/coverage' rather than 'functions applied to outputs to create priority structures determined to be probably useful like subsets') can influence the output without regard for other cross-interface structures like 'meaning' (as in the impact of the change structures, such as whether they can identify error/solution/useful structures)
        - alternate functions may optimize for finding 'maximally different/useful/covering changes' better than another function (like 'change (variable) combinations', such as 'function combinations')

    - identify other required/possible inputs to the solution by applying other inputs, as any of the inputs can be connected to the solution with different paths, so whichever structures an input is connectible to (solution structures) are required outputs of other inputs as well, so deriving the solution from some inputs can derive other inputs
      - input1 -> solution-finding method -> solution
      - input2 -> solution-finding method -> solution
      - once the 'input1 -> solution' path is known, the 'solution -> input2' path can be derived if the inputs are high-variation causative or cross-interface inputs
      - 'other inputs' and the 'paths to connect one input with the solution' are useful structures for the problem-solving intent of 'identifying missing inputs to a solution (like other components or changes)'
      - solution success cause: this works bc everything can be connected
        - all variables all connectible with the right functions & information
        - just like one particular insight like 'structural problems are automatable problems' can be used to derive all other insights (by applying interface structures like cause/function/concept, etc)

      - this interface query is an example of how to find important structures (like 'cell cycle limits') causing an error structure (like 'cancer')
        - find attributes ('systemic') of the error structure ('cancer')
          - apply relevant interfaces ('system') of those attributes ('systemic')
            - apply the 'system' interface (to find systems related to 'systemic' error structure, standardizing the data type for 'comparison & connection of inputs/outputs' to fulfill 'connect problem/solution')
              - find related structure (metabolism) of the same type (system) of an input (diabetes) in an input-output sequence (diabetes -> cancer) where an error structure (cancer) is the output'
              - apply the 'function' interface (including inputs/outputs)
                - 'find related structure (cell cycle regulation) of an input (cell division) to the error structure'
                - apply the 'structure' interface (including 'opposites')
                  - 'find opposite structures of cell cycle regulation (like cell cycle disrupters and the inputs like triggers/requiring structures of both processes)'
        - this query is more complicated than a standard useful structure like 'input-output sequence' but is more specific & useful and is adjacent to that useful structure, requiring only a few conversions to create it

    - apply specific structures for a problem format that are inputs to deriving other solution automation workflows to generate/find/derive other workflows
      - example: identify the specific structuers of a problem format (like 'find a prediction function' problem format) that can be used to derive other workflows/queries (a small or unit data set can be used to derive different solution-finding methods of prediction functions since all thats required in a minimal degree of variation in the data set to accurately represent the complexity & variation of the problem format leading to multiple alternative solution-finding methods and solutions)

    - apply explicit structures like 'definitions' to identify structures like 'requirements' (required inputs like 'core functions' to an intent like 'build a structure') that can be useful structures for problem-solving intents like 'identify useful structures'
      - example: for the 'find a prediction function' problem this would take the form of identifying 'core functions' as structures 'required' to be useful for some intents, since any structure is necessarily constructed from the only available core functions, so identifying core functions is useful for intents like 'identifying components/subsets of a structure' or 'building a structure'

    - apply interface structures that are useful in identifying useful structures like 'individual variable causative potential (impact)' to identify structures useful for multiple intents like 'preserving info' and 'adding info'
      - example: for the 'find a prediction function' problem, this would take the form of a 'variable network that isolates variables, where each data point is represented as a horizontal sequence of the variable network, where differences in a variable value are depicted by vertical differences', allowing the change structures such as 'change patterns' of each variable value to be represented separately from other variables, so the interface structures like patterns of each variable can be more clearly identified, as a 'variable' is a change structure, so isolating the change structures (like 'patterns' and 'causative potential') of each 'variable' change structure is a useful application of the 'change' interface (resulting in the 'individual variable change patterns') to the 'change' interface application (resulting in the 'variable network')
      - this structure is an alternative to structures like 'clusters' of a data set, which indicate 'similarity of data points' but reduce the clarity of change structures of each variable
      - this structure would make it more obvious when a variable is near an asymptote, minimum, average, sub-type, or other attribute of its change patterns, while retaining other useful info like 'variable interaction functions' and 'co-occurrence of variable values' (as a horizontal & vertical query of the variable network)

    - apply useful structures like 'alignments' between problem/solution structures to find alternate solution-finding methods to apply
      - example: for the 'find a prediction function' problem, this would take the form of 'aligning input variable values in the same order to create multiple aligned input vectors, where the output variable is the next item in the sequence, then apply sequential prediction functions to predict the next item in the sequence (the output variable)'
        - this creates an 'alignment' structure between the variable value positions in the vector, allowing them to be used to predict the next value (in this case the value that needs to be predicted is the output variable), making use of alternate solution-finding methods like sequential pattern-prediction functions that apply useful structures like 'sequential patterns' to create predictions

    - apply useful structures (like 'opposite' structures of 'negative' structures like 'possible error structures') as a way of improving the 'accuracy' of a problem/solution structure (like a problem 'input' such as a data set)
      - example: for the 'find a prediction function' problem, this would take the form of applying common error structures like 'misidentifying random coincidence as the variable interaction being examined' or 'measurement errors' or 'formatting/processing errors' to the data set to generate alternate versions of inputs that would potentially reflect the actual correct data set
        - a variant of this is applying errors to a standard solution like a regression function found for a data set
        - this attempts to correct 'possible error structures' given error structure possibilities & probabilities when actual error structures cant be definitively derived using requirements/definitions/data

    - identify variables in specific applications of workflows & interface queries to specific problem formats to generate alternate solution-finding methods
      - example: for the 'find a prediction function' problem, given the set of all possible prediction functions created by connecting subsets of non/adjacent points, and given the set of all possible 'merge' functions of sets of those subset-connection functions, apply these 'interim structures for a specific problem format' as variables of problem-solving intents like 'connect problem/solution' and when applying aligning structures to those problem-solving intents like 'input-output sequences'
        - 'merge functions applied to the sets of subset-connecting functions' is an interim structure when solving the problem by connecting 'subset-connection functions' with the 'prediction function', after applying 'find subset-connecting functions' to 'data points'
        - these interim structures can be useful as variables to generate other possible solution-finding methods for a problem format like 'find a prediction function'

    - apply specific useful structures for a particular problem/solution structure like a problem format to structures of useful structures like inputs/outputs to find specific structures for a problem format that can fulfill problem-solving intents like 'reduce difference between problem/solution' or 'filter the solution space'
      - example: for the 'find a prediction function' problem, this would take the form of applying specific useful structures like 'average' or 'minimum' which are useful for the problem format of 'find a prediction function', to structures like 'subsets' of useful structures like 'inputs/outputs' like specific data points, to test if each data point can act like a useful structure (either locally for a subset or globally for the whole prediction function) to reduce computation time of determining the remaining structures and/or to filter the solution space once a representative or otherwise useful data point is found

    - apply useful structures like 'abstractions' to useful structures like 'input-output sequences' to create useful structures like 'alternates' of useful structures to apply in solving other problems
      - example: abstracting the 'input-output sequence' that leads to a solution for a problem type leads to a general 'input-output sequence' that can have specification structures like constants applied to it to convert the abstract version into a specific solution
      - generalization: this can be generalized to 'apply structures that are useful for general problem-solving intents like "creating alternates of useful structures" that are useful fro specific problem-solving intents like "specifying a specific instance of an abstraction like a type" which is a function useful for problem-solving intents like "apply changes to a standard solution (the abstract type structure) to generate specific/improved/new solutions"'

    - identify changes that create optimizations when applied to various different standard solutions, to find change structures that indicate the location of the solution
      - example: for the 'find a prediction function' problem, this would take the form of applying changes to various standard solution functions, identifying which changes improve the optimality of the standard solution, and identify the location of the solution given the combination of the directions of optimization indicated by those changes
        - if two standard solutions are a constantly increasing line and another constantly increasing line with a higher slope, and when you apply changes that increase the higher line and decrease the lower line & produce an improvement in both, that indicates there are multiple possible optimal solutions - whereas if you apply changes that decrease the higher line and increase the lower line & produce an improvement in both, that indicates an optimal solution function in between them, where those 'applied change sets' are the only change sets found to result in an improvement in solution metrics

    - apply the solution format as a starting point for the 'problem/solution-connection' problem-solving intent bc its more limited (in that it has more requirements than the inputs ,which is why solutions are a subset of possible states)
      - example: when building a chatbot, there are a few valid solution outputs (different extensions) of the chat session, so those should be used as the starting point of the problem/solution-connecting method, which can include 'regex for the keywords of a particular solution' (such as how the 'speaking to a team member' extension is selected if no other 'extension-matching patterns' are found, like selecting the 'appointment scheduling' extension when 'appointment' is mentioned)
        - starting from the outputs (the 'set of valid optimal solution outputs', such as the supported extensions) reduces the level of detail/complexity required, so the problem isnt to 'handle all possible inputs' but to 'filter inputs into valid solution outputs where possible', which may require 'approximate-matching' functions to separate inputs into the limited set of valid optimal solution outputs
      - generalization: this workflow can be generalized to 'find an optimal problem/solution variable change to fulfill a problem-solving intent more optimally, such as how changing the starting point can fulfill the 'problem/solution-connection intent' or the 'filter solution space' intent more optimally bc valid optimal solutions are more limited so there are fewer options to search'

    - identify changes that would break a solution (such as by 'changing the solution format', which are 'inputs to errors'), and changes that would not break a solution (which are 'possible variables' of the solution) and differences between them to identify directions to apply changes in
        - example: some changes applied to nn inputs would invalidate a solution format of 'prediction function coefficient vector values' (like a 'matrix multiplication operation' that creates multiple coefficient output vectors or a matrix of dimension higher than one) and other changes (like 'sums of weighted inputs') would not invalidate the solution format
          - the difference between the 'matrix multiplication operation' and the 'sums of weighted inputs' is trivial to create (in the form of an 'extra dimension'), but important to avoid (in order to avoid changing the solution format)
          - the pattern/structure/other interface structures of this 'difference type' are indicators of error structures like a 'change to the solution format', which can direct an interface query that finds 'changes to inputs that would not invalidate the solution format' (which are 'possible variables' of the solution), which would produce changes like 'changing how node layer outputs are combined' or 'changing which node layer outputs are combined'
          - knowing more possible variables of the solution allows valid variants of the solution to be generated/applied/derived for various problem-solving intents like 'improving an existing solution' or specific problem format intents like 'prioritizing different input structures (like subsets or adjacent features)'
        - related workflows: a related workflow of this workflow involves deriving intents of the problem/solution/solution-finding method formats (like 'find a prediction function' problem, 'neural network' solution-finding method, 'prediction function coefficient vector' solution format), such as 'prioritize different input structures like feature subsets such as adjacent features according to usefulness of those structures & coverage of input-output feature sequences' which is a useful intent for the 'find a prediction function' problem when the solution-finding method is 'neural networks'
        - workflow fit: this is a specific version of a combination of the 'avoid error structures (such as 'error inputs') & move toward solution structures (like 'solution variables')' & 'change solutions to improve them' workflows
        
    - find alternate formats of problem/solution structures by applying useful structures to find alternate structures with the same input/output sequence to optimize problem/solution structures for different input/output formats
      - how to create a 'notebook' solution to the problem of 'having to print variable state and function output in order to see if a function is correct or applied correctly':
        - problem type 'sub-optimal separation of information'
        - solution type 'combination of information'
        - solution format 'editable web asset to show function outputs like info formats such as 'graphs', in addition to info formats that can already be stored in existing solution format (code files) like 'text in comments"'
        - problem-solution connection query
          - identify problem type & corresponding solution type
            - identify requirements of solution type
              - has to store useful info of all types (code, variable attributes like 'variable state' or 'variable state sequence', and function attributes like 'function outputs' like graphs) in one location (like a 'file' or 'web page' or 'editor app widget view')
                - these requirements fulfill the requirement of 'all useful info types' being 'visible at or approximately at the same time (in the same window/screen)'
          - identify solution format that would support the solution type's requirements
            - identify any difference that contradicts a particular solution format, in order to filter the solution space of possible solution formats
              - an example is the difference between a graph info format and a code info format, which contradict/filter the 'code file' format as a possible solution structure in its current implementation
                - identify similarity of 'code files' and 'html files' in having a possible function of 'storing code', given that 'storing code' is a requirement of the solution
                - identify difference of 'code files' and 'html files' in having a possible function of 'storing graphs', given that 'storing alternate info formats like graphs' is a requirement of the solution
                - given that the 'html files' info format fulfills both solution requirements, it fulfills an optimization metric of 'fulfilling more solution requirements'
      - apply 'alternates' structure to problem/solution structures like 'interface queries' to find alternate formats of the query, such as a 'set of questions', a 'set of function calls to answer questions', a 'set of requirements such as info or structures', which have varying usefulness for different input formats, like if input is formatted in terms of 'structures' in which case 'requirements' for optimal structures may be a better format bc requirements can be structures, not just a 'missing information' structure like questions

    - derive other useful structures like 'inputs' or 'alternate routes' of a known useful structure (like 'core components') by applying useful structures like 'definitions' ('core components' being 'simpler' than other components, so there is a 'structural similarity' between 'core components' and the 'easy' attribute) to find useful structures like attributes of the structure (like 'easy') which can be used as solution formats of sub-queries to find relevant structures ('adjacent structures' such as 'inputs/outputs' or 'alternate routes') to the original known useful structure
      - this would find 'what question would adjacently find useful structures', with results like:
        - 'what structure would be the most optimal solution' (with different definitions of 'optimality' including attributes like 'easy' including definitions like 'default' or 'adjacent' or 'known')
          - this question can adjacently lead to useful structures like 'core components', which have a 'structural similarity' with the structure of 'easy' (so theyre 'one useful structure/function away')
          - the default 'solution filter sequence' produced by the question are:
            - 'find possible solutions, then find optimal solutions' ('possible' solution, then 'optimal' solution)
          - when another useful structure like 'reverse/opposite of the input-output sequence' is applied to the 'optimal' structure of the question, a different 'solution filter sequence' is produced by the question
            - 'find optimal structures, then find solutions in those optimal structures' ('possible' optimal structure, then 'solution' optimal structure)
        - so this useful question (which is a problem-solving intent) created various useful structures ('alternate solution filter sequence', 'alternate useful structures', 'problem/solution structures useful for problem-solving intents') with adjacent transforms ('find useful structures like structural similarities in the original structure', 'default problem/solution structures like "solution filter structures"', 'apply useful structures like "reverse" to useful structures like "input-output sequence" of the original structure') resulting from applying various useful structures ('default solution filter sequence', 'structurally similar structures', 'reverse a substructure'), making this useful structure (the question) a useful structure in general, since it can adjacently generate other useful structures, providing a problem-solving intent to 'find useful structures that can adjacently generate other useful structures' and a way to generate them (applying 'structural similarities' to useful structures to find inputs of the useful structures which may be generative functions of the useful structures, then testing if those input generative functions can also generate other useful structures by applying other inputs/structures to the functions)

    - find structures of useful structures that are useful for the problem-solving intent of 'optimizing useful structures' & apply them to optimize useful structures
      - this is a generalization of the below workflow which found a useful structure to optimize (the 'interface query', specifically the 'sequence of queries in the interface query')
        - 'find' useful structures 'of' useful structures to 'apply' useful structures (such as problem-solving intents) by 'applying' useful structures to useful structures to 'find' useful structures 'in' useful structures
        - the query "'find' x 'of' x to 'apply' x by 'applying' x to x to 'find' x 'in' x" may be a useful pattern to apply to useful structures in general, as a query of useful structures that involves fewer computations than other interface queries which would involve standardization to multiple other interfaces than the meaning (useful/relevance) interface
        - any 'structure (like a network, sequence, or combination) of functions' applied to useful structures may also be a useful structure bc functions connect useful structures, so if one useful structure is known, another can be easily found/derived/generated using that 'connecting function structure'
          - this generalizes to other structures having the same interaction level, where the applied functions are valid operations in the form of 'interaction functions' of structures on that 'interaction level', so they can be applied to any of the structures as the structures are interchangeable and so are the interaction functions, so this can be used to identify possible connection functions of structures on an interaction level once the interchangeable interaction functions for that interaction level are known
      - generalization: finding 'alternate values' of problem-solving variables like 'problem-solving intents' and 'useful structures' is a generalization of this solution automation workflow, as each new value of these variables identified adds a variant of existing solution automation workflows using that structure

    - find useful structures (generative functions like 'filter for rarer attributes') of useful structures (like 'more reductive filters') to fulfill problem-solving intents (like 'filter solution space') by applying useful structures (like 'opposite of an input-output sequence') to useful structures (like 'standardized problem definitions or sub-problems' like components of an interface query like 'what is the easiest solution') to find useful structures (like 'efficiencies' or 'optimizations') in useful structures (like 'interface queries')
      - to find a useful structure like 'core components', an interface query would ask the question:
        - 'what would be the easiest (as in simplest, default, etc) structure to create' or 'what structures are easy (available or otherwise adjacent/trivial to find/derive/generate)'
        instead of the question:
        - 'what are the easy solutions to the problem', a question which arrives at the 'core component structures' from another direction
      - 'what are the easy solutions (like existing/known/available, standard, basic, simple, or default solutions, as produced by various solution automation workflows) to the problem' is a problem-solving intent 
      - instead of looking for structures that are solutions to the problem & have an attribute (easy), which involves first finding solutions, then filtering them for the solution metric attribute - it becomes a query for structures with an attribute (easy to find/derive/generate), after which they can be filtered for solution fit, which first finds structures with the solution metric attribute, then filters them for a solution, which may be more efficient than the reverse direction bc the 'set of possible solutions' may be larger than the 'set of easy structures', both of which would need to be filtered, so filtering the smaller solution space of 'easy structures' (which are solutions to the problem of 'find easy structures') may be more optimal if it's actually smaller than the solution space of 'possible solutions to the original problem'
      - this workflow finds 'useful structures' that fulfill problem-solving intents like 'filter the solution space' more optimally than other structures, then finds ways to generate those structures ('filter for the "rarer" or "more computable" attribute'), ways to find the outputs of those structures to find unique signals of the structures ('more reduced solution space'), and fulfills other problem-solving intents applied to those structures to generalize the workflow

    - identify finding/deriving/generative functions of equivalent interchangeable interface queries that can identify a solution or solution-finding method
        - example: 
          - 'the possibility of a one-possibility decision' is equivalent to a 'requirement' but uses different structures in its definition, just like a 'possible solution filter' acts like a 'required solution metric application'
          - neural network examples of sets of 'useful structures' that can be used to adjacently derive the 'neural network' idea (in addition to core structures like 'inputs/outputs' and 'sum/multiply operations')
            - 'change structures like change combinations', 'input-output sequences'
            - 'solution requirements', 'problem assumptions', 'adjacent structures'
            - 'multiple alternatives', 'large outputs built by small inputs', 'difference-maximization'
            - 'variable network', 'change-application functions'
            - 'change variables of existing solutions' (change coefficients of default prediction function, which is unit coefficients, biased/specific coefficients, or average coefficients among likely alternative probable coefficient sets)
            - these structures are all useful for building the 'neural network' structures bc they align with other useful structures on some interface:
              - a 'requirement' is similar to an 'assumption' so it can be used in place of an assumption in some contexts where their inputs/outputs align in a way that wont produce errors in that context
              - connecting an 'initial' input with a 'target' output is similar to 'connecting problem/solution' or 'connecting suboptimal solution with a more optimal solution' so it can be used in place of a 'problem/solution sequence', like where the 'problem-solution sequence' is a known 'requirement' but only 'input-output sequences' are available to fulfill it, because of the 'alignment' on different interfaces between problem/solution and input/output structures which are aligned by some definition making them similar (like a 'state/format/change/filter/function sequence')
              - so applying the 'change' interface highlights the similarity between problem/solution and input/output sequences and identifies 'input/output sequences' as a useful structures that can be used to convert a problem into a solution, if available functions as 'change units' are known, which are a 'requirement' of the aligning structure of the 'input/output sequence'
          - examples of structures that can be used to adjacently derive any of neural network, interface analysis, or other prediction function-finding methods
            - 'input-output sequences' (like 'data set' => 'prediction function') applied to 'interface structures' (like 'combinations') of 'highest change-reducing/finding structures' (like 'interface analysis' or 'neural network' functions or 'statistical prediction function-finding methods' or 'function decomposition methods (into other functions)') create functions that support core important functions ('find', 'reduce') applied to 'changes', a primary interface structure
        - these examples have similar inputs/outputs but are more optimal in different edge cases, so applying a useful structure like an 'input-output sequences' isnt always enough to find robust solutions, edge cases such as where a variable looks like an independent variable in one data set but is clearly correlated with another variable in another data set
          - finding structures that are 'adequate' for a particular prediction task but not robust to other inputs takes more structures to accurately describe than just 'one structural similarity between inputs/outputs'
          - these solutions dont find these counterintuitive structures that are non-adjacently created from known functions/attributes/structures by default, so they are necessarily incomplete or otherwise suboptimal solution-finding methods without interface analysis injected at some point
      - generalization: this workflow finds 'useful structures' (like 'alignments across interfaces') that can be used for the intent of 'building interchangeable alternate optimal structures' for a given problem-solving intent like 'find a solution or solution-finding method'

    - index functions by whether they fulfill problem-related metrics like 'functions that are called the most frequently' (which by definition of 'usefulness' is relevant to most or all 'intents' which is inherently relevant to 'problem-solving')
      - this applies the definition of 'useful structures' to 'useful structures' in a way that allows 'adjacent structures of useful structures' to be identified, applying the 'input-output sequence' structure to known useful structures to identify the inputs/outputs of useful structures, such as 'useful ways to index functions that injects the definition of usefulness so the most useful functions are found first, reducing the solution space of possible functions' which fulfills the problem-solving intent of 'finding useful structures'

    - identify variables of useful structures (like 'feature-changing functions') in an existing solution-finding method (like 'neural networks') for the problem (like 'find a prediction function' problem) & useful structures like 'connections of useful structures & other useful structures' (like 'feature-changing functions of a neural network' and 'usage intents of a neural network' or 'problems solved by a neural network' such as 'find high variance-capturing variables using input features'), to find out what interface queries a useful structure can fulfill and generate alternate useful structures for particular problem-solving intents related to usage intents of the solution-finding method
      - a network that allows repetition or reuse of a structure in other variables/functions will be flexible enough to store both a function and the component functions it calls, as well as a variable and the structures creating/using/changing that variable
      - a network that allows 'combinations of changes' in building features from combinations of other features will be able to identify 'change combination features' like a 'feature of multiple versions of another feature or feature combination'
      - variables
        - other structures can generate features than 'change combinations' to build 'general or high variance-capturing features from many input features, which are the default feature-changing structure & input-output structure of a standard neural network, and other causes for these feature-generating structures (such as 'identify high variance-capturing feature like type from many input features like image subsets') can be identified
        - other feature-generating functions exist than 'change combinations'
          - 'change combinations of change combinations'
          - 'concepts'
          - 'combinations, once standardized to the change interface (comparing combinations rather than combinations of changes)'
          - 'input-output sequences, connections, or maps'
          - 'functions' and 'patterns'
          - 'filters' and 'requirements'
        - other usage intents exist than 'find features that are inputs to high variance-capturing features & predict high variance-capturing feature values'
          - 'find variables that can cause inputs'
          - 'find variables that can create approximations'
          - 'find interface where other variables are high-variance'
          - 'find high variance-capturing functions to identify probable input-output sequences that should be used in a solution-finding method'
      - a useful structure involving this useful structure would be a "connecting function to find the associated 'usage cause' (like 'find high variance-capturing feature like type') of a 'feature-generating structure' (like 'change combinations')", as a function that is particularly useful in the neural network problem space to find/derive/generate other neural network algorithms that would fulfill a particular usage cause/intent
    
    - merge alternate found/derived/generated solution-finding methods for the sub-problems theyre optimal/useful for in solving a problem, applying 'break a problem into sub-problems' and applying different known solution-finding methods to those sub-problems, once the optimal sub-problem for a solution-finding method is found
      - applying interface structure solution-finding method to find 'useful intents' that a neural network could implement in order to 'find a prediction function [connecting variable sets]', given probable variable interaction functions
      - applying the neural network solution-finding method to find 'high variance-capturing variables & their input variables' which are likely to be usable with & connectible to 'useful intents' like 'find high variance-capturing features' which are probable to be identified as a particularly useful intent that a neural network should fulfill
      - these two methods connect in the interim structures connecting the 'feature-generating methods' with the target 'useful intents of a neural network' solution structure, as an interim step to the final step
      - the final step involves using the integrated output of the previous step (in applying the 'useful usage intents of a network' and 'feature-generating methods' of a neural network) for the intent of 'finding optimal feature-generating methods or useful usage intents of a network when either is required' (such as in changing a known solution-finding method to solve different problems)
        
    - apply differences to useful/core/default structures like 'function networks' to find their associated useful structures like 'interaction rules with other structures' that could help find/derive/generate useful structures to fulfill problem-solving intents
      - example:
        - asking the question such as 'what is the cause/context of the suboptimality of a useful/core structure' such as "why is a 'function network' inadequate/incomplete for some intents" generates the following insight path which reveals a new solution automation workflow to find useful structures
          - differences between 'function network' and 'optimal function network'
            - a function network can only represent 'functions using the function network' as 'queries of the function network', rather than representing the 'functions using the network' as 'nodes on the function network'
            - so if an agent calls a function 'go to specific point A', that cant be represented as a function on a function network that only contains functions like 'go' and 'go to destination' and 'convert destination to point', rather than 'go to specific point A', only as a call to functions on that network, so a function network cant contain the functions calling it as nodes but rather 'structures (like sequences/trees/networks) of nodes'
          - 'requirements' of 'function network'
            - inevitably in order to be useful, a function network will require that other structures/functions be converted to functions on that network so it will necessarily be incomplete
          - 'inabilities', 'suboptimalities', and 'errors' of a 'function network'
            - a function network cant figure out why its not being used for a task unless it has functions to adjacently identify 'alternate functions' and/or 'tasks that would find those alternate functions useful' and has functionality to apply its own functions to identify those structures as a 'function network metric' to optimize
          - 'differences' in 'function networks' relating to 'problem-solving'
            - some function networks are more adjacent to solving problems just by framing problems in terms of functions defined on that network, so some function networks are more generally optimal or specifically optimal for solving some problems than other function networks, while producing errors for other problems
            - this implies the usefulness of 'finding a function network that would make problems in general easier to solve', and 'finding a function network that is optimal for solving known different problem types/formats', and 'finding the differences in specifying a more optimal function network for a given problem than the generally optimal function network, as a generative method of more optimal function networks'
              - abstracting these useful structures ('finding differences' and 'finding difference-causing variables' for problem-solving intents mentioned above) leads to this workflow
    
    - identify variables & patterns of successful/optimal interface queries, apply abstraction to remove any unnecessary constants specified, & frame them in terms of the 'problem' interface to connect them to new solution automation workflows
      - example: an interface query like 'find alternate input-output sequences to find other ways to connect problem inputs and solution outputs' can be abstracted to the solution automation workflow 'find useful structures to fulfill problem-solving intents having structural similarities to the useful structures' (such as how 'connect' is structurally similar to 'input-output sequences' and 'other ways' is structurally similar to 'alternate input-output sequences' as 'alternate routes'), given that:
        - 'connecting problem inputs and solution outputs' maps directly to the problem-solving intent of 'connect problem/solution', as every solution automation workflow needs an associated problem-solving intent that it fulfills, either explicitly or implicitly
        - the 'solution success cause' of the interface query (the reason it is successful) is because it 'finds other solutions, which may be more optimal than the original solution to improve' and involves 'finding a structure that is relevant for a problem-solving intent' which by definition is useful for problem-solving, and because it has a useful structure (a 'structural similarity') allowing the specific useful interface structures ('alternate input-output sequences') & the problem-solving intent ('connect problem-solution') to be connected by that 'structural similarity', this connection determining usefulness of the structure for the problem-solving intent
      - generalization: a general method of 'apply useful structures to find specific interface structures to fulfill problem-solving intents' can be derived from this workflow, as well as variants like 'apply useful structures to find specific interface structures to fulfill adjacent functions to problem-solving intents' and the abstract version of both which is 'apply useful structures to find specific interface structures to find/build/derive problem/solution structures (like problem-solving intents)'

    - identify useful structures like 'alternate routes to fulfill the problem-solving intent to "find/derive/generate useful structures"' and 'alternate changes applied to find/derive/generate useful structures' as a way of fulfilling the problem-solving intent of 'finding new useful structures', such as the 'outputs of useful structures', 'systems leading to the generation of useful structures', 'requirements triggering generative functions of useful structures' other than known useful structures like 'generative functions' of useful structures, which can be found by applying changes to the relevant structures such as the 'input-output sequence creating useful structures', which if a change is applied to add the next output (the 'useful structure output'), a new useful structure is found with that 'adjacent applied change to a known useful structure' in the form of 'outputs of useful structures' which is useful for 'finding new useful structures' by enabling identification of 'common outputs of useful structures' or 'input-output patterns of useful structures' to provide inputs to fulfill the 'find new useful structures' intent

    - identify useful structures like 'equivalencies created by problem-solving structures' like interface queries & solution automation workflows, such as interface queries that are adjacently created by multiple solution automation workflows, as a way to identify attributes of commonly successful problem-solving structures so those structures can be generated/found/derived using those attributes

    - identify useful structures like 'optimization interchangeables' ('alternatives to optimize that can effectively replace other structures to optimize') as a way of identifying useful target interim solution outputs to use as input to fulfill problem-solving intents like 'avoid errors' and 'apply generally successful solution or optimization structures'
      - example: if the interface query to find a solution or a solution-finding method is optimized, other structures such as input formats dont have to be optimized as much

    - find interface structures that when applied, make a problem more solvable with a more efficient interface query (involving fewer operations or already computed operations, etc)
      - example: for the 'find a prediction function' problem, this might take the form of identifying 'interaction levels' of 'important variables' to solve the problem or a related problem (the 'find neural network parameters' problem) at, such as the 'personality' interaction level or the 'mental model' interaction level of the 'neural network parameter' variable space, where the problem can be re-formatted as an interface query to 'build a neural network with this personality or this mental model' given that neural network parameters having different personality/mental model can solve problems with corresponding varying success, optionally with other queries to 'build a neural network to predict the solution to a problem or problem-solving accuracy given input of a network personality or network mental model and a problem format' to select network parameters to solve a problem with an 'alternate route', which might be more solvable with available data than 'build a neural network to solve this problem', which is less specific and may require manual selection of params or selection of param-selecting algorithm or injections of assumptions in selecting parameters/algorithms, and then repeat the process for other interaction levels such as 'known outputs/effects/errors associated with parameter changes for various problem formats', as a way of generating an 'alignment' across interaction levels indicating a structure of 'certainty' and integrating useful interface structures like 'facts' (from 'known outputs') and alternatives (as 'solution testing and generalizing structures'), integration of which make a solution more complete, robust, and based on understanding
      - this identifies the 'neural network parameters' as the important variable for the 'find neural network params' problem that it identified as particularly useful to solve, which if solved, would be the 'most efficient problem to solve' to reduce the interface query steps
        - this is bc a particular neural network may have relatively few parameters to search, and compared to reducing the solution space of all possible prediction functions, reducing the solution space of 'all possible neural network parameters' is easier to solve by comparison
        - this workflow then identifies the 'neural network parameter' interaction levels as important to vary when creating the solution space of all possible neural network parameters to search, as 'personality' or 'mental model' may significantly reduce the parameters of a neural network even further than they are already reduced, so the problem becomes the query 'find the neural network personality' to fulfill the intent of 'train a model with in order to find the prediction function' instead of the query 'find the prediction function [coefficients]' which is more general and requires more sub-queries to implement
      - workflow fit: this workflow is specifically designed to fulfill the problem-solving intent 'solve a problem in fewer or easier steps' specifically fulfilling the related function 'reduce the interface query sub-queries as much as possible', given that the interface query determines the resources (functions, memory, computations) required to solve a problem, as 'selecting between alternate interface queries' is an important intent relevant to solving a problem optimally, as one interface query can effectively match 'trial & error' in terms of efficiency, and another interface query can effectively match its optimal opposite in optimization metrics, so these differences are important to optimize

    - find interchangeable alternate structures of structures (like combinations) of useful structures (like how 'requirements', 'changes', 'sequences', & 'inputs' or 'solution', 'error' & 'change' are useful by default but are more useful when applied together for known problem-solving intents like 'change problem structure until its a solution structure given solution requirements') that can be used as default interface queries to execute first or to run other interface queries on to improve them, given that they are relevant for known relevant problem-solving intents or related functions
      - example: some error structures are useful when applied together, in that they are sufficiently causative of enough errors that a solution avoiding those error structures can be considered to be a 'better than standard' or otherwise optimal solution
      - sets of interchangeable structures are themselves interchangeables, occupying the same interaction layer and not invalidating the other interactive components, as interchangeable structures such as the set ('solutions', 'errors', 'changes') and the set ('requirements', 'inputs', 'outputs', 'changes') and the set ('irreducible function network') and the set ('useful structures') and the set ('core interaction functions' and 'problem-solving intents') and the set ('interface queries and solution automation workflows') and the set ('changes' and 'known or standard solutions') and the set ('structural interface' and 'interface application function') can be substituted in for each other and can interact with each other, such as applying one set to another, without invalidating the other sets (as they can coexist in the same system), just like how find/build/derive can replace each other & interact, so these structures can be applied as variables when they occur in solution automation workflows, interface queries, or other structures, as opposed to structures like 'alternate input-output sequences' which might invalidate each other

    - apply useful structures like 'maximally different' structures to generate relevant structures like 'change structures' applied to 'solution automation workflows' as a way of fulfilling relevant problem-solving intents like 'generating new solution automation workflows'
      - example: apply 'trial & error' to 'break a problem into sub-problems', which involve very different functions, outputs & structures, to generate workflows applying the concepts of one workflow
        - like applying the concept of 'every combination' or 'all instances of a type' from 'trial & error' to the other workflow to generate:
          - 'try breaking every or other problem into sub-problems' which is useful for problem-solving intents like 'finding common problem attributes like causes or types' that is useful for 'improving an existing solution'
          - 'try every problem-breaking method' which is useful when a particular problem-breaking method generates more solvable sub-problems
          - 'try every combination of problems or sub-problems as a way to generate more obvious or solvable errors or solution structures to other problems'
        - these workflows are different enough for their application to each other to be a useful source of new structures of change to use as variables (in this case applying the concept of 'every' to fulfill the problem-solving intent of 'alternating the structures' of the workflow having the 'trial & error' workflow applied to it to generate new workflows)

    - index useful structures by useful structures such as 'input requirements' or 'input causes' of a useful structure or 'usage intents the useful structure is most useful for' (like a 'variation-causing sequence of useful structures') as a way of finding useful structures of useful structures quickly for a particular problem-solving intent or interface query intent

    - identify useful structures by which structures would implement a useful structure like a function relevant to problem-solving (such as a 'function to fulfill a particular problem-solving intent' like a 'solution-finding function') across multiple problem types/formats, which by definition means they are useful for problem-solving, as a way of deriving useful structures
      - an example of this is "identify useful structures (like 'core components') by which structures fulfill generally useful functions (like 'build') which are generally useful and therefore useful to problem-solving as well"
      - workflow fit: this identifies useful structures in a new way, by deriving them from changes, inputs, requirements, & other useful structures applied to problem/solution structures as the structures determining usefulness so they should be used as inputs to a function determining usefulness of a structure

    - identify general interface structures or specific problem space system structures that would be useful (a solution-finding method parameter update function) for various problem-solving intents as the target solution structure, and apply interface structures to structures of the problem space system to apply useful structures like patterns of interface structures to the problem space system structures to implement these optimal useful structures
      - example: for the 'find a prediction function' problem, this would take the form of identifying the impact of a particular change type in the system of variables that are inputs to the prediction function, thereby mapping interface structures like a change type to structures in the problem space system such as 'coefficient value changes', so that useful structures like patterns of that change type can be applied to calculate coefficient value changes, as opposed to a method like 'create a set of sets of independent variables, representing functions of input variables, (like a set of layers of neural net nodes) and test if their impact on a prediction function is useful'
      - other useful mappings from interface structures to problem space system structures include:
        - identifying if a particular probable interface structure (like a 'processing function' that is hypothesized to be applied to the inputs at some point in the problem space system) would produce a particular change associated with a function or function network applied to some function of input variables of a data set, to derive which functions should be present in the neural network to implement or filter out that interface structure
        - this connects the solution-finding method with interface structures of the problem space system, so a theory of which functions of interface structures determine the problem/solution are probable can be tested by changing a solution-finding method's structure to test that theory
        - multiple workflows can be derived from a structure (like a sequence) of useful structures, by applying change functions relevant to the structure, such as adding/removing items at either side of the sequence, wrapping an interface query with an interface query, or substituting alternate structures
          - this interface query identifies specific problem space system structures associated with useful structures: 'neutralization'
            - identify problem space structures (network node functions having neutralizing effects) implementing useful structures (neutralization) of useful structures (parameter changes) of useful structures (solution-finding method) 
              - to identify problem space system structures implementing useful structures of neutralization
                - to use in a function improving the solution-finding method
                  - to fulfill the problem-solving intent 'improve existing solution'
          - this interface query identifies an input-output sequence having a useful structure in the form of a useful function (an update function of solution-finding method parameters) as its output 
            - identify useful structures (inputs) of useful structures (update function) of useful structures (solution-finding method parameters)
            - to identify neutralization structures as useful structures
              - to use in a function improving the solution-finding method
                - to fulfill the problem-solving intent 'improve existing solution'
          - identify useful structures like 'possible error structures' such as whether the input-output direction of solution-finding methods & queries implementing or creating them reflects the problem space system direction (whether specific features can construct general features or vice versa), and whether the direction can be determined or if the variable type/interaction could be different from the input-output sequence assumed by the solution-finding method
          - for example, identifying where effects of nodes' functions have effects like 'neutralization' to identify relevant structures to the solution-finding method's changes (the network's parameter (weight) changes), like 'deactivation probability of a node' given that structures of 'neutralization' would by definition not change the outputs of a network, and would therefore be relevant for processes relevant to the solution-finding method like 'node deactivation', so 'neutralization' structures are relevant to the solution-finding method's changes so 'neutralization' structures can be considered an input to a function that changes (improves) a solution-finding method's parameters
            - other example useful structures would be the opposite related function of a function with useful outputs like 'decreasing the solution-finding method structures' (to fulfill the useful structure of 'removing unnecessary structures that dont change outputs'), such a function with useful outputs like 'increasing the solution-finding method structures' (to fulfill the useful structure of 'improving a solution metric like accuracy or robustness that changes outputs in a useful way')
        - identify useful structures (like 'inputs') of useful structures in the problem space system to identify structures that can create useful structures
          - identifying structures like 'weight update functions' that enable or create other useful structures such as requirement-invalidating structures like 'unnecessary training iterations' to identify where iterations can be skipped
        - identifying which combination of optimal standard functions would be useful to start filtering the solution space & derive the neural network for that function combination as the initial first iteration output of the final dense layers (which structures of functions of inputs such as 'functions of functions of inputs' would produce the changes to inputs necessary to create those optimal standard functions)
        - identifying the default or useful interface structures such as change types (such as adjacent change types) that should be tried first as connecting structures (such as core combinations, core transformations of original data set variables) and whether those change types (as applied to input variables, to create output coefficients) are supported or likely to be produced by a neural network's parameters (whether a neural network can identify a particular variable subset combination as a useful structure for predicting another variable, in its supported weight update changes, which are the aligning problem space structure with the 'change types' interface structure)
          - this could allow avoiding the need for a neural network in best case scenarios, instead identifying the useful change types & other relevant interface structures before training
            - rather than 'finding network parameters that enable finding the change types required to produce the prediction function', this is a problem of 'finding the change types applied to weights & other interface structures that can convert input features to output prediction function coefficients'
              - given that the sum of weighted coefficient vectors will act like operations (such as exponent operations, multiplication operations, or multiplication by zero operations) in some cases, varying change types (like exponents) are supported with that 'overlap' structure between the input-output sequence of those operations, supported by a neural network structure given the configured weight update rate, thresholds & other params, and given the data set's changes required to get from input features to output coefficients
        - applying the 'minimum info' requirement interface structure to neural networks, it could identify that if it cant improve its solution metrics beyond a certain range considered useful (like a 'better than average prediction success rate'), it doesnt have the minimum of info required to solve the problem, so it can handle this sub=optimality by injecting a structure to 'prevent nodes from being deactivated in a higher ratio of cases' since it cant be absolutely certain given its inability to narrow the solution space that some nodes wont contribute with other training data sets or in some cases, so identifying nodes that could contribute in alternate probable edge cases inferred from the data set which should have a lower deactivation probability injected would be a solution structure to improve that sub-optimality
        - identifying a pattern structure such as 'node-weight unit sets that change a function around various base or symmetry functions within the data set' can identify useful structures in the data set, like alternate or component functions, and re-creating those node-weight unit patterns will allow those function structures to be detected in the data set
        - organizing weights & nodes to have a similar impact on the prediction function given similarity in node position can allow various sections of the network to be de/activated and the useful weight paths & other weight structures identified more quickly & consistently
        - describing node input as a 'sum of functions of input features having different coefficients created by different weight paths' with intent to 'test alternate weights applied to the same summing tree' is a perspective that makes it obvious that other structures can be applied to create the useful structures of the network ('different coefficients applied to input features'), such as a change-generating or change-maximizing function applied to input features, a function to combine those changes & a function to derive contribution of change combinations
          - 'deriving what contribution types/structures would be particularly influential in finding a prediction function' is the reverse direction, to find what changes should be applied to produce those contributions
            - example: a function might benefit from various extreme scalar values applied to various input features, a scalar that changes a default function into a function similar to the prediction function, an exponential-increasing value, and other contributions from values that are more powerful for some intent than other value types/structures, so these values which have a higher probability of contributing to a prediction function should be guaranteed as being generated by the change types supported by a neural network, as a requirement in selecting initial network parameters to start training from, or as a priority of which network parameters to test first
        - given an assumption such as 'converting a discrete set of features to a continuous output value' (like 'more cat-like or dog-like on the species spectrum', 'output dependent variable value which has a value for adjacent input variable value sets, implying continuity'), it can be inferred that a neural network should have 'many components applying changes to input combinations', as simple combinations of features like sums are unlikely to create a smooth curved function that is likelier than simple constant-sloped functions than complex subtler coefficient changes that are likelier to be created by incremental weight changes
          - applying one scalar to a function can create certain change types like 'increasing a constant slope', but summing it with other one scalar-applied functions can introduce other change types given the differences in input values, and since these are the useful change types, that 'summing with other one scalar-applied functions' could be identified as a useful structure to implement 
          - 'find useful combinations of changes' can be identified as a solution requirement to fulfill once 'change combinations' are identified as a useful structure to apply to input features, supporting various change types like 'changing coefficients to zero (or near-zero)' to handle the error structure of 'variables not contributing to the prediction function' and the error structure of 'subtle/small changes being required', both error structures being potentially improved by a network supporting the change type 'change coefficients to zero'
          - simple regression in general corrects for one error structure ('difference from average'), but if other error structures are embedded in the solution-finding method (like 'difference from specifically/exactly accurate function in one case'), the method can improve in general accuracy in most cases, an error structure that can be identified by applying the structure of 'alternate' to the 'data set' component of the problem space, to infer the existence of 'alternate data sets' and errors the standard simple solution would create with the possible changes associated with that variable
        - the intent path that follows could also generate the structure:
          - 'resolve difference between inputs/outputs' -> 'change inputs until theyre outputs' -> 'find change-applying function' -> 'find change-combining function' -> 'find change combination testing function' -> 'find influence-assessing function (PDEs)' -> 'find change-combination influence-prioritizing update function (weight update)' -> 'find function to identify optimality of solution (stopping function)'
          - where actionable (specific structural) intents can be directly mapped to math structures having those inputs/outputs
        - given that some important variable structures cant be quantified using available input variables, the neural network structure of 'isolating unique contributing variables, variable structures, and variable values' will have an 'error structure' of 'miss these variables that cant be generated from original data set variables', which may require interface structures to find
        - in cases where the variable of 'number of node layers left' (representing the 'number of changes available in this weight path') is suboptimal at changing some states in a weight path to their optimal state (a weight path that would be highly useful in determining the final prediction function coefficients), in cases where the 'optimal weight path state to form a useful weight path relevant for determining final prediction function coefficients' can be determined, this can be used to conditionally add nodes where 'adjacent optimizations' are identified to ensure a particular weight path is optimized by the following changes, if enough other weight paths would benefit from such a neural network structure change, making 'weight paths' into important/influential structures (like 'voters' and 'inputs' and 'causes') in a 'voting/weighting system'
        - applying structures like 'adjacencies' such as 'adjacent weight paths or states' that would 'change threshold filter outcomes' (deactivations) can identify useful neural network structures like 'parameter change &initialization change recommendations'
        - alignments between useful structures like 'input-output sequences', 'priorities (as weights)', 'filters (as deactivations)', 'feature-generating functions', 'directed function networks as overlaps in input-output sequences' 'alternatives' and 'specific-to-general variable generation direction' can identify the 'neural network' structure as an optimality in the 'solution-finding method' solution space
        - identify 'error structures' such as a 'false similarity/connection' in variables that may be correlated coincidentally such as by 'availability/accessibility' or by 'no preventative structures like limits in place that would prevent the connection', but not for a relevant cause such as a 'requirement' indicating a required association rather than a potentionally irrelevant association (from coincidence such as the availability of an input/variable), by finding/deriving/generating & simulating a system where a correlation could occur that is false in terms of being irrelevant for determining another variable like a type as a way of identifying possible & probable error structures
        - applying 'input' structures to 'error' structures, in identifying the possible inputs to found/derived/generated error structures can determine if a neural network is likely to create those errors if it can fulfill the inputs to those errors, like functions or priorities or lack thereof that are inputs/enablers/triggers/users, or other useful structures of a particular error structure
        - identify the sets of changes producing known errors represented by changes to input/interim variables (as in, a variable should be deactivated with a weight of zero to represent the 'irrelevant input variable' error structure, so make sure theres a structure of nodes/weights that will probably produce a zero for each variable at some point during training)

      - this workflow would identify these structures ('unnecessary nodes or training iterations or weight updates', 'solution-finding method update function parameter-improving function', 'neutralization/magnification structures', 'node deactivation probability') as useful structures to implement, in order to avoid other interface applications to solve the original problem, and instead apply interfaces to implement those useful structures specific to a problem, and then given the mapping between the interface structures & problem space system structures to identify useful structures to implement, apply that mapping to identify useful structures to implement in other problems, 'identify useful structures to implement' being a problem-solving intent

    - find/derive/generate specific structural structures (like structural sequences) that can be used to solve a problem and apply them as inputs to an alternate solution-weighting function or as parallel processes to find a solution function first or as inputs to another problem-solving intent like 'generating probable standard solutions'
      - example: for the 'find a prediction function' problem, specific structural sequences include the following, which are input-output sequences of structures that can generate a prediction function
        - non-adjacent subset connection functions + function weighting scheme = prediction function
        - adjacent subset functions + smoothing function = prediction function
        - functions having various average definitions + function weighting scheme = prediction function
        - common function components (peaks, inflection points, extremes) + adjacent transforms to fit the function = prediction function
        - standard solution function + transforms to fit the function = prediction function
      - these 'specific structural sequences' are useful as default components of interface queries to fulfill a solution automation workflow involving structural similarities in required structures such as 'connecting states' or 'finding input-output (connection, interactive) sequences', where the 'specific structural sequences' involve structures that are efficiently resolved in a problem space ('common function components' or 'smoothing functions' being more structurally defined than the 'find a prediction function' problem, thereby adding value in these specific structural definitions)
      - these 'specific structural sequences' might be the output of an interface query, thereby building interface queries in reverse, given interactive/connective structures in a specific problem space, so that interface queries can alternate between these structures ('iterate through & apply these input-output sequences') that could be generated by other queries such as 'find interactive/connective functions that can connect a data set and a function'
      - these 'specific structural sequences' are nonetheless still abstract in terms of the specific functions required to implement them, allowing variation in their implementation, so they can act like sub-interface queries ('apply these input-output sequences to find inputs to the function-weighting scheme'), interim interface queries ('apply these input-output sequences where relevant such as on-demand' or 'skip to these input-output sequences rather than finding alternative input-output sequences' or 'apply these input-output sequences as placeholders for an interface query to be executed when other input-output sequences are found to be sub-optimal for a problem'), or alternate interface queries ('apply these input-output sequences in parallel to see which finds a solution faster')

    - apply useful structures like 'alternate input-output sequences' to various problem/solution structures across interface queries & workflows & apply them in relevant workflows/queries/problems (relevant in being specific to the same standard problem format like the 'find a prediction function' problem format)
      - example: for the 'find a prediction function' problem, this would take the form of 'finding alternate input-output sequences for possible functions required by various interface queries', like how the specific sub-function 'find relevant connections between points' is used by various solution automation workflows implemented with various interface queries that solve the 'find a prediction function' problem in a particular way involving connecting data set subsets such as adjacent pairs or high-priority pairs (such as adjacent subset averages or high-density averages), so alternate functions such as 'clustering methods' that also require these specific sub-functions may be relevant to the 'find a prediction function' problem bc they have a similar input/output (data set input, connection function output), and applying 'clustering methods' to a data set can identify data points that can be connected using clustering functions like 'connect to the nearest points having the most adjacent points', where these 'subset connections' can be used to build a prediction function, just like how 'anomaly detection methods' can be applied to a data set to identify 'non-standard connections' which is a useful structure to the 'find a prediction function' problem

    - apply anti-error structures like a 'requirement of decisions/selections impacting the solution' where decisions ('selection' structures, acting like 'solution filters or specifiers') are relevant to solving the problem (such as where the solution format requires it or its required by a process required to generate the solution) to avoid an interim error structure such as 'false certainty in making a selection resolving an ambiguity' when applied to problem/solution structures (such as when choosing between alternative solutions randomly, rather than for a reason), and apply variables to the solution where selections are not required, to avoid other error structures caused by not avoiding that error structure which would impact the solution structure, like 'over-specifying a solution' or 'over-solving for a constant value that isnt required to be a constant'
      - example: for the 'find a prediction function' problem, this would involve applying variables to structures where optimization is ambiguous (such as regions where a function can vary without changing the optimization of the solution metric) to parameterize solutions restricted to a particular variable value
        - when applying 'trial and error', a decision about whether to 'sort the options first before applying the test of the solution metric attribute' or 'find all solutions or an optimal solution, rather than the first successful one' is not required in the standard problem format of 'find an item having this attribute in this set of items'

    - apply change structures to relevant structures for a particular problem format or problem type to identify other structures these relevant structures can be useful in for the original problem
      - example: for the 'find a prediction function' problem, structures of 'representation' are useful in finding a function that is 'representative' of the data set and can therefore be used for prediction/approximation
        - applying change structures to 'structures of representation' can generate alternate methods of finding a prediction function, such as 'representative structures' like:
          - 'connecting points by finding direction of greatest number of points to navigate in' (similar to clustering algorithms), which prioritizes connections & points that are in higher-density regions of the data set
          - 'averages of dense regions' indicating higher-priority data that should be more representative of the data set as a whole
        - these 'representative structures' can be used to identify high priority points to use as input to the 'find a prediction function' problem or to connect or otherwise interact in such a way that they are used to build the prediction function (like connecting various adjacent high-priority points to form subset functions and then weighting or connecting these subset functions)
        - to find/derive/build 'representative structures' like 'averages of dense data point subsets' or 'connecting prioritized points to form subset functions & then connecting subset functions', other representation structures like 'average' can be applied to problem space structures like 'data points' and 'sets of data points', and known calculations of weight/priority of a point can be analyzed to reveal the input data points that result in a higher weight (those in high-density regions or those representing an average of adjacent surrounding data points) in order to determine attributes of data points that should be prioritized, or identifying the subset of points that could adjacently generate a prediction function in other known example data set/prediction function pairs

    - find useful structures such as 'patterns' of 'differences between multiple alternate solutions' so those differences can be applied to the origin problem state where possible, to maximize chances of finding a solution
      - example: for the 'find a prediction function' problem, patterns of differences between multiple optimal solutions might look like adjacent functions forming an 'area of optimality', where for the 'find a local minima' problem, patterns of differences between multiple optimal solutions might look like 'separations by upward curves of a polynomial', indicating that optimal minimum points are likely to be separated by a parabola/peak or similar structure, where these difference patterns can be applied to an origin problem state like a data set or standard solution (linear regression function) or a random point on a 3-d graph or a standard solution (adjacently computable local minimum)

    - apply alternative structures (like differences from variables, randomness, opposites) to change problem/solution structures (like a specific solution-finding method for a specific problem format) in a way that optimizes a general solution metric (like 'reducing number of required steps/functions' or 'adding information') 
      - example: for the 'find a prediction function' problem, this would take the form of finding 'probability of a data point in this determining area of a function' or 'probability of a data point within range of determining point x of a function' instead of 'determining points of a function', applying interface structures like 'probability' and 'surrounding area of a point' and 'generative functions' rather than the structure of an 'exact point', as a more optimal structure for determining whether an area is 'representative' of a 'determining point of a function', applying 'find generative function of x' as an alternative to 'find x' to optimize a solution metric like 'number of steps required' to generate a different workflow than 'find determining points of a function', therefore 'finding inputs, alternatives, or approximations of determining points rather than the determining points themselves'
        - apply 'mix' interface structure to other solution-finding methods to find structures that add different information, such as 'function coefficients' and 'determining points':
          - generated different workflow: 'find determining points of a function and predict a subset or all of those from the data set, instead of or in coordination with predicting the function coefficients', 

    - find structures such as 'representative' structures that are useful for the problem-solving intent of 'finding a solution balancing various solution variables once multiple probable solutions are found' (solution variables like optimization, determined by solution metric fulfillment)
      - example: for the 'find a prediction function' problem, this would take the form of finding an 'average function' or 'base function' of probable functions that can be solutions, to represent the probable alternate solutions based on a weighting schema, or to represent the 'probable versions' of a function once some parameter change is applied to the 'solution base function'

    - find the relevant structures (like the 'representative' structures such as averages, representative examples like important data points, important variables, counterexamples like outliers, etc) for a particular problem given alternate definitions of that problem ('find a prediction function' as a 'find a representative function' problem)
        - these relevant structures give a different format to aim for when solving the problem, such as finding 'representative examples' as inputs to a 'find a prediction function' problem rather than the original input of the entire data set, 'representative examples of the data set' being relevant because the problem is to find a 'representative function of the data set'
        - workflow fit: this applies 'alternate' structures to the 'problem format' to find 'alternate problem formats'
        - generalization: this can be generalized to apply 'alternate' structures to other problem/solution structures to find 'alternate routes' to fulfill a problem-solving intent or solution automation workflow

    - derive useful structures that can act like alternatives to interfaces, such as structures including objects of a particular interface structure type (like functions, variables, vectors, data sets, or positions)
        - where problems are standardized to 'find a set of transforms to convert one vector/matrix into another', given interaction rules of the problem space system (such as available operations & interaction levels), a useful structure would be the format of a space where a vector is transformed into another vector, where points represent operations applied to an adjacent vector as specified by the interaction rules, so that solution automation workflows can be applied, such as 'finding alternate solution vectors similar to the original solution vector to aim for instead'
          - this space can function like a 'base network' similar to how a language map is used as a base network for solving nlp problems
          - these 'base structures' can act like sub-interfaces and can be useful as alternate structures to interface structures or other useful structures which can be used as 'standardized problem spaces' to convert a problem to, as opposed to converting a problem to an interface
          - the 'base structures' dont need to be comprehensive to be useful - examples of the problem system interactions to specify a vector at a point relative to another vector may be sufficient for solving some problems

    - find structures where functions can be applied to adjacently create the solution, such as 'solutions fulfilling opposing filters' where a 'connect' or 'average' function can be applied to 'solutions fulfilling opposing filters' to find a more optimal solution between these opposites, where opposites represent error structures of varying extremes (extremely general/specific)
        - example: for the 'find a prediction function' problem, solution space filters such as solutions fulfilling the filters 'most intersections with actual data points (most specific function)' and 'most general function or generally representative function' allow those solutions to act like opposing filters, where the optimal solution is between these filter structures implemented as solution structures (a specific function and a general function)
        - the opposite structures represent an 'extreme' error structure, fulfilled by the abstraction attribute ('general/specific')
        - 'finding an average between values (such as extremes)' is a relatively simple function
        - other alternate functions would be 'finding a function that reduces extreme metric 1 (number of data point intersections) and extreme metric 2 (linearity) while still representing the data set to some degree'
        - this workflow finds a structure representing an error structure of the solution (such as opposite extremes of a solution attribute, such as abstraction) then finds structures (solution space filters like 'most intersections with data points') that could create those structures (solution functions having opposing extremes of abstraction) then applies problem-solving functions (connect) or other functions (average) to correct those errors, finding the solution from those sub-optimal solution structures that are adjacent to the solution

    - parameterizing the impact of solution automation workflows on problem/solution structures like the solution space can offset the need to apply the workflow itself, rather than applying its average or otherwise representative output (like its impact on the solution space or the changes applied to the problem origin state)
      - differences in reductions of the solution space created by various solution automation workflows can fulfill various problem-solving intents and help solve various problems more optimally than others
      - a 'trial & error' workflow doesnt reduce the solution space at all, whereas other solution automation workflows like 'derive possible/probable solutions or solution structures like solution limits/boundaries based on solution requirements' would likely reduce the solution space
      - the difference between these workflows' impact on the solution space encodes information about what assumptions are made which can reflect whether a solution space reduction is useful for a particular problem or problem-solving intent
      - example: for the 'find a prediction function' problem, a workflow might derive solution space-reducing filters such as 'a prediction function has to come this close to the data set average for each data subset of size n' or 'a prediction function has to be this similar to these data subset-connecting functions'
        - the resulting reducing impact on the solution space is different from the unfiltered solution space by some parameter set

    - find/derive/generate requirements of various required functions of the solution-finding process like calculating solution success (in the form of determining calculation possibilities such as whether one solution is more optimal than another), requirements such as 'being able to calculate & compare optimality of solutions', to identify useful structures that would invalidate these processes or make them easier/more difficult to execute (such as 'areas of error or solution ambiguity' on a graph, which if you can calculate can make reducing the solution space or finding an acceptable solution easier)
      - example: for a 'find a prediction function' problem, this would take the form of finding/deriving/generating the sections where it would & would not be possible to derive if a solution was better than another (structures of optimization and structures of ambiguity), deriving which of these sections of ambiguity is more optimal than others (a 'solution ambiguity' rather than a 'error ambiguity'), and aiming for a function that intersects with those solution sections (there may be areas or other structures like function bundles or adjacent parameterized function points on a graph where it is clear that a function is sub-optimal, clear that a function is successful, ambiguous whether a function is a solution or error, areas where it is clear that a function is a solution but ambiguously successful when compared to similar solution functions, etc)

    - apply changes to the attributes like position of a useful structure applied in a solution-finding method to implement the intent of the useful structure in its original position in a different way
      - example: for a 'find a prediction function' problem, this would take the form of applying changes in the filter types or change types applied to create 'maximally different' weight path patterns, rather than applying changes to weight path patterns themselves, where 'maximally different' weight path patterns are a useful structure applied to a neural network structure or coefficient sets to test various prediction functions or use them as a base to apply changes to, but applying the useful structure to the filters or change types used to create the maximal differences in weight path patterns can be a more optimal structure than maximizing the differences in the weight path patterns themselves
        - this uses 'maximal differences' and variation in change/filter types bc those structures are useful for reducing the solution space the most 
          - changing a standard base prediction function with one change type like 'adding an exponent to a linear function' creates more differences when compared to another change type applied to the standard function like 'adding gaps in the function', which would create two functions that are more easily filtered out because they are more different, whereas very similar functions may not only be difficult to filter out as they are an ambiguity, but they may also be equally or similarly optimal functions that both could be considered correct
      - generative interface query: the interface query to generate this workflow is 'apply alternate routes to the input-output sequence of the intent of the difference-maximizing changes/filters rather than to the input-output sequence of the weight paths (or their patterns) to identify different solution automation workflows to fulfill the problem-solving intent of "connect the problem & solution" than those applying useful structures like difference-maximizing structures' (changing the position to the input (the change/filter types) rather than to the structure theyre applied to (weight path patterns))

    - apply changes to problem/solution structures, derive functions required to use these problem/solution structures, and from those required functions derive a solution automation workflow fulfilling those function requirements
      - example: in finding different solution formats like 'determining prediction function points' or 'function limits acting like the prediction function', a new requirement for a function is generated like 'find determining prediction function points', from which a solution automation workflow can be derived like 'find a different solution format and find the structures required for that different solution format' which involves different operations than solving the original problem, therefore comprising a different solution automation workflow

    - identify structures (such as a number of significant or accessible structural similarities) required to apply structures from other systems to automatically find useful structures to apply as templates for problem-solving structures, filtering the set of structures produced by an 'input-output sequence' similarity with additional similarities or other structures that indicate enough relevance to apply the structure to other problems requiring the original structure, those similarities or other structures having compounding value in indicating relevance
      - example: for the 'find a prediction function' problem, this could take the form of 'finding similar structures in other systems like a "superposition" that can act like a template to create the same output structures such as attributes like "certainty" or functions like "uncertainty resolution"' to apply to standard structures in other systems, fitting standard structures like functions & activation states in the various structure of a superposition (either having multiple possible activation states of many possible functions, or having an uncertainty in a function's activation state that is resolved at a later time), where the inputs (atomic components), interim uncertainty structures (superposition of many possible states), and outputs (measured metric value) have a structural similarity to the relative dimensions of the neural network, so the similarity in uncertainty resolution & possible dimension sequences create enough similarities to justify applying this structure as a template for a neural network to solve the 'find a prediction function' problem converting a data set into a function parameter set (and the associated conversion of a data set example of independent variable values into the dependent variable value)

    - apply useful structures like 'input-output sequences' or 'causal sequences' to other solution automation workflow inputs, such as inputs or generative functions of 'determining points of the prediction function', to identify new problem/solution structures to aim for in implementing the original solution automation workflow
      - example: for the 'find a prediction function' problem, this could take the form of 'finding functions separating data subsets that act like tangents to the determining points of the prediction function'

    - find useful structures in structures like combinations of interface structures such as 'optimization functions' and useful concepts like 'complexity' (as an error structure), to find for example 'points optimizing for minimized error structures' which are relevant to problem-solving intents
      - example: for the 'find a prediction function' problem, this would take the form of identifying what structures of complexity or simplicity the data set is likely adjacent to, and at what structures of simplicity the data set is likely to converge or stabilize to, at what point in the future, given adjacent stable points and the probability of changing and/or converging in the direction of any of those points, given that stable systems tend to be simpler, so if a prediction function is going to be correct for a longer period of time, it will likely be simpler than other possible prediction functions
        - if structures of complexity have just occurred at the time the data was measured, such as if a system has collided with another system, the effects will either invalidate the system, leading to dissolution of the system into core components, core components which may include the variable interactions of the data set, or the system may stabilize in a different or its original state depending on how it sustains variance injections - if an optimal minimum of simplicity can be derived as adjacent, that simpler function may be the best prediction function until a change occurs in the system hosting those variables that doesnt result in system invalidation

    - apply problem-solving functions like core interaction functions fulfilling a problem-solving intent to various problem structures in a particular solution-finding method
        - example: for the 'find a prediction function' problem, apply 'reduce' function to various possible problem structures like 'more operations than necessary' in a regression method, such as 'finding a representative line of more points than necessary' by reducing the number of points that need to be represented (by finding representative points of point subsets or finding the minimum determining/differentiating points that should be connected or averages to create the prediction function), or 'finding a representation or optimized error metric calculation for a subset of points at regular intervals in the data set so these representation metrics can be connected instead of applying the representation or error metric calculation at every point', at which point 'finding the points to connect or average or represent' or 'finding the interval at which error metrics can be calculated to find an approximation of the prediction function' becomes the primary problem rather than 'finding a representative function of the data set that minimizes an error metric or represents a ratio of points', and if a solution is found by applying core interaction functions to solve these 'problem structures', they can be considered problem structures of a solution-finding method

    - identify interchangeable solution structures (like interchangeable solution-finding methods or interface queries) and their variables to generate other items that could also be interchangeable solution structures
      - example: for the 'find a prediction function' problem, this would take the form of identifying interchangeable solution-finding methods like various regression methods such as 'connecting dependent variable value averages between different point pairs' and 'finding the line that minimizes an error definition' (or identifying alternate structures to this set of interchangeable functions, such as interchangeable interface queries or other generative functions to generate those solution-finding methods) and identifying the variables to generate the interchangeable structures on that interaction level, given that interchangeable methods act like structures having the same interaction level and will likely have adjacent variables in common as they are likely to find/build/derive/apply the same structures
        - for the two example regression-implementing solution-finding methods indicated above, both use an error metric, the first one implicitly ('minimize differences in averages between point pairs so they can be connected in a line') and the second one explicitly, which could be derived as a variable of a solution-finding method - an error metric determining whether the output is accurate (fulfills a solution metric) - that could be used to find other solution-finding methods (generate other error metrics that could determine a solution metric of a prediction function, and reverse-engineer a solution-finding method in deriving/finding/generating a solution-finding method that uses that error metric)

    - some interface queries are more efficient than others in arriving at solutions, other things being equal, so deriving interface queries of solution automation workflows & optimal solutions and selecting the query with the fewest or least computation-intensive steps is a way to find generally useful interface queries, which are likely to result in a set of interface queries that make use of 'useful structures' like 'alternate input-output sequences', so identifying these optimal interface queries is likely to be another way to identify useful structures bc those are likely the reason for the efficiency of those queries

    - apply changes to different problem/solution structures like applying changes to create different solution formats to create different structures that can act as inputs to problem-solving structures like solution automation workflows
      - example: with the 'find a prediction function' problem:
        - different solution formats include a network of conditional prediction functions, representative averages for subsets of the function, function limit structures (like function range caps/boxes), a solution-finding method or generative function of prediction functions, a prediction function for inputs of the original prediction function, a prediction function range or parameterized function to produce functions in that range instead of the original solution format of one prediction function
        - different solution-finding methods include different parameters of regression, merging different subset functions, prioritizing representative data points, data subsets or function subsets to weight their contribution to the prediction function differently, neural networks, applying solution metric filters, applying solution metric filter-fulfilling structures as solution structures, applying adjacent structures like transforms or combinations of standard or base functions, moving subsets of a standard regression line to be more general and/or more accurate & smoothing the resulting set of linear functions, etc
      - this works to generate new solution automation workflows bc applying 'changes' by definition changes the structures involved, so if changes are applied to problem/solution structures in a way that doesnt invalidate their definitions, that method can generate new problem/solution structures that use those changed problem/solution structures as input

    - apply interface structures to solution metrics to derive structures useful in generating those solution metrics, and aim to fulfill those structures of solution metrics instead of the original solution metric
      - example: for the 'find a prediction function' problem, use structures derived from solution metrics like 'sets of allowed function subset ranges' or 'alternate sets of allowed function subset ranges' as filters of how much the solution can vary in a particular location (like a set of 'open-ended boxes' capping variation in a particular subset position of the data set) while still fulfilling the level of accuracy or other solution metric specified by the solution requirements, given that many functions will produce the same solution metric fulfillment if solution metrics arent specific enough, so any function within certain ranges will be considered a sufficient or optimal solution in those cases
        - to find out what structure would fulfill the solution metrics of 'representative accuracy' and 'representative coverage' of a prediction function, an interface query would identify that there are many possible solutions, then identify a structure that would place a limit on what solutions are allowed to fulfill the solution metrics while allowing variation in which solution is selected, which would result in either prediction function parameters to allow for variation in the prediction function that would have the same effect as a 'set of function subset range boxes', or the structure of the 'set of function subset range boxes' itself, boxes having a 'containing' or 'boundary' effect that aligns with the 'limited range' structure of the 'accuracy' solution metric requirement, and subsets of the function having different boxes limiting their range, allowing the boxes to fulfill the 'coverage' solution metric requirement

    - apply interface structures such as a 'change' to the perspective applied at various interaction levels & in various problem/solution structures in solving a problem to change the functions required to solve it
      - example: for the 'find a prediction function' problem, converting a standard 'find a regression line' problem into the same problem from a different angle such as the perspective of an agent standing at the first data point at the lowest x value and looking in the direction of the data point at the highest x value, at which point the problem becomes 'minimize the distance from the center formed by a line intersecting with the point, where the first point represents a point on a line formed by the sequence of y-value points', which is a different way of framing the 'reduce distance from the regression line to the data points' problem that results from a different perspective applied to the problem, changing the functions required to solve it - instead of finding a function for a line fitting the data the best, its finding a function for the point that minimizes distance from other points on the line of y-values, where the y-value line can represent weights of values with minimized distance from higher-weight points have a greater count in the original data set
        - finding a line minimizing vertical distance to points  vs. finding a point minimizing distance to other y-value points, which is a way of finding the average value in a set, which can be applied to adjacent subsets and the averages of these subsets connected or used as input to the regression method, thus making the problem easier to solve by reducing the number of points to find a prediction function for, as the prediction function-finding method may be more costly than a method to find the average in a subset
                                                         /.                  .
                                                      . / .      vs.         .
                                                     . /                     .
        - this method reduces the data set to subsets using some partitioning method, then applies a perspective that reduces variation to one-dimension (y), finds a representative metric for that dimension for that subset, then connects the averages as a standard regression line to base improvements on, or inputs the averages to a regression method, thus reducing the number of points to find a regression line fitting
        - the application of the perspective adds value in isolating a change type, which is useful for finding attributes of that change type such as averages, ranges, change rates, probability distributions, & other attributes
        - the perspective cant be used on its own bc it loses info about the other dimension, so it needs to be integrated back into the original solution format (prediction function for the whole data set) by finding a line connecting or fitting the average y-values of adjacent x-value subsets
      - workflow fit: this is similar to applying a perspective to find perspectives that immediately precede or otherwise usually lead to a solution, but generalizes the application of interface structures to the perspective before applying the perspectives across various problem/solution structures, like in the problem system, to find useful perspective changes and other interface structures applied to perspectives that fulfill a problem-solving intent like making the problem easier to solve

    - identify identifying metrics of possible error cases where a particular found/derived/generated solution would be sub-optimal and apply those case-identifying metrics as filters (in the form of error structures) of a solution to design a solution that doesnt fulfill any of the case-identifying metrics, as a way of fulfilling the 'avoid errors' problem-solving intent
      - example: for a regression method, the case where the data points have randomness or outliers is a case where a regression method such as using an average definition is sub-optimal bc it loses useful information about outliers in cases where they exist, so check if the case fulfilling the identifying metric of 'outliers' applies before designing a solution using the associated sub-optimal solution for that case (the regression method), or apply changes to that solution if that case if either known to be fulfilled or probable, or design a solution that is different from the associated sub-optimal solutions for these cases in ways that are useful for the original problem (rather than just any difference from the sub-optimal solution such as 'not using that average definition' if the identifying metric of 'outliers' is fulfilled, a useful difference such as 'using an average definition that integrates the outliers such as a weighted average', which integrates the outliers' information and avoids the error of 'losing information' that applies to the sub-optimal solution)
      - this avoids errors for worst-case scenarios in a way that applies useful methods of avoiding the error structure of the worst-case scenario, rather than a standard method of avoiding errors, using efficient identifying metrics of cases (the outliers being the identifiers of a 'worst-case scenario' for that particular solution method)
      - this identifies worst-case scenarios where a solution would break by the variables that the solution doesnt handle which are required to be handled, like extra info, where this info should be preserved in the solution format
      - extension: this could be extended to include a method to integrate solutions for various cases, like a weighting scheme for case-specific solutions, and a method to identify worst-case scenarios using variables that are not handled by a solution, such as extremity of outliers or number of outliers which are relevant to the original problem, which is aiming for a prediction function fulfilling accuracy metrics

    - apply interface structures like 'conditional network' or 'combination' to problem/solution structures like solution formats where useful
      - example: for the 'find a prediction function' problem, a function might be better represented as a mixed structure including a function and a set of points or conditions linking those points in a network, if some points would distort the function too much if incorporated into the function given some solution-finding method that handles outliers in a way that would distort the function beyond their relevance to the other points, or a parameterized function indicating some parameter preserving the original difference between the outlier & average
      - generalization: this applies a variable to the problem/solution structures & their attributes (like the number of solution formats being a variable instead of a constant of one)

    - identify useful changes & other interface structures (like 'self-reference' structures such as 'connect connections', 'patterns', and 'common' structures, or formatting a structure by its 'opposite' structures identifying a 'difference-resolution' structure as opposed to a 'connection' structure) to apply to problem/solution structures like function types such as core interaction functions to make these problem/solution structures more specific & useful for a problem
      - example: for the 'find a prediction function' problem, this could take the form of 'apply difference-resolution structures like maximally-different point connection functions or tension-resolution structures such as wave-generation functions or momentum-application functions to account for variation in data that follows normal or common patterns of difference-resolution (connection) to find a prediction function'
        - the 'difference-resolution' structures would connect the 'most different' points in the data set, to find difference types and the common structures (like an average) resolving those differences in the form of a connection between different points or a connection between connections between different points of varying difference types (difference types such as 'greatest distance on both variables' or "greatest difference in a variable once standardized on the other variable such as adjacent points' vertical differences"), fulfilling a 'connect' or a 'connect connections' core interaction function
        - the 'tension-resolution' structures would connect the data set points using common connection patterns, such as patterns of momentum (like the 'wave generation' tension-resolution pattern resolves the difference created in the wave-triggering event), thereby fulfilling the 'connect with common connections' core interaction function
      - 'connecting connections between connections' is useful because it finds similarities between connections, allowing the identification of a pattern or common attributes including common differences which can be variables of the connections
      - 'self-reference' structures are useful bc they apply the same definition repeatedly, and are useful in identifying structures relevant to the 'connect' function, such as similarities, patterns, & common structures in identifying meta-attributes (filters of filters, reductions of reductions, structures of structures, limits of limits, connections of connections)
      - these create structures that are specifically useful for the 'find a prediction function' problem in identifying useful functions that can re-use known concepts like 'connections' which are already defined for that problem space (as in 'line structures having distance, intersecting with points as endpoints') to identify useful info like the 'probable prediction function', useful functions such as 'connect connections' to create useful info like the 'average function' that is relevant to this problem
      - the useful functions to create this useful info can be derived by which functions could connect the 'connect' function with the 'average function' (a 'self-reference' structure, thereby identifying the specifically useful 'connect connections' function), a useful function derivation process that can be applied to other problems, and the specifically useful structure being re-usable with other problems to create specifically useful functions, as well as the useful structure used to generate it, and the identification of the 'average function' as a useful interim target being derivable from its adjacence to known solutions or the original problem's solution as a base standard, 
      - workflow fit: this workflow involves customizing problem/solution structures such as core interaction functions with useful structures like self-reference or opposites to create specifically useful structures for a problem

    - identify opposite structures like 'gaps' of useful problem/solution structures like 'variable handling coverage' created by available structures of the problem & known solutions, structures such as filters, where available filters represent what can be or is measured, so anything not covered by them is a source of possible error structures, as a way of identifying/finding/deriving new error structures, fulfilling the problem-solving intent of 'avoid errors'
      - example: for the 'find a prediction function' problem, filters include requirements and assumptions built-in to the solution-finding method such as the definition of the concept of average that is used, or the method applied to tune the function such as generalization, smoothing, simplifying, regularizing, normalizing, standardizing, or curving methods, and given these filters, other change types may not be applied to the function, such as an outlier not handled by the function that is nevertheless valid, or a break in continuity that is valid, because the assumptions inherent to applying a 'smoothing' or 'generalizing' method will ignore these points in the data set and prioritize creating a continuous function to represent or connect certain subsets of the data set, so the 'gap' in the available filters including a particular definition or a solution-finding method contains the alternate structures not creatable with that definition or method, as a discrete point such as a valid outlier is not creatable with a method that produces a continuous function that 'represents' the data set, unless it coincidentally intersects with that structure, which is relying on randomness rather than a reliable method of preventing those errors
      - the 'variable handling coverage' refers to structures that can be built/derived/found with a set of structures in the problem space or known solution set
      - 'variable handling coverage' is a useful structure in determining what is adjacently & non-adjacently possible with a set of structures, such as a set of known solutions
      - 'variable handling coverage' could also be applied to problems, in finding gaps between problem structures given variables of problems in that problem space, which are possible solutions

    - identify interface structures such as the change types required to find/derive/generate a useful structure for a particular problem format given its definition and apply those change types to find useful structures for a particular problem
      - example: for the 'find a prediction function' problem, useful structures include the function's parameters, its determining points & requirements, and generative functions of the function
        - to convert 'find a prediction function' into these useful structures, various change types are applied which can be applied in other problems to find useful structures
          - changing its format to include other components (exponent terms), converting it to a set of 'constants' applied to these standardized exponent terms
          - changing its format into its 'determining points & requirements'
          - changing the causal position by identifying 'generative functions'
      - workflow fit: an alternative to the 'apply known useful structures for a particular problem after standardizing to that problem'

    - derive interface queries of known solutions and identify other interface queries that could be applied to generate other solutions, where known solutions are inefficient, impractical, or sub-optimal in another way
      - example: for the 'find a prediction function' problem, the following interface queries can generate existing solutions
        - the 'parameter matrix' is generated by an interface query to 'find "structure" that can store "output" of applying "multiple" structure to "generative parameters" of the "function"'
        - the 'regression' solution is generated by an interface query to 'find "required" structures given the problem definition such as "representative line"' (leading to the concept of the 'average' as an input to a 'representation' structure)
        - 'find "subset" prediction functions and apply a "average" definition to them'
        - 'find "adjacence" structures (like adjacent y-values) and apply adjacent change structures found'

    - once useful structures are found/derived/generated/applied for a particular problem, apply other useful structures like change structures that are likely to create differences that could make useful structures like patterns in problem structures like values more obvious
      - example: for the 'find a prediction function' problem, a useful value structure would be a matrix of alternate parameters of the function if one exists (if there are parameter sets with equal number of parameters across different generative function types), so that other changes could be applied to this structure, to create other value structures to examine for connections (such as representing this matrix as a grid of scalar vectors mapping to each value in the parameter matrix)

    - find possible errors in the problem statement such as assumptions that could change the other problem/solution structures, such as solution formats
      - example: for the 'find a prediction function' problem:
        - find different parameterized lines (such as lines converted into a set of segments with different formatting, where the parameters have relevance structures like complimentary variance handling or an overlap structure across different parameter sets) that can generate the line
        - find parameters of a matrix representing different parameter sets of a function (descriptive parameters, compression parameters, generative neural network parameters, wave function parameters, regression parameters, etc) & how they relate to actual function parameters (coefficients, powers)
        - or find a function for an area/shape rather than a line to represent different possible averages and probabilities that each average definition or type should be used as the output's predictor where the area indicates probability of an output occurring between the various lines
        - or find a function that emphasizes/prioritizes data points likely to be near to the actual prediction function or standardizes points in a way that makes them closer to the actual prediction function or removes points that are not closer, to give a shape of an implied function by a data set with a more obvious pattern
        - all of which have a different solution output format that is a variable when the assumption that the output should be a line and nothing else is removed, or that one definition of a component should be used
      - workflow fit: this is similar to other workflows that change the source/destination but applies error structures to find possible variables to apply to problem/solution structures like the solution output format

    - find a space where the prediction function's parameters will be more clearly identifiable (such as by extreme values like minima, or by converging values, or by values that signal a change in change types like inflection points or changes in change rate), like how adding a dimension to a 2-d prediction function might allow identifying a minimum point in the 3-d space where the change types stabilize, which reveals where the peaks & other features of the function are located, so that the task becomes finding the original feature-differentiating minima or other structures in the 3-d space

    - identify connections to generate different solution formats and how to find the format most adjacent and useful for a particular problem
      - example: with the 'find a prediction function' problem, the prediction function can take multiple formats - a set of function parameters like coefficients, a set of interactions with other structures of different dimensions like the pattern of a straight line's intersecting points with a curved function, a set of change rates/types in a structure like a sequence, a set of neural network parameters that could find the prediction function parameters, an infinite sequence, a set of minimal points that could derive the rest of the function (like how a parabola can be derived from 3 - 5 points depending on their positions & other requirements like positive values, where these points can be found with 'subset function' averages & other representations, or in finding the core structures like angles or change types like fast/slow curved/linear change of the function and determining points that determine those core structures), and some of these formats are more amenable to being used with particular solution-finding methods

    - identify multiple solution-finding methods and apply components of one to the other in a way that is likely to be useful
      - example: applying the method used to 'generate different variants' of the function coefficients in neural networks is likely to be useful in an altered method of regression, which doesnt inherently find alternate variants of a function and merge them in a weighting method, bc regression uses the concept of the 'average' already and incorporating another version of the concept of 'average' is likely to be useful

    - find/derive/generate solutions having specific error structures (such as an incorrect assumption that the 'average of a data set is the correct average but is incorrect') as a way to quickly identify what is not the solution by applying these incorrect solutions as an initial non-solution space to use as a filter to find the correct solution
      - workflow fit: this is similar to finding the set of 'maximally different' solutions as an initial solution space to filter, with the exception that known error types are injected in these initial solutions, which are likelier to be not solutions but by definition non-solutions that have a known probable error, as a way to determine the actual solution by its difference from these known error non-solutions

    - apply useful structures like system layer graphs as a general structure that is useful for problem-solving, integrating new problem & information learned from them into the same structure, adding as few structures as possible in the form of new objects, connections, and levels, which are default structures with which most other structures can be framed, similar to a network graph
      - new structures like problems/systems may take new structures to accurately represent on a graph like the system layer graph, and while that is being identified, vacillating between an 'exploratory' state (to consider the possibility of new structures encountered in a problem/system) and a 'learning fitting' state (to integrate new learned info into the system), the system layer graph has an 'uncertainty structure' to capture the extra possible complexity (in the form of an interface structure network or system layer graph, containing more useful interface structures nearer to the origin)
      - derive useful structures by which structures reduce the most variation (as in 'capture the most complexity')
        - other useful structures for problem solving would include networks/trees, interfaces (and more specific input-output sequences & core components) as different structures to standardize representations of information in a way that is likely to be useful for problem-solving in general

    - identify which structures would enable finding relevant structures for a particular problem
      - example: for the 'find a prediction function' problem, this would take the form of 'predicting other variables from various sets of other variables in the variable set', because 'causal dependencies' are a particularly relevant structure for this problem format, and applying 'change' to the important structures of the problem space like 'variable' structures using various change types generated by interface analysis will help in identifying these relevant structures for solving the problem ('causal dependencies')
        - apply interface analysis to determine probabilities of dependence structures
          - apply the 'subset' structure to the 'variable set' structure
            - can one variable predict multiple other variables
            - which variables are necessary to predict the others
            - which variables can be removed without high impact on most of these subset predictions
          - apply the 'change position' structure to the 'variable positions' determined by 'variable type' 
            - can the dependent variable adjacently determine an independent variable
            - is one of the independent variables a possible output variable (multiple output variables)
          - apply the 'change position' and 'subset' structure changes
            - can the dependent variable adjacently determine a subset of independent variables
          - apply the 'sequence' structure
            - can the variables be framed in a sequence structure, where one set determines an interim set that determines an output set
          - apply filters of possible variable interactions based on:
            - probability: 
              - how many operations are necessary to convert one variable set into another, given that fewer operations implies a structure of truth in an optimized system
              - do variable interactions align on multiple interfaces
              - are variable interactions probable given patterns of other variable interactions
              - are variable interactions possible given known limits, requirements, and rules

    - apply solutions to a more complex problem than the problem like how predicting a value in an infinite sequence (or determining differences between infinite sequences or their functions) is more complex than predicting a dependent variable value in a polynomial function, so filtering a set of many (possibly infinitely many) different prediction functions can be solved with methods from a related but more complex problem

    - identify information that is an output of a solution that can be used to derive/find/generate the solution, rather than identifying info about the solution or solution inputs, to use as a different target when solving a problem
      - example: for the 'find a prediction function' problem, this would take the form of identifying solution output metrics such as proxy metrics to a prediction function (including area under curve, number of peaks, probability distribution of local averages, etc) that can be used to identify/determine/approximate the prediction function, and change the problem to predicting those function metrics instead of function parameters like constants/multipliers/exponents applied to variables

    - apply interface structures to problem space to determine other problem structures than those directly relevant to the original problem that can be used to infer solutions to the original problem
      - example: 
        - in the 'find a prediction function' problem, apply interface structures such as 'alternates' and 'adjacence' to problem space structures such as 'input variables', to infer adjacent variables of a data set, such as direct inputs & outputs of the original inputs, to find a set of connecting variables that can be further connected to the original outputs to predict, or to find out if the original inputs can be corrected in some way given these adjacent variables, or if adjacent variables to the original inputs can more easily be used to predict original outputs
        - another example is how 'cause' is a directly relevant structure to the 'find a prediction function' problem, which attempts to identify variable dependencies, dependencies being a causal structure
      - an alternate general variant of this would be to apply interface structures to identify structures like 'alternate' applied to general problem/solution structures, like 'alternate' function types (such as finding an alternate general function than find/derive/apply/generate) 

    - apply interface structures to determine what structures could produce a useful structure for solving a problem in a particular problem format (such as how a 'variable relationship' is useful for solving a problem formatted as a 'find a prediction function' problem) and whether those structures have structures of truth (such as whether they are interactive, exist, or are probable), after standardizing the original problem to a problem format (such as 'find a prediction function' problem) where such a problem structure (such as a 'variable relationship') could be used as a target to generate with interface structures
      - example: in the 'find a prediction function' problem, identify which structures could produce a 'exponential to integer or boolean' variable relationship, including such information as what degree of structures applied could connect the variable structures, and identify whether these structures are possible in a standard or specific system given how structures interact, such as a 'continuous input variable' being used as an 'activation signal' or 'type variable' in a standard system
      - another example is whether these structures can be connected using other interface structure connections, like whether the intent sequence of a structure sequence is possible/probable, thereby indicating with multi-interface alignment that the structural sequence is likelier to be true

    - identify & fulfill other problem-solving intents such as 'solve for relevant structures to solving the original problem' instead of 'solving the original problem'
      - standard problem formats (such as 'find a prediction function' or 'sort a sequence') where all problems can be solved with known methods once formatted to these standard problem formats are generally found with standard interface structures, such as how the 'prediction function' involves a standard function interface structure of a 'variable relationship' and how the 'sort a sequence' problem format involves a standard structural interface structure of a 'sequence', where these structures are 'relevant structures' to solving these problems
      - 'solving for relevant structures to solving a problem' is another problem-solving intent
      - other problem-solving intents can be identified by variables of problem-solving intents, such as which problem/solution structures & functions are involved, which interface structures are applied or fulfilled in the problem/solution structure interactions such as an input/output sequence between problem/solution structures that applies across various problem-solving intents, which problem/solution structures & formats are required by a problem-solving intent (such as a 'set of solutions'), and the degree to which & method in which the problem is solved by that intent

    - apply interface structures like concepts to a problem system and find useful structures like interaction structures of those applied interface structures as a way of finding structures that can be used to determine probability of solution success to apply as a filter of solutions
      - example: in the 'find a prediction function' problem, apply concept interface to find concepts like 'average', 'correlation' and 'causation', and apply useful structures to these applied interface structures to find structures of possible relevance as a solution filter like 'functions of data subsets that could represent correlations' and 'correlation functions that when averaged could produce the prediction function' and 'functions that would indicate causation vs. functions that would indicate correlation and filters of these function types', that can be used as 'prediction function' solution filters, given the possible meaning of these structures

    - apply structures of absolute certainty (like variables that can absolutely vary in all cases and requirements such as required variable limits or interactions) to use as starting point to reach structures of approximate or probable certainties
      - example: 
        - in the 'find a prediction function' problem, absolute certainties include 'some points may be due to randomness', 'some points may indicate change in the underlying variable interaction', etc, which indicate the absolute truth of a variable of the data set (randomness, change in interaction function) that may vary
      - from these absolute truths, approximate/probable truths connecting absolute truths that are otherwise unconnectible with absolute truths can be identified as more probable than other approximate/probable truths, having filtered out 'impossible truths' and 'improbable truths' implied or required by the 'absolute truths'

    - apply solutions that are known to be exactly incorrect/sub-optimal as a way of finding/deriving/generating errors to generate useful information to find correct/optimal solutions
      - example: if you applied a regression algorithm to a classification problem, you would identify a difference in the output regarding data type, alerting you to the fact that the data type is where the error is and is related to or is the component of the solution that needs to be changed

    - apply other useful structures than 'connections', 'sequences', or 'interactive components' applied to components of problem/solutions in fulfilling problem-solving intent functions, problem/solution core interaction functions, or other functions related to problem/solutions
      - example: rather than finding 'input-output sequences' to connect problem/solution formats/states or components available in the problem space, or finding 'interactive components' that can be chained together (starting from the 'core component' structure), apply other core structures to fulfill core interaction functions between problem/solution
        - finding structures that are by definition related to the 'connect' function, such as 'input-output sequences', are a useful structure by default in fulfilling/implementing that function, but other structures like 'sets' or 'combinations' can be usedul as well, indicating objects that are frequently or by definition found together, indicating an 'implicit' connection by adjacence/approximation rather than 'explicit' connection by match/equivalence in definition

    - apply useful structures (such as 'mix') to connect various solutions or solution-finding methods in a solution space, to useful structures (like 'components') to generate new solutions or solution-finding methods
      - example: for a problem with multiple generated/derived/found solutions or solution-finding methods like 'find a prediction function', apply the combination useful structure 'mix components' to switch various components of the solutions or solution-finding methods with alternate variants
        - for the 'find a prediction function' problem, this could take the form of switching error calculation methods or solution metrics
      - workflow fit: this is similar to the workflow of 'generating different solutions' or 'identifying variables of different solutions & generating solution variants with these variables', but is another way of achieving similar or equal intents using structures of useful structures

    - generating example solution-finding methods of varying specificity & results by applying various alternate solution automation workflows, then abstracting & parameterizing differences between examples and imputing missing differences that could generate new solution-finding methods without changing output of known examples
      - example: 
        - the difference between 'regression methods' lies in the 'error calculation metric & associated function'
        - the difference between 'regression' & 'finding prediction functions of subsets & merging their outputs by some weight assignment function' includes differences in the different percent of data input to each function, the difference in outputs of subset prediction functions, the difference-resolution/merging method of subset functions
        - these differences can be added as abstract variables like 'alternate input-output routes' & 'alternate inputs/outputs' or as specific variables like 'alternate data subset routes' & 'alternate data subsets'
        - these differences can be increased through combination with other interface structures than 'abstraction'
        - these identified variables can be used to calculate other methods not already identified in the variable interaction space
      - workflow fit: this workflow is similar to other workflows that apply 'alternate routes' to structures to find/derive/build a solution, with a specific focus on 'solution-finding methods' which can function as 'standalone interface queries' if they fit into a workflow that uses a 'standardization' step so standard solution-finding methods can be applied, or if the solution-finding methods are found/derived/generated in a specific problem space, so theyre already in a format that is relevant to the original problem

    - convert solutions from another format for an adjacent solution-finding method into a useful format for the original problem when applying another solution-finding method
      - example: for the 'find a prediction function' problem, apply solutions associated with 'classification' to solution-finding methods like 'regression'
        - meaning, for a given subset of the data set, use 'classification' to predict a value out of several discrete possible values, as calculated by various different averages of that subset of the data set, rather than finding the prediction function using standard regression methods for the entire data set
      - workflow fit: this is applying 'alternate routes' useful structure to the 'solution-finding method' solution structure, rather than the normal position of applying it to the 'problem/solution connecting functions' to find 'alternate problem/solution connecting functions'
      - interface query: the interface query for this workflow involves 'finding 'approximation structures' or 'generative structures' or 'prediction structures' of solutions' ('solution-finding methods'), rather than 'finding solutions', where the 'alternate' structure is applied to 'type' of 'solution-finding method' as well alternating the 'solution-finding method of solution-finding methods' (like 'a method to find regression-calculation functions') from the standard 'solution-finding method' (like a 'specific regression function to find the prediction function')
      - generalization: in general this method finds/derives/generates variables of the problem space (like 'alternate routes between inputs/outputs' such as: 'method of finding y from x' or 'method of finding all ys from all xs' or 'method of finding a subset of ys from a subset of xs' or 'method of finding ys from adjacent ys' and injects relevant useful structures (like how 'alternates' are useful for 'variables', as they both have a common structure of 'change')

  - add to problem/solution structures
    - definitions & other truth structures as symmetries

  - add to problem-solving intents
    - solve for relevant structures to solve a problem 

  - add to predictions
    - where you say that 'activity interacting with a neuron is relevant in its functionality': https://www.quantamagazine.org/how-computationally-complex-is-a-single-neuron-20210902/
    - relevance of intelligence (successful learning/thinking) & disease (unsuccessful learning/thinking/stressor-handling): https://www.quantamagazine.org/brain-cells-break-their-dna-to-learn-more-quickly-20210830/

  - add to science
    - 'sets of n particles' that act in aligned ways like:
      - 'components of a temporary local field/lattice having n points'
      - 'a connection created from equivalence structures' based on:
        - a 'homotopy equivalence between points'
        - a 'set of rotations/arrangements transforming points into a set of equivalent duplicates by definition-compliance, like the infinite cloning paradox solution'
    - 'ozone hole' was a specific structure that gave people one achievable goal to aim for so they could organize resources around achieving that goal
    - what ratio of gene copies is required to offset cancerous processes/responses without interfering with non-cancerous process
      - can junk dna be altered to fix sub-optimal ratios?
      - are some dna copies acting as 'placeholders' that prevent other copies from being made or prevent certain illnesses
      - how to 'create dependency' of cancer cells on a particular process/structure so when removed it can create cell death
      - drugs that response to 'high quantities of a signal' leave room for an error of 'missing low quantities' - an error where the signal is assigned a boolean and should be a continuous spectrum with a threshold that can re-create the original problem
      - how to target useful cells with growth-enhancing mechanisms like extra-chromosomal dna
      - are there genetic sequences that can offer protection against extra-chromosomal dna interactions temporarily while cancer cells are being targeted (like if a protective sequence is spaced at intervals that disincentivize interactions with extra-chromosomal dna, such as when extra-chromosomal dna merges with a chromosome)
      - how to account for probability of a known possible error (like mutations compared against a mutated rather than healthy reference genome) and adjust data accordingly
      - if a structure continues to develop, that means it represents a 'stability point' that is incentivized - which incentives of extra-chromosomal can be altered without harming other systems?
      - if a package of growth/maximizing genes continues to develop bc of 'survival bias' (the genes that are useful for survival are protected & maintained & replicated), can incentives for dna repair/defense genes be created/increased to compete with these cancerous extra-chromosomal gene structures - why is the 'survival bias' of cancerous cells & cancerous mutations stronger than other cells/genes/mutations' bias - just bc theyre more useful for cancerous intents like 'unrestricted growth' or bc of some other cause like 'genetic adjacence' or 'genetic functionality like jumping/coordination', or bc of the fact that growth factors promote themselves by the definition of their own functionality, or bc these gene/mutation types like enhancement/growth have aligning intents like 'promotion' so theyre often found together?
      - can antibodies against high-growth/maximizing extra-chromosomal gene structures be developed or can vaccines be developed to produce antibodies against them?
        - https://cen.acs.org/pharmaceuticals/oncology/curious-DNA-circles-make-treating/98/i40
    - a generally useful 'quantum algorithm' is optimally implemented as a network where any node is equally accessible regardless of steps separating them or other definition of 'distance' (each node pair can be entangled on demand to get calculations/output from that node to reduce overall repeated function calls of the network)

  - add to automation tech
    - more devs converting to 'task description writers' creating standardized task descriptions & identifying repeated work, automation opportunities & other high-level semantic tasks is a better career once programming turns into a 'task bidding freelance market' where AI coders frequently outcompete devs
    - organized & structured code by 'core component' (function input/output variables, input/output differences, patterns, core function-calling function (high-level functions, so building core functions is the source of changes in functionality of high-level functions), uses, logical derivation connections (equivalence, requirements, dependencies) as logic trees, logically directed graphs, changes/states, function, reference, relative position of thread/process/function, limits/requirements, dependencies, data, prior operations (cache), differentiating attributes, context, mix/iteration/cycles/sequence, type, example, value)
    - 'infinite compression' as a combination of infinite sequences & numerical references to those sequences as parameters (similar to rainbow tables), where infinite random sequence subsets can be recursively compressed into hashes, where infinite sequences can be represented as a sum of sequences (infinite random adjacent base sequence + an infinite geometric sequence to generate it), or a sequence & a set of vectors indicating changes to generate the sequence from an infinite random adjacent base sequence

  - additional attributes (metadata) of interfaces include attributes like validity & relevance, which vary absolutely across other interfaces, and contextually across interface queries/solution automation workflows & problems being solved with them

  - additional examples of error causes
      - example of priorities leading to errors:
        - the 'selfish' perspective prioritizes simple, structural objects (like thos within an adjacent range), which is why 'selfish' programs can only see & act on those things rather than producing solutions that can be used for many problems, even ones they dont have
      - example of how a 'lack of errors' in a closed system can produce an error in system-level interactions, like a cross-system merge

  - add to stats
    - give example of visualizing the stats object connections in a stat insight rule
      - add example of how to derive the rule 'choose algorithm with low bias/high variance for large data sets'
        - use 'random forest' bagging method to reduce high variance
        - high variance is a problem when a sample data set is not representative of the population, producing accuracy for the training data subsets & errors for all other data subsets
          - if a data set is relatively large in relation to the population, the 'sample data set is not representative of the population' is less likely to be a relevant problem
          - 'variance can only be reduced by averaging over multiple observations, which inherently means using information from a larger region'
        - high bias is a problem when sample data sets differ from the population mean by a lot, producing errors for most samples unless they happen to be represented by the general model
          - if a data set is relatively large in relation to the population, the 'sample data sets differ from the population mean' is less likely to be a relevant problem, so the large data set can be used as a basis for the general model
          - 'bias is reduced using local information'
          - 'If training set is small, high bias / low variance models (e.g. Naive Bayes) tend to perform better because they are less likely to be overfit.'
          - 'If training set is large, low bias / high variance models (e.g. Logistic Regression) tend to perform better because they can reflect more complex relationships.'

  - give examples of why other tech solutions are insufficient
    - why ml would be inaccurate on math problems (like 'predict convergence value for an infinite series, based on training data of infinite series param input & convergence value output')
      - regularization, bias/noise changes, and other feature changes for intents like 'generalization' or 'feature selection' may add to inaccuracies
      - the structures that can be composed by various function/node/weight/threshold unit combinations may not be capable of the math operations involved in transforming one value/format into another
      - the math problem has emergent structures that are not visible in the training data set, for example a point where the relationship function between the inputs/outputs changes (like an asymptote or a maximum)
      - the training data may reflect patterns that are simpler for the network to compose/filter/reduce, without enough data points representing more complexity (a parabola instead of a hyperbolic function)
        - the network may be good at providing filter/compose/reduce functionality, but not other functionality like 'converge' or 'select between similar alternatives'
        - the support for 'multiple alternative input-output routes' in networks may add too much complexity in reducing its ability to specify a particular answer, with built-in tools to find averages or other representative values rather than selecting one value over another
        - for some functions, there are too many inputs that could produce the labeled output, and those inputs may be too similar to differentiate/specify (very similar functions can produce the same area)
      - neural networks are primarily good at certain data transformations, like where the problem input can be converted into the solution output by a system of linear equations or matrix operations supported by the network, where coefficients/weights of versions of function components (inputs & interim weighted combinations of inputs) are the required output of the network, and those operations may not adjacently handle operations like summing an infinite series, which might require specialized structures like memory/state-embedding or online learning if those structures cant be produced by adding additional layers to a network
      - each feature of the infinite series input would contribute to the output, but a neural network is designed to learn weights of features, implying that some are less important, and the contribution of terms in a series to the convergence value can usually be determined by its size/position
      - if parameters of the series are used instead of the series, that is a low feature space compared to the input features available in other applications of neural networks
      - the operations in some formulas do not produce reliable learnable patterns (some structures of randomness would counteract the ability of the network to learn a function)
      - the inputs dont provide enough data for continuously differentiable spaces where methods like gradient descent can be applied, given that math functions often cover a wide range of possible inputs, and a training data set is unlikely to be representative of enough examples to fill in the gaps in this space

    - VAR & reservoir computing's random sample of matrixes is inadequate bc the randomness is an attempt to identify 'very different' difference types, without generalizing that into a unique set of differences that are likely to be useful (like differences distributed across pairs of variables, so many random samples dont represent difference types in the same pair)
    - regression is insufficient even if its a good temporary solution if you dont have other resources than the data set, bc its conclusion/outputs (in the form of the regression function) can have the opposite meaning 'random noise' as the intended meaning 'causal relationships'
    - statistics in general is built on the insight that 'probability is associated with certainty/truth', but it ignores other certainty structures like structures that are more useful than patterns/probability, such as definitions, concepts, meaning/understanding/integration/organization, cause, inevitability/requirement, determination/generation/description structures, functions of varying interaction levels, etc
    - machine-learning can have the opposite functionality given extreme data values or update functionality manipulation (to train it to give the wrong answer in its online learning functionality), as well as other exploits from interactions of the algorithm, network, parameters, emergent structures, & data, and it is not built on understanding
      - 'one-degree connection structures' which are present in 'foundation models' are incapable of capturing multi-degree connection structures like sequences/chains or structures of connections like trees/networks/groups, even though other structures can be formatted as core structures like 'connections', it doesnt mean a one-degree connection network will capture them, or that a network of connections is the most optimal structure for that info given its usage
      - machine-learning based on neuroscience leaves out other brain interfaces like psychology, chemistry, & language
        - a psychologist might interpret a thought as 'an emotional reaction to a chemical stimulus that retrieved a memory'
        - a neuroscientist might interpret a thought as 'a response to electrical stimulus given weighted connections between neurons that previously handled that stimulus'
        - a linguist might interpret a thought as 'a deviation from a previous phrase that captured an experience to handle a change to that experience'
        - a chemist might interpret a thought as 'a result of scaled electron dynamics in response to a chemical'
        - a biologist might interpret a thought as 'a useful way to produce serotonin to offset a signal from the gut'
      - https://thegradient.pub/has-ai-found-a-new-foundation/

  - add to problem/solution structures

    - add example of how to derive 'apply differences to inputs to see if they can change the output to see if the solution is true'

    - add example of impact of methods on various network types given the differences in method/network structures & include assumptions

    - give example of how structures could have been derived (symmetry, isomorphism/interchangeability as common important objects to derive an interface, alternative interfaces to solve a problem) from another direction

    - derive logic types that would be necessary to complete the logic interface & give examples of logical object interactions

    - a 'find a solution' function should be able to be converted into a 'generate a solution' function & other functions like 'test a solution' & 'apply a solution', bc as the brain learns, it can generate solutions on demand once understanding develops

    - give example of identifying meaning of emergent structures (like 'weight trees' in neural networks)

    - organize workflows using useful structures as being on the meaning interface, where useful structures from other interfaces overlap & connect with the meaning interface

    - write interface queries to generate each workflow

    - give examples of how each workflow can be applied to various standard problems (find a prediction function, sorting problem, ml configuration/algorithm-design problem), which can be used as a data source to derive implementation logic/interface queries to generate solutions

    - finish math mapping so you can find other useful/solution structures (interaction space as convolution, core functions as basis vectors, etc)

    - basic solution automation workflows
      - trial & error
        - use a rules/solution database & look up the answer (try known solutions)
        - apply machine-learning with various configurations (try known/probable configurations)
        - apply rules from other systems to see if they work in another system (try other known systems)
        - mix & match solution components/variants (try known solutions)
      - reverse engineering
      - break problem into sub-problems & merge sub-solutions

    - example of format/intent matching
      - formatting a 'tree' as a 'set of overlapping sequences with overlaps in either inputs or outputs' so functions can be formatted for different intents like in 'parallel processing'

    - add to input structures
      - input variable/trigger/requirement/component

    - add to output structures
      - limits on what a structure can be used to create
      - similarities/differences to inputs (inputs change can be preserved in outputs)

    - identify new interactions/structures
      - trying structures of structures that havent been tried yet (like how new words evolve as a 'combination' of other words to describe new experiences that are similar to both combined words)

    - 'testing/simulation' involves querying for related rules (like how 'gravity' rules are related to 'motion' rules so any change involving motion should have a 'gravity rule check' applied as a filter) & checking if they apply to relevant components (like how specific components are involved in 'motion', like 'energy', 'motion restrictions', 'motion functions', 'motion triggers/inputs/components')
      - this is an important process for checking if a structure is valid/consistent in a system, which is a useful function
      - this is different from basic testing, which is where a function is applied and the output is checked against an expected value, bc it involves testing for validity/consistency in a system context where the change is being applied

    - examine interaction space of tech stack layers (ml models, algorithms, data, apps, bugs, os, chips) as a source of new errors
      - example: 
        - ai applied to design chips
        - chips with data erasure bugs that exacerbate os data erasure bugs
        - chip designs that produce error types for various ai models/algorithms/parameters
        - how 'gpus are known to be better at building ai models'

  - add to error-finding methods
    - identifying & generating known useful structures like 'symmetries', 'variables', 'subsets', 'interchangeable alternatives', 'maximally different inputs' & 'bases' & 'type/phase shift thresholds'
      - identifying & generating combination structures of useful structures like 'maximally different values around bases'
    - identifying gaps in known useful structures explaining data points (where data points arent explained by those known structures) & generating inputs in those gaps other than those data points

  - add to conceptual math
    - example of a conceptual math operation that builds a boundary structure leaving an inevitability of a matching concept (numbers) filling the structure
      - the concepts of 'missing', 'multiple/more', 'unit', 'type', 'identifiable as similar/equal/different' and 'difference in amount' allow for/require/build the concept of 'numbers'
      - also functions like 'compare' or 'reduce' or 'expand' require the concept of 'numbers' when comparing objects of that data type or objects having a quantifiable attribute

  - add to causation variables
    - ability to change (if a variable cant be changed, it is less causative for problem-solving intents)

  - add to info problems
    - this manipulates:
      - audience objects:
        - ego
        - assumptions (about patterns, what you would notice/figure out)
        - attention
        - feelings 'opposite' to logic (safety, confusion)
      - using objects like distractions, activations, distortions, core structures like combinations/sequences, complexity, patterns, input/output similarities/alternatives (complex/simple implementations), logic, patterns of logic, logic avoidance, jokes
      - to produce:
        - errors in expectations (in order for the audience to expect y, they have to have assumption x, as x is an input to y)
      - these important variables can be identified by identifying the inputs to these objects
        - what 'input' is 'required' for this expectation error to happen? (an assumption)
      - https://www.smithsonianmag.com/arts-culture/teller-reveals-his-secrets-100744801/?all&no-ist

  - when is it optimal to store a mixed structure of varying specificity (like a type, intent, cause & a specific example)
      - when there are potential uncertainties to resolve, like whether the example represents a new error, type, or variable, bc the example doesnt fit known structures

  - all primary interfaces can act like the problem-solving interface (start solving problem from the concept or structure interface and integrate all info back into that interface & frame the solution in terms of that interface) but the meaning interface (the interface interface) is the most powerful

  - apply concepts to structures

    - concept of attention in structures
      - mixed interim high-variation & high-similarity structures tend to maximize attention
    - examine error type of conflating intent & requirement
    - consciousness as choice to move between neural nodes (rather than being directed) required:
      - the development of alternative node paths performing equal/similar functions, requiring:
        - the development of excess resources, delaying required decision time (making immediate decision unnecessary, avoiding a forced decision), requiring:
          - the existence & application of previous efficiencies & functions for alternative evaluation, energy storage, storage-checking, & energy requirement-identifying
      - the cause could be framed as structures such as an 'efficiency stack' or 'energy maintenance functions' or 'alternative options' or 'navigation/motion control' or 'lack of requirement/need'
    - examine similarity (alignment/overlap) structures between: 
      - extremely different components (when an error type is an incentive or a function used for other intents) 
        - when the solution format of some problem has similarities to the error type, like when you need randomness so errors generating randomness are a possible function to use for that intent
        - contradictory/opposite components (have some metric in common, with opposite values)
    - examine the distortion vector paths that adjacently decompose a data set into a prediction function from a base point/function set

  - add examples of:
    - mapping to structures & identifying contradictions its safe to ignore for applying a structure
    - system/object/rule/type change patterns
    - query examples for use cases like:
      - lack of information stored (match problem of type 'information lack' with interface query 'check pattern interface for similar patterns')
      - query problem breakdown & integration diagram
      - calculating various different problem breakdown strategies first before executing normal query-building logic for each
    - example of how to predict most interactive/causal concepts in a system


## diagram
  
  - diagrams:
    - error types
    - network of formats
    - efficiencies
    - alternate interfaces (information = combination of structure, potential, change or structure, cause or structure, system)
    - chart type: overlaying multiple 2-dimension variable comparisons to identify common shapes of variable connections (density of points added with a visible attribute like more opacity)
    - structures of emergence
      - example: 1-1 input/output relationship up an interaction layer, where extra resources that dont dissolve immediately on the higher interaction layer aggregate & form core structures like combinations, where interactions between combinations & sequences have different dynamics than the individual output interacting with other individual outputs
    - how emergent functionality/attributes come from interaction structures (sequences & layers)
    - intent-matching
    - interface overflow (to sub-interfaces), interface foundation
    - workflow
      - function to identify relevance filter ('functions', 'required') from a problem_step ('find incentives') for a problem definition, to modify problem_steps with extra functions/attributes ('change_position') to be more specific to the problem definition ('find_incentives_to_change_position') for problem_steps involving 'incentives', so you know to use the function_name to modify the problem step if it's between the type 'functions' and the object searched for 'incentives'
    - conceptual math interface query
      - use lattice multiplication as standard example, other than core operations (add/multiply mapped to language, concepts like irreversibility/asymmetry mapped to math)
    - interface conversion, matching, starting point selection (applying structure, checking if relevant information is found)
    - sub-functions of core functions with distortions (identify/filter of find)
    - dimension links higher than 3d that are depictable in the same network space
      - should show variables that impact other variables, the change rates of these relationships
      - overall impact should be calculatable from these relationships
      - should show similar movements for correlated variables
      - should show skippable/derivable variables (variables that can be resolved later than they normally are)
      - should show meta forces for overall trends in change rules (direction of combined variable forces)
      - should show limits of measurability & threshold metrics
    - specific concepts, core functions, concept operations (combine, collide, connect, merge, apply), ethical shapes
        - variable accretion patterns (how an object becomes influenced by a new variable, complex system interaction patterns, etc)
        - potential matrix to display the concept
          - map parameter sets to potential matrix shapes 
        - cause (shapes & ambiguity), concept (evolution of concepts, networks, distortion functions)
        - argument
      - system layer diagram for each interface to allow specification of core interfaces & other interface layers (interface interface)
        - system layer diagram for structures to include layers of structures 
          (beyond core structures like curves, to include n-degree structures like a wave, as well as semantic output structures like a key, crossing the layer that generates info structures like an insight, a probability, etc)
    - map variable structures to prediction potential for problem types, given ratio of equivalent alternate signals
    - vertex variable structures
      - quantum physics, prediction/derivation tools, build automation tools, testing tools, learning/adaptation tools, system rules, computation power are all vertex variables of information, since they can generate/derive/find information
        - which structure (sequence, network, set, or cycle) of vertex variables is most efficient
    - core component attributes: identify any missing attributes/functions that cant be reduced further
    - absolute reference connections with metadata structures like networks/paths


# content/config

    - import insight history data to identify insight paths (info insight paths like 'lie => joke => distortion => insight', system insight paths like 'three core functions + combine function with this definition + n distortions to nearest hub')
    - define default & core objects necessary for system to function (out of the box, rather than minimal config necessary to derive other system components & assemble)
      - add default functions to solve common problem types
      - alternate utility function implementations have variation potential in the exact operations used to achieve the function intents, but there are requirements in which definitions these functions use because they are inherent to the system. For example, the embodiment may use a specific definition of an attribute (standardized to a set of filters) in order to build the attribute-identification function using a set of filters - but the general attribute definition is still partially identifyd in its initial version by requirements specified in the documentation, such as a set of core attribute types (input, output, function parameter, abstract, descriptive, identifying, differentiating, variable, constant), the definition of a function, and the definition of conversion functions between standard formats.
    - systematize definitions of info objects
      - include analysis that produces relationships of core objects like opposites to their relevant forms (anti-symmetry) in addition to permuted object states (asymmetry), such as an anti-strategy, anti-information, anti-pattern
      - organize certainty (info) vs. uncertainty objects (potential, risk, probability)
      - make doc to store insight paths, counterintuitive functions, hidden costs, counterexamples, phase shift triggers
      - add technicality, synchronization, bias, counterintuition, & certainty objects leading to inevitable collisions
        - error of the collision of compounding forces producing a phase shift
        - lack of attention in one driver and false panic in a second driver leading to a car crash given the bases where their processes originate
      - define alignment on interfaces (compounding, coordinating, parallel, similar, etc)
      - add core info objects (core strategies, core assumptions) so you can make a network of graphs for a system
    - add function logic for:
      - concept analysis:
        - how new concepts (gaps in network rules) evolve once structure is applied to prior concepts 
      - interface analysis:
        - limitations of interfaces & how to derive them
        - how rules develop on stability & how foundations are connected & destroyed
        - explainability as a space limited by derivable attributes from data set & cross-system similarity
        - vertex definition & give examples (as an intersection/combination of interface variables, such as determining/description(compressing)/generative/causative/derivation variables), around which change develops
      - change analysis:
        - generated object change types
          - constant to variable
          - variable to removal of assumption in variable type/data type
    - examine implementing your solution type (constructing structures (made of boundary/filter/resource sets) to produce substances like antibodies, using bio system stressors)
    - resolve & merge definitions into docs/tasks/implementation/constants/definitions.json
    - update links
