# to do

  - finish processes:
      
      - finish applying systematization of solution automation
      
      - finish interface analysis of physics & other interfaces to identify other useful components like efficiencies, incentives, trade-offs, closed systems
      
      - finish config
        - add useful structures & questions from index.md to systematize_solution_automation.md
        - for each workflow involving useful structures, indicate an actual interface query example connecting the workflow with the example
        - useful structures
            - identify filters for useful structures like definition routes
            - the system structure format where the maximum number of interface queries can be executed structurally, with minimal conversions required? is it a merged format of variable/function/concept/cause network graphs, or system state networks, or a set of variable subset graphs, or differences visualized as vectors, or input-output sequence visualizations, or a network with all identifiable interface components visualized
            - interface queries optimizing finding useful interface component filters
            - useful perspectives/specific interfaces
              - useful to think of prediction functions as generative functions to select the variable interactions that are most likely
            - useful solution filters to apply in functions
            - aligning/balancing structures, to solve problems like 'a balance position of structures producing errors when unbalanced'
            - questions formatted as a disconnection between components like causal positions, paths, directions
            - subset indexes of an interface useful for solving most problems (structure indexed by metadata like problems solvable, fitting systems, interactive structures, supported intents)
            - ml structures with supported intents & solution success causes
            - most valuable interface queries & workflows
              - find the sets of differences/dependencies/formats/errors & other useful structures that are the most valuable in a particular structure like a sequence to solve a problem
                    - interface component definition routes
            - useful component/sub-structures of interface queries (interface components, interaction rules, cross-interface interactions, generative functions)
            - useful interface components (like abstract) of useful interface components
              - core interaction functions of core interaction functions
          - creating useful structures
            - organize automating useful structures like combinations of concepts such as "format sequence", "solution automation workflow", "insight path", "reverse-engineer solution from problem requirements or opposite structures", "connect problem & solution"
            - convert structural queries to insight paths
              - alignments present in security innovations (like alignment in inputs like keys)
              - source of rule development as structures of conflict between forced interactions like change causes & constant structures like limits
                - incomplete inevitability of interaction as a decision structure
              - group device history authentication: authenticate credit card by proximity to cell phone & continuity applied to user usage history pattern
            - functionalize insight paths & integrate functions in optimized program with parameters to select function subset & structure for input problem
        - default config
          - write some default interface queries to use until logic is written
      
      - finish scripts
        - create compilation script to compile code/config into a network graph on every change
          - add support for standardizing equivalent synonyms
            - add conversion to standard vocab

  - integrate logic
      - integrate objects/.md text with interface implementations
      - integrate archive_notes/finder_info/functions
      - organize interface analysis logic definitions
        - organize functions in problem/interface definitions, before organizing functions in implementations/*
      - integrate problem_solving_matching.md
      - integrate find/apply/build/derive logic from system_analysis/ & maps/defs.json
      - separate interface analysis logic into implementation/functions (functions dont need unique info)
      - add functions from workflows & analysis (to do list, questions answered, problems solved, interface definition & functions) as files in functions/ folder
        - organize into primary core functions & list sample parameters (like objects to identify for the identify function)
      - integrate rules from diagrams in patent applications to relevant documents
      - organize function logic (interface query design logic)
        - document default static config objects that are inputs to core objects (like functions & concepts)
          - core functions like 'change', with locked objects which should be generated as inputs to other functions and should not be removed bc they enable other rules & core objects
            - a 'check for errors' function
            - a concept of 'self-correction/optimization'
          - these locked objects can be used to generate rule-generating/deriving/finding structures, by forming an initial structure of locked objects and filling that structure with conditional & changeable structures
            - these rule-generating/deriving/finding structures can be used as solution automation workflows
        - design an optimal sorting structure for general interface queries to apply to problems manually
        - list interface selection (based on inputs like available APIs/data sets/definitions)
        - problem interface structures: solution constraints/metrics, problem space variables, available functions, useful formats/structures
        - function to translate interface query logic into interface language (combination of core functions (find/build) & other core components)
        - function-usage-intent::output or demand::supply combination/merging/building/matching functions (alternatively formatted as a solution-finding query for a problem or lack-resource matching function) as an alternative solution to ads
        - decision points (required/optional resolution of variables to constants, as in selecting a variable value)
          - identify when a method & data set can be identifyd to be capable of deriving the answer to a prediction function problem
        - alternative intent coordination & compatability of metrics
          - calculating interactivity by coordinating/adjacent/convertible structures
        - check reduced language components for any other useful functions (what terms cant be adjacently, clearly & accurately framed in terms youve defined) for completeness

  - integrate examples

      - index examples so they can be queried more structurally when implementing functions

      - move examples from:
        
        - drinkme/examples_from_faq.md
          - check other examples of high-value use cases (other than identifying important concepts) from faq:
            - identifying the important base to frame changes on (identifying new interfaces)
            - identifying the right interaction level to focus on (identifying the change-maximizing layer of a system to examine a particular relationship)
            - identifying the right perspective to filter with (like 'identifying whether the legal/scientific/progressive perspective is most useful for an intent')
            - identifying the right context/position for an object (derive context when it's missing or fit an object to a system)
            - identifying the most causative function set (like identifying core functions, or the most misused functionss, or the most change-causing functions)
            - identifying important differentiating types (like function types indexed by intent & structure types, like boundary/change functions)        
        
        - patent implementation_examples
          - identify any examples missing from patents in docs/tasks once examples are organized

        - specific examples from specific_problem_analysis
          - example of permuting assumption: "reports of power consumption have to be exact measurements" 
            - a temperature monitor sensitive to a hundredth of a degree might provide similar but non-specific power reporting for important/extreme usage patterns without revealing such specific information as that which could infer exact operations being done, bc the interval of temperature measurements allows for greater variation in calculations that could explain it
          - example of using set theory in query operations:
            - edges as core organizing/formatting operations (find/apply) & interfaces (connecting/explanatory concepts/functions)
              - https://en.wikipedia.org/wiki/Hypergraph
          - example of structural version of solution difference from original solution: 
              - this is like using a pair of connected lines at different angles to connect two points (multiplying alternate multiplier pairs to create a product), where summing the line lengths produces an equivalence, so different solutions would look like differently angled triangles connecting the two points
                - https://www.popularmechanics.com/science/math/a30152083/solve-quadratic-equations
          - examples of identifying vertex variables
              - general vertex variables: topic, origin/destination, reason/cause/point/intent, errors, variables, types
              - comedy vertex variables: sincerity, stupidity, stakes, tension-resolution/expectation-subverting pattern variation
              - music vertex variables: tone, tension-resolution/expectation-subverting pattern variation, lyrics
              - optimization metric vertex variables: solution metric patterns (what other solutions optimize for, to identify optimization metrics to apply)
          - example of resolving a conflict between structure/limits using a structural similarity between a structure (gradient of function) & its container/limits (gradient of constraints)
              - https://en.wikipedia.org/wiki/Lagrange_multiplier
              - also an example of a solution space (the whole function is the solution space of possible minima/maxima) and a filter applied to it (constraint)


## examples

  - why problems will always have new variables determining how solution automation workflows can be generated
    - problems are a 'lack of available structures that can fix the problematic structures', requiring new structures to be found, built, or derived to solve those problems
    - by definition, problems will always change from previous problems
    - therefore problems are a structural source of new change types
    - new change types map to new variables describing those change types
    - new variables may not be adjacently buildable, derivable, or findable with existing structures, leading to a requirement to find/build/derive new structures to structure those new problem variables
    - this may lead to new values of existing variables like new problem-solving intents or new problem/solution core interaction functions, or other new structures that may be representable as new variables, bc they exist on another interface where problems & solutions interact, such as new variables regarding a new function type that can be used for problem/solution interactions
    - even if a problem is static, there will always be new ways to solve it because of the inherent variation captured in the concept of 'problem', and these solution-finding methods will always have new interfaces or other structures which can be used to frame them
    - there will always be new structures that are adjacent to available tools to fulfill interaction functions (like 'connect') between given new change types generating new interaction functions, and new interaction levels to reduce a system to in terms of finding core components of the system, new structures of errors from new change types, and possibly new primary interfaces created by new interactive change types and new useful structures to connect other new structures

  - why understanding is usually a better option than pattern-guessing methods like prediction function-finding functions
    - understanding can be used more effectively to derive a 'guessing method' than the opposite
    - understanding is more generally applicable & can fulfill more functions than finding/deriving/generating a particular 'guessing method'
    - understanding is less dependent on data such as context data
    - understanding has compounding value, where guessing methods usually offer temporary & otherwise conditional value while & where the original prediction function is accurate
    - understanding can find/derive/generate error structures of a particular guessing method
    - understanding is better at guessing than most guessing methods in most cases (except where a guessing method handles new contradictory information, a new problem type, a new change type, etc better than existing understanding)
    - understanding, learning, simulation, organization, prediction, testing are interchangeable functions with varying advantages but generally can replace each other

  - add to govt
    - ai to guide leadership decisionns to avoid takeover/war/anti-democracy conditions given info like 'trust structures'
    - democracy errors include:
      - alternate illegitimate routes to power like buying info goods like opinions, usage & user bases & user compliance/dependence, & artificially manipulating prices with information market manipulation, voter free time, job market info
    - 'excess regulation' errors include:
      - punishing successful solution-finders (by de-funding through taxes on profits)
      - regulations that 'take away or invalidate property or otherwise violate property rights' prevent people from benefitting from their labor
    - 'lack of regulation' errors include:
      - rewards those who are best at 'finding loopholes to exploit' rather than best at 'solving problems'
      - exploitation without accountability
        - companies that exploit customers can liquidate their remaining assets to build solutions for those customers
          - example: colleges that charge exploitative fees can liquidate assets to build useful AI models for banks that gave student loans
      - making people fight to survive (without extreme advantages like 'information' or its proxies like 'education to derive information' or 'internet access to find information') so they frequently see crime as the best option likeliest to help them survive
        - solution: creating 'organization' to make basic goods affordable, or 'artificially creating jobs' instead of organizing
        - example of an 'organization' structure:
          - if a supply chain is optimized (where 'optimal states' include such states as 'local supplies are distributed locally (such as by 3d printers that use cheap resources like sun/air/dirt as input'), basic goods will be cheaper
          - this is a case where 'labor & other resources' are 'organized' into an optimal 'supply chain' structure
        - other examples of 'organization' structures include 'location', 'language', 'communities', 'info tech' (specifically 'databases/apps', 'internet access/search', 'cell phones'), all of which can increase the probability of other organization structures occurring, like 'markets' and 'coordination' and 'companies'
        - these can be created by creating structures of 'interactivity' that enable organization structures (having a 'language in common' or 'good translation tools' allows 'interactivity' to enable 'markets')
      - lack of education or its interchangeables, which results in other errors like 'not identifying interchangeable inputs to requirements (basic goods)' (such as how 'independence', 'power', 'love', 'happiness', 'understanding', 'intelligence', 'information', 'solutions', 'potential' can be interchangeable alternates as they can be different formats of the same structure which is 'freedom' as in 'ability to adapt, as in self-correct')
    - companies exist to solve customers' problems, so if there's a problem people can & are willing to pay to solve, companies will solve it, so all thats required is to tell them what consumers want at what prices and what costs are preventing govt budget from being optimized for solving unprofitable problems, & the products will be built & tech will be built to reduce those costs, rather than taxing to create govt agencies to slowly & inadequately prevent the same group or corporate loophole-exploitations that can be identified by algorithms, exploitations which are symptoms of an inefficient system that hasnt invested in reducing its primary costs like 'health care' or 'form processing' yet, rather than unique exploitations that requires budget to solve inadequately when reducing primary costs would invalidate the need for these exploitations by group or corporate entities
      - example: 'tracking assets' is only necessary if assets are scarce & unique, and 'regulating the health insurance industry' is only necessary if health care costs are unpredictable, extremely variable, & subject to randomness, which they dont have to be if costs are reduced 
    - 'actively preventing their money from ending poverty' (with tax avoidance, not trying to end poverty, not trying to be sufficiently different from people who failed to end poverty, etc) is a sub-optimal (inefficient) use of money, proving that people who could end poverty (either individually or as a small group) without negatively impacting their own lives and dont do so, are people who cant use their own money in an optimal way, indicating that their success was almost a complete accident plus a little hard work such as 'filling out forms' or 'making a pitch to investors' or 'hiring people'
      - if their success wasnt an accident, they will be able to replicate it bc theyll understand how to become successful and will be happy to share that knowledge and will have rules they follow to achieve that goal (creating huge value out of almost nothing) and will be happy to do so for a government or other entity trying to solve a big problem, otherwise their success was an accident and they dont deserve to keep the outputs of that success (just like how I claim I can find a new way to automate problem-solving in a few minutes and I regularly prove the legitimacy of that claim of my value, they also need to prove they deserve their claimed value on a regular basis so it's believable, by demonstrating how they didnt need luck but rather their own work/intelligence to make good decisions that few others would have made in the same position, indicating that these are their identities differentiating them from other people, rather than an out-of-character decision, and teaching other people how to create huge value out of almost nothing, and making sure other people succeed at doing so, and regularly executing their process to create huge value out of almost nothing to solve important problems)
      - poverty negatively impacts these individuals who could easily end poverty, given that theyre constantly harassed about it by similarly selfish & stupid people, but they try to maintain poverty instead bc its cruel and they prefer cruelty
      - these individuals dont try to teach people how to use their money efficiently or figure out how to do so themselves (teach them to 'apply interface analysis' or even just how to apply one interface such as logic), which would solve the problem of 'their money being used inefficiently', so its possible they dont want to solve that problem
      - these individuals are not being offered protection by some powerful entity like a government if they do give most of their money away, after being criticized for many years, which has made them afraid of non-rich people, and should be offered things like some immunity & protection by governments & then they need to see that promise honored so that someone is safer after giving their money away rather than more vulnerable - there have to be rewards in terms of things they care about, such as more customers/profits - if they get richer by ending poverty bc everyone wants to buy the products made by a company that ended poverty, thats a sufficient reason to justify doing so, so a company that struggles to make advances in technology could get customers another way, by ending poverty which would improve their brand beyond any possible attack
      - if you told these companies that their customers/users really want to end poverty or really want cheaper medicine (which would indirectly end poverty) or really want to buy & donate cheap phones to poor people so every person at least has a cell phone so they can find the resources they need, these companies would find a way to create that product (the product being cheaper medicine, an easy way to buy phones for poor people, etc) & offer it, then market that product until it was used & successful at fulfilling the demand from consumers/users, bc thats the whole reason companies exist (to create value for customers), and if 'cheaper medicine' has value to consumers, they will create it - the tech companies are in the best position to do that so tell tech companies that people really want to end poverty and theyll find ways to create that value if the market exists (if enough consumers want it and are willing to help them pay or otherwise create value for it)
      - 'open-source accounting' takes places every time you buy something on amazon and you see the shipping costs added to your total, which you can compare with the comparable usps shipping costs & cost of finding the seller/product without Amazon, which I notice people who could individually end poverty by buying enough phones/water/food/medicine/generators to end poverty are not doing, possibly to avoid giving Bezos/Amazon any credit, so lack of transparency in how charities use money isnt good enough because theres another charity offering full transparency that ensures donations get to the right person, which is Amazon - all thats missing are the addresses to send these packages to and a coordinated effort (of large groups with small resources or a small group with large resources or both) to buy & activate those transactions
    - changes that can improve markets, laws, & govt interactions
      - 'spend govt tax money or excess corporate profits on hiring people to solve large govt/community/company organizational & social problems that add permanent value and are one-time problems (where once the solution is found, it can be deployed repeatedly and new problems can be solved instead) problems such as:
        - reduce their costs (automate tasks), unpaid debts, sub-optimal markets that are over/under-monopolized, find low-cost, high-value products that can be re-sold without modification (ad space, subscriptions/queries of an API, charger cables/headphones/cases, t-shirts/nft's/posters, medicines) or products that have a large probable user base to generate research funding for companies/govts, fix supply chains, high tax rates, allocating basic goods quickly at low cost, disincentivizing crime, fixing mental health problems, reducing unimportant jobs like re-building the same apps repeatedly & allocating labor to more important tasks, ending tribal feuds by similarizing ('humanizing') sides & physical separation, customized education for neurodiverse learners with different experience/interests, enabling vulnerable groups to report crimes or request help with cell phones, inventing & building crime prevention technology, over/under-spending, mismatched cause/responsibilities, unequal laws/opportunities, sub-optimal technology like biased algorithms, repeated work like govt websites, expensive medicine, deploying solutions once a decision is made about which solution to apply, design experiments to find solutions faster, coming up with new ideas to solve problems, lack of oversight/transparency, fixing corruption, preventing dictatorships & rent-seeking
        - alternatively, hire them to automate solving those problems (my invention), such as automating 'system analysis' to identify low-cost optimizations that wont cause errors in a system, automating 'interface analysis' to come up with new ideas to solve problems
      - 'temporary transparent directed costs' like 'temporary subscriptions' or 'temporary price hikes' that a company charges customers who opt-in to pay slightly more temporarily for particular purposes like 'researching a medical problem' or 'using clean energy' or 'funding transportation of supplies where needed', where the company offers a way to check how theyre using the money & states specifically what the extra money is for & specifically how theyll use it
      - 'multi-purpose investment funds' where people who invest are guaranteed returns in multiple possible formats, so if the fund doesnt fulfill an intent like 'make x% ROI', it can guarantee that it will fulfill another intent of the fund, like 'increasing clean energy use' or 'increasing quality of life for vulnerable groups', so either way investors get some value type that they care about
      - 'finding interim "state-connecting" solutions' that solve problems 'temporarily' or 'emergently when applied at scale or under other conditions', such as how covering/reducing 'transportation costs to high job-density areas or cell phones costs or basic good (medicine, clean water) costs or "valuable idea"-inventing costs (computers, machine-learning, encryption, dictionary/encyclopedia, solution/rules database, quantum computing, batteries, solar power, math, clean energy, search engine, sorting algorithm, bitcoin, interface analysis)' is an interim solution to 'ending economic opportunity inequality' that when applied at scale might temporarily end economic inequality, & once other solutions are found to improve job distribution in general, it might complete the task if the previous state is the 'state after applying the interim solution', rather than applying it starting from the 'worst case scenario'

  - add to tech
    - since most people have access to water but its not clean enough to avoid getting sick from it, mobile phone with uv light functionality built-in that can kill bacteria in water since people were going to want a mobile phone anyway and theyre at the target price point of other solutions (5% of income, or $4), the remaining task is removing chemicals from water, which some tools like condensation tools and plants that dont absorb pollutants could help with (rubber, cattail, water mint) 
      - long-term strategies can involve digging strategically placed canals or wells in high areas that will flow to other areas with automatic digging devices, relocating people, efficient air harvesting devices, harvesting energy from bacteria (EET), brains, thermoelectric energy harvesting through exercise or overheating (wearing enough clothes to produce sweat but harvesting the heat as its released instead), plants, making salt water less salty over time (slowly adding other electrolytes in a timeframe that animals can adapt to, or adding salt-binders or salt-breaking enzymes/bacteria), etc
    - tech companies can distribute approved info to people directly by email or notification rather than waiting for them to hear about news or read the news, so they can see how many notifications corroborate each other
      - tool to identify 'common' opinions on a matter, 'changing' opinions, 'trending', 'trolling/triggering', 'extreme' opinions, 'corroborated' opinions matching various science studies or other info sources, opinions 'matching patterns of conspiracy thinking or patterns of emotional reactions eventually tempered with facts', etc
      - https://www.washingtonpost.com/technology/2022/03/11/tik-tok-ukraine-white-house/
    - carmichael function and rsa encryption
      - example of why a 'structural similarity' isnt all thats required to determine useful structures
        - https://arstechnica.com/information-technology/2022/03/researcher-uses-600-year-old-algorithm-to-crack-crypto-keys-found-in-the-wild/
      - adds multiple outputs for an input (multiple y's for an x value) and multiple ways to get the same output (multiple x values to get the same y), and the output A(n) obscures the input n to some degree with increasing obscurity with increasing n
      - '2 ensures a 'vector position of 8' for all coprimes of 8' (totient function of n = 8 is m = 2)
      - a^m = (some multiple of n) + 1, for each a 
      - 'find the smallest integer power the coprimes should be raised to in order to get a position of n'
      - the modulo function is useful for finding 'distance from a value (like products of 8)' which is useful for 'finding primes'
      - can any other attributes of q & p be derived other than:
        - the lower limit represented by e, which the totient of p * q must be higher than
        - the difference in magnitude/length required for security
      - searching for prime pairs p & q that could create n should be filterable by removing ranges that dont contain a prime that could produce an approximate value, like filtering out the ranges between integer pairs that could produce 100, as numbers in these ranges cant pair up with another integer in a way that could create 100 by just using one multiplication operation
        - multiples of 2/50, 5/20, 10/10 can create 100, given that its even (factorible to 2), produced by a multiplier of 10, and given that its produced by a multiplier of 10 can also be divisible by 5
          - so values around 50, 20, and 10 can be 'possible solutions' given that 2, 5, and 10 are known possible factors, until the multiplication is calculated to filter the solution space
        - this would be useful for filtering primes that dont have a paired prime that could create n (or a value near n)
    - identify useful questions to avoid researching/testing pointless possible solutions (solutions that will always be high-cost, solutions that have a built-in flaw that invalidates them) 
      - is there an 'encryption algorithm' that allows identification of each attribute of the original document (like homomorphic encryption) that is required to build sufficient 'attribute combinations' to filter the space of possible original documents at all
      - is there any way to hide information in a way that cant be easily found except with info about a shortcut to find it (the decryption key) or will information-obfuscation operations (and any info interaction function) always create other info (side effects like computation/usage/access effects, outputs/inputs), meaning 'is it possible to isolate info operations perfectly so that these operations dont create these measurable side effects'
      - are explanatory structures of randomness (like 'commonness') useful in offsetting the random structures used in encryption algorithms or are those irrelevant given adjacency to pure randomness rather than the false appearance of randomness
      - 'injecting randomized variables' is a common technique in encryption algorithms to obfuscate the original value by additional random value conversions, storing the determining variables of the actual randomly selected options
        - a useful question to answer would be:
          - 'what is the structure that stores the least options (attribute values) that creates the most alternative options required to test when brute forcing the original value' (which values can be re-used as options elsewhere in the process without reducing the number of alternatives required to check when brute forcing), as this structure would be a good place to start when deriving encryption algorithms
      - is there a set of adjacent info states (opposite+add+multiply, opposite-randomize-add) that can be stored & used to identify/filter any other functions applied in the encryption process
        - can you verify if an info state was likely to be or possible of being a state in a process using these sets
    - 'signal with public/semi-private chats' where sessions/passwords are not used, but there are multiple levels of possible permissions, to treat chats like directories/files that you can change permissions for, to make them public or visible to a private user list, where all of your posts are stored locally, and permission checking is done (does this computer have a permission to view the requested resource) before allowing internet requests to proceed, and the user with the earliest native source of the database of their own data (which can be encrypted with a one-time password/identifier like 'device/app usage patterns' for all the apps deployed on their device in this way, and which can be imported to a local semi-private blockchain in a local user group so other people they know have copies) has priority in multiple claims of ownership of an account (which is just a username and the associated database in this case) - the disadvantage is being tied to the device, but local network (internet of things or wireless device network in a neighborhood or family which are connectible by proximity only) or usb backups to their other devices like their mobile are a possible solution to that
    - file systems that are purely for storage (which dont allow one of edits or reads by any/specific programs, or which dont allow installing any non-default apps/libs bc its purpose is to work or to be a source to copy from, for example) and other intent-based file system designs can be used for specific intents needed by various apps, which can self-lock or self-destruct if used for any other intent, damaged, or altered, and these file systems can be generated dynamically and embedded for a one-time use of an app, similar to containers but without the infrastructure to identify it as a particular product that supports containers
      - for example, it could generate a 'file system where a "pseudo-symlink map" is applied to a random ratio of files, to point to random alternate locations, where the map is visible to protected users or services, so once the symlink map is generated, it shouldnt be required to be accessed again and can be deleted, as each service that uses it will have its own algorithm to generate it as needed, or each service can generate its own symlink map, a map that can itself change frequently within each app/service/usage' to obscure paths in order to avoid bugs that take advantage of known paths, or a file system where a program can only request the new path of a file or search for the new path of a file if it can provide the previous location which was randomized
    - other filters can be applied by user activities (is the user using their laptop or cell phone, is the user using other apps, is the user doing too many other things to possibly have logged in at the time, which can be boolean flags provided by a third party service or a user's device)
    - 'download/generate & delete app' process for each use as a preventative measure against malware that checks for an app thats installed before exploiting a vuln in the app
    - add concept of 'value overflow to adjacent nodes', 'local conditional deactivation', and 'local feedback' to nn
    - depending on propagation/routing function, a neural network node can contribute multiple features (influences) (in deactivating multiple weight paths) on the final set of generated coefficient sets for each iteration during training
    - examine other weight update functions, like those having an oscillating sequence that converges around a value rather than those having incremental updates in one direction
    - related problem: as weight updates occur, predict the remaining weight updates (what is the likelihood that weight updates follow one pattern over another or end at one value over another) so that some weight updates can be skipped
      - applying 'find prediction function' to various inputs/outputs in a particular solution-finding method like neural networks for the general 'find prediction function' problem format can be useful in some positions
    - add example of a 'generate variants' or 'generate possibilities' function by applying variables to a particular structure, to generate variations of functions including workflows, queries, or problem-solving functions like 'find structure in a structure'
      - 'structure x' can be replaced by 'alternative structures' like 'find generative functions of x' or 'find alternate routes to x or its outputs' or 'find generative functions of requirements & other inputs of x' or 'find variables of x and alternate variants of x' or 'find interchangeables of x' or 'find invalidating structures of x' with varying degrees of success according to various solution metrics
    - identify the 'reason why something works' (solution success cause) formatted as the 'structure of change applied to inputs/outputs that enables a later output to be optimized by some metric' and 'why the structure of change enables that or does so better than alternatives'
    - which format is optimal to standardize to when implementing function
      - using 'vectors' to represent everything require a function to map non-numerical values to vectors, which will normally either lose information or encode it inefficiently, storing more information than necessary, and not storing it in a way that can be compared with other numericalized non-numerical values in a meaningful way by default, without referring to the original non-numerical variable values
      - using 'inputs/outputs' to represent everything is more likely to avoid losing information, but does represent everything as a sequence structure, where sometimes sequences arent implicitly described, such as in a set, which would require a description in the form of a 'generative sequence' or an artifical order imposed on the sequence

  - add to science
    - only some interface structures like certainties need to be preserved in order to proceed to the next space-time, not all structures (not everything thats true needs to always be true), so as not to destroy the stability of the interacting structures that allows a space-time to occur at all
      - adjacent changes dont always need to connect adjacent space-times, if the change is significant but doesnt destroy the stability of the interacting structures
    - roots to provide a core structure of alternatives as opposed to a requirement through one option
      - https://www.livescience.com/imaginary-numbers-needed-to-describe-reality
    - 'metformin and a low carb diet' reduce 'serum vitamin b' which can be used as inputs to 'cancerous growth'
    - check how folding relates to growth patterns like exponential growth, learning & information storage, and related structures like waves
    - the compromise structure is a network structure of 'multiple solutions' rather than one: require anti-vaxxers to self-isolate from people until the vaccine is distributed
    - life forms as 'structures of potential' (as in what changes a structure can cause that is different from what other structures can cause)
    - prioritize 'scouting nanobots' to clean out chemicals as a prevention measure
    - structures of invalidity like 'hiding health data avoids the solution to the health problem, which would invalidate the reason to hide the health data' and 'funding a prevention measure rather than a cure, which would invalidate the reason to fund prevention measures'
    - general relativity: does gravity exert limits on the change types/rates possible so that time cant pass as quickly/slowly depending on gravity, enforcing a ratio of similarity between states to conserve energy in higher gravity environments?
      - how do you direct gravity to restrict time in one position so it can occur in another
      - how to manage risk of 'matter-forming cascades' using technologies to form matter that can restrict time in some positions from gravitational effects
      - what changes occur if you decrease gravity of black holes like by positioning other black holes so they can exert gravity on the other
      - re-define time as 'change potential' so its clearer whether an increase in time means 'slower change, meaning more space-times or more opportunities for change' or whether an increase in time means 'faster change bc time is moving faster' or 'faster change of change types as in new change types' or 'relative change rate/types' or as 'unresolved uncertainties' or 'calculatabilities' as a measure of what hasnt been calculated or isnt calculatable in a space time given its relative change rate compared to other space times that will solve it faster and prevent change in other space times
        - time as 'structure', as in structures that are allowed to be 'efficient/interactive/powerful/organized/accumulated/irreversible/robust/derivable/measurable/inevitable/required/defined/sequential/maintainable'
        - observers have 'potential' in the form of functions like 'applying changes to matter', adding a variable to the 'position' attribute, obeying possible rules like:
          - connected structures (like observer/particle) exchange structures until they are equal (energy transfer of change potential between observer and particle)
      - whether 'time' has an ending, or a high probability of an ending, as determined by the configuration of space-time distortions in this universe, indicating theres no way to ultimately guarantee the preservation of time, as defined in the form of gravitational variations that ensure a space-time can slow down enough to be measured or distorted by high-potential or high-change sources like humans, or whether the current configuration guarantees that black holes will continue sucking up time until there's none left, or whether the current configuration has a high probability that change types allowed by lower-gravity areas will inevitably intersect in a way that creates a change type that will invalidate conditions necessary for space & time
        - what ratio of these scenarios are likely, possible, and involve risks or guarantees?
      - how time in one universe interacts with time in another - whether constants created by gravity and variables created by lack of it can interact with other constants/variables in adjacent, aggregate/net, reflective, or otherwise interactive universes
      - whether the similarities acting as inputs to standard gravity have corrollaries in the similarities acting as inputs to quantum gravity
      - what gravitational impact on time exists on the quantum scale - the farther away from standard scales a particle or other structure is, gravity applies less and time moves faster?
      - does structure/matter (order) act like an opposite structure of time (disorder) so destroying matter can create time in a particular position
      - can you navigate to other space times by increasing entropy to the entropy levels adjacently surrounding the target space-time (after which any matter navigating there reduces entropy to the target space-time)
      - does that mean you have to destroy matter (order) in order to power time-travel - would that invalidate motion in certain time-directions, as increasing entropy tends to cascade and create irreversibilities
      - is there a possible energy configuration that can increase entropy with enough precision that it can reach the target level
      - are constants like irreversibilities more similar to the definition of order or is the initial lowest-entropy state of matter closer to the definition of order
      - how do different structures that can create order (time crystals, black holes, other semi-closed or independent systems) interact - are they interchangeable in any way
    - uninhibited cell division
      - a process of applying regular damage of cell types successfully handled by immune system, rotating between locations & systems, to make sure the immune system is being sent to these components on a regular basis
      - alternate methods of increasing blood flow/circulation to every cell & distributing energy in the form of heat through exercise/sweat by increased connectivity/circulation
      - alternate methods of increasing cell replacement rate to prevent proliferation of dna mutations
      - rotate periods of inflammation, immune activation, scar formation, cell lifecycle triggering & stem cell differentiation, fasting/recovery, by triggering these processes in a way that avoids excess or compounding tissue like in fibrosis or excessive scar formation
      - forming or adding antigens on cells with sub-optimal configurations (like rb-binding, p53-deactivations or oncogene activation or insertion, or malignant transformations) as vaccine targets & to trigger creation of cells with correct configuration
      - 'aggressive regulator structures' as opponents to 'aggressive cell division structures', given that dna has built-in vulns that allow cancerous mutations to develop easily, what is the dna configuration that would aggressively prevent these mutations, and what is the cost of this being mis-applied to one in every x normal cells as opposed to one in every x cancer cells
      - 'variation-generating structures' to ensure a variety of dna configurations in a location in case cell division occurs so theres always a supply of active p53 copies nearby
      - directing fibrosis and other constraints to induce apoptosis in tumors
      - engineer nanobots to generate uv light & other p53/rb-activators where cell communication is hindered in ways that indicate cancer
      - activating genes or proteins inhibiting s-phase or death phase of cell cycle once the cell is malignantly transformed (is missing a 'protogen' as opposed to having an 'antigen', an 'opposite' vaccine target)
      - preventing degradation of pain or other immune signals by repeated exposure to triggers like heat
      - decreasing cell cycle timing in susceptible or vulnerable cells to reduce chance of mutation persistence after cell division
      - injecting any cancerous regions found with pro-health mutations or immune cell formation to offset ratio of healthy or immune cells & malignant cells produced by cancerous processes
      - moving cells with useful functions like 'destroying aggregates' to other positions where that function would be useful in 'preventing unrestricted cell division'
      - fixing 'illnesses that modify genes' is a problem of 'fixing genetic mutations', as genetic mutations can occur that mimic the effects of illnesses that modify genes, so prevention is not the most powerful problem-solving intent
      - fasting to direct energy toward 'replacing (killing & growing) cells' instead of just 'growing cells'
      - processes that increase metabolism (like exercise) as a way of fighting cancer, since metabolism is a regulator of the cell life cycle, and cell life cycle disregulation is a promoter of cancerous growth (more cells created than dying when cell division isnt necessary or otherwise useful)
        - look for other cell life cycle disrupters (triggers of cell division like chemicals causing specific mutations) & regulators (triggers of cell death like 'plant immune proteins')
        - look for 'immune states (including specific immune responses) that change metabolism temporarily' that could be used as anti-cancer tools, triggered by brain processes (memory) or other inputs to immune state changes (pathogen exposure, exercise) - 'addressing the systemic issue using other systems bc theyre on the same interaction level'
        - serotonin & other feedback mechanisms that act as signals to processes like cell life cycle regulators
      - useful questions
        - what is the difference between structures having problematic differences (pathogens for which immune response is always/adjacently constructed & those that are not) & what can resolve these differences (what changes can be applied to convert one into the other)

  - add to definitions
    - solution-finding method can mean a solution-finding method on any interaction level, such as a domain-specific problem space (like how 'regression' is a solution-finding method in the 'find a prediction function' problem space), or a problem-solution interaction level (like how a solution automation workflow or interface query is a solution-finding method), or interim interaction levels (like how a 'find connecting input/output sequence' is a solution-finding method on the function interface)
      - if the problem is (or can be) to 'find a solution-finding method', and can be applied to solve the original problem, these will also be called 'solutions' to that problem
    - pattern: similarity created by repetition
    - a 'problem' could be defined in various formats including:
      - difference between the initial problematic sub-optimal state and the target more optimal solution state
      - lack of functions preventing the problem or its causes, or lack of error-correcting or error-containing/isolating functions, or lack of variable-handling functions
    - where a 'solution' would be a structure that fulfills the solution/problem-solving requirements, and a 'solution-finding method' would be the structure that found/derived/generated the solution, optionally using the problem or problem attributes like problem type as an input (or using another solution, or the problem space as an input)


  - add to examples

      - example of logical fallacies like the rarity of simple rules, as everything is a simple transformation away from something else
        - such simple rules often reflect other simple rules like 'a simple rule is representable with more complex structures (like an infinite sequence)'
        - 'simple rules using core structures that can be replaced with other simple rules' is a rare structure
        - some simple rules are required to exist (everything cant be complicated or there would be no adjacent transforms possible)
        - in the iteration of all possible rules with incremental changes applied to generate new possible rules, inevitably the iteration will encounter simple rules that involve units at various points in the iteration
        - https://mapehe.github.io/math-problem/index.html

      - examples of simple interface queries that can find/derive/generate useful info
        - apply 'multiple' to constant structures like 'threshold counts'
        - examine lower-dimensional structures like boundaries as structures of 'simplification'
        - apply 'opposite' to 'sequence' structure like 'discrete to continuous structures' (where the continuous structures are the inputs)

    - write definitions of interfaces in terms of other interfaces
      - example:
        - the math interface is useful bc it involves absolute structural facts regarding how structures organize, interact, & stabilize ('math' means 'specific standardized referential structural stability/measurability'), where useful structures on the math interface are other interface structures like similarities (alignments, commonalities, core components, 'lack of structure' like zero and randomness, 'symmetry structures' like zero and averages, intersections, overlaps), etc
          - for example, the overlaps through 'aligning intersections' with numbers like (-1, 0, 1, 2, 3, 5, 6) identify unit or unit-adjacent ('first-in-a-sequence' or 'first-of-type') numbers that appear often in core formulas
          - given that there have to be some constants that are useful for these formulas, a number has to exist that is particularly useful for these formulas, it doesnt necessarily mean anything beyond this requirement
          - however its likelier to mean that the number is useful in another way (its a core or unit number, it has useful unique attributes like its factors, it has a particular relationship with another number that is useful like being the opposite of another number or the first number of a type/sequence or adjacent to or different from another number, etc)
          - for example, the 'meaning' of the 'reason' why 4 is an 'integer factor of 20' is that:
            - its different from non-factors like 3 and different from factors that cant be repeated to make 4 a factor (like 5) and it has an 'integer' attribute which makes it possible to multiply by another integer to create 20
          - the 'meaning' of the 'reason' why 4/5 are integer co-factors of 20 is that other adjacent integer co-factors (10, 2), (20, 1) can be changed in a way that makes it required for them to be integer co-factors as well (if you take half of 10 to produce another integer factor, in order to still get to 20 you have to increase the other factor by amount x, which is doubling it, which also produces an integer (4) so another integer co-factor pair is derivable as required to exist in this case) and also these changes follow a pattern (graph of (20,1), (10, 2), (5, 4), etc) that strongly imply (but dont require) 4/5 to be integer co-factors
            - other requirements such as that 5 is less than 10 and greater than 1, and 4 is greater than 2 and greater than 1 (the lowest possible cofactor) can also be used in place of these requirements to determine the requirement of 4/5 to be integer co-factors of 20, in the trajectory of integer pairs between (10, 2) and (2, 10) or acting as the other endpoint given the known endpoint limit of (20, 1) in a sequence of unique cofactor pairs
            - how to identify 'find the other endpoint in the sequence of integer co-factor pairs of a number given a known endpoint' as a useful intent to derive the other integer co-factor pairs (useful in that it applies a structure like a sequence as a useful structure to derive change types between adjacent numbers in the sequence and therefore predict the next numbers in the sequence, rather than trying every integer combination of integers between 1 and 20)
              - apply useful structures like a 'sequence' of co-factor pairs with adjacent co-factors determining position in the sequence
              - apply useful intents like 'predicting a co-factor pair from another co-factor pair' to identify that a 'sequence' is a useful structure given the relevance of adjacent numbers in the sequence for intents like 'determining the next number from the previous number' by its inherent structure of isolating & organizing change types relating numbers in the sequence
        - given the net impact of the interactions of all these interface structures (requirements, patterns, ambiguities, etc), the math interface emerges
          - the math interface acts like a standardized structure (a 'system of references') to format interface structures like alignments, connections, concepts, etc
            - for example, describing wave functions or elliptic curves or circles as having a structure of 'ambiguity' (in the lack of certainty in the mapping between a particular input and a particular output in the lack of exclusivity) is useful for finding functions for 'obfuscation' intents which require ambiguous structures to provide distance from the original value
            - for example, describing equivalent change types at different positions as 'parallel' (a math interface conceptual attribute)
              - an example would be the two language network queries 'a system of references' and 'a network of connections' which change in equivalent ways but occupy different positions in the language network, so these two queries can be described using a math term such as 'parallel'
              - automatically solving a problem like 'find another way to say this statement' (formatted like 'find another query equivalent to this query but using different terms' on the language network interface) would be trivial once this structure is applied, to reduce the task to the more structured & automatable task of 'find parallel queries to this query' (meaning 'queries with equivalent change types and a different starting/end point') to avoid comparing equivalence of queries with other change types, or avoid comparing equivalence of queries at all different starting/end points, as the intent of the query is to 'find similar queries with different positions' which makes parallel a relevant structure, as parallel queries wouldnt change the meaning of the statement, as its clear is a requirement of the solution
          - possible new math structures that dont fulfill or use these interface structures determining other math structures are either unlikely to be true or definitively not true, so these interface structures can be used as a filter of possible solutions on the math interface
            - for example, if a new math structure cant be adjacently framed in terms of references to other math structures, its unlikely to be a true math structure, even if there are no known constraints preventing its existence by definition, as the 'connectibility' and 'consistency' of structures are structures of certainty (if its true, it can be connected to other truths, and is consistent with other truths)
        - the limit of the math interface is determined by its definition 
          - wherever a numerical reference cant be applied to specifically & accurately derive/describe a structure (like structures in a black hole, structures of uncertainty like structures of incomputability or structures of immeasurability or structures of instability, or predicting a number in a perfectly random sequence with perfect accuracy)
            - this can help explain physics structures such as how certain attributes can be measured in certain positions more easily than other positions, as the structures being measured may degrade in structure or change structure when traveling between the measurable states, falling back to their original structure when reaching a measurable position
            - the measurable positions are positions where the structures being measured can reach stability, allowing their structures to exist long enough to be measurable
            - structures of uncertainty (like 'changes' applied to structures between measurable positions) are useful for intents like 'conserving energy'
            - the structures that exist between these measurable positions are required to be more 'energy-conserving' states, so the set of states that conserve energy more than measurable states (which take energy to maintain) is the set of possible solutions to the problem of 'determing the structure of a structure between measurements'
            - by applying math and other interface structures like 'stability', 'uncertainty', 'change', and 'requirements', we can derive information about unmeasurable structures even if we cant measure them by definition (math-meaning that 'the distribution between possible alternate solutions is evenly distributed')
        - given that the math interface can be used as a base or absolute 'system of references' to 'format structures on' or 'standardize structures to' (like framing language in terms of math), the math interface should have objects/functions/attributes that can describe/measure any other structure (connections, ambiguities, etc)
          - the math interface can therefore be interpreted as an organized interface of every possible interface structure interaction
            - rather than interfaces which offer advantages in compressing other interface structures, the math interface is an expansion of other interface structure interactions, containing repeated examples of these interactions to fully capture all possible interactions in a standardized way
              - similarly, the math interface contains all possible interfaces embedded in the math interface (different bases for change, like different number systems/types)
              - why are there patterns on the math interface? (like why are there patterns like 'circles' in structures like 'roots of unity') bc:
                - repetition (such as repetition of a structure) is a useful structure to create interchangeable structures, which are required for other structures to exist, and is required to describe all possible interactions specifically, which is an intent fulfilled by the math interface given its definition
                - similarity of structures is useful and also required in many cases, leading to a high probability of patterns in references
                - there is no rule enforcing the uniqueness of all structures on the math interface
                - change types can be applied to a particular interaction type that often find other interactions of that type (its rare for an interaction to be unique)
                - the change types that lead to these interaction types usually require that other examples exist or indicate a lack of constraints preventing their existence
                - intents like 'changing the interaction type example' are required to be allowed given other allowances, and are required for other requirements
                - structures determining the different interaction type examples (such as equivalence of differences from a base like zero) are allowed to exist
                - the conditions determining the connection between an interaction type example (a particular root point) and related numbers (like its difference from zero and its direction) exist in other positions
            - similarly, the info interface is an expansion of the math interface (containing repeated examples of math interface structure interactions)
          - the objects/functions/attributes of the math interface should be derivable from the objects/functions/attributes of other interfaces (applying the insight that all primary interfaces are interchangeable and can be used in place of the others) and from the requirement of formatting all other interface structures (answering the question 'what structure can support describing all other interface structures')
          - this means 'if there is a number, there is a reason to use it' and 'if there is a number, there is a concept determining that number'
            - for example:
              - the number has a connection to another number that is useful for some intent, like 'changing another structure', 'approximating another structure', 'converting between structures', etc
              - this means 'numbers can be derived from these intents as well as using other methods like applying requirements and applying insights'
            - as another example:
              - the concept 'prime' exists bc of the concept like 'factor' and 'unit' and 'identity'
              - the number 0 exists bc of concepts like 'unit' (a unit example of a lack of structure), 'adjacence' (to other numbers near it following sequential patterns), 'consistency' (of the integer sequence pattern), 'nothing' (lack of structure), 'symmetry' (the number connecting positive/negative number attributes), 'derivation' (in order for 1 and -1 to exist given math interface requirements/rules, there must be a number connecting them)
          - given that math can be formatted as a 'system of references', this means all structures on the math interface can be converted into any other structure on the math interface (the references are 'connectible')
          - the math interface can be used as a filter of possible structures, given the usefulness of its structures in determining the possibility of a structure
            - structures that violate a math interface requirement/rule can be easily found bc math interface definitions are by definition computable & structural
        - the math interface can be written in terms of these other interface structures (overlaps, limits, requirements, etc) which help explain its meaning/usefulness to other interfaces & interface operations
      - the structure interface means 'useful abstractions of structures such as ambiguities, efficiencies, optimizations, alternatives allow interfaces to interact in a standardized way using these cross-interface structures'
      - the concept interface means 'concepts are a useful structure to compress other structures like functions to fulfill intents like finding/building/deriving a structure'
      - the pattern interface means 'frequently occurring structures tend to be more useful in finding/building/deriving other structures'
      - the potential interface means 'the certainty/uncertainty of a structure can be estimated/approximated/predicted using structures like proxies & approximations when minimal information is not reached to measure it'
      - the info interface means 'specific examples of structures are useful for finding/building/deriving other structures'
      - the cause interface means 'interface structures like time, adjacence, uniqueness, energy, complexity, & structure are useful in determining causal structures like sequence of operations & possible alternatives/ambiguities'
      - the intent interface means 'the intent of a structure (like a function) is a useful way to compress a structure to fulfill intents like finding/building/deriving that structure'
      - the logic interface means 'structures follow absolute rules (like how when only one solution hasnt been tried and all others have failed, the remaining solution is correct)'
      - the function interface means 'logical units (functions) applied to change structures (variables) are useful for many intents (like find/build/derive variable interactions using input/output sequences)'
      - the change interface means 'useful structures like structural references (connections, comparisons) can be usefully formatted in terms of changes applied to another structure, as changes (differences) are a core useful structure capturing high variation and every structure can be adjacently converted into every other structure when formatted in terms of their similarities/differences to other structures'
      - the meaning interface means 'interface structures like structure logic, math, cause, intent, and change follow absolute rules fulfilling various useful intents like organization/interaction'
      - each interface is contextually useful
        - for example, the 'cause' interface is useful while/where there is time (meaning a lack of possibility of multiple timelines, lack of possibility of switching between states in a non-sequential manner, like in a network structure or all states at once or an approximation of 'all states at once' like the possibility of traversing any state in any order/structure/amount)

      - the idea of a 'consciousness' and 'personality' as a useful interim structure connecting a 'brain' and the 'intents' of the 'bio-system containing the brain', acting as the controller of the brain to serve requests made by the 'bio-system' in fulfilling its intents, but may be a false structure where the actual functional structure is a 'router' or 'platform' for these connections to be made, with no agency, where the 'consciousness' is the 'output' or 'input' rather than a 'connecting structure', where its likely to be all of the above bc it exerts similar interchangeable control as the bio-system intents, general system incentives, and brain structures, indicating a 'mutually dependent' structure

      - add to other useful functions on the interim interaction layer which include 'invalidate' & 'enable'

      - examples of alternate function representation formats (as opposed to formats like 'cellular automata' or a basic function/variable network) to apply in interface queries requiring format finding/conversion/derivation/generation to implement solution automation workflows

        - Conditions as 'direction changes' or 'integrations/splits' of a vector sequence/tree/network, inputs as a vector, input vector changes as a 'vector sequence', iterations as 'possible branching vectors from the base vector representing a set' (each point in or component of the vector indicates a possible point of change during iteration of a set represented as a vector with orthogonal vectors indicating a solution metric to check during iteration), where branching vectors (items selected during iteration) can be connected to or combined in another vector (or vector sequence)
          - formatting a 'set' as a vector formed by component vectors (isolated items of the set) with a function that changes inputs in a way that differentiates input sets, such as variables that can be mapped to vector attributes like 'degree of difference from a standard base function' and 'function limits'
        - A 'set of variable change sequences' is a useful function representation since it allows for interactions between variables as multiple input structures that may or may not contribute to the same outputs and which can be combined
        - Input/component functions as 'tensor components' of a state or function network
        - 'differences created in inputs, when amortized across inputs' as the 'degree of direction change' in the next vector in the sequence of function vectors
        - Functions can be represented as an area (volume, etc) or equivalently its 'limit conditions' in multiple alternate ways
          - the area can represent the set of possible functions that could be solutions in the original problem format (connect the input variables to the output variable, given the original input format of the 'data set of variable connection examples')
          - the area can represent a topology of possible solutions (coefficient vectors) adjacently connected by similarity
          - the area can represent the set of connecting vector sequences between another input/output ('function steps as connection sequences indicating the steps of the solution/solution-finding function implementation', 'function state changes as solution/solution-finding method state sequences connecting the problem input with the solution output (iterated sets of coefficient vectors)')
          - the area can represent function components (sub-functions or variable connections/combinations) that a particular function implementation uses (each sub-square in the square representing a component function or variable combination)
        - Functions can be represented as their inputs or components and the impact on solution metrics
          - an 'area indicating a function input change' and an 'area indicating a function output change' can be organized as a 'set of connected areas where the changes are aligned sequentially' (like 'areas connectible by rotating areas around a line') to indicate function changes that co-occur
        - Functions can be represented by approximations in more useful dimensions (like 'lower dimension count') which are useful for intents like 'approximation' which dont require exact matches
          - example: a polynomial with high rank can be represent in a lower rank by a 'sequence (like a 'rotation' dimension sequence or a sequence on an existing dimension) of lower-dimensional functions that approximate the higher rank function within an accuracy range' (such as 'parabolas that can approximate a wave function' or a 'circle function')
            

      - write example implementations of trial & error (various sorting methods, identifying solution requirements, testing function, identify solution variables like position/state that generate additional solutions, identifying solution space given solution variables, various solution automation workflows & interface queries applied), as an example of translating solution automation workflow into code that can be applied to example problem definitions
        - include example of creating all possible variable values, variable combinations & other structures, and other interface variables such as assumptions/context

      - solution automation workflow metadata, like for trial & error:
        - assume the problem space is equal to the solution space (rather than a normal workflow, which might filter the problem space of all possibilities to identify the solution space containing possible/probable solutions)
        - alternate required function sets for different implementations of the workflow: identify possible solutions, iterate through possible solutions, test possible solution
        - which workflows optimize various general solution metrics (require the fewest changes to the problem space or problem structure)

      - example of why a structure is more useful than another which may be equivalent in some ways
        - an 'input-output sequence' (a 'state sequence') is more useful for things like 'connecting specific inputs/outputs' than 'a function database' bc 'inputs/outputs' are specific data/examples of cases where a function is applied making them useful for connecting specific problem inputs/solution outputs, and a function database takes less memory than an 'input-output sequence' database given that inputs/outputs will vary more than functions, whereas functions are more useful for 'finding connection functions quickly'

      - example of various implementations with 'user-submitted visit purpose' data and 'cellphone location' data, and requirement to "create an app to predict a user's wait time"
        - identify relevant intents to problem-solving intent to fulfill the solution requirement using the specific available input data
          - intents such as 'predict user wait times', 'import data', 'identify relevant data', 'predict end time of appointment', 'predict conditional average time taken by appointment', etc
        - apply function to find relevant data (out of existing available data, such as user location and user visit purpose) for intents relevant to the solution requirement 
          - 'location' data is relevant for importing 'actual appointment start/end times', 'user arrival times', as well as determining the 'order & number of people in the wait list' data, and may also be used for predicting 'appointment end time nearer to the end of the visit' data
          - 'visit purpose' data is relevant for finding 'static average appointment times' for a 'visit purpose type' or 'conditional average appointment times' data for various conditions like 'business' and 'hour of the day' which may be variables that change 'average appointment times'
          - 'audio' data could predict 'real-time updates to appointment end times'
          - these have differing accuracy for various intents: 'location' data is a proxy for wait list & appointment time data, whereas audio data could be a more exact representation, and location can exactly determine variables like arrival time
        - identify relevant structures to the solution requirement ('predict wait time') & determine variables of relevant structures
          - appointment times (predicted time before the appointment end, and actual time once known)
          - wait list
          - alternate relevant structures could include:
            - 'appointment complexity or urgency' which would mean the assigned staff should be considered unavailable for x period of time or even closing the office and routing users elsewhere
          - determine variables of the relevant structures to the solution requirement, like how 'business', 'hour of the day', 'scheduled appointments', 'urgent purpose visit', 'canceled appointments' may be relevant to the wait list as well as appointment times
          - apply variables of the wait list & appointment times to generate a set of possible/probable cases, and filter by whether available data can detect those cases (detect a chatty patient or a higher ratio of urgent cases)
        - determine variables of the solution requirement structures (user's wait time) (how many people in wait list, available staff, predicted wait times of preceding appointments, probability of more urgent cases arriving)
        - determine differences between variables of relevant structures of solution requirement (wait list, appointment times) and the solution requirement structure itself (user wait time)
          - which 'wait list' variables ('business', 'hour of the day', 'scheduled appointments', 'urgent purpose visit', 'canceled appointments') and 'appointment time' ('purpose visit', 'business', 'hour of the day') variables can change or connect to 'user wait time' variables (such as 'position in wait list', 'urgency of purpose visit', 'arrival time')
        - different implementation methods
          - predict any relevant interim variables like 'appointment durations' and 'wait list changes' as a way of finding inputs to predicting the original variable 'user wait time'
            - predict using conditional/average predictions of appointment times or sequential-data neural networks trained on location or wait list change data, based on prior data patterns
          - calculate exact wait list, arrival, start & end times of appointments and predict the biggest uncertainty (probable appointment duration) as inputs to a user's wait time
          - predict highest variation variables like 'purpose visit complexity' as an input to relevant interim structures like 'appointment duration' and plug into a standard calculation of wait time based on arrival/start times & wait list attributes
          - select one variable like 'audio' or 'location' data and apply sequential-data neural networks
          - find most explanatory features of as many variables as there are variables available & integrate them into a prediction function
            - 'co-occurrence' or 'adjacence' of variables like 'high urgency user location adjacent to another high urgency user' indicating a higher wait time for users after them
            - integrate with derived user routing information such as 'user prioritization' to determine what changes to location/audio/visit purpose will impact user prioritization (and position in wait list)
          - other various implementations using different inputs/outputs (predict original solution requirement variable from available data or data derived from available data)
            - use existing solution-finding method (regression, machine learning, etc) and apply "different input/output variables" to prediction function
              - 'given existing wait list & purpose visit data of other users, predict user wait time'
              - 'given location patterns of other users before current user, predict current user wait time'
              - 'given arrival & exit patterns of users in general, predict current user wait time'
              - 'given values of high-variation variables like purpose visit complexity & staff skill set & staff fatigue, predict user wait time'
            - use existing solutions and apply "different prediction function inputs/outputs" (predict changes to standard prediction function borrowed from other existing solution code base or model, based on differences in problem definitions & requirements & data)
              - 'identify problem type (queues, timing/duration predictions), generalize & apply generalized functions from existing solutions for that problem type to available data'
          - the above implementations require different functions to be found/derived/generated & applied in various interface queries (the above workflow requires a function to derive 'purpose visit complexity' from 'purpose visit', possibly using prior purpose visit & appointment time data)

    - representing extra dimensions as 'queries of interfaces' or 'queries on a network' where nodes are endpoints (that can be connected to other points with queries)

    - add to explanations

      - my system implements concepts like probability, logic, requirements, functions, variables, structure, intent, and cause in an integrated manner that generates meaning relevant to user requests

    - example of problem-solving intent definitions
      - 'filter the solution space' has a defined value of 'identifying structures (combinations, ranges, positions, alternate sets) of solution variables (components/inputs) that violate a solution requirement'
      - however 'filter the solution space' is a general intent and can be implemented in many ways, varying on this structure of its defined value
      - for example, a solution-finding function that filters the solution space could identify the threshold values of solution prediction function coefficients beyond which the solution isnt possible if the solution is to fulfill a solution requirement like 'accuracy' (in summarizing/representing a data set)


    - add to solution automation workflows

      - apply interface structures to problems to generate new solution automation workflows that involve a reduction of some solution metric (problem variable set, distance from solution, number of un/solved sub-problems as a ratio of total sub-problems, etc)
        - all of the solution automation workflows in problem_solution_diagrams.svg involve changes to the variables of problem/solution structures
          - change problem until its a solution (changes the problem state until its useful in some way)
          - solve different problem (change problem identity, like to the 'causal problem')
          - break problem into sub-problems (change problem scope/complexity)
          - connect problem/solution (change problem/solution position)
          - reduce problem (change problem variable set to a subset)
          - filter solutions (change problem space possibilities to a subset)
          - change existing solution until its more optimal (change suboptimal solution)
        - the interface structures (changes) applied to the variables of problem/solution structures need to fulfill definitions ('make sense, having no relevant contradictions')
          - for example: 'changing a suboptimal solution' is only useful when the change fulfills solution metrics more optimally than the previous state
          - so the 'change' applied would only qualify as an implementation of a solution automation workflow if it 'improves the suboptimal solution' (fulfills the definition of 'solving a problem')
        - sub-intents of each workflow, like 'find input info of the workflow', are components of the workflow implementation (interface query)
          - workflow: connect problem/solution (change problem/solution position)
            - sub-intents (alternative, overlapping, sequential sub-problems)
              - 'find input info of the workflow' sub-intent: find connective/interactive structures to use in connections
              - 'find structures to input to the connect function' sub-intent: find structures required to be connected
          - various workflows can be used to implement these sub-intents:
            - 'find input info of the workflow' sub-intent: find connective/interactive structures to use in connections
              - 'derive the structures that are adjacent inputs to the solution by applying solution requirements like solution metrics'
                - this implements the workflow 'derive structures in between problem/solution using input-output sequence'
            - 'find structures to input to the connect function' sub-intent: find structures required to be connected
              - this implements the workflow 'apply requirements to derive certain structures that can be connected with uncertainties (variables)', where the uncertainties here are the 'structures or methods to connect those structures which need to be connected'
              - the output of the previous sub-intent can be used as input to this workflow
              - the 'adjacent inputs of the solution (derived from solution requirements)' can be input to the 'connect' function
                - meaning the 'problem structures' should be connected to the 'solution-adjacent inputs'
        - these sub-intents of each workflow are each problem-solving intents, bc they all add useful information to use a solution automation workflow, even if the current workflow cant be completed (just the sub-intent is completed)

      - identify useful structures like 'alternate structures' that can be combined to create solution automation workflows which are not default components of solution automation workflows (solutions, problem-solving intents, etc)
        - example: the 'iteration' structure of a 'for loop' is the corresponding structure of 'try every possible solution, score each one, and compare scores' or 'try every possible solution & score each one, until one score is above the solution score threshold', as 'for loops' are often used to 'build a reduced list out of another list'
        - similarly, the 'function parameters' (inputs, values being changed) & 'function return values or value changes' (outputs) are corresponding structures of the 'find input-output sequences that connect problem/solution' workflow
        - the 'filter the solution space' workflow also corresponds to the 'iteration to reduce a set' structure represented by a for loop with a filter applied to a list
        - given that core logic structures are corresponding structures of core workflows, combining core logic structures as components of other workflows is a way to generate workflows

      - identify useful structures (like 'inputs') of solution automation workflows and identify methods to convert structures into those requirements, to select which workflow to apply when one is more optimal than other
        - example: the 'break problem into sub-problems' workflow has a requirement of 'isolatable change types', given the structural impact of the workflow which is 'isolating change types'
          - these change types can be sub-problems (reductions of the problem), which can take the form of:
            - 'solving the original problem, for a subset of the original problem variables'
            - 'solving the original problem, broken into a sequence/tree/network of sub-problems'
            - 'solving the problems causing the original problem which are "components" of the problem in that they are "inputs" of the problem'
          - similarly, the 'filter a solution space' workflow has a requirement of 'existing solutions to try' or 'existing possible combinations/changes (in the problem space) to try (which may or may not be solutions but are possibilities)'

      - identify useful structures which store info such as 'representative data (data set shape) of other info (data set)', where the stored info can be used as a proxy or substitute for the other info to fulfill problem-solving intents like 'reduce required computations'
        - example: knowing the 'general shape' of the data set is useful information to assess whether a directional change or parameter of a prediction function is likely to produce a correct prediction function, and may be more useful in some cases than the actual data point, as the 'general change types like change patterns and change rates and variable interactions' are included in the 'general shape' of the data set but are not included in individual data points, which are used as the input to most 'find a prediction function' solution-finding methods
          - the 'representative data' is also useful for other intents like 'generalizing a function', as 'specific data points' are likelier to be corrupted, mistaken, or influenced by other irrelevant factors than the 'general shape of the data set'

      - identify useful structures (like 'additional inputs', as in 'additional input data derived/inferred/imputed' or 'probable additional input data') for specific intents in a problem space like 'find a prediction function' that could fulfill solution metrics like 'accuracy'
        - example: if predicting whether a particularly high output (target y-value) exists (or is likely to exist) in a data set (for intents like 'augmenting data' and 'inferring new data' and 'imputing data (filling in missing data)')
          - first find the x-range of lowest/highest x-values, then select random subsets of the data set and find a function that describes each random subset
          - then find the maximum y-value of this function over the x-range, using a method such as 'find out if there are adjacent points to the target y-value in a pattern that would make it possible for the y-value to exist in the function (like "are there two points surrounding the peak of a curve, where the peak is the y-value" or "are there two points surrounding the y-value in opposite directions, at a change rate that is plausible for the function as it exists elsewhere in the function")' or "is it adjacent to known points in the function within an accuracy range"
          - apply this maximum y-value of the random subset function to check if the particularly high target y-value could be output by that function (do any known approximations/representations of the data set - or metadata of these approximations/representations like the 'maximum y-value of the approximation' - indicate this output is possible/probable)
          - repeat the steps starting from the 'selection of random points' until there are enough statistically significant functions that could output the y-value, up to a number of repetitions that is fewer steps than applying a standard method like a regression method
          - alternatively apply a filter like 'random points in a set of sub-ranges of the x-range' to evenly distribute the random points selected to make the random selections more meaningful in similarity to the actual probable prediction function
          - this applies info about the possible y-variable value, compares it to corresponding info about the representative data (maximum y-values of random subset functions) and filters out possible y-values based on a threshold selected to be a similarity identifier (the 'accuracy range' allowing a degree of adjacent points to be included in a function)
          - this method looks for a 'corresponding structure in a known structure' (maximum y-value of known representations) to compare a 'possible structure' (possible high target y-value) with
          - similarly, when applying a standard regression method, looking for a 'corresponding structure' ('most adjacent point') in a known structure ('function implied by calculations already done') to compare a 'possible structure' ('possible next or other y-value') with can fulfill intents like 'reduce computations required' (as given the lack of volatility in most functions, points can be connected by curves that match the change rates shown in the rest of the function, so given a 'most adjacent point of a probably correct function', the relevance in the form of the accuracy of a 'possible next point' can be assessed except where the change types like change rates of the function experience a phase shift or other change requiring that some other points' averages be computed as well (instead of applying inference rules to identify other points))
            - similarly, a 'random next point' can be chosen and corrected once compared with change types/rates of the probably correct function, which is likely to reduce computations as well
          - finding other data to include in a data set can increase the accuracy of a solution-finding method for the 'find a prediction function' problem, so is generally useful for fulfilling problem-solving intents like 'improve base solutions on some solution metric'

      - identify useful structures like 'overlapping structures in common' between useful structures like 'stored structures' which can be applied as inputs to functions like 'merge' to generate new possible useful structures
        - example: a 'generative filter' (such as an iteration of a set of possible solutions which applies 'differences' to filtered out solutions to generate more possible solutions to apply as future possible solutions in the iteration of possible solutions) can be derived by noting that 'apply differences to error structures to generate possible solutions' and 'filter possible solutions' have structures in common, that being 'errors in the set of possible solutions which are filtered out as not solutions' and 'known error structures as input to a change function to generate possible solutions', therefore identifying that 'generating possible new solutions from solutions determined to be not solutions in the set of possible solutions' is a useful structure is a matter of identifying this structure in common and the possible usefulness of this new structure (identifying new possible solutions not already in the set of possible solutions by applying differences to the possible solutions once theyre identified as errors), which makes use of the overlapping common structure to integrate/merge these structures to generate a possible new useful structure, which is useful in cases when the set of possible solutions is unlikely to be complete, such as when a function is applied to generate structures which are different from optimal solutions, such as 'maximally different possible solutions' or 'base solutions' as the set of possible solutions

      - identify useful structures by generating a set of possible useful structures (like "possible useful functions with an 'intent' but no logic built yet") and filtering them by metrics of usefulness like 'common components', 'alternate intents theyre useful for', 'what problem-solving intents they would fulfill or functions they would reduce requirements/computation for'
        - example: making a list of useful functions to build to fulfill intent likes 'finding errors' and 'fixing errors' (a function to 'identify useful structures like "root causes" or "common inputs" in error information', a function to 'identify solutions already tried which failed in error information', a function to 'identify possible/probable errors before they happen') would have the output of useful functions like a function 'that generates errors in a codebase/function', as this generative function can generate the inputs/outputs (like 'error information' and 'causes of errors') to other useful functions in the list and can fulfill the primary intents like 'finding/fixing errors' with just one function rather than several, or a function set that can be combined to generate the functions in the list (like 'get definition of solution/error' and 'check if a structure matches the definition like a solution/error'), which are useful bc they reduce the number of functions required, can generate other useful functions, can generate useful information for other functions like 'cause' info, fulfill multiple intents, and/or can be combined/chained to fulfill other intents
        - 'identifying a list of useful functions to build' can be a simple matter of 'identifying common work done which is not implemented in a function yet' (like 'tasks which are done manually'), and filtering this list can be as simple as 'identifying which functions have overlapping functionality, which functions have common component functions which might be more useful to build, which functions enable other functions, which functions fulfill useful metrics like "reduction of requirements of other functions"', rules which can be applied structurally to filter the set of possible useful functions to build
        - a 'useful structure-identification function' is an adjacent alternative to a 'solution-finding method', as the only structure required to convert it into a 'solution-finding method' would be applying adjacent changes to combine & otherwise connect the useful structures found until they fulfill solution requirements
        - this workflow involves a method of identifying 'useful filters of useful structures' (variables which can separate the set of possible useful structures) and 'generative methods of useful structures' (such as by applying workflows like 'derive solutions from solution requirements'), and applying these structures as opposing structures (like 'functionality' and 'requirements' are opposing structures) when generating and filtering possible structures like solutions to the problem of 'identify useful structures to apply in a specific problem or across problems'

      - identify structures which are alternates to other structures that fulfill useful intents like 'reducing storage requirements' or 'reusing existing structures' which are generally useful for problem-solving intents like 'reduce resources required to solve a problem'
        - example: 
          - 'specific information' (like 'examples') can be useful for intents like 'function implementations' (like storing input/output maps if theyre relatively static, or applying changes to these input/output maps based on input similarity, instead of implementing specific function logic) and 'filtering out reasons to use abstract variants like types' which are useful for other intents like 'identifying which structures to store (known constants) and which to generate dynamically (uncertainties/variables)'
          - 'reasons to use a structure (such as the required or probable outputs of using the structure) like a "specific" or "abstract" structure' can also identify when to use a specific structure vs. an abstract structure 
        - these structures ('reasons to use a structure', 'expected outputs of the structure', 'known intents fulfilled by a structure') are alternate structures which can be useful in intents like 'filtering what structures to use'
          - storing the 'reasons to use a structure' vs. storing the 'expected outputs of the structure' (a specific example of the 'reasons to use a structure') vs. storing 'known intents fulfilled by the structure' can provide optimization opportunities, if some of these structures are used more frequently across interface queries, functions, or solution automation workflows
          - other structures with similar functionality as these structures include:
            - 'requirements including the structure or its outputs'
            - 'known contexts/systems where the structure is useful or required'
            - 'structures that are useful for determining the probable output of a structure (like a function), when applied in a system where it hasnt been tested' (such as by finding any structures in the system that could neutralize or alter the steps or requirements of the function, which are capable of interacting with the function or its requirements in that way)
        - other relevant structures, like 'phase shifts where one structure stops being more useful than an alternate structure', are similarly useful for this workflow

      - identify interface structures based on 'interface structures (like change types, cause, etc) of interface structures', to fulfill useful intents like 'find interface structures that are probably found adjacent to each other' to fulfill intents like 'connect interface structures'
        - example: the structures nearest to an interface are likely to be composed of 'core structures' of that interface, are likely to be in the shape of 'overlapping pyramids' (describing multiple 'combinations of core structures' (bases) with different outputs (peaks)) than other shapes like a 'set of circles returning to the interface' or a 'tree with outputs pointing at the interface', where 'similar changes across these pyramids' are likely to represent 'phase shifts' and 'interaction levels' and 'system layers (of a system layer diagram)', and the changes farthest from the interface are likely to be 'outputs' of the other changes rather than 'adjacent changes resulting from core combinations of core structures', given how changes on an interface usually interact, such as in 'fractal patterns'
        - 'finding connected interface structures' is useful for many problem-solving intents, like 'find alternate interface structures (like alternate "definitions")', 'find cause of interface structures (like changes)', etc

      - identify useful structures that fulfill required intents like 'preserving an interface' which is required for a system to exist, interfaces being a core structure of systems, representing stable states of the system
        - example: a 'limiting structure' and a 'changing structure' are required 'opposite' structures, which are required to fulfill required 'preservation' intents like 'make sure change doesnt get out of control' in a system which would destroy the system, which is necessary if the system is to exist or be useful, and necessary while the system is still more useful/beneficial than costly
          - similarly, a 'change that is possible on an interface' (like a 'base line') is a 'change that doesnt destroy or destabilize the interface' (like a 'smaller line than the base line, applied in a direction other than that of the base line')

      - identify useful structures determining relevance, such as 'similar differences that make variables relevant' to supplement, replace, or implement known useful structures like 'interaction levels' to fulfill problem-solving intents like 'find relevant variables'
        - example: a predator's 'claws' and 'teeth' might be the most relevant variables to its prey, unless it also has a tail which can attack and reach the prey, which is relevant to the other variables by a 'similarity' in the 'possible values' of the 'position' variable, answering the question, 'which variables are relevant to the prey, when the prey is in position x' to identify 'variables that can have a "similarity" in position', thereby making these variables possibly relevant to each other ('teeth' and 'claws' and 'tail' could be relevant to each other by being 'interchangeable alternates' for the 'attack' function), so the question 'which variables are relevant to the prey, when the prey is in position x' can be converted to 'what variables can occupy the same position as each other, and the same position as the prey'
          - similarly, 'tools which can be used by some variable to "extend their range"' would also be relevant variables, as they change a 'previously irrelevant' variable for a position into a 'relevant' variable for that position by increasing the 'positions the variable can be relevant to', which is a target structure for identifying relevance of a variable
            - this can be used to derive relevant variables, by identifying 'variables which can make another variable relevant' (what types of change would interact to fulfill intents like 'extending distance reachable' which are relevant intents to variables like 'position' which are relevant for being under the control of the prey) and identifying 'whether those variables exist'
          - 'variables' can be determined to be relevant by a 'similarity' like this, which makes them 'interactive' (interactions such as 'coordinating', 'neutralizing', or 'overlapping') in some way, so that they can be 'combined', which is a core interaction function of 'variables', and this means that variables can be derived by applying 'similar differences' which allow finding other variables which are different enough to be classified as an isolatable variable but similar enough (in a similarity like 'possible positions') to be interactive (on the same 'interaction level' as another variable)
          - this is similar to how finding an orthogonal dimension can be found by applying a 'similar difference' to another dimension ('rotate the x axis 90 degrees to find the orthogonal dimension y'), which is not so different that it is equal to the original dimension (as 'rotate 360 degrees' would accomplish) or 'irrelevantly different' (as 'rotating 180 degrees' would accomplish) both of which would essentially generate the same line as the original x axis, and not 'irrelevantly different' in that the rotated line has an overlap in the 'change type' it describes, in describing horizontal as well as vertical change, and applies the x axis as a base for this change, which can be used to derive the new axis by applying core change types, or applying intents like 'find the changes required to create a useful difference, like describing different differences (vertical as opposed to horizontal) by identifying the changes described by the original structure (horizontal) and identifying changes not described by that structure and identifying a base (y axis) that could describe it', creating a useful difference in a line rotated to 90 degrees which can measure an 'isolatable (different change type) but interactive (similar interaction level)' change type (these changes are not so different that they cant interact, but are not so similar as to be interchangeable)

      - identify structures of relevance to find relevant structures for functions commonly used in solution automation workflows like primary functions like find/build/derive/apply to reduce the computations run by these functions, and therefore reduce steps to implement workflows
        - example: the 'apply' function applies changes in the form of a structure to another structure, in which the 'structure being applied' has to match the inputs of the 'structure its applied to', otherwise the structure being applied is not relevant to the structure being applied to, for example, applying a 'collision function' to 'blocks' is only relevant if there are 'multiple structures like "blocks" which can be collided' and if the 'blocks dont have an overriding function which prevents collisions like magnetic forces', meaning 'does the collision function have its inputs (blocks to collide, no collision-prevention structures) fulfilled'
          - so checking for '"inputs" which can make a structure useful to apply' is a filter of 'structures which can be applied to other structures'
          - similarly, with the 'find' function, only 'structures which can be found (structures which are measurable, structural, stable, stored, unique/determinable/identifiable compared to other structures)' can be found, so only these structures are relevant to the 'find' function
          - similarly, 'structures that can be connected' are relevant to the 'derive' function, and 'structures which can be combined' are relevant to the build function (as well as any structures which can make other structures findable, derivable, buildable, and applicable, making these structures relevant to these functions)
        - this workflow applies a definition of relevance to find structures that are useful (in the sense of being usable and therefore relevant) to the primary interaction functions, to filter out structures which are irrelevant when using these functions
        - this means 'structures which are findable' are a reduction of the solution space when applying the 'find' function
        - similarly, 'reduction structures' (such as 'any attribute which can differentiate a subset from another subset' applied to the 'solution space' of 'possible solutions'), are another useful structure to the 'find' function, given its definition
          - these reduction structures can be particularly useful when designing a filter sequence/tree/network that can optimize a query based on relevant 'find' intents
          - for example, identifying 'differentiating attributes (variables) of a solution space' can identify which 'differentiating attributes' should be applied as filters to 'maximize coverage of the solution space', to 'filter a subset of the solution space', to 'increase the chance of finding multiple alternative solutions or a unique solution', and other intents related to 'find', like 'filter', 'identify', 'differentiate', etc
        - identifying 'variables that can be controlled' is another example of variables which can be filtered (focused on) to identifying 'possible variables of a solution', as 'variables which can be controlled' are irrelevant

      - identify useful structures like 'sufficient similarity' to find structures which can be used to fulfill common intents like 'find alternates of a structure' which are useful for problem-solving intents like 'find alternate solutions'
        - example: applying code validation rules like the following can be used to correct logic, given the general or common applicability of such rules
          - 'if an input variable is tested for and not found to be initialized, and is assigned a value if this condition applies, apply the output of that assignment as a filter of the input value, if the input is initialized'
          - example of this code validation rule:
            - if the 'interfaces' input variable has a value, apply the primary interfaces found with the find('list', 'interface') function call which returns all defined interfaces, as a filter of valid values of the interfaces input variable
        - this code validation rule is not always true, but is generally or commonly useful
        - finding 'invariant structures' such as 'rules which are always true, in all possible cases or case types' which can be used to derive definitions, as structures of certainty such as structures which dont change
          - similarly 'rules which are generally or commonly true' can be used to find 'probable structures of certainty'
        - given that 'structures which dont change' can be used to find structures like 'definitions' given their common relation to 'certainty' structures, structures which are sufficiently similar can be used to identify/derive/substitute each other

      - identify the structures that can fix the causes of a problem, applying insights about problem causes such as that 'problems or problem causes are usually useful in some way to some entity if they exist enough to be noticed, or they wouldnt exist'
        - example: if a problem exists, its either beneficial to some entity with the ability to use the problem to its advantage or create the problem, or its not a big enough problem for enough entities to trigger the structures (like "priority increase to highest priority") that would be necessary to fix it
        - so finding structures like 'functions that find alternative ways to benefit those entities than the problem' would be useful in addressing the 'causes' of the problem (the 'benefit to some entity', the 'problematic attributes of that entity like "parasitic/predatorial strategies to benefit from causing problems"', or the 'lack of solution-triggering structures')
          - the root causes of these causes includes:
            - 'inability of all entities to understand & prioritize meaning (meaning in the form of the impact on the host, as if a pathogen kills the host, its also hurting itself, but it cant detect that bc it cant grasp context, time, emergent conclusion like "host death" of structures such as "parasitic patterns applied at scale which dont encounter reducing/opposing structures like limits", and other structures that could make it aware of the impact of its structures on other structures)'
            - such a solution would apply the 'distribute solution-finding methods and priorities to all structures, such as functions/objects (like pathogens relying on a host to survive), rather than limiting problem-solving capacity to some structures (like immune cells)', which would possibly take a form such as 'distributing host immune markers to all cells and creating costs to all cells based on those markers, so the pathogens get negative feedback for parasitic strategies, rather than short-term positive feedback leading them to repeat that strategy until the host dies' or 'creating short-term feedback that represents long-term feedback for pathogens or mutations capable of killing the host, if they can only use short-term feedback and dont have the resources like "storage" or functions like "computation" to make long-term feedback useful', or 'find all specific strategies (or strategy inputs/states/components) used by cells that eventually lead to host death and prevent cells from using them'
        - similarly the problem causes may be useful bc they cause the problem, meaning the problem is 'caused by some function' and 'not solving the caused problem makes that function useful to some entity by reducing the work involved in that function', so functionality to 'minimize resources needed' would help that entity avoid creating such a function, so 'distributing this functionality to all entities' would be a useful structure to fix the problem causes, one of which is the 'inability to minimize resources needed'

      - identify structures like 'high variation causative structures' that are useful in changing relevant variables like solutions/errors, as in 'causing a solution to be an error structure' or 'causing an error to become a solution structure', which are useful variables for many problem-solving intents like 'filter solution space' or 'change existing solution to improve a solution metric', as the 'opposite of a solution' is an important structure to avoid, and knowing the cause of an 'opposite' is useful in avoiding that
        - example: identify 'high-variation causing structures' like 'reasons that completely change the meaning of a structure (such as how "testing for a reaction" can completely change the meaning of a particular "statement")' or 'cycles (such as circular or wave functions) are a structure which completely change the resolvability of an ambiguity and change what info is required to resolve it, like adjacent point context of the original point to find the output value for', as 'high variation causing structures' are likely to be useful in identifying 'opposite' structures which can change a solution into an error
        - this workflow involves identifying useful structures to avoid to fulfill problem-solving intents, and the structures that can make that identification trivial

      - identify useful structures like 'contradictions of insights' that can identify useful structures like 'relevant changes to apply to existing solution-finding methods' in a particular problem format like 'find a prediction function' to 'filter the solution space' of 'possible solutions' to the problem of 'find changes to apply to solution structures' in the workflow 'improve an existing solution by applying changes to it'
        - example: a 'regression' or 'machine-learning' method might find no correlation between two sets of variables (like the 'number of products purchased in one region' and the 'environment impact in another region') but as 'all variables are related', this would be a false result, so a robust solution method would have a mechanism to find correlations given this insight that 'all variables are related', allowing for distant causation or different interaction levels than those in the original data set like 'agent decisions', 'time', 'markets' and 'market manipulation', which can be produced by applying interface structures rather than simple 'combinations of input variables', thereby applying inferred or general or common variables (like 'variable patterns'), rather than just the input variables and just the 'combine' operation
        - just by applying this one insight, interface structures can be inferred, so identifying a 'contradiction of the insight' present in specific problem-format solution-finding methods like regression & machine-learning methods is particularly useful
        - an adjacent method to derive interface structures is useful on its own, but is also useful for deriving new useful structures adjacently
        - similarly other insights like 'identifying connections of an isolated variable set is rarely useful without context' is another insight that can adjacently derive interface structures (like the 'cause' of the variables and the 'intent' of identifying the connection between isolated variables, as in 'to correct lack of info' or 'to find specific info like causal variables' or to 'filter out non-causal variables')
        - if the insights are in fact insights, they should be confirmed or reflected in a good solution that reflects the truth, rather than contradicted by that solution
        - similarly, a statistical or machine-learning solution-finding method might identify a 'correlation' between the 'green skin' and 'aliens' but a solution-finding method using interface structures would identify a 'combination of errors' that would cause a 'false similarity', such as 'filtering out or under-prioritizing examples of failures (where a similarity is not significant or related or accurate)' and 'over-prioritizing simple structures like structural similarities (like similar colors or shapes)', leading to 'agent' interface structures like 'conspiracies' ('systems created by lack of information in the form of an over-prioritization, that seem based on truth until a counterexample is found to identify the missing information resulting from the priority'), since interface structures apply 'cause' to find the cause of a structure like a 'similarity' (causes like errors of over-prioritization, missing information, flexibility resulting in unenforced truth adherence, tendency to over-simplify, such as by simplistically classifying rare variable values like 'green' as meaningfully similar to other rare variable values like 'foreign' despite no reason to associate these)

      - identify the reason ('equivalence' in inputs/outputs) for the usefulness of a structure (like commonly useful structures such as 'simple' and 'complex' structures) and apply it to find useful structures like 'simple/complex set that have equal inputs/outputs', given that 'equivalences' are generally useful for problem-solving intents like 'connect problem/solution' or 'find a function implementing an intent'
        - example: identify that structures like sets of 'simple and complex structures with the same inputs/outputs' can fulfill useful intents like 'find implementation/fulfillment structures of an intent', as a simpler structure like an 'intent' is likely to be fulfilled by a more complex structure like a 'function fulfilling that intent' if the inputs/outputs of the two structures are equivalent, which makes one structure useful to the other for various intents ('find a simplified representation of the function (such as the intent)', and 'find a function that fulfills the intent')
        - these structures are useful for other intents, like 'find the variables to apply that convert a simple structure (like the intent) into an equivalent complex structure (like the implementing function)' which is useful for problem-solving intents like 'find a function that implements an intent'

      - identify useful structures such as 'solution component filters' that are useful for common intents like 'filter' to apply in workflows that use those intents like 'filter the solution space'
        - example: a function necessarily changes some inputs, so anything that doesnt change an input can be ruled out as a possible solution function to the 'find a prediction function' problem, and any 'combination of changes' can be included in the set of possible solutions to the 'find functions' problem
        - this workflow involves identifying useful structures by applying common core intnets like 'filter' to common problem/solution structures like 'solution components'

      - identify useful structures like 'contradiction example input/output mappings' that fulfill useful intents like 'find useful structures like "change filters"' which are useful to many problem-solving intents, like 'improve a solution by applying changes to a function'
        - example: storing a 'input/output mapping' of 'contradiction examples' as metadata to store with a 'function definition' is a useful structure to store with a function bc it provides a target for improvements to the function which is useful for filtering future changes to the function (based on a solution metric of whether they improve handling of those contradiction examples), as well as taking the place of logic that would be required to handle those specific counterexamples

      - identify useful structures like connections between problems and structures connected to solutions (like functions, each function being a solution to a problem)
        - example: solving 'which problems create which functions' (which problem maintains the need for a function or leads to the creation of that function) as a way of determining alternate implementation methods of functions (applying problems as a function-generating structure, if the problem is adjacent enough to the function with existing resources that it can be converted into the function by default or otherwise adjacently)
        - this workflow involves applying the 'inputs' of solutions (including 'problems') as an 'input' to solve problems like 'find a function' which may be applied in interface queries, answering the question 'what problem would develop the function required to solve a problem like "find/build/derive a function"'

      - identify useful structures like 'probability' and 'reason for probability' that are useful in identifying useful structures like 'certainty structures' which are useful for many intents like 'identifying truth structures like insights', 'testing if a structure is true/certain', and 'implementing functions that are useful in that they reflect reality' which are usable in many problem-solving intents like 'filter the solution space'
        - example: certainty structures like constants & limits are relatively uncommon compared to uncertainty structures like variables, bc certainty structures are useful for changing the changes allowed by variables and act like a controlling structure on variables, which is useful when a problem occurs bc of the variables, and given the corrective power of certainty structures on variables, this further indicates that fewer certainty structures are required compared to variables (to explore useful differences to new problems), as a problem is only likely to result from an uncertainty structure that doesnt reflect truth and is only likely to be changed by a certainty structure correcting that incorrect uncertainty structure so it better reflects truth, this dichotomy serving as a useful opposite structure to balance problems/solutions between, as solutions must have some variation (uncertainty) to allow for adaptation to different contexts and different new problems, but must have some degree of certainty (constants, limits) in their reflection of the truth
        - the reason for the 'lower probability' of the truth structure is the 'lower requirement for truth structures to manage uncertainty structures', the indication that 'if a truth structure is found, its likely bc a problem (a problematic difference from reality) had occurred which the truth structure corrects', and the 'higher usefulness of uncertainty structures like variables in many intents except for intents like "correcting an incorrect structure generated by an uncertainty"'
        - a variant of this workflow is to 'apply changes to certainty structures' (like insights, patterns, or known solutions) to create useful differences like improvements to suboptimal known solutions, changes that stay within the limits created by those certainty structures (not so different that the differences violate the truth), where certainty structures are not sufficient to solve a problem, so uncertainty structures like variables/differences can increase the usefulness of a solution, since 'previous truths' act like a base for 'new truths' so applying changes to 'previous or knwon truths' can produce 'new/unknown truths', which applies this as an insight generating this solution automation workflow, which applies truth structures as an interface around which uncertainty structures like changes vacillate before resolving into truth structures (the 'interface network' being a structure that requires the fewest differences in order to become useful and reflective of reality while maintaining its ability to change)

      - identify useful structures like 'opposites' that are useful for multiple core intents like 'find differences' and 'apply difference' and 'filter' which are adjacently useful to problem-solving intents like 'filter solution space', when applied to specific structures like 'requirements' to identify other specific structures like 'the set of (interchangeable alternates and requirements)' which are useful in specific contexts like 'where requirements are suboptimal' such as by 'applying variables to create interchangeable alternates of the requirements' to solve the problem of 'removing suboptimal requirements' or 'apply changes to requirements to make them different from requirements (optional)'
        - example: identify structures like 'interchangeable alternates' as an 'opposite structure' to structures that can be suboptimal like 'requirements'
        - a related workflow involves identifying problems where a particular solution automation workflow would be more useful to correct the problem (such as how this workflow is particularly useful when 'variability in solution requirements would be useful', or when a 'requirement is a problem cause') and applying changes to change the problem into that problem (changing the original problem into a problem of 'invalidating, removing, or changing requirements'), by applying differences to the solution automation workflow to find problems that would benefit from that workflows' connection structures
          - example: apply differences to the 'identify "opposites" as useful for intents like "find differences" to identify other useful structures like "interchangeable alternates" as useful for creating other useful structures like the "set of interchange alternates and requirements"' workflow in order to find the problems (like 'removing suboptimal requirements') where the workflow is particularly useful
          - "identifying interchangeable alternates" would help with the problem of 'removing requirements'
          - applying 'changes' to "identifying interchangeable alternates" would result in identifying 'opposite' structures of those structures ('requirements'), thereby identifying any problems that involve 'problematic requirements' as a useful workflow to solve that problem
          - keeping 'example' structures in a solution automation workflow definition is useful in 'reducing computations' to 'find relevant structures to that workflow', which is useful for intents like 'find similar/different workflows' or 'find interactive/connective structures with a workflow like a problem where the workflow is particularly useful'

      - identify structures like "connections between solution metrics (like 'fairness') and structures that can be identified with existing functions (like 'certainty')" as a 'reason' to apply the 'existing (certainty) structures' to fulfill the 'solution metrics' (fairness), as opposed to other structures, to fulfill problem-solving intents like 'find structures that fulfill solution netrics' and 'filter the solution space (of structures that fulfill solution metrics)'
        - example: identify structures (like more structures such as variables or structure-generating structures like questions) that capture info better than simpler structures (like fewer structures), such as how the 'question of whether an individual is good or evil' is optimized by asking additional questions like 'what is the connection of their abilities/functions and their requirements (inputs and required outputs)' and 'what is the impact of their decisions (weighing the importance of outputs (as interactive with other structures) as being similarly important as their inputs, at various scales like global/local and other variables like time (the impact on their future decisions) and examples (the common impact on future decisions by other recipients of that feedback)' and 'are their decisions common (invalidating the focus on an individual as opposed to a group)' and 'are their decisions avoidable' and 'what is the impact of feedback on their decisions', questions which apply 'additional' interface structures (which act like connection structures and certainty structures) and apply 'fairness' as a 'solution metric like accuracy' of a solution (solving the problem of 'finding solutions reflecting truth' as a way of identifying the solution metric for identified alternate question sets of 'does the question generate fairness')
        - this workflow involves identifying better formats (like a 'minimized' variable set or 'accurate' variable set) for structures (like a variable set) that fulfill an intent more optimally despite impact on other solution metrics of problem-solving intents like 'minimizing storage requirements', bc these better formats apply additional interface structures and therefore fulfill the problem-solving intent of 'improving a solution (by solution metrics like completeness, accuracy, robustness)', and identifying how to convert to those formats, such as by applying "core interface structures like 'input/output'" to "other interface structures like interaction level structures (like 'decisions' on the 'agent' interaction level)" to identify structures of meaning 'what is the impact of their decisions'
        - a variant of this workflow would identify 'structures (like 'additional') of interface structures' that improve solutions, such as how a solution involving additional connected interface structures is likelier to be robust, correct, and relevant/useful/meaningful

      - identify structures that can fulfill problem-solving intents like 'filter error structures' when fulfilling useful structures like other problem-solving intents such as 'find and remove error structures'
        - example: identify that 'selfishness' is a useful structure bc of the 'reason' that its a 'high variation structure', and identify that 'high variation structures' are unlikely to be an 'error structure' that should be 'removed' (as opposed to 'changed') when applying the 'find error structures like error causes and remove them' problem-solving intent
        - for example, the 'selfishness' structure is not 'required' to be applied (its optional, which adds a variable), and it doesnt have to be applied in the same way (like only to protect the individual, as opposed to protecting groups they belong to, which adds another variable), so when selfishness is suboptimal (like defending a group member just bc of membership), it can be avoided, and when its optimal (like in deadly contexts), it can be prioritized
        - the high number of variables in how selfishness is applied indicate its an important structure and also that its unlikely to be an error structure, given its variable usefulness (rather than 'absolute uselessness')
        - this workflow involves identifying useful structures such as 'high variation structures' which can 'filter the solution space' when applying problem-solving intents like 'find & remove error structures', as 'structures too important to remove' are a useful structure to 'filter the solution space (of structures to remove)', as 'required tasks' like 'organizing resource allocation' and 'sustaining life' would be almost impossible with existing brain functions if people didnt organize their attention using 'selfish' priorities like 'finding resources fulfilling their own basic requirements'

      - identify useful structures like 'high information reduction structures' (such as the 'similarity attribute') that fulfill common useful intents (like 'reduce calculations required', 'summarize', 'identify alternatives/equivalents') in a problem space, as these structures are likely to be useful for other intents, some of which may be problem-solving intents ('reduce calculations required')
        - example: a program could only store the 'similarity' of content rather than searching all the attributes of the content or the content itself, and use that similarity to show recommended alternatives, rather than requiring a custom complex algorithm to find what users are likely to view next, applying the insight that users are likely to be interested in similar content to that they intentionally viewed, rather than deriving their intent in a complex algorithm, as some problems can be reduced to fewer attributes, and these fewer attributes can be useful for fulfilling intents like 'privacy' or 'reducing memory use' or 'reducing computations required'
        - this workflow involves finding structures (like an 'attribute' such as 'similarity') that encapsulate a high amount of information (such as whether information is related to other information, to fulfill intents like "identify whether the information is connected, alternative, similar, coincidentally similar, or some other intent that can use 'similarity information' as an input") without losing usefulness for other intents such as semi-derivability of that information
          - if information is similar to other information, it can likely be derived to some degree (or the set of possible information can at least be filtered) by applying the difference represented by the similarity to the other information, given the similarity definition
          - the information is required to be slightly different, otherwise there is no 'reason' that could trigger a 'view decision' (such as 'viewing new content (new meaning slightly different)')
        - related workflow: 
          - a variant of this workflow involves identifying structures (like the 'similarity attribute') that can be simultaneously useful for commonly useful intents like 'deriving a degree of info' and 'identifying similar info' and 'preserving privacy' which are likely to be required simultaneously
          - a variant of this workflow involves identifying the insight ('find similar information to show users bc its likelier to be something they want to view if its similar enough to what they already indicated they like, through structures of intent that are also structures of certainty') associated with the useful structure (the 'similarity attribute') that can be used to identify the useful structures, which would involve identifying the 'reason' ('similarity to content that was defined to be liked') of the target structure (the 'view decision'), as the user supplies a 'certainty structure' and a 'structure of intent' in their 'view decisions' (the solution metric being optimized), and the 'reason' for the 'view decision' (they wanted to view it, liked it, it was relevant/interesting, they liked the specific information in the content, etc) can be applied as an 'input' to trigger additional 'view decisions', an algorithm that prioritizes the 'view decisions' as trusted information indicating what they want to view, and prioritizing the 'specific information in the content' as the structure that the users are aiming to get (rather than 'structures of lack of intent' like clicking accidentally, or because it was the top search result or otherwise available/adjacent), an algorithm that could apply the 'apply changes to a base solution to find other solutions' workflow to 'identify the insight that similar content might also trigger a view decision'

      - identify useful structures (like 'attribute count') that are useful when a particular interface query is run (like 'applying the probability-structure interface') to fulfill a particular problem-solving intent (like 'filter solution space') to solve a problem (like 'implement a function such as identify/find structures')
        - example: an ambiguity is likelier to be a 'core structure' (with fewer attributes) bc of the improbability of finding 'ambiguous alternatives with a high number of attributes' or a 'high number of missing attributes' leading to the ambiguity, which applies the 'probability-structure' interface to solve the problem of 'filter the solution space' of possible solutions to the problem of 'identifying/finding structures' (find 'ambiguities' by finding 'core structures'), like how saying "we're (verb)-ing" could be interpreted as a decision of a group or a description in a conversation
        - solution success cause: this is useful bc of the counterintuitive nature of the connection between 'ambiguity' and 'core structures' (as in, its not specified by any definition route of ambiguity or core and can be added as an alternative definition route when more certain definition routes fail)
        - generalization: this workflow can be abstracted to find the specific intents/problems and interface queries that would produce a useful structure like a 'counterintuitive connection', such as identifying the probability-structure interface as being usable to connect an 'ambiguity' and a 'core' structure, given their similarity in an attribute metadata attribute (attribute count) which is a structure of 'probability', so identifying the probability interface as useful amounts to the operation of 'identifying the reason for common structures between two structures to connect' (probability being a reason for commonness) and 'applying that reason for commonness as a method of connecting the structures' (connecting them in the sense of finding relevant similarities, relevant to the task of 'identifying alternate/connected structures of a structure, as a way of identifying that structure')

      - structures like 'functions applied' are useful to identify useful uncertainty structures like 'assumptions/predictions about structures like the usefulness, resources, requirements of those functions', 'assumptions/predictions' which may or may not be certain or otherwise useful insights that can be generalized or applied to find other useful structures, insights which are useful to derive
        - example: if an agent calls a function, the agent assumes and predicts that the 'outputs will be useful for its intents'
          - the 'reason the outputs are useful for their intents' may not be clear but are likely derivable and likely to be useful to derive
          - these are similarly useful structures as 'implications' of 'using a function', as 'structures of probability' (as opposed to certainty) adjacent to the function (there is a probable reason to find the outputs useful, there is a probable reason that is considered useful, there is a probable resource making that useful) which are useful in finding 'connecting structures' to connect the function with other structures like other useful functions fulfilling those predictions, given that if it exists, its likely to fulfill its predictions better than average

      - identify alternate routes to fulfill intents like 'identify useful structures' that are likely to already exist (like problem structures are likely to be already known)
        - example: identifying 'problem/suboptimality' structures is a way of identifying 'functions & inputs that dont exist yet' and 'differences not handled by existing functions, which are suboptimal for some useful intents given that these problems have been identified as a problem for some agent', where 'functions that dont exist yet' are a useful structure to determine adjacent structures like opposite structures, such as 'functions that do exist or are known', applying the insight that 'if a structure (like a problem) exists and has not been reduced (solved), the functions to reduce it (solution functions, solution-finding functions) are less likely to already be known or applied to solve it, if there is a reason to reduce it (the structure is a problem)' which indicates that this structure (a problem) can be used to determine which 'reduction functions' (solution functions) exist

      - identify useful structures like 'connecting structures' of 'problem/solution structures' and 'causal sequences' which are useful in intents like 'finding alternate connections between problem/solution structures' (finding new solution automation workflows)
        - example: the 'cause' of a 'new solution-finding method' might be identifiable at any point in its causal sequence (a 'suboptimality/error is identifiable (measurable) & identified', a 'solution is required', 'solution is possible', a 'solution is tested', a 'solution automation workflow (like 'applying all known solutions until one works') is applied and didnt fix it' a 'solution is identified as an improvement'), but only some structures in that sequence are useful as identifiers of a 'new, more optimal solution' ('inputs', 'requirements', 'reasons to find a new solution, such as a suboptimality/error'), as other structures in the sequence are irrelevant (the 'requirement' of a solution is irrelevant if it is 'impossible' to be solved so 'possibility' filters would have to be applied first in a sequential workflow, and the 'identifiability of a suboptimality' doesnt cause the solution, and none of these structures cause the specific solution structures applied in the optimal solution, unless there is only one solution, making those structures 'required constants')
        - if a workflow like 'trial & error applied to known solutions' is applied and found to not find a solution, that is a cause of applying functions to fulfill problem-solving intents like 'find new workflows', and is a cause of identifying the problem, but is not a direct cause of the solution

      - find useful structures like functions to apply to useful structures to find other useful structures
        - example: an abstract variant of a 'dichotomy' (with the requirement of a 'count' attribute value to be a constant value) is an 'exclusivity', which is useful for intents like 'find structures that dont co-occur' (like an 'impossibility' and a 'possibility' being connected as 'equivalents' of the same structure, these connections not occurring in any system that follows rules (a logical system, a real system, etc)
          - 'find structures that dont co-occur' is useful for other useful intents like 'selecting between alternates' and 'find opposite/different structures' which are useful for intents like 'differentiate/connect structures' and 'filter structures'
        - 'mutual exclusivities' are a 'core structure' related to other core structures like 'differences'
        - a 'falsehood' combined with the 'exclusivity' (a 'false dichotomy') is a useful structure in that it applies a core error structure 'false' to a core structure 'exclusivity' of core problem structures like 'difference', identifying a 'specific example of the error structure' from applying this combination function (thereby applying a 'false' filter to the set of abstract exclusivity structures to identify the specific structures that are 'error structures')
        - 'abstraction to find alternate examples' and 'combination with other useful structures like error structures to find specific error structures' are the useful functions to apply to the useful structure of a 'dichotomy' to find other useful structures to fulfill other useful intents

      - find structures that are useful in other problem-solving intents like 'finding other useful structures'
        - example: find 'new examples' of a structure like a 'type', since new examples are likely to enable adjacently identifying other useful structures like the 'route to find the useful structure', such as by connecting the 'system context' to the 'new example', which is likely to be a 'new route to useful structures' (like the example structure) if the example is genuinely new, and since there's likely to be a 'reason' for the 'difference' in the new example if its an actual unique example, a reason such as a 'new interface structure' causing the newly different example (if the difference is on the right interaction level and not just any difference in any attribute), and as differences in examples of the same structure are an important signal since 'examples of the same structure' are likely to have multiple similarities, probably more similarities than differences
          - 'finding attributes that connect very different structures' is a useful structure to 'find more adjacent routes connecting structures' (like how finding the opposite of a structure like a 'counterexample of a statement' or the limits of a structure can be more adjacent than finding the original structure) and 'find new differences to apply to create different structures' (like 'like creating maximally different alternate solutions to filter the solution space or the solution space of base solutions to change')
        - 'finding connections to useful structures' is a useful intent to make 'new examples' useful structures
        - 'finding a new example of a useful structure' is likely to enable adjacentlying identifying a new reason for the structure being useful, which is useful to apply to 'find new useful structures' or 'find alternate useful structures'

      - find structures that are useful for problem-solving intents like 'filter solution space' as useful structures like 'filters' applied to fulfill the problem-solving intent of 'find useful structures'
        - example: the 'reason for the irrelevance' of a 'structural similarity' is particularly useful to fulfill core interaction functions like 'filter' or 'differentiate' between relevant & irrelevant structural similarities, which are difficult but useful to identify/filter/differentiate, as 'structural similarities' are very useful structures when relevant
          - the 'reason for the relevance/irrelevance' of a 'structural similarity' may just be that the structure found to be similar to another structure is just a 'common structure' or a 'core structure', rather than its similarity to the other structure being similar for a relevant reason, like a similarity in the reasons why the structure was caused (like to 'apply it for the same intents')
          - the 'similarities in cause (reason) of the structure' and the 'similarities in intent of the structure' are therefore better structures to check for filtering structures that are relevant, as opposed to just applying 'structural similarities' on their own
          - the reason why 'causal similarities' and 'intent similarities' are useful to find relevant structures that are similar (as opposed to irrelevant structures that are similar) is that the cause/intent structures are likelier to be aligning with the reason for identifying/finding that structure in the first place (structural similarities arent typically sought to 'find common structures', theyre typically sought to 'find alternate structures to fulfill a particular purpose' or 'find all the examples of a structure'), therefore there is a (useful) 'structural similarity' between intent/cause of 'structural similarities' and the intent/cause of 'finding structural similarities' (which include intents like 'find all the examples of a structure')
          - in the reverse direction of derivation, 'structural similarities' can be a useful structure to fulfill intents like 'find common structures' or 'find core structures', as common structures are likely to be in a set of similar structures, and common structures are likely to be core structures
            - this fulfills the useful intent of 'find the context where a structure would be useful' to enable fulfilling other intents like 'check for that context to find useful structures to apply in an input context'
          - 'cause/intent' are therefore useful as a filter of 'structural similarities'
            - another reason a structure might be found to be similar is the structure is 'optimal' in some way, such as that its useful for some intent like 'find stable structures', so the structure is likely to be repeated as its optimal for some intent which means it may be useful for other intents or that intent (and the associated optimal structure fulfilling it) may be required elsewhere
            - the 'repetition' attribute is a useful 'connecting structure' of related structures like 'similarity' and 'commonness' as a 'cause' or 'output' of both structures, which makes it useful for intents like 'connecting/differentiating structures' which are useful for problem-solving intents like 'connect problem/solution' and 'filter solution space', which enables intents like 'identify useful reasons why a structure would have attributes' like 'similar but not common' (its a new optimization that hasnt been deployed elsewhere yet), 'similar and common' (its repeated often), etc, which is useful to differentiate the 'related and similar but unique' structures and differentiate when one structure can be used to identify the other or as a proxy for the other (when commonness is relevant vs. when its not)
            - 'related and similar but unique' structures are useful for many intents like 'approximate', 'substitute', etc, but should be filtered by relevance (the 'reason for the similarity is the same', or the 'input/output connections are the same' (inputs/outputs being a core useful structure))
          - adding a similarity of a 'similar system' in which the structures both develop is another structure indicating the relevance of the 'structural similarity', acting as a 'cause of relevance' which is a useful structure to find/evaluate in place of finding/evaluating relevance to fulfill intents like 'find useful structures'
          - this emphasizes the usefulness of structures like the 'count of similarities (like cause, intent, and structure) between structures' which fulfill the useful intent of 'increasing the probability of a structure being useful' which is useful for 'finding useful structures', as the more similarities in interface structures of a structure there are, the likelier that structure is to be useful, as on its own, one similarity may not be enough for usefulness
            - the more interface structures that two structures have in common, the likelier it is that they have the same meaning and related structures of meaning like usefulness
          - having a cause/intent in one position of a system means its likely there are other causes/intents that are similar to that cause/intent bc there is no 'limiting rule enforcing uniqueness of structures in a system' and 'repetition of structures in a system is a common attribute of system structures', so 'identifying if the structures both exist in the same system' is another useful intent to fulfill in finding out whether a 'structural similarity' is relevant to intents like 'find all examples of a structure'
            - 'having the same cause/intent' means that those causes/intents exist and are likely to be repeated (occur elsewhere), and the causes/intents are likely to have the same output when found elsewhere

      - identify & store the interface structures associated with interaction levels that make those structures more useful for useful structures like 'dichotomies' (like by fulfilling 'required' or 'otherwise useful' intents) in fulfilling a problem-solving intent like 'fulfilling as many problem-solving intents as possible'
        - example: the 'attribute interaction level' would have identifying differences like the 'solution metric value (such as the accuracy) of a particular solution function'
          - the 'adjacent structure interaction level' would have identifying differences like 'similarity of sub-function or function component sets with solution functions'
          - these interaction levels are optimal in both different & overlapping contexts, for both different & overlapping intents
          - different operations can be fulfilled more adjacently on different interaction levels, like:
            - how components on 'adjacent structure interaction levels' (such as 'function components' or 'incomplete functions' or 'sub-functions' or 'input functions') can be more adjacently converted to a solution function with fewer steps than other formats
            - how components on 'attribute interaction levels' can be more adjacently abstracted, as they already depend on a particular attribute, so abstracting the attribute or removing components of the attribute's interaction with specific structures or specific problem/solution structures (removing the 'solution function' specification to allow connecting the attribute to other functions like 'solution-finding functions') is an adjacent operation on that interaction level
          - 'finding a solution function with an accuracy level' may be more useful than 'finding a solution function composed of known incomplete solution functions', given the associated interface structures of these interaction levels (the contexts, intents, & functions more easily fulfilled with those structures) and the 'info requirements', 'change requirements', 'success probability', 'general usefulness across common problem-solving intents' and other useful metrics of these interface structures & interaction levels
          - this applies the 'functionality/resources vs requirements' dichotomy to find useful structures for problem-solving intents like 'fulfill solution requirements', like 'functions fulfilled by applying an interaction level' (a mix of interaction levels & other interface structures)
          - this identifies a new useful function structure as 'incomplete functions' (useful for having a common error type built-in to a solution-finding function, that is resolvable with derivation methods like 'applying patterns', to fulfill useful problem-solving intents like 'apply solution to real information which is likely to have common error types, to "connect erroneous inputs to solution outputs" as well as "connecting correct inputs to solution outputs"', applying the insight that 'errors are probable in inputs' to fulfill the problem-solving intent of 'find a solution-finding function that handles common or otherwise predictable error types')
          - this identifies another new useful structure of 'different and overlapping structures (like functions/attributes)' as useful for finding commonalities, similarities & differences
        - differentiation: this workflow applies an abstraction ('dichotomy') to the core problem structure of 'differences' and finds interface structures that can find/build/derive problem-solving structures to resolve that problem structure, to fulfill the problem-solving intent of 'resolve problematic differences' (a variant of 'connect problem/solution'), as 'different and overlapping functionality' is a useful structure for fulfilling 'difference resolution structures' to fulfill 'requirements of a solution', thereby connecting the 'dichotomy of functionality/requirements' with 'interface structures that can adjacently fulfill its opposite structures (to resolve the difference between functionality & requirements)'
          - this workflow connects interface structures on multiple interaction levels (the problem/solution interface, the structure interface, the difference interface), thereby making it likelier to be useful by default
          - a variant of this workflow would 'find structures that can also connect structures on these interfaces' as inherently useful structures to fulfill problem-solving intents

    - add to ml 

      - if the variables exist in the input data set and the only structure required is additions and exponents/multipliers of those variables, and if the input variables' true interaction functions follow a function that fulfills the assumptions of the model (continuous, etc), and if the function doesnt have to include inferrable but not included information, like cause, implications, requirements, etc
        - http://neuralnetworksanddeeplearning.com/chap4.html

      - error structures
        - the standard ml structure of 'building bigger features from smaller features' doesnt allow more complex variable interactions like:
          - deriving an abstract variable from some of its attribute variables, as some of its attributes may not be simply aggregated but may be conditionally relevant variables in determining that abstract variable
          - identifying interface variables
          - identifying connections between non-adjacent variables or other structures contradicting the similarity structure embedded in the network algorithm/config
        - ml networks tuned to prevent all bias cant use any structures of bias 'hard-coded rules' to fulfill other intents, like approximate solutions or 'temporary interim solutions' while other solutions are being developed, as there are reasons why these hard-coded rules develop in brains, such as over-simplifications, life or death situations requiring fast thinking, and group dynamics leading to biased rules like 'dont trust dissimilar agents' which lead to evolved defaults like these hard-coded rules, and these reasons are less valid in the current problem space where these situations occur infrequently, but the rules are default or otherwise hard-coded and therefore are applied when not useful (such as in any stressful situation rather than in life or death situations)
          - any network that develops a hard-coded rule like a 'false maximum variable value limit' that emerges bc of a network configuration is applying the same learning process in any case where such a rule can 'develop for no reason' or where the network can 'apply the rule for no reason', so these networks are by default some degree of incorrect bc of that structure that develops
        - bias toward 'constants' (using variable values' co-occurrence as a constant, leading to bias errors based on those constants)
        - bias toward 'irrelevant variables distantly connected to relevant variables', where relevaant variables should be targeted for acting like a 'interface structure' ('interface variables' on which other variable changes are based) and 'distance from interface structures' ('variables based on interface variables')
          - bias toward excluding 'constants' or 'low-variation variables' which are 'change-invariant variables' (which may in fact change but act like an interface in the original data set and are therefore useful to include rather than over-prioritizing just the variables that are high-variation)

      - add to ml definition of a 'solution-finding method' as it finds the solution 'neural network configuration' (prediction function) by a 'sequence of weight changes'

    - examine 'locally same, globally different' as a structure of 'interchangeable alternates' on 'interaction levels'
    
    - standardize primary functions in terms of common functions
      - 'derive' mapping to 'find cause', 'build' mapping to 'find combination', apply mapping to 'find changes', and 'find' mapping to 'find filters'

    - example of possible usefulness of a structure
      - an example structure like '1/2' can be useful for the following structural intents
        - equivalence of two values (two halves), equivalence being a structure of randomness, which when applied to 'interactivity of values' can produce other structures of randomness like 'complex systems with many variables/structures having high interactivity'
        - a default structure of a 'comparison' and 'a randomizing division (1/2 slope line) of an interaction space (like the area created by multiplying x & y)' between a default unit structure '1' and a default unit interactivity structure '2' when applied to dimensions (as an exponent)

    - add to useful structures
      - core math structures (point, line, path, network, factors) and their counterparts when applied to other interfaces (object, connection, function, system, components) as a way of finding important (as in 'core') structures in another interface

    - write example of code as a set of parallel sequences on different abstraction/interaction/intent levels and in different formats (app/function, task-oriented/core, requirement/implementation, change type/variable map, uniqueness/usefulness, network/tree/sequence)

    - add to why a language network is insufficient
      - doesnt describe new concepts not already adjacently embedded in the language
      - doesnt have a reason for not standardizing terms to a common core set or a set of generative functions, as opposed to including all or most terms in the language
      - doesnt have a reason for positions other than similarity on one metric, which is inadequate to determine absolute position across all differentiating metrics
      - doesnt reflect multiple difference types like usage, meaning state changes, evolution of terms, which require different structures to represent
      - alternate formats could include: sub-networks to depict common usage queries of a term & state changes of the usage queries as well as alternate meanings of the term in those alternate usage contexts & synonyms of the term

    - add to why there will always be new problem-solving methods 
      - problems/solutions will always change, as will related structures like structures to generate them and structured derived from them
        - related structures like optimization methods, requirements, solution metrics, etc will always change
        - solutions like available tech like storage/computation/derivation tech will always change which solutions are more optimal for a given problem
        - there will always be new alternate methods to derive something, given that everything is connected, and derivation methods will always change and be optimized in new ways
        - there will always be problems, and problems are suboptimal differences, so solutions will be new ways of resolving new differences or resolving known differences in new ways

    - alternate definitions of neutral structures and errors as 'sub-optimal solution states' to identify what moves to make (in what directions, to what degrees) to find solution states (optimal states) given patterns between sub-optimal and solution states
      - apply patterns of sub-optimality to reduce errors (solutions are where states are 'sub-optimal error states', as in 'not an optimal error but a suboptimal error, as in a solution')

    - add to useful cross-interface structure mappings: 'possible usages' as a 'tangent bundle' or 'convolution' structure

    - example of using info as the standard interface to use as a base
      - a 'reason' is the 'information' about a reward/cost that can be used to determine if a change has the possibility of being optimal for some intent ('finding the reward/avoiding the cost'), meaning the reason is the cause of the change

  - add to differentiation from other inventions
    - theorem-provers are written by a person who is mis/interpreting logic in their own way, as a 'control flow' language between 'formulas' as data, hard-coding assumptions like the specific, hard-coded/constant solution automation workflow & problem/solution structures being used ('break a problem into sub-problems', 'dependency chains', 'sequence of current formula state compared to target formula state', 'useful structures like combination/substitution functions', a hard-coded set of 'goals' when solving each problem, such as 'find counter examples' and 'find probable variable values to test' and 'check if current state and target state are equal'), rather than a way to automatically solve all problems, including problems other than 'checking if a math formula has any counterexamples', or a way to automatically derive & apply logical rules like 'check for a counter-example') which are hard-coded manually in these tools as assumptions of the developer
      - these tools basically map standard programming control structures like 'if/then' to high-level math theorem interaction structures like 'assumptions/conditions', as proxies of sequential flow signals, in an inaccurate/approximate structure other than a network/tree (allowing for mis-interpretation and mis-mapping of the actual connections between these corresponding structures), an 'assumption' being treated as an 'input dependency' structure without actually equalling an input dependency in its definition, leaving out other 'input dependency' structures like randomness, system context (implications of the assumption and their impact on input variables), dependency alternatives/requirements, etc, which should be taken as implicit inputs to every function
      - these tools are over-simplified for limited gains in utility value, but are probably useful for solving highly formatted specific problem types like 'checking formulas for equivalence'
      - also of note: a program that 'checks every possible formula' is also a 'solution automation tool', but hardly anyone would find that useful, and almost anyone could write that, even if it is pleasant to imagine that as being a useful alternative to my inventions
      - https://github.com/jrh13/hol-light/blob/master/tactics.ml

  - add response to critique of 'being runnable in human brain' 
    - differentiate it from human brain structures & from automatic/default problem-solving as well as which people can execute the processes in the invention & at what scale/speed/accuracy which differ from the way the invention works and whether other people identified any variables and whether they can apply them in the same way as an interface query which I havent given a full example of yet, rather filtering the query for the structures that are probable or are found to be useful

  - add to why my invention isnt a language or a language network
    - its a way to generate/find/apply/derive languages to interpret systems
    - its purpose is not to accurately represent 'every possible variant of a concept' (synonyms) or 'every possible connection' (verbs) but to 'reduce problem-solving structures, like requirements to fulfill structures of usefulness like "common intents"'

  - add to problem/solution structures
    - definitions & other truth structures as symmetries

  - add to implementation methods
    - to automatically find a structure like 'intent' in a set of functions:
      - test usefulness of functions for known intents
        - workflow: apply the original problem output ('intents') as input, 'testing' as a solution filter to find functions for each intent, reversing the problem direction
      - index functions known to be useful for common/core intents and index other functions by similarity to these
        - workflow: identify common/core problems to solve or common/core solutions to solve for, and apply an assumption of 'solution similarity' of other solutions to generate other solutions once those common/core solutions are solved for
      - index function intents as the function requirements (resources like computation) and function outputs (use requirements, use requirements to generate outputs, etc)
        - workflow: apply related/similar structures instead of the original structure

  - add to predictions
    - where you say that 'activity interacting with a neuron is relevant in its functionality': https://www.quantamagazine.org/how-computationally-complex-is-a-single-neuron-20210902/
    - relevance of intelligence (successful learning/thinking) & disease (unsuccessful learning/thinking/stressor-handling): https://www.quantamagazine.org/brain-cells-break-their-dna-to-learn-more-quickly-20210830/

  - add to science

    - gravity batteries
      - stations that start at tops of hills to initialize routes with energy already stored, and uphill trajectories back to these stations which are similar in shape to a 'diagonal wave function' to power each section of uphill ascent with the energy from the downhill dip
      - particles drawn from air which, when electricity or other reaction is applied, form heavier particles which create energy when falling
      - magnets that repel an object to a height which is useful for energy generation when falling
      - structures that, when dropped from a high point like the iss, cliffs, sides of buildings, or mountains (carrying the object horizontally with wires to a position where it can fall in a useful way) where theyre constructed, dissolve by the time they reach the ground to avoid crashes, or when 'automatically moved' in another way, like with ocean tides directed by fields/rotation of the earth (digging holes in the ocean floor where the tides would direct water in a predictable way that could be harvested)
      - pressure-based propulsion (a device that returns to its 'loaded or ready to spring' state when gravity/time/magnets are applied) to eject objects to heights that will generate enough energy when falling

    - finding space time trajectories that use as much information as possible to avoid 'reversal of direction' errors

    - bc it uses hormones, diverts resources to building the embryo, and creates stem cells that can become any other cell
      - of those that are common, they are associated with reproductive organs and related systems
      - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6396773/

    - solutions to water supply problem
      - multiple solutions including solutions like reverse osmosis and solution formats like using reverse osmosis to create a reusable water supply that is more filterable, requiring one use per user with access to the tool in urban centers
      - using processes like applying heat to evaporate water to create clouds/condensation which can be separated from minerals
      - solving a different problem - optimizing production of new or known elements/organisms that bind with chloride more strongly than sodium, convert sodium chloride molecules into more filterable molecules, replace the sodium in a bonded molecule of sodium chloride, or break the bond of the bonded molecule

    - calorie expenditure as a budgeting structure, where high energy usage optimizes usage (like routing energy to essential processes like creating new stem/immune cells, replacing damaged cells), so low energy usage (low exercise) doesnt develop these optimizations to energy usage, so theres more energy available to nonessential processes (like creating cells randomly or in other suboptimal ways) bc of the lack of routing or other optimization functions

    - quantum structures are structures that can be combined or otherwise interact (such as by neutralizing each other, or reducing change types, or interacting to form a stable structure) to form stability structures (measurable across adjacent time sequences) on which further changes can be based (creating further time structures/expanding time by increasing the number of possibilities), as quantum structures are 'information-adjacent' in that they are relatively unstable and difficult to measure but adjacently form information (measurable, stable structures)
      - 'connectible stable time sequences' and 'local structures' and 'change structures' and ('overlapping time sequences' or 'coordinating/interactive time sequences' or 'non-contradicting/non-interactive time sequences') as structures that form space-time, definable as 'structures that support certain change types which are executable within a given time, and if surrounded by other structures'

    - 'ratio of evolution' - rather than trying to address each bio-system vuln as it is discovered to be used by a particular pathogen or otherwise beneficial to a particular condition, the ratio of the bio-system's evolution to pathogens' evolution and conditions' benefit should be optimized so that the bio-system can evolve faster than pathogens
      - this means limiting the mechanism of evolution/learning of pathogens (modifying themselves in ways that kill hosts) at various points of attack
        - the success of a pathogen in killing any particular host can be prevented so the pathogen doesnt learn how to kill hosts
          - this would require advanced technologies like creating blood to replace existing blood, customized treatments per patient, etc
        - the pathogen can alternatively be tricked into deciding its been successful at killing the host, as a way of preventing it from learning how to kill hosts (produce false success signals for its intent that dont harm the host)
        - the pathogen can alternatively be tricked into deciding a different strategy which doesnt harm the host is more successful at its intents (obtaining energy, growing, etc)
        - the pathogen can alternatively be limited in its learning functionality by removing genes in pathogens that allow this learning, such as limiting pathogens to learn self-destruct functions, or limiting pathogens to only learn by testing rather than by using/borrowing other DNA external to itself, so its learning rate must necessarily be slower than the host cells' learning

    - 'sets of n particles' that act in aligned ways like:
      - 'components of a temporary local field/lattice having n points'
      - 'a connection created from equivalence structures' based on:
        - a 'homotopy equivalence between points'
        - a 'set of rotations/arrangements transforming points into a set of equivalent duplicates by definition-compliance, like the infinite cloning paradox solution'
    - 'ozone hole' was a specific structure that gave people one achievable goal to aim for so they could organize resources around achieving that goal
    - what ratio of gene copies is required to offset cancerous processes/responses without interfering with non-cancerous process
      - can junk dna be altered to fix sub-optimal ratios?
      - are some dna copies acting as 'placeholders' that prevent other copies from being made or prevent certain illnesses
      - how to 'create dependency' of cancer cells on a particular process/structure so when removed it can create cell death
      - drugs that response to 'high quantities of a signal' leave room for an error of 'missing low quantities' - an error where the signal is assigned a boolean and should be a continuous spectrum with a threshold that can re-create the original problem
      - how to target useful cells with growth-enhancing mechanisms like extra-chromosomal dna
      - are there genetic sequences that can offer protection against extra-chromosomal dna interactions temporarily while cancer cells are being targeted (like if a protective sequence is spaced at intervals that disincentivize interactions with extra-chromosomal dna, such as when extra-chromosomal dna merges with a chromosome)
      - how to account for probability of a known possible error (like mutations compared against a mutated rather than healthy reference genome) and adjust data accordingly
      - if a structure continues to develop, that means it represents a 'stability point' that is incentivized - which incentives of extra-chromosomal can be altered without harming other systems?
      - if a package of growth/maximizing genes continues to develop bc of 'survival bias' (the genes that are useful for survival are protected & maintained & replicated), can incentives for dna repair/defense genes be created/increased to compete with these cancerous extra-chromosomal gene structures - why is the 'survival bias' of cancerous cells & cancerous mutations stronger than other cells/genes/mutations' bias - just bc theyre more useful for cancerous intents like 'unrestricted growth' or bc of some other cause like 'genetic adjacence' or 'genetic functionality like jumping/coordination', or bc of the fact that growth factors promote themselves by the definition of their own functionality, or bc these gene/mutation types like enhancement/growth have aligning intents like 'promotion' so theyre often found together?
      - can antibodies against high-growth/maximizing extra-chromosomal gene structures be developed or can vaccines be developed to produce antibodies against them?
        - https://cen.acs.org/pharmaceuticals/oncology/curious-DNA-circles-make-treating/98/i40
    - apply 'solve problem cause' by 'changing' the 'input-output sequence' of that 'problem cause' at a particular position
      - https://medicalxpress.com/news/2022-02-cancer-cells-tunnels.html
      - other workflows would be:
        - minimize problem
          - close most cells' tunnels to let a smaller number of pain signals reach immune system, while making pain signals more powerful 
        - apply solution as a prevention measure without using 'missing variables' (cell identity/health) that cant be identified, applying the solution at a 'smaller scale' (to increase chances of hitting a cancer cell above zero, while not causing extreme harm when hitting a healthy cell, and not applied to many cells)
          - apply structures to function applications where input identity cant be verified so it may as well be disregarded ('randomly' apply) the solution output (reducing motility) to correct the problematic output (the deadly effect of motility)
        - use problem to solve another problem
          - apply 'mobility mutations' and 'destruction signals' to move cancer cells to specific locations where theyre less likely to survive
        - solve problem cause at another position in the causal input-output sequence
          - change sequences of genes to make cancer that searches a genome less likely to find useful genetic mutations immediately, wherever genes with carcinogenic mutations are switchable

    - a generally useful 'quantum algorithm' is optimally implemented as a network where any node is equally accessible regardless of steps separating them or other definition of 'distance' (each node pair can be entangled on demand to get calculations/output from that node to reduce overall repeated function calls of the network)

  - add to automation tech
    - more devs converting to 'task description writers' creating standardized task descriptions & identifying repeated work, automation opportunities & other high-level semantic tasks is a better career once programming turns into a 'task bidding freelance market' where AI coders frequently outcompete devs
    - organized & structured code by 'core component' (function input/output variables, input/output differences, patterns, core function-calling function (high-level functions, so building core functions is the source of changes in functionality of high-level functions), uses, logical derivation connections (equivalence, requirements, dependencies) as logic trees, logically directed graphs, changes/states, function, reference, relative position of thread/process/function, limits/requirements, dependencies, data, prior operations (cache), differentiating attributes, context, mix/iteration/cycles/sequence, type, example, value)
    - 'infinite compression' as a combination of infinite sequences & numerical references to those sequences as parameters (similar to rainbow tables), where infinite random sequence subsets can be recursively compressed into hashes, where infinite sequences can be represented as a sum of sequences (infinite random adjacent base sequence + an infinite geometric sequence to generate it), or a sequence & a set of vectors indicating changes to generate the sequence from an infinite random adjacent base sequence

  - additional attributes (metadata) of interfaces include attributes like validity & relevance, which vary absolutely across other interfaces, and contextually across interface queries/solution automation workflows & problems being solved with them

  - additional examples of error causes
      - example of priorities leading to errors:
        - the 'selfish' perspective prioritizes simple, structural objects (like those within an adjacent range), which is why 'selfish' programs see & act on those things rather than producing solutions that can be used for many problems, even ones they dont have
      - example of how a 'lack of errors' in a closed system can produce an error in system-level interactions, like a cross-system merge

  - add to stats
    - give example of visualizing the stats object connections in a stat insight rule
      - add example of how to derive the rule 'choose algorithm with low bias/high variance for large data sets'
        - use 'random forest' bagging method to reduce high variance
        - high variance is a problem when a sample data set is not representative of the population, producing accuracy for the training data subsets & errors for all other data subsets
          - if a data set is relatively large in relation to the population, the 'sample data set is not representative of the population' is less likely to be a relevant problem
          - 'variance can only be reduced by averaging over multiple observations, which inherently means using information from a larger region'
        - high bias is a problem when sample data sets differ from the population mean by a lot, producing errors for most samples unless they happen to be represented by the general model
          - if a data set is relatively large in relation to the population, the 'sample data sets differ from the population mean' is less likely to be a relevant problem, so the large data set can be used as a basis for the general model
          - 'bias is reduced using local information'
          - 'If training set is small, high bias / low variance models (e.g. Naive Bayes) tend to perform better because they are less likely to be overfit.'
          - 'If training set is large, low bias / high variance models (e.g. Logistic Regression) tend to perform better because they can reflect more complex relationships.'

  - give example of 'different meaning interaction levels'

  - give examples of why other tech solutions are insufficient
    - why ml would be inaccurate on math problems (like 'predict convergence value for an infinite series, based on training data of infinite series param input & convergence value output')
      - regularization, bias/noise changes, and other feature changes for intents like 'generalization' or 'feature selection' may add to inaccuracies
      - the structures that can be composed by various function/node/weight/threshold unit combinations may not be capable of the math operations involved in transforming one value/format into another
      - the math problem has emergent structures that are not visible in the training data set, for example a point where the relationship function between the inputs/outputs changes (like an asymptote or a maximum)
      - the training data may reflect patterns that are simpler for the network to compose/filter/reduce, without enough data points representing more complexity (a parabola instead of a hyperbolic function)
        - the network may be good at providing filter/compose/reduce functionality, but not other functionality like 'converge' or 'select between similar alternatives'
        - the support for 'multiple alternative input-output routes' in networks may add too much complexity in reducing its ability to specify a particular answer, with built-in tools to find averages or other representative values rather than selecting one value over another
        - for some functions, there are too many inputs that could produce the labeled output, and those inputs may be too similar to differentiate/specify (very similar functions can produce the same area)
      - neural networks are primarily good at certain data transformations, like where the problem input can be converted into the solution output by a system of linear equations or matrix operations supported by the network, where coefficients/weights of versions of function components (inputs & interim weighted combinations of inputs) are the required output of the network, and those operations may not adjacently handle operations like summing an infinite series, which might require specialized structures like memory/state-embedding or online learning if those structures cant be produced by adding additional layers to a network
      - each feature of the infinite series input would contribute to the output, but a neural network is designed to learn weights of features, implying that some are less important, and the contribution of terms in a series to the convergence value can usually be determined by its size/position
      - if parameters of the series are used instead of the series, that is a low feature space compared to the input features available in other applications of neural networks
      - the operations in some formulas do not produce reliable learnable patterns (some structures of randomness would counteract the ability of the network to learn a function)
      - the inputs dont provide enough data for continuously differentiable spaces where methods like gradient descent can be applied, given that math functions often cover a wide range of possible inputs, and a training data set is unlikely to be representative of enough examples to fill in the gaps in this space

    - VAR & reservoir computing's random sample of matrixes is inadequate bc the randomness is an attempt to identify 'very different' difference types, without generalizing that into a unique set of differences that are likely to be useful (like differences distributed across pairs of variables, so many random samples dont represent difference types in the same pair)
    - regression is insufficient even if its a good temporary solution if you dont have other resources than the data set, bc its conclusion/outputs (in the form of the regression function) can have the opposite meaning 'random noise' as the intended meaning 'causal relationships'
    - statistics in general is built on the insight that 'probability is associated with certainty/truth', but it ignores other certainty structures like structures that are more useful than patterns/probability, such as definitions, concepts, meaning/understanding/integration/organization, cause, inevitability/requirement, determination/generation/description structures, functions of varying interaction levels, etc
    - machine-learning can have the opposite functionality given extreme data values or update functionality manipulation (to train it to give the wrong answer in its online learning functionality), as well as other exploits from interactions of the algorithm, network, parameters, emergent structures, & data, and it is not built on understanding
      - 'one-degree connection structures' which are present in 'foundation models' are incapable of capturing multi-degree connection structures like sequences/chains or structures of connections like trees/networks/groups, even though other structures can be formatted as core structures like 'connections', it doesnt mean a one-degree connection network will capture them, or that a network of connections is the most optimal structure for that info given its usage
      - machine-learning based on neuroscience leaves out other brain interfaces like psychology, chemistry, & language
        - a psychologist might interpret a thought as 'an emotional reaction to a chemical stimulus that retrieved a memory'
        - a neuroscientist might interpret a thought as 'a response to electrical stimulus given weighted connections between neurons that previously handled that stimulus'
        - a linguist might interpret a thought as 'a deviation from a previous phrase that captured an experience to handle a change to that experience'
        - a chemist might interpret a thought as 'a result of scaled electron dynamics in response to a chemical'
        - a biologist might interpret a thought as 'a useful way to produce serotonin to offset a signal from the gut'
      - https://thegradient.pub/has-ai-found-a-new-foundation/

  - add to problem/solution structures

    - add example of how to derive 'apply differences to inputs to see if they can change the output to see if the solution is true'

    - add example of impact of methods on various network types given the differences in method/network structures & include assumptions

    - give example of how structures could have been derived (symmetry, isomorphism/interchangeability as common important objects to derive an interface, alternative interfaces to solve a problem) from another direction

    - derive logic types that would be necessary to complete the logic interface & give examples of logical object interactions

    - a 'find a solution' function should be able to be converted into a 'generate a solution' function & other functions like 'test a solution' & 'apply a solution', bc as the brain learns, it can generate solutions on demand once understanding develops

    - give example of identifying meaning of emergent structures (like 'weight trees' in neural networks)

    - organize workflows using useful structures as being on the meaning interface, where useful structures from other interfaces overlap & connect with the meaning interface

    - write interface queries to generate each workflow

    - give examples of how each workflow can be applied to various standard problems (find a prediction function, sorting problem, ml configuration/algorithm-design problem), which can be used as a data source to derive implementation logic/interface queries to generate solutions

    - finish math mapping so you can find other useful/solution structures (interaction space as convolution, core functions as basis vectors, etc)

    - basic solution automation workflows
      - trial & error
        - use a rules/solution database & look up the answer (try known solutions)
        - apply machine-learning with various configurations (try known/probable configurations or variable interactions like combinations of a solution-findind method like neural networks)
        - apply rules from other systems to see if they work in another system (try other known solutions from other known systems)
        - mix & match solution components/variants (try known solutions)
      - reverse engineering
      - break problem into sub-problems & merge sub-solutions

    - example of format/intent matching
      - formatting a 'tree' as a 'set of overlapping sequences with overlaps in either inputs or outputs' so functions can be formatted for different intents like in 'parallel processing'

    - add to input structures
      - input variable/trigger/requirement/component

    - add to output structures
      - limits on what a structure can be used to create
      - similarities/differences to inputs (inputs change can be preserved in outputs)

    - identify new interactions/structures
      - trying structures of structures that havent been tried yet (like how new words evolve as a 'combination' of other words to describe new experiences that are similar to both combined words)

    - 'testing/simulation' involves querying for related rules (like how 'gravity' rules are related to 'motion' rules so any change involving motion should have a 'gravity rule check' applied as a filter) & checking if they apply to relevant components (like how specific components are involved in 'motion', like 'energy', 'motion restrictions', 'motion functions', 'motion triggers/inputs/components')
      - this is an important process for checking if a structure is valid/consistent in a system, which is a useful function
      - this is different from basic testing, which is where a function is applied and the output is checked against an expected value, bc it involves testing for validity/consistency in a system context where the change is being applied

    - examine interaction space of tech stack layers (ml models, algorithms, data, apps, bugs, os, chips) as a source of new errors
      - example: 
        - ai applied to design chips
        - chips with data erasure bugs that exacerbate os data erasure bugs
        - chip designs that produce error types for various ai models/algorithms/parameters
        - how 'gpus are known to be better at building ai models'

  - add to error-finding methods
    - identifying & generating known useful structures like 'symmetries', 'variables', 'subsets', 'interchangeable alternatives', 'maximally different inputs' & 'bases' & 'type/phase shift thresholds'
      - identifying & generating combination structures of useful structures like 'maximally different values around bases'
    - identifying gaps in known useful structures explaining data points (where data points arent explained by those known structures) & generating inputs in those gaps other than those data points

  - add to conceptual math
    - example of a conceptual math operation that builds a boundary structure leaving an inevitability of a matching concept (numbers) filling the structure
      - the concepts of 'missing', 'multiple/more', 'unit', 'type', 'identifiable as similar/equal/different' and 'difference in amount' allow for/require/build the concept of 'numbers'
      - also functions like 'compare' or 'reduce' or 'expand' require the concept of 'numbers' when comparing objects of that data type or objects having a quantifiable attribute

  - add to causation variables
    - ability to change (if a variable cant be changed, it is less causative for problem-solving intents)

  - add to info problems
    - this manipulates:
      - audience objects:
        - ego
        - assumptions (about patterns, what you would notice/figure out)
        - attention
        - feelings 'opposite' to logic (safety, confusion)
      - using objects like distractions, activations, distortions, core structures like combinations/sequences, complexity, patterns, input/output similarities/alternatives (complex/simple implementations), logic, patterns of logic, logic avoidance, jokes
      - to produce:
        - errors in expectations (in order for the audience to expect y, they have to have assumption x, as x is an input to y)
      - these important variables can be identified by identifying the inputs to these objects
        - what 'input' is 'required' for this expectation error to happen? (an assumption)
      - https://www.smithsonianmag.com/arts-culture/teller-reveals-his-secrets-100744801/?all&no-ist

  - when is it optimal to store a mixed structure of varying specificity (like a type, intent, cause & a specific example)
      - when there are potential uncertainties to resolve, like whether the example represents a new error, type, or variable, bc the example doesnt fit known structures

  - all primary interfaces can act like the problem-solving interface (start solving problem from the concept or structure interface and integrate all info back into that interface & frame the solution in terms of that interface) but the meaning interface (the interface interface) is the most powerful

  - apply concepts to structures

    - concept of attention in structures
      - mixed interim high-variation & high-similarity structures tend to maximize attention
    - examine error type of conflating intent & requirement
    - consciousness as choice to move between neural nodes (rather than being directed) required:
      - the development of alternative node paths performing equal/similar functions, requiring:
        - the development of excess resources, delaying required decision time (making immediate decision unnecessary, avoiding a forced decision), requiring:
          - the existence & application of previous efficiencies & functions for alternative evaluation, energy storage, storage-checking, & energy requirement-identifying
      - the cause could be framed as structures such as an 'efficiency stack' or 'energy maintenance functions' or 'alternative options' or 'navigation/motion control' or 'lack of requirement/need'
    - examine similarity (alignment/overlap) structures between: 
      - extremely different components (when an error type is an incentive or a function used for other intents) 
        - when the solution format of some problem has similarities to the error type, like when you need randomness so errors generating randomness are a possible function to use for that intent
        - contradictory/opposite components (have some metric in common, with opposite values)
    - examine the distortion vector paths that adjacently decompose a data set into a prediction function from a base point/function set

  - add examples of:
    - mapping to structures & identifying contradictions its safe to ignore for applying a structure
    - system/object/rule/type change patterns
    - query examples for use cases like:
      - lack of information stored (match problem of type 'information lack' with interface query 'check pattern interface for similar patterns')
      - query problem breakdown & integration diagram
      - calculating various different problem breakdown strategies first before executing normal query-building logic for each
    - example of how to predict most interactive/causal concepts in a system


## diagram
  
  - diagrams:
    - error types
    - network of formats
    - efficiencies
    - alternate interfaces (information = combination of structure, potential, change or structure, cause or structure, system)
    - chart type: overlaying multiple 2-dimension variable comparisons to identify common shapes of variable connections (density of points added with a visible attribute like more opacity)
    - structures of emergence
      - example: 1-1 input/output relationship up an interaction layer, where extra resources that dont dissolve immediately on the higher interaction layer aggregate & form core structures like combinations, where interactions between combinations & sequences have different dynamics than the individual output interacting with other individual outputs
    - how emergent functionality/attributes come from interaction structures (sequences & layers)
    - intent-matching
    - interface overflow (to sub-interfaces), interface foundation
    - workflow
      - function to identify relevance filter ('functions', 'required') from a problem_step ('find incentives') for a problem definition, to modify problem_steps with extra functions/attributes ('change_position') to be more specific to the problem definition ('find_incentives_to_change_position') for problem_steps involving 'incentives', so you know to use the function_name to modify the problem step if it's between the type 'functions' and the object searched for 'incentives'
    - conceptual math interface query
      - use lattice multiplication as standard example, other than core operations (add/multiply mapped to language, concepts like irreversibility/asymmetry mapped to math)
    - interface conversion, matching, starting point selection (applying structure, checking if relevant information is found)
    - sub-functions of core functions with distortions (identify/filter of find)
    - dimension links higher than 3d that are depictable in the same network space
      - should show variables that impact other variables, the change rates of these relationships
      - overall impact should be calculatable from these relationships
      - should show similar movements for correlated variables
      - should show skippable/derivable variables (variables that can be resolved later than they normally are)
      - should show meta forces for overall trends in change rules (direction of combined variable forces)
      - should show limits of measurability & threshold metrics
    - specific concepts, core functions, concept operations (combine, collide, connect, merge, apply), ethical shapes
        - variable accretion patterns (how an object becomes influenced by a new variable, complex system interaction patterns, etc)
        - potential matrix to display the concept
          - map parameter sets to potential matrix shapes 
        - cause (shapes & ambiguity), concept (evolution of concepts, networks, distortion functions)
        - argument
      - system layer diagram for each interface to allow specification of core interfaces & other interface layers (interface interface)
        - system layer diagram for structures to include layers of structures 
          (beyond core structures like curves, to include n-degree structures like a wave, as well as semantic output structures like a key, crossing the layer that generates info structures like an insight, a probability, etc)
    - map variable structures to prediction potential for problem types, given ratio of equivalent alternate signals
    - vertex variable structures
      - quantum physics, prediction/derivation tools, build automation tools, testing tools, learning/adaptation tools, system rules, computation power are all vertex variables of information, since they can generate/derive/find information
        - which structure (sequence, network, set, or cycle) of vertex variables is most efficient
    - core component attributes: identify any missing attributes/functions that cant be reduced further
    - absolute reference connections with metadata structures like networks/paths


# content/config

    - import insight history data to identify insight paths (info insight paths like 'lie => joke => distortion => insight', system insight paths like 'three core functions + combine function with this definition + n distortions to nearest hub')
    - define default & core objects necessary for system to function (out of the box, rather than minimal config necessary to derive other system components & assemble)
      - add default functions to solve common problem types
      - alternate utility function implementations have variation potential in the exact operations used to achieve the function intents, but there are requirements in which definitions these functions use because they are inherent to the system. For example, the embodiment may use a specific definition of an attribute (standardized to a set of filters) in order to build the attribute-identification function using a set of filters - but the general attribute definition is still partially identifyd in its initial version by requirements specified in the documentation, such as a set of core attribute types (input, output, function parameter, abstract, descriptive, identifying, differentiating, variable, constant), the definition of a function, and the definition of conversion functions between standard formats.
    - systematize definitions of info objects
      - include analysis that produces relationships of core objects like opposites to their relevant forms (anti-symmetry) in addition to permuted object states (asymmetry), such as an anti-strategy, anti-information, anti-pattern
      - organize certainty (info) vs. uncertainty objects (potential, risk, probability)
      - make doc to store insight paths, counterintuitive functions, hidden costs, counterexamples, phase shift triggers
      - add technicality, synchronization, bias, counterintuition, & certainty objects leading to inevitable collisions
        - error of the collision of compounding forces producing a phase shift
        - lack of attention in one driver and false panic in a second driver leading to a car crash given the bases where their processes originate
      - define alignment on interfaces (compounding, coordinating, parallel, similar, etc)
      - add core info objects (core strategies, core assumptions) so you can make a network of graphs for a system
    - add function logic for:
      - concept analysis:
        - how new concepts (gaps in network rules) evolve once structure is applied to prior concepts 
      - interface analysis:
        - limitations of interfaces & how to derive them
        - how rules develop on stability & how foundations are connected & destroyed
        - explainability as a space limited by derivable attributes from data set & cross-system similarity
        - vertex definition & give examples (as an intersection/combination of interface variables, such as determining/description(compressing)/generative/causative/derivation variables), around which change develops
      - change analysis:
        - generated object change types
          - constant to variable
          - variable to removal of assumption in variable type/data type
    - examine implementing your solution type (constructing structures (made of boundary/filter/resource sets) to produce substances like antibodies, using bio system stressors)
    - resolve & merge definitions into docs/tasks/implementation/constants/definitions.json
    - update links
