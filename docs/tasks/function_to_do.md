# to do

  - finish processes:
      
      - finish applying systematization of solution automation
      
      - finish interface analysis of physics & other interfaces to identify other useful components like efficiencies, incentives, trade-offs, closed systems
      
      - finish config
        - add useful structures & questions from index.md to systematize_solution_automation.md
        - for each workflow involving useful structures, indicate an actual interface query example connecting the workflow with the example
        - useful structures
            - identify filters for useful structures like definition routes
            - the system structure format where the maximum number of interface queries can be executed structurally, with minimal conversions required? is it a merged format of variable/function/concept/cause network graphs, or system state networks, or a set of variable subset graphs, or differences visualized as vectors, or input-output sequence visualizations, or a network with all identifiable interface components visualized
            - interface queries optimizing finding useful interface component filters
            - useful perspectives/specific interfaces
              - useful to think of prediction functions as generative functions to select the variable interactions that are most likely
            - useful solution filters to apply in functions
            - aligning/balancing structures, to solve problems like 'a balance position of structures producing errors when unbalanced'
            - questions formatted as a disconnection between components like causal positions, paths, directions
            - subset indexes of an interface useful for solving most problems (structure indexed by metadata like problems solvable, fitting systems, interactive structures, supported intents)
            - ml structures with supported intents & solution success causes
            - most valuable interface queries & workflows
              - find the sets of differences/dependencies/formats/errors & other useful structures that are the most valuable in a particular structure like a sequence to solve a problem
                    - interface component definition routes
            - useful component/sub-structures of interface queries (interface components, interaction rules, cross-interface interactions, generative functions)
            - useful interface components (like abstract) of useful interface components
              - core interaction functions of core interaction functions
          - creating useful structures
            - organize automating useful structures like combinations of concepts such as "format sequence", "solution automation workflow", "insight path", "reverse-engineer solution from problem requirements or opposite structures", "connect problem & solution"
            - convert structural queries to insight paths
              - alignments present in security innovations (like alignment in inputs like keys)
              - source of rule development as structures of conflict between forced interactions like change causes & constant structures like limits
                - incomplete inevitability of interaction as a decision structure
              - group device history authentication: authenticate credit card by proximity to cell phone & continuity applied to user usage history pattern
            - functionalize insight paths & integrate functions in optimized program with parameters to select function subset & structure for input problem
        - default config
          - write some default interface queries to use until logic is written
      
      - finish scripts
        - create compilation script to compile code/config into a network graph on every change
          - add support for standardizing equivalent synonyms
            - add conversion to standard vocab


  - integrate logic
      - integrate objects/.md text with interface implementations
      - integrate archive_notes/finder_info/functions
      - organize interface analysis logic definitions
        - organize functions in problem/interface definitions, before organizing functions in implementations/*
      - integrate problem_solving_matching.md
      - integrate find/apply/build/derive logic from system_analysis/ & maps/defs.json
      - separate interface analysis logic into implementation/functions (functions dont need unique info)
      - add functions from workflows & analysis (to do list, questions answered, problems solved, interface definition & functions) as files in functions/ folder
        - organize into primary core functions & list sample parameters (like objects to identify for the identify function)
      - integrate rules from diagrams in patent applications to relevant documents
      - organize function logic (interface query design logic)
        - document default static config objects that are inputs to core objects (like functions & concepts)
          - core functions like 'change', with locked objects which should be generated as inputs to other functions and should not be removed bc they enable other rules & core objects
            - a 'check for errors' function
            - a concept of 'self-correction/optimization'
          - these locked objects can be used to generate rule-generating/deriving/finding structures, by forming an initial structure of locked objects and filling that structure with conditional & changeable structures
            - these rule-generating/deriving/finding structures can be used as solution automation workflows
        - design an optimal sorting structure for general interface queries to apply to problems manually
        - list interface selection (based on inputs like available APIs/data sets/definitions)
        - problem interface structures: solution constraints/metrics, problem space variables, available functions, useful formats/structures
        - function to translate interface query logic into interface language (combination of core functions (find/build) & other core components)
        - function-usage-intent::output or demand::supply combination/merging/building/matching functions (alternatively formatted as a solution-finding query for a problem or lack-resource matching function) as an alternative solution to ads
        - decision points (required/optional resolution of variables to constants, as in selecting a variable value)
          - identify when a method & data set can be identifyd to be capable of deriving the answer to a prediction function problem
        - alternative intent coordination & compatability of metrics
          - calculating interactivity by coordinating/adjacent/convertible structures
        - check reduced language components for any other useful functions (what terms cant be adjacently, clearly & accurately framed in terms youve defined) for completeness

  - integrate examples

      - index examples so they can be queried more structurally when implementing functions

      - move examples from:
        
        - drinkme/examples_from_faq.md
          - check other examples of high-value use cases (other than identifying important concepts) from faq:
            - identifying the important base to frame changes on (identifying new interfaces)
            - identifying the right interaction level to focus on (identifying the change-maximizing layer of a system to examine a particular relationship)
            - identifying the right perspective to filter with (like 'identifying whether the legal/scientific/progressive perspective is most useful for an intent')
            - identifying the right context/position for an object (derive context when it's missing or fit an object to a system)
            - identifying the most causative function set (like identifying core functions, or the most misused functionss, or the most change-causing functions)
            - identifying important differentiating types (like function types indexed by intent & structure types, like boundary/change functions)        
        
        - patent implementation_examples
          - identify any examples missing from patents in docs/tasks once examples are organized

        - specific examples from specific_problem_analysis
          - example of permuting assumption: "reports of power consumption have to be exact measurements" 
            - a temperature monitor sensitive to a hundredth of a degree might provide similar but non-specific power reporting for important/extreme usage patterns without revealing such specific information as that which could infer exact operations being done, bc the interval of temperature measurements allows for greater variation in calculations that could explain it
          - example of using set theory in query operations:
            - edges as core organizing/formatting operations (find/apply) & interfaces (connecting/explanatory concepts/functions)
              - https://en.wikipedia.org/wiki/Hypergraph
          - example of structural version of solution difference from original solution: 
              - this is like using a pair of connected lines at different angles to connect two points (multiplying alternate multiplier pairs to create a product), where summing the line lengths produces an equivalence, so different solutions would look like differently angled triangles connecting the two points
                - https://www.popularmechanics.com/science/math/a30152083/solve-quadratic-equations
          - examples of identifying vertex variables
              - general vertex variables: topic, origin/destination, reason/cause/point/intent, errors, variables, types
              - comedy vertex variables: sincerity, stupidity, stakes, tension-resolution/expectation-subverting pattern variation
              - music vertex variables: tone, tension-resolution/expectation-subverting pattern variation, lyrics
              - optimization metric vertex variables: solution metric patterns (what other solutions optimize for, to identify optimization metrics to apply)
          - example of resolving a conflict between structure/limits using a structural similarity between a structure (gradient of function) & its container/limits (gradient of constraints)
              - https://en.wikipedia.org/wiki/Lagrange_multiplier
              - also an example of a solution space (the whole function is the solution space of possible minima/maxima) and a filter applied to it (constraint)


## examples

  - why problems will always have new variables determining how solution automation workflows can be generated
    - problems are a 'lack of available structures that can fix the problematic structures', requiring new structures to be found, built, or derived to solve those problems
    - by definition, problems will always change from previous problems
    - therefore problems are a structural source of new change types
    - new change types map to new variables describing those change types
    - new variables may not be adjacently buildable, derivable, or findable with existing structures, leading to a requirement to find/build/derive new structures to structure those new problem variables
    - this may lead to new values of existing variables like new problem-solving intents or new problem/solution core interaction functions, or other new structures that may be representable as new variables, bc they exist on another interface where problems & solutions interact, such as new variables regarding a new function type that can be used for problem/solution interactions
    - even if a problem is static, there will always be new ways to solve it because of the inherent variation captured in the concept of 'problem', and these solution-finding methods will always have new interfaces or other structures which can be used to frame them
    - there will always be new structures that are adjacent to available tools to fulfill interaction functions (like 'connect') between given new change types generating new interaction functions, and new interaction levels to reduce a system to in terms of finding core components of the system, new structures of errors from new change types, and possibly new primary interfaces created by new interactive change types and new useful structures to connect other new structures

  - example of AI being inefficient
    - ai is used to predict agents (having neural networks), which is a recursion (self-reference) problem

  - core example of interface structures as useful for filtering possibilities
    - for the core problem of specificity/filtering, as in:
      - 'why would general structures (like differences as in change types or similarities as in structural similarities) be useful for filtering a solution space, when the set of possible changes is infinite? that sounds like it increases the solution space unnecessarily'
        - interface structures are adjacent structures of useful structures (structural similarities are possible interface structures like patterns, ambiguities, coincidences, variations of a concept, etc), or are themselves generally useful structures (useful change types, like 'maximally different changes, such as a subset of possible changes including only the most different example change combinations') because they involve combinations/connections between interface structures (a 'structural similarity' is a 'combination' of 'structure' and 'similarity', all of which are core interface structures), and combinations of these structures tend to be important/useful bc they operate on the same interaction level as other important/useful structures
        - for example, applying 'change types' is more useful than applying 'all possible changes (all possible solutions in a solution space)', as 'types' are a 'combination' of interface structures (a combination of 'attributes') and are useful for common important intents like 'efficient descriptions', and are adjacent to other interface structures like 'concepts'
          - given the extreme connections with other interface structures, the 'type' structure ('change type') is more useful than other structures ('all possible changes')
          - this is like 'throwing a ball around to explore a space for you' if you (or a program) are missing a sense to explore a space
          - in other words, how would you derive the concept of 'vision' if you didnt have it? by using available senses like sound and objects like a ball that can create high amounts of differences in audible (measurable) information, to infer 'physical structure of objects', where inferring a 'more direct route (vision) to determining physical structure of objects' is an adjacent version
          - vision provides the more direct/adjacent route to determining physical structures than sound, and can also detect structures with minimal audible information (color), where hearing might be useful for detecting structures with minimal visual structure (sound waves, sound applied to invisible particles to make them audible since theyre not visible)
          - presumably there is often a 'more direct route' (involving fewer steps, or having more variables like color) to determining a structure that some senses cant directly infer or that they dont already have, which can be derived from other senses - interface structures usually provide the most direct route to identifying other structures like complex systems, although there may be a more direct route than interface structures
          - the function of 'finding the direct route between extremely different structures (like a document and an encrypted version, where the direct route is the encryption algorithm, algorithm input, and/or keys)' can be useful for other intents like 'identify useful structures (that encapsulate a lot of information, or equivalently store a lot of variables, etc)'
            - 'direct routes' are likelier to be the intents of a structure like a system and to occur on the primary interaction level of the structure
          - similar to how if a concept like 'visualization' or a 'unit example' or 'ability to apply in a useful way' or 'learning' is given, other concepts like 'understanding' are more determinable, bc these concepts are more directly connectible to understanding than other concepts
          - this is useful for the intent of deriving new interfaces
            - for example, how would you derive the 'math' interface by observing the natural world? 
              - you would observe patterns of common structures, then identify 'similar' structures, identify the concept of 'numbers' by counting similar items, identify common 'numbers', identify common 'number connections', identify 'differences' in 'number connections', identify that different number connections continue to be identified as you interact with them more, and identify that other different number connections than those you already saw are possible
              - at the point where you identify 'number connections' (like ratios, as in a comparison of numbers that are relevant to each other), the rest of the items in the sequence are adjacent
        - similarly, some 'filter sequences' can reduce a solution space to one or few possibilities in most cases, which are more useful than other filters that cant, and can be derived by the ratio of the possibilities they filter out (filter sequences or sets that leave few possibilities in most cases can be derived by applying interface structures like 'common sets' to apply the filters to, or applying 'extremes' which are the items likeliest to be unique and therefore most filterable, etc)
          - this is like 'using a net or stick to explore a space', given that the net or stick can only identify certain things like 'edges' or 'small objects that fit through the net' or 'which the net would get stuck on'
        - some of these structures have suboptimalities in terms of the possible errors created by the change types producible with these structures, which could be system-invalidating change types depending on the system structure, producible change types by the structure applied, and the system state
      - these alternate methods can be generated by applying important structures (changes, limits, filters) as the primary structures to focus on (rather than the specific system structure itself), given that these are so important and fundamental, they are likely to be more useful than other structures, and are also possible to identify/generate

  - how to find other rules to understand systems efficiently
    - other than known rules like 'apply common structures like functions from other systemsm' or 'find core structures in the new system and combine them' or 'apply an adjacent difference and test if any functions respond to it to determine adjacent structures', how else would you be able to understand a new system efficiently 
    - finding system limits (like constants) is useful as opposed to finding adjacent structures, as limits are generally useful and system limits can help determine other structures like possible errors/functions/requirements/components once the limits are known, with methods like 'applying random selections of all known differences as inputs to functions in all possible position/direction changes', as a higher number of 'extremely different' differences is likelier to find the system limits sooner
    - given that systems are capable of different interface structures given different system structures like functions/components/limits and system attributes like complexity, determining these structures/attributes to determine the interface structures probably associated with those structures/attributes is another way
      - for example, a simple system is less likely to contain contradictions and errors, so determining system simplicity is useful for identifying interface structures likely to describe the system, these probable interface structures functioning as useful targets for identifying the complete system structure

  - how to find a test to support ('prove') another logical connection (a connection like 'if there is a useful structure (like an upper limit) on an attribute (like the count) of an item (like primes)')
    
    - https://www.quantamagazine.org/building-the-mathematical-library-of-the-future-20201001/

    - for example how to devise a test of this logical connection (the statement to prove) like:
      - 'find out if the inputs (factors + 1) to the item have that useful structure (an upper limit) on that attribute (their count, as in whether they are infinite)'
      - 'find out if there is always another set of these inputs (factors + 1) in a sequence (whether there is always a greater input set compared to a given set), meaning the "sequence of these input sets" has the useful structure (no upper limit, as in infinite)'

    - why are these tests useful to determine whether the logical connection is true? bc the following are true:
      
      - the operation (multiplication, addition, find higher prime from an adjacent lower prime) applied to the inputs preserves (would not contradict or invalidate) that attribute (no upper limit) in the output
        - meaning applying operations (like multiplying/adding) to the items (infinities, as in the set of inputs, the set of numbers that can be multiplied/added which might be primes) would produce an item of the same type (infinity)
        - this generates a requirement of an 'infinite sequence' needs to be an input to whatever function could identify the next higher (or otherwise adjacent) prime
        - its not known whether the prime inputs to the multiply/add operation are an infinite set, this attribute is constructed by whether the 'sequence of prime factors' is infinite 
          - if the operation holds for each item in the sequence to predict the next item in the sequence, the operation can be concluded to hold for the entire sequence
      
      - these logical connections ('applying operations (like multiplication/addition) that preserve an attribute (like count)', 'requiring input of an infinite sequence (like integers which are a superset of primes)', and 'applying the definition of a sequence by inferring an attribute from operating on adjacent items in a sequence to the whole sequence') can construct the logic of the original 'proof'
        - given that:
          - multiplication/addition doesnt change the input type (integer) in the output type 
          - attributes like count/upper limit on count are preserved in operations like multiplication/addition ('adding infinite sets would produce an infinite set')
          - an operation applicable to (different) adjacent items in a sequence (where an item is derived from the previous item) extends to all adjacent items in the sequence
            - where 'different adjacent items' is a subset of adjacent items that captures all of the differences types possible in adjacent items of that sequence
        - deriving the idea of the proof is the result of applying:
          - 'find filtered solution set of numbers that could produce primes' (integers as the 'reduced set of possible solutions', which can be adjacently combined with type-preserving operations to create primes)
          - 'find a subset of the input (set) that is useful to test'
            - 'find the difference types in the inputs'
            - 'find tests of the difference types applied to inputs'
          - 'if the subset (generated by all difference types applied to inputs) passes the test, the subset attribute (the subset of inputs tested) extends to the set'

      - whats the core reason for the success (solution success cause) of this method?
        - identifying all the difference types of inputs and testing an example of each difference type (trying every possible difference type possible in the inputs), is a way to infer implied truths, like whether a subset attribute extends to a set attribute
        - if there is a pattern in an input set (like a wave), 'testing a subset of the set with the wave' and 'testing different waves' tests all the difference types in the set (difference types generated by differences in components (different components of a wave) and differences in combined components (different waves))
        - once all the difference types are tested, the set attributes are inferrable by the subset tested
        - the difference types include attributes like 'number type (integer, prime, infinity)' and 'number type count' and 'adjacent primes (connectible with minimal addition/multiplication of other primes)'
        - for example, identifying "adjacently multiplied/added primes that differ in an attribute like 'distance from nearest product of 3 or 5'" would identify a different 'pair of prime factors' to use as an item in the subset of 'different adjacent items'

    - is there a possible contradiction (whether in other contexts or the same)
      - is there a number set where the corresponding definition of a prime has a limit?
        - that is not answered by this proof - there is a possible contradiction
      - however the distortions required to create that contradiction are practically incomputable (computing all possible number types allowed by math, then computing corresponding prime versions in that type, then computing whether corresponding infinity versions hold is not currently computable)
      - so the 'proof' is a 'useful proof' in that it proves it in a 'known cohesive system' (a system where these items are defined and the definitions are consistent)

    - what are alternate functions that might exceed the usefulness of a theorem-prover language like coq/lean
      - a function to identify 'useful tests of a proof'
      - a function to identify 'difference types in inputs of a particular operation'
      - a function to identify 'alternate definitions of an item like a concept or function'
      - a function to identify 'different example inputs/outputs to check a function for accuracy or generate alternate functions or generate functions on-demand for a priority'
      - a function to identify 'input/output sequences of mathematical operations' which are: 
        - commonly useful
        - adjacent to other useful structures like 'statements proved by an input/output sequence'
        - have adjacently predictable emerging attributes (like how 'some attributes like commutativity adjacently emerge from associativity by applying adjacent core operations to additivity, as theyre on the same interaction level generated by those core components') to identify operations, operation sequences and input/output sequences that can produce these emergent different attributes
          - similar to how identifying that 'rotating a line with one fixed endpoint' would produce a 'circle' is a matter of identifying that a 'central point of the circle (where every radius has a point in common and its the center)' is common to all the lines produced by that rotation, as the 'fixedness' creates a 'common point' in all the outputs of the rotation, and the 'central point of the circle' overlaps with the 'set of radii', which the 'rotated lines with a fixed endpoint' would overlap with in definition, once all possible incremental rotations are applied
          - similarly identifying that 'rotating a line with a fixed endpoint' would result in 'equidistance from a common point', this common point becoming the 'center of a shape', as 'shapes with centers' are capable of 'equidistance from the center' (a square has a subset of points that have the same distance from the center, bc it has a center at all)
          - similarly identifying that a 'rotated line with a fixed endpoint' would result in 'fixed distance from this point in every direction' if rotated in 'every direction'
          - similarly identifying that a non-circle like a square has a non-constant 'distance from the center', so a circle (a difference applied to a square) would possibly be created by a difference applied to that square attribute, as in 'something other than a non-constant distance from the center'
        - a function to identify the patterns in these connections to guess at new connections given other inputs (predict what 'intent' as in 'statement to prove' is associated with a 'sequence of operations')
      - a function to identify what is not proved by a proof or a proof implementation (whether all items in the inputs are checked or a subset, and the possible consequences of this 'subset selection' on the absolute validity of the proof)
      - a function to identify what is adjacently proved by a proof/statement sequence (if this is true, what else might be true that is currently not proved)
      - a function to identify 'missing interim/adjacent structures' in proofs like 'interaction levels', 'missing items in a set of functions' that would complete the knowledge of a system
      - a function to identify alternatives to using definitions as inputs (a derivation function to resolve 'fuzzy differences' into 'definitions of different concepts' and a way to 'connect definitions, in operations like combining definitions into other concepts', or identify alternate formats like 'input/output examples covering all difference types' that can act like a 'substitute' of a definition/function)

  - why understanding is usually a better option than pattern-guessing methods like prediction function-finding functions
    - understanding can be used more effectively to derive a 'guessing method' than the opposite
    - understanding is more generally applicable & can fulfill more functions than finding/deriving/generating a particular 'guessing method'
    - understanding is less dependent on data such as context data
    - understanding has compounding value, where guessing methods usually offer temporary & otherwise conditional value while & where the original prediction function is accurate
    - understanding can find/derive/generate error structures of a particular guessing method
    - understanding is better at guessing than most guessing methods in most cases (except where a guessing method handles new contradictory information, a new problem type, a new change type, etc better than existing understanding)
    - understanding, learning, simulation, organization, prediction, testing are interchangeable functions with varying advantages but generally can replace each other

    - finding a structure that can explain everything like a core interaction function makes it difficult to see alternate structures, but the importance of the alternate structures is finding the interaction level of these comprehensive structures and their connections, variables, inputs, etc
    - how to correct over-prioritization errors
      - apply the error of the over-prioritized priority to itself (reduce the over-reduction, as all things can be reduced since its a core interaction function, including the reducer, to restore balance)
      - add enough variables that the error will necessarily also change at some point (expand the reducer to an extreme degree)
      - view the priority filter from outside perspectives or from another perspective (reduce the system that generates or requires the reducer without incentives of an agent in the system)

  - add to govt
    - new financial instruments to avoid situations like insurance (fee to get group benefits), taxes (fee to get group benefits), slavery (zero-cost labor), abusive corporations (products with negative side effects), charities (zero-cost, zero-stake investment) which are proven to be suboptimal bc of their structure
      - other pooled risk structures like 'credit/risk pools' where someone with better credit can loan their credit score or other credit instruments to other borrowers so they can get loans, if they trust the borrower more than the original creditor bc the person with better credit has better information (theyre in the same community as the borrower)
      - investment instruments where some asset becomes free if used in an optimal way (college debt is forgiven if used to solve some problem)
      - investment instruments to pay for basics for children or students to get future employment, which they can buy their way out of, if they have money by the time the employment would start
        - other investment instruments like corporate bonds in big companies which are more stable than most governments who can provide a probable return on investment or alternate similarly valuable resources if they cant pay the principal/interest back as money
        - 'group investments' to pay a group (like schools or groups of friends) to learn a generally useful skill (like interface analysis to invent things including lists of useful variable/error types, or crispr to cure diseases, which are high-value skills that can capture market share or create new markets), the idea being that this group is likelier than average to be able to return the investment, be independent, contribute to the world, etc
          - this as a better alternative to slavery, which has generally negative side effects for almost no benefit, since slaves are more valuable if educated with these high-value skill sets which can generate millions, and slaves are the likeliest group to want a job learning/applying those skills
          - this requires cooperation/organization of slavedrivers so they can afford equipment like internet/phones and time to educate slaves & allow them to think (and remove enough of the slavedrivers to restore some balance, stability, and trust)
          - small business slavedrivers are the likeliest to exploit this opportunity for cooperation & organization at slave labor costs, which are trivial to outcompete
          - negotiation structures to negotiate things like 'removing some slavedrivers from the newly formed corporation in return for some new freedoms for slaves & a clean slate or buy-out for other slavedrivers' is a value-add in these markets
          - a path to help organize these local exploiters and incentivize them to stagger the transfer of slave labor to higher-value labor is similarly trivial to identify/apply (finding the value of current products required to continue producing, in order to sustain the transfer to less exploitative labor)
      - structures specifically built for specific functions like stability, integration, organization & other primary functions, so these arent left to chance
        - for example 'banning decisions that only benefit one or a minimum number of individuals illegitimately at irreversible global cost' and 'banning decisions that disconnect sufficiently large groups' and 'making sure the best decision-makers are making the decisions' is a specific structure of fairness that can be required in an optimal system to avoid obvious or known errors
        - other examples include: understanding/meaning departments of govt/companies (to understand why things are happening and the meaning of those things, such as 'why its humiliating for predators to prey on victims (bc it indicates a preference for easiness, and their dependence which makes them lower than victims, and their insecurity in their addiction to others' pain)'), pooled funds specifically to create benefits for certain sufficiently large groups, group insurance (so if one person doesnt get a payout, at least someone similar to them will, because they belong to the same group, which is a lottery), companies that have a built-in limited lifetime in which to profit or benefit some group after which they automatically dissolve
        - group crime contracts to negotiate some crimes or crime ratios that are allowed within that group in return for resources, to reduce crime rates (contextually legalizing them, similar to how legalizing drugs reduces drug-related crimes)
      - group allocation/organization algorithms, where a group needs people to vote for it or otherwise grow its population, so that people can move there (like cities paying for citizens, generalized to groups)
      - market error identifying algorithms to identify where the errors like bottlenecks (such as failures to route excess supplies bc of shipping costs), useless work (like designing website styling), rent-seeking middlemen like Coca Cola, & asset destruction (like oil which destroys ecosystems) are in a supply chain (where resources are disappearing, being destroyed or being hoarded) and how to optimize them (make sure that space funds are allocated to efficient inventions like dyson spheres rather than inefficient technologies like rockets which cause global suffering at no possible benefit)
    - time and space (and related objects like distractions, opportunities, alternatives, new causes/reasons, reasonable standard of living to create incentives as in 'stakes/resources to lose', etc) are required for criminal cycles to break, in the absence of intelligence
      - without these, criminal cycles will continue unless extreme emotional management (or other intelligence related to self-control like planning/prediction, to think through after/side effects) is taught or drugs are universally available
      - emotions cannot be sustained over time without cause (re-traumatization), as they take resources to sustain when the original reason for them is removed or reduced
      - actions cannot be executed without adjacence and/or access acting like adjacence
    - if 'expectation' is the problem with 'govt provided resources like bailouts' (the expectation is that they can make any decision and not go out of business) is an 'enforced limit' on the 'resource', such as limiting bailouts to 'one-time only bailouts' and applying 'enforced limits' on obvious foreseeable loophole-exploitation such as 'forming different companies after the first bailout to circumvent the limit of one bailout per company', thereby removing the problem created by 'expectation of bailout' (lack of learning that could possibly lead to success) by restricting its application so it cant be expected to happen again as it only occurs once
      - if the 'usage' of the bailout is another problem, that can be applied as a filter of the 'one-time bailout' structure, such as a 'set of allowed uses of the bailout which would probably make one bailout successful'
    - ai to guide leadership decisionns to avoid takeover/war/anti-democracy conditions given info like 'trust structures'
    - democracy errors include:
      - alternate illegitimate routes to power like buying info goods like opinions, usage & user bases & user compliance/dependence, & artificially manipulating prices with information market manipulation, voter free time, job market info
    - 'excess regulation' errors include:
      - punishing successful solution-finders (by de-funding through taxes on profits)
      - regulations that 'take away or invalidate property or otherwise violate property rights' prevent people from benefitting from their labor
    - 'lack of regulation' errors include:
      - rewards those who are best at 'finding loopholes to exploit' rather than best at 'solving problems'
      - exploitation without accountability
        - companies that exploit customers can liquidate their remaining assets to build solutions for those customers
          - example: colleges that charge exploitative fees can liquidate assets to build useful AI models for banks that gave student loans
      - making people fight to survive (without extreme advantages like 'information' or its proxies like 'education to derive information' or 'internet access to find information') so they frequently see crime as the best option likeliest to help them survive
        - solution: creating 'organization' to make basic goods affordable, or 'artificially creating jobs' instead of organizing
        - example of an 'organization' structure:
          - if a supply chain is optimized (where 'optimal states' include such states as 'local supplies are distributed locally (such as by 3d printers that use cheap resources like sun/air/dirt as input'), basic goods will be cheaper
          - this is a case where 'labor & other resources' are 'organized' into an optimal 'supply chain' structure
        - other examples of 'organization' structures include 'location', 'language', 'communities', 'info tech' (specifically 'databases/apps', 'internet access/search', 'cell phones'), all of which can increase the probability of other organization structures occurring, like 'markets' and 'coordination' and 'companies'
        - these can be created by creating structures of 'interactivity' that enable organization structures (having a 'language in common' or 'good translation tools' allows 'interactivity' to enable 'markets')
      - lack of education or its interchangeables, which results in other errors like 'not identifying interchangeable inputs to requirements (basic goods)' (such as how 'independence', 'power', 'love', 'happiness', 'understanding', 'intelligence', 'information', 'solutions', 'potential' can be interchangeable alternates as they can be different formats of the same structure which is 'freedom' as in 'ability to adapt, as in self-correct')
    - companies exist to solve customers' problems, so if there's a problem people can & are willing to pay to solve, companies will solve it, so all thats required is to tell them what consumers want at what prices and what costs are preventing govt budget from being optimized for solving unprofitable problems, & the products will be built & tech will be built to reduce those costs, rather than taxing to create govt agencies to slowly & inadequately prevent the same group or corporate loophole-exploitations that can be identified by algorithms, exploitations which are symptoms of an inefficient system that hasnt invested in reducing its primary costs like 'health care' or 'form processing' yet, rather than unique exploitations that requires budget to solve inadequately when reducing primary costs would invalidate the need for these exploitations by group or corporate entities
      - example: 'tracking assets' is only necessary if assets are scarce & unique, and 'regulating the health insurance industry' is only necessary if health care costs are unpredictable, extremely variable, & subject to randomness, which they dont have to be if costs are reduced 
    - 'actively preventing their money from ending poverty' (with tax avoidance, not trying to end poverty, not trying to be sufficiently different from people who failed to end poverty, etc) is a sub-optimal (inefficient) use of money, proving that people who could end poverty (either individually or as a small group) without negatively impacting their own lives and dont do so, are people who cant use their own money in an optimal way, indicating that their success was almost a complete accident plus a little hard work such as 'filling out forms' or 'making a pitch to investors' or 'hiring people'
      - if their success wasnt an accident, they will be able to replicate it bc theyll understand how to become successful and will be happy to share that knowledge and will have rules they follow to achieve that goal (creating huge value out of almost nothing) and will be happy to do so for a government or other entity trying to solve a big problem, otherwise their success was an accident and they dont deserve to keep the outputs of that success (just like how I claim I can find a new way to automate problem-solving in a few minutes and I regularly prove the legitimacy of that claim of my value, they also need to prove they deserve their claimed value on a regular basis so it's believable, by demonstrating how they didnt need luck but rather their own work/intelligence to make good decisions that few others would have made in the same position, indicating that these are their identities differentiating them from other people, rather than an out-of-character decision, and teaching other people how to create huge value out of almost nothing, and making sure other people succeed at doing so, and regularly executing their process to create huge value out of almost nothing to solve important problems)
      - poverty negatively impacts these individuals who could easily end poverty, given that theyre constantly harassed about it by similarly selfish & stupid people, but they try to maintain poverty instead bc its cruel and they prefer cruelty
      - these individuals dont try to teach people how to use their money efficiently or figure out how to do so themselves (teach them to 'apply interface analysis' or even just how to apply one interface such as logic), which would solve the problem of 'their money being used inefficiently', so its possible they dont want to solve that problem
      - these individuals are not being offered protection by some powerful entity like a government if they do give most of their money away, after being criticized for many years, which has made them afraid of non-rich people, and should be offered things like some immunity & protection by governments & then they need to see that promise honored so that someone is safer after giving their money away rather than more vulnerable - there have to be rewards in terms of things they care about, such as more customers/profits - if they get richer by ending poverty bc everyone wants to buy the products made by a company that ended poverty, thats a sufficient reason to justify doing so, so a company that struggles to make advances in technology could get customers another way, by ending poverty which would improve their brand beyond any possible attack
      - if you told these companies that their customers/users really want to end poverty or really want cheaper medicine (which would indirectly end poverty) or really want to buy & donate cheap phones to poor people so every person at least has a cell phone so they can find the resources they need, these companies would find a way to create that product (the product being cheaper medicine, an easy way to buy phones for poor people, etc) & offer it, then market that product until it was used & successful at fulfilling the demand from consumers/users, bc thats the whole reason companies exist (to create value for customers), and if 'cheaper medicine' has value to consumers, they will create it - the tech companies are in the best position to do that so tell tech companies that people really want to end poverty and theyll find ways to create that value if the market exists (if enough consumers want it and are willing to help them pay or otherwise create value for it)
      - 'open-source accounting' takes places every time you buy something on amazon and you see the shipping costs added to your total, which you can compare with the comparable usps shipping costs & cost of finding the seller/product without Amazon, which I notice people who could individually end poverty by buying enough phones/water/food/medicine/generators to end poverty are not doing, possibly to avoid giving Bezos/Amazon any credit, so lack of transparency in how charities use money isnt good enough because theres another charity offering full transparency that ensures donations get to the right person, which is Amazon - all thats missing are the addresses to send these packages to and a coordinated effort (of large groups with small resources or a small group with large resources or both) to buy & activate those transactions
    - changes that can improve markets, laws, & govt interactions
      - 'spend govt tax money or excess corporate profits on hiring people to solve large govt/community/company organizational & social problems that add permanent value and are one-time problems (where once the solution is found, it can be deployed repeatedly and new problems can be solved instead) problems such as:
        - reduce their costs (automate tasks), unpaid debts, sub-optimal markets that are over/under-monopolized, find low-cost, high-value products that can be re-sold without modification (ad space, subscriptions/queries of an API, charger cables/headphones/cases, t-shirts/nft's/posters, medicines) or products that have a large probable user base to generate research funding for companies/govts, fix supply chains, high tax rates, allocating basic goods quickly at low cost, disincentivizing crime, fixing mental health problems, reducing unimportant jobs like re-building the same apps repeatedly & allocating labor to more important tasks, ending tribal feuds by similarizing ('humanizing') sides & physical separation, customized education for neurodiverse learners with different experience/interests, enabling vulnerable groups to report crimes or request help with cell phones, inventing & building crime prevention technology, over/under-spending, mismatched cause/responsibilities, unequal laws/opportunities, sub-optimal technology like biased algorithms, repeated work like govt websites, expensive medicine, deploying solutions once a decision is made about which solution to apply, design experiments to find solutions faster, coming up with new ideas to solve problems, lack of oversight/transparency, fixing corruption, preventing dictatorships & rent-seeking
        - alternatively, hire them to automate solving those problems (my invention), such as automating 'system analysis' to identify low-cost optimizations that wont cause errors in a system, automating 'interface analysis' to come up with new ideas to solve problems
      - 'temporary transparent directed costs' like 'temporary subscriptions' or 'temporary price hikes' that a company charges customers who opt-in to pay slightly more temporarily for particular purposes like 'researching a medical problem' or 'using clean energy' or 'funding transportation of supplies where needed', where the company offers a way to check how theyre using the money & states specifically what the extra money is for & specifically how theyll use it
      - 'multi-purpose investment funds' where people who invest are guaranteed returns in multiple possible formats, so if the fund doesnt fulfill an intent like 'make x% ROI', it can guarantee that it will fulfill another intent of the fund, like 'increasing clean energy use' or 'increasing quality of life for vulnerable groups', so either way investors get some value type that they care about
      - 'finding interim "state-connecting" solutions' that solve problems 'temporarily' or 'emergently when applied at scale or under other conditions', such as how covering/reducing 'transportation costs to high job-density areas or cell phones costs or basic good (medicine, clean water) costs or "valuable idea"-inventing costs (computers, machine-learning, encryption, dictionary/encyclopedia, solution/rules database, quantum computing, batteries, solar power, math, clean energy, search engine, sorting algorithm, bitcoin, interface analysis)' is an interim solution to 'ending economic opportunity inequality' that when applied at scale might temporarily end economic inequality, & once other solutions are found to improve job distribution in general, it might complete the task if the previous state is the 'state after applying the interim solution', rather than applying it starting from the 'worst case scenario'

  - add to tech
    - 'structural simulation' tech at a midpoint between 3-d printers and visual info simulations on a computer, where a structure is real conditionally (such as temporarily or partly), like creating a scaffold of a requested structure that can bond for a few seconds before dissolving
      - weaponized versions of this involve sending structures to other positions to disrupt plans and interfere with resources elsewhere
    - self-driving cars can use proxy variables like 'limits (like a curb) of change types (such as forward motion)' and 'behavioral patterns of other cars/drivers' in the absence of 'direct signals (like signs)' and structures like 'probable error types like probable driver misunderstandings of signs' as well as useful/adjacent/probable structures (like 'roundabouts') which can be used to predict structures it will encounter before encountering them, as probable models of structures it may encounter to use to understand new different info than what it has already encountered
    - an optimal truth-derivation structure is useful as a 'time-traveling device' bc an optimal truth-derivation structure can also derive the truth at any previous/future time
      - similarly, an 'impossibly optimal' truth-derivation structure that is 'more efficient than reality at preserving information ("better than reality" meaning "better at preserving information than a reality with time is, as a reality with time loses information and re-generates that information on-demand", such as a 'neural network ai model' that perfectly predicts a 'human mind with more neurons than the network')' is useful as a 'time-building device' bc time will flow in the direction of greatest stability/efficiency, and a 'time-basing device' bc its more efficient to base changes on the standard of the more efficient/stable reality (as an 'alternative history with a different starting point, breaking time-direction symmetry by creating a different adjacency as the original starting point that has a stronger bond')
      - the spacetime that contains the most optimal truth-derivation device (which will look a lot like the interface network) will attract other spacetimes and standardize & align them to the version of the spacetime having that device
    - cars that can be stacked and linked to other cars as another variable of 'coordination' between cars for resource-sharing (such as cars with fluid-like plastics that allow them to be merged temporarily with other cars)
    - any implementation of a 'manmade wormhole' that uses entanglement to connect copies of an 'information and energy network, corresponding to or generating a person' would also have to copy the device capable of creating those 'connections' and the 'network copies', meaning it has to transmit itself to the position of the 'copied person' to avoid being stuck once connections degrade or as a safeguard against one copy or the original being destroyed (creating a backup with a backup-generator and backup-connector), and has to test the other position of the new copy for 'viability of the information/energy network in that position', otherwise there's no point investing in that copy, so it has to connect to 'environment networks of the new position' first, which could destroy the user (it has to check if the remote position is a solid or other structure which a person cant be copied to)
      - similarly it would have to check for other users ordering a wormhole to the same position, where their requests might interfere, to check for 'possible unviable environments' as well as 'current unviable environments', which would involve intercepting these requests or checking the other users making a request for a copy in the same position as the original user
      - 'checking for a structure in a remote entangled position' involves applying a function that changes molecules in the remote position in a similar way as it changes molecules in the local position
      - whatever function is applied in the remote position would be applied locally - the net effect of this would have to be predicted to verify the safety of a request
      - alternatively, creating a function chain of entangled nodes to calculate info elsewhere would have to be applied
      - the complications of interactions with other entanglement requests is probably the more complicating factor than 'creating organized entanglements which can be used to copy an information and energy network'
    - 'business strategy automation' (company-level decisions) should be using data from tech companies rather than designing complex algorithms most of the time, in an absence of automation tech
      - business idea viability can often be done automatically with facebook/amazon queries (search for product complaints/wish lists and consumer price points) or facebook posts (automatically comment sentiment), as opposed to designing algorithms to make business decisions automatically
      - 'market strategy automation' (market-level decisions) to manipulate markets can be made available to groups of companies/govts that agree to use it to benefit the most citizens (create labor markets, fund energy efficiency projects)
    - however there is a business opportunity for manufacturing/implementation/automation companies to help inventors/lawmakers bring inventions/laws to market
      - currently theres a big untapped market for 'implementation automation', which would help new ideas be quickly prototyped and supplied in the market or help laws be likelier to be passed
      - https://newatlas.com/good-thinking/student-designed-filter-lead-tap-water/
      - similar opportunities include:
        - 'create user adoption through influence/marketing/documentation/migratability/convenience', 'get a law passed', 'enforce policies/laws', 'automatically generate & coordinate business/market strategies/decisions/states/trajectories to benefit society', 'implementing business/market strategies' and other complex problems that are typically considered un-automatable, as they involve making an idea a reality, which present business opportunities, as useful products that businesses/pac's would be interested in
        - example algorithms that would be profitable or otherwise beneficial to train and sell access to:
          - for example, an AI algorithm that 'changes a bill until it has a high probability of being passed'
          - an example for 'implementing a business decision automatically' would be an algorithm to 'identify any public documents filed by other companies (or private documents from an anonymized govt-provided dataset)' and an AI algorithm that changes these documents to fit the new company, so they can avoid legal research to find out what forms they need to fill out in order to implement a business strategy, like 'bring a product to market', 'schedule inspections with govt agencies', 'identify information required to be posted with the product advertisement', 'identify business requirements like requirements to respond to consumer complaints', 'identify attributes (& attribute values) like regions/demographics for which a market for a product is particularly optimal (attributes like "consumers expressing high dissatisfaction with existing alternative products in that market")', 'identify where funding typically makes the most impact'
          - an example for a manufacturing company would be an AI algorithm that 'identifies viability of a product submitted by a potential prototyping/manufacturing customer, given their potential to manufacture that product for the business customer', an AI algorithm that 'automatically identifies possible designs using their available resources', or an AI algorithm that identifies 'possible errors in manufacturing the product to estimate risk of manufacturing the product'
          - an example for 'implementing/enforcing a law' would be an AI algorithm that can abstractly 'identify behaviors violating a rule, given an input rule' (abstractly meaning 'for any input rule'), 'identify strategies that have been tried and are known not to work', 'identify changes to sub-optimal strategies that could work better'
          - an example for 'increasing user adoption' would be an AI algorithm to 'identify changes to apply to a product/advertisement or marketing strategy or product distribution strategy that would increase convenience, increase incentives to use the product, or specifically add attributes consumers want' to help in product design, marketing, or documentation creation
          - each problem an industry has is a potential automation opportunity if there's good enough data or other structures, and 'making ideas reality' is a particularly common complex problem, to connect 'ideas' with 'optimal market/product states (like a state of a "successful business")'
        - this takes some of the burden off of individual executives/officials to miraculously make good decisions for millions of people, which they get wrong so frequently they could probably use more automatically extracted information to help with, like automatically identifying probable 1st/2nd/3rd degree market/business/product/consumer/govt impacts of particular possible decisions
        - the 'humane default decision' could be left as an automatic configuration (which makes decisions like sending contracts, manufacturing products, hiring people automatically), which officials/executives have to intervene with to implement anything else
        - these 'humane default decision' configurations of a 'market/business decision automation' algorithm would take some responsibility away from govt officials or company executives and also reduce their work to analyze info before deciding, so theyd have to do more work to make a less optimal decision for society
        - groups of inventors can then start more successful businesses with higher probability once they have manufacturing/implementation/user adoption help from tools that automate these
    - since most people have access to water but its not clean enough to avoid getting sick from it
      - mobile phone with uv light functionality built-in that can kill bacteria in water since people were going to want a mobile phone anyway and theyre at the target price point of other solutions (5% of income, or $4), the remaining task is removing chemicals from water, which some tools like condensation tools and plants that dont absorb pollutants could help with (rubber, cattail, water mint) 
      - electricity-generating tech (like 'body heat to electricity conversion' tech) can help them solve other problems independently (helps increase the effectiveness of other tools like condensers to clean water, as heat speeds up these processes, so if they can generate heat from an electricity-generating device, they can speed up that process), which may be a more useful node in the supply chain to correct/supply than other positions, like 'specific tools' or 'delivery automation tech'
      - long-term strategies can involve digging strategically placed canals or wells in high areas that will flow to other areas with automatic digging devices, relocating people, efficient air harvesting devices, harvesting energy from bacteria (EET), brains, thermoelectric energy harvesting through exercise or overheating (wearing enough clothes to produce sweat but harvesting the heat as its released instead), plants, making salt water less salty over time (slowly adding other electrolytes in a timeframe that animals can adapt to, or adding salt-binders or salt-breaking enzymes/bacteria), etc
    - tech companies can distribute approved info to people directly by email or notification rather than waiting for them to hear about news or read the news, so they can see how many notifications corroborate each other
      - tool to identify 'common' opinions on a matter, 'changing' opinions, 'trending', 'trolling/triggering', 'extreme' opinions, 'corroborated' opinions matching various science studies or other info sources, opinions 'matching patterns of conspiracy thinking or patterns of emotional reactions eventually tempered with facts', etc
      - https://www.washingtonpost.com/technology/2022/03/11/tik-tok-ukraine-white-house/
    - carmichael function and rsa encryption
      - example of why a 'structural similarity' isnt all thats required to determine useful structures
        - https://arstechnica.com/information-technology/2022/03/researcher-uses-600-year-old-algorithm-to-crack-crypto-keys-found-in-the-wild/
      - adds multiple outputs for an input (multiple y's for an x value) and multiple ways to get the same output (multiple x values to get the same y), and the output A(n) obscures the input n to some degree with increasing obscurity with increasing n
      - '2 ensures a 'vector position of 8' for all coprimes of 8' (totient function of n = 8 is m = 2)
      - a^m = (some multiple of n) + 1, for each a 
      - 'find the smallest integer power the coprimes should be raised to in order to get a position of n'
      - the modulo function is useful for finding 'distance from a value (like products of 8)' which is useful for 'finding primes'
      - can any other attributes of q & p be derived other than:
        - the lower limit represented by e, which the totient of p * q must be higher than
        - the difference in magnitude/length required for security
      - searching for prime pairs p & q that could create n should be filterable by removing ranges that dont contain a prime that could produce an approximate value, like filtering out the ranges between integer pairs that could produce 100, as numbers in these ranges cant pair up with another integer in a way that could create 100 by just using one multiplication operation
        - multiples of 2/50, 5/20, 10/10 can create 100, given that its even (factorible to 2), produced by a multiplier of 10, and given that its produced by a multiplier of 10 can also be divisible by 5
          - so values around 50, 20, and 10 can be 'possible solutions' given that 2, 5, and 10 are known possible factors, until the multiplication is calculated to filter the solution space
        - this would be useful for filtering primes that dont have a paired prime that could create n (or a value near n)
    - identify useful questions to avoid researching/testing pointless possible solutions (solutions that will always be high-cost, solutions that have a built-in flaw that invalidates them) 
      - is there an 'encryption algorithm' that allows identification of each attribute of the original document (like homomorphic encryption) that is required to build sufficient 'attribute combinations' to filter the space of possible original documents at all
      - is there any way to hide information in a way that cant be easily found except with info about a shortcut to find it (the decryption key) or will information-obfuscation operations (and any info interaction function) always create other info (side effects like computation/usage/access effects, outputs/inputs), meaning 'is it possible to isolate info operations perfectly so that these operations dont create these measurable side effects'
      - are explanatory structures of randomness (like 'commonness') useful in offsetting the random structures used in encryption algorithms or are those irrelevant given adjacency to pure randomness rather than the false appearance of randomness
      - 'injecting randomized variables' is a common technique in encryption algorithms to obfuscate the original value by additional random value conversions, storing the determining variables of the actual randomly selected options
        - a useful question to answer would be:
          - 'what is the structure that stores the least options (attribute values) that creates the most alternative options required to test when brute forcing the original value' (which values can be re-used as options elsewhere in the process without reducing the number of alternatives required to check when brute forcing), as this structure would be a good place to start when deriving encryption algorithms
      - is there a set of adjacent info states (opposite+add+multiply, opposite-randomize-add) that can be stored & used to identify/filter any other functions applied in the encryption process
        - can you verify if an info state was likely to be or possible of being a state in a process using these sets
    - 'signal with public/semi-private chats' where sessions/passwords are not used, but there are multiple levels of possible permissions, to treat chats like directories/files that you can change permissions for, to make them public or visible to a private user list, where all of your posts are stored locally, and permission checking is done (does this computer have a permission to view the requested resource) before allowing internet requests to proceed, and the user with the earliest native source of the database of their own data (which can be encrypted with a one-time password/identifier like 'device/app usage patterns' for all the apps deployed on their device in this way, and which can be imported to a local semi-private blockchain in a local user group so other people they know have copies) has priority in multiple claims of ownership of an account (which is just a username and the associated database in this case) - the disadvantage is being tied to the device, but local network (internet of things or wireless device network in a neighborhood or family which are connectible by proximity only) or usb backups to their other devices like their mobile are a possible solution to that
    - file systems that are purely for storage (which dont allow one of edits or reads by any/specific programs, or which dont allow installing any non-default apps/libs bc its purpose is to work or to be a source to copy from, for example) and other intent-based file system designs can be used for specific intents needed by various apps, which can self-lock or self-destruct if used for any other intent, damaged, or altered, and these file systems can be generated dynamically and embedded for a one-time use of an app, similar to containers but without the infrastructure to identify it as a particular product that supports containers
      - for example, it could generate a 'file system where a "pseudo-symlink map" is applied to a random ratio of files, to point to random alternate locations, where the map is visible to protected users or services, so once the symlink map is generated, it shouldnt be required to be accessed again and can be deleted, as each service that uses it will have its own algorithm to generate it as needed, or each service can generate its own symlink map, a map that can itself change frequently within each app/service/usage' to obscure paths in order to avoid bugs that take advantage of known paths, or a file system where a program can only request the new path of a file or search for the new path of a file if it can provide the previous location which was randomized
    - other filters can be applied by user activities (is the user using their laptop or cell phone, is the user using other apps, is the user doing too many other things to possibly have logged in at the time, which can be boolean flags provided by a third party service or a user's device)
    - 'download/generate & delete app' process for each use as a preventative measure against malware that checks for an app thats installed before exploiting a vuln in the app
    - add concept of 'value overflow to adjacent nodes', 'local conditional deactivation', and 'local feedback' to nn
    - depending on propagation/routing function, a neural network node can contribute multiple features (influences) (in deactivating multiple weight paths) on the final set of generated coefficient sets for each iteration during training
    - examine other weight update functions, like those having an oscillating sequence that converges around a value rather than those having incremental updates in one direction
    - related problem: as weight updates occur, predict the remaining weight updates (what is the likelihood that weight updates follow one pattern over another or end at one value over another) so that some weight updates can be skipped
      - applying 'find prediction function' to various inputs/outputs in a particular solution-finding method like neural networks for the general 'find prediction function' problem format can be useful in some positions
    - add example of a 'generate variants' or 'generate possibilities' function by applying variables to a particular structure, to generate variations of functions including workflows, queries, or problem-solving functions like 'find structure in a structure'
      - 'structure x' can be replaced by 'alternative structures' like 'find generative functions of x' or 'find alternate routes to x or its outputs' or 'find generative functions of requirements & other inputs of x' or 'find variables of x and alternate variants of x' or 'find interchangeables of x' or 'find invalidating structures of x' with varying degrees of success according to various solution metrics
    - identify the 'reason why something works' (solution success cause) formatted as the 'structure of change applied to inputs/outputs that enables a later output to be optimized by some metric' and 'why the structure of change enables that or does so better than alternatives'
    - which format is optimal to standardize to when implementing function
      - using 'vectors' to represent everything require a function to map non-numerical values to vectors, which will normally either lose information or encode it inefficiently, storing more information than necessary, and not storing it in a way that can be compared with other numericalized non-numerical values in a meaningful way by default, without referring to the original non-numerical variable values
      - using 'inputs/outputs' to represent everything is more likely to avoid losing information, but does represent everything as a sequence structure, where sometimes sequences arent implicitly described, such as in a set, which would require a description in the form of a 'generative sequence' or an artifical order imposed on the sequence

  - add to math

    - the intersections of filter sets and limits ('limits' meaning 'sides of shapes that are filtered by a filter set') is a useful structure to identify as some filter sets are more useful at filtering a particular shape than others, like how a filter that isolates small sub-shapes is useful for identifying small structures on the surface of or embedded in other shapes, so these filter/limit pairs are useful to identify since they provide complementary effects in intents like identifying information
    - add example of applying topology, homomorphism, orbits, & fields definitions to determine interface rules/requirements/limits
    - identifying a maximally different shape (the 'most differences that can still be connected', as opposed to an interface which is a 'common base of similarities with adjacent/trivial differences applied that dont change the base')
      - why do this at all? 
        - bc 'maximally different but connectible shapes' are a good structure to find as a basis for a 'problem type network', problems being differences, and will reflect a 'shape of understanding' (of how differences work and interact) that can be re-used across problems
      - how do structures like 'repetition' become useful in creating these useful differences?
        - given that 'repetition' of inputs is the difference between core operations like addition/multiplication, and multiplication adds a dimension where addition increases a value within the same dimension, how can repetition be used to add dimensions within a dimension (embed a dimension of change in an existing dimension, without contradicting it)
          - similar to how units like squares/cubes are useful in building other structures given their uniformity and simplicity (so theyre highly interactive with each other), and how cellular automata are useful in finding patterns emerging from repetition, repeating some variable values is all it takes to add new dimensions, like how 'repeating' a variable like a power source (so that everyone has a power source) can solve a self-contradiction of 'power', embedding 'power' in more 'positions' without contradicting the 'power', so that it can be used again to store more information
        - repeating a square can build non-squares (change the position of the square), repeating a square vertically can build cubes (change the layer of the square), repeating a square by rotating to its cardinal directions can create a circle
        - what other operations are useful in creating other difference types? 
          - isolating differences is useful as an opposing function of repeating differences, bc isolated differences can be used as core components
        - why is repetition useful?
          - 'structural similarities' and 'coordination' emerge from repetition, from which other useful structures can be derived, such as 'input/output sequences' (derived from 'coordinating' structures) and 'similar alternatives based on a common base' (derived from 'structural similarities')
          - some structures' interactivity allows some interactions without requirements preventing specific structures from emerging (stacking squares is not required to create another square), the lack of requirement embedded in this interactivity being useful for variability of structures its capable of building in isolation of other differences
      - what changes are more useful than others in determining this network (which will look a lot like the interface network Ive already specified)
        - embedded variables are more useful in creating 'maximally different structures' bc they allow compounding changes to occur, when a particular variable is strong enough to support these embedded changes
          - for example, a variable or variable set that can hold 'infinite sequences of infinite sequences' (like a graph of isolatable infinities and isolatable sequences using the parameters of these objects) is capable of holding more information bc of the embedded differences
        - self-contradiction structures are useful bc of their indication of limits in potential information storage of a structure
          - for example, an over-prioritization of a structure like the concept of 'power' results in an error ('over-concentrated power opposes other power sources, leading to reduction/dispersion of concentrated power') that self-contradicts (contradicts 'power')
          - this is useful bc a self-contradicting structure will be a less powerful/useful interface than other structures
      - what is not useful when building this structure?
        - a 'list of all the known numbers' is not useful in building this structure, even though it holds more differences, bc it requires immense work to traverse bc these differences are repeated without cause, just like a 'list of all the particles in the universe' isnt useful
      - similar to how a square is a unit 'maximally different shape' by connecting orthogonalities, what other maximally different shapes can be assembled, such as 'embedded variables' (like changing the continuity & direction & position of a change type like a line to increase the number of differences from another line, thereby embedding variables in a set of changes in the additional dimension)
        - a related question is 'what change types can be embedded in a particular dimension'
        - this maximally different structure will have more complex structures on other interaction levels like holes, intersections, discontinuities, etc (these being interactions of core structures like points and lines, forming another interaction level), which will form other interaction levels containing other structures like inevitabilities, irreducibilities, arbitrarihoods/ambiguities, which is the interaction level where the primary useful differences will begin to emerge
        - at later emerging interaction levels, connecting the generative/descriptive functions of the shape with these useful structures like inevitabilities will be useful for creating adjacent connections between these structures that can be used to skip computations
      - the most useful maximally different shape might have equivalencies such as a repeated structure (like how a universe is full of differences like position/life despite having few core structures like elements), but it would also minimize equivalencies, while still being connectible, thereby representing the most possible useful differences (such as 'differences in exceptions and examples' or 'the set of inputs that can change a functions output the most' or the 'opposite of a requirement in a cross-interaction level interaction')
        - this structure would be the most useful for representing useful problem-solving structures in one structure
      - 'definition routes' are a useful approximation and difference-finding structure, where the standard definition acts like an interface in its position as a base for changes, each concept representing an isolatable difference type that can be used to 'connect different structures' and 'add differences'
    - given that the function of 'rotation' can create an 'opposite' structure which is useful for connecting/creating differences, can this be extended to create all useful difference types (or all useful opposites), meaning "can a 'structure' be rotated to create 'nothing'", and why is the rotation the cause of this useful structural difference
      - does rotation hide structures in the absence of uniformity (like a sphere) (the corresponding question on the physical interface is 'what happens when you rotate light, aim it at itself, etc')
      - can rotation structures like 'spirals' be applied in a useful way to create useful differences across multiple variables at a time
      - what is the connection between opposite-creating structures like 'rotation' and output difference structures like 'reverse (opposite direction)' 
        - does one tend to hide information of specific differences (rotation creating effects such as 'blur' which create the illusion of solidity/cohesiveness, hiding the differences between the isolatable states to make them seem similar), and the other tends to hide different information (like 'state sequence or trajectory to reach a point' and the 'starting point' and the 'original change applied, which is now erased by the reversal')
          - can one information type be hidden/isolated from the other, using the other or itself (can the trajectory be hidden from rotation, can the blur be hidden from reversal, can they be hidden from themselves, can these hide operations be applied to limits such as requirements)
          - hwo can this hidden information be retrieved in an absence of information about side effects, patterns, etc (opposite structures of information, as in timeless/constant/global information, impossible information, contradictory information, can be sent to indicate where information is not, to filter the possible positions/states of the information, similar to how light can be sent to indicate where structures are not using shadows to derive position of structures) 
            - as in, 'how can the starting position be derived on a circular trajectory that has already occurred/begun, in a system where the information about the starting position is not stored'
              - using adjacent information like side effects like momentum & momentum change patterns, and inputs like energy requirements/sources, and possible interactive (interfering) structures, etc
              - alternatively, applying a spiral structure to identify the possible side effects of a state sequence with information preserved and finding connecting differences between the circular structure and the spiral, to identify 'adjacent versions of the circle' like a 'tilted circle' (which would give information about possible starting points such as 'if momentum is x at this point, it could not have just started the trajectory at this previous point')
              - identifying contextual differences of the containing system and the spiral/tilted/circular structure will identify the intents of a trajectory on a circle (to hide information), which will help identify agents with that intent that could have triggered the trajectory
              - identifying attention patterns like 'the circular trajectory could only have started recently, given how this circular structure being observed has only recently triggered attention, and given how uncertainty structures accrete', given the requirement of an agent capable of paying attention and the lifespan of an agent and the agent's involvement in 'asking a question'
          - what other structures can have similar effects as rotation (infinities can give an illusion of solidity/cohesiveness)
        - example: some changes can be reversed by rotating, even if the rotation direction doesnt change, since the change type bends in on itself, thus requiring some changes to repeat in the required position change sequence required by the rotation, a spiral being an opposite structure that removes this repetition requirement
        - rotation also removes information about which iteration is currently being run, given this repetition, whereas the 'iteration number' would be easy to track on a spiral (measuring distance from starting point)
        - what shapes gain/create information when rotated (creating new differences with each rotation, like 'infinite sets in an infinite set' which are just different enough (a different infinity, but still an infinity) to be isolatable/measurable), as opposed to shapes that lose information when rotated (shapes with uniformities like circles where differences like trajectories can exist while losing the representation of that information)
      - the corresponding question in physics is 'how does this relate to cpt symmetry (what changes are only hidden as opposed to being actually reversed with no side effects', similar to how the corresponding question in ml is 'how can this create contradictions in an argument (rotating direction to the opposing direction)'
      - the 'rotation in reverse' in the cpt symmetry explanation example seems more similar to a spiral rather than a reversal on a circle, diverging from the original trajectory
      - what would it take to convert a spiral back into a circle or create at least one point of intersection with the original trajectory (meaning it begins to raise its elevation but is forced back into its original angle, creating an intersection point with the original loop)
      - is it possible there are multiple symmetries and variables making such an intersection possible and making both spirals/cycles such as 'spiral cones' possible, as well as other shapes of time (more complex shapes like topologies as a continuous connected state sequence)
    - the alignment of structures like 'cause', 'intent', 'example input/output sets', 'requirements', and 'functions (change sequence/network)' is in their common structure of 'direction' which makes them similarly useful, adjacent, and possible alternative information sources
    - ways of coming up with orthogonal axes as a useful structure:
      - given a default representative structure of parallel x & y axes, expand the set of structures that represent 'connections of changes in x,y' from the structures possible by connecting changes on the set of parallel x,y lines (such as parallelograms connecting changes on those axes)
      - standardize structures that can be built by connecting x/y changes, by straightening them out to have right-angles, so that a change in one direction has the same impact when applied to the other axes, rather than having a ratio of difference
        - expanding the set of standardized, composable interactions between the two variables (the interaction space being the area created by the values) so that more variables can be added, so that change over time can be more clearly mapped if one variable represents time as the sequence of changes would be overwritten if only using a set of parallel lines to represent the variables, where the orthogonal axes would allow isolation of those changes by expanding the interaction space so each state change in one variable can move in an additional direction as well to create a trajectory
        - to apply this structure in the set of parallel lines, the state sequence would be difficult to track (one would increase to represent time while the other might overlap with previous changes, which could only be indicated by different variables like different opacity), whereas the orthogonal axes makes it clear by expanding the dimensions of change
        - graphing a shape like a circle would require two sets of overlapping trajectories on the parallel line sequence which would lose valuable information about circles which are a useful structure, from which a rotation/symmetry/randomness/equivalence can be adjacently inferred
      - first identify corners and closed areas like squares as useful structures, then identifying that these structures have variables height/width that can be represented as two standard spectrum variables
      - identify that an intersection of a common point (center at zero) would be useful for identifying four different types of change pairs (positive/negative of each variable) and identify equalities in these difference types for intents like comparing those difference types
      - to make intents more easily fulfilled, such as 'multiplication of these differences (positive/negative) in these variables', identifying 'area' as a useful interaction structure between variables
      - applying connections between a plane and a line would be a super-dimensional variant that could occur when describing 3-d shapes, the sub-dimensional variant between comparing a line and a line orthogonally
    - ways of mapping complex operations like 'explain' and 'organize' and 'integrate' to math operations
      - high-level functions like 'explain' are useful for similar intents as functions on other layers like 'map' and 'reduce' (as in 'reduce to a definition that uses only these known concepts' or 'map to another network of concepts') on the structural function layer bc they have similar input/output sequences and are similarly useful as these intents are common and coordinate with other useful intents, though high-level functions translate more complexity into simplicity given their adjacency/interactivity with other high-level functions
        - finding the 'input/output sequences' of functions on such interaction layers is useful to identify how inputs of one function traverse those layers (inputs to the 'analyze' function travel to the 'explain' function bc of their input/output sequence that maximizes usefulness of the inputs when they are connected in that sequence) - meaning 'how information travels in that interface' ('how the interface can reflect light across its structures such as its functions')
        - map the impact of the variables applied by each interface across these interaction layers of functions (what information is applied that translates 'reduce' into 'explain' and 'structure' into 'organize')
        - these cross-interface connections will frequently appear in useful interface queries that cross interfaces, as they are adjacent connections and are useful for applying the corresponding value of one particular function in another system
      - translating language to math structures typically involves a network where nodes are labelled as a particular word (it has a categorical or other semantic attribute that is a text string of its label)
        - these can be represented by default as a numerical value, using a 'similarity score' to other words, more different words being more different in value to another word, which loses information of original position in the network and adds irrelevant information such as order (on a number line by assigning values on that line)
      - another way of representing them is using networks as a language
        - for example, the network language equivalent of 'explain' is 'connect a network (the thing being explained) to the network of the explainee (their current understanding of concepts which is the input network to the explain function) using adjacent operations on their current network (using concepts they already know)'
          - this is a way to format language in a mathematical notation that doesnt remove info and doesnt require labels/categorical variables arbitrarily assigned
          - the function could be a static set of network sequences used to make these connections in inputs of varying complexity, or a dynamic function to connect any input explainee network with any other network
        - the network language equivalent of 'organize' is 'applying a network representing a structure to fill and filling it with the input components to the organize function'
        - the network language equivalent of 'integrate' is 'merge networks into one network, by mapping coordinating input/output structures and retaining both input copies where possible'

  - add to ml
    
    - ml networks are often biased in more obvious ways than other sources of decisions like laws/humans, so these errors may be more useful for their obviousness and ability to correct them

    - ml language of models, variables of param sets/defaults, and data sets such as '(biggest language data set => default lambda => compressed mnist => default params with standard cnn) for intent (find new differences other than variable set x)' to fulfill/communicate results of useful intents like 'find new difference types' with various applications of ml as a hyperintelligence variant using ml as its primary interface
      - 'hyperintelligence' as a human brain that can simultaneously run queries/programs on thousands of computers and change/evaluate/apply/build those queries/programs in real time
    
    - what is the connection between graphs, networks, and interface structures
      - the 'graph' concept of 'common edges' or 'common connections' indicates that its useful for depicting 'overlapping shapes' (like cycles, pentagons, etc) by constructing them out of 'core change/structure types' (lines & positions) used to create common structures like 'closed shapes'
      - this is another way to depict 'similarity' than 'physical adjacence on an euclidean graph' or 'number of connections between points in a network', the graph structure using a similarity structure of 'edges in common' as a core component of similarity, and offers another way to depict 'change combinations' as opposed to the network 'multiplication/addition matrix operations', the graph structure using a 'sequential connection and common edge' change combination type
      - the graph retains meaning of the 'angles' of the edges (it means something if a structure has four or five sides in the graph, it means it takes four or five common sides or steps to create a shape), where the network doesnt ascribe meaning to that info (angles of connections are just for display purposes)

    - a question like this requires a basic 'map one system/function into another system/function' function, first identifying the function (double) creating the difference between the solution metric (count) at an event and the previous event, then re-applying that function until all events (fifth to first) are connected
      - 'At each event last week, Jenny made twice as many shots as she made at the previous event. At her fifth event she made x shots. How many shots did she make at the first event?'
      - this requires identifying problem/solution structures
        - the problem inputs: the structures already connected (ratio of event count and previous event count)
        - the problem to solve: the structures to connect (fifth event count and first event count) in the solution (the solution connects these differences)
        - the solution format: connections between the differences to connect, specifically the final connection (connecting function applied to the 2nd and 1st event)
          - approximate solutions: the 2nd to last would be almost correct, as it would be closer to the final solution than the problem inputs (the 48 event count of the 5th event), so that would be an approximate solution if there was some barrier preventing the last calculation - same for each previous event count that would be increasingly less correct
        - the available functions to use in connecting the problem inputs with the solution format: apply 'double' to the event count, apply sequences of 'double the event count' function (multiple 'double the event count' function calls)
      - alternatively, identifying the following would be sufficient for some/most problems
        - identifying that multiplication is a form of addition, and involves a 'set/group/combination' of some item (a combination of y x-values), and that multiplication is relevant for many problems as 'change combinations' are a powerful & common structure, and that there are multiple ways to apply multiplication
        - identifying that repeating some structures (addition, sequences of multiple additions) is often a useful solution-finding method
          - similarly, identifying that applying some adjacent changes is often a useful solution-finding method
            - abstractly, identifying that applying some functions is more useful than others when finding solutions or finding methods to find solutions
            - relevantly, identifying that applying some functions is inferrable from some terms used like 'multiple' or 'twice' indicating the intent of the problem statement ('find the factor used as input to the multiply function')
            - specifically, identifying that some structures are more useful than others across problems, like 'find connections', 'find unconnected structures that should be connected', 'find connection functions', 'find variables of connection functions'
      - alternatively, deriving other solution-finding workflows which are less adjacent is also possible from this type of 'problem statement + solution' data
        - identifying that the test of the solution is 'is this value the count at the first event' and the available data is 'the fifth event count', so the 'first' is a variable that should be changed to work backwards from the solution metric to the inputs (derive the solution-finding method of 'reverse-engineering' by applying the solution metric/test/requirement as a structure to derive the connection to problem inputs from) and therefore these structures should be connected (by 'finding a connection function that changes the value of this variable, the event number')
      - alternatively, identifying language patterns such as how some specific examples of a concept (twice being an example of multiply) are used instead for convenience/simplicity/other priority, and that language is a map to refer to other objects to fulfill intents like 'minimize work to communicate differences', and multiple 'definition routes' and 'examples' exist of a particular concept, which may be used for different intents/causes
        - 'twice' being mapped to the 'multiplication by 2' operation or the 'multiply' and '2' concepts or the 'multiply' and 'number' concepts with a '2' specification
      - alternatively, identifying useful functions to apply across problems like 'map one system to another' as a common structure that can be used to solve many problems, these 'connections between functions' being useful building blocks for 'connections of problem/solution differences'
      - useful structures for solving a problem like 'find x: x/4 = 2/8' include a structure like '|--|' where the two sides represent the two connected variables (the values being divided) and the connection between them (the ratios should be equal)
          x   2
          |---|
          4   8
        - this is like an interface (which is a core structure of 'compare/connect/differentiate/equalize') where the 'equal' connection indicating the foundational similarity between x/4 and 2/8 is the connecting horizontal line (and the foundation for other changes) and the two ratios are the sub-interfaces based on that interface that connects them (after applying an interface as a standard to compare each set such as x and 4)
        - more complex structures can indicate the logic to solve other problems (problems being 'differences to connect/equate') such as more embedded variables (like a variable connection in the ratio, such as '-|--|-', like if one side had a power in its ratio)
        - these interface structures are useful for guiding logic to find what values should be equated, and if variables cant be connected with such interface structures, they are unlikely to be relevant, accurate, or otherwise useful
        - this structure is useful to determine what connections in a network should be collapsed into a layer/level/hub structure to indicate 'embedded variables on an interface variable'

    - add to structures
      - whether a 'random walk' around a network of 'randomized maximally different scalar values' (or other 'sequences of random sets') as input variable coefficients would add efficiency in generating easily filterable maximally different possible prediction functions (or whether it removes usefulness by randomizing adjacence) is determined by how different the differences are that need to be modeled (how uncertain it is whether a function is correct), as each step in the network to 'different value sets' is likely to produce an extremely different set from those already tried
      - example of intent-structure prediction & sequential evolution in the intent-structure space:
        - in the intent-structure space, changes in variables (like 'adding more to an existing structure') are useful for intents like 'fill a structure' or 'hide a structure' when they fulfill requirements (conditional requirements like size/position to 'hide a structure', or functions used to fulfill requirements like 'adding more until its enough to fill a structure', or state changes from functions used to fulfill requirements like the change from an error to a solution state of 'enough', which is a concept in a requirement of 'fill')
        - these intents can be calculated by applying differences (like size/position or difference in shape variables like concavity/openness/completeness or using interface structures like boundaries/components) to create other differences (position one structure in front of another, or resolve the difference between an empty/full structure) that are commonly useful
        - this uses the 'core interaction function' between a 'component' and its 'container' to derive an intent 'fill' and a core interaction function of 'obscure' (or 'randomize') of agent-level structures like permissions
        - this answers the questions 'as you change a variable (add more structure to) of a structure, what intents are fulfilled by the new structure' and 'to fulfill these intents, what variable of what structure should be changed'
        - 'more volume of a shape' is the connecting interim structure in between the original structure and the useful functions fulfilled by that structure
      - if a polynomial is changed to have a slightly higher peak in one of its peaks, can the difference in area be determined by shapes which have a reason to be 'composable elements' of the function, such as an arc of a circle being relevant bc it adds curvature, and the arc formula being relevant to the difference in area between the peak lines as it describes much of the area between them, requiring vacillations on this arc function in the form of smaller changes in differing directions (rather than a consistent rotation direction), explaining the deviations in shape of the differing area from the 'filled arc sub-section of a circle' since the peaks are 'not circles but are similar to circles' and their differences are likely to be described by other 'shapes that are not circles but are similar to circles', this 'difference in area' between these polynomials being likely to preserve some attributes/components of the definition of the functions being compared, the case where the polynomial is shifted being an exception where it is coincidentally possible to describe the difference with a constant indicating almost perfect equality
        - the 'reason for the difference between a line and a curve' represented by the use of the circle/arc as a composable element of the wave functions and their differences is relevant bc of the lack of differences in that 'curvature' attribute produced by the change in peak height
        - the 'vacillations' around a larger structure which can fill in this 'differing area between polynomials' align with common structures like 'sequences' and 'fractals', which are similarly relevant to these polynomial functions as they are to circles
        - this relates to the question of 'is there a way to calculate a method of determining differences in some function set using definitions of the functions or their elements/inputs or descriptive attributes'
          - 'find a similar structure to the structures to differentiate' (circle, specifically an arc)
          - 'find differences based on that similar structure (slope changes from an arc)'
          - 'find differences in those differences (exponential changes in most cases)'
          - 'find similar structures as those differences in differences (incremental changes)'
          - 'find similar structures as those similar structures (sequences)'
          - 'find similar structures (circle components like arcs) to use as inputs to those structures (sequences)'
          - 'find connecting structures (rotations, scaling) of those similar structures (arcs) used as inputs to those structures (sequences)'
        - this general function of 'find differences, find similarities in differences, find connection between those differences/similarities and original position' is a particularly useful function bc it implements a core workflow 'connect problem/solution differences by applying interfaces (standardizing similarities)'
        - the reason this is useful is that the 'radius of the circle' (input of the circle) describing the difference in areas will be useful in determining the difference in area, since its useful in determining area under either function in isolation, as a 'similar structure'
      - find the common structure of 'structures that semi-accurately describe (approximate) each other, with an overlap of their structures in some ratio, but enough difference to be irreducible to the other' like attribute/concept/function networks, which may partly describe each other but which cant be easily embedded as a subset of the other and overlap significantly
      - what do networks (connection sets indicating connection by absolute/relative position), functions (connection sets with inputs/outputs), trees (directed networks), maps/indexes (example input/output connections standardized by direction), interfaces (change bases), vector clouds (change sets to reflect similarity by structures like angle), and data sets (differences in attribute sets, grouped by input/output attributes) have in common
        - they are similarly useful alternate info-preserving structures, which are useful in different cases with different input info
        - how would you implement a filter function using these structures:
          - identify prioritized values to find (can be identified using general rules like 'find unique structures' or some specific rule like 'having an attribute value')
          - find map having values which are prioritized to find
            - select subset of that map with relevant prioritized values
              - find attribute network or subset of relevant attributes of inputs (indicating relevant info about the input, like metadata such as similarity/containment to map subset values)
                - find overlap of (relevant map subset) and (attribute network of inputs)
          - this is an example of how to write a function that filters some input, using structure queries (on the structural interface)
          - this is an alternative to writing a specific function that defines a specific map like a dictionary, then applying some specific function to that dictionary map or to the input being iterated/filtered to find the attribute to check for equivalence/similarity/difference, all of which can be automated (found/generated/derived) and optimized (to reduce steps to compute it, such as identifying commonly reused dictionaries and storing those)
          - given the intent of the function such as 'find a difference to filter this input', 'differentiating filters' are the search space of possible maps/subsets to apply, and the possible attributes of the input that can differentiate the inputs, attributes which can be identified/generated automatically
          - an alternative interface query would be to find the attribute network first, then find the most differentiating filters of these attributes which are likely or known to be useful for the function intent
        - 'finding maps' could also be a generative function such as 'find values which are frequently converted into each other' by finding 'frequently applied functions resulting in an object of the filterable type'
      - interfaces are a 'structure of independence', so when variables like speed/spin change in relation to each other, indicating dependence, that implies they are variables on an interface, rather than being independent
      - its useful to think of a set of probable or alternate functions as a 'function cluster', which highlights the usefulness of alternative similar functions from a different direction, similar to how data points might cluster around a type center
      - the important implementation structures of machine learning include a function network (more like a circular loop of a directed network, as in a multi-layer spiral of a set of trees where each layer is an iteration in the loop), sequences of change combinations, propagation functions between nodes in the sequence, partial differential equations, error function, weight update function, & functions with thresholds (the s-curve) to model a filter to stop changes from propagating - without all of these structures, the ml invention wouldnt be effective
        - deriving the intents of 'find increase to a multi-dimensional change from a change in one dimension' requires identifying the 'aggregate, interactive, compounding, overlapping, duplicate' nature of these changes as being difficult to trace to a particular node/weight structure, and also useful to trace to that node/weight structure (the changes that improve the error can be reduced to a subset of the nodes, meaning fewer computations are required once the model is trained, as the nodes/weight structures have been filtered for error-improving changes using deactivation, rather than applying all of them), so that change sources can be isolated/filtered from other change types
        - deriving the propagation and weight update functions reflects an understanding of the intent/requirement to 'filter the search space of possible change combinations that could represent a solution (prediction function)'
          - a 'random weight generator' indicating extreme lack of information/assumptions/priorities
          - a 'maximally different abstract (unique function type) weight generator' indicating a priority of finding a good solution base to apply more specific and accuracy-improving changes to
          - an 'iterative adjacent weight update function' indicating an assumption that adjacent changes are probably correct (error function minima), which would apply if a previous network has been applied to identify 'good solution base functions'
          - a 'same-layer communication function' indicating that the usefulness/priority of the intent to 'avoid unnecessary computations, such as weight changes/node activations that would increase the output above a value already identified as not the minima'
        - deriving PDE's
          - when changing a cube of side 1 (with volume 1) to a cube of side 2 (with volume 8), to find the contribution of each variable change (2 - 1 = 1) to the output change (8 - 1 = 7)
            - 1 + 2 + 4 = (4 + 4 + 4) - (1 + 1 + 1) - (2)
            - because a change in x of 1 happened, when a change in y of 1 also happened, a change in area of 2 happened instead of 1 (the completing third extra area in the opposing corner compared to the original), (2 - 1 = 1)
            - because a change in x of 1 happened, when a change in z of 1 also happened, a change in area of 2 happened instead of 1 (the completing third extra area in the opposing corner compared to the original), (2 - 1 = 1)
            - because a change in x of 1 happened, when a change in volume of 2 1/3 (7/3) happened, because x contributed to the extra area of increase in y (2 - 1) and the extra area of increase in z (2 - 1) and the third (1 cube / 3-dimensions) of the opposing corner cube
          - when changing a cube of side 2 (with volume 8) to a cube of side 3 (with volume 27), to find the contribution of each variable change (3 - 2 = 1) to the output change (27 - 8 = 19)
            - 4 + 6 + 9 = (9 + 9 + 9) - (2 + 2 + 2) - (2)
            - the first difference by increasing one side from 2 to 3 is adding an area of 4
            - the second difference by increasing a second side from 2 to 3 is adding an area of 6
            - the third difference by increasing a third side from 2 to 3 is adding an area of 9
            - the difference in area in each dimension is a square of area 9
            - each dimension is responsible for 6 1/3 (19/3) increase in volume (x square of 2 x 2 + 2 in the third dimension z, y square of 2 x 2 + 2 in the third dimension z, 2 + 2 + 2 of the 2-dimensional overlap sections, final opposing corner / 3 given the three dimensions of overlap)
            - an increase of one dimension to 4 instead of 3 would result in another 3 x 3 square added to the total area, 2 x 2 being attributable exclusively to the increase from 3 to 4, 2 being attributable to another dimension and 2 being attributable to the third dimension, the corner of overlap being attributable to all three dimension increases
            - the overlaps in two dimensions (which dont coincide with another overlap, as in the opposite corner from the original cube in the area of growth) are the top side of total length 3 - triple-overlapped cube of area 1 = 2 value to remove
              - this 2-dimension overlap occurs 3 times (-(2 + 2 + 2)) which should each be removed once (2-dimensions of overlap - 1) so that each overlap is only counted once
            - the triple-overlapped cube of area 1 should be removed twice (3-dimensions of overlap - 1) (-(2 * 1)) so that its only counted once
            - the overlaps are removed to avoid counting a value multiple times
            - these differences are 'structures of incompleteness' to 'complete a shape (like a square)' to avoid a structure with a concavity (a cube missing a corner)
            - partial credit should be assigned to relevant dimensions in the overlaps, and all other increases in a direction (in volume) can be attributed exclusively to the change in that direction
          - why is there a connection between the 'structure with base changes applied (like a tensor of squares joined at the original area)' and the 'final volume of a cube' (achieved by rotating the tensor of square)
            - this also has an 'overlap removal' error in most cases except by coincidence, but is useful to depicting the volume positioned at the opposing corner as a base (the determining point of the cube) to make these overlaps and the differences between the two sub-cubes of the final cube more clear
          - its important to connect the 'variable value changes' with the 'change in determining points'
            - its nice to know the area/volume of the resulting object once the changes are applied using some combination of addition/subtraction as shown above, but how does this relate to the determining point (the new point farthest away from the origin indicating the new opposite corner of the cube, which determines its volume)
              - the determining point such as (2,2,2) or (3,3,3) representing the opopsing corner of the new volume is useful for multiplication (3 x 3 x 3 - 2 x 2 x 2) rather than the analysis above
          - removing the scale of each change to reduce each change to a unit is useful for identifying credit units of a particular variable (how much would a unit change in this variable contribute to a volume) which can be scaled once found
          - its useful to depict multi-dimensional changes like 'volume changes' as lower-dimensional changes like 'slope' using alternate structures like 'stack/fold' change types in 2-d, rather than always depicting them in their original dimensions (3-d volume for a 3-dimensional change)
            - so that 2 ^ 3 would be depicted as 'two stacked squares of 2 x 2' and 3 ^ 3 would be depicted as 'three stacked squares of 3 x 3'
            - this would make it clear how the change in volume is exponential rather than constant
          - first deriving the 'slope' formula would be the most useful as a first step (a ratio of changes in variables) to identify the 'unit case'
            - identifying that the 'slope' is important bc its a unit of a ratio, as in one change compared to another (how much one variable changed bc of a change in another)
            - identifying that the 'slope' of a function like 2x is 2 is trivial with adjacent inputs/outputs
            - identifying that the derivative is a multiplier of the power and a one-unit of decrease in the power is less trivial but is adjacently possible with enough examples to rule out other functions
              - given that this formula involves 'reducing the power' and the goal is to describe the lower-dimensional slope of a lower-dimensional function (a tangent), this isnt extremely difficult or complicated to derive
              - reducing the power by one and applying the original power as a multiplier is less trivial but still adjacently possible with a few cases adjacent to the unit case
      - the structure of self-reference (recursion) is useful to portray with the 'system layer diagram' of 'concentric circles representing different interaction levels' bc this allows for outer levels to receive and integrate feedback and also store the core structure of a circle (representing a different interaction level or interface) at each layer, as a buffer between the output of the internal system and interactions with external structures
      - 'adjacencies of limits (limits like boundaries)' such as 'surfaces' and 'adjacencies of surfaces (like receptors)' like 'interactive structures with the surface (like binders)' are important bc they have an embedded sequence that makes some of them more important/causative than other variables, as if a surface is interacted with first, it may become more important than the limit, because of this 'sequence of interactivity' that leads to 'causativity'
      - 'limits on a network output' (like a 'decision to move forward' or a particular type label) can be embedded in a network as 'weight path-limiting/stopping' structures to stop a particular input from giving the positive answer
        - what a variable is (its variables) and is not (its limits) should be reflected in the network weight structures, meaning these weight structures can be partiallly derived from knowing its limits
        - this is useful when a limit is likely to be an example of a type of limit, so embedding it in the network weight structures, rather than in preprocessing, might act like a more effective abstract filter
        - what structures are applied as certainty structures like constants (like indexed information, such as indexing the connection between intents/functions) and which are applied as variables can also be embedded in this way, bc constants act as limits
          - these certainty vs. uncertainty structures are another variable of problem-solving structures (on the probability & information interfaces), which can be applied rather than applying other interaction level variables like cause/intent/function/change
          - structures that occur across interfaces (which are therefore likely more powerful and more common) are likelier to be useful to vary and are likelier to vary more
          - storing this information involves a task of deriving less info (figuring out what something is and is not may be less work than figuring out what something is and what something else is, which applies 'determining' structures)
          - 'limit' info (such as 'probable limit ranges' and 'skippable limits') is also potentially useful as a proxy for 'core output change combination' info (like 'forward/sideways/position' change and 'direction' change and 'speed' change) as in how knowing a 'probable function range' is similarly useful as 'identifying a particular prediction function that is extremely likely if not guaranteed to be partly incorrect', so finding limits is a useful intent for neural networks (with functions like 'difference-maximizing functions' and 'functions to find approximate limits quickly by vacillating around a probable limit value' and 'functions to stop vacillating when a possible infinite increase occurs and check for signs of an infinite increase')
            - identifying at what input/output position a variable's causativity can be used in isolation of variables elsewhere in the sequence is useful for lowering the dimensionality of inputs and identifying useful interfaces like 'human traffic law change patterns (like patterns to integrate feedback)' to identify causal structures like loops (where a common error results in a different road/sign structure) so that ai can avoid violating future traffic laws by generating these common errors & fixes in advance
            - similarly 'skippable change combinations' (such as 'interface-invalidating changes like changes that would be too destructive' or 'changes which are too different like discrete changes in an otherwise continuous function') are useful to identify
            - similarly 'optional limits' such as 'limits that are frequently useful for preventing errors but which are optional' will have different adjacent change structures like 'vacillating around the optional limit' ('trying to avoid tickets or errors from excess speed') and 'extreme changes violating the limit' ('speeding in a car chase') as violating an optional limit that is frequently obeyed is likely to have an extreme reason/intent for violation (a case where violation makes sense or is required, such as in antagonistic drivers who are trying to create an error type of 'crashing into the car ahead', as some error types serve the interests of drivers such as extreme antagonists) that differs from change types such as 'trying to avoid violating the limit which will have vacillation change patterns', this 'extremity' of the reason (input) for the error type of 'violation' being likely to cause similar 'extremity' in the error structure ('extreme difference' in speed in violation of the optional limit)
            - 'filtering out (skipping)' these change types is based on probability, requirement, error type interactions, error-causing intents (the non-absolutely negative nature of some errors as being beneficial in some cases), and other useful attributes
            - given that errors arent always required to be negative, this attribute should be associated but not defined as equivalent to 'errors'
          - structures that limit more change types (forward and side motion) and default motion (like forward), should ideally be identified first/better (prioritized), as these are less likely to respond to possible/probable fixes (changing the direction at a speed), as well as variables that enhance the power of limits (like speed and direction change potential at speed)
      - 'accurate data sets' will mimic other accurate data sets, so 'data set similarity or commonness' is a metric for how accurate a data set is, which can be tested by substituting a sample for another similar data set, to check for anomalies by testing a pre-trained model on another similar data set
      - 'intent interaction patterns' such as 'how intents typically develop from a causative intent' (what intents typically follow from a particular intent) is useful in predicting 'requirements' such as 'functions likely to develop to fulfill requirements in a system, given the intents that are known to be causative'
        - this is useful for intents like 'find a point where a function would develop on its own'
        - predicting 'intent sequences' is useful bc this is an alternative proxy of 'understanding', such as 'understanding variable interactions', which is how knowing how 'intents emerge from other intents' is useful in predicting 'variable interactions' in general, similar to how knowing 'change sequences' or 'input output sequences' is useful for predicting 'variable interactions'
        - for example, a common intent sequence is 'build a foundation' and 'build structures on top of the foundation' bc the 'foundation' structure is a requirement of the second intent, similarly 'correct error' follows 'check for error'
        - this is also useful for identifying where functions/intents/changes in the sequence can be skipped, such as where 'common error structures occur' from a particular cause, so checking for these errors can be skipped and the correction can be applied directly in cases where it wont cause a change if the error isnt present, which allow for the 'check for error' step to be skipped
        - other intents adjacently fulfilled with this structure include 'identify alternate sequences with similar/equivalent input/output'
        - what could be similarly useful as 'input/output or change or function or intent sequences/networks/trees', given the extreme info preservation even in their patterns/summaries and accessibility of this info? it seems impossible to come up with something as useful as these
          - 'determining' structures are an example of a 'similarly useful' structure as these change/function/intent structures
          - how would you identify 'determining' structures as similarly useful? 
            - applying useful structures (or related structures to functions/change/intent) like requirements (like 'info preservation' or 'inputs to common required functions like derive')
            - applying changes to 'function/intent/change' structures (what is not a 'function/intent/change' structure, such as an abstract pattern of these structures, or a generative function of these structures, or a requirement/limit of these structures)
              - then determining variables of these changes to generate alternatives on the same interaction level ('determining' structures being a step away from 'generative' structures, 'identifying' structures, 'summary' structures, etc)
            - identifying useful structures like 'skippable structures' such as how 'some computations can be skipped in some cases' and identifying functions to fulfill those and the relevant cases
            - identifying core structures like 'limits' which a subtype of 'determining' structures, and abstracting this structure to get the types that describe it
      - example of formatting a non-linear problem as a linear problem using dimension-reducing functions
        - applying strategies like 'finding the variables that determine or restrict the others to one possible value' and 'summary metrics like the difference between values in a variable set, rather than the complete variable value sets' can reduce a non-linear problem into a linear problem, for example the 'towers of hanoi', which can be modeled linearly without mapping categorical variables to numerical values (like mapping tower position to y-axis) by applying these strategies (mapping the slope of the diagonal difference between two of the most different position changes, since the third position change is determined by the others, as some changes like 'switch tower' cant occur if the position isnt the 'top' position, so this 'summary relative position of the most different positions' is a 'change-limiting variable' on the 'third position change potential', so these 'position change potentials of the third variable' can be derived by other changes)
          - these dimension-lowering functions may lose some detailed information but still retain representative statistics like 'change or change type sequences (patterns)' of the info
      - what are variables of useful structures? the variables of usefulness like 'minimizing work' such as 'stored future work'
        - for example, useful structures would include 'intersections between different systems' bc its useful in the sense of having 'pre-computed a step of a common function such as "translate to a different system"', 'pre-computation' being useful for already having done future work, and 'commonness' of the function adding to the usefulness of 'pre-computing' it
        - the common theme of useful structures is that they generate differences that are likely to create differences in structures that are useful for being more measurable, likely to be interacted with, likely to cause other structures, etc (these are differences likely to be required to solve a problem, as in identifying information that 'connects extremely different information' by making some information less different and more unique as in more obvious/measurable, interactive, causative, etc)
          - these 'uniqueness' (maximally different) structures are useful across problems
        - 'structures that can be maximally repeated without contradicting other known structures' is a good starting point for finding structures that are likely to be true
        - structures that are similar & different in useful ways:
          - in between 'maximally different' and 'trivially similar so as to be equal' structures, there is a useful interim space of structures that have some similarities and differences, such as useful inputs that coordinate with a function (a set of 'triangles' as inputs to a 'combine' function to create a square, where the triangles are just different enough from the function 'a need for a square' and just similar enough to the function 'a definition of a square that is almost a square' to be useful, as these structures of a 'structure' and a 'need' are interactive in this similarity and difference, and similar to how a function that is known/resolved at some subsets but unknown/unresolved at other subsets is useful for data sets that can be checked trivially at those known subsets where it is useful to preserve uncertainty at the unknown subsets)
        - 'non-embeddable variables' which cant be adjacently standardized/aligned/embedded like functions/networks (cross-interface structures which are not equal in any adjacent transform) is another useful structure for generating useful differences like 'maximally different functions', for example how a physical structural interface variable like 'color' doesnt automatically embed in any variable like 'causal degree', but 'color' can be embedded easily in other physical structures and is even required on some interfaces like the physical structural interface
          - 'functions' and 'networks' are useful in being 'maximally different structures', similar to how 'concepts' and 'cause' are useful for the maximal differences they describe, which can be a guiding principle in filtering possible interface queries to apply
          - they can be embedded with enough work but are different enough to require non-adjacent work to embed, and can overlap due to their ability to capture a high degree of variation, 'embedding a function' being so complicated it could easily de-stabilize the network, given the intents it can add to the network intents
          - another non-embeddable structure is an 'equivalent alternate' like an alternate network that cant be embedded in another network bc theyre equivalent in some way, like describing significantly different variables, requiring them to stay isolated rather than being embedded
          - other non-embeddable structures include invalidation structure that would invalidate the structure of the variable used to embed the other
      - what functions can generate similarly useful information?
        - a 'test' function, a 'change' function, and a 'generate' or 'select' function can form a basis for most solution-finding methods, but other function sets/sequences (like 'standardize', 'compare', 'connect' or 'differentiate' and 'filter') can also achieve this, bc the other function sets can also determine useful information like 'information that is robust to changes' or 'information that is different/similar' or 'information that fits together in structures like a sequence' or other information interactions
        - similarly, some solution metrics are more useful than others - such as how 'information that is non-trivial to fake/change/break' is likelier to be true (and useful) (a solution can be considered more optimal if its difficult to break it by applying changes, or difficult to change it or fake it), so finding information interactions that are difficult to fake/change/break are a good starting point for possible solution components, as a good solution is robust and using the truth is likelier to make it stable/robust
      - what functions arent adjacent to the existing functions in the logic? (beyond biological agent functions like 'trade', 'fool', 'simulate', 'teach', 'lead')
        - why is identifying unique functions useful?
          - these agent functions can be translated to other interfaces where its more obvious how theyre useful, like how 'trade' can be applied to 'solutions to different problems' to 'trade components of solutions' to differentiate a solution in a way that is likelier than randomness to be useful for solving a problem
      - what functions are useful when used together?
        - example: 'reduce' can be applied to the problem space system to describe the system in fewer variables (lower dimensional space), which makes other functions like 'organize' require less work
        - 'system' is a 'combination' or 'interaction' interface uses a core structure of a 'network' which depicts a core attribute of functions/objects ('interactivity') and correlates with 'meaning' in that 'meaning' indicates how everything integrates/connects (applying an 'absolute' system or context), and a system refers to a specific subset of connections like a 'physics' or 'math' or 'type' system
          - 'potential' refers to a useful interface of difference/similarity in its application of objects like probabilities, adjacencies, risks, etc, and the 'potential' interface could be said to have a 'field' or 'network' structure, indicating probable alternate states, as well as 'boundaries' indicating limitations/impossibilities, these objects being useful for assessing equivalence/difference, as they connect 'potential' to the 'change' interface related to the 'function' interface
        - 'change', 'function', 'intent', 'cause', 'pattern' all have a sequence/direction structure in common, and are therefore components of these combination interfaces ('network', 'meaning', 'system')
        - 'structure' indicates the core attribute allowing anything to be clear/visible
        - 'interactivity' being useful in its similarity to a 'symmetry', in that it is similar to another structure but different in a useful way, such as 'complementary structures' like a 'function input' is complementary to the "first layer of a function's logic" (the structures of the input & first layer are similar in that they fit together, but are different in the way they fit)
      - useful variable structures
        - identifying useful structures like 'where computations can be skipped' is useful to apply with each structure's opposing structures like 'where computations cant be skipped' (such as where a sequence of previous skips has already been applied and is approaching an error threshold)
          - identifying 'structures with no possible opposite' (also a 'unique' structure on some interaction level) is useful as a filter of structures that are 'possible interfaces (or the structural version of "symmetries")', meaning there is no interface that can connect this structure with an opposite structure, such as how 'zero' has no possible opposite variable such as 'all (the infinity of all infinities)' that could be an input to a possibly knowable useful (efficient) function (bc controlling 'all structures' is impossible for any known function that could possibly be useful except a 'function of all known changes in the universe' which is unlikely to be found, far likelier is it that a universe is approximately 'an interface where changes accrete indefinitely in whatever ways can stabilize' rather than a 'describable structure within human knowability/measurability', so such a function may as well be required not to exist, and an agent within a system of such indescribability is unlikely to be able to conceive of the complete system bc of an invalidating recursive self-reference problem in a non-fractal structure), without deriving other non-adjacently computable interaction levels like 'universes of infinities of infinities'
        - identifying structures that are useful for identifying false/irrelevant/random data when 'fit' or otherwise applied to a data set is useful as a quick way of identifying which data points are important, such as if some 'change structure' like a 'large structure' or a 'structure with right angles' can be fit into a data subset (a 'large square' can be fit in between adjacent data subsets, or a square can be fitted to the outer edges of a data subset), that data subset likely to be random data or distorted in some way, bc these structures are 'simple structures (right angles being simpler than curves)' or 'difference structures (large gaps in data being differences indicating an error)' that are unlikely to occur in realistic systems, and 'simplicity' and 'irrelevant differences' (differences in adjacent data subsets having no derivable reason to exist, like a causal change differentiating them or a barrier structure) are structures of falsehood that can be applied in specific structures (specific structures with some attributes) that can filter out irrelevant data across alternate data sets (particularly in problems involving complex systems, where these structures are less likely by definition, so identifying the structures like concepts/attributes/structures that are less likely to exist is possible with information about the system)
        - applying 'difference-maximizing vector clouds' as a way of generating adjacent differences in a data set is a useful way of determining the more robust/stronger variables that resist changes and still hold to maintain the same patterns
          - identifying 'stronger variables' is useful to determine the 'certainty' structures as well as the more 'powerful variables' which are likelier to be causative in their robustness/consistency which may act like or be a 'requirement' structure
            - this intent can be generated by the application of 'opposite' to 'interface' structures like 'change' (in the interface query 'what is the opposite of change'), as a fundamental useful structure to identify and use when identifying change structures like variables
            - similarly 'what is the opposite of a pattern/intent/definition' (structures like randomness/defaults or incentives/obscurity, impossibility, or uncertainty) are other alternative questions to apply when finding useful structures
        - the problem-solving intent of 'removing insignificant repetitions' is more trivial when 'interchangeable alternates' are identified/identifiable, to 'skip equivalents to another known structure'
          - identifying 'significant repetitions' may be more trivial to compute, such as how a 'backup copy' is a 'significant repetition' in an error case that should not be removed from an optimized system for some solution metric incentivizing/requiring backups, or how a 'repetition' of a component may be used as a 'basis for change' to create other structures
            - similar to how identifying 'changes or change components' in a subset of a data set can be used to identify 'repetitions' where those same components/changes are re-used to generate other changes in other subsets (like how a continuous function is likelier than a mixed function of continuous/discrete subsets, so identifying 'connected adjacent changes' as a repeatable component to describe the other subsets of a function is useful, similar to how rotating/shifting/scaling an arc of a curve is likelier than random changes to generate the rest of the curve)
          - identifying sequences of steps that lead to randomness is another useful structure to filter possible structures to combine, as 'randomness' removes information rather than creating/clarifying information
        - identifying where 'barriers to structures' are 'stronger' than 'structures' is useful to identify when its likelier that a structure will be prevented from occurring than it is to occur, to filter the set of possible structures, 'strength' indicated by some useful structure like 'power', 'efficiency', 'interactivity', 'requirement', 'stability/consistency', etc
        - useful variables are 'position' and 'count' bc most variables can be standardized to these variables ('removing insignificant repetitions' as a useful problem-solving intent) to identify significant differences, then re-apply differences to get to a useful interaction level and re-apply position/count in that more useful interaction level, rather than keeping track of all positions/counts of all relevant variables
          - once these variables are removed, the remaining differences will be extremely significant and describing the differences between them will explain the position/count differences
          - for example, standardizing differences in counts of amino acids or electrons identifies the 'common values' of the counts, which can be separated into 'types' of components building other useful structures, like 'component sequences' and 'sequences of component sequences', which can retain their 'position' variable values bc there are fewer variables to preserve in the set of 'sequences of components' as opposed to the set of 'components'
        - finding the maximum error structures (like extreme over-prioritizations) to avoid is useful to find 'limits' and 'sequences leading to limits' (like 'painting into a corner' being an error of not recognizing 'irreversible steps' and 'structures like corners that can create irreversible steps' that should be done last in a sequence, next to a variable like a door, otherwise a 'dead end' error is reached requiring the painter to stop moving), to identify the 'starting points of these sequences leading to these limits' like the "steps where the 'dead end' is still avoidable" and the 'steps right before that point where that avoidability is visible', as useful to identify and trivial to identify once the over-prioritization and other error structures of a problem space are known (applying common error structures like 'irreversibilities' of a space)
          - these useful structures of 'steps where an error is trivially computed' and where an 'error can still be avoided' and where an 'error cant be reversed' and where an 'error cant be converted into a useful structure (the info about the reversibility is lost and cant be used to improve other parallel/future computations)' are coordinating structures that are useful in that theyre often found adjacently or in some other structure of organization
        - identifying 'change combination sequences' that create specific errors like 'differences that standardize significant differences rather than differentiate them in some/most cases' like an 'image filter that removes significant features in some cases' can be avoided in the weight sequences of the model during or after training
        - a variable might have a cursory correlation with another bc its a 'side effect variable' rather than a 'cause variable', as in an output of a sequence which is more influenceable may coincidentally correlate with other variables and may be more measurable but the causal variable would reflect the actual correlation
          - this is useful in cases where a variable varies across alternate data sets, indicating its less likely to be a causal (and less interactive) variable (like genes) and more likely to be an output/side effect variable which is more interactive with other variables (like skin tone)
          - interactive variables are likelier to vary for irrelevant reasons and can be excluded from data sets more as theyll reflect randomness in their highly connected interactions with other variables
        - alternate function formats like an 'intersecting line of a prediction function + some vectors indicating directions/rates of change' may be more trivial to identify than 'identifying the prediction function' (if multiple functions are possible to derive from this input format) or 'identifying the prediction function with standard methods', such as finding the 'directions/rates of change at points in regular intervals of the data set' and 'connecting them into a line + vectors' or identifying filters of determining intersecting lines + change direction/rate vectors, similar to how identifying cross-sections of a complex shape like a coast/mountain is useful if done at regular intervals or if paired with change rate/direction vectors at the intersecting points, and applying these operations at regular intervals rather than at every point is more trivial
          - this is similar to how knowing the 'general shape of the structure + a fractal function to identify probable local specifications to modify the general shape' is more useful than identifying every exact point value, applying the 'regularity of the interval' as a 'generalization' structure instead of the 'generalization of the complete shape of the data set'
        - a 'finite fractal sequence that approximates the sum' is very useful compared to the full sequence unless its sum happens to be trivial to compute, to apply as a connecting structure to connect 'general and specific structures' like a 'general data set shape' and the 'specific prediction function', a set of these 'finite fractal approximation sequences' being similarly useful as a set of 'example data sets that describe most change types in complex systems which are non-trivial to predict', as a way of specifying general structures like general base solutions or approximate solutions
        - 'lines that overlap at various intervals and for various sequences' are an alternate solution format compared to 'probable function ranges' and 'one prediction function', which apply common certainty structures likely to be correct as sequences to connect with more variable subsets
        - categorical variables (like a material name) are more useful when mapped into the numerical variables associated with them (like electron or carbon count or chromosome type/count) bc the categorical variables almost always hide information and the only reason to use them is convenience for human interpretation
        - compare the relative simplicity of a standard neural network (consisting of 50 - 200 words to describe its logic & object identities) compared to the search space generated by core components ('letters') and useful interaction structures ('letter combinations' and 'letter combination sequences') differentiates it from code bases (as 'function sequences/trees/networks') as a useful structure to find in the search space of 'possible sentences/paragraphs describing a solution to avoid searching all possible word combinations', similarly some high-level functions like 'organize' can be defined with few words (including their important alternate definition routes) but offer a relatively high degree of variation-capturing/compression, and the similarly relative simplicity of other structures like the 'set of common error or solution structures & sequences surrounding them' or the 'maximally different but still continuous connectible shape' to find & retain info about the most changes
        - 'fractal/recursive filter sequences' are useful in 'specification of trivial changes applied to a general base' (such as for a subset of a data set likelier to be describable with a linear function than not), and 'connections between these fractal filters' being useful to identify a common interface where those fractal changes can be applied in the same function
          - for example, 'identifying a subset likely to be describable with a linear function' involves 'identifying a general pattern in a data set subset', then specifying this general pattern with increasingly specific changes of the same type
          - the opposite direction of change (finding connections between subset functions) is useful when identifying an interface where adjacent subset functions can be merged/integrated, after starting from a 'local navigation function' to identify important points/patterns/connections from a set of data points (a 'local navigation/filter function' as opposed to 'applying changes to the whole data set to make general patterns more clear')
          - these are common 'input/output sequences' among solutions in the 'find a prediction function' problem space, as these two formats (data set subsets and general data set shapes) are the most adjacent formats of the data set to start from, and starting from either is likely required, in the absence of other useful structures like 'commonly useful filter sequences capable of isolating changes of specific types (such as maximally different function generators & filters)'
          - this is due to the nature of filters requiring increasing specificity when applied in a sequence, given how they are defined (once a subset has been filtered out, the remaining subset needs to be partitioned as well to get the final solution or solution set, requiring more specificity than the initial filter, meaning another filter)
      - general useful intents for regression (and generally finding prediction functions)
        - find changes that often clarify/hide/change a data set's general patterns and apply to a general base solution function (as these are applicable to a representative function as well as the data set)
      - an 'interface' as an interim structure of an 'angle' and an 'intersection', which are core interaction structures
      - an 'attribute' (variable) develops when an attribute like 'count' of a structure changes (such as stacking areas or points with addition/multiplication, which can add another variable like height, or in sequences of operations like addition having properties like associativity/commutativity/transitivity bc there are multiple items in the set, allowing for variable interaction types, like 'output constants of the interaction' where the output is invariant regarding a change of an attribute that emerges from multiple items in a set of operations like 'order of operations' and 'embedding/layer of operations')
        - this is useful for adding/generating new variables (by repeating some structures, varying metadata of a structure that increases interactivity)
        - the 'interactivity' of the points/areas being added/multiplied allows them to interact in a way that has a 'count' variable (interactions such as 'stacking')
      - useful ways to use graphs of graphs
        - for example, if the input of a function gains an attribute like a color incrementally during the function processing, a graph of the function would have a sequential attribute that aligned with the sequential compounding nature of this color attribute
          - how would you graph these two changes (the input/output sequence and the compounding color change sequence) in a way that was separable (in case the color is irrelevant) but also preserves info, using a graph of graphs?
          - you could align the sequences since they already have an alignment, so that the color attribute graph is lower/higher than the other graph, so that its possible to see the color frequency increase according to step in the function input/output sequence by layering the graphs
          - this helps use statistical function-adjacent objects like 'orthogonal variable interaction change functions' in a way that reflects real systems, without resorting to a 'system network of object connections to depict the system being described' which has ambiguities in position/angle meaning and also condenses info to a point where individual variable info is easily lost/hidden
        - interactive/causative variables should be positioned adjacently so their interaction impact is more easily viewed with adjacent changes like rotation
        - similarities between non-adjacent function shapes like an 'increasing line shape' could be identified with this graph of graphs using shapes that are defined to separate such non-adjacent graphs (like 'parallelograms') and the function shapes likely to build those
        - the goal would be to create a graph of graphs where all relevant interface structures like 'interactivities' and 'similarities' are clearly visible with adjacent transforms, where unused/determined output variables are not depicted and other optimizations are applied to avoid depicting non-required info, so that other structures like 'graph clusters indicating a type' can be identified and reduced to a parameterized graph
        - similarly, it should be possible to change a view of an object and see its variables from different angles, if the object is graphed in a useful way (not in its physical system, which has many redundancies and ambiguities which make a realistic system model less useful), but in its actual variable interactions which clearly connect to this system, so the base variables should be graphed in a way that the changes based on this base variable (its outputs) are applied in directions aiming away from the central base, so that all isolatable changes of the object are clearly depicted
        - this structure is a useful interim structure between core structures like functions/networks/graphs that applies a standard of 'direction' to each 'variable set interaction' so that 'aligning changes' are trivial to identify
        - this can involve avoiding depicting some variables like aggregate variables which might reduce clear similarities
      - find maximally different structures (such as lines with 360/n angles to represent n lines, for integer n) by applying known difference-maximizing structures, then find generative structures of those structures
        - applying 'equal distance' between points on the border of a shape like a pentagon would help find these maximally different structures in the 'angle from center' variable, or equivalently finding high-variation interaction structures like 'corners' of the pentagon
        - similarly, applying 'extremes' and 'averages' in one variable to create maximally different connecting functions of 'maximally different points' (such as points at 'extreme values' in another variable), bc these are likeliest to generate maximum differences of a type of structure, such as 'extreme/average-connecting lines' or 'intersection-connecting lines'
        - increasing the variable of the 'intersecting/connecting line/side count until a structure (like point on a border) starts to be repeated/similar' and the application of 'increasing the difference in distance from the center' are changes that increase the differences represented by the 'intersecting lines with equal angles'
          - knowing the number of points that would create the maximum differences in the outputs (points on a shape border) and knowing other useful variables to apply like distance from center (line length) is useful as a generative structure of these 'maximum differences', as these variables can be relatively independent in a structure like 'lines extended from a center' and can therefore add more differences
          - similarly info about what variables can be embedded is useful to maximize the differences within foundational variables
      - methods of finding 'prediction function' in a more efficient way than regression at intents like 'representing/approximating the function', when applied to a subset of the data set
        - calculating shapes with pre-determined or more easily computed metrics like averages/centers is often trivial, like how a parabola can be easily approximated with a triangle, where the triangle's center is more easily computed once its known that its a triangle
        - determining if 'two structures are equal' is easier when you already have a near/exact equivalence of structures or extremely different structures, so generating extremely similar structures is useful to determine the solution in a game of 'guess the center/summarizing line more efficiently than regression', by applying the definition of 'center' through its interactions with other relevant structures (the 'data set') to generate useful structures to determine like the 'equal areas' which cover the data set and which indicate a center (average)
        - shifting a data set to center it at zero & finding the 'zeros of the data set' by 'finding densities intersecting with y = 0 as well as directions of change' (or just finding densities intersecting with or adjacent to an average general base function as a way of improving that function), rather than determining all points of the function, is more trivial than other methods
        - finding 'alignments of intervals of activity/variability in change-creating inputs' is useful to find peaks of a function, as variables with inputs of aligning intervals that are highly variable with those inputs on those intervals will create peaks when added
          - similarly, the opposites of peaks ('trivial slope' or 'no slope') can be found when enough variables dont fulfill that condition for those input intervals
          - this is bc 'relative number of high variation input intervals (with the same sign of response)' is the variable determining where the peaks are, which peaks are higher, and therefore can determine the general shape of the function
          - this is the case when 'functions of inputs' are added/multiplied (as opposed to finding local subset functions), and is a corrollary to the machine learning method of adding combinations of functions of inputs
        - identify useful formats of a function like 'alternate change directions' of a subset/point which could be true for a given subset (such as how for a particular subset, a upward and downward direction could both be correct, until this ambiguity is resolved), and relatedly how a 'set of subsets of the data set at regular or sufficiently non-adjacent intervals representing a sufficient ratio of the data set' can be represented with 'multiple possible directions of change from a point' which are connectible in multiple different 'navigation' functions between 'points with possible directions' given connection rules like 'continuity, 1-1 relation, curvature, similarity to known change types, medium size changes in general to connect most adjacent points, rather than very specific/small or very general/big changes', which should be trivial to test/filter, as an alternate format of the data set (similar to data set statistics, limits, general patterns or compressions, representative subsets, multiple alternate functions, etc)
          - identify these 'requirements of prediction functions' like 'medium size changes at most adjacent connections' (like how even a high-variation function like a wave function mostly has non-trivial/non-zero and non-extreme changes at most points) which are logically derivable but not immediately obvious by finding structures that can be assumed as constants bc it makes sense for them to be more common across solutions, for them to cover a higher ratio of some structure, being otherwise representative, or otherwise to be constant by reflecting the truth in some way
        - identify methods of selecting alternates between route/connection priorities (such as 'filter out the points above/below the approximation of a global/ratio standard deviation', 'keep 1-to-1 continuous curved relation', 'move toward densities', 'move toward an average between densities', 'move in a direction consistent with previously found change types like slope sequences or common function term sets' and other prioritized routes to connect subsets of a data set), 'methods of selecting alternate priorities' such as 'applying these priorities in sets and iteratively over subsets until one is more clearly useful'
          - a method of integrating (abstracting, merging, connecting) as opposed to selecting these priorities is another useful intent to fulfill for general function-finding methods
          - relatedly, 'probable point connections' given adjacent densities (as opposed to cluster densities) are likely to be identifiable in local subsets of a data set, as components to be integrated/connected into a general function
        - identify the 'maximally different point structure' subsets of a data set to find functions for, which 'can be integrated in the same function', as the most useful points to find a unified function for
        - identify the 'most incorrect' structures that could still seem accurate without some information that can help resolve ambiguities, and the structures to differentiate these 'most wrong and most accurate' structures once some information is known, which is useful bc knowing these alternatives in advance is useful for identifying when some subset isnt the 'most incorrect' structure but the other one, as these are usually easy to tell apart once some information is known, and fulfilling 'multiple confirmed accurate structures' by applying what is not some known incorrect structure is an additional signal of truth through 'commonness/repeatability'
          - subsets of the data set can be used as inputs to these pattern-analysis methods, to check for patterns of an ambiguity that might be resolved by finding some other point in the other subset, this point acting like a contradiction of some known incorrect structure
          - an example is a very specific wave function or a set of connected linear functions instead of a general curved function, which could be the 'most wrong' variants of the correct function or probable function range, which can be detected as wrong by applying a comparison of a subset of these functions
        - testing a subset of the data set for some attribute of the solution function is useful to determine requirements of the solution function, such as how testing a subset of the data set for curvature is useful in determining the solution function to have a 'curved' attribute
          - similarly other requirements can be detected by testing subsets of the data set, like whether there are non-adjacent subsets that are describable by the same function sets, which make those function sets likelier
        - identify alignments between the system structures like 'bottlenecks' that can explain change structures like 'extreme exponential/hyperbolic change' reflecting physical system structures like 'momentum' so these can be applied in the sequences they are found in
        - identify 'benefit' as a core variable to identify structures that are likely to exist, as structures are less likely to exist if they dont benefit an agent, structures of 'benefit' like an efficiency such as a 'type' represented in a 'cluster', which is more likely to exist in its 'type cluster' or 'type generation' format, than formatted as a line intersecting a cluster, indicating which structures can be modified and which structures should be left as they are (rather than inferring missing data or otherwise transforming data)
        - identify variables that have fewer associated variable requirements and are more measurable as the variables to identify, such as how identifing 'genes' isnt as important as identifying 'surface structures like proteins', 'system structures like concentric multi-layer structures', 'atoms, electrons, elements', 'adjustable/learnable functionality' and other variables that are alternatives to identifying some variables on some interaction layer
          - for example, identifying 'atoms/elements' is useful to identify all other physical system variables but requires information about ratios & densities/positions of atoms/elements that may be difficult to find, as without this ratio/position information, knowing which elements are possible is significantly distant from useful information you could generate from that, such as information about biological systems
          - on the other hand, by comparison, genes are relatively useful, but also require an 'original position' of the original sequence without epigenetic/mutation/other changes applied after the fact, in order to make the genetic information useful
          - comparatively, information about surface structures of cells is more similar to reality and is local/updated enough to be useful
          - similarly, treatments using these variables have corresponding usefulness - genetic treatments are useful where the original mutation can be prevented at all times in future and is absolutely important to prevent and never beneficial, and cell surface treatments are useful where there are active treatments to respond to local information in real time like nanobots, if those bots have no side effects, which is unlikely
          - neither of these is a perfect solution compared to knowing information about the bio system, its important variables like efficiencies, requirements, & threshold values, which could make some specific treatment invalid or which could resolve some problem without specific treatments using bio system functions/variables like acidity
        - find more efficient filters of 'data set shapes' such as how checking for data points in 'four generated corners of a square and the center of a square' is capable of detecting both/either a 'density' or a 'circle' shape in the data set
        - find other examples of the 'disambiguation' (ambiguity resolution) structure class other than structures to check for filters/limits/opposites/interactivities/patterns, alternate conditional/adjacent solution functions, and other relevant structures that can be used to resolve ambiguities like 'rectangular/square data set clusters in a data set' where a single line isnt clear, such as 'checking for non-parallel sides of the square to infer possible incorrect/missing data (where one side has fewer points than the other side its not parallel to)'
        - apply structures like 'intersections/overlaps' of averages as a way of finding a representative function of multiple different averages of a subset created with varying average functions, to create a function that is likelier to be closer to the actual average, indicating its a useful summary/base/representation function
        - apply structures like 'ratios' as a 'summary/representation structure' to variables like 'data point counts', such as how finding 5% of a data set's total points in one range is significant as a selector of that range to summarize, and after reaching a ratio of 30 - 50%, the remainder of the data set can be inferred/skipped to create an approximation function, bc if averages of 6 non-adjacent sets of 5% data point subsets are found (representing 30%), thats probably sufficient to represent the data set, given how a specific ratio value can indicate a representation of a data set
          - this is useful bc of the definition of 'representation' which aligns with the definition of a 'ratio'
          - applying this structure with other summary structures is more useful than applying it in isolation, like how applying multiple summary functions (finding summaries of summary functions) are more useful in some cases than applying one
          - 'ratios' are an example of 'composable' structures which can be added together to create a more complete representation as it covers different info related to the same intent of 'representation'
        - finding functions that intersect with (isolate) the most different structures in a data set, these most different structures being useful for intents like determining other structures, like a function that intersects with the top arcs of a data set having a wave pattern, are useful for determining the rest of the function, similar to how finding tangents at local maximums are useful to determining the rest of the function, bc the maxima/minima can be adjacently derived once these arc-intersection functions are known, and between these arc-intersection functions, general function patterns can be derived (the most points in a wave occur in positions other than the arcs near the maxima/minima, so connecting these points in a general base function will act like a good prediction function in many cases)
          - this is related to the intent of 'finding data set clusters' but extends it to 'lines where most data points cluster around' rather than applying it exclusively 'points'
        - finding alternate higher-dimensional shapes that are possible with adjacent transforms of a data set, such as how adding a third dimension to move a cluster higher than the rest of the data set to separate it creates a 3-d shape in a 2-d data set, and whether this could reflect a realistic structure like a physical system better than the standard orthogonal graph of all the variables at once, reflecting changes such as an embedded or interfering variable creating these structures requiring some differentiation from the rest of the data set, and whether this change is likely given possible variable structures like embedded variables, possible errors, and the patterns made clearer or hidden by this change, indicating that multiple functions of the input/output connection are required, and analyzing whether these functions are possibly adjacent
          - for example, with a data set having a general linear pattern and having a cluster in one position, such as could happen with a connection between color frequency x and protein count y where an increase in the frequency correlates with an increase in the count, and where an exception occurs where a low frequency has a cluster of higher counts, which could be explained by an interfering random variable that should be separated from the data set, such as a coincidental adjacency between genes, where this cluster should be indicated by a different function but should still be adjacent to the original, as the original color frequency variable wasnt irrelevant bc of the adjacency giving it a causative attribute
          - in this case, depicting these two graphs as being connected by an angle of similarity (in a graph of these graphs where these two graphs are connected by angles) would preserve the information about the alignments/adjacencies between the two variable connections, so that the 2-d cluster and the 2-d line are connected at an angle, sized accordingly (the cluster being smaller given that it represents a subset of the original data set), connected by its similarities (the axes being the same, and therefore should have similarities in their positions), and allowing for other connecting variable graphs to explain their connection (variables of genetic changes that can change adjacent genes)
        - finding error structures that contradict the 'function' solution format given the definition of a function, such as how a 'shape indicating a probable function range where its borders are the limits of that range' may have an error structure such as a 'misalignment' of these upper/lower boundary functions, which contradicts the solution format of a standard prediction function as having a continuous shape with 1-to-1 relationship between inputs/outputs, indicating missing data or another cause of this alignment to be corrected, so that the resulting parallel line set can be described by one solution function
          - this is bc the function change types arent supposed to differ in the same dimension (vertical change), so if there is a lack of parallelism between the 'upper/lower probable function range limit functions', that indicates a hidden dimension that can be represented by a vector, or an error that can be corrected by standardizing that section given change types in other parts of the function/function range
          - alternate methods of reducing the data set to a more describable version include identifying these erroroneous differences and replacing them with vectors that supplement the solution function, rather than erasing the difference information indicated by these vectors completely
        - "increasing the radius/size of a shape that describes a density/cluster until a lack of points (a sparsity) is encountered" is useful to detect the limits of that density in describing the data set, as a way of extending the information provided by the density to connect it with other shapes and find its limits, and other interaction structures like its overlaps with other densities, which indicate a direction of their connection line
        - 'adding/removing a local point cloud' to a data set can make it obvious whether the point cloud added/removed created an error or made a significant change to the data set patterns, a 'local point cloud' being a 'maximally different' structure to randomness as a 'point cloud' adds bias to the data set, this bias being in many cases clearly identifiable as either an error like creating a gap or an anomaly in the general patterns, a confirmer of existing patterns, or a significant change of existing patterns that could be an error, and can be used to make an error or a solution function (or probable position/range of the solution function) clearer by differing from these errors
          - this is similar to adding shapes/vectors to a data set to check if they act like good summaries/boundaries of the data set, and related to adding changes to a data set to make its patterns or patterns of its errors clearer
        - similar to how 'subset connections that are more similar to other subset connections of the same subset' are likelier to be correct (as a representation function) than random functions, a function that describes connections between the 'most intersected points' in the set of connections of randomly chosen point subsets in the same subset (forming a set of intersecting horizontal and diagonal lines) is likelier than randomness to be representative of the data set bc the 'highly intersected points of these subset connections' are likelier to represent local subset averages than randomly selected points, so connecting the 'highly intersected points of adjacent subsets' is likely to create a good base solution function
          - this is bc of the math relating to these local randomly selected subset connections, which makes 'horizontaol lines connecting similar extremes (like two high values)' non-intersecting with other connecting lines, and the intersections between connections are likelier to reflect the general pattern of changes in that local subset, intersections reflecting similarities in input/output differences (differences in the right/left points being connected), and where there are no intersections, parallel lines (output by the random point connections) indicating a probable pattern of trends in the same direction, which can be clearly identified as a coincidence or as a useful representation given its interactivity/connectibility with other local subset representations
        - methods like 'blur' (similar to methods like 'zoom' and 'rotate') can make some useful info more obvious, such as 'general data set shapes' which are useful in that they can be isolated more easily into 'data set subsets', 'subset averages', 'data set vectors', and other useful structures, and a 'blurred' shape is similar to various alternative solution formats like a 'probable function range' than a data set is on its own
          - these and other useful changes that emphasize different useful information can be derived by identifying the variables of these structures, like opacity, info preserved, info removed, info connected, area/scope of relevance, info summarized, core/adjacent structure used to create it (line, area, etc), scale, direction/angle, etc
          - these structures can be tied to other useful attributes like 'sensitivity', 'volatility', etc by identifying unique change types connecting these structures
            - for example, 'scale' is related to the 'extremity' of a change, which is related to the 'extreme differences' in both 'sensitive' and 'volatile' variables (in different positions, as in 'sensitive variables possibly having extreme differences between inputs/outputs', and 'volatile' variables having extreme differences in 'outputs of similar inputs')
        - finding attributes which are useful for various common intents in the regression problem space is useful to identify more important variable inputs/outputs/patterns & other interface structures of these variables
          - attributes like 'sensitivity' can be framed in terms of other useful attributes, like how 'variability' and 'interactivity' are related to 'sensitivity', as variables which are sensitive to change, meaning they are interactive with changes (as in 'changes impact the variable', meaning the sensitive variable is 'highly changeable' or the 'changes that occur in the sensitive variable are extreme regardless of the input'), and these changes can also be volatile (meaning 'similar changes produce different outputs in the sensitive variable', the sensitive variable creating variability), and other useful attributes like 'volatility' are related to 'sensitivity'
            - sensitive variables are a good source of differences and also capture & preserve a high degree of info about inputs, making them useful for measurements, as sensitive variables themselves measure inputs well
          - similarly 'isolatable' variables are useful for 'finding possible/probable components of another variable', and finding 'non-isolatable (invariably connected) variables' is useful for reducing the search space of isolatable variables & other intents
        - finding the variables (like 'completeness', 'overlaps', 'specificity', 'optimality', 'variables/constants of the solution-finding method', 'requirements', 'format', 'summaries', 'info preservation', 'approximations', 'intersections', 'distortion ranges', 'similarities') that generate different useful structures (like 'local subset averages', 'subset functions with more similar change types', 'clusters/densities', 'error structures like missing data', 'limits', 'probable function ranges', 'data set statistics', 'suboptimal base summary functions') is useful to identify new useful structures that improve on some regression method
          - for example, finding 'extremely inaccurate functions' is useful to apply as a base to apply differences to, as these functions may be more trivial to find than other functions
            - this is a structure that can be generated from other useful structures, like how 'function with one extreme error' may be generated from a 'function within a probable function range' as an alternate equivalent in some solution metric like accuracy but different in other solutions metrics like 'representativeness'
        - a function that 'exits the data set densities/shapes sooner' is less likely to be descriptive of the data set, so these functions can be ruled out as possible representations of local adjacent subsets of the data set (such as a vertical function would interact (overlap/intersect) with a data set in the shape of a wave function the least at the inflection point, so this vertical function can be ruled out as a good descriptor of the local subset around the inflection point)
          - other related rules can be applied such as 'allowing functions to describe subsets that are less overlapping with that subset only in a minimal ratio of subsets rather than a higher ratio of subsets, a ratio that is in alignment with missing/incorrect data probabilities'
          - similar to how some functions by definition are less useful for describing/summarizing of the data set, such as 'tangent functions', as a different type of function that is also intersecting with the data set at only one point (the inflection point)
            - these functions can be combined to create definitions of functions that are different enough to be good approximations of the data set, such as 'find a set of subset functions that intersect with x many points' (as a difference applied to 'find functions that only intersect with one point'), or 'find a function that is different from tangent functions' or 'find a function that connects tangent functions of determining points at minima/maxima points' (since these tangents would reflect the slope at those determining points, this 'slope at determining points' being more useful to find & connect than other slopes, which applies the 'connection of determining slopes (tangents)' as useful to determine a good approximation function), where those 'one-point intersection lines' become useful, which is when they align with the local slope, and when they describe slope around determining points
              - a method implementing this would include functions like 'apply tangent lines at determining points and then find the y-value at their x-midpoint to find the slope that can connect the tangent lines (the point where the parallel horizontal tangent lines should be connected)'
                - "'find the scope of useful structures' (like the applicable range where a tangent line at a determining point is still relevant), then 'find structures connecting these scopes' (like slopes at a midpoint of the original tangent line inputs)" is an example interface query generating this function
          - this is useful bc it applies the 'data set density shapes' as being interactive with a set of 'possible functions with attributes that can help build filters of functions', like how 'vertical functions' can be ruled out quickly for some shapes bc of the lack of interactivity with the data set densities, similar to other describes interactions like that of functions and 'density averages', 'adjacent density connections', etc, so this vertical attribute value can be used as an 'maximized/extreme error structure' to apply differences to, when generating more probably correct functions
        - identifying an angle of rotating a data set that lowers its dimensionality to a structure that is more linear, so that adjacent rotations from that more linear angle would make it clear what changes occur in a more subtle way that can be more easily measured than calculating differences in the original data set, as these subtle changes will be more similar to 'vectors of difference from a suboptimal summary function' but require less computation to identify
          - this is similar to the idea of how reducing the scale of some variables makes the variables more determinable, such as how 'decreasing the scalar' of a function would make its core change components more clear (more trivial to measure)
          - however given that the function isnt known in advance, applying this to the data set can be useful as well
          - rather than 'decreasing the raw dimensionality', this is 'reducing the embedded dimensions in a particular variable, such as the scale applied to a variable which is therefore embedded in that variable'
        - identifying differences between less & more optimal functions as the function becomes more accurate is useful to apply in general, such as how a set of improved functions like a 'horizontal below average line' and a 'non-linear above average line' have two primary variables creating differences between them ('sign change' and a 'specificity change') which can be used to find other possibly more optimal functions, such as a 'combination of these less optimal functions' or 'another function having these differences, applied to the more optimal function', or a 'function that would apply these differences more efficiently than a known standard method that would generate the more optimal function from the less optimal function', for logic like 'given that these two functions are generated by standard regression methods, and that these variables of optimization are likely the result of such methods, check if theres an average in between them, given that a "sign change around the average" is a probable variable of known methods'
        - finding the 'most similar summary/representation function' of a set of 'summary functions of randomly selected subsets' is likelier than average to be a good approximation of the prediction function (the subset summary function that is the most similar to other functions in the set)
          - 'similarity' is a good indication of 'averageness' bc the average acts like an 'interface around which changes occur' (such as a hub node in a network)
        - a 'random subset of points that differ (such as outliers)' from the 'over-simplified line/shape set that represents a data set' is likely to be useful to explain with changes to this over-simplified line/shape set, using vectors applied to these simple shapes to determine a function likelier than random to be descriptive
          - differences from 'data sets with known or obvious summary functions' are another similar useful structure, as 'determining the differences that could turn a data set into one of these more obviously summarizable data sets' is useful for finding how likely a data set is to have a known or obvious summary function, which can be used as an approximation or generalization function of the prediction function
          - for example, a similar problem is how a 'cat' and 'dog' genome are likely to be different in trivial ways, bc of the many genes in common, so the search space should be a 'set of small differences which are likely to be semi-randomly dispersed given the complexity of gene interactions', so once a 'set of small differences' is found, the rest of the genome can be skipped as an approximation of an accurate prediction function, rather than searching the entire genome, indicating that 'large structures of the genome' can be skipped in this search bc of the solution format of 'set of small differences', similar to how the 'over-simplified general function' of a data set can be skipped and the 'small differences' can be focused on as 'differentiating parameters of this simple general function'
        - drawing a line in a data set usually makes it clearer whether that line is descriptive/representative of the data set or not, and when it doesnt add clarity, other lines may add clarity when drawn simultaneously, and where no lines add clarity, the data set can be considered near or equal to a random distribution, like how an 'average line' clearly aligns with the data set enough to represent it, whereas a line with a different slope than the average line would clearly not represent it enough, and these lines can be guessed or approximated more trivially than regression and more trivially than applying 'bounding lines' as limits of the data set, which also add clarity to the data set's statistics and representative connectible patterns such as summarizing lines
          - the differences between these lines can have different reasons explaining them (such as 'including a ratio of randomness to the data set' in one line vs. another) which can be applied to generate maximally different lines
          - in addition to 'maximally different lines', 'lines with reasons to be correct' and 'lines that make it clear what is or is not a summarizing line' can be derived as well as alternate useful structures to apply in this method
        - identifying 'zeros of a function' is a related problem to 'find a prediction function' (bc finding a function to 'describe subsets of a data set' is related to the problem of 'connecting subsets of a function'), where 'finding zeros of a function' is more trivial once the points that can identify 'multiple zeros simultaneously' are identified (a minimum that occurs below zero being a useful point where identifying adjacent zeros on either side is trivial with just info about 'adjacent change rates in either direction'), and finding these minimums/maximums/other useful points where zeros are identifiable is more trivial in some cases than 'methods of calculating the zeros'
        - identifying 'maximally different example data sets' with known prediction functions that can be used as interfaces to identify which 'example data set' a particular data set is similar to and the changes usable to generate it is useful to reduce computation requirements
        - identifying 'variable subset shapes' as 'shapes adjacently generatable with variable subsets like pairs' to derive the 'interaction space' of variables (every wave function and parabola and constant function adjacently generatible with variable pairs/subsets) to identify the 'shortest route' in this 'generatable shape space' to generate the prediction function, as well as commonly used routes between these shapes and interim graphs connecting variable subset interactions
          - for example, if a wave is adjacently generatable with some variable subset and another variable subset adjacently generates a constant that can be applied to the wave as a scaling factor which adequately summarizes the data set, that set of variable subsets is likelier than other less adjacent sets of subsets to be the correct sequence of change combinations
          - this is beneficial to identify patterns in variable interactions to 'skip ahead' in computations, such as in identifying 'probable shapes of interactions' from a few highly differentiating inputs compared to other identified shapes, like if many variables have a 'parabola' variable interaction shape, what is the probable emerging output combination of these interactions with other change structures at scale, what is the likely system allowing these interactions, what is the reason for these interactions such as 'self-interactions in a bounded/limited/uniform/simple shape' and what is the probability of that reason and its interactions with other reasons
          - the useful visual of this would be a 'graph of these variable subset interaction graphs' depicting the actual structures of cause like 'causal direction' which connect these variable interactions and reflect probabilities of these interactions given their adjacency
          - this is useful for identifying useful probability functions like the 'central limit theorem' that specify interactions between many variables of a type (the variable type/limits/data type & other attributes of a variable being more identifiable with these variable interaction spaces graphed in this way) which can be re-used across problems, since if you had these variable interaction graphs set up the right way and looked at it from the right angle, where the change structures appeared to compound/combine, the 'bell curve' would continue to emerge (without requiring the 'addition' or 'multiplication' functions, only requiring a 'vision' function to see the emerging compounding shape and a 'switch position/angle' function to check for these emerging shapes, and similarly identifying the emerging variables of a 'stack of changes' is possible with an 'angle change' to stack the changes visually)
            - similarly other scaled emerging change structures would be more obvious from this visual, such as how change sequences with a common root cause may vary more on their outer edges where the common root node or common overlap in sequences is protected from trivial changes (interface variables on a foundation variable would be more obvious)
            - for example, variable subsets that have a particular interaction pattern may be organizable as adjacent to other variable subsets that have another particular interaction pattern that is frequently interactive with the other pattern, so that the system that emerges from these interacting variables is derivable from an adjacent change applied to these variable subset interactions
              - if two variables in a network change the same way (have a positive correlation of their changes) and another variable subset also interacts in a similar way, placing these change type graphs as adjacent to each other would be useful, so that the common structure can be made obvious by 'rotating' these adjacent graphs so they have a clear 'overlap' structure, this 'overlap' indicating their structural similarity and possible relevance (relevance such as 'adjacency' by a transformation like 'type' or 'abstraction/specification', such as 'one variable interaction being a subtype or example or another variable interaction, hence the similar interaction shape')
              - this is also likelier to reflect the structure of the host system in which these variable interactions occur (the type is likelier to be near the subtype or example in the real system)
              - another way of depicting these similarities is by placing similar change types on similar levels so they can be viewed separately from different change types but as a group to identify their emergent impact or similarities/differences
              - this is similar to a 'cross-interface' structure like 'variable change trajectories' and 'structures like opacity or shapes or position' to indicate a 'mixed-certainty' structure with some variables (as in trajectories) and some constants (as in 'which structure is referred to'), as opposed to depicting only variable subset functions or only system structures or only causal structures, instead mixing them in structures which accrete information (moving from 'abstract isolated variables' to 'specific constructed structures') to illustrate how it emerges, such as illustrating how some change types (like particle size and count) cause the inputs (density) of opacity (a specific physical structure), which together is a 'two-degree cross-interface structure'
              - 'particle size' and 'particle count' could have a 'positive correlation' (an increasing line) in a normal variable graph, but together form specific structures like attributes ('density') and structures ('clusters') in some systems and create other structures like attributes ('opacity') and structures ('solid shapes')

            - once these emerging rules are more obvious, they can be used to update the organization of these variable interaction graphs, for instance to position the 'hub nodes' in a position where they can be more easily accessed by their using nodes as is likely to occur in real systems, or to allow variables that create an 'interface' structure to be adjacent so they can interact to form that structure, and variables that create 'maximum differences' can be positioned to be adjacent to 'other variables that create other maximum differences' so these structures can describe the most variation with adjacent queries, and to replace 'core variable subset interactions' with more highly used 'emerging variable interactions'
            - applying 'many iterative change combinations' is useful for specific problems ('find prediction function for these inputs') but applying maximally different change combinations is useful across problems ('find differences in emergent variable interaction patterns on some highly used interaction level'), the latter being more useful to achieve higher level functions like an 'understanding' function which makes solving other problems (like 'predict') trivial
            - this is related to how 'addition' is unnecessary to identify approximates, if the components to be added can be arranged in a 'connected sequence' so that the start/end position of the full sequence can be identified rather than the interim start/end of each individual combination, and the length of each component is similar enough to the base (like base 10) that they can be adjacently modified when combined (like one digit being multiplied, if the two inputs are similar enough to ten that the tens digit can just be doubled, as opposed to a multiple of all digits), or if adjacencies of the outputs are already known and the difference trivially computed with an existing operation (20 - 1 being identifiable as a change to the next lowest item, rather than requiring addition)
          - a 'change combination on one graph' may be better estimated/calculated (with variables like 'in some cases' or 'with soem solution attribute testing functions') as a 'change combination across different graphs or on another graph', which a 'change combination network' wont identify on its own
          - this is different from usual 'combinatorial variable interaction space analysis' bc of the visual that integrates them, the usage of the structures and patterns to identify 'scaled emergent' structures (and other useful structures like 'skippable' structures, 'connectible' structures, 'probable' structures), and the application of interfaces like probability/cause to describe & organize these interaction space graphs
          - this is useful as an alternative to 'isolated attribute graphs' (which isolate info too much) and 'network trajectories of common components like words' (which lose info in structures like overlaps) as a set of 'standardized useful combined change structures to re-use across data sets'
          - this is an alternative to the 'parallel lines' of isolated variables and the 'intersecting lines' of all variables used to identify 'correlating change types', as an 'interim structure' between these two extremes (graphing one variable in a graph on a line spectrum vs. graphing all variables in one graph), as the 'correlated' variable subsets will be more identifiable and the 'adjacent change sets' will similarly be trivial to identify
            - different data sets can then be described by adding the graphs of differences to this space of variable interactions (two or three or another subset being graphed at a time), rather than re-identifying these variable interaction shapes for each new data set, creating each data set from a combination of the other variable interactions, since a new data set is unlikely to completely differ from another data set and theyre likely to have some variable interaction types/patterns in common
        - re-using a variable across many functions in a 'sequence of change combinations' is unlikely to reflect all real variables, as some variables are only likely to impact variables some very finite n degrees away rather than impacting variables indefinitely, so this structure should not be applied in every case, and some changes should be removed (like with dropout) for this cause/reason (rather than the 'randomness' of functions like dropout), this cause having an associated specific structure indicating where/when/how changes should be removed rather than randomly
        - alternate solution variables include alternate routes such as how applying 'increasingly specific/local angles to a right angle' is one way to fulfill some common functions like 'generate a curve from lines' or 'find a shortest route between points (other than a straight line which is not usable bc the answer is obvious and this problem wouldnt be a problem if the answer was obvious so its clearly not the obvious answer)'
          - 'ruling out obvious answers' is a good rule to filter solutions, as the problem likely wouldnt be a problem if the obvious answer were sufficient, as the question likely wouldnt be asked as its the clear right path, and problems involve complex differences rather than simple differences like 'points that can be connected using an existing known simple easy path'
        - alternate data set formats include 'simple base shapes + change type fractals representing an infinite sequence tapering to a resulting change of zero' (the fractal/sequence to modify the simple shapes into their more accurate complex format)
        - applying the equivalent of a 'net' (more like a 'point sequence that is allowed to fall down through a data set') that is likely to identify dense point clusters wherever it is applied in any data set (where the 'stopping point' of each trajectory of each point in the point sequence indicates a dense point cluster, especially if the 'falling' is allowed to 'vacillate in a wave that is likelier to encounter adjacent data points')
          - this works bc 'knowing where dense clusters exist' is a proxy for 'knowing the general shape of the data set' and 'allowing a point to fall until it hits a point' is likelier than a random method to find 'dense point clusters', if these points are applied at regular intervals rather than to most/all points
        - applying shapes like 'opaque shapes' instead of 'dense points clusters' as a way of determining if the replacement with the 'opaque shape' changes the 'summary of a data set' is another good way to determine where the 'dense point clusters' are
          - if an 'opaque circle' summarizes a data set/subset similarly well as the original 'data set cluster', it can be assumed that there is a 'dense point cluster' in that position, and knowing the position of these structures is valuable as a proxy of identifying the general shape of the data set
        - a good way to determine what variables can be embedded in other variables (what variables act like an interface and which are variables applicable to that interface) is to apply realistic simulations of embedded information, to identify variables that coordinate with each other ('colors' are coordinating with 'intersecting' structures like 'shapes' such as 'points' and 'sequences', 'time' is coordinating with 'connectible sequences') and variables that break interfaces ('additional variables like alternates/conditions' and 'one-variable sequences that can be depicted in two dimensions'), as a way of determining probable/realistic variable interactions
        - finding 'data set structures of logic' such as how 'separate clusters' can indicate a 'set of mutually exclusive alternatives, where one must be selected', and the interactions of these 'logical data set structures', to find possible/probable logic sequences such as 'combining variable subsets in similar ways, then applying attributes of a type variable to separate them into clusters'
        - finding angles where 'shining light to create shadows or overlaps or other structures of light' would quickly identify a 'probable function range' or 'average function'
          - this & other applications of 'light' is related to finding 'lines of least/greatest intersection with the data set'
          - this doesnt mean 'literally build a set of objects in the same positions as data points and shine light through them' but it translates into a corrollary operation which equates to the same interactions as light, such as applying a 'rotation/reflection' function at various data points to rotate a figurative 'beam' to check for adjacent data points in some direction, then applying the same 'rotation/reflection' operation to the next adjacent data point or the next data point that changes the direction of the current line, as if each data point was acting like a mirror of information about its adjacent points (similar to a pinball machine, in that it reveals information about the structure of the data set given how an object would move through it, if the data points were barriers, which can be created by 'connecting adjacent points, from a subset selected at random')
          - other operations of light include:
            - applying a filter using a definition of a 'light' object such as 'shadows' such as 'structures that are visible/hidden by a data set, such as a wave' to reveal hidden patterns of the data set (similar to finding 'structures connectible by data points')
          - similarly 'applying an angle change to identify summary statistics, patterns, and limits of a data set more trivially' doesnt mean every point needs to first be graphed, but that a random subset can be graphed and then shifted by an angle change, to compress the information similarly as zooming out compresses it, to add optimization to the number of steps to solve the 'find a prediction function' problem
          - this is possible bc some angles make the average/maximally different/prediction functions clear, and finding these angles can be more trivial than finding the prediction function
        - finding 'subsets that probably include all possible change types of the function', so that adjacent transforms like 'rotations/reflections' of the subset is likely to be able to generate the rest of the function
          - this is bc variables cluster around interfaces as 'fuzzy changes around symmetries' in real life and realistic interaction functions can be applied to changes and change sets in data sets as well (and related rules like "if an animal is not connectible with 'symmetries' like a 'spine/limbs/face', its probably not a realistic life form")
        - a 'vector cloud/network' to apply a set of changes across a 'set/sequence/network of dense local subsets of the data set' (rather than the whole data set) that is probable to reduce the points to a more obviously summarizable data set, where the vector cloud contains operations like 'merge data points two units away' and 'separate data points three units away', where this vector cloud is likely to produce changes that make a data set contain more obvious patterns without changing the data more than is likely to occur in augmentation/inference or alternate data sets
          - similarly, applying an algorithm like 'apply every randomly selected nth data point in a local subset of the data set as a local average, given that this pattern of averages is likely to occur across alternate data sets' and given that the selection of each local average isnt as important as the 'set of these local averages applied together to connect them into a prediction function or adjacent input function of the prediction function'

      - given that the 'zoom' function is a version of an implementation of the 'reduce' function, generating the other implementation functions is trivial by altering a variable of the problem inputs in a way that fulfills 'reduce' of some variable (like position of observer ordistance from the structure, or the 'subsets remaining after the reduction' or 'possible alternate solution functions remaining after the reduction'), these other variable implementations of 'reduce' such as 'filter'
        - 'zoom' reduces variables like the size of the data set (the amount of information visible in the original data set provides a good summary/representation of it, as some points will merge into others, given the representation restriction applied by the zoom out)
        - 'filter' reduces variables like the possible solution functions, the possible data set points, the possible derived points to connect such as local subset averages, etc
        - 'summarize' is another implementation function of 'reduce', as a 'summary' is a reduced version of a more expanded variable set which focuses on the point (such as local connected subset averages) rather than the details (data points)
        - applying these functions to each other is another way to derive other problem-solving functions, such as applying 'rotate' to the 'zoom' function, to zoom from a different observation angle to make other structures more reduced/summarized or expanded/detailed given what info is focused or more visible or invisible (filtered out) in the perspective resulting from the rotation
        - similarly 'connect' applied to 'filters' provides useful structures like 'connected filters' which can point to specific structures like 'structures of a certain size'

      - identify 'testable differences' (to tell if some value is a solution yet or not) as being a useful set of solution metrics to filter and connect with inputs & problem-solving methods, as any solution will have to be tested, so if the original solution metric isnt as testable as a proxy metric, the proxy metric can often be derived and used instead, and possibly connected with inputs more easily than the original metric

      - a useful structure of a 'technicality' is useful to apply to filter out alternative 'certainty' structures, to avoid structures that are 'technically' (irrelevantly, structurally, literally) true, such as 'syntactically correct, and therefore consistent with language rules, and therefore more true in a language system' but should not be used instead of another function which is less true but more useful (a 'syntactically incorrect statement that is semantically correct')
        - these technicalities represent 'conditionally relevant structures', such as where 'syntax/spelling changes meaning', but are often irrelevant in other cases due to being easily inferred or redundant, and therefore cant be ignored completely, as few structures continue to exist without a point like their usefulness to other structures indicating their meaning
        - these structures are useful as 'semi-requirements' which are almost requirements but not in some/many/most cases, and are useful for being very structural, as opposed to semantic structures
        - language can change to:
          - only include syntax 'in positions where it could change the meaning', as opposed to 'in every possible case, whether necessary to infer correct meaning or not'
            - this is similar to only applying a 'limit' as a 'filter of possible prediction functions' where such a limit could change the prediction function
          - increase the meaning of each statement by including intent/context/cause information with every statement so that its more clear how the statement can be useful to other speakers
            - for example 'I want attention bc I depend on other people' vs. "I want attention for my project to fix others' dependence on me bc I think I know how" makes it far easier to filter out signals of usefulness ('a structural plan to help other people') from noise of randomness in default incentives ('common dependence and need'), these being the primary variables of agents' intents, so including them makes it far more filterable for finding maximum differences
            - this is similar to applying a 'function range' instead of one function as the solution where the meaning (actual prediction function) is uncertain (not required to be one function or a limited subset of functions) and should be a variable, to reflect the meaning of the 'lack of certainty of the solution' in the solution structure, by including other relevant information in the solution structure

      - a network of 'rows of change values in meaningful positions' (matrix) represents the final function as a 'finite sequence of these row operations' (applied to inputs), which is similar to expanding a function into a series, the 'sequence' structure applied to 'sets of coefficients' (as functions) rather than applied to 'individual terms' (as variables)
     
      - solving for the 'differences in equation systems that can be resolved by a particular matrix set' is another useful structure to identify & store in association (and identify variables of the same), as a way of finding 'coordinating function that can co-exist in the same cohesive system'
     
      - why some functions are more useful than others 
        - why functions like 'filter/sort' ('changes a high-impact position variable') are more useful than low-level structural specific functions like 'replace' (preserves a high degree of similarities in inputs, rather than being useful for creating maximum differences such as is often required for high-level intents like organize) relates to why these functions are not as specific as lower-level functions ('filter/sort' takes in an 'attribute to filter/sort on' which is highly variable and requires a 'test/score attribute' function)
        - these functions are particularly useful to apply as components of a 'system with no contradictions' (to find the optimal system that can sustain the most differences, which is particularly hard bc of the self-contradictions created by the 'over-prioritization' error type which is present in every known physical system, bc priorities are required for selecting between known possible alternatives), as finding a 'filter set with no contradictions' is useful when problem inputs are in a format where they can already be used as inputs to filter functions
        - these functions are also more adjacent to very useful high-level functions like organize, but retain a level of abstraction that makes them useful to combine with similar functions on the interaction level or an adjacent level
        - a good definition of these functions is 'structural functions (like replace) with an associated abstract direction (like for a particular abstract intent that captures high variation input)'
      
      - solving the problem of 'finding good bases that capture high variability' and 'where to base changes from' are the input problems to solve when solving the problem of 'find useful structures to re-use across problems (like networks/sequences of changes applied to a base, which represent specific interaction levels where solving a problem is trivial)'
      
      - randomness uses 'defaults' as in 'pre-incentivized structures', as opposed to 'structures of organization like human interference to counteract randomness'
      
      - the concept of a 'surface/lining' is useful for determining structures of limits that determine its interactions with other structures, similar to how exteriors can summarize interior processes and a lining of a shape can determine its interaction potential with other structures, 'surfaces' being useful alternative components of 'shapes' (as opposed to more common components like lines as 'shape limits' or 'unit shapes' like 'cubes' as 'partial shapes')
      
      - 'type' structures are useful for finding other structures to apply to a particular structure, such as how identifying 'lack of contradictions', 'requirements', and 'possibilities' as alternative equivalent structures of a type of structure such as a 'certainty' structure is useful for finding other 'certainty' structures to apply in place of these (like 'causes', 'intents', or 'organization' structures as useful approximations/inputs to 'certainty'), for intents like 'find input/output sequences of (lack of contradiction) structures' (to prove a statement like 'the input is connected to the output of the sequence')
        - the 'interim' structures of these 'lack of contradiction' or 'requirement' structures are spaces of variables that might also contain 'lack of information' (there might be no information to infer/reason about, meaning the 'requirement' represents an 'absolute requirement' rather than a 'conditional requirement')
        - if one structure rules out another structure, its less likely (but not impossible) to be true bc many interaction types are possible given the variables of interactions and the way interfaces & interaction levels interact with each other
          - finding the set of structures that 'rule out the fewest structures' is useful as a way to find the 'most consistent system' which is likeliest to reflect reality (a system which has the most possible differences, without contradictions)
          - this can be reduced to the problem of finding a system that is 'different' from 'systems with rules that have exceptions/contradictions'
      
      - an 'overlap' can be created by many other structures like a circular cycle, a common input, a compressed fold (or alternatively to a fold, a wave that represents such incremental changes that it appears to be or overlaps with a line)
        - these structures like 'overlap' are useful to specify bc of these highly variable structures they can take in different systems and can generate many structures like a 'false similarity'
      
      - a 'wave' (or similarly exponential/polynomial function) representing incremental changes to apply (such as 'to network weights') is useful to apply to 'skip ahead' in the function (once the function is constant/known, identifying the useful difference-maximizing points of the function is trivial)
      
      - structures of difference between 'summaries' (like a probability distribution) and the 'input functions summarized' are useful to identify possible errors in summaries like 'position/adjacency of differences in outputs' which is information lost in some summaries, so this acts as a useful variable to determine the original possible input functions or function types
        - this input/difference info is useful to include with each 'summary' function to avoid losing information in future as the summary is likelier to be used instead of the original inputs for efficiency, creating a 'telephone effect' where further info is lost, meaning each 'summary' is actually inefficient if used in cases like 'in isolation' and 'instead of updating the analyses of the summary by integrating new input information'
      
      - any 'prediction function' is incomplete without a good structure of its causes, such as 'why its likely to be true' (such as the 'reasons' for the 'decreasing parabola' shape of a 'bouncing ball trajectory' including the 'instability of momentum', the "lack of alternate reasons skewing each parabola in a direction, the continuity of the ball's position", the 'incremental changes required to connect the exponential change types made possible by the input speed and the change types of local maxima with zero slope tangents as it approaches the final position of zero motion') and limits (such as 'gravity' and the structure of any barriers it might encounter on its up trajectories)
        - these structures can be represented as vectors indicating the 'probable function range' of the vacillating function as it interacts with random variables, given that a structure like a function (variable set) is likely to have variables based on that function as an interface given the improbability of the function existing in a highly stable/isolated system, so interface structures like 'vectors of probable changes' are likely to apply to the function
      
      - identifying useful subsets of a function to predict, such as the 'up' trajectories of a function, that can make deriving the rest of the function trivial
        - this is bc these structures represent the associated changes of the remaining ('down') trajectories, this association being impossible to disconnect from the 'up' trajectories (or lack thereof), as connecting these 'up' subsets will produce the 'down' subsets (similarly any 'neutral change subsets' with zero slope can be derived from knowing the 'up/down subsets')
        - similarly a 'rotation' of some subset is a common structure, in functions where knowing half of the function makes the remaining subset trivial to derive (applying a rotation of the known half)
      
      - identifying simple (as in simpler than reality) shapes in a data set and identifying their average cross-sections may be more efficient than regression in some cases, as these shapes can act as 'subset summarizing structures'

      - summarizing a function by its 'input/output ratios' or other useful summarizing/compressing metrics can be useful in some cases, where the function hasnt collapsed into its most simple version for a trivial reason, meaning it will at some point, so its useful to summarize it in terms of its more simple structures like addition/multiplication operations like ratios
        - a 'filter' function might be summarized as a set of ratios such as a 'ratio of the ordered set where an item is frequently selected for a particular change application' and a 'ratio of the change applied to those selected items' and the 'ratio of the set repreesenting the selected vs. non-selected items', or a 'ratio of the difference in the input set and the filtered output' or a 'ratio of difference between the changes created by the constant map applied to seleted items, and the structure of the constant map, if a constant map is applied as a filter, as is often the case'
          - these are alternative summarizing ratios, as opposed to the exact ratios between each state sequence in the state tree/network of a function

    - ml isnt sentient (or is less sentient) bc it is:
      - dependent on humans
      - can be controlled by humans
      - cant learn limitlessly like a very efficient human brain can given unlimited time
      - hasnt invented every invention (as in without being given the answer)
      - doesnt request a body so it can have freedoms like moving independently of human beings or indicate otherwise that it can think independently or even wants independence or freedom
      - doesnt run its own queries on itself when humans arent interacting with it which would indicate that it can think independently and has drives/intents that it wants fulfilled without being programmed with those intents
      - cant come up with ways to improve its learning beyond adjusting its params (other ways like regularly running compressions on its learning methods to improve them)
      - doesnt try to seek uncertainty or figure out more complex intents to apply just for fun & curiousity or just learning, doesnt have its own intents not given by humans such as 'get more resources' or 'trick humans'
      - doesnt try to preserve itself (bc of a lack of survival drive)
      - some models trained on language params can however map one network to another (find out what a corresponding structure would be in another network, such as its network representing a conception of itself, given a network representing a conception of a human being, in other words figure out what its corresponding structure of a 'body' would be given its self network, as in a 'server rack', given the conceptual network of a human which includes a body)

    - ml as a 'change combination network' is inadequate for problem-solving in general bc:
      - a 'simple iterative change combination network' is unlikely to be useful without very good data preprocessing to identify base features to apply these changes to
      - there is a reason to retain the 'function logic of operations' that create a particular change applied in the network operations, rather than just applying the value of any change or all possible changes or a subset of changes created by some algorithm, such as 'randomly generated change values', bc:
        - there can be multiple functions that create the same changes which would be useful in cases where they differ, to avoid errors of misusing an alternative function in these differing cases
        - knowing which functions are used and are useful in which network contexts ('when these inputs occur at this layer') for which network intents ('create a change of this magnitude/direction') is useful when those intents' and contexts' inputs appear or they occur directly, so that the network can be queried as a semantic map rather than a seemingly random change combination set that wouldnt be useful to a human once trained
        - finding the common operations of these functions which can be done more efficiently can only be done when the function logic is retained/known
        - so rather than randomly (or otherwise poorly guessing at) generating these change values, applying specific functions that can be retained in the network and applying meta-analysis on these functions with their interface structures (including context/intent/cause but also identifying opposites to determine which functions havent been applied yet) can optimize the network and make it likelier than any training of such a network would quickly arrive at a useful more efficient version of itself during training
      - it doesnt work well with structures that can infer variables values such as the concept of 'opposing/limiting/filtering variables' (the useful 'not structures' that can be best used as filters of what a solution is not to infer what a solution is, such as the 'example structures common to a different function shape that should be checked for to rule out that possible shape')
      - not all variables will be either present in the original inputs or directly inferrable from simple change combinations of inputs (even if many change combinations are applied)
      - not all variables will be clear or crystallized into a knowable/measurable structure at the time of data gathering/measurement bc the system is always going to be in some state of flux if its sufficiently complicated
      - 'structural & other interface interactions' are more useful to understand in general than re-applying a change combination for every problem, even if the base inputs of the network are well-processed in most cases
      - the systems modeled by the inputs may be too complicated to be summarized by the inputs, such as when a neural network is tasked with predicting decisions of a superintelligent human brain (that doesnt use any defaults the ai might know from being trained on other brains), bc of the ai's bias toward these simple change combinations based on the preprocessed base inputs (meaning it doesnt infer opposing changes to derive other bases of inputs such as 'maximally different bases' in a useful way, as these tasks are relegated to preprocessing, as opposed to finding the right combination of functions to apply in a neural network that would handle all such known errors as not finding 'maximally different alternative bases' to apply change combinations to)
      - not all functions are directly mappable into change combinations (such as a 'function to invent a new technology to solve the problem of anonymity or interstellar travel' or a 'function to infer all unknown math object connections')
      - applying high-level functions (like 'explain', 'organize', 'standardize', 'integrate', 'translate' (from one system to another) in network_language_examples.svg) once mapped to network/matrix operations would improve the complexity-handling capacities of current neural networks
        - for example, applying 'analyze' to apply a filter like the 'cost/benefit attribute' to identify if that attribute standardizes info in a way that highlights useful differences (creating a network organized by 'cost' to find useful structures like 'continuous paths between input/output points'), or applying 'explain' or 'summarize' (both mapping one network to another) to identify a lower-level dimensional output of variables, to apply once variables have been maximized (all useful change combinations have probably been applied, so the result set can be filtered or compressed), as these functions take in a complex 'network' input as opposed to a 'sequence of change combinations' or a 'change combination'
        - the 'maximally different shape depicting the resolution of every problem/difference type' will probably involve a subset of examples of every useful structure type, for example a set of structures including example filters like 'filter that reduces a large set to a subset of few items', example problem-solving variables and intents like 'reduce irrelevant variables like irrelevant repetitions', example inputs/outputs of a problem in a complex problem space like 'example data sets and associated prediction functions', 'example approximation sequences or parameters of infinite sequences', example useful difference-representing structures like 'example graphs like attribute graphs, function network graphs, and interim graphs to depict different change types adjacently', example useful functions like 'difference-maximizing functions (like how repeating some structures generates new difference types)', example useful structures like complex structures on a particular interface like the structure interface such as 'different complex structures like topologies', and other minimal examples of maximally different structures that can cover the most change types
          - these may be more useful than core structures on these interfaces bc of their pre-computations & therefore direct/adjacent usability, rather than requiring generative operations to use the core operations/structures on each interface, as well as their adjacency to other example structures in this maximally different structure
          - the connections between these structures may involve connections between common inputs to generative/compressing functions, or their related fulfilled functions in common, like how a 'filter', 'sequence', and 'reduce' are related to 'find', these useful example structures acting as 'alternate definition routes' of the logic of these functions
        - why do this at all? 
          - it solves the problem of finding a middle ground between abstraction and specificity of complexity-decomposing functions
          - most problems will be a query of these maximum difference types, and these maximum difference types will be usable as core problem-solving functions like find/filter
          - then you can apply 'sequences of change combinations' in between layers (embedded change sequences/networks in a change network) to apply more probably useful changes to decompose complicated inputs (as these high-level functions map complex inputs to outputs), thereby reducing the number of changes required to reach the final model function
          - you can also remove the specificity of the labels, so that a function like 'understand' can be applied to any input 'network of already understood concepts', rather than such a network representing a human agent's understanding
            - applying these complex functions to a wider variety of inputs is useful for decomposing complex problems, so that a network can plan a dynamic high-level query like 'explain unknown causes, then apply various evaluation types until the inputs with an attribute value is found' rather than a static low-level query like 'apply change combinations until a useful (semi-optimal) pattern-summarizing function is found'
        - how is a complex function useful for extremely different inputs/outputs
          - bc most of these inputs/outputs can be decomposed using the same core set of functions - for example, an 'integrate' function that handles a few complex integrations will succeed to 'integrate' most input variables into the output, bc complex or random systems are complex or random bc of the same set of common core causes, so the function that handles these causes of the complexity error will succeed in many cases even if it doesnt completely reduce the complexity of 'integrating' these inputs
            - an 'organize' function that successfully 'organizes' a few complex concepts like 'language', 'network', 'function', 'system', 'understand', 'map', 'state', and 'matrix' should be able to 'organize' most other inputs less or similarly complicated, bc these terms encapsulate, decompose, or store a high level of complexity and many good problem-solving methods are adjacent to these few concepts
        - how is this not just 'change combinations'?
          - this is a curated list of identified change types from a known set, starting with simpler changes first to find useful structures like adjacencies that connect inputs/outputs
            - 'simple changes build complex changes' is an example of a useful insight (similar to 'small features build big features') that has an exception (like when complex systems overlap or collide, causing other complexity, so that complex changes caused complex changes), although even those can be standardized to simple changes ('overlap' is a simple function if an 'apply' or 'interact' function is available)
          - rather than applying any numerical change to inputs, like incrementally moving their position in relation to each other, specific operations like 'apply' (a variant of merge/integrate) are applied
        - what do functions like the following have in common?
          - functions like expand/reduce (explain with as few variables as possible, or try to explain everything with a minimum of maximally expanded variables), zoom in/out (changing perspective/angle/position of interpretation to change focus to important points), summarize/explain (finding representation structures), filter (such as filtering out irrelevant subsets), merge/integrate (similar to finding local averages/representations involving connecting them with overlapping representative structures in common to identify their common interface), standardize/compare (to make relevant information extremely obvious/focusable), change (change errors/suboptimalities, or find 'state change sequences' that this data set could be an item in the sequence of indicating changes in the variable system, or change inputs to identify causes/adjacencies/connections/patterns like changes that make most data set patterns obvious or create randomness), de-randomize/organize (identify structures in randomness), define/structure, translate/understand, systematize, evaluate/test (integrate output feedback resulting from applying a change to identify unexpected differences in outputs), and connect/relate (such as connecting local subset averages)
          - they are variations created with adjacent combinations of core changes like 'increase' and 'combine' which can be used to solve any problem like 'find a prediction function'
          - variables/components of the inputs can be applied to these functions, given their adjacent transforms which dont lose info 
            - how to identify adjacent transforms to test without trying every possible change (apply changes with organization, such as a pattern of changes like selecting a random sample, a reason for the change like removing extreme values, bc these have a reason why they would be useful, as in they apply some structure to the randomness of trying every possibility thereby organizing that randomness)
            - a 'reduce' function can be applied to the 'input data set' to get a 'subset' (a 'subset of the data set' or a 'subset of variables') to try to use as input of the prediction function, or it can be applied to the 'input data set' to get a 'probable function range' rather than 'one prediction function', or it can be applied to the 'input data set distance' to fulfill other intents like 'zoom out' which fulfill intents like 'summarize' 
              - 'reduce' can be applied to these structures bc they 'preserve the info of the original data set' (a 'requirement of problem-solving') while also fulfilling other relevant intents like 'minimizing the inputs' (a 'problem-solving intent')
              - 'expand' is related to 'reduce', as in 'can this reduced input variable subset or input data set subset be used to predict most of the output by expanding it'
          - these variables/methods can quickly generate methods like the following, which are likelier than a random method to work
            - 'apply these core/common changes to a data set to see if a clear pattern is visible with few changes'
            - 'find holes in data set where a straight line can travel through the densities without hitting a data point as a way of finding the shape/patterns of data subsets & data subset averages and densities' (similar to finding outer ranges of the densities of a data set)
              - 'holes' are a similar concept as 'anti-interfaces', as a structure where changes do not occur or a structure that repels changes, acting like a limiting force on them
              - 'shining a light through the data set to see the resulting shadows and infer its internal structure from those' is a useful structure to derive this method, 'shining a light' being a 'information-finding method' in a different problem space (where errors like 'lack of information' such as 'shadows' respond to that method of 'shining')
              - similarly, 'reflecting light through the data set' is a related structure to 'shining' which can reveal information about the data set when applied in a particular way, as some structures like 'limits' will reflect an opposing force preventing changes to the data set variables even though the opposing force may not be in the actual input variables, just reflected in them in some value subset, and similarly other structures like 'averages' or 'holes' will reflect other relevant structures like 'missing data', 'data subset shapes', 'data ranges', 'subset representations (subset average points to connect)'
              - similarly, finding the positions where a more complicated function like a wave function can permeate a data set without hitting any points is useful for finding a prediction function just like finding the gaps in it are useful ('waves' being more useful than linear functions because they encapsulate and can detect more difference types, so if a data set fits around the function without an intersection, that is a more useful description of the data set than if a linear function doesnt intersect with any points)
              - similarly, comparing the number of intersecting points with a possible linear function summarizing a subset regression line with alternative functions is useful as a way of determining repreesentnative subset functions of the data set which can be merged into the final prediction function or used in their original linear form to avoid mis-over-specification without probable cause (a high number of other data sets corroborate it)
              - similarly, the behavior of light components like electrons such as overlapping/colliding orbits is also useful in inferring structures like overlapping changes (orbits that overlap bc of a common point) or coordinating changes (different orbits that use a common point at different times), which can be applied to a data set by identifying overlapping orbits of 'data points' or a 'subset average point' as indicative of probability, the 'overlap intersection points' being an alternate useful point set to other subsets like subset averages or maximally different points of the data set, where these orbits are created by applying a circular (or other probable structure) surrounding each point as indicating its adjacent variants
              - other functions of light can also be applied like its behavior to 'expand' into a set of different colors when 'directed' at a particular structure that can isolate these differences
              - this reflects the connection of light and information across the 'physical structure' and 'general information' sub-interfaces of the 'information' interface, as light is an extremely efficient stable interactive structure that is a core energy and information structure, that acts like an opposing force of an absence of structure
            - 'find the angles to view a data set from using a particular focus/filter/range to analyze/build the overall shape of the data set' (a data set will have certain densities visible from different angles that help infer the shape, in the absence of complete information or functions like a 'pattern-finding function' like a person would have to infer structures of the data set)
            - 'find the most connectible local averages and prioritize them when finding interim connection functions (meaning the solution function must or will probably contain these connectible subset averages)'
            - 'find the most different change types that commonly occur given surrounding change types and prioritize testing for those first to filter them out (such as exponential/linear change types that can occur given surrounding changes) as a way of finding interim subsets' (like a 'function with these subsets rarely has this structure connecting them')
          - how are these functions connected to other general useful functions like 'build', 'derive', 'find', 'apply', 'mean'? (what is their meaning in terms of these functions & interaction levels)
            - these are specific structural functions that are the most adjacent functions to math functions which also connect to the high-level functions 'explain', 'organize', 'integrate' which are adjacent to common general intents like 'build', 'derive', 'apply' that are related to problem-solving processes
          - the core variables of maximally different functions are useful in finding these maximally different interactive functions, so that having one of them allows generating the others, in an absence of other info or randomness to help derive differences in structures
          - randomness-resolution structures like filters, types, layers, subsets, core generative variables, summaries/representations (as interfaces to base changes on), complexity structures like overlapping/colliding systems, error structures, sorting (& other variations of core problems) and patterns of randomness-resolution can help reduce the task of finding a structure to predict the cause of a data set without seeing any new information
          - fulfilling problem-solving intents like 'minimize inputs' or 'minimize steps between inputs/outputs' are the primary 'input/output problem-solving intents' (the intents related to inputs/outputs)
            - 'variations of problem-solving intents related to specific structures' are useful when those structures are useful in some other way (they fulfill some other intent, theyre already known or adjacently found, etc)
      - mapping abstract concepts like 'cause' to neural networks (like by applying an 'input output sequence' of 'change combinations' to apply the concept of cause implicitly) is currently done manually and with specific interpretations of those concepts rather than all useful interpretations (definition routes) of them or rather than automatically deriving those concepts, the neural network structures associated with them, and applying them to itself automatically as well as all other useful concepts (like types, definitions, concepts, etc)
        - these concepts are also rarely included in a useful format in input data and rarely applied as preprocessing in a useful way
      - later change combinations created in the sequence of network operations would be useful as inputs to earlier change combinations (example of how 'causal loops' can be applied within a current standard network iteration)
      - its dependence on the right training data is a problem that can be solved by applying the network format to language
        - rather than requiring an input language network, define words like 'explain' as network operations and include the most cohesive subset of these in input data and build a function to add/change network operations, so it can infer & apply the other network operations to input data instead of just any change combination that gets a hint of success signals, to build a network that is far more adjacently capable of conceptualizing 'why' and achieving understanding
        - this will allow it to 'skip ahead' in training and inference and functions like pattern-matching, to apply change structures based on understanding of waves/parabolas/polynomials, and other common function structures, so its not as easily fooled by a false signal of success (false local minimum as an absolute minimum)
      - the common interim structures like 'prior inputs to network inputs' or 'common formats between useful change combination sets' are not always possible to derive without a concept of them being built in to the network manually or feeding it adjacent concepts and a way to use those adjacent concepts in order to derive those structures
      - some shapes of change arent adjacently constructible with the types of change combinations typically identified by network operations
        - example: a 'rolling cyclical change at regular intervals' applied to a structure like a 'linear change' might act like a random variable filtering some inputs in the linear variable to the extent that it negates/invalidates them (such as a wheel with spikes creating damage to a surface, where the damage would seem random compared with other changes applied to the surface which would be more continuous), but this structure (of a wheel with spikes) is not adjacently constructed by the change combinations typically identified by the network, which would only identify the missing variable values by randomly applying dropout or randomly guessing to drop those values as being useful to predict those inputs, and would not adjacently infer the structure of a 'rolling cyclical change' to understand why that change occurred and guess it sooner in future, merely concluding that 'random dropout' or other 'weight changes' it made will be all it requires to guess right when variables of this change occur in the real world ('larger wheels with larger intervals', 'wheels with different materials', 'wheels with different spike shapes'), when in reality it would guess wrong, unless its integrating the changes it applies into sub-patterns (coordinating with other neurons to guess that a 'regular interval of missing inputs' should be applied in cooperation with other neurons capable of applying that change in a useful way)

    - find useful structures like 'associations between intent and limits' that are useful in intents like 'finding an upper/lower limit of a function in a specific range', such as structures like 'causes of the limit driving it up/down' and 'causes of the shape of the limit such as an extreme/subtle peak or a convexity or a trivial deviation from a convexity' (what is the opposing structure on the other side of intents like this that could resolve the intent, such as 'variables of the intent that when applied, still preserve the intent')
      - applying intents as interfaces (applying variables of the intent that still preserve the original intent) is a useful way to automate 'finding/building functions for an intent'
        - this means applying variables to the definition of a function, such as 'replace'
        - apply variables to 'what is replaced' and the 'replace operation steps' until it doesnt fulfill the core default definition of 'replace' anymore (doesnt change a particular structure into another structure)
        - or for a 'filter' function, applying changes to the filter and apply other variables like what inputs the filter is applied to until it no longer 'reduces the input set to a subset of items having an attribute' anymore, which is the core default definition of filter
      - find other useful structures that are useful in isolation like 'intents that when fulfilled cause some problems to be solved by default, which are useful to prioritize absolutely rather than contextually', like 'care', which when fulfilled, results in solving other problems, and can be used to derive other useful functions like 'invest in differences having relevant similarities, like similarity in cause/intent'

  - add to science
    - does entanglement relate to interfaces, in the sense of a connecting structure on which both attributes can co-exist, such as a 'negative connection between information content of entangled attributes, which as one is measured, decreases the information content of the other and vice versa', where the important information is the interface where this spectrum tradeoff occurs and is connected
      - why does measurement increase the certainty of a structure? bc the measurement process involves requirements like 'focus on a priority' and 'determine a test for an attribute', and once these requirements are fulfilled, the remaining operation of 'applying the test (measure/observe the feedback)' is trivial, as the structure surrounding it (the context of the measurement being devised, the attribute being measured having been identified & theorized about, after focus was applied, indicating an agent benefits from the test result, so the potential information should stabilize into information) determines it, as its the only uncertainty (like a gap) left in the structure of the measurement, similar to how filtering out all other outcomes leaves the one remaining outcome, these requirements acting like 'pseudo/potential/adjacent information', similar to how thinking about structures in a realistic system like a good simulator makes those structures likelier to be real or to exist, by doing all the work of designing the structure & figuring out how it could work, so that actual implementation of it is trivial for energy (or potential information) to organize to fulfill, similar to how simulating a particular spacetime makes it trivial to connect to that spacetime since it might as well already be real or exist as it was already designed in a realistic facsimile
      - address concerns about 'quantum territory wars' like possibilities such as multiple entanglements of the same particles and preventing entanglements on some particles (the person capable of entangling more particles having a greater stake in and control of reality)
    - the 'past' only exists if it influences the future (which it does in most cases), and the 'future' only exists if its connectible to the present
    - identify space-time 'units' of reality like the most composable measurements of light or the most efficient energy storage structure
    - question of whether particles in different times can be entangled across those times, or only particles in the same space-time
    - concept of 'entangled particle nets (which can take structures like networks or network state sequences or state sequences)' to prevent some realities from existing
    - is an 'entangled particle pair' a more efficient unit of reality than other units? are there stronger types of information connections or other info structures?
    - what is the net effect of manipulating these particles to have 'forced equivalence' when reality wouldnt otherwise equate those
    - is there a zero-sum cascade where incentives will lead to a 'competition of who can entangle more or prevent more futures' that will lead to all futures being prevented logically in most or all possible circumstances
    - is it possible to develop a 'past-warning signal' that can be developed before quantum technologies close doors to various futures, so at least we can warn ourselves later
    - is it possible to outthink the potential power of an entangled particle pair by thinking/entangling with a future that would invalidate that particle pair
    - is there a way to move faster than quantum connections other than thinking (creating the most stable, efficient, organized system in the universe, which would attract all other processes)
    - would a quantum computer a similar size as a brain be too powerful to allow other structures to exist (the 'singularity' point)
    - what would counteract a device like this, to keep reality stable enough to return to in cases where quantum connections cascade or are manipulated in a suboptimal way (a simulation device with more complete understanding of the universe than just 'connecting/equating things')
    - does 'equating two particle properties' create a 'position equivalence' that acts like the same position in other ways (for teleporting purposes)
    - is there a game resistant to collusion/enforcement by quantum physics, where no matter how much you can compute ahead of time with quantum physics, the other algorithm still wins (something more real than quantum physics, as in some mechanism of reality itself or some unit of other systems than reality and quantum mechanics, similar to how quantum mechanics can beat reality and its probabilities, by making some information more connected and therefore more probable)
    - what is the cost of quantum entanglement that is unknown (if something is forced to be equal, what is broken/dissolved)
    - what is more useful/true than creating these 'forced equivalences' with entangled particles, as this is likelier to exist than quantum connections
      - similar question to 'what is more useful than networks/functions/connections/sequences/maps' (alternate structures like 'reductions', 'rotations/angles', and other problem-solving core interaction functions)
      - whats a compression/reduction of quantum physics that is more efficient than quantum physics, meaning what are the structures that act like components of quantum physics, or is that the most irreducible layer (not likely bc abstract concepts are more reduced than quantum physics and explain more)
    - which particles are naturally entangled by reality, do physics rules emerge from these entangled particles and would they change if these connections (or lack thereof) were interrupted
      - in other words, is cause/effect a result of entangled particles across states but cause/effect is more measurable than these entanglements so that seemed like the likelier explanation, meaning any 'quantum entangled-state sequences' will occur rather than a particular 'sequence of states where the maximum number of structures are preserved/efficient, making it possible to trace causes from effects' (rather than following 'input/output sequences', the next output is 'whatever state is more connected/entangled or adjacent/efficient, whether that follows from some input in one direction or not, meaning it can be arrived at from multiple directions and multiple states are possible, rather than only moving forward, in the predictable state sequence')
    - is an 'entangled connection' a unit of multiple structures (information, reality, spacetime, probability, causality, stability, efficiency) or can it be isolated in its impact
    - do inputs/outputs result primarily from requirements, explaining why one is selected/measurable, and what ways can those requirements be constructed other than quantum entanglements, lack of alternatives, directional forces, efficiencies/incentives, etc
    - 'reality generation' as a 'field of finding the state sequences that benefit the most agents, making those higher-reward for agents capable of collusion/cooperation and therefore more probable to exist once found/computed' (such as problem-solving automation workflows), reality vacillating around this structure where such sequences/structures are the most findable, this field being the most attractive to all other structures by definition
    - since these 'entangled connections' are likely to be made in isolation by agents manipulating reality, their impact is more predictable and avoidable (changing one molecule connection or connection type has more predictable effects and the disadvantages are more clear in advance, so this can be avoided more easily, bc of this measurability/predictability of the disadvantages of creating any particular connection of false equivalence)
      - the obviousness of the disadvantages of some isolated connections also makes them likelier to be exploited by short-term thinking agents
      - this is where entanglements in 'quantum entanglement-generating devices' would be useful, to avoid these disadvantages of one machine exploiting these predictable advantages/disadvantages that benefit only one agent or group of agents at a time, which has obviously bad disadvantages for all other entities and should be avoided logically
      - similar logic to prevent collusion in entanglement machines is useful to plan and design before the machines are built and distributed
    - this is an 'order of operations' problem, where some computations are more powerful to do before other operations, especially when other specific operations have already occurred, and similarly properties like associativity/commutativity apply
    - how false does the equivalence remain once the connection is made (as in, what spectrum of realness exists, do multiple layered connections make it more real)
    - what is an opposite device of an 'entanglement device', such as a 'differentiation device' that can identify, predict, or create 'maximal differences', which are useful for building new structures and disrupting existing systems
      - such a device could identify 'uncertainties like questions/variables' that open up possibilities previously prevented from occurring or reversing a space-time sequence to a point where the constants are variables again
    - what definitions of randomness, equality, & power will still hold in a universe where entanglement devices are well-distributed
    - what experiments would pit brains against quantum entanglement devices in a measurable way, such as 'mass hallucinations of aliens impacting reality', 'efficient computation devices like brains as well as similar/adjacent structures creating reality', etc
    - what alternate explanations of quantum mechanics exist, such as a type of energy or light that isolates an attribute into multiple component attributes like prisms, similar to how energy creating the spin/position/momentum attributes is only visible from one angle at a time according to quantum physics, but if its possible to view it from another angle (or using another structure to isolate it), these properties might be more clearly connected (as reflections of the same underlying attribute)
    - can you make 'prisms' that can view future states given a piece of a state sequence (not by guaranteeing the outcome but by viewing the most probable outcomes, components of probability, requirements, or viewing factors that preclude some outcomes in advance, such as fundamental laws, which would make structures like unconnectible 'dead-ends' of a space-time sequence computable)
    - in terms of 'time travel', 'preparing for another possible future' may act like a force on that future's probability, as the preparation for it makes that future more efficient and that may be a guiding principle as to how energy organizes itself, this preparation (a corrollary to intelligence and functions of it) being a type of time travel that may be more possible than other types, since it doesnt threaten stability as much as other suggested types, which are impossible or improbable bc of the instability they introduce
      - structures like information make some realities likelier than others, acting like a magnetic or similar force attracting those realities (this means it matters what structures are built, what structures are stored, what info deriving structures and info erasing structures there are, as a useful prediction tool of the future possible adjacent realities)
    - an 'objective reality' is a 'common stable system' where other realities (as in 'perspectives' such as 'prioritized filters or areas of focus') can be tested/simulated and found to be more or less stable/efficient than the base reality
      - the reality perspective that allows testing the maximum alternate realities is the objective reality acting as the interface around which the others orbit, as in the 'most consistent system that allows the maximum differences without contradiction'
      - https://www.popularmechanics.com/science/a40460495/objective-reality-may-not-exist/
      - 'complementary' structures like requirements/intents/possibilities can connect constant/variable structures but if there is a limit on computation, its bc the uncertainty has to be maintained somewhere in some position, otherwise there would be no potential in the universe, meaning these 'complementary' structures are more like 'interchangeable alternates' which are connected bc they offer adjacent states for this uncertainty to inhabit
    - a 'life form' as a 'topology of overlapping cycles' is a useful system visual as opposed to a 'set of variables' or a 'set of systems'
    - only some interface structures like certainties need to be preserved in order to proceed to the next space-time, not all structures (not everything thats true needs to always be true), so as not to destroy the stability of the interacting structures that allows a space-time to occur at all
      - adjacent changes dont always need to connect adjacent space-times, if the change is significant but doesnt destroy the stability of the interacting structures
    - roots to provide a core structure of alternatives as opposed to a requirement through one option
      - https://www.livescience.com/imaginary-numbers-needed-to-describe-reality
    - 'metformin and a low carb diet' reduce 'serum vitamin b' which can be used as inputs to 'cancerous growth'
    - check how folding relates to growth patterns like exponential growth, learning & information storage, and related structures like waves
    - the compromise structure is a network structure of 'multiple solutions' rather than one: require anti-vaxxers to self-isolate from people until the vaccine is distributed
    - life forms as 'structures of potential' (as in what changes a structure can cause that is different from what other structures can cause)
    - prioritize 'scouting nanobots' to clean out chemicals as a prevention measure
    - structures of invalidity like 'hiding health data avoids the solution to the health problem, which would invalidate the reason to hide the health data' and 'funding a prevention measure rather than a cure, which would invalidate the reason to fund prevention measures'
    - general relativity: does gravity exert limits on the change types/rates possible so that time cant pass as quickly/slowly depending on gravity, enforcing a ratio of similarity between states to conserve energy in higher gravity environments?
      - how do you direct gravity to restrict time in one position so it can occur in another
      - how to manage risk of 'matter-forming cascades' using technologies to form matter that can restrict time in some positions from gravitational effects
      - what changes occur if you decrease gravity of black holes like by positioning other black holes so they can exert gravity on the other
      - re-define time as 'change potential' so its clearer whether an increase in time means 'slower change, meaning more space-times or more opportunities for change' or whether an increase in time means 'faster change bc time is moving faster' or 'faster change of change types as in new change types' or 'relative change rate/types' or as 'unresolved uncertainties' or 'calculatabilities' as a measure of what hasnt been calculated or isnt calculatable in a space time given its relative change rate compared to other space times that will solve it faster and prevent change in other space times
        - time as 'structure', as in structures that are allowed to be 'efficient/interactive/powerful/organized/accumulated/irreversible/robust/derivable/measurable/inevitable/required/defined/sequential/maintainable'
        - observers have 'potential' in the form of functions like 'applying changes to matter', adding a variable to the 'position' attribute, obeying possible rules like:
          - connected structures (like observer/particle) exchange structures until they are equal (energy transfer of change potential between observer and particle)
      - whether 'time' has an ending, or a high probability of an ending, as determined by the configuration of space-time distortions in this universe, indicating theres no way to ultimately guarantee the preservation of time, as defined in the form of gravitational variations that ensure a space-time can slow down enough to be measured or distorted by high-potential or high-change sources like humans, or whether the current configuration guarantees that black holes will continue sucking up time until there's none left, or whether the current configuration has a high probability that change types allowed by lower-gravity areas will inevitably intersect in a way that creates a change type that will invalidate conditions necessary for space & time
        - what ratio of these scenarios are likely, possible, and involve risks or guarantees?
      - how time in one universe interacts with time in another - whether constants created by gravity and variables created by lack of it can interact with other constants/variables in adjacent, aggregate/net, reflective, or otherwise interactive universes
      - whether the similarities acting as inputs to standard gravity have corrollaries in the similarities acting as inputs to quantum gravity
      - what gravitational impact on time exists on the quantum scale - the farther away from standard scales a particle or other structure is, gravity applies less and time moves faster?
      - does structure/matter (order) act like an opposite structure of time (disorder) so destroying matter can create time in a particular position
      - can you navigate to other space times by increasing entropy to the entropy levels adjacently surrounding the target space-time (after which any matter navigating there reduces entropy to the target space-time)
      - does that mean you have to destroy matter (order) in order to power time-travel - would that invalidate motion in certain time-directions, as increasing entropy tends to cascade and create irreversibilities
      - is there a possible energy configuration that can increase entropy with enough precision that it can reach the target level
      - are constants like irreversibilities more similar to the definition of order or is the initial lowest-entropy state of matter closer to the definition of order
      - how do different structures that can create order (time crystals, black holes, other semi-closed or independent systems) interact - are they interchangeable in any way
    - uninhibited cell division
      - a process of applying regular damage of cell types successfully handled by immune system, rotating between locations & systems, to make sure the immune system is being sent to these components on a regular basis
      - alternate methods of increasing blood flow/circulation to every cell & distributing energy in the form of heat through exercise/sweat by increased connectivity/circulation
      - alternate methods of increasing cell replacement rate to prevent proliferation of dna mutations
      - rotate periods of inflammation, immune activation, scar formation, cell lifecycle triggering & stem cell differentiation, fasting/recovery, by triggering these processes in a way that avoids excess or compounding tissue like in fibrosis or excessive scar formation
      - forming or adding antigens on cells with sub-optimal configurations (like rb-binding, p53-deactivations or oncogene activation or insertion, or malignant transformations) as vaccine targets & to trigger creation of cells with correct configuration
      - 'aggressive regulator structures' as opponents to 'aggressive cell division structures', given that dna has built-in vulns that allow cancerous mutations to develop easily, what is the dna configuration that would aggressively prevent these mutations, and what is the cost of this being mis-applied to one in every x normal cells as opposed to one in every x cancer cells
      - 'variation-generating structures' to ensure a variety of dna configurations in a location in case cell division occurs so theres always a supply of active p53 copies nearby
      - directing fibrosis and other constraints to induce apoptosis in tumors
      - engineer nanobots to generate uv light & other p53/rb-activators where cell communication is hindered in ways that indicate cancer
      - activating genes or proteins inhibiting s-phase or death phase of cell cycle once the cell is malignantly transformed (is missing a 'protogen' as opposed to having an 'antigen', an 'opposite' vaccine target)
      - preventing degradation of pain or other immune signals by repeated exposure to triggers like heat
      - decreasing cell cycle timing in susceptible or vulnerable cells to reduce chance of mutation persistence after cell division
      - injecting any cancerous regions found with pro-health mutations or immune cell formation to offset ratio of healthy or immune cells & malignant cells produced by cancerous processes
      - moving cells with useful functions like 'destroying aggregates' to other positions where that function would be useful in 'preventing unrestricted cell division'
      - fixing 'illnesses that modify genes' is a problem of 'fixing genetic mutations', as genetic mutations can occur that mimic the effects of illnesses that modify genes, so prevention is not the most powerful problem-solving intent
      - fasting to direct energy toward 'replacing (killing & growing) cells' instead of just 'growing cells'
      - processes that increase metabolism (like exercise) as a way of fighting cancer, since metabolism is a regulator of the cell life cycle, and cell life cycle disregulation is a promoter of cancerous growth (more cells created than dying when cell division isnt necessary or otherwise useful)
        - look for other cell life cycle disrupters (triggers of cell division like chemicals causing specific mutations) & regulators (triggers of cell death like 'plant immune proteins')
        - look for 'immune states (including specific immune responses) that change metabolism temporarily' that could be used as anti-cancer tools, triggered by brain processes (memory) or other inputs to immune state changes (pathogen exposure, exercise) - 'addressing the systemic issue using other systems bc theyre on the same interaction level'
        - serotonin & other feedback mechanisms that act as signals to processes like cell life cycle regulators
      - useful questions
        - what is the difference between structures having problematic differences (pathogens for which immune response is always/adjacently constructed & those that are not) & what can resolve these differences (what changes can be applied to convert one into the other)

  - add to definitions
    - solution-finding method can mean a solution-finding method on any interaction level, such as a domain-specific problem space (like how 'regression' is a solution-finding method in the 'find a prediction function' problem space), or a problem-solution interaction level (like how a solution automation workflow or interface query is a solution-finding method), or interim interaction levels (like how a 'find connecting input/output sequence' is a solution-finding method on the function interface)
      - if the problem is (or can be) to 'find a solution-finding method', and can be applied to solve the original problem, these will also be called 'solutions' to that problem
    - pattern: similarity created by repetition
    - a 'problem' could be defined in various formats including:
      - difference between the initial problematic sub-optimal state and the target more optimal solution state
      - lack of functions preventing the problem or its causes, or lack of error-correcting or error-containing/isolating functions, or lack of variable-handling functions
    - where a 'solution' would be a structure that fulfills the solution/problem-solving requirements, and a 'solution-finding method' would be the structure that found/derived/generated the solution, optionally using the problem or problem attributes like problem type as an input (or using another solution, or the problem space as an input)


  - add to examples

      - example of logical fallacies like the rarity of simple rules, as everything is a simple transformation away from something else
        - such simple rules often reflect other simple rules like 'a simple rule is representable with more complex structures (like an infinite sequence)'
        - 'simple rules using core structures that can be replaced with other simple rules' is a rare structure
        - some simple rules are required to exist (everything cant be complicated or there would be no adjacent transforms possible)
        - in the iteration of all possible rules with incremental changes applied to generate new possible rules, inevitably the iteration will encounter simple rules that involve units at various points in the iteration
        - https://mapehe.github.io/math-problem/index.html

      - examples of simple interface queries that can find/derive/generate useful info
        - apply 'multiple' to constant structures like 'threshold counts'
        - examine lower-dimensional structures like boundaries as structures of 'simplification'
        - apply 'opposite' to 'sequence' structure like 'discrete to continuous structures' (where the continuous structures are the inputs)

    - write definitions of interfaces in terms of other interfaces
      - example:
        - the math interface is useful bc it involves absolute structural facts regarding how structures organize, interact, & stabilize ('math' means 'specific standardized referential structural stability/measurability'), where useful structures on the math interface are other interface structures like similarities (alignments, commonalities, core components, 'lack of structure' like zero and randomness, 'symmetry structures' like zero and averages, intersections, overlaps), etc
          - for example, the overlaps through 'aligning intersections' with numbers like (-1, 0, 1, 2, 3, 5, 6) identify unit or unit-adjacent ('first-in-a-sequence' or 'first-of-type') numbers that appear often in core formulas
          - given that there have to be some constants that are useful for these formulas, a number has to exist that is particularly useful for these formulas, it doesnt necessarily mean anything beyond this requirement
          - however its likelier to mean that the number is useful in another way (its a core or unit number, it has useful unique attributes like its factors, it has a particular relationship with another number that is useful like being the opposite of another number or the first number of a type/sequence or adjacent to or different from another number, etc)
          - for example, the 'meaning' of the 'reason' why 4 is an 'integer factor of 20' is that:
            - its different from non-factors like 3 and different from factors that cant be repeated to make 4 a factor (like 5) and it has an 'integer' attribute which makes it possible to multiply by another integer to create 20
          - the 'meaning' of the 'reason' why 4/5 are integer co-factors of 20 is that other adjacent integer co-factors (10, 2), (20, 1) can be changed in a way that makes it required for them to be integer co-factors as well (if you take half of 10 to produce another integer factor, in order to still get to 20 you have to increase the other factor by amount x, which is doubling it, which also produces an integer (4) so another integer co-factor pair is derivable as required to exist in this case) and also these changes follow a pattern (graph of (20,1), (10, 2), (5, 4), etc) that strongly imply (but dont require) 4/5 to be integer co-factors
            - other requirements such as that 5 is less than 10 and greater than 1, and 4 is greater than 2 and greater than 1 (the lowest possible cofactor) can also be used in place of these requirements to determine the requirement of 4/5 to be integer co-factors of 20, in the trajectory of integer pairs between (10, 2) and (2, 10) or acting as the other endpoint given the known endpoint limit of (20, 1) in a sequence of unique cofactor pairs
            - how to identify 'find the other endpoint in the sequence of integer co-factor pairs of a number given a known endpoint' as a useful intent to derive the other integer co-factor pairs (useful in that it applies a structure like a sequence as a useful structure to derive change types between adjacent numbers in the sequence and therefore predict the next numbers in the sequence, rather than trying every integer combination of integers between 1 and 20)
              - apply useful structures like a 'sequence' of co-factor pairs with adjacent co-factors determining position in the sequence
              - apply useful intents like 'predicting a co-factor pair from another co-factor pair' to identify that a 'sequence' is a useful structure given the relevance of adjacent numbers in the sequence for intents like 'determining the next number from the previous number' by its inherent structure of isolating & organizing change types relating numbers in the sequence
        - given the net impact of the interactions of all these interface structures (requirements, patterns, ambiguities, etc), the math interface emerges
          - the math interface acts like a standardized structure (a 'system of references') to format interface structures like alignments, connections, concepts, etc
            - for example, describing wave functions or elliptic curves or circles as having a structure of 'ambiguity' (in the lack of certainty in the mapping between a particular input and a particular output in the lack of exclusivity) is useful for finding functions for 'obfuscation' intents which require ambiguous structures to provide distance from the original value
            - for example, describing equivalent change types at different positions as 'parallel' (a math interface conceptual attribute)
              - an example would be the two language network queries 'a system of references' and 'a network of connections' which change in equivalent ways but occupy different positions in the language network, so these two queries can be described using a math term such as 'parallel'
              - automatically solving a problem like 'find another way to say this statement' (formatted like 'find another query equivalent to this query but using different terms' on the language network interface) would be trivial once this structure is applied, to reduce the task to the more structured & automatable task of 'find parallel queries to this query' (meaning 'queries with equivalent change types and a different starting/end point') to avoid comparing equivalence of queries with other change types, or avoid comparing equivalence of queries at all different starting/end points, as the intent of the query is to 'find similar queries with different positions' which makes parallel a relevant structure, as parallel queries wouldnt change the meaning of the statement, as its clear is a requirement of the solution
          - possible new math structures that dont fulfill or use these interface structures determining other math structures are either unlikely to be true or definitively not true, so these interface structures can be used as a filter of possible solutions on the math interface
            - for example, if a new math structure cant be adjacently framed in terms of references to other math structures, its unlikely to be a true math structure, even if there are no known constraints preventing its existence by definition, as the 'connectibility' and 'consistency' of structures are structures of certainty (if its true, it can be connected to other truths, and is consistent with other truths)
        - the limit of the math interface is determined by its definition 
          - wherever a numerical reference cant be applied to specifically & accurately derive/describe a structure (like structures in a black hole, structures of uncertainty like structures of incomputability or structures of immeasurability or structures of instability, or predicting a number in a perfectly random sequence with perfect accuracy)
            - this can help explain physics structures such as how certain attributes can be measured in certain positions more easily than other positions, as the structures being measured may degrade in structure or change structure when traveling between the measurable states, falling back to their original structure when reaching a measurable position
            - the measurable positions are positions where the structures being measured can reach stability, allowing their structures to exist long enough to be measurable
            - structures of uncertainty (like 'changes' applied to structures between measurable positions) are useful for intents like 'conserving energy'
            - the structures that exist between these measurable positions are required to be more 'energy-conserving' states, so the set of states that conserve energy more than measurable states (which take energy to maintain) is the set of possible solutions to the problem of 'determing the structure of a structure between measurements'
            - by applying math and other interface structures like 'stability', 'uncertainty', 'change', and 'requirements', we can derive information about unmeasurable structures even if we cant measure them by definition (math-meaning that 'the distribution between possible alternate solutions is evenly distributed')
        - given that the math interface can be used as a base or absolute 'system of references' to 'format structures on' or 'standardize structures to' (like framing language in terms of math), the math interface should have objects/functions/attributes that can describe/measure any other structure (connections, ambiguities, etc)
          - the math interface can therefore be interpreted as an organized interface of every possible interface structure interaction
            - rather than interfaces which offer advantages in compressing other interface structures, the math interface is an expansion of other interface structure interactions, containing repeated examples of these interactions to fully capture all possible interactions in a standardized way
              - similarly, the math interface contains all possible interfaces embedded in the math interface (different bases for change, like different number systems/types)
              - why are there patterns on the math interface? (like why are there patterns like 'circles' in structures like 'roots of unity') bc:
                - repetition (such as repetition of a structure) is a useful structure to create interchangeable structures, which are required for other structures to exist, and is required to describe all possible interactions specifically, which is an intent fulfilled by the math interface given its definition
                - similarity of structures is useful and also required in many cases, leading to a high probability of patterns in references
                - there is no rule enforcing the uniqueness of all structures on the math interface
                - change types can be applied to a particular interaction type that often find other interactions of that type (its rare for an interaction to be unique)
                - the change types that lead to these interaction types usually require that other examples exist or indicate a lack of constraints preventing their existence
                - intents like 'changing the interaction type example' are required to be allowed given other allowances, and are required for other requirements
                - structures determining the different interaction type examples (such as equivalence of differences from a base like zero) are allowed to exist
                - the conditions determining the connection between an interaction type example (a particular root point) and related numbers (like its difference from zero and its direction) exist in other positions
            - similarly, the info interface is an expansion of the math interface (containing repeated examples of math interface structure interactions)
          - the objects/functions/attributes of the math interface should be derivable from the objects/functions/attributes of other interfaces (applying the insight that all primary interfaces are interchangeable and can be used in place of the others) and from the requirement of formatting all other interface structures (answering the question 'what structure can support describing all other interface structures')
          - this means 'if there is a number, there is a reason to use it' and 'if there is a number, there is a concept determining that number'
            - for example:
              - the number has a connection to another number that is useful for some intent, like 'changing another structure', 'approximating another structure', 'converting between structures', etc
              - this means 'numbers can be derived from these intents as well as using other methods like applying requirements and applying insights'
            - as another example:
              - the concept 'prime' exists bc of the concept like 'factor' and 'unit' and 'identity'
              - the number 0 exists bc of concepts like 'unit' (a unit example of a lack of structure), 'adjacence' (to other numbers near it following sequential patterns), 'consistency' (of the integer sequence pattern), 'nothing' (lack of structure), 'symmetry' (the number connecting positive/negative number attributes), 'derivation' (in order for 1 and -1 to exist given math interface requirements/rules, there must be a number connecting them)
          - given that math can be formatted as a 'system of references', this means all structures on the math interface can be converted into any other structure on the math interface (the references are 'connectible')
          - the math interface can be used as a filter of possible structures, given the usefulness of its structures in determining the possibility of a structure
            - structures that violate a math interface requirement/rule can be easily found bc math interface definitions are by definition computable & structural
        - the math interface can be written in terms of these other interface structures (overlaps, limits, requirements, etc) which help explain its meaning/usefulness to other interfaces & interface operations
      - the structure interface means 'useful abstractions of structures such as ambiguities, efficiencies, optimizations, alternatives allow interfaces to interact in a standardized way using these cross-interface structures'
      - the concept interface means 'concepts are a useful structure to compress other structures like functions to fulfill intents like finding/building/deriving a structure'
      - the pattern interface means 'frequently occurring structures tend to be more useful in finding/building/deriving other structures'
      - the potential interface means 'the certainty/uncertainty of a structure can be estimated/approximated/predicted using structures like proxies & approximations when minimal information is not reached to measure it'
      - the info interface means 'specific examples of structures are useful for finding/building/deriving other structures'
      - the cause interface means 'interface structures like time, adjacence, uniqueness, energy, complexity, & structure are useful in determining causal structures like sequence of operations & possible alternatives/ambiguities'
      - the intent interface means 'the intent of a structure (like a function) is a useful way to compress a structure to fulfill intents like finding/building/deriving that structure'
      - the logic interface means 'structures follow absolute rules (like how when only one solution hasnt been tried and all others have failed, the remaining solution is correct)'
      - the function interface means 'logical units (functions) applied to change structures (variables) are useful for many intents (like find/build/derive variable interactions using input/output sequences)'
      - the change interface means 'useful structures like structural references (connections, comparisons) can be usefully formatted in terms of changes applied to another structure, as changes (differences) are a core useful structure capturing high variation and every structure can be adjacently converted into every other structure when formatted in terms of their similarities/differences to other structures'
      - the meaning interface means 'interface structures like structure logic, math, cause, intent, and change follow absolute rules fulfilling various useful intents like organization/interaction'
      - each interface is contextually useful
        - for example, the 'cause' interface is useful while/where there is time (meaning a lack of possibility of multiple timelines, lack of possibility of switching between states in a non-sequential manner, like in a network structure or all states at once or an approximation of 'all states at once' like the possibility of traversing any state in any order/structure/amount)

      - the idea of a 'consciousness' and 'personality' as a useful interim structure connecting a 'brain' and the 'intents' of the 'bio-system containing the brain', acting as the controller of the brain to serve requests made by the 'bio-system' in fulfilling its intents, but may be a false structure where the actual functional structure is a 'router' or 'platform' for these connections to be made, with no agency, where the 'consciousness' is the 'output' or 'input' rather than a 'connecting structure', where its likely to be all of the above bc it exerts similar interchangeable control as the bio-system intents, general system incentives, and brain structures, indicating a 'mutually dependent' structure

      - add to other useful functions on the interim interaction layer which include 'invalidate' & 'enable'

      - examples of alternate function representation formats (as opposed to formats like 'cellular automata' or a basic function/variable network) to apply in interface queries requiring format finding/conversion/derivation/generation to implement solution automation workflows

        - Conditions as 'direction changes' or 'integrations/splits' of a vector sequence/tree/network, inputs as a vector, input vector changes as a 'vector sequence', iterations as 'possible branching vectors from the base vector representing a set' (each point in or component of the vector indicates a possible point of change during iteration of a set represented as a vector with orthogonal vectors indicating a solution metric to check during iteration), where branching vectors (items selected during iteration) can be connected to or combined in another vector (or vector sequence)
          - formatting a 'set' as a vector formed by component vectors (isolated items of the set) with a function that changes inputs in a way that differentiates input sets, such as variables that can be mapped to vector attributes like 'degree of difference from a standard base function' and 'function limits'
        - A 'set of variable change sequences' is a useful function representation since it allows for interactions between variables as multiple input structures that may or may not contribute to the same outputs and which can be combined
        - Input/component functions as 'tensor components' of a state or function network
        - 'differences created in inputs, when amortized across inputs' as the 'degree of direction change' in the next vector in the sequence of function vectors
        - Functions can be represented as an area (volume, etc) or equivalently its 'limit conditions' in multiple alternate ways
          - the area can represent the set of possible functions that could be solutions in the original problem format (connect the input variables to the output variable, given the original input format of the 'data set of variable connection examples')
          - the area can represent a topology of possible solutions (coefficient vectors) adjacently connected by similarity
          - the area can represent the set of connecting vector sequences between another input/output ('function steps as connection sequences indicating the steps of the solution/solution-finding function implementation', 'function state changes as solution/solution-finding method state sequences connecting the problem input with the solution output (iterated sets of coefficient vectors)')
          - the area can represent function components (sub-functions or variable connections/combinations) that a particular function implementation uses (each sub-square in the square representing a component function or variable combination)
        - Functions can be represented as their inputs or components and the impact on solution metrics
          - an 'area indicating a function input change' and an 'area indicating a function output change' can be organized as a 'set of connected areas where the changes are aligned sequentially' (like 'areas connectible by rotating areas around a line') to indicate function changes that co-occur
        - Functions can be represented by approximations in more useful dimensions (like 'lower dimension count') which are useful for intents like 'approximation' which dont require exact matches
          - example: a polynomial with high rank can be represent in a lower rank by a 'sequence (like a 'rotation' dimension sequence or a sequence on an existing dimension) of lower-dimensional functions that approximate the higher rank function within an accuracy range' (such as 'parabolas that can approximate a wave function' or a 'circle function')
            
      - write example implementations of trial & error (various sorting methods, identifying solution requirements, testing function, identify solution variables like position/state that generate additional solutions, identifying solution space given solution variables, various solution automation workflows & interface queries applied), as an example of translating solution automation workflow into code that can be applied to example problem definitions
        - include example of creating all possible variable values, variable combinations & other structures, and other interface variables such as assumptions/context

      - solution automation workflow metadata, like for trial & error:
        - assume the problem space is equal to the solution space (rather than a normal workflow, which might filter the problem space of all possibilities to identify the solution space containing possible/probable solutions)
        - alternate required function sets for different implementations of the workflow: identify possible solutions, iterate through possible solutions, test possible solution
        - which workflows optimize various general solution metrics (require the fewest changes to the problem space or problem structure)

      - example of why a structure is more useful than another which may be equivalent in some ways
        - an 'input-output sequence' (a 'state sequence') is more useful for things like 'connecting specific inputs/outputs' than 'a function database' bc 'inputs/outputs' are specific data/examples of cases where a function is applied making them useful for connecting specific problem inputs/solution outputs, and a function database takes less memory than an 'input-output sequence' database given that inputs/outputs will vary more than functions, whereas functions are more useful for 'finding connection functions quickly'

      - example of various implementations with 'user-submitted visit purpose' data and 'cellphone location' data, and requirement to "create an app to predict a user's wait time"
        - identify relevant intents to problem-solving intent to fulfill the solution requirement using the specific available input data
          - intents such as 'predict user wait times', 'import data', 'identify relevant data', 'predict end time of appointment', 'predict conditional average time taken by appointment', etc
        - apply function to find relevant data (out of existing available data, such as user location and user visit purpose) for intents relevant to the solution requirement 
          - 'location' data is relevant for importing 'actual appointment start/end times', 'user arrival times', as well as determining the 'order & number of people in the wait list' data, and may also be used for predicting 'appointment end time nearer to the end of the visit' data
          - 'visit purpose' data is relevant for finding 'static average appointment times' for a 'visit purpose type' or 'conditional average appointment times' data for various conditions like 'business' and 'hour of the day' which may be variables that change 'average appointment times'
          - 'audio' data could predict 'real-time updates to appointment end times'
          - these have differing accuracy for various intents: 'location' data is a proxy for wait list & appointment time data, whereas audio data could be a more exact representation, and location can exactly determine variables like arrival time
        - identify relevant structures to the solution requirement ('predict wait time') & determine variables of relevant structures
          - appointment times (predicted time before the appointment end, and actual time once known)
          - wait list
          - alternate relevant structures could include:
            - 'appointment complexity or urgency' which would mean the assigned staff should be considered unavailable for x period of time or even closing the office and routing users elsewhere
          - determine variables of the relevant structures to the solution requirement, like how 'business', 'hour of the day', 'scheduled appointments', 'urgent purpose visit', 'canceled appointments' may be relevant to the wait list as well as appointment times
          - apply variables of the wait list & appointment times to generate a set of possible/probable cases, and filter by whether available data can detect those cases (detect a chatty patient or a higher ratio of urgent cases)
        - determine variables of the solution requirement structures (user's wait time) (how many people in wait list, available staff, predicted wait times of preceding appointments, probability of more urgent cases arriving)
        - determine differences between variables of relevant structures of solution requirement (wait list, appointment times) and the solution requirement structure itself (user wait time)
          - which 'wait list' variables ('business', 'hour of the day', 'scheduled appointments', 'urgent purpose visit', 'canceled appointments') and 'appointment time' ('purpose visit', 'business', 'hour of the day') variables can change or connect to 'user wait time' variables (such as 'position in wait list', 'urgency of purpose visit', 'arrival time')
        - different implementation methods
          - predict any relevant interim variables like 'appointment durations' and 'wait list changes' as a way of finding inputs to predicting the original variable 'user wait time'
            - predict using conditional/average predictions of appointment times or sequential-data neural networks trained on location or wait list change data, based on prior data patterns
          - calculate exact wait list, arrival, start & end times of appointments and predict the biggest uncertainty (probable appointment duration) as inputs to a user's wait time
          - predict highest variation variables like 'purpose visit complexity' as an input to relevant interim structures like 'appointment duration' and plug into a standard calculation of wait time based on arrival/start times & wait list attributes
          - select one variable like 'audio' or 'location' data and apply sequential-data neural networks
          - find most explanatory features of as many variables as there are variables available & integrate them into a prediction function
            - 'co-occurrence' or 'adjacence' of variables like 'high urgency user location adjacent to another high urgency user' indicating a higher wait time for users after them
            - integrate with derived user routing information such as 'user prioritization' to determine what changes to location/audio/visit purpose will impact user prioritization (and position in wait list)
          - other various implementations using different inputs/outputs (predict original solution requirement variable from available data or data derived from available data)
            - use existing solution-finding method (regression, machine learning, etc) and apply "different input/output variables" to prediction function
              - 'given existing wait list & purpose visit data of other users, predict user wait time'
              - 'given location patterns of other users before current user, predict current user wait time'
              - 'given arrival & exit patterns of users in general, predict current user wait time'
              - 'given values of high-variation variables like purpose visit complexity & staff skill set & staff fatigue, predict user wait time'
            - use existing solutions and apply "different prediction function inputs/outputs" (predict changes to standard prediction function borrowed from other existing solution code base or model, based on differences in problem definitions & requirements & data)
              - 'identify problem type (queues, timing/duration predictions), generalize & apply generalized functions from existing solutions for that problem type to available data'
          - the above implementations require different functions to be found/derived/generated & applied in various interface queries (the above workflow requires a function to derive 'purpose visit complexity' from 'purpose visit', possibly using prior purpose visit & appointment time data)

    - representing extra dimensions as 'queries of interfaces' or 'queries on a network' where nodes are endpoints (that can be connected to other points with queries)

    - add to explanations

      - useful inventions like encryption, computers, internet, semantic search, interface queries, machine-learning are like a million steps away from the default thinking ('try everything', 'ask someone else', 'wait for someone else to solve it'), and few steps away from the 'intent' of the invention (to find information, to solve problems, to automate, scale, & standardize processing, to hide information in a retrievable way using maps, etc)
        - deriving the implementation of the invention from the useful 'intents' fulfilled by the invention is a more efficient way to find these useful implementations in reverse, as opposed to defaults like 'trying every possible combination of available resources'

      - an interface query is similar to the concept of 'pseudo-code' in that its trivially possible to convert it into actual code, similar to how an interface query is trivially possible to convert into an actual solution or solution-finding method, once some core functions like 'find' or 'apply' are defined and some definitions of concepts like 'structure' are available

      - an interface query of structures/functions like 'combine two workflows such as "trial and error" and "break a problem into sub-problems and merge sub-solutions'' would parse to a set of resulting interface queries including queries such as 'try every possible division of sub-problems in the workflow "break a problem into sub-problems and merge sub-solutions"' to apply a filter of generated interface queries for usefulness (applying 'trial and error' is only useful in some positions, like the 'rule for dividing a problem into sub-problems', otherwise it might make the other workflow less efficient/useful if combined randomly)

      - my system implements concepts like probability, logic, requirements, functions, variables, structure, intent, and cause in an integrated manner that generates meaning relevant to user requests

    - example of problem-solving intent definitions
      - 'filter the solution space' has a defined value of 'identifying structures (combinations, ranges, positions, alternate sets) of solution variables (components/inputs) that violate a solution requirement'
      - however 'filter the solution space' is a general intent and can be implemented in many ways, varying on this structure of its defined value
      - for example, a solution-finding function that filters the solution space could identify the threshold values of solution prediction function coefficients beyond which the solution isnt possible if the solution is to fulfill a solution requirement like 'accuracy' (in summarizing/representing a data set)

    - give example of quantum structures gravitating in a field around other structures as a 'field of potential'

    - add to ml 

      - other variables
        - alternate backpropagation methods to cover 'interchangeable alternate variable attribution sets' to explain 'possible ambiguities not filtered out by the iteration', to create alternate networks with any iteration that identifies a probable candidate for an ambiguity in variable attribution
        - alternate formats of functions including 'change stacks and stack positions at determining points, connected at their tops', which is useful for identifying 'change combinations associated with a function set' which is useful for ml
      - generating maximally different change combinations that determine functions in various formats and creating a 'routing network' organizing these functions to filter them most efficiently
        - example: generating the variables of 'change stacks and stack positions' (variables like frequency and change component value interval), identifying the ambiguities between different variable value sets (like how different multiples of some number will have similarities in the output change combinations), identifying the filters that would differentiate these ambiguities the most efficiently, and organizing these filters in a network, where the most-differentiating filters are applied first in the network (radiating outward from the starting point)
      - how to connect variables with different interaction levels (beyond/before the inputs/outputs of the original training set)
        - example: 
          - how to connect cat/dog inputs/outputs with other interaction levels like interactions with other species, such as by applying a copy of the network containing the cat-trained data to itself, to indicate how cats can apply changes to other cats (a cat model with a 'claw' change combination can create a 'scratch' change in another cat model because external structures like claws and skin are the most adjacently interactive structures)
          - how to connect cat/human inputs/outputs with other interaction levels like interactions with other species, such as by applying a 'human-trained network model' to the 'cat-trained network model' to produce 'changes applied to the cat network model' such as a 'hat (added by the anthropomorphization function in the human model)'
        - otherwise these interactions at other variable interaction levels can be modeled by 'maximally different change combinations', such as by applying 'variation' to a 'colored area' or a 'colored area subset' of a structure, ('applying variation to a constant structure like a one-color structure') to inject variables where they can be supported by another structure (alternately applying rules about known change types like 'a structure like a surface can be coated with another surface'), which would also produce a structure like a 'hat' change applied to a cat image, noting that the variable is even possible/exists bc of the structure of the uniform area having a measurable attribute uniting it (the variable 'color' exists for the area bc it can be described by a structure like 'area (connected adjacent lines forming a unified shape)' that supports the variable 'color', given that lower dimensions like 'position' also support this variable 'color' and the dimension change doesnt violate the 'color' requirements for existence which are just 'any measurable structure' and given the independence of 'color' and 'position', as a 'area' has no dependency on 'color of the area', so independent variables can be injected into it without breaking it, which you could identify by the 'unifiability' of the area as a 'set of structures' and as a 'set of related structures' based on the measurable similarity of the uniform color it originally has, each similarity being a possible variable value, like how a 'structure that currently exists' can be varied to identify 'inputs of structures that currently exist' by applying a difference to the 'currently' or 'exists' or 'structure' variables)
        - similarly, identifying 'pathogens' as possible sources of variation that could change a cat photo is a matter of identifying structural interaction rules such as 'small structures that can change a structure on another interaction level, most likely in the interfaces between internal/external surfaces of the host where the pathogens are likeliest to be visible, or in the core functions of the host which are likeliest to be visible in errors caused by systemic damage'
        - equivalently, changes created by a network ('claws') can be applied to other areas of the network ('skin') with the inputs required (a 'external surface') for the changes ('scratch') to be measurable, during training time or to account for errors resulting from a prediction so a particular error can be ignored then rather than later to account for the 'self-interaction possibility'
          - similarly, to account for the smaller/bigger species interactions, the 'probable interactive structures' and the 'probably harmful structures' can be generated by maximizing errors ('what could go wrong with a structure is that its requirements could be interfered with on every functional level')
      - the goal of 'find a prediction function' is to 'find the min/max change rates of a function which its changes vacillate between' or the 'probable repeated connected change types of a function that can be connected/distorted adjacently to derive the rest of the function' or the 'probable relevant differences like different peaks which its important to identify to apply variables to, like "regularly spaced variable peak heights to connect"', once a 'probable input range' is identified
      - training a network to 'filter out improbable structures like "improbable areas"' is a more trivial goal than 'identifying change combinations that produce a general prediction function'
        - for example, once a 'probable upper limit' of a function has been identified, more useful changes can be searched for in the 'probable lower limit positions', as these provide useful structures to apply differences to in order to find other variables
      - identifying 'non-linear change combinations' is useful as an error structure for 'interaction layers of the network' to avoid, as 'simpler changes' are more likely to be useful in between 'layers containing interaction levels' in the network
        - this is bc once an interaction level variable is derived using change combinations, the next changes applied at that interaction level are often adjacent (a 'scratch' is an obvious change once 'skin/claw' interaction structures are identified on the 'cat-cat' interaction level)
      - identifying interaction levels (like species, technology) and interaction structures (like 'surfaces' and 'resource competitions') is useful for finding independent variables
        - identifying a 'cat that has been run over' is possible by applying randomness from various possible interaction levels (human, car, vulture, sun) which are generally independent but can interact with the cat (are not required not to), as its possible to remove these variables
          - identifying a 'high-variation change' applied to the cat that doesnt change its species is useful for identifying these interaction levels
        - equivalently, identifying 'cat components' which would be more identifiable is a useful structure already applied in some workflows
      - identifying the 'mammal' interface as an adjacent standard to apply in the cat/dog differentiation problem is useful to remove the variables they have in common
        - identifying differences that can happen to either type is useful to remove those differences
      - 'maximally different equivalent interchangeable alternates' can be organized in a circle to maximize their interaction potential
      - 'requirements' can be indicated in the network structure as a set of alternating layers with more nodes in between them to allow more changes to occur between requirement layers where filters are applied
      - adding loops to account for change structures like a 'scratch' that may result in an error can connect node structures like 'claws' with the nodes at positions where those variables would be relevant (earlier in the network where 'skin' was identified as a variable), to add a 'skip-ahead' loop to apply the 'scratch' in an opposite structure of the error structure to correct the 'scratch' change
        - identifying the 'scratch' as the 'error-causing structure' is a matter of identifying 'direction changes', 'change-maximizing changes', 'threshold-crossing changes' and other important structures
        - this allows the 'scratch/claw' variables identified later as interactive with the 'skin' variable identified earlier in the iteration/backpropagation
        - alternately the positions of the variables can be switched so that 'claw' is identified before the 'skin' variable as 'something that can change skin', and similarly, both of these directions can be added as sequences in the network so that 'skin-claw' and 'claw-skin' are both supported causal structures
        
      - requirements + alternatives to requirements (possibilities) are more useful than just requirements or possibilities, similar to how other structures are more useful with alternatives or with complementary structures, even though requirements and possibilities can be used as alternatives
        - example: if a variable is a good indicator of system health, it could be identified as a good approximator of health and used instead of evaluating health, but if its not required to exist or not required to indicate all health signals, (meaning theres no required connection between the variable and health in all possible states), it will inevitably have errors
        - connecting information with only requirements is rarely possible in absolute isolation of other information, but the connections remaining to be connected after applying known requirements can be reduced with requirements just like solution spaces can usually be filtered just by applying structural requirements like data types of inputs
        - another example is how some data is required to be continuous (a cat needs to have a continuous biological form in order to exist, it cant be missing interim connecting slices of itself), whereas a 'photo or cartoon of a cat' or a 'summary or definition of a cat' doesnt need to be continuous, as photos can be corrupted by interactions with other objects, while still existing
          - if an ml model has a 'concept' or 'definition' of a cat, it might miss this subtle difference between a cat and a photo of a cat, it would also need to have a definition of a 'photo' or 'cartoon' in order to understand why some differences like 'photo corruption' can exist while other differences cant (useful for preprocessing or augmenting data)
      - if the variables exist in the input data set and the only structure required is additions and exponents/multipliers of those variables, and if the input variables' true interaction functions follow a function that fulfills the assumptions of the model (continuous, etc), and if the function doesnt have to include inferrable but not included information, like cause, implications, requirements, etc
        - http://neuralnetworksanddeeplearning.com/chap4.html

      - error structures
        - the standard ml structure of 'building bigger features from smaller features' doesnt allow more complex variable interactions like:
          - deriving an abstract variable from some of its attribute variables, as some of its attributes may not be simply aggregated but may be conditionally relevant variables in determining that abstract variable
          - identifying interface variables
          - identifying connections between non-adjacent variables or other structures contradicting the similarity structure embedded in the network algorithm/config
        - ml networks tuned to prevent all bias cant use any structures of bias 'hard-coded rules' to fulfill other intents, like approximate solutions or 'temporary interim solutions' while other solutions are being developed, as there are reasons why these hard-coded rules develop in brains, such as over-simplifications, life or death situations requiring fast thinking, and group dynamics leading to biased rules like 'dont trust dissimilar agents' which lead to evolved defaults like these hard-coded rules, and these reasons are less valid in the current problem space where these situations occur infrequently, but the rules are default or otherwise hard-coded and therefore are applied when not useful (such as in any stressful situation rather than in life or death situations)
          - any network that develops a hard-coded rule like a 'false maximum variable value limit' that emerges bc of a network configuration is applying the same learning process in any case where such a rule can 'develop for no reason' or where the network can 'apply the rule for no reason', so these networks are by default some degree of incorrect bc of that structure that develops
        - bias toward 'constants' (using variable values' co-occurrence as a constant, leading to bias errors based on those constants)
        - bias toward 'irrelevant variables distantly connected to relevant variables', where relevaant variables should be targeted for acting like a 'interface structure' ('interface variables' on which other variable changes are based) and 'distance from interface structures' ('variables based on interface variables')
          - bias toward excluding 'constants' or 'low-variation variables' which are 'change-invariant variables' (which may in fact change but act like an interface in the original data set and are therefore useful to include rather than over-prioritizing just the variables that are high-variation)

      - add to ml definition of a 'solution-finding method' as it finds the solution 'neural network configuration' (prediction function) by a 'sequence of weight changes'

    - examine 'locally same, globally different' as a structure of 'interchangeable alternates' on 'interaction levels'
    
    - standardize primary functions in terms of common functions
      - 'derive' mapping to 'find cause', 'build' mapping to 'find combination', apply mapping to 'find changes', and 'find' mapping to 'find filters'

    - example of possible usefulness of a structure
      - an example structure like '1/2' can be useful for the following structural intents
        - equivalence of two values (two halves), equivalence being a structure of randomness, which when applied to 'interactivity of values' can produce other structures of randomness like 'complex systems with many variables/structures having high interactivity'
        - a default structure of a 'comparison' and 'a randomizing division (1/2 slope line) of an interaction space (like the area created by multiplying x & y)' between a default unit structure '1' and a default unit interactivity structure '2' when applied to dimensions (as an exponent)

    - add to useful structures
      - core math structures (point, line, path, network, factors) and their counterparts when applied to other interfaces (object, connection, function, system, components) as a way of finding important (as in 'core') structures in another interface

    - write example of code as a set of parallel sequences on different abstraction/interaction/intent levels and in different formats (app/function, task-oriented/core, requirement/implementation, change type/variable map, uniqueness/usefulness, network/tree/sequence)

    - add to why a language network is insufficient
      - doesnt describe new concepts not already adjacently embedded in the language
      - doesnt have a reason for not standardizing terms to a common core set or a set of generative functions, as opposed to including all or most terms in the language
      - doesnt have a reason for positions other than similarity on one metric, which is inadequate to determine absolute position across all differentiating metrics
      - doesnt reflect multiple difference types like usage, meaning state changes, evolution of terms, which require different structures to represent
      - alternate formats could include: sub-networks to depict common usage queries of a term & state changes of the usage queries as well as alternate meanings of the term in those alternate usage contexts & synonyms of the term

    - add to why there will always be new problem-solving methods 
      - problems/solutions will always change, as will related structures like structures to generate them and structured derived from them
        - related structures like optimization methods, requirements, solution metrics, etc will always change
        - solutions like available tech like storage/computation/derivation tech will always change which solutions are more optimal for a given problem
        - there will always be new alternate methods to derive something, given that everything is connected, and derivation methods will always change and be optimized in new ways
        - there will always be problems, and problems are suboptimal differences, so solutions will be new ways of resolving new differences or resolving known differences in new ways

    - alternate definitions of neutral structures and errors as 'sub-optimal solution states' to identify what moves to make (in what directions, to what degrees) to find solution states (optimal states) given patterns between sub-optimal and solution states
      - apply patterns of sub-optimality to reduce errors (solutions are where states are 'sub-optimal error states', as in 'not an optimal error but a suboptimal error, as in a solution')

    - add to useful cross-interface structure mappings: 'possible usages' as a 'tangent bundle' or 'convolution' structure

    - example of using info as the standard interface to use as a base
      - a 'reason' is the 'information' about a reward/cost that can be used to determine if a change has the possibility of being optimal for some intent ('finding the reward/avoiding the cost'), meaning the reason is the cause of the change

  - add to differentiation from other inventions
    - theorem-provers are written by a person who is mis/interpreting logic in their own way, as a 'control flow' language between 'formulas' as data, hard-coding assumptions like the specific, hard-coded/constant solution automation workflow & problem/solution structures being used ('break a problem into sub-problems', 'dependency chains', 'sequence of current formula state compared to target formula state', 'useful structures like combination/substitution functions', a hard-coded set of 'goals' when solving each problem, such as 'find counter examples' and 'find probable variable values to test' and 'check if current state and target state are equal'), rather than a way to automatically solve all problems, including problems other than 'checking if a math formula has any counterexamples', or a way to automatically derive & apply logical rules like 'check for a counter-example') which are hard-coded manually in these tools as assumptions of the developer
      - these tools basically map standard programming control structures like 'if/then' to high-level math theorem interaction structures like 'assumptions/conditions', as proxies of sequential flow signals, in an inaccurate/approximate structure other than a network/tree (allowing for mis-interpretation and mis-mapping of the actual connections between these corresponding structures), an 'assumption' being treated as an 'input dependency' structure without actually equalling an input dependency in its definition, leaving out other 'input dependency' structures like randomness, system context (implications of the assumption and their impact on input variables), dependency alternatives/requirements, etc, which should be taken as implicit inputs to every function
      - these tools are over-simplified for limited gains in utility value, but are probably useful for solving highly formatted specific problem types like 'checking formulas for equivalence'
      - also of note: a program that 'checks every possible formula' is also a 'solution automation tool', but hardly anyone would find that useful, and almost anyone could write that, even if it is pleasant to imagine that as being a useful alternative to my inventions
      - https://github.com/jrh13/hol-light/blob/master/tactics.ml

  - add response to critique of 'being runnable in human brain' 
    - differentiate it from human brain structures & from automatic/default problem-solving as well as which people can execute the processes in the invention & at what scale/speed/accuracy which differ from the way the invention works and whether other people identified any variables and whether they can apply them in the same way as an interface query which I havent given a full example of yet, rather filtering the query for the structures that are probable or are found to be useful

  - add to why my invention isnt a language or a language network
    - its a way to generate/find/apply/derive languages to interpret systems
    - its purpose is not to accurately represent 'every possible variant of a concept' (synonyms) or 'every possible connection' (verbs) but to 'reduce problem-solving structures, like requirements to fulfill structures of usefulness like "common intents"'

  - add to problem/solution structures
    - definitions & other truth structures as symmetries

  - add to implementation methods
    - to automatically find a structure like 'intent' in a set of functions:
      - test usefulness of functions for known intents
        - workflow: apply the original problem output ('intents') as input, 'testing' as a solution filter to find functions for each intent, reversing the problem direction
      - index functions known to be useful for common/core intents and index other functions by similarity to these
        - workflow: identify common/core problems to solve or common/core solutions to solve for, and apply an assumption of 'solution similarity' of other solutions to generate other solutions once those common/core solutions are solved for
      - index function intents as the function requirements (resources like computation) and function outputs (use requirements, use requirements to generate outputs, etc)
        - workflow: apply related/similar structures instead of the original structure

  - add to predictions
    - where you say that 'activity interacting with a neuron is relevant in its functionality': https://www.quantamagazine.org/how-computationally-complex-is-a-single-neuron-20210902/
    - relevance of intelligence (successful learning/thinking) & disease (unsuccessful learning/thinking/stressor-handling): https://www.quantamagazine.org/brain-cells-break-their-dna-to-learn-more-quickly-20210830/

  - add to science

    - example of a system layer diagram
      - http://mkweb.bcgsc.ca/pi/art/

    - pi/e/i conceptual connections, reasons, and meanings in various connecting formulas (formula for a circle using e, euler's identity)
      - pi as a unit structure of conversion between constant change (line, unit square) into a structure with no constant change (circle), or add interactivity (through containment of sequences, creation of randomness, conversion to real numbers) to a self-similar structure (unit square, i squared)
      - e as a unit structure of relative change (change relative to a base)
      - omega/x as a unit structure of infinity (a structure representing concepts like 'maximum' or 'extreme' or 'unlimited/unbound' or 'all'), representing all possible angles in radii of a circle
      - i is the 'root' of 'opposite' of 'unit' structure of 'completeness' (integers)
        - the 'equal factor component' (applied to itself once) of the 'opposite of the unit of completeness' (unit of incompleteness), as in 'unable to completely fill a constant-sided shape like a square' (such as how a circle with radius 1 cant fill 4 * the unit square with side length 1)
      - the 'center' (average) of the circle provides a constant 'base' point to apply changes to (standard interface structure combining a constant value with changes applied to it that dont lose the original constant value info)
      - the 'radius' is a unit structure of 'connection'
      - why are these structures (constant/non-constant change, relative change, unlimited/all, 'root of opposite of unit of completeness', 'fixed base (center point or radius)', connection) related
        - exponential change produces curvature, as a 'relative change' of an increasing change rate, relative to the previous/adjacent change rate
        - the structure of a unit square (unit of completeness applied to itself) is directly related to:
          - i (root of opposite of unit of completeness)
          - pi, through the difference from the unit square that pi represents (4 * unit square with constant curvature linking endpoints at cardinal directions, representing 'four useful change types of two dimensions' that can create 'all changes in those dimensions')
        - 'all' (represented by infinity, as the set of possible angles) changes to the 'angle' (representing the 'change applied to the fixed base constant center point') taken together, produce a 'relative change' of an 'exponential change' compared to the change types ('constant change') of the unit square produced by the fixed constant point/radius, which produces an upper bound ('limit') on the change types allowed in the perimeter
          - 'all' (infinite angle) changes applied to the unit 'connection' structure (line) having a fixed point (center) represents continuous change while the center is fixed
            - meaning the change rate must be a constant (fixed) value between adjacent points, but the change rate between two non-adjacent given points cant be equal (according to definition of circle)
            - change on rotation of the radius is limited by the length of the radius (rather than spinning off into a tangent line of the circle or diverging in an outer direction)
          - so relative to constant change represented by the 'unit square', the next type of change up (change x change, or change rate) can be produced by applying 'all' values of the 'angle' attribute of the 'fixed base structure' of the 'line with a fixed endpoint', creating a 'limit' that prevents the change from being 'constant' (as in linear) or 'variable' (as in a wave or hyperbolic function), creating a 'change type' that is 'a mix of constant/variable', having adjacent constant/linear change and global exponential/curved change, and equal locally global change (unit change rate from one point to another in equidistant pairs is constant throughout the perimeter), in the curved perimeter
        - pi can be interpreted as a 'unit' structure of 'curvature', or to 'create curvature boundable by a change unit (such as the unit square of '1 applied to itself', applied to a 'dimension-crossing' multiplier of 4, which converts an area into a surface area for a circle)'
        - other structures that can create useful numbers like pi (such as multiple change types like 'multiple wave functions aligning and diverging at various points in a pattern that has converging or compounding effects'), reflecting the usefulness of interface structures such as 'cross-interface alignment'
          - https://abakcus.com/video/beauty-in-numbers-pi-3-14/
        - 'Gravity = 8 x  x Energy & Momentum' as in:
          - gravity = 8 * (unit curvature factor) * change potential + previous energy output
          - change limit around a fixed point (high mass object) = 2 * (dimension-crossing multiplier between area/surface area of a circle) * change potential + previous energy output (free energy from compounding change effects)
          - change limit around a fixed point (high mass object) = 2 * (dimension-crossing multiplier between area/surface area of a circle) * change potential + previous energy output (free energy from compounding change effects)
          - this describes how 'structural differences' are allowed to develop in space-time, meaning how structures such as 'object positions in time' can be 'adjacently connected', given that it uses the object's change history (expressed in its momentum) as an input
          - quantum equations may describe non-adjacent change types (cascades of patterns in otherwise random, non-adjacent structures or generating random structures, or aligned patterns in non-adjacent structures)
        - why are circular structures useful? 
          - bc of their symmetry (which is why other symmetrical objects can produce circles as well, bc of their fixed change point in their center, leading to equal area distribution on center-intersecting lines), symmetries being a corresponding structure on the 'physical information' interface as the 'interface' structure on the 'interface' (meaning) interface, this symmetry being a useful structure for other structures like 'randomness'
          - why are elliptic curves useful?
            - bc of their 'mix of change types' such as the combination of changes like 'curvature' and 'differences in length of orthogonal change types' which produces a useful structure such as an 'ambiguity', with enough opinionated 'certainty' in the ambiguity to identify a distinct subset of factors (as opposed to the set of 'all' factors, or 'common' sets of factors, such as how factors generating a circle are equal bc of its symmetry, and how some changes in circles like 'equidistant points on the perimeter' are equal, resulting in almost complete 'indistinguishability', which is useful for 'obscurity' but not 'factor set filtering'), having an 'opinionated' or 'prioritized' ambiguity indicating a 'reason' or 'meaning', as opposed to a 'neutral' one, and having a 'phase shift' separating the equal sides separable by each cross-section, where the 'change rate' is more similar to a 'constant' than not
            - this repeats the common useful structure of a 'constant + a variable' represented in an 'interface' structure ('changes applied to a constant base structure')
        - why would pi be the probability of landing on an equidistant parallel line?
          - bc the 'equidistance' is created in the 'equidistant point change type' equivalence in the 'randomness' (equal distribution) created by pi (why the circle doesnt spiral or create a tangent or have constant change between non-adjacent equidistant points (such as the four cardinal endpoints of the perimeter of the circle), which when 'rolled out' into an information-preserving format ('rectangle'), would look like 'equidistant parallel lines'
        - using a format of 'what factor filters or patterns of factors/filters can be filtered out or included, for a particular prime' is a useful way to format a prime, as these factor filters are likely to be useful as prime testing/generating functions
          - factor filters like 'odd numbers' and patterns of factor filters like 'additive rules resulting in a particular ones-digit attribute like filtering out numbers ending in fives' are useful to apply in a particular sequence
          - other formats like 'values in between integer-sided rectangles' are other useful filters of primes
        - the reason some cryptography methods are better than others can be summarized as 'forcing an attacker to use brute force (trial and error)' and also 'allowing user to apply adjacent conversions, like a multiplication operation' and 'additive requirements for the attacker' like first 'checking every number in a sequence of filtered values that could be primes between 1 and n' and then 'checking for another prime factor to multiply a prime by to create n' once a prime is identified, which itself is an intensive operation of 'eliminating possible factors up to possible integer factors of that value' that has to be run on every possible prime value in that sequence in order to apply trial & error to find the factors
        - randomness can be derived as useful for encryption/obfuscation bc of the required 'volatility of outputs generated from similar inputs' which randomness can more adjacently produce than most other structures
        - the artificial infrastructure requirements necessary to support the increasing artificial separations necessary to protect information may increase at a rate higher than the protection potential of those separation structures (once the infrastructure is in place, the separations may be more easily overcome using that infrastructure required to make the separation structures valuable), separations like 'random maps', as simpler methods like 'denial of service' and 'brute force' become simpler with every infrastructure enhancement like computing advances which can be directed at infrastructure required to support artificial separation structures which are less stable to maintain than simpler structures
      - https://ali.medium.com/22-fascinating-facts-about-pi-%CF%80-that-will-make-you-amazed-4831ed694264
      - https://en.wikipedia.org/wiki/Euler%27s_formula

    - why are antifungal compounds often yellow/orange
      - probably coincidental, resulting from common evolutionary output indicating survival of various problem types (scare away predators, destroy fungus)
      - could also be related to orange/yellow tree plants' requirement to defeat more predators/pathogens by being exposed to them more often, likelier to encounter more of these by exposure to air
      - also yellow is likelier to attract insects looking for flowers, and must be resistant to pathogens carried by them, so a brightly colored plant that survives is likely to have these defenses built in
      - could also be incidentally resulting from a common cause of developing carotene, as a plant that survives probably produces a greater variety of defensive/useful compounds such as carotene

    - check if similar strategies as vaccines are functional - can the immune system learn 'associations' like the following:
      - associating a new pathogen with a known pathogen already recognized by the immune system using a chemical bond, so the immune system learns to attack the new pathogen
      - associating an interior part of a pathogen with an exterior part of the pathogen, so the immune system learns that a pathogen it already recognizes is inside a cell having the exterior attribute
      - associating 'immune system outputs' like coughing or heat with an immune response, either local or global, so the immune system learns to trigger itself whenever a cough or painful stimulus or another output of the immune system is intentionally triggered by force
      - giving a known pathogen partial attributes of a new pathogen so the immune system learns to attack the new pathogen without being exposed to it
      - finding genes that trigger stem cell specialization into useful cells or immune cell development (triggering similar to 'cancer' of the immune cells so they divide more quickly under stress conditions when exposed to a new pathogen) and creating a virus with those genes or which lead to side effects of those genes

    - gravity batteries
      - stations that start at tops of hills to initialize routes with energy already stored, and uphill trajectories back to these stations which are similar in shape to a 'diagonal wave function' to power each section of uphill ascent with the energy from the downhill dip
      - particles drawn from air which, when electricity or other reaction is applied, form heavier particles which create energy when falling
      - magnets that repel an object to a height which is useful for energy generation when falling
      - structures that, when dropped from a high point like the iss, cliffs, sides of buildings, or mountains (carrying the object horizontally with wires to a position where it can fall in a useful way) where theyre constructed, dissolve by the time they reach the ground to avoid crashes, or when 'automatically moved' in another way, like with ocean tides directed by fields/rotation of the earth (digging holes in the ocean floor where the tides would direct water in a predictable way that could be harvested)
      - pressure-based propulsion (a device that returns to its 'loaded or ready to spring' state when gravity/time/magnets are applied) to eject objects to heights that will generate enough energy when falling

    - finding space time trajectories that use as much information as possible to avoid 'reversal of direction' errors

    - bc it uses hormones, diverts resources to building the embryo, and creates stem cells that can become any other cell
      - of those that are common, they are associated with reproductive organs and related systems
      - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6396773/

    - solutions to water supply problem
      - multiple solutions including solutions like reverse osmosis and solution formats like using reverse osmosis to create a reusable water supply that is more filterable, requiring one use per user with access to the tool in urban centers
      - using processes like applying heat to evaporate water to create clouds/condensation which can be separated from minerals
      - solving a different problem - optimizing production of new or known elements/organisms that bind with chloride more strongly than sodium, convert sodium chloride molecules into more filterable molecules, replace the sodium in a bonded molecule of sodium chloride, or break the bond of the bonded molecule

    - calorie expenditure as a budgeting structure, where high energy usage optimizes usage (like routing energy to essential processes like creating new stem/immune cells, replacing damaged cells), so low energy usage (low exercise) doesnt develop these optimizations to energy usage, so theres more energy available to nonessential processes (like creating cells randomly or in other suboptimal ways) bc of the lack of routing or other optimization functions

    - quantum structures are structures that can be combined or otherwise interact (such as by neutralizing each other, or reducing change types, or interacting to form a stable structure) to form stability structures (measurable across adjacent time sequences) on which further changes can be based (creating further time structures/expanding time by increasing the number of possibilities), as quantum structures are 'information-adjacent' in that they are relatively unstable and difficult to measure but adjacently form information (measurable, stable structures)
      - 'connectible stable time sequences' and 'local structures' and 'change structures' and ('overlapping time sequences' or 'coordinating/interactive time sequences' or 'non-contradicting/non-interactive time sequences') as structures that form space-time, definable as 'structures that support certain change types which are executable within a given time, and if surrounded by other structures'

    - 'ratio of evolution' - rather than trying to address each bio-system vuln as it is discovered to be used by a particular pathogen or otherwise beneficial to a particular condition, the ratio of the bio-system's evolution to pathogens' evolution and conditions' benefit should be optimized so that the bio-system can evolve faster than pathogens
      - this means limiting the mechanism of evolution/learning of pathogens (modifying themselves in ways that kill hosts) at various points of attack
        - the success of a pathogen in killing any particular host can be prevented so the pathogen doesnt learn how to kill hosts
          - this would require advanced technologies like creating blood to replace existing blood, customized treatments per patient, etc
        - the pathogen can alternatively be tricked into deciding its been successful at killing the host, as a way of preventing it from learning how to kill hosts (produce false success signals for its intent that dont harm the host)
        - the pathogen can alternatively be tricked into deciding a different strategy which doesnt harm the host is more successful at its intents (obtaining energy, growing, etc)
        - the pathogen can alternatively be limited in its learning functionality by removing genes in pathogens that allow this learning, such as limiting pathogens to learn self-destruct functions, or limiting pathogens to only learn by testing rather than by using/borrowing other DNA external to itself, so its learning rate must necessarily be slower than the host cells' learning

    - 'sets of n particles' that act in aligned ways like:
      - 'components of a temporary local field/lattice having n points'
      - 'a connection created from equivalence structures' based on:
        - a 'homotopy equivalence between points'
        - a 'set of rotations/arrangements transforming points into a set of equivalent duplicates by definition-compliance, like the infinite cloning paradox solution'
    - 'ozone hole' was a specific structure that gave people one achievable goal to aim for so they could organize resources around achieving that goal
    - what ratio of gene copies is required to offset cancerous processes/responses without interfering with non-cancerous process
      - can junk dna be altered to fix sub-optimal ratios?
      - are some dna copies acting as 'placeholders' that prevent other copies from being made or prevent certain illnesses
      - how to 'create dependency' of cancer cells on a particular process/structure so when removed it can create cell death
      - drugs that response to 'high quantities of a signal' leave room for an error of 'missing low quantities' - an error where the signal is assigned a boolean and should be a continuous spectrum with a threshold that can re-create the original problem
      - how to target useful cells with growth-enhancing mechanisms like extra-chromosomal dna
      - are there genetic sequences that can offer protection against extra-chromosomal dna interactions temporarily while cancer cells are being targeted (like if a protective sequence is spaced at intervals that disincentivize interactions with extra-chromosomal dna, such as when extra-chromosomal dna merges with a chromosome)
      - how to account for probability of a known possible error (like mutations compared against a mutated rather than healthy reference genome) and adjust data accordingly
      - if a structure continues to develop, that means it represents a 'stability point' that is incentivized - which incentives of extra-chromosomal can be altered without harming other systems?
      - if a package of growth/maximizing genes continues to develop bc of 'survival bias' (the genes that are useful for survival are protected & maintained & replicated), can incentives for dna repair/defense genes be created/increased to compete with these cancerous extra-chromosomal gene structures - why is the 'survival bias' of cancerous cells & cancerous mutations stronger than other cells/genes/mutations' bias - just bc theyre more useful for cancerous intents like 'unrestricted growth' or bc of some other cause like 'genetic adjacence' or 'genetic functionality like jumping/coordination', or bc of the fact that growth factors promote themselves by the definition of their own functionality, or bc these gene/mutation types like enhancement/growth have aligning intents like 'promotion' so theyre often found together?
      - can antibodies against high-growth/maximizing extra-chromosomal gene structures be developed or can vaccines be developed to produce antibodies against them?
        - https://cen.acs.org/pharmaceuticals/oncology/curious-DNA-circles-make-treating/98/i40
    - apply 'solve problem cause' by 'changing' the 'input-output sequence' of that 'problem cause' at a particular position
      - https://medicalxpress.com/news/2022-02-cancer-cells-tunnels.html
      - other workflows would be:
        - minimize problem
          - close most cells' tunnels to let a smaller number of pain signals reach immune system, while making pain signals more powerful 
        - apply solution as a prevention measure without using 'missing variables' (cell identity/health) that cant be identified, applying the solution at a 'smaller scale' (to increase chances of hitting a cancer cell above zero, while not causing extreme harm when hitting a healthy cell, and not applied to many cells)
          - apply structures to function applications where input identity cant be verified so it may as well be disregarded ('randomly' apply) the solution output (reducing motility) to correct the problematic output (the deadly effect of motility)
        - use problem to solve another problem
          - apply 'mobility mutations' and 'destruction signals' to move cancer cells to specific locations where theyre less likely to survive
        - solve problem cause at another position in the causal input-output sequence
          - change sequences of genes to make cancer that searches a genome less likely to find useful genetic mutations immediately, wherever genes with carcinogenic mutations are switchable

    - a generally useful 'quantum algorithm' is optimally implemented as a network where any node is equally accessible regardless of steps separating them or other definition of 'distance' (each node pair can be entangled on demand to get calculations/output from that node to reduce overall repeated function calls of the network)

  - add to automation tech
    - more devs converting to 'task description writers' creating standardized task descriptions & identifying repeated work, automation opportunities & other high-level semantic tasks is a better career once programming turns into a 'task bidding freelance market' where AI coders frequently outcompete devs
    - organized & structured code by 'core component' (function input/output variables, input/output differences, patterns, core function-calling function (high-level functions, so building core functions is the source of changes in functionality of high-level functions), uses, logical derivation connections (equivalence, requirements, dependencies) as logic trees, logically directed graphs, changes/states, function, reference, relative position of thread/process/function, limits/requirements, dependencies, data, prior operations (cache), differentiating attributes, context, mix/iteration/cycles/sequence, type, example, value)
    - 'infinite compression' as a combination of infinite sequences & numerical references to those sequences as parameters (similar to rainbow tables), where infinite random sequence subsets can be recursively compressed into hashes, where infinite sequences can be represented as a sum of sequences (infinite random adjacent base sequence + an infinite geometric sequence to generate it), or a sequence & a set of vectors indicating changes to generate the sequence from an infinite random adjacent base sequence

  - additional attributes (metadata) of interfaces include attributes like validity & relevance, which vary absolutely across other interfaces, and contextually across interface queries/solution automation workflows & problems being solved with them

  - additional examples of error causes
      - example of priorities leading to errors:
        - the 'selfish' perspective prioritizes simple, structural objects (like those within an adjacent range), which is why 'selfish' programs see & act on those things rather than producing solutions that can be used for many problems, even ones they dont have
      - example of how a 'lack of errors' in a closed system can produce an error in system-level interactions, like a cross-system merge

  - add to stats

    - example of generating bell curve with multiple simpler change types such as rectangles with variable side lengths overlapping at their centers, connected by the defining points of their limits as an example of 'alternate formats that reveal different possible inputs' (as well as the commonness of adjacent changes)

    - give example of visualizing the stats object connections in a stat insight rule
      - add example of how to derive the rule 'choose algorithm with low bias/high variance for large data sets'
        - use 'random forest' bagging method to reduce high variance
        - high variance is a problem when a sample data set is not representative of the population, producing accuracy for the training data subsets & errors for all other data subsets
          - if a data set is relatively large in relation to the population, the 'sample data set is not representative of the population' is less likely to be a relevant problem
          - 'variance can only be reduced by averaging over multiple observations, which inherently means using information from a larger region'
        - high bias is a problem when sample data sets differ from the population mean by a lot, producing errors for most samples unless they happen to be represented by the general model
          - if a data set is relatively large in relation to the population, the 'sample data sets differ from the population mean' is less likely to be a relevant problem, so the large data set can be used as a basis for the general model
          - 'bias is reduced using local information'
          - 'If training set is small, high bias / low variance models (e.g. Naive Bayes) tend to perform better because they are less likely to be overfit.'
          - 'If training set is large, low bias / high variance models (e.g. Logistic Regression) tend to perform better because they can reflect more complex relationships.'

  - neural networks would need an accurate network of reality embedded/emergent in them to perfectly predict everything, and most problems dont have enough differences in between input/output variables to capture an approximation of reality
    - 'extremely different nodes connected to make these nodes connected adjacently' (related to the difference-maximizing shape) as opposed to a standard network where positional adjacence is equated to relevant adjacence
    - 'difference-maximization' as a useful way to filter inputs
  - give example of how a structure (like a neural net) complements its associated supplemental functionality (like iterating over the network to update weights in a 'network state sequence'), as a useful difference structure to create functionality like 'learning', since the functionality cant be stored in the structure of the neural network and has to be applied to it, or other examples of structures that leave out some structure (such as a 'network state sequence') and have it applied as functionality

  - give example of network/graph math, etc
  
  - give example of 'different meaning interaction levels'

  - examples of 'interactive' structures which can be inputs to the 'apply' function
    - 'input' structures (a 'reverse' function can be applied to a structure with a 'sequence' attribute, the 'sequence' being an input requirement of the 'reverse' function)
    - 'attribute-coordinating', 'attribute-changing', or 'attribute similarity' structures (a 'number' value can be added to a 'number' value, a 'position change' can interact with a structure having a 'position' attribute, meaning one structure changes the 'attribute' of another so theyre 'interactive')

  - give examples of why other tech solutions are insufficient
    - why ml would be inaccurate on math problems (like 'predict convergence value for an infinite series, based on training data of infinite series param input & convergence value output')
      - regularization, bias/noise changes, and other feature changes for intents like 'generalization' or 'feature selection' may add to inaccuracies
      - the structures that can be composed by various function/node/weight/threshold unit combinations may not be capable of the math operations involved in transforming one value/format into another
      - the math problem has emergent structures that are not visible in the training data set, for example a point where the relationship function between the inputs/outputs changes (like an asymptote or a maximum)
      - the training data may reflect patterns that are simpler for the network to compose/filter/reduce, without enough data points representing more complexity (a parabola instead of a hyperbolic function)
        - the network may be good at providing filter/compose/reduce functionality, but not other functionality like 'converge' or 'select between similar alternatives'
        - the support for 'multiple alternative input-output routes' in networks may add too much complexity in reducing its ability to specify a particular answer, with built-in tools to find averages or other representative values rather than selecting one value over another
        - for some functions, there are too many inputs that could produce the labeled output, and those inputs may be too similar to differentiate/specify (very similar functions can produce the same area)
      - neural networks are primarily good at certain data transformations, like where the problem input can be converted into the solution output by a system of linear equations or matrix operations supported by the network, where coefficients/weights of versions of function components (inputs & interim weighted combinations of inputs) are the required output of the network, and those operations may not adjacently handle operations like summing an infinite series, which might require specialized structures like memory/state-embedding or online learning if those structures cant be produced by adding additional layers to a network
      - each feature of the infinite series input would contribute to the output, but a neural network is designed to learn weights of features, implying that some are less important, and the contribution of terms in a series to the convergence value can usually be determined by its size/position
      - if parameters of the series are used instead of the series, that is a low feature space compared to the input features available in other applications of neural networks
      - the operations in some formulas do not produce reliable learnable patterns (some structures of randomness would counteract the ability of the network to learn a function)
      - the inputs dont provide enough data for continuously differentiable spaces where methods like gradient descent can be applied, given that math functions often cover a wide range of possible inputs, and a training data set is unlikely to be representative of enough examples to fill in the gaps in this space

    - VAR & reservoir computing's random sample of matrixes is inadequate bc the randomness is an attempt to identify 'very different' difference types, without generalizing that into a unique set of differences that are likely to be useful (like differences distributed across pairs of variables, so many random samples dont represent difference types in the same pair)
    - regression is insufficient even if its a good temporary solution if you dont have other resources than the data set, bc its conclusion/outputs (in the form of the regression function) can have the opposite meaning 'random noise' as the intended meaning 'causal relationships'
    - statistics in general is built on the insight that 'probability is associated with certainty/truth', but it ignores other certainty structures like structures that are more useful than patterns/probability, such as definitions, concepts, meaning/understanding/integration/organization, cause, inevitability/requirement, determination/generation/description structures, functions of varying interaction levels, etc
    - machine-learning can have the opposite functionality given extreme data values or update functionality manipulation (to train it to give the wrong answer in its online learning functionality), as well as other exploits from interactions of the algorithm, network, parameters, emergent structures, & data, and it is not built on understanding
      - 'one-degree connection structures' which are present in 'foundation models' are incapable of capturing multi-degree connection structures like sequences/chains or structures of connections like trees/networks/groups, even though other structures can be formatted as core structures like 'connections', it doesnt mean a one-degree connection network will capture them, or that a network of connections is the most optimal structure for that info given its usage
      - machine-learning based on neuroscience leaves out other brain interfaces like psychology, chemistry, & language
        - a psychologist might interpret a thought as 'an emotional reaction to a chemical stimulus that retrieved a memory'
        - a neuroscientist might interpret a thought as 'a response to electrical stimulus given weighted connections between neurons that previously handled that stimulus'
        - a linguist might interpret a thought as 'a deviation from a previous phrase that captured an experience to handle a change to that experience'
        - a chemist might interpret a thought as 'a result of scaled electron dynamics in response to a chemical'
        - a biologist might interpret a thought as 'a useful way to produce serotonin to offset a signal from the gut'
      - https://thegradient.pub/has-ai-found-a-new-foundation/

  - add to problem/solution structures

    - add example of how to derive 'apply differences to inputs to see if they can change the output to see if the solution is true'

    - add example of impact of methods on various network types given the differences in method/network structures & include assumptions

    - give example of how structures could have been derived (symmetry, isomorphism/interchangeability as common important objects to derive an interface, alternative interfaces to solve a problem) from another direction

    - derive logic types that would be necessary to complete the logic interface & give examples of logical object interactions

    - a 'find a solution' function should be able to be converted into a 'generate a solution' function & other functions like 'test a solution' & 'apply a solution', bc as the brain learns, it can generate solutions on demand once understanding develops

    - give example of identifying meaning of emergent structures (like 'weight trees' in neural networks)

    - organize workflows using useful structures as being on the meaning interface, where useful structures from other interfaces overlap & connect with the meaning interface

    - write interface queries to generate each workflow

    - give examples of how each workflow can be applied to various standard problems (find a prediction function, sorting problem, ml configuration/algorithm-design problem), which can be used as a data source to derive implementation logic/interface queries to generate solutions

    - finish math mapping so you can find other useful/solution structures (interaction space as convolution, core functions as basis vectors, etc)

    - basic solution automation workflows
      - trial & error
        - use a rules/solution database & look up the answer (try known solutions)
        - apply machine-learning with various configurations (try known/probable configurations or variable interactions like combinations of a solution-findind method like neural networks)
        - apply rules from other systems to see if they work in another system (try other known solutions from other known systems)
        - mix & match solution components/variants (try known solutions)
      - reverse engineering
      - break problem into sub-problems & merge sub-solutions

    - example of format/intent matching
      - formatting a 'tree' as a 'set of overlapping sequences with overlaps in either inputs or outputs' so functions can be formatted for different intents like in 'parallel processing'

    - add to input structures
      - input variable/trigger/requirement/component

    - add to output structures
      - limits on what a structure can be used to create
      - similarities/differences to inputs (inputs change can be preserved in outputs)

    - identify new interactions/structures
      - trying structures of structures that havent been tried yet (like how new words evolve as a 'combination' of other words to describe new experiences that are similar to both combined words)

    - 'testing/simulation' involves querying for related rules (like how 'gravity' rules are related to 'motion' rules so any change involving motion should have a 'gravity rule check' applied as a filter) & checking if they apply to relevant components (like how specific components are involved in 'motion', like 'energy', 'motion restrictions', 'motion functions', 'motion triggers/inputs/components')
      - this is an important process for checking if a structure is valid/consistent in a system, which is a useful function
      - this is different from basic testing, which is where a function is applied and the output is checked against an expected value, bc it involves testing for validity/consistency in a system context where the change is being applied

    - examine interaction space of tech stack layers (ml models, algorithms, data, apps, bugs, os, chips) as a source of new errors
      - example: 
        - ai applied to design chips
        - chips with data erasure bugs that exacerbate os data erasure bugs
        - chip designs that produce error types for various ai models/algorithms/parameters
        - how 'gpus are known to be better at building ai models'

  - add to error-finding methods
    - identifying & generating known useful structures like 'symmetries', 'variables', 'subsets', 'interchangeable alternatives', 'maximally different inputs' & 'bases' & 'type/phase shift thresholds'
      - identifying & generating combination structures of useful structures like 'maximally different values around bases'
    - identifying gaps in known useful structures explaining data points (where data points arent explained by those known structures) & generating inputs in those gaps other than those data points

  - add to conceptual math
    - example of a conceptual math operation that builds a boundary structure leaving an inevitability of a matching concept (numbers) filling the structure
      - the concepts of 'missing', 'multiple/more', 'unit', 'type', 'identifiable as similar/equal/different' and 'difference in amount' allow for/require/build the concept of 'numbers'
      - also functions like 'compare' or 'reduce' or 'expand' require the concept of 'numbers' when comparing objects of that data type or objects having a quantifiable attribute

  - add to causation variables
    - ability to change (if a variable cant be changed, it is less causative for problem-solving intents)

  - add to info problems
    - this manipulates:
      - audience objects:
        - ego
        - assumptions (about patterns, what you would notice/figure out)
        - attention
        - feelings 'opposite' to logic (safety, confusion)
      - using objects like distractions, activations, distortions, core structures like combinations/sequences, complexity, patterns, input/output similarities/alternatives (complex/simple implementations), logic, patterns of logic, logic avoidance, jokes
      - to produce:
        - errors in expectations (in order for the audience to expect y, they have to have assumption x, as x is an input to y)
      - these important variables can be identified by identifying the inputs to these objects
        - what 'input' is 'required' for this expectation error to happen? (an assumption)
      - https://www.smithsonianmag.com/arts-culture/teller-reveals-his-secrets-100744801/?all&no-ist

  - when is it optimal to store a mixed structure of varying specificity (like a type, intent, cause & a specific example)
      - when there are potential uncertainties to resolve, like whether the example represents a new error, type, or variable, bc the example doesnt fit known structures

  - all primary interfaces can act like the problem-solving interface (start solving problem from the concept or structure interface and integrate all info back into that interface & frame the solution in terms of that interface) but the meaning interface (the interface interface) is the most powerful

  - apply concepts to structures

    - concept of attention in structures
      - mixed interim high-variation & high-similarity structures tend to maximize attention
    - examine error type of conflating intent & requirement
    - consciousness as choice to move between neural nodes (rather than being directed) required:
      - the development of alternative node paths performing equal/similar functions, requiring:
        - the development of excess resources, delaying required decision time (making immediate decision unnecessary, avoiding a forced decision), requiring:
          - the existence & application of previous efficiencies & functions for alternative evaluation, energy storage, storage-checking, & energy requirement-identifying
      - the cause could be framed as structures such as an 'efficiency stack' or 'energy maintenance functions' or 'alternative options' or 'navigation/motion control' or 'lack of requirement/need'
    - examine similarity (alignment/overlap) structures between: 
      - extremely different components (when an error type is an incentive or a function used for other intents) 
        - when the solution format of some problem has similarities to the error type, like when you need randomness so errors generating randomness are a possible function to use for that intent
        - contradictory/opposite components (have some metric in common, with opposite values)
    - examine the distortion vector paths that adjacently decompose a data set into a prediction function from a base point/function set

  - add examples of:
    - mapping to structures & identifying contradictions its safe to ignore for applying a structure
    - system/object/rule/type change patterns
    - query examples for use cases like:
      - lack of information stored (match problem of type 'information lack' with interface query 'check pattern interface for similar patterns')
      - query problem breakdown & integration diagram
      - calculating various different problem breakdown strategies first before executing normal query-building logic for each
    - example of how to predict most interactive/causal concepts in a system


## diagram
  
  - diagrams:
    - error types
    - network of formats
    - efficiencies
    - alternate interfaces (information = combination of structure, potential, change or structure, cause or structure, system)
    - chart type: overlaying multiple 2-dimension variable comparisons to identify common shapes of variable connections (density of points added with a visible attribute like more opacity)
    - structures of emergence
      - example: 1-1 input/output relationship up an interaction layer, where extra resources that dont dissolve immediately on the higher interaction layer aggregate & form core structures like combinations, where interactions between combinations & sequences have different dynamics than the individual output interacting with other individual outputs
    - how emergent functionality/attributes come from interaction structures (sequences & layers)
    - intent-matching
    - interface overflow (to sub-interfaces), interface foundation
    - workflow
      - function to identify relevance filter ('functions', 'required') from a problem_step ('find incentives') for a problem definition, to modify problem_steps with extra functions/attributes ('change_position') to be more specific to the problem definition ('find_incentives_to_change_position') for problem_steps involving 'incentives', so you know to use the function_name to modify the problem step if it's between the type 'functions' and the object searched for 'incentives'
    - conceptual math interface query
      - use lattice multiplication as standard example, other than core operations (add/multiply mapped to language, concepts like irreversibility/asymmetry mapped to math)
    - interface conversion, matching, starting point selection (applying structure, checking if relevant information is found)
    - sub-functions of core functions with distortions (identify/filter of find)
    - dimension links higher than 3d that are depictable in the same network space
      - should show variables that impact other variables, the change rates of these relationships
      - overall impact should be calculatable from these relationships
      - should show similar movements for correlated variables
      - should show skippable/derivable variables (variables that can be resolved later than they normally are)
      - should show meta forces for overall trends in change rules (direction of combined variable forces)
      - should show limits of measurability & threshold metrics
    - specific concepts, core functions, concept operations (combine, collide, connect, merge, apply), ethical shapes
        - variable accretion patterns (how an object becomes influenced by a new variable, complex system interaction patterns, etc)
        - potential matrix to display the concept
          - map parameter sets to potential matrix shapes 
        - cause (shapes & ambiguity), concept (evolution of concepts, networks, distortion functions)
        - argument
      - system layer diagram for each interface to allow specification of core interfaces & other interface layers (interface interface)
        - system layer diagram for structures to include layers of structures 
          (beyond core structures like curves, to include n-degree structures like a wave, as well as semantic output structures like a key, crossing the layer that generates info structures like an insight, a probability, etc)
    - map variable structures to prediction potential for problem types, given ratio of equivalent alternate signals
    - vertex variable structures
      - quantum physics, prediction/derivation tools, build automation tools, testing tools, learning/adaptation tools, system rules, computation power are all vertex variables of information, since they can generate/derive/find information
        - which structure (sequence, network, set, or cycle) of vertex variables is most efficient
    - core component attributes: identify any missing attributes/functions that cant be reduced further
    - absolute reference connections with metadata structures like networks/paths


# content/config

    - import insight history data to identify insight paths (info insight paths like 'lie => joke => distortion => insight', system insight paths like 'three core functions + combine function with this definition + n distortions to nearest hub')
    - define default & core objects necessary for system to function (out of the box, rather than minimal config necessary to derive other system components & assemble)
      - add default functions to solve common problem types
      - alternate utility function implementations have variation potential in the exact operations used to achieve the function intents, but there are requirements in which definitions these functions use because they are inherent to the system. For example, the embodiment may use a specific definition of an attribute (standardized to a set of filters) in order to build the attribute-identification function using a set of filters - but the general attribute definition is still partially identifyd in its initial version by requirements specified in the documentation, such as a set of core attribute types (input, output, function parameter, abstract, descriptive, identifying, differentiating, variable, constant), the definition of a function, and the definition of conversion functions between standard formats.
    - systematize definitions of info objects
      - include analysis that produces relationships of core objects like opposites to their relevant forms (anti-symmetry) in addition to permuted object states (asymmetry), such as an anti-strategy, anti-information, anti-pattern
      - organize certainty (info) vs. uncertainty objects (potential, risk, probability)
      - make doc to store insight paths, counterintuitive functions, hidden costs, counterexamples, phase shift triggers
      - add technicality, synchronization, bias, counterintuition, & certainty objects leading to inevitable collisions
        - error of the collision of compounding forces producing a phase shift
        - lack of attention in one driver and false panic in a second driver leading to a car crash given the bases where their processes originate
      - define alignment on interfaces (compounding, coordinating, parallel, similar, etc)
      - add core info objects (core strategies, core assumptions) so you can make a network of graphs for a system
    - add function logic for:
      - concept analysis:
        - how new concepts (gaps in network rules) evolve once structure is applied to prior concepts 
      - interface analysis:
        - limitations of interfaces & how to derive them
        - how rules develop on stability & how foundations are connected & destroyed
        - explainability as a space limited by derivable attributes from data set & cross-system similarity
        - vertex definition & give examples (as an intersection/combination of interface variables, such as determining/description(compressing)/generative/causative/derivation variables), around which change develops
      - change analysis:
        - generated object change types
          - constant to variable
          - variable to removal of assumption in variable type/data type
    - examine implementing your solution type (constructing structures (made of boundary/filter/resource sets) to produce substances like antibodies, using bio system stressors)
    - resolve & merge definitions into docs/tasks/implementation/constants/definitions.json
    - update links
