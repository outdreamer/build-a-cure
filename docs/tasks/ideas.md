to do: organize into docs

  - it should be possible to reverse engineer a set of system-generating rules by observing outputs and calculating 

    - the set of core components that would create them with the least work (prioritizing resource re-use, sharing functions, cost minimization, etc) & other determining factors of most system development like other system interactions

      - example: the most efficient way to achieve two functions that are variants of each other is using a variable as a function input

    - the set of required components for a system to function, & then calculating the sub components of required components simultaneously with information about other sub components as more calculations are done

      - example: the bio system needs a storage function, and edit function, a regulatory system, etc

    - a causal diagram and a system diagram can intersect on the causes that maintain their shape in the system

      - example: a causal structure that is generative & causative of system changes

  - example definition route of symmetry (same regardless of a transform & transform direction):

    - structural definition:
      - when energy gathers into a state that is easier to create than to destroy (when it uses efficiencies & shortcuts)
      - when the boundary formed is stronger than the energy contained in it
      - when information leaves traces/side effects that are not erased by reversing the transform (not a closed system with energy storage built in)
      - example of processes that are easier in one direction: it can be easier to navigate over a local maxima than to navigate up a global valley, or to navigate a map to identify type than a map to identify individual members of a type, which can change in ways that differ from the type definition, average, etc

  - empathy nn algorithm

    - start from positions: 
      - worst case scenarios related to attacks (planning an attack)
      - emotional sources triggering attacks (fears, etc)
      - intents related to attacks (get resources)
      - causes of attacks (irrational thinking)
      - strategies of attacks (false correlation, false sense of security, surprise)
      - strategies to prevent attacks ('never be surprised', 'assume the worst')
      - asymmetries leading to different perspectives (such as "lack of resources like information, understanding or concepts (no concept of 'fair fight', 'attack first', etc)")
      - reasoning to fulfill intents (there's usually a reason to take an action that they think will fulfill an intent)
      - alternatives to reasoning, including politeness, lack of thought, and other reasons for a decision

    - starting from all the positions & actions that have efficiencies & reasons to move the way they do or in the direction they do is a good starting point once those are identified

  - with information representing the constant vertices of a system, by representing information a certain way, efficiencies are gained in other information, like related calculation outputs 
      - what does the efficiency provide to uncertain/uncalculatable objects external to information?
      - with certain concepts as priorities or embedded in the structure (like orthogonality generating intersection spaces for mapping interactions), certain efficiencies in calculations are created
      - what filter set or subset of possible concepts can explain the information description system?
        - what other objects are explanatory, in addition to concepts (a slice of a system with a similar object or pattern), filters, core/generative functions, limits?
      - what connects the constants in this system, representing information behavior descriptions - are there system objects like validation/formatting filters, efficiencies aligning between information & uncertainty objects, or other objects beyond the conceptual layer of discoverable information systems?
      - given that information attracts information differently in different structures, what intents can those structures & the information rules be used for?
      - if paradoxes are representable as holes in logical value connections (gaps or jumps in the information system, like asymptotes are for values), what type of information values can occupy those holes, or do they act like a symmetry to connect different spaces?

      - does information withdraw into a superposition (structure with multiple potential information states) or an abstract generalization (generative structure of information or concept network trajectory of the information) or a compression (retaining some attributes of the information), once you remove its structure or remove it from a space where it can stabilize enough to attain a structure?
      
  - what is the correct position of a market, insurance, price, future value predictions, trades, currencies, etc given their definitions?
      - a market being a reason to trade unevenly distributed resources, like matching supply & demand
      - insurance being the allocation of risk

  - identify parameter subsets relevant to a function

    - what is the relationship between parameters of one function subset and parameters of another subset? 
      - example: in a function with an s-curve, one parameter set for the upright parabola function subset has an opposite parameter to the parameter set for the upside down parabola function subset
      - the significance of these relationships is that being able to generate all parameter sets of all function subsets involves finding common parameters & the transforms to translate parameter sets into the other sets

    - other parameter sets of a polynomial or other curve include:

      - the vectors that can be used to generate them, such as a 90 degree vertical wavelength vector from the axis to each curve midpoint

      - alternate ways to split a function (like horizontal slices, subsets determined by change rates, etc) can also have parameter sets with relationships to other parameter sets that produce a more efficient function description or generation method

      - minimal parameter subsets, like min/max points plus a vector set in between each to indicate direction/speed/iterations of change

    - these alternate parameters sets to frame/generate/compress a function can map to semantic objects 
      - the vertical vectors that can be used to generate a polynomial function could map to causal factors, and the same can be said for any other set with the potential to generate the function

    - other function metadata, in addition to alternate determining/generative/causative/descriptive parameters for the function & function subsets
      - alternate functions producing output probability distribution
      - adjacent functions using function parameters or parameters subsets, and functions that are likely to occur in a sequence including the original function
      - opposite functions (functions/parameters generating the exact opposite function given opposites of parameters or parameter subsets)
      - symmetry functions (functions of symmetries that determine/limit/filter the function)
      - vertex functions (functions generating determining/generative/causative/descriptive vertices, vertex subsets or vertex parameters)

    - determining: removes all uncertainties (no alternate function states left possible)
    - generative: can re-create the function in isolation of any other information
    - causative: generative factors that are causative (not all possible generative factors would be the cause or capable of being causative given other information)
    - descriptive: describe the function or its parameters or the subset that is its determining/generative/causative parameters, may compress the function or describe a subset of its information, and may describe output of the parameters or emergent function behaviors/states

    - determinant component-based (using variables and subset components like rows, highlighting useful differences like switching pairs) definition route: 
      - the determinant of a 2x2 matrix is the difference between:
        - a's impact on the top row x d's impact on the bottom row
        - c's impact on the top row x b's impact on the bottom row 
      - the difference in impact of two different sets of pairs, other than defined multiplication (a's impact on top x b's impact on bottom, c's impact on top x d's impact on bottom)
    
    - the standard output-based definition route: "the determinant is the scale of the impact on the n-dimensional multiplication metric (length, area, volume) given by a n x n matrix"
    
    - how do you connect these two definition routes?
      
      - aggregate variables (impact of a vs. total impact)
      - connect process uniting the two or the primary intended process (multiplication) with metric for that process (length/area/voluem)
      - highlight the point of using the matrix multiplication (to transform a vector set or its metric by a certain factor)

    - how do you identify the reason, or why you would use permuted vectors & the difference between them to calculate the multiplication metric, from these definition routes:

      - the reason to multiply permuted vector pairs:

        - multiplying the original pairs (a x b and c x d) gets you the area describing the original vectors: 1 * 2 = area of 1 x 2 rectangle 2
        
        - switching the pairs allows examining the interaction space of the components (the shape created by the points: 
          (a,b), (c,d), (a,d), (c, b) to find the changes possible with the variables described by different values of x & y
        
        - the generalization is permuted subset determinants scaled by the remaining row
          - 'permuting' meaning pairing a remaining element with those it isnt multiplied with (same row) and elements it doesnt alternate with (same column)

      - addition/subtraction operations are alternated to handle component pairs that align with different subsets

      - the reason to subtract one permuted pair from another:

        - from the interaction space, to get the length of the sides of the interaction space, we subtract coordinates:

          (c - a, b - d) is the length of the differences between the two points (a,d) and (c,b): (x' - x, y' - y)

        - the output of the subtraction represents the difference in scale changes achievable from the two permuted operations (a,d) and (c,b),
          - with the assumption that the difference between them is relevant

          - sub-reason why their difference is relevant: bc we are looking for the length of the shape generated by connecting origin points and permuted points, not the length creatable by adding them, so we have to subtract them

        - 1 2    2  2     = 6  6    = factor of change matrixes: 6 3   3  3   = determinant  -6
          4 2    2  2       12 12   =                            3 6   6  6

      - connecting rule: from variables & sub-components & subsets, the interaction space can be generated, and an aggregate impact matrix determined from different combinations of the variables/sub-components/subsets

        - sub rules:
          - given that multiplication generates multiplication metrics like length/area/volume, and subtraction generates difference metrics in a dimension, which combination of multiplication, subtraction, and variables/sub-components/subsets will produce a multiplication metric for the original operation
            - generate interaction space
            - find each difference between generated points & original points of interaction space
            - sum differences 

      - the generalized definition route & conceptual path linking definition routes to explain the intent of the operation would involve objects & processes like the multiplication of the remaining row elements to embedded objects that may be an element or a matrix (x * y subset matrix scaled by a third element z)

    - persuasion & sales automation

      - types

        - no sell: 'they dont need you or your purchase, you need them'

        - trick sell: involves a lie of some sort, like a logical fallacy

          - lie of omission: not correcting someone
          - logical fallacy: slippery slope

        - hard sell: portraying a non-essential product as essential

        - personal manipulations like:
            - flirting
            - similarity
            - compliments
            - understanding/sympathy
            - giving them a chance/benefit of the doubt
            - trust
            - extended examples: 
              - setting a trap so they break something then forgiving them
              - stories to demonstrate their personhood & trigger your social urges to give them more opportunities to connect

      - attributes

        - association
          - associate the product with things they like or that everyone likes
          - includes attributes like a personality trait (only adventurous people buy it)

        - relatability
          - analogies based on their life or with terms they understand, given the information theyve told you or common information

        - distortion
          - example: portraying a likelihood that is non-extreme as extreme

      - functions

        - add filler speech associated with good communicators
          - checking their understanding
          - repetition

      - state

        - emotional state
          - fear of a negative event that the product could prevent
          - confused or thinking or busy, other mental states than emotions than can influence sales
          - jokes to make you laugh so youre in a generous mood

        - adjacent to an invalidating state (soon wont need the product)

      - as a neural network, persuasion algorithms can be used to 'sell' a different label to a network

    - confusion nn: algorithm that tries to trick a network into thinking a set of labels is correct, and the algorithm either:

      - optimizes for confusion/ambiguity for obfuscation, or for deriving differentiation methods (patterns of difference) between conflated types
      - tries to identify confusion distortions as a way to implement a common sense algorithm (if it fulfills confusion metrics, its unlikely to fit with a common sense system)

    - creativity nn: algorithm that mimics creativity in thinking of alternate possibilities/explanations
      - switch structure, abstraction, type, attribute, function, direction (like how random jokes come from a totally different direction but still have an intersection)

    - these mental processes can be used in different positions, for different functions:
      
      - creativity
        - new paths/combinations
        - alternate paths/combinations

      - confusion
        - obfuscation
        - ambiguity
        - differentiation
        - deriving patterns (in order to resolve the point of confusion, they have to figure out why & how two types might differ & test distortion patterns to get to those differences)
      
      - persuasion: 
        
        - 'get resources without earning them' (get funds for investment in another product or a future version of the product)
          - the structural translation of this is: 'move to a target position without the aligning action to propel the move'
        
        - 'get someone to act against their own interests' (get funds for investment in another product)
          - the structural translation of this is: 'move to a target position without the aligning self-interested intent to motivate the move'

        - 'get someone to match their behavior to their own interests' (the product is good for them but they dont automatically seek it out)
          - structural translation: 'move to a target position by changing the intent direction toward the beneficial resource' (to prioritize doing whats good for them)
        
        - 'get someone to align their interests with the group' (the product is good for humanity, like free trade or other products with distributed benefits)

    - intent analysis computation example: function design for a particular intent

      - example for selecting bernoulli function as a parameter to embed other difference-maximizing sigmoid-scaled regression functions into, in order to find an error-minimizing function:

        - embedding difference-maximizing sigmoid-scaled regression functions into the bernoulli distribution for each xi/yi pair produces the cross-entropy loss function:

            - https://towardsdatascience.com/the-statistical-foundations-of-machine-learning-973c356a95f

            - given the classifier modeling function generating each yi from xi, which for fitting a function for linear regression is:
                o(ax + b), where o(t) is a sigmoid function to differentiate outcomes
              
              for the probability of E given p above with the binomial distribution function (bernoulli case) 
                p^(p occurrences) * (1 - p)^(not-p occurrences)

              framing it instead as the probability of yi given xi,
                f(xi)^(yi) * (1 - f(xi))^(1 - yi)

              then substituting the classifier modeling function for f(xi),
                o(ax + b)^(yi) * (1 - o(ax + b))^(1 - yi)

              then apply the log to find the maximum of the function

      - the reason for applying the bernoulli distribution for each x & y pair is not-determined, whereas the reason for applying the log is determined (calculate maximum)

        - meaning there are other ways to map each xi to yi than by:

          - mapping number of occurrences of an outcome => yi
          - mapping likelihood of an outcome => function applied to xi

        - so why use this specific function to model xi & yi?

          p(E|p) = p    ^(p occurrences) * (1 - p)    ^(not-p occurrences)

          p(yi | f(xi) = f(xi)^(yi)            * (1 - f(xi))^(1 - yi)

          p (probability of event 'TTH' | probability of 'H' 0.3) = 0.3 ^ (H occurrences) * 0.7 ^ (not H occurrences)

          p (probability of event yi | f(xi)) = f(xi) ^ (yi) * (1 - f(xi) ^ (1 - yi)

          = the likelihood of getting outcome yi, given the connection function f(xi) = connection function ^ outcome * (1 - not connection function) ^ (1 - outcome)

        - we're treating this as a conditional probability problem, where we want to find the success of a predictor f(xi) on an outcome yi, so f(xi) is treated as a proportion that can predict the outcome

        - other models like Bayesian probability would fit their functions of p(B|A) as probability of (yi given f(xi)), or some parameter/parameter set/component of f(xi)

        - so the above example uses the sigmoid-scaled linear function (maximizing differences to aggregate values near 0 or 1), by injecting it into the bernouill model (which is appropriate for estimating probabilities of boolean outcomes 1/0) in the position of a predictor, applying it to each data point & summing the outputs, wrapping it in the structure of a general error-calculation function

        - this is why this set of operations done to these functions can produce a loss function for estimating error of a classifier

  - why do you arrange dimensions at 90 degrees? to examine the full interaction space of all possible combinations of the two variables
 
  - stereotype nn algorithm

    - log objects of bias like:

      - false similarity between rule & data
      - correct contextual rules, that are incorrect in new/most/unexperienced/possible/common contexts
      - correct rule component/input
      - similarity between biased rule and correct rule structure/input
      - lack of concept of alternative possibilities
      - priorities that mimic or create error types like over-prioritizing safety can look like prioritizing efficiency or being biased
      - incorrect rules applied to other data like other biased rules producing strong bias functions rather than correction functions
      - structures that incentivize bias like lack of update rules or adjacent destruction of correction rules or lack of protection for connections preventing bias
      - mismatch between time to required decision and time to make change or query for new rules
      - concepts:
        - stereotype rule dynamics (how they form, are destroyed/changed/replaced)
        - scope/specificity/generalization
        - stupidity which is related to bias
        - reasons not to look for contradictory/other information types
      - misidentify output (wearing a similar outfit to criminal for different reason than crime) or cyclical output (wearing similar thing as an activist given culture progression who is known to be against the police can increase potential for police violence) as a cause
      - misassigning the concept of responsibility (the source of root cause), such as misidentifying the aggressor/victim as being at fault
      - misassigning the burden of the prevention function (burden is on the aggressor), according to power dynamics (the aggressor has the information about their own violent intents, the power to check information & the responsibility to use it)
      - structural gaps allowing bias to gather/develop (gaps in rule enforcement/validation checks or over-prioritizing priorities known to create bias if not restricted, like efficiency/safety, or processes that have the same output like fear/caution/scanning for threats)
      - structural components that form the inputs or components of bias, making it likelier to develop if those components are allowed to interact
      - structural components that make bias more likely, like frequently requiring fast survival decisions

  - voting for algorithm of resource distribution

    - example algorithm input/output: 
      - 'this algorithm will guarantee these resources at x intervals if you do y work, and provide these possible resource streams if z work is available & a technology is accessible'

    - example algorithm:
      - 'invest in diverse research projects to fulfill basic intents first (researching number of evolution steps you can skip when gene editing before you create toxic plants, researching whether plants should be optimized or a composite plant should be designed with multiple intents like reducing water intake & maximizing oxygen output'
      - 'invest in solutions so that cost/reward ratio of solutions is kept above ratio x to avoid exploits unless the system is closed and optimized, and below ratio y to avoid high-cost solutions unless theyre abstract & fulfill basic intents'

    - the number of people required to participate to get to that resource distribution could be a filter, so only realistic algorithm participation (algorithms that people are likely to vote for) is an option
    - sets of realistic algorithms (each algorithm set covering income for a population) make it to the voting ballet, and the set that is voting for the most above the realistic threshold is the default winner
    - iterations of voting can apply subsequent filters of resulting algorithm sets like optional/potential work agreed to, and allow debate and lobbying to get higher rates by promoting or educating people for different types of work (they can vote for an algorithm requiring more investment if they have x signatures & if the work will produce solutions to reduce costs overall for all or other groups, otherwise an income rate isnt guaranteed)
    - monitoring, fixing, & enforcement agencies can be used as necessary (when new drugs are discovered, systemic failures occur, etc)

  - tool to make bacteria grow until detectable & then represent with AR
    - this closes the gap in information:
      - the information needed for the product (a testing & display tool) was insufficient
      - the mechanism to make the information sufficient is accessible (solutions to feed bacteria)
      - rather than make the measurement tool better, you can make the information more accessible (change the position of information rather than the position of the measurement tool)
      - the AR component would augment the size to make it displayable to users without using the measurement tool (microscope) or indicate size/type of pathogen with standard data visualization rules

      - applying the testing tool to hub nodes (then inputs/outputs of hub nodes, and adjacent objects to hub nodes) to check with increasing certainty whether surfaces were compromised
      - redistributing resources based on immunity/infection status to avoid cleaning, like distributing unchecked, unsanitized, or contaminated goods to consumers with immunity/infection 
      - keeping an animal likely to develop fast infections in buildings and then using air conditioner to distribute viral RNA when the building is unused and checking remotely if the animal shows symptoms to see if a building is safe is one way around the limitation to detect live copies of the virus, since the animal symptoms are easier to measure than a pathogen, so the problem becomes ensuring the animal will develop symptoms rather than immunity, and making sure the symptoms show up faster than a test done on hub nodes or quickened with enzymes/sugar/cells that help the pathogen replicate

  - to do: 

    - why are polynomials with a leading coefficient of one & having optional zero coefficients capable of being multiplied & added to give the roots to the system of equations?

      - bc scaled versions (through multiplication) and combinations (through addition) can position leading coefficients to be 1 while also positioning trailing coefficients to be zero

      - is there a polynomial coefficient set that you cant multiply or add in a way that makes a coefficient in a position to be one and every coefficient after it except the solution coefficient to be zero? 
        no, bc that would mean the leading term cant be solved

      - once you have leading coefficients equal to one and the trailing coefficients equal to zero, the system is solved for each variable

      - why would you try to solve for each leading term in a matrix? bc they are ordered at that point (formatted to have the same positions in each row) and the solution is also ordered (on the right side)

        - using the concept of position to produce additional organization of information, you can benefit from the alignment of the variable positions by isolating each variable (transforming with multiplication & addition of coefficients until the variable solved for in each row has a coefficient of 1 and all other coefficients except the solution are zero)


    - how would you determine how to use matrix multiplication to solve systems of linear equations, without knowing matrix multiplication?

      - use common function in solutions: organization
      - apply that function to inputs (disorganized info) using a common definition of organization (order)
      - what changes across functions that could benefit from being ordered? variable position
      - once ordered, what has been added? standardization of variable position
      - whats missing for full standardization of position? coefficients with value zero, which adds the information necessary to make functions have an equal number of terms
      - what other changes would add information?
      - another common function in solutions: 'applying a filter to isolate differences'
      - what describes difference across core objects (variables)? coefficients
      - isolating the differences in a function takes the form of a vector of their coefficients
      - now you have an ordered vector of coefficients
      - whats the best way to compare lists of numbers, where the positions align (theyre ordered in the same way, and are of equal length bc of adding variables with zero coefficient)? stacking them
      - whats the goal of the solution (find value of each variable that fulfills all equations in the set), framed in this format of stacked lists? 
        - a format where each row may set each variable equal to its solution value
      - what could produce this format, where a variable is isolated on each row (other variables have coefficient zero)?
      - using a version of the other functions in the set, as assumptions of transforms that can be done on other functions in the set, which therefore dont disrupt the validity of the function being reduced to a variable and its solution, since the reason theyre in the same matrix is because theyre supposed to be solved together
        - rules about the values of x & y also apply to rules about the values of y & z,
          if x, y & z are in the same set of linear functions that have a common solution,
          so rules about x & y values may be used to reduce the y term in the rule about y & z values, leaving z to be equated with its solution
      - another way to get here is noting that not every function will have every variable, and that a function missing a variable can be framed as having that variable with a zero coefficient, and zero coefficients of other variables are that frame applied to the solution for the remaining variable
      - now that youve identified the important object of assumptions that can be used as information to organize the system, you can examine how these assumptions allow paths to the target structure implied by your goal, which is a matrix where each variable indicated by the column has its own isolated row where its been tranformed so the other variables have zero coefficients
      - operations like multiplication and addition are within the bounds of the function type definition (linear) which allows shifting and scaling without leaving the type
      
      - this means by:
        - using other functions in the set as a base
        - using term coefficients, variables, & solutions as core objects
        - identifying assumptions that can be used as inputs to generating a set of definitively allowed operations
        - using the zero coefficient format to frame solutions
        - using multiplication/addition as changes allowed within the type symmetry
        - identifying target structure, 

        you can know that these operations will add information to the structure, 
          - given the organization already applied when creating the matrix, 
            - by producing an insight like:
              - "rules hold across functions in the system, within a limit allowed by a type symmetry defined in the problem" (system of linear equations)

      - which can be reduced to a general method like:

        - standardize objects (terms) to a common format where common format parameters (order & position) & their common definitions are applied
          - identify insights like 'in order to align the position attribute, the missing variables in each equation need to have a coefficient of zero'
        - align relevant attributes (position), objects (terms), and functions (equals)
        - isolate variables of difference (coefficients) across objects (terms), where a constant is considered a solution term & isolated from the other terms bc of its different type
        - identify target solution format (set of equations where each variable is equal to a constant)
          - identify implied insights like 'a solution defined in this way also has missing variables with a zero coefficient'
        - query for available rules to use:
          - identify assumptions (both implications derived from the structure, such as the implication of a rule being included with other rules, & explicit assumptions)
          - identify operations that dont change the problem space (operations that preserve linearity of functions)
        - map isolated, standardized, formatted, & aligned objects to the target solution format, using assumptions & common operations & containing structures (like sequences of operations) that dont change the problem space, to keep organization, structure, & alignment relevant
        
      - you can also start from the target solution format (or insights to build it, like the zero coefficient method of aligning term positions), and derive organization methods, assumptions, & operations that can convert the function set to that format of variables & constants, given that solution format

      - you can also start from the insight interface rather than the structural interface and build assumptions from those insights, then look for objects matching the objects in those assumptions and check if information is gained by applying the assumptions to those objects which helps the target solution intent of 'aligning each isolated variable on one side and a constant on the other'

      - you can start from the core interface using operations core to the problem type (linear equations) and the core objects of the matrix structure (coefficients of a function)

      - to do: look for corrollaries between matrix multiplication and ml, as theyre both combining sets of equations using value vectors as input to create an output set of coefficients/values, such as:

        - a matrix of values is used in the position of a term or a function in the matrix multiplication example
        - by applying order & position in the form of node layer & trajectory (like a row in the matrix), the info of the input vector is organized in a way that adds value once applied across the rows
        - by varying weights & weight path patterns within a range (maximizing differences & other useful metrics across weight paths), it mimics the variation of operations within a range (linear)
        - weight paths can map to rows in the matrix, bc the rules/success of one can be used to infer the rules/success of another - same for nodes in a layer & nodes across layers that contribute to each other (nodes can be used to infer the success of later nodes, even before multiplying all of the node outputs & weights for the final node)
        - consecutive weights in a path can map to consecutive multiplication operations applied to a function in the set - so the weight path would indicate the steps required to turn a function (initial weights multiplied by input features) into a prediction of the dependent variable
        - the rules of other functions are used as inputs to other function reductions, so they act similarly to filtering influences in the neural net, like threshold values, optimization limitations, weight path differences, weight path gaps, weight paths & patterns indicating type or other determining metadata, pooling functions, etc
        - standardization allows for comparison of values across both structures (standardization of position vs. standardization of value)
        - alignment between data set patterns and aggregation & weight patterns
        - networks infer the coefficients and matrix multiplication infers the values needed to produce the coefficient sets
          - you could start with a set of probable coefficients as determined by function patterns, and reduce them to the solution of those sets of equations to see if it can produce real data
          - in a matrix, the coefficients act like data vectors, and the solutions are the coefficients:
            - "what value of x, y, & z will make this coefficient data work together"
          - in a network, the data is used to produce function coefficients
            - "what coefficients of x, y, & z can make this value data work together" 
              - translated to a matrix structure: which scalars applied to data with the multiplication operation can isolate the most representative data or most probable weight paths/patterns
          - the coefficients & the variable values occupy a similar position to the weights & the input values - can you extrapolate this to isolated scalars applied to different terms
            - matrix multiplication is to solve a set of equations - weight paths represent theories of operations to transform equal/random coefficients to prediction coefficients
            - if you could select the representative data vectors out of the data set, matrix multiplication might be a useful operation to solve for linear coefficients of the function
              - which method do you use to relate a function's linear version with other versions, where you know either the coefficients or variable exponent output values, but you dont know the exponent used for variables & you have enough examples to calculate it? how does the log change in relation to the prediction function as you change exponents?
              - where input data values are the terms in the matrix, where variables are the coefficients you use to modify those values to get the dependent variable value 

      - integration methods are another place to look for corrollaries, bc the method of aggregation can be used to achieve different levels of complexity better than other methods

        - example: aggregate, starting with big blocks or even blocks, until the level of change allowed without crossing a boundary is insignificant, where the change rate between block sizes as they decrease mimics momentum patterns like found in type differences & differences between function versions (like the generalized version and the specific or adjacent or subset versions)

        - absolute change: a metric measuring across change types (so the absolute change value at the top of a parabola would include the slope & impending direction change & tangent encounter & impending intersection with 0)

        - the momentum of change in a wave is like an ellipse, which has slower change in some areas - do rules from ellipses apply to waves?

      - the main causal structure where existing prediction tools are useful is where the variables are all on different dependence paths/tree branches that dont tie back to the causal network except to contribute to the dependence variable (& other irrelevant variables to that function), or theyre on the same degree of causation (or the same layer) away from the dependent variable

        - all other causal structures arent well-handled by existing prediction tools


  - add to ml explanation

    - in a fully connected network, every weight path from the previous layer in passed on to each node in the next layer, meaning that if a set of weight paths with additional weights applied cant produce a 'feature found' or 'yes' value above the threshold, those weight paths & additional weight combinations arent passed on as candidates for the prediction function or building blocks of it

    - the aggregation of weights & input values creates a tree of trees on each weight path, where leaves are the input values, which are multiplied by a weight as they hit the branch, and the branch represents addition creating a new hub or joined branch, and each tree is sent to every other tree branch hub in the next node layer
      - can you calculate anything faster by changing this structure, like adding memory in between each tree, arranging them around the memory like in a circle, so changes, similarities, etc are synced with the memory before proceeding (so the nodes would be computing things like change rate to check if its worth mentioning to the hub, rather than checking value compared to a threshold, which would be the responsibility of the hub memory)


  - definitions:
    - derivatives/options/other instruments: possible money if your prediction about future value is correct (where value is highly subject to risk & influenced by many factors & randomness)
    - insurance: future money from negative unlikely predictions occur
    - debt: temporary money/resources in exchange for possible future value of a prediction (like that your plan will succeed or that a resource will increase in value), & possible future debt if purchased resources or plans dont increase in value

  - diversification in that context means creating an investment portfolio/smart contract based on permutations of the insight powering the bet and the likely distortions of the insight, as well as permutations of related bet assumptions & other objects, such as a logical tree of conditions & tests to deactivate that investment
  
  - borrow from lenders with aligned incentives to borrowers' planned resource usage/investments
  
  - hedge the risk of borrowing by investing a ratio in alternatives/contradictory trends/products

    - example: borrow to start a delivery business vs. investing in local gardening or 3d printing, and be prepared with a plan for investing resources when delivery business turns into supply pipeline, logistics, auto-mining, or tech business

  - the logical flow of these objects given their definitions is to arrange possible prediction money, risk expectation, and temporary money/possible future debt in a way that creates a self-sustaining loop that keeps revenue streams active enough to keep the circuit accessible or convert it into a more profitable one

    - for example the combinations:
      - 'the risk expectation of possible future debt'
      - 'a prediction payoff of a risk expectation'
      - 'temporary money borrowed from a prediction funded by value prediction & risk expectation payoffs'

    - a three-item circuit such as:
        - predict future debt (default of a loan) to get a payoff if loan default occurs
        - insure against future debt (default of a loan) to get a claim payment if loan default occurs
        - invest temporary resources borrowed in prediction & risk analysis tools/methods or loan business dependencies

    - a one-item circuit: 
      - borrow from investors who already invested in dependencies of your plan or bought insurance from a company that did
      - go into debt to fund purchase of insurance against interest rate increase

    - a circuit based on randomness:
      - invest borrowed resources in projects/patterns/types that appear frequently in diversified portfolios - they may be uncertain in isolation, but given the frequency of their investment & bundling, they may be less uncertain

    - building a structure logically

      - system analysis:

        - given a set of contradictory predictions (a trade-off or where only one prediction can be true), you can buy insurance against:
          - the question of which one is true being invalidated
          - the predictions having a wrong assumption restricting it to two options & the third option being true
          - the prediction with higher investment having lower return
          - either or both predictions having no return
          - the assumption of them being mutually exclusive being wrong and both being capable of being true at the same time

      - causal analysis:

        - investing in something can increase the value of it
        - predicting an event can increase the probability of it
        - buying insurance against something can decrease the risk and the payoff value (bc the insurance company may invest in decreasing the cost of fixing it & change the terms)
        
      - structural analysis:
        
        - which structures (set, tree, loop, network) are useful to frame investing objects (stock, derivatives, price, product, purchase, loan, insurance, probability, value, payoff, risk, etc)

      - core analysis:
        
        - which core component combinations are already used (such as 'option to sell', 'promise to buy', 'periodic repayment of temporary borrowed at interest', etc) and what are the next combinations on other layers or filling in gaps on existing layers?
        
        - given core functions (sell, buy, bundle, trade, promise, predict, etc) and core objects (option, transaction, contract, etc) and core attributes (probability, dependencies, value), what useful combined objects can be generated?
          - value: initial value, combined value, potential value, conditional value


    - these structures like combinations, embeddings, and loops can be used to construct an optimal investment tree or network to allocate resources in a way that maximizes payoff potential

    - arranging these objects in a useful way can create exploit opportunities
      - example: if one risk is artificially increased by spoofers during risk expectation calculation

    - these objects can be arranged in optimal ways using system rules & filters like:
      - check for aligned intent (maximize profit) of combinations & other structures
      - check for adjacent interactions (a prediction can take information as input, so any object that generates information is one of the candidates for the preceding object to a prediction in a sequence)
      - check for compounding & cooperative effects, which generate efficiencies
      - check for false similarities to avoid side effects
      - use alternate definition routes to check for alternate similarities and routes between objects
      - standardize definitions to use same terms where possible to calculate outputs (frame prediction, risk, and resource investment objects in terms of payoffs, risk/probability, dependencies, & assumptions)

  - core investment attributes:
    - fundamental
    - abstract
    - produces a valuable attribute in relevant problem space
    - produces a proxy for work (trust)
    - can override fundamental rules (like those regarding power structures)
    - can increase interaction rate/space
    - produces attribute alignments & other important system objects that output change, stability, or other valuable system attributes
    - captures previously unhandled variance
    - interface good (an average/limit, a currency, an interim good)

  - risk based on key metrics like:

    - max price of a good, given its relative value cap, absent of extreme or phase shift conditions like war
    - 'risk caps' (worst case scenario as a standard) and 'potential caps' (potential value of good's attributes/components) in various market condition types (applying filters to avoid worst positions)
    - attribute combinations that create phase shifts (alignments, resolved conflicts)
    - intersecting interfaces that can override logical positions (social interface can influence even basic good prices like vitamins/food)
    - variance flow (adjacent input/outputs/systems compromised) & adjacent variance objects allowed to interact 
    - exploit potential (exploits can be assumed as guaranteed, so investments in easily exploited products should be either concentrated to guard against these, or distributed)

  - an example of how over-dependence on a fact makes it false:

    - fact: "fossil fuels are valuable in a problem space with cars that use them or their byproducts for fuel"
    - heavy investment in fossil fuels could indicate their popularity
    - if many entities rely on them, their supply will be reduced, assuming they cant be generated with imminent tech
    - the competition given reduced supply will produce fast innovation, resulting in side effects
    - side effects in a fundamental interface for existence could put existence at risk
    - side effects from mining could also produce an environment that generates new "fossil fuels" or their equivalent position (natural resources that are easy to use for a fundamental intent), but this is not as likely as causing side effects that put the whole system at risk

      - why is this less likely to produce good side effects?

        - the methods of transforming substances into other substances have costs (pollution of reaction byproducts)
        - the elements likeliest to be useful as fuel produce the most pollution
        - the elements likeliest to survive millions of years in this environment on/near the surface are the likeliest to produce the most pollution
        - the mechanism of using fuel (engine & other components) isnt likely to require innovation invalidating fossil fuels right away
        - alternate fuels arent as adjacent as digging in the ground & burning, which are core mechanical/chemical functions
        - the byproducts of fossil fuels & reactions arent likely to be easily generated using alternate methods, given likely tech

    - over-dependence on that fact (beyond its ability to support stress) makes it false: they were valuable in that problem space, but over-dependence on them created negative systemic side effects, and now dependence on them is a liability

      - the fact collapsed into a network of related facts like "alternate fuels are valuable" and "an additional use of fossil fuels could cause system collapse"

      - this is a mismatch problem type of imbalanced dependence & support functions

  - analyzing just by change rate makes it less likely to spot other patterns like overlap/intersection of patterns

  - difference develops where there's potential for new interactions to develop (so a steady or increasing rate of change) & intent (like a possible gain from the difference)

  - the object model may not be the right default to start from in most situations - there arent many whole objects in existence if there are any
    - even particles have sub-particles, and the extent of that chain isnt known, and may have a causal relationship where the smallest particles act as inputs or injection points
    - should ratios/bases or sets be used instead ('a set of particles' rather than a 'plant' as a standard unit)
    - when selecting a default, you should be checking for attribute matches (does a whole object make sense to describe a set)
    - the idea of a whole number may describe something that doesnt exist in 3-d physical reality - does that mean its a concept that will never occupy a form, or is it a goal physics will move towards, or it causally independent from other systems or interfaces that are known, or it evolves as brains can measure information

  - math doesnt just describe concepts like 'whole number', it also describes basic interactions like pairs, sets, combinations, which appear in most systems 
    - are there objects in systems that are real but not describable with our measurement/description tools

    - this system of math describes certain change types, values, & relationships 

      - so something that doesnt have a structure, or value, or cant be changed with any operation, and doesnt relate to anything might not be describable with known math

      - for example a space-time with one object, which has no other objects to be used to compare to it except the defined boundary of the space-time (which itself may not be measurable or comparable to the object), and the space-time is in constant flux so position, structure, & change cant be measured, just estimated 
        - or the idea of distance is invalid because change is so fast or chaotic it cant be measured, so you might try to describe its behavior with a probability distribution, but if it doesnt follow one, you may need a network of distributions, rather than a scalar value describing some distance type between positions
        - or its in all positions as a possibility (an adjacent structure that would generate or attract the object at faster-than-information speed) and its next occupied position cant be measured, so the idea of distance is invalid, because not only is it equally likely to be in any position, but its next position is also equally likely as any other position because distance doesnt matter, as it can either travel fast enough to make distance irrelevant or it can travel through other spaces faster or each position is a potential object thats about to crystallize and which position actually does is determined by the travel of faster objects like energy

      - math breaks down with some interactions of change types that create ambiguities so information cant be measured

      - if you try to create another description system, you end up changing definitions, variables, bases, or methods but not the system itself, which is founded on numbers having absolute value & core operations to compare/combine them

        - in order for the system to exist, some objects have to be defined (a base concept crystallized into a structural object)
        - what description system do you get if you dont use units/value as a base (ranges, definitions, attribute sets, interfaces as a base)
        - are there other possible description systems than these
        - in generating other description systems, you end up adding assumptions to physics rules instead
          - if there was a system with no concept of equal (no objects are ever allowed to be equal) or where continuity didnt exist (no fully connected objects or whole objects, just sets/sequences) - you can create other spaces with defined operations, but if you tried to apply it in euclidean space (y could never equal x and x could not cause y), you wouldnt be able to do some operations like comparison or cause, so that would be applicable for spaces where objects couldnt occupy the same position or spaces that could only contain one object so objects couldnt interact
      
        - in general it describes change & relationships & spaces but not structures like rules/filters that generate description systems & its advantages, and describes information but not:
          - missing information
          - possible information (ambiguity resolution, adjacent/likely information, shapes of uncertainty, information route)
          - unmeasurable information
          - information logic (what constitutes validity or logical inference in a space, given that validity is usually defined for core objects rather than all the possible emergent interactions in a space, and whether space components like number types or definitions are generated by or used to generate other spaces given inferrable/measurable logic)
          - information metadata (intent, cooperative/contradictory information)
          - semantic information (how does position/distance/value map to the concept of equivalence/similarity, given the concepts of thresholds indicating which value differences are important)
          - embedded information (parameters, bases, conversion functions applied, prior parameter values)
          - complex information (concept position in a network is generally attempted to be described as a network but that leaves out position/direction as information, even though the information of concept node cluster or linking function is useful in 2d, partially real numbers that are generated by combinations of attributes but arent completely described or understood in all their possible interactions)
          - meta information (describing itself): what is the set of all equals, what is the most unique number, what is the average definition of difference

    - are there materials/processes that erase information (convert back into a superposition) or which can vacillate between discrete/continuous like waves, skipping information & leaving gaps that can break the chain of information (breaking time reversal symmetry)


  - mask design can be optimized as a cover with one output flap like an esophagus preventing input on one nostril so the other can be used exclusively as an input with a filter 
    (better to sanitize at point-of-usage in environments with many unpredictable interactions like wind direction and interpersonal contact, which can get around most masks)

  - causal shapes integrated with networks (patterns of aggregation matching causal shapes like trees & circuits)

    - integrating system analysis with networks
      https://twitter.com/remixerator/status/1150578597339340805
      https://twitter.com/remixerator/status/1205724743741014018

    - system of causal types (integrated with type path example as a version of weight paths)
      https://twitter.com/remixerator/status/1156860484294852609

    - causal types
      twitter.com/remixerator/status/1126040476023279616

    - applying causal shapes to a network
      https://twitter.com/remixerator/status/1004579263507566592

    - position on causal type network
      https://twitter.com/remixerator/status/1018540899859607552


  - types can be represented as directions (going farther from origin goes further up type stack, where similar types are adjacent)
  - change phases for causal analysis (interim, changing, diverging, standard, efficient state, constant, interacting, converging, on the verge of obsolescence, outlier, etc)
    - superficial cause, alternate cause in the case of a function, addressing input/output causes
  - framing on interfaces, decomposing causation, then identifying parameters of problem on layer & matching solution
  - independence (closed trade loops) as time storage
  - vertex as a pivot point for an interface

  - add diagram for question derivation for service list to automate chat bot entries

    - deriving the questions customers will ask for a set of services

      - which processes are complicated or not optimized (need to be in person for certain transactions that people would rather do online)
      - which processes involve changing information (account balance, transaction approval)
      - which processes are likely to have errors (auth)
      - which processes people will likely be interested in using the most

    - what percent of changes are just from finding efficiencies, using those as a foundation for common distortion types (random change, directed change, connecting change, etc)

    - what system of core objects/functions/attributes generate a space where:
      - circles & squares are fundamental or standard objects
      - there is a continuous spectrum of values (real numbers) around which alternate number types rotate (complex numbers, etc)
      - comparing change generated by two variables (one independent value function determining the dependent value) has patterns of measurement potential
      - isolating by attributes (like isolating direction & scale to transform to a vector space) or framing information in different structures (sets, matrixes, sequences) allows patterns that are calculatable (implying the framing filter is determining, so matrix attributes can be determined by its definition)

    - what objects describe lack of information like ambiguities or lost information, other than randomness (difficult to identify randomness), variance (lack of patterns), and infinities (lack of information being difficulty of computing the sequence except in terms of other infinities if it doesnt converge, or lack of guarantees that the sequence can be maintained/stabilized to continue)

  - when physics rules stabilize, they attract & generate information, which gathers into measurable numbers
  - if the point of the universe is not to find the initial filters but to prevent that information from being discovered, that could keep open options for other change sources

  - do repulsion principles mimic or have causal relationship to symmetries around boundaries (like whole numbers represent a container that is full) 
    - or do they indicate a lack of symmetries in that space (attractive forces act on the irrational & fractions rather than integers)

  - as change is injected into the number of sides (moving towards non-linearity) of a closed regular shape (polygon), change may leak outward from the center to form a polynomially described wave/spiral or other shape described by the polynomial's exponential motion
    - in what space would the number of sides or non-linearity of a closed regular shape leak into another direction, indicating the wave shape
    - how do you map the polynomial shape to the change generated by adding (or subtracting) a side

  - polynomials are like transforms around a base which forms the constant for the symmetry to develop at (adding or subtracting versions of the base, as the transforms)

  - cyclotomic polynomial roots may act like a wave function around the x-axis

  - imaginary numbers are like roots with a value attribute that compresses to a position attribute (square root of -1)

  - the roots (intersections at zero) are the different types of transform (addition & subtraction of scaled base versions) positioned as equivalents

    - the implied question is:

      - "how do you transform a set of addition transforms of this scaled base version into a set of subtraction transforms, given the constant term" (using the root values as the base)
      - or "how do you equate these two vectors (addition/subtraction set constants) with one number, in the case of a one-variable polynomial"
      - or "how do you combine the concept of area related to points, lines, squares, cubes, etc to generate these vector sets"

      - system analysis of solution: https://www.quantamagazine.org/new-math-measures-the-repulsive-force-within-polynomials-20200514/

        insight: "polynomials and power series can confirm attributes of the other"

        convert problem type:
          "question about the size of roots of polynomials"
          "question about the size of values associated to a power series"

        requirement: coefficients needed to be positive or negative whole numbers

        insight path:
        "
        a non-cyclotomic polynomial
        found its roots
        raised those roots to different powers
        multiplied them together
        took the square root of that product
        based on that square root, he could construct a power series with the essential property (coefficients are whole numbers)
        "

        intent:
        "
        a non-cyclotomic polynomial             (roots not on unit circle/roots following repulsion pattern)          get relevant object (non-cyclotomic polynomial)
        found its roots                         (found numbers that equate its transform vectors)                     find the symmetry of sets generated by its transform types
        raised those roots to different powers  (permute polynomial exponents and apply to roots rather than base)    create polynomial with roots as bases (or even orthogonal change distortions of roots as bases)
        multiplied them together                (apply change from roots orthogonally)                                multiply terms of multivariate polynomial (multiply orthogonal change distortions of roots)
        took the square root of that product    (find number to produce product with one multiplication (squared))    find alternate standard generative factor (specifically square root) of the product of that multiplication
        "

      - insights (mostly based on similarities):

        - after finding roots, youre doing a similar operation twice:
          A. first create alternate output (root to a power) of factors (roots)
          - then (after multiplying the alternate output)
          B. find alternate factors (roots vs. the final square root) of output (product of distorted roots)

        - another key point is the default relationship between roots (multiplied by each other) and the interim operation (multiply roots taken to powers) between symmetric operations A & B 
        - another point is the transformation of a root into a polynomial term, given that its being multiplied by itself
        - then these polynomial terms are being used to create another polynomial term (x^2), whose output x is relevant to a related power series of the original polynomial, with integer coefficients
        
      - generate insight chain (of specific insights):

        - factors (x - a) are important objects in polynomials and act as power series factors (x - c), so given that theyre an important object, we can select the 'factor' operation as something to try in the insight path first
        
        - given that factors differ between the two function types, we should look for alternate factors as one of the insight path steps

        - since we're transforming something, symmetries should be a key object we use first (like embedded or containing symmetries, such as the symmetry between the two similar operations)

      - questions

        - what is the relationship between polynomials with duplicate roots & distribution of roots around the unit circle
        - why would this function/insight/intent path produce a power series with integer coefficients (scalars) applied to some center c?

  - an infinite series implies a stabilized symmetry (a platform for change that goes on forever) - clearly there are different degrees of stability - how do these different degrees of stability relate to different infinities like infinite sets given by number groups
    - the chain of events mentioned here implies a stability in the energy preservation with each successive event
      - https://www.quantamagazine.org/what-goes-on-in-a-proton-quark-math-still-conflicts-with-experiments-20200506/


  - organize db by intent & features for quicker access - like if types are a common filter, organize a graph into type clusters, and store node id's to limit size of various different graphs to depict the same database, a subset of indexes represented per graph

  - stat problem: "Sunrise problem: What is the probability that the sun will rise tomorrow? Very different answers arise depending on the methods used and assumptions made"

    - interface analysis questions:

      - what are the shapes & patterns of errors in assumptions & selection/generation of methods? (what ratio of incorrect are people with each additional assumption, given the level of certainty per assumption & complexity of problem)
      - what are the consequences of not correcting those errors? (how wrong will the predictions be)
      - what are the shapes of cause in generating/selecting assumptions & methods
      - what is the usual correct assumption pattern once false assumptions are corrected, and whats the insight path to transform the incorrect to the correct version?
      - whats the rate of discovery of new sub-systems, objects, or variables in related systems like physics
      - whats the likelihood we created certainty out of what ratio of our assumptions (over-relying on assumptions to make them conditionally true)
      - whats the possible causative impact of measurements & application of science knowledge on other knowledge
      - whats the possibility that a subset/state of physics rules gathers in increasingly isolated space-times, but outside of it, the rules are more flexible
      - whats the possibility that every science rule we take as certain is a false similarity or other false object?

  - product platform:
    - filters: predicting filters that will be used the most (features that differentiate products and alternate purchases the most)
    - products: product query language ('product with feature x and without component y')
    - supplies:
      - adjacent supply cost estimation ('adjacent product built from these suppliers would cost x')
      - estimate future demand & estimate cost of production methods (how many times will you need it? if above x, then its more cost effective to build it yourself, buy from these suppliers with price-lowering trends, buy this robot to make it regularly, or build a robot to do it - plus the timed sequence of those purchases for most cost effectiveness)
    - code as solutions:
      - code search (code as product solutions, like code to print a product or code to predict a compound or adjust vitamin combination as needed)
      - feature-to-code translation ('need a product with existing feature x and add new feature y')

  - example of a phase shift about a vertex:

    - in the problem of a rock on a hill, how do you determine at which point it will start rolling down?

      - vertex: minimum side length to maintain position

      - at increasingly large incremental additions to the side length variable, there will be a phase shift at the vertex (minimum side length to maintain position), after which it will start moving
      
      - this is because either:

        - the attribute (change rate of side length facing ground) and the attribute (slope of the hill and emerging force) align (rate of side length decrease increases and slope increases) or intersect (change rate of side length matches slope of hill in a way that fulfills motion intent)
        
        - the 'side length facing the ground' and 'its adjacent side in the direction of downward motion' are similar enough to allow momentum to develop from repeated motion (if the adjacent next side is too different, the rock might stabilize again)
        
        - the shape of the side length facing the ground and the next n sides aligns with the shape of the ground 
          (a curve can align with more ground shapes, but a rock with different side set shapes aligns with a smaller number of ground shapes)


  - most variables can be converted into one that has a normal distribution by injecting a symmetry or adding interactions from different enough sources that it generates randomness

  - why are randomness & symmetry often found together, like with the central limit theorem or circles? 

    - because symmetries are a default object that develop in complex systems on top of efficiencies, and growth on those symmetries is random (undirected) because symmetries dont typically come with default filters/limits directing the output

    - the normal distribution would likely be generated by a set of subsets where each subset has most of its values in the 1 standard deviation range and a few outliers in either or both directions

      - theres some variation in the subsets, which could include other patterns in small quantities that dont influence the average

      - why would 'most values in the 1 standard deviation range + a few outliers' be the standard or most common pattern in random independent variables?

      - this involves random independent variables like dice 1 outcome & dice 2 outcome - meaning the common pattern takes the form of most values in the 3 - 4 range, and a few values in the 1-2 and 5-6 range

      - given the commonness of that pattern, it can be interpreted as a default state of the dice probability distribution, where other patterns are distortions of that default

      - that common pattern has a hard limit placed on the central values 3 - 4, and the set of values that are outliers are less limited (can be 1 - 2 or 5 - 6 or a combination) - it requires some central values, but the outlier values are more flexible

      - the alternate version of that common pattern is that 'the proportion of 5/6 values relative to the proportion of 1/2 values will be equal (the sides of the curve are similar in shape), and the proportion of 3/4 values will be greater than those outlier values'

      - randomness means 'lack of influence or direction' in this case, which translates to 'lack of default similarity in adjacent outcomes', removing the normal semantic value of position

      - this means in a large sample with random selection, that a sequence of different values is likelier than a sequence of the same value

      - the 3/4 set is different from the 1/2 set and the 5/6 set

      - this means the 3/4 set is likelier to appear with the 1/2 set and the 5/6 set (same relationship applied to the interim sets, 2/5, in relation to the extreme outlier sets 1/6)

      - given the ratio of the outliers in the set of 1 - 6 (4/6), the likelihood that the average set (3 & 4) will provide the most difference from the previous value in set type is higher than the likelihood that another outlier will provide the most difference from the previous value in the set type metric (average, interim, extreme set types)

      - this implies the dynamic is more of a circle shape than a spectrum, where the pairs of adjacent values in a random sequence occupy points in a circle and the central tendency holds (pairs are likelier to include a 3/4 than other values, pairs are less likely to include two consecutive extreme values like 1/6)

      - analyzing adjacent pairs as the important objects is more useful than analyzing individual outcomes in isolation

      - random can also mean 'difference is default' - difference in this case represents the efficiency, and the distortions from that efficiency represent the outlier values

      - so in random functions, given the definition of random, there are biases rewarding:
        - difference between adjacent values (only 1/6 probability of the next value being the same value, and 2/6 (1 and 6) values have only a 1/6 probability of the next value being one unit away) if the number of possibilities is greater than 2
        - similarity to default/efficient value
        - difference in set type (average vs. extreme)
        - compliance with common patterns (like 'many symmetry origin values (like an average) plus few extreme/different values')

        which favors difference (given the concepts of limits which have limited directions of difference, averages which have more limited difference degrees, difference types (extreme vs. average), difference in probability of various adjacent pairs, & the number of options), even though the difference is not enforced (theres no rule guaranteeing the next value will be different or a specific degree of difference)

      - violation of randomness about a symmetry:

        - a random variable will change if another symmetry is adjacent enough to exert gravity on the variable, at which point the random variable will conver to & be distorted around the new symmetry

          - if a fly gravitates around one light but in their random motion, they encounter the edge of the light's reach, and another light is more nearby at that limit point than the original light, they may gravitate toward the new light - same with symmetries that are sufficiently adjacent as to be nearer to another symmetry's limit than the original symmetry's radius 

            - factors include if lights are likely to be evenly distributed, if they have different radii, if they have different types of light, etc

          - the corresponding example with the dice includes factors like:
            - people who use dice often happen to buy magnets (or magnetic material is often used in building/furniture construction)
            - the dice contain metal that responds to that charge 
            - the charge is strong enough to exert a force if the dice are thrown near enough to those objects (standing near a wall or sitting on furniture)
            - people arent careful to remove magnetic material from the experiment location

          - this is an object called an 'efficiency/symmetry overlap', where radii (or the equivalent semantic object) provide an intersection between symmetries
          
          - this either must not happen much in nature, or the overlap isnt usually enough to cause more than a few outliers, or these trends arent often described

      - how would you generate an independent variable? 

        - for example, how would you generate a high degree of randomness in the compounds that your bio system encounters?

        - youd make sure to interact with many different objects (like plants, locations, and experiments combining other objects) 

        - youd seek out differences, and try to eliminate certainties & their patterns

        - the way you would seek out differences & eliminate certainties would probably not be random - it would be normally distributed - youd try one interaction for a while, then move on to another interaction within a similar range of difference, and the final output would gravitate toward an average, since different compounds are usually different to test out variable combinations or because of local/conditional optimality, rather than because theyre known to be absolutely optimal by nature, and the average is often the most stable & therefore the most efficient state, given that very different values are unlikely to coordinate with all other system objects as well as the average does given its commonness, which implies that functions to handle it already exist

        - so stability often develops from aggregating many sources of difference, around the sets that offer the most difference in set type (default/efficient/common/average vs. extreme/conditional type) and the least difference in value (1/3 and 4/6 rather than 1/6 and 2/5) because of efficiency/commonness/stability, which may as well be proxy variables for each other in this context

      - a ratio of randomness is allowed in nature because the rules limiting interactions are finite

    - why do symmetries evolve in complex systems? so that differences can develop/stack within the symmetry range, leading to more differences when those differences interact with differences from other symmetries

  - explore how to map position to variable structures like networks/loops/trees (like how rank assigns standardized relative position to values - how would you assign a position to nodes in a network in a similarly standardized way - an attribute like connection count or node type, or a trajectory position, or another method)

    - how do rankings map to ratios, and what errors would result from direct mappings of various initial data types?

    - is there a standard set of structures like networks that should be applied to a sequence to get its probable prediction function the fastest (framing numbers as 1, a map from number type to node types, 2, a node's connection count, & 3, distance between nodes, in order to map the sequence in the most robust way)

  - how do you infer the existence of objects we cant measure:

    - the same way you infer that a whole object is complete, without being able to measure an object in its entirety simultaneously (being able to see it from every angle and on every scale at once)

    - without being able to see an object, you still use intersections of its vertices (one of its determining behaviors & one of its identifying attribute values) to rule out or otherwise limit the possible solution set of possible identities & degrees of completeness, between the object and adjacent possible objects, where each vertex intersection you check is between the object and the adjacent possible objects, even though you cant see the whole object in every possible way to measure it

    - example: without being able to see a ball in its entirety, you can check:

        - vertex intersection of its shape:
          - vertex 1: that it casts a sphere's shadow
          - vertex 2: that its boundary is circular

        - vertex intersection of its motion:
          - vertex 1: that it bounces instead of falling once
          - vertex 2: that it rolls on a hill in a vacuum of other forces (responds to gravity)

        - vertex intersection of its symmetries:
          - vertex 1: doesnt dissolve in substance that dissolves non-plastics
          - vertex 2: doesnt rest in a square or triangle shape even when compressed to those shapes

      - and you dont have to check other vertices for most probable relevant intents (dodging ball, throwing ball) bc the intersections of these vertices rule out other adjacent classes of objects:
        
        optimization vertexes:
        - 'not made to maximize difficulty on some metric'
        - 'not made to maximize ability to catch'

        symmetry vertexes:
        - 'not aligning weight & size attribute values with hands & strength attribute values' 

        structural vertexes:
        - 'not a block'
        - 'not a disc'

        intent vertexes:
        - 'not a hat'
        - 'functional for intents that are not entertainment intent'
        - 'not used for physical games'
        - 'not manmade' (also a control vertex)

      - you can see that subsets of the vertices and their intersections (like two sides of a triangle forming a point) are sufficient to identify the object for a subset of intents relevant to the object

    - so we can infer whole objects and object identities without documenting their every particle from every angle

    - similarly we can infer other objects we cant fully measure (symmetries, origin, paths) using various vertexes and their intersections:

      - infer symmetries with vertex intersections:
        - radius & origin
        - adjacent symmetries & limits

      - infer origins with vertex intersections:
        - current potential energy and distance from possible origin points
        - current potential energy and patterns of movement

      - infer paths with vertex intersections:
        - destination and alternative selection metrics
        - efficiency or resource conservation priorities

  - financial risk products

    - routes between nodes with different information perspectives can optimize for load-balancing of the payout liability, or maximizing asset flow/trades/distribution, or connecting outcomes (creating a circuit rather than a one-directional rule or decision fork)

      - allocating better decision information, models & tools to those with fewer assets or whose bets dont pay off can offset initial resource imbalances, offering better risk assessments & trades to those who need them

      - distributing info & tools (like probability/causal analysis, fallacy/incentive/insider trading identification) is one way to reduce risk across agents

      - exchange of funds should be tied to value creation by default, unless the funds are invested in useful high-value research, which has inherent necessary valid risk (as opposed to risk of buying a faulty product that cant be converted to something useful in an adjacent way, which happens frequently and is from bad design/implementation rather than necessity because of fast research)

        - value can be created by valuable attributes like convenience, speed, efficiency, minimized cost, quality, relevance, fairness
        - this value can be to agents in the transaction or to society
        - value creation isnt guaranteed, like when you buy property that breaks & is unfixable, but each transaction should have a fallback mechanism to create value, rather than relying on insurance
        - the fallback can be decision analysis tools, tax deductions, or insurance payouts, but it can also be built-in to the transaction contract
          - if your house is burned down, you can have a fallback default transaction designed to sell the data about what caused the fire, sell the property to someone who doesnt want to use any damaged infrastructure (pipelines, electricity) or to researchers who need fire byproducts to test with or a small business thats been trying to move in to that neighborhood but wants to do so at lower cost
          - this transaction would be designed by default with cost profiles for each option (cost/benefit of the transaction) based on specific demand for attributes created by the event - matching the negative event with the highest adjacent profit opportunities in the market by algorithm to minimize their losses
      
    - trading predictions for value & risk isnt always the trade being made

      - sometimes its trading predicting that the price will go up (buyer) for information that the price will go down (insider trading short seller)
      - or trading a prediction of price increase for market demand risk + disaster risk
      - the set of these trades can be optimized to make all agents better off most of the time
      - how do you arrange trades so the insurance company, supply company, homeowner, and future property owners are ok when a house burns down?
        - the attributes created by the fire include (depending on reason for the fire & damage caused):
          - adjacent houses are cheaper
          - land of house is cheaper
          - infrastructure repair costs may occur (power lines, pipelines)
          - byproducts are generated (heat, carbon)
        - there are agents that can benefit from most attributes created by the event
          - if you can find a buyer interested in moving in at lower cost or someone interested in the neighborhood (real estate developer), the fire might not produce a major loss for the homeowner, even without insurance
          - if repair costs are high, repair companies can purchase information of where new opportunities are available & what their possible profit is
          - if the house is near enough to a water source, it can be turned into a well or other natural resource supply, so the neighborhood could buy the property directly or with taxes
          - you can use the fire as a temporary energy source for the neighborhood which can be distributed with markets/taxes (whether they opt-in to buy the equipment to do that)
        - you can sell the risk of the house burning down beforehand, so that if the risk of it happening is 20%, you sell the risk of it not happening to people who need/earn money more (or people with existing incentives against the house burning down if its a higher cost than benefit across all agents, so no one has an incentive for arson) & sell the risk of it happening to people who dont need/earn money (or people with existing incentives against the house burning down)

      - similarly, other sources of systemic risks like natural disasters can be used for creating value 

        - if a flood happens, suppliers & researchers can pivot to using damaged wood as their building material (researchers can experiment on converting it to a useful resource & then its used as a default supply source, or researchers are assigned to convert dead wood into plants more efficiently)

        - product invalidation with tech produces obsolete tech, which can be allocated to neighborhood recycling centers rather than taken away so it can be used as a community supply for engineering & research

        - a pandemic produces either:
          - high demand for protective supplies (so people can still go out), medical service suppliers, cleaning services, delivery services
          - high demand for tech to invalidate protective supplies (buy computers/internet/robots/drones so people dont have to go out, and minimize quarantine time with medical solution tech investments)
          
        - the change in demand isnt likely to be accompanied by climate consciousness (people would rather buy gloves right away than wait for environmentally friendly gloves), so theres some increase in systemic risk bc of the requirements for fast solutions - companies with climate-friendly solutions can benefit if they can scale supply quicker, which will reduce risk for everyone
        - companies that adapt quickly, companies already in manufacturing business, companies building tech, and companies that host/enable trades will already be in position to benefit
        - companies building testing tools are another possible beneficiary, if people still need to interact (if you dont buy computers/internet/robots and medical solutions)

    - there can be a cap on financial instruments with better price/relative value assessments, so that someone cant buy something at a higher price out of irrational fear (someone whos afraid of fire is likelier to pay more for insurance, even if the risk doesnt match their price point given their irrational fear) or as a premium (charged more bc they earn more)

    - you can also calculate the fewest bets that are necessary (invest in a small business like an experiment automator printer, invest in a research project like determining cause of a systemic imbalance, invest in a particular solution like a farming technology) that will trigger the phase shifts necessary to produce an outcome (protect the environment from risk spirals or cascades or toxic causal shapes like tangled networks or one-directional trees)

    - taking the net outcome of all known bets (info/pattern/probability bets, industry bets, product bets, price bets, demand bets, tech bets, uncertainty bets) & bet patterns (diversify risk levels & timing in portfolio) can probably predict future risks (new incentives/priorities, new products/tech, new attribute sets, new alliances, new paradoxes (conflicting products becoming more popular), new trade loops, new financial instruments) - which can be used to adjust recommended price of a bet once the relative value is known, if that risk outcome is good across agents

      - example: 'if 50 people dont take this bet, another alternative tech will probably develop at lower cost' or 'if 100 people invest at this price, this product will enable investment in these fields given these company contracts'

    - load-balancing can also be automated between funds, so one fund isnt overloaded with liabilities - rather than one fund that made a bad bet bearing the cost, they get updates to their decision models (explaining why a price was false or why a natural event occurred), a small percentage is distributed to funds that arent as socially conscious (dont align their incentives with socially conscious intents, or create trades that prioritize value creation, or organize their trades to build loops so incentives are connected across agents)


  - automatic aggregated information formatting queries as an alternative to unstructured/keyword searches pointing to isolated content in manually entered formats like:

    - 'show me stock/financial instrument/cryptocurrency popularity data in graph format' and the output would be a graph of relative usage statistics available, with suggested content links to definitions of the financial instruments since that's a related intent to looking up their popularity, which implies an intent to invest/profit
    - 'show me product search data according to demos in a table with sorts' and the output would be a table with product search data by age group, economic group, in a table format, with sorts to sort each column
    - 'show me insights from language tutorials' would return a list of insights about learning a language, which is a primary implied intent of that search, with suggested content links to music in that language which is one way to learn a language

    - this would be done by:
      - using previous queries & feedback on search results
      - auto-formatting
      - aggregated data from existing content
      - pulling definitions of keywords like 'demos' to determine what supported keyword they mean, or create a new term out of core functions (groups separated by attribute & attribute value)

  - graph search (with queries like 'show me relationship between time and gdp' or show me relationship between using lysol and cancer')

    - could scan studies related to graph for logical fallacies and adjust graph accordingly, then present a composite graph of data found

    - data from searches & product purchases can be integrated into graph (buying lysol followed by searches for cancer symptoms)

    - 'deploy an AI model to do tasks: find/predict relationship, categorize, or rank' option can be included to train on public data based on plain language queries like the above

  - search data + verified purchases can be used to assess the value of a particular product solution for a problem (like a supplement to treat a health condition), to offset fake reviews or faulty recommendation/removal algorithm or account for product fixes over time, as well as customize it to the user (avoid this product if you have condition x, this product has correlation with onset of condition y

    - customization can also be done for user groups like intelligence - so people likelier to believe a story without checking it like anti-vaxx stories can be shown true stories with more repetition

    - example of a system object being useful for customatization (a false categorization):
      - busy can look like stupidity under certain circumstances - what are those circumstances and when are they most important to avoid 
        (if someone's too busy to check a news site, send them a notification about a pandemic so theyre likelier to see it)
 
  - detecting objects of uncertainty

    - for genuinely invisible sub-systems, we may only be able to find related objects (the boundaries containing them, the filters allowing them to develop) rather than their trajectories on the shape/other attribute interfaces

    - we may also be able to predict a finite set that they may be contained in (given the full set of combinations, what is a probability distribution not found in any natural process but still possible, that could describe uncertainty object behavior that we cant measure)

    - we may also be able to derive accurate opposite insights (given the existence of an object, what is impossible to describe, limit, define, etc)

  - minimum information:

    - whats the most efficient way to depict a physical object - as a network graph of:
      - splits & projections
      - splits & limits
      - gaps & limits
      - corners & angles
      - shapes & positions
      - intersections
      https://en.wikipedia.org/wiki/Orthographic_projection#/media/File:Graphical_projection_comparison.png

      - limits may seem like the best object but youd have to list limits of every side or side type
      - positions & shapes may seem ideal too but then youd have to store shape information
      - intersections may seem useful (intersections of functions like planes & lines at certain points) but theyre similar to number of sides in count

  - type of chart: a map of the trajectory between low-to-high dimensional representations of a function

  - false correlation example:

    - false correlation between two functions:
      - parabola representing position from origin around a circle
      - linear function with positive slope representing change rate of motion around circle
    - the first half would seem correlated, the second half would not - it would seem like a false correlation
    - the two variables are related because of the shape they are describing motion around, but not causative of each other unless there are other factors involved like a compounding force/momentum
    - but if you just looked at the first half of the functions, it would look like a similarity

    - how to generate the list of change types to check for when looking for minimum information like pivot points or vertices that could contradict the apparent correlation:
      - find attributes of the change type (inflection point or change in direction at the top of the parabola is a significant factor)
      - find standardized format (compare change rate of the parabola to the linear function which already represents a change rate - which would identify a slowing of the change rate in the parabola that indicates a limit or inflection point)
      - apply change patterns (a curve like the first half of a parabola doesnt normally just drop back to a position of zero after its change rate slows down)

  - what attributes determine symmetries so you could differentiate between symmetries (distortion functions, origin)

  - code should only be used to when there's an unsolved problem in a domain that doesnt respond to algorithmically determined solutions (when optimization of implementation is uncertain), otherwise algorithms should be selecting code

  - cause is determined by:
    - uniqueness of influence (structures that evolve even in very different boundary shapes arent likely to be caused by the boundary shape)
    - difference from randomness
    - difference between actual/possible functions (if an agent doesnt solve a problem, but they could have efficiently solved it, is the problem caused by them or its origin)
    - degree of clarity (is it certain or ambiguous cause)
    - adjacence (is it directly/near to dependence or indirectly/near to independence)

    - these forms manifest as the corresponding assumptions of independent random variables that are emergent outputs (having no agency to interfere with cause) and are resolvable into orthogonal dimensions and reason to believe theyre causative in the pattern (direction of influence established, direct causation, similar object interaction layer, lower-layer symmetry established like DNA being established as a cause of species variation)

    - info objects like games/trade-offs/forced decisions/equivalent alternates can be integrated with algorithms portraying the set of possible info objects and allowing traversal, to identify causal objects

      - once you identify causal info objects of independent variables of a data set, that can be used to select an algorithm or abstract the prediction function
        example: 'tradeoff between efficiency & accuracy creates types with vertices x, y, z which match algorithm or prediction function a'

    - how often is cause determinable given the attribute sets necessary to determine it? 

      - how often is a variable set determinable as uniquely causing a relationship, definitely different from random interactions, adjacent in causal distance, having no agency, and clear? 

      - which systems/vertices generate determinable cause (difference ratios, change rates, alignments, interaction layers, pivot points, causal structures, problem types, trade-offs, symmetries)?

  - vertices: variables where once theyre assigned a value, the rest of the uncertainties are resolved or resolvable


  - data structures:

    - what kind of data structure would look like the original sequence from one angle, but look like its metadata (like the ordered sequence, or average value statistics) from another angle?
      - is the extra storage of a tree, network, or other structure with more than one dimension worth the computation gains
    - is the best storage format of a list where position would be checked later in code a map retaining order, with keys as ordered values & values as positions in original sequence (in case original position is significant and youre not just trying to find if the value is in the sequence)


  - question-answering algorithm

    - example: 'what is the definition of this word', 'what is the best route to destination', 'why does evolution occur', 'how to implement an intent with this tool'

      - question is the set of nodes that should be connected

      - answer is the path between nodes
        - set of steps to take
        - set of possible alternate equivalent routes
        - network query if other networks are needed
        - ordered combination of words & sub-definitions to form a definition

        - the answer can have various forms:
          - moving in the question intent direction 
          - approaching an interim answer to the final destination node or answering a sub-question
          - answering on another node layer (cause) or answering the reason for the question
          - arriving at a non-answer (there is no right answer) or a conclusion of ambiguity or immeasurability
          - taking a sub-optimal route to the final destination node for one metric (accuracy) to fulfill another metric (understandability)

  - whats the best base object for building a prediction function:
    - constant/tangent subset functions (like a tangent at a maximum of length 1)
    - averages/differences
    - adjacent functions
    - probabilities/patterns
    - filters

    - adjacent/tangent subset functions, where deactivated nodes function as:
      - the subset functions that couldnt be transformed to components of the actual prediction function without a forced value intervention
      - transformation functions (or their parameters/values like direction) that didnt convert adjacent subset functions to components of the prediction function

    - filter functions, where:
      - the likeliest limits narrowing down a solution space into a function are successively selected & applied in subsets
      - likeliest vertices determining/generating a function are applied in subsets 
        - test that change is slowing which means approaching a limit or an inflection point, and check for enough vertex points past the possible pivot point to confirm which one
      - the likeliest changes/types are applied in subsets
        - slow change is likelier to follow slow change, direction change likelier to follow stabilization of curve


  - for nn:

    - preventing consecutive extreme examples could prevent the need for some corrections during training

    - how to identify false correlations (irrelevant features, similar features with no direct connection, interactive features like external agents)

      - example: dogs wearing a collar more than cats, so collar is identified as a feature predicting species, but other data sets have more cats with collars

      - output/emergent qualities of dogs are:
        - domestication
        - lack of aggression/docility (personality can be identified bc they smile more & bare their teeth less) 

        which could position the collar feature as an output of:
          - being a dog (its easier to get collars on them)
          - external interactions/agents (like culture, education, & DNA are sources/expressors of agency that can appear to cause similar output or chosen features)

        and therefore not causative of the dog type (instead associated with the domesticated animal type)    

      - identify that any domestic animal exhibiting any aggressiveness at all would also have a collar, so its not a type indicator 
        (concept trajectory: animal.domestic + animal.aggressive = animal.has['collar'])

        - identify the concept of the controlling species in photo background, or infer their existence by the fact that the collar couldnt be made by the dog given its biological limitations

      - organizing features by intent (collar is to control/track identity, and external agents are likely to control other animal species in a similar way)

    - reducing noise (both in data so less training is required, & weight path patterns, so fewer variants need to be checked & variants outside of probable ranges can be eliminated early)

      - reducing noise or identifying cause with decision objects 

        - identify decisions interfering with or causing data (decision chains & other shapes, including variants like incentivized decisions, efficient decisions, etc)

          - examples:

            - decision to interact with an object (causes future options or limitations)
            - decision of short-term thinking/efficiency-prioritizing species to pollute environment (causes dangerous chemicals in water)

          - rather than trying to infer the cause of the polluting chemical from physics/bio rules, you can infer starting from a decision chain given agent rules, because not every data pattern will be an accident, and agents interfering with/causing data can produce noise or legitimate data variation

            - you can start by looking for signs of agency, in objects like 'required technology to build something' (fingers necessary to build collar) or 'extreme variation not found in nature' (hats)

      - similar to reducing noise from the concept of agency, noise from other irrelevant interactions can be reduced by identifying:

        - adjacent objects that have interaction potential (weather system is likely to interact and can change how an animal looks)

        - objects that are likely to interact, either by:

          - incentives (anyone who takes their toy is going to be interacted with)
          - probability (anyone walking around the neighborhood could be interacted with)
          - enforced rules (the mailman is required to interact with unless window/sound are blocked)

      - probable ranges & vertices of a prediction function can be estimated better once you know the reason the function emerged, which is a key function metadata attribute linking the causes & the output intents/priorities

        - reasons on interfaces such as 'clustering rules' or 'boundary rules' or 'interaction rules'

        - a function's different emergence reasons can be used as a way to generate probable vertex/range sets, given a particular reason

          - example: is a function shaped like a random cloud bc several clusters are overlapping/colliding/converging, or bc theres noise in the data, or bc the type clusters just hit a variance injection point at similar times

        - the 'emergence reason' concept is related to cause but is more specifically definable as 'a rule interface directly preceding the function, generating a trajectory'

          - 'because they hit a point' is a reason for the function's current state, and it also describes the trajectory of the function's future behavior, so it can be called a priority/intent as well as a cause, since it relates the origin & the target of the function

    - objects in the ml system need to make sense together, not just being valid or functional according to their definitions, but changing in ways that benefit system intents under change/interaction conditions (contribute feature contribution information to prediction function) and dont contradict each other under certain cases

      - for example:

        - with two related system parameters having different bases:

          - learning rate (improving with respect to time, with value based on change ratio)
          - activation function (based on threshold)

        - should the activation function change across the concept of time as it occurs in the system (meaning 'a training cycle', 'a traversal of network weight paths'), given that activation and learning are related (should they use the same base and change according to the same base)?

            - should it be different if its the first or last cycle in training, or the first or last layer of a type, given that adjacence to the final category output makes the final decisions more important, just like highly differentiating weight combinations are important?
            - should weights be adjusted according to this difference in importance

        - should system limits like learning rate & activation threshold have attribute values in common, or do they occupy system positions where similarity isnt productive for system intents

        - should possible weight paths have gaps in between them which are not reachable with learning?

        - when estimating corrective measures to adjust the learning rate, high-cost error types should be identified & prioritized:

          - one error type scenario is that another global minimum exists thats severely different from identified minimum
          - another error type scenario is that the global minima steadily decrease or decrease in an unpredictable way bc the function has many peaks, so theorizing the existence of minima will be high-cost without semantic information

        - when an answer is guaranteed/determinable in a weight path is when it should be checked, which may be earlier than the final layer, so calls to nodes on the same layer may be justified

        - some alignments in the system shouldnt occur (like function aligning perfectly with data) bc the intents dont match (function intent 'predict other data' and data intent 'to represent a sample')

      - error cascades should be evaluated for system design

        - if learning rate is above a range of correctness, or weights/bias are slightly too high & the activation threshold is just below what it would take to deactivate that node
          - the error ranges that can happen from parameters should be looked for in differences between category examples that are the most similar (or likely to be similar given system analysis of problem space like 'how species typically differ')

        - features that are too similar for the system parameters to catch can be accounted for 
          - by magnifying the differences before training time, or accounting for error types at training time, when weight paths are found to have a gap overlooking a weight path that wont catch a particular similarity

      - high cost errors can be evaluated for alternate outcomes with distortions (a system parameter determining the range of distortions that can be used to correct outcomes, that is fewer for example than the distortions that are typically required to turn one category into another)


    - add to explanation: 

      - fully connected network passes weighted versions of different weight sets from all previous inputs 

        - 3rd layer will have weighted sum of weighted sums of input features, so each weighted sum in layer 2 is contributing to a weighted sum with the other layer 2 nodes
        - the fully connected network is like a reverse decision tree, where decisions represent sums rather than path selections & all lower inputs are passed up to each node above it
        - if A, B, C, and D are the second layer nodes representing a particular weight set of the original input features, the third layer nodes is composed of weighted sets of A, B, C, & D, and so on

        - another way to visualize a particular node path is as the successive progression of that path toward a central origin (the target label) where each layer is the weighted summation of all nodes on that circular layer (to do: make a diagram)

    - whats the value of identifying weight path patterns that can generate a clear answer vs. the patterns that can generate a clear 'not' answer or a lack of clarity between alternatives? should you train to identify an alternate type that can be used to determine the other, given that they are alternates in a set?

    - similarities & other system filter objects between weight paths can help predict errors in pooling layers later in the network

    - what happens when one variable is at a different causal level than the others? 

      - example: 

        - proxy variable: the tail of a cat is more constant than that of a dog, so it can be used as a proxy for a categorization function of multiple variables

        - higher cause variable: DNA vertices are a higher-cause variable than any particular feature set, because they generate identification features

          - how do you keep your data at the same level of cause, so DNA data isnt mixed with image data for categorization? 
          - physical conditions can also be higher cause and could interact with image data by default if the condition is visible 
          - a physical condition could override genetic influences to determine differentiating features (if the dog was in fights, that will override their DNA to determine their feature shapes)
          - its especially important to keep variables at similar causal levels because algorithms will aggregate them, and if you can skip analyzing a causal variable set and instead analyze the causal output variable of that variable set, that will reduce training time
          - in this case there should be alternate paths in the network to handle edge cases, or you could restore the overridden data to its original beforehand (restore a torn ear to its original shape)
          - this means different weight path patterns for false similarities, false differences (a torn ear isnt really a species-specific feature, unless cats are in far fewer fights)
          - you can generate default weight paths to handle different cases like that by:
            - ignoring features that are corrupted (distorted) or that override the real causal variables
            - building in paths that lead to deactivated nodes or zero-weighted features in the case of multiple distortions applied to a feature that dont appear to be from the real causal variable level (output distortions like tearing, which arent from the DNA causal layer)
            - allocating these weight paths that found distortions or different causal level variables to other causal structures
              - looking for DNA causal variables in the ear distortion would mean looking for causal concepts like aggression & then looking for signs of that concept in the data, for features that would cause or indicate aggression, like bigger size or sharper teeth - then the output of this causal search, which is the on the target DNA causal level, could be integrated with the original data set & training could continue
              - or it could be determined to be an output variable rather than a causal variable, and restored to its original value & integrated with the original data set to continue training (and all nodes from that point on could apply a weight transform for that feature if the distortion level is found in other weight paths - installing a memory of weight transforms)


    - error types:

      - example: 'training an ai model by remote controlling a drone to pick coconuts' will have some errors with outputs like 'changing direction to account for wind blowing target in other direction', when the inputs were a monkey pushing the leaf to the side which made the drone's model think that a gust of wind moved it and it should turn to maintain its trajectory

      - how would you design an ai algorithm to:

        - account for these error types in algorithm design
          - identify error types in data like 'assign weight to leaf motion to create a direction change' and convert them to the right combination of factors like 'check for other sources of motion first'
          - apply activation functions to node sets to identify patterns that are error types which shouldnt be passed on as valuable information to determine when to change direction
          - look for similarities (mapping to attribute alignments) in weight path patterns and other system filter objects that could be error types
          - error type deactivation in a hybrid network of networks mapped to sub-problem type (solving a sub-conflict between similar alternatives with a standard categorization network), where combinations of objects like an info asymmetry + assumption dependence lead to error types, so activations can be applied to limit the passage of data after these combinations in the hybrid network

        - predict these error types
          - look for attribute alignments (direction changes produced by both monkey pushing leaf & wind but appearing to be similar enough to be difficult to differentiate given that the monkey may not be visible) and other system filters

    - what would the value be of keeping some parameters randomized, some constant, some locally determined or ambiguous until training/run time (parameter or weight superposition)?

      - determining threshold values & aggregation/grouping methods when particular value sets or weight paths are determined to be causative or require disambiguation:

        - to provide a clear distinction in a categorization or feature selection problem

        - to adjust for errors in the algorithm-problem type match or the algorithm-complexity match if they become significant, so changing data is handled better

          - to find the optimal position of a particular function in the network by training
          - to allow for node clusters/paths to solve sub-problems
          - to allow for iteration of a node & other causal shapes to be applied locally 

    - when a decision is increasingly clearly ambiguous/indeterminable during training, what is the sequence of strategies to follow before returning an 'unknown' prediction
      - navigate to previous nodes when decision wasnt clearly ambiguous & distort data to check for adjacent alternate versions that would be clearly differentiable
      - check for randomness (found with corrupted data, false similarities, and other system objects)
      - check for different causal route to features 
        (species with similar features will have different routes to those features, and the route would leave traces, if not in that data point, then in others, so integrate data set statistics)
      
    - certainty networks vs. isolated predictions

      - a network of predictions with certainty rates would be more flexible as an output than a clear answer, where the network could be applied to protect against unforeseen cascading data changes
        - if an edge case value is in a certainty network, the prediction functions will be able to handle cases where edge cases cascade better than storing isolated predictions

        - this would handle a range of 'potential cause' rather than just 'known cause', for variables that are likely to be causative in high degrees (in conditions like if the system is similar to known systems or has a randomness injection point or other system features)

    - when you organize a network so that each node is only doing one task (executing a task function without any other analysis, like a dev creating a service rather than first or regularly analyzing if the service will actually help the business), its less flexible than a network of nodes that can do multiple tasks

      - a node that can answer 'yes/no' should be a node that can answer 'yes/no if' (given a threshold condition) and 'aggregated yes/no if' (asking adjacent nodes a question before deciding) and 'averaged aggregated yes/no if' and so on, to mine differences between weight paths for insights


    - identify vertices such as cases where individual nodes or subsets can totally change the outcome of the training or produce phase shifts or other important system objects and make decisions about thresholds for those cases before training (what do you do when adding a node adds error 60% of the time and more accuracy 10% of the time and neutral impact the rest, given the data (delegate to different network architecture, gather data, use system objects/patterns to make predictions in those cases)

    - how do you check for optimal combinations, causation, feature sets, and system objects reached before the end of training at a particular node, or a feature/weight set that matches the correct prediction function bc its a high-impact feature/weight combination (weight x applied to weight path 1, weight y applied to weight path 2) that happens to identify an important system object (efficiency, interaction, variance gap, causal structure) - you can include a test of original data in each node for predictive potential once you determine the level of complexity needed to create a prediction function (number of terms or level of variation, which can be used as a filter before running tests for system objects or correct prediction function similarity at each node)

    - how do you identify different types of relevance & nodes that supply them for an intent like categorization/prediction?

      - different types of relevance like direct causation (is causative), imminent causation (will be causative, or may be if imminent conditions occur), alternate causation (can be causative)

        - for categorization, direct causation may be identifying/differentiating features - imminent causation may be converging or adjacent features

        - if all a node does is answer yes/no for a particular question, that will be sufficient for that question but not other important questions that could be answerable with the data

        - how to tell which questions are answerable given that one question is answerable

          - meaning you can reach answer node B given a starting point of attribute values, a function set, and a question intent direction identifying an information gap in attribute values, where the answer node may not fully answer the question but it will move toward the question intent direction

            - for example, asking 'is ingredient A toxic' can be answered with filters (probability filter such as 'not likely in moderation') or mapping a trajectory between nodes agent starting position & agent death

            - so the question ('is substance A toxic') can be formmatted like: 
              - a path between nodes agent position and agent death
              - a movement in the direction of node clusters with the high-toxicity attribute (implying toxic intent of the substance)
              
              where the question is the information gap: 
                - will event 'ingest substance A' move in direction of high-toxicity clusters, an adjacent intent direction or state, or toward agent death

            - so other answerable questions would involve a subset of that information (a subset of the path of the answer) or other related object (alternate, opposing, etc)

          - involve a subset or same set of attributes
          - involve opposing/complementary attributes
          - involve direct causative or caused attributes
          - involve abstract attributes

      - node sets/weight paths that arent important for direct causation may be important for imminent causation

  - given that certain algorithms can only add so much certainty for a particular problem type, that should be integrated into output (trust shouldnt be default handler for algorithm output)

  - behavior data, search data, purchase data can be used to link health conditions and train AI to predict conditions from a symptom search

  - statistical tests & hypotheses should be standardized for false objects (false similarities), errors in assumptions (non-normal distribution) or data collection/measurement, change types (about to become another distribution/in the process of being converted)
    
    - 'a hypothesis assuming random independent variables is more likely to include m/n non-random variables or x causal shapes'

    - these insights can be used to adjust test critical values in the absence of data on correctness to use instead:
      - 'a z-value of 1 for a data set like this is likely to be accurate 5% of the time and is likelier to be 1.2 most of the time'

  - structural mismatches of solutions & problems

    1. algorithm structures: theres an inherent structural mismatch between some algorithms (decision tree, neural net) and some problem types (prediction) given the intent & abstraction layer 

      - example: 

        - some algorithms are too specific or have a structural mismatch (decision tree has structural split, specify & direction intents) with the problem type (predicting a set of variables that is about to change)

        - the decision tree occupies a very low-level, neutral, granular, abstract/structural position - that means it can be used for many intents 

        - the decision tree can be hacked with various edge case, phase shift & boundary manipulations

          - if two variables on different layers are about to converge, they should have been on the same layer or in the opposite causal direction or treated as one variable - the model will be increasingly wrong until it's updated or until these ambiguities and likely relationships are accounted for in the design

          - the reason algorithms can be hacked is bc of the structural mismatch between the algorithm and the problem type

        - the intents resulting from the decision tree's layers, direction, and thresholds can only contain so much variation in the data 

        - generated varied data with permutations of data objects (loops, sets, alternates) can be used to make the tree more robust to adjacent/likely changes

    2. test structures: a similar problem is when the test/method will identify false similarities or other objects & return the false version, limiting the potential for the correct version to be identified

      - the structure of the test can be a barrier to the truth, if its over simplified or excludes too much information

      - a test that's done iteratively on new data without changing the function according to new information would fail if the data changes more than the function can handle

    3. brain analysis structures: structural biases in human brains prevent us from seeing the truth - we're biased toward objects we understand or which are simple to derive

    - algorithms should identify mismatches (in complexity, variance, completeness & other metrics) & other problem types between the data/algorithm/problem type

    4. prediction method structures: 

      - bayesian probability has the incorrect structure for solving a problem of predicting the dependent variable if its not actually the dependent variable

      - meaning if the input is actually an output, like a southern dialect is an output of location, so predicting criminal activity for a location based on whether they have a southern dialect may correlate with some location-crime data bc of hot weather increasing emotions, but southerners can move while retaining their manner of speech & can change their emotion-based behavior from the new cold weather

      - probability of (criminal activity | dialect) should be probability of (criminal activity | weather), since weather is the more causative reason why location is a significant factor in criminal activity


  - algorithms should produce a set of solutions (an obvious/simple answer, a pattern-compliant answer, a common answer, a robust answer)

    - example of why youd want an 'obvious solution' tag on the output:

      - how could you build an algorithm that wouldnt overidentify a race attribute as being a potential criminal indicator?

      - query maps & causal shapes:
        - query for data on related word sentiment (if theres a negative association with a word related to that race)
        - query for data on intent ('identifying potential criminals' is the intent of the training process, which needs to be an input to the algorithm)
          - why would identifying an individual by a related attribute to a negative related term be useful for the 'identify potential criminal' intent?
          - if the intent is to 'identify potential criminals', which is a high-stakes intent
          - the algorithm should identify that a causal loop would be dangerous to treat as an input, given that a causal loop (like mistreatment or persecution of minorities leading to poverty which leads to crime) can hide original inputs (root cause being mistreatment)

      - alternatively, it could use interface analysis:

        - use inference & intent (to predict) to arrive at insights like:

          - 'if the answer is obvious, it must not be true bc otherwise I wouldnt have been asked to predict it, unless this is a test situation'
          - 'if the attribute value is common across the population, it must not be a predictor bc there are many people with that attribute value who are not in this data set bc they didnt commit a crime'

        - hard-code insights like above to be consulted if a variable is mistakenly identified as significant

        - treating one involuntary variable as high-impact belies the complexity of social games, which involve learning competitions (who has the best manners, whos the smartest) that would mean there is high potential for economic status variation within nodes having that attribute

        - given the changeability of that variable (easily changed with structural tools & also frequently changed in gene pool), it should not be treated as a predictor

        - it should identify location & economic status as an indicator of criminal activity given the health/drug addictions symptoms of the people whose data is used for training

        - it should also identify social games that are used for criminal activity & skill at those games (making intimidating or emotionless facial expressions)

        - it should identify culture as a key factor in variation in criminal activity, which can be specific to an attribute but has high variation within that attribute (producing gang violence, govt corruption, or a culture of karma/street justice) with low variation in outcomes (kill or be killed) - and identify that if it doesnt have cultural information, it cant make predictions

      - most algorithms dont have the complexity to identify complex sub-systems or related systems like social information games, communities, economies, or cultures

      - a really smart algorithm would immediately identify a few insights like:

        - example of deriving a social game like bullying:
          - with a priority of 'avoiding criticism'
            - most nodes would deserve criticism, because avoiding it is easy, especially at scale
            - this incentivizes criticizing nodes who dont deserve criticism

        - 'there are different reasons people do crimes'
        - 'some reasons they do crimes include need (resource acquisition, asset/reputation protection), goal attainment (enable a career), culture (avoiding crimes isnt a priority), social games (dares, threats, bullying, corruption, group dynamics), enjoyment (test if anyones paying attention, test how fragile the system is, rebel against authority)'
        - 'randomness is the biggest distributor of those reasons' (luck)
        - 'randomness can lead to lack of justice or other types of meaning'
        - 'people who have lack of justice are likelier to do crimes'
        - 'which people dont have justice - unfairly persecuted people'
        - 'which people are unfairly persecuted - different people, excellent people'
        - 'some privileged people with justice/meaning do crimes anyway just for fun to see how much they can get away with' (counterpoint - other reasons to do crimes than need, culture, social games)
        - 'which facial expressions are associated with the criminal reasons we're trying to prevent'
        - 'which factors do we need to complete the prediction function'
        - 'which expressions are often false signals despite being good predictors in subsets'

        - how would you build an algorithm to identify those insights just from maps & from mugshot data? 

          - the point is invalid once you have those insights, bc the answer is clearly not predicting who will do crimes but figuring out:
            - a system of social rules to prevent those situations from happening in society
            - at what point people start trying to have fun rather than contribute
            - deriving intent (either mechanically or with a prediction tool)

          - the algorithm would identify a high-variance data set and derive that its not the obvious visible attributes that will be predictors (except a few like visible signs of crack addiction, which will only predict the ratio that are crack addicts) but the subtle visible attributes, which will necessarily be incomplete without data about each person's specific traversal of the maps

          - the algorithm would identify that certain facial expressions are associated with criminal activity, such as:
            - dead eyed hopeless expression (lost hope of good treatment)
            - crazy expression (went on a rant after putting up with some stressor)
            - shocked expression (cant believe this happened to them)

          - most of them would also have signs of stress destructuring their faces, except resilient criminals, bosses, or some first-time offenders

          - the different routes to each expression should show up in traversals of social game, community, & culture maps as their status & decisions change, and cross concepts like randomness (in which ideas people encounter, which skills they learn, which social games they play), equivalence (in resource distribution), justice (as a counter object to randomness), & meaning (group belonging, goal attainment, success)

          - the output would be a set of paths that leave traces, some of which would show up in mugshots, some of which would require questions

        - identifying a concept (like 'reason to do crime') can be as trivial as exploring combinations of core functions

          - once the algorithm identifies the concept of language (from input data or insight maps) as 'information trades formatted as paths', it should be able to identify the concept of lies, as it will know that 'information can be inaccurately described' from its own errors

          - once it identifies lies, it can identify the concept of social games (reward from coordinating a lie with another node, reward from a successful lie, reward from adding a distant transform to another node's information)

          - once it identifies social games, it can derive their intent (the point of social games is to control other people or to get resources like enjoyment or funds)

          - then it can identify specific social games as anti-societal behavior that hurts the group

          - the counter-object is pro-societal rule compliance that helps the group

          - then it can move on to identifying specific social games (initial crimes, bullying) that remove inputs of pro-societal decisions (complying with rules), creating a 'reason to do crime'

          - then it can look for outputs of those games (stress, difference) and make an attempt to translate that to the data features (facial expressions, signs of addiction, signs of aging)

        - this requires that society-wide data needs to be integrated into the algorithms (like genetic variance) as well as data on criminal activity


  - rather than using data & the training process as an indicator of consensus, they can use patterns (patterns of data, patterns of change, patterns of variables) as an indicator of consensus

  - algorithm based on problematic adaptive systems like cancer bc theyre learning faster than the host system

  - if something can generate a change predictably/consistently, it's a change supply - otherwise it's a change request, so output as well as causal position relative to the output is important when determining category
  
    - time may be a variance gap (a space where change is possible) to resolve a question/problem set - so not resolving it can preserve time, unless resolving it will allow for more potential or moving on to other variance gaps

  - vitamin 3-d printer to print vitamins so that you can design your own multivitamin that:
    - fits your bio conditions & requirements
    - is released in the right order & timing
    - excludes interactions that are contradictory (antimicrobials & probiotics)

  - multiple servers/processors in one computer with one-way data transfers, so one server can be for local communication, one can be for offline work, one can be for browsing internet, and local/offline can communicate to internet-browing processor but not the other way around

  - rules-to-code translation tool - translating domain-specific plain language rules to robot code can be short-term useful for automation of service industry tasks like:
    - converting recipes/flavor-mixing strategies to cooking robot code (chefs can use a tool like this to make money short-term or sell their rules, if they have unique strategies)
    - converting new plant designs to genome editing code
    - converting local social insights to global code (avoid personalities like this, use these tactics to persuade, make this argument to get them to an insight position, etc)
    - converting adaptation insights to change-attracting system adaptation code
    - converting routing mechanisms/optimizations to drone code (short-term human insights like 'avoiding a particular street bc of construction' that data isnt adequate for)
    - the general task of converting rule sets (systems) or human-made visuals (graphs, blueprints) to code

    - machine learning can be used for initial conversion, then tweaked with coded filters like priorities, logic, organization, output
    - system analysis can be used to optimize beyond those standard filters
    - this needs to identify existing rules (or specific versions of abstract rules, distorted versions of standard rules) & filter them out
    - this is an alternative & and an interim step to raw code-generation given a set of intents

  - data viz can be automated using:
    - lie core function layer graph or individual lie type graphs, with an output intent layer (hide information, layer information, minimize information, obfuscate information)
    - intent-structure maps (this graph structure serves this intent stack, just like a function serves an intent stack)

  - each superposition contains components representing different possible filters for the physical laws they create at scale
    - some superpositions collapse into a particular attribute set
    - superpositions with different configurations may represent other interface queries or structures
    - knowing the internal structure of a superposition would mean we get to choose which queries come to life & become real
    - the design implies we shouldnt get to choose - but external forces (or unmeasurable/uncomputable forces inside the universe) should get to determine which configurations collapse & which differences are allowed
    - information has a lifecycle - its likelier to become more true the more its observed, up to a maximum - then it's likelier to erode as its depended on
    - observing a state (to produce the information of the observation) may initialize the static nature of that information, so other observers see either static information or lack of it depending on their perspective, as information becomes truer the more its observed, and they may focus on the lack of information or a different perspective than the initial state of the information

  - make AI models/graph databases of theoretical physicists opinions so I can suggest these concepts to them

  - search ideas:
      - inferring useful search filters based on customer usage history & intent
        - linked searches/user data with type/intent identification - if they are in a location with a certain pathogen and they search for cleaners, theyre probably trying to clean that pathogen so cleaners should be specific or at least an optional search results set should be linked to
      - automated attribute extraction/addition to search as a filter
      - search results as graphs: variables entered in search to display relationships found in data or graph images or graphetized articles
      - processed (aggregated) results - find the average/combined or plain language definition when searching for a definition
      - predicting what questions theyll ask next and adding those search results (or a summary) on the side
      - intent-based search guidance:
        - usually people who search for an answer are studying for a test, so additional widgets like suggested content could include snapshots of/links to: 'study guides', 'summaries', 'tutorials'
        - people searching for recipes are hosting a party & cooking other things, so suggested content could include snapshots of/links to: 'flavor graphs'
        - people searching for symptoms are trying to diagnose themselves or someone else, so suggested content could include links to diagnostic tools or graphs of symptom set frequency for conditions

  - shared custom meaning/dictionary maps so communication can be queries on their shared custom dictionary map - or a common map where queries specify pattern & sub-set to apply pattern to, and sub-sets are rotated