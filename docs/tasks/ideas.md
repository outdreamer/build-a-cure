to do: organize into docs

  - analyzing just by change rate makes it less likely to spot other patterns like overlap/intersection of patterns

  - difference develops where there's potential for new interactions to develop (so a steady or increasing rate of change) & intent (like a possible gain from the difference)

  - the object model may not be the right default to start from in most situations - there arent many whole objects in existence if there are any
    - even particles have sub-particles, and the extent of that chain isnt known, and may have a causal relationship where the smallest particles act as inputs or injection points
    - should ratios/bases or sets be used instead ('a set of particles' rather than a 'plant' as a standard unit)
    - when selecting a default, you should be checking for attribute matches (does a whole object make sense to describe a set)
    - the idea of a whole number may describe something that doesnt exist in 3-d physical reality - does that mean its a concept that will never occupy a form, or is it a goal physics will move towards, or it causally independent from other systems or interfaces that are known, or it evolves as brains can measure information
    - math doesnt just describe concepts like 'whole number', it also describes basic interactions like pairs, sets, combinations, which appear in most systems 
      - are there objects in systems that are real but not describable with our measurement/description tools

    - this system of math describes certain change types, values, & relationships 
      - so something that doesnt have a structure, or value, or cant be changed with any operation, and doesnt relate to anything might not be describable with math
      - for example a space-time with one object, which has no other objects to be used to compare to it, and the space-time is in constant flux so position, structure, & change cant be measured, just estimated
      - math breaks down with some interactions of change types that create ambiguities so information cant be measured

      - if you try to create another description system, you end up changing definitions, variables, bases, or methods but not the system itself
        - in order for the system to exist, some objects have to be defined (a base concept crystallized into a structural object)
        - what description system do you get if you dont use units/value as a base (ranges, definitions, attribute sets, interfaces as a base)
        - are there other possible description systems than these
      
    - are there materials/processes that erase information or which can vacillate between discrete/continuous, skipping information & leaving gaps that can break the chain of information (breaking time reversal symmetry)

  - mask design can be optimized as a cover with one output flap like an esophagus preventing input on one nostril so the other can be used exclusively as an input with a filter 
    (better to sanitize at point-of-usage in environments with many unpredictable interactions like wind direction and interpersonal contact, which can get around most masks)

  - causal shapes integrated with networks (patterns of aggregation matching causal shapes like trees & circuits)

    - integrating system analysis with networks
      https://twitter.com/remixerator/status/1150578597339340805
      https://twitter.com/remixerator/status/1205724743741014018

    - system of causal types (integrated with type path example as a version of weight paths)
      https://twitter.com/remixerator/status/1156860484294852609

    - causal types
      twitter.com/remixerator/status/1126040476023279616

    - applying causal shapes to a network
      https://twitter.com/remixerator/status/1004579263507566592

    - position on causal type network
      https://twitter.com/remixerator/status/1018540899859607552


  - types can be represented as directions (going farther from origin goes further up type stack, where similar types are adjacent)
  - change phases for causal analysis (interim, changing, diverging, standard, efficient state, constant, interacting, converging, on the verge of obsolescence, outlier, etc)
    - superficial cause, alternate cause in the case of a function, addressing input/output causes
  - framing on interfaces, decomposing causation, then identifying parameters of problem on layer & matching solution
  - independence (closed trade loops) as time storage
  - vertex as a pivot point for an interface

  - add diagram for question derivation for service list to automate chat bot entries

    - deriving the questions customers will ask for a set of services

      - which processes are complicated or not optimized (need to be in person for certain transactions that people would rather do online)
      - which processes involve changing information (account balance, transaction approval)
      - which processes are likely to have errors (auth)
      - which processes people will likely be interested in using the most

    - what percent of changes are just from finding efficiencies, using those as a foundation for common distortion types (random change, directed change, connecting change, etc)

    - what system of core objects/functions/attributes generate a space where:
      - circles & squares are fundamental or standard objects
      - there is a continuous spectrum of values (real numbers) around which alternate number types rotate (complex numbers, etc)
      - comparing change generated by two variables (one independent value function determining the dependent value) has patterns of measurement potential
      - isolating by attributes (like isolating direction & scale to transform to a vector space) or framing information in different structures (sets, matrixes, sequences) allows patterns that are calculatable (implying the framing filter is determining, so matrix attributes can be determined by its definition)

    - what objects describe lack of information like ambiguities or lost information, other than randomness (difficult to identify randomness), variance (lack of patterns), and infinities (lack of information being difficulty of computing the sequence except in terms of other infinities if it doesnt converge, or lack of guarantees that the sequence can be maintained/stabilized to continue)

  - when physics rules stabilize, they attract & generate information, which gathers into measurable numbers
  - if the point of the universe is not to find the initial filters but to prevent that information from being discovered, that could keep open options for other change sources

  - do repulsion principles mimic or have causal relationship to symmetries around boundaries (like whole numbers represent a container that is full) 
    - or do they indicate a lack of symmetries in that space (attractive forces act on the irrational & fractions rather than integers)

  - as change is injected into the number of sides (moving towards non-linearity) of a closed regular shape (polygon), change may leak outward from the center to form a polynomially described wave/spiral or other shape described by the polynomial's exponential motion
    - in what space would the number of sides or non-linearity of a closed regular shape leak into another direction, indicating the wave shape
    - how do you map the polynomial shape to the change generated by adding (or subtracting) a side

  - polynomials are like transforms around a base which forms the constant for the symmetry to develop at (adding or subtracting versions of the base, as the transforms)

  - cyclotomic polynomial roots may act like a wave function around the x-axis

  - imaginary numbers are like roots with a value attribute that compresses to a position attribute (square root of -1)

  - the roots (intersections at zero) are the different types of transform (addition & subtraction of scaled base versions) positioned as equivalents

    - the implied question is:

      - "how do you transform a set of addition transforms of this scaled base version into a set of subtraction transforms, given the constant term" (using the root values as the base)
      - or "how do you equate these two vectors (addition/subtraction set constants) with one number, in the case of a one-variable polynomial"
      - or "how do you combine the concept of area related to points, lines, squares, cubes, etc to generate these vector sets"

      - system analysis of solution: https://www.quantamagazine.org/new-math-measures-the-repulsive-force-within-polynomials-20200514/

        insight: "polynomials and power series can confirm attributes of the other"

        convert problem type:
          "question about the size of roots of polynomials"
          "question about the size of values associated to a power series"

        requirement: coefficients needed to be positive or negative whole numbers

        insight path:
        "
        a non-cyclotomic polynomial
        found its roots
        raised those roots to different powers
        multiplied them together
        took the square root of that product
        based on that square root, he could construct a power series with the essential property (coefficients are whole numbers)
        "

        intent:
        "
        a non-cyclotomic polynomial             (roots not on unit circle/roots following repulsion pattern)          get relevant object (non-cyclotomic polynomial)
        found its roots                         (found numbers that equate its transform vectors)                     find the symmetry of sets generated by its transform types
        raised those roots to different powers  (permute polynomial exponents and apply to roots rather than base)    create polynomial with roots as bases (or even orthogonal change distortions of roots as bases)
        multiplied them together                (apply change from roots orthogonally)                                multiply terms of multivariate polynomial (multiply orthogonal change distortions of roots)
        took the square root of that product    (find number to produce product with one multiplication (squared))    find alternate standard generative factor (specifically square root) of the product of that multiplication
        "

      - insights (mostly based on similarities):

        - after finding roots, youre doing a similar operation twice:
          A. first create alternate output (root to a power) of factors (roots)
          - then (after multiplying the alternate output)
          B. find alternate factors (roots vs. the final square root) of output (product of distorted roots)

        - another key point is the default relationship between roots (multiplied by each other) and the interim operation (multiply roots taken to powers) between symmetric operations A & B 
        - another point is the transformation of a root into a polynomial term, given that its being multiplied by itself
        - then these polynomial terms are being used to create another polynomial term (x^2), whose output x is relevant to a related power series of the original polynomial, with integer coefficients
        
      - generate insight chain (of specific insights):

        - factors (x - a) are important objects in polynomials and act as power series factors (x - c), so given that theyre an important object, we can select the 'factor' operation as something to try in the insight path first
        
        - given that factors differ between the two function types, we should look for alternate factors as one of the insight path steps

        - since we're transforming something, symmetries should be a key object we use first (like embedded or containing symmetries, such as the symmetry between the two similar operations)

      - questions

        - what is the relationship between polynomials with duplicate roots & distribution of roots around the unit circle
        - why would this function/insight/intent path produce a power series with integer coefficients (scalars) applied to some center c?

  - an infinite series implies a stabilized symmetry (a platform for change that goes on forever) - clearly there are different degrees of stability - how do these different degrees of stability relate to different infinities like infinite sets given by number groups
    - the chain of events mentioned here implies a stability in the energy preservation with each successive event
      - https://www.quantamagazine.org/what-goes-on-in-a-proton-quark-math-still-conflicts-with-experiments-20200506/


  - organize db by intent & features for quicker access - like if types are a common filter, organize a graph into type clusters, and store node id's to limit size of various different graphs to depict the same database, a subset of indexes represented per graph

  - stat problem: "Sunrise problem: What is the probability that the sun will rise tomorrow? Very different answers arise depending on the methods used and assumptions made"

    - interface analysis questions:

      - what are the shapes & patterns of errors in assumptions & selection/generation of methods? (what ratio of incorrect are people with each additional assumption, given the level of certainty per assumption & complexity of problem)
      - what are the consequences of not correcting those errors? (how wrong will the predictions be)
      - what are the shapes of cause in generating/selecting assumptions & methods
      - what is the usual correct assumption pattern once false assumptions are corrected, and whats the insight path to transform the incorrect to the correct version?
      - whats the rate of discovery of new sub-systems, objects, or variables in related systems like physics
      - whats the likelihood we created certainty out of what ratio of our assumptions (over-relying on assumptions to make them conditionally true)
      - whats the possible causative impact of measurements & application of science knowledge on other knowledge
      - whats the possibility that a subset/state of physics rules gathers in increasingly isolated space-times, but outside of it, the rules are more flexible
      - whats the possibility that every science rule we take as certain is a false similarity or other false object?

  - product platform:
    - filters: predicting filters that will be used the most (features that differentiate products and alternate purchases the most)
    - products: product query language ('product with feature x and without component y')
    - supplies:
      - adjacent supply cost estimation ('adjacent product built from these suppliers would cost x')
      - estimate future demand & estimate cost of production methods (how many times will you need it? if above x, then its more cost effective to build it yourself, buy from these suppliers with price-lowering trends, buy this robot to make it regularly, or build a robot to do it - plus the timed sequence of those purchases for most cost effectiveness)
    - code as solutions:
      - code search (code as product solutions, like code to print a product or code to predict a compound or adjust vitamin combination as needed)
      - feature-to-code translation ('need a product with existing feature x and add new feature y')

  - example of a phase shift about a vertex:

    - in the problem of a rock on a hill, how do you determine at which point it will start rolling down?

      - vertex: minimum side length to maintain position

      - at increasingly large incremental additions to the side length variable, there will be a phase shift at the vertex (minimum side length to maintain position), after which it will start moving
      
      - this is because either:

        - the attribute (change rate of side length facing ground) and the attribute (slope of the hill and emerging force) align (rate of side length decrease increases and slope increases) or intersect (change rate of side length matches slope of hill in a way that fulfills motion intent)
        
        - the 'side length facing the ground' and 'its adjacent side in the direction of downward motion' are similar enough to allow momentum to develop from repeated motion (if the adjacent next side is too different, the rock might stabilize again)
        
        - the shape of the side length facing the ground and the next n sides aligns with the shape of the ground 
          (a curve can align with more ground shapes, but a rock with different side set shapes aligns with a smaller number of ground shapes)


  - most variables can be converted into one that has a normal distribution by injecting a symmetry or adding interactions from different enough sources that it generates randomness

  - why are randomness & symmetry often found together, like with the central limit theorem or circles? 

    - because symmetries are a default object that develop in complex systems on top of efficiencies, and growth on those symmetries is random (undirected) because symmetries dont typically come with default filters/limits directing the output

    - the normal distribution would likely be generated by a set of subsets where each subset has most of its values in the 1 standard deviation range and a few outliers in either or both directions

      - theres some variation in the subsets, which could include other patterns in small quantities that dont influence the average

      - why would 'most values in the 1 standard deviation range + a few outliers' be the standard or most common pattern in random independent variables?

      - this involves random independent variables like dice 1 outcome & dice 2 outcome - meaning the common pattern takes the form of most values in the 3 - 4 range, and a few values in the 1-2 and 5-6 range

      - given the commonness of that pattern, it can be interpreted as a default state of the dice probability distribution, where other patterns are distortions of that default

      - that common pattern has a hard limit placed on the central values 3 - 4, and the set of values that are outliers are less limited (can be 1 - 2 or 5 - 6 or a combination) - it requires some central values, but the outlier values are more flexible

      - the alternate version of that common pattern is that 'the proportion of 5/6 values relative to the proportion of 1/2 values will be equal (the sides of the curve are similar in shape), and the proportion of 3/4 values will be greater than those outlier values'

      - randomness means 'lack of influence or direction' in this case, which translates to 'lack of default similarity in adjacent outcomes', removing the normal semantic value of position

      - this means in a large sample with random selection, that a sequence of different values is likelier than a sequence of the same value

      - the 3/4 set is different from the 1/2 set and the 5/6 set

      - this means the 3/4 set is likelier to appear with the 1/2 set and the 5/6 set (same relationship applied to the interim sets, 2/5, in relation to the extreme outlier sets 1/6)

      - given the ratio of the outliers in the set of 1 - 6 (4/6), the likelihood that the average set (3 & 4) will provide the most difference from the previous value in set type is higher than the likelihood that another outlier will provide the most difference from the previous value in the set type metric (average, interim, extreme set types)

      - this implies the dynamic is more of a circle shape than a spectrum, where the pairs of adjacent values in a random sequence occupy points in a circle and the central tendency holds (pairs are likelier to include a 3/4 than other values, pairs are less likely to include two consecutive extreme values like 1/6)

      - analyzing adjacent pairs as the important objects is more useful than analyzing individual outcomes in isolation

      - random can also mean 'difference is default' - difference in this case represents the efficiency, and the distortions from that efficiency represent the outlier values

      - so in random functions, given the definition of random, there are biases rewarding:
        - difference between adjacent values (only 1/6 probability of the next value being the same value, and 2/6 (1 and 6) values have only a 1/6 probability of the next value being one unit away) if the number of possibilities is greater than 2
        - similarity to default/efficient value
        - difference in set type (average vs. extreme)
        - compliance with common patterns (like 'many symmetry origin values (like an average) plus few extreme/different values')

        which favors difference (given the concepts of limits which have limited directions of difference, averages which have more limited difference degrees, difference types (extreme vs. average), difference in probability of various adjacent pairs, & the number of options), even though the difference is not enforced (theres no rule guaranteeing the next value will be different or a specific degree of difference)

      - violation of randomness about a symmetry:

        - a random variable will change if another symmetry is adjacent enough to exert gravity on the variable, at which point the random variable will conver to & be distorted around the new symmetry

          - if a fly gravitates around one light but in their random motion, they encounter the edge of the light's reach, and another light is more nearby at that limit point than the original light, they may gravitate toward the new light - same with symmetries that are sufficiently adjacent as to be nearer to another symmetry's limit than the original symmetry's radius 

            - factors include if lights are likely to be evenly distributed, if they have different radii, if they have different types of light, etc

          - the corresponding example with the dice includes factors like:
            - people who use dice often happen to buy magnets (or magnetic material is often used in building/furniture construction)
            - the dice contain metal that responds to that charge 
            - the charge is strong enough to exert a force if the dice are thrown near enough to those objects (standing near a wall or sitting on furniture)
            - people arent careful to remove magnetic material from the experiment location

          - this is an object called an 'efficiency/symmetry overlap', where radii (or the equivalent semantic object) provide an intersection between symmetries
          
          - this either must not happen much in nature, or the overlap isnt usually enough to cause more than a few outliers, or these trends arent often described

      - how would you generate an independent variable? 

        - for example, how would you generate a high degree of randomness in the compounds that your bio system encounters?

        - youd make sure to interact with many different objects (like plants, locations, and experiments combining other objects) 

        - youd seek out differences, and try to eliminate certainties & their patterns

        - the way you would seek out differences & eliminate certainties would probably not be random - it would be normally distributed - youd try one interaction for a while, then move on to another interaction within a similar range of difference, and the final output would gravitate toward an average, since different compounds are usually different to test out variable combinations or because of local/conditional optimality, rather than because theyre known to be absolutely optimal by nature, and the average is often the most stable & therefore the most efficient state, given that very different values are unlikely to coordinate with all other system objects as well as the average does given its commonness, which implies that functions to handle it already exist

        - so stability often develops from aggregating many sources of difference, around the sets that offer the most difference in set type (default/efficient/common/average vs. extreme/conditional type) and the least difference in value (1/3 and 4/6 rather than 1/6 and 2/5) because of efficiency/commonness/stability, which may as well be proxy variables for each other in this context

      - a ratio of randomness is allowed in nature because the rules limiting interactions are finite

    - why do symmetries evolve in complex systems? so that differences can develop/stack within the symmetry range, leading to more differences when those differences interact with differences from other symmetries

  - explore how to map position to variable structures like networks/loops/trees (like how rank assigns standardized relative position to values - how would you assign a position to nodes in a network in a similarly standardized way - an attribute like connection count or node type, or a trajectory position, or another method)

    - how do rankings map to ratios, and what errors would result from direct mappings of various initial data types?

    - is there a standard set of structures like networks that should be applied to a sequence to get its probable prediction function the fastest (framing numbers as 1, a map from number type to node types, 2, a node's connection count, & 3, distance between nodes, in order to map the sequence in the most robust way)

  - how do you infer the existence of objects we cant measure:

    - the same way you infer that a whole object is complete, without being able to measure an object in its entirety simultaneously (being able to see it from every angle and on every scale at once)

    - without being able to see an object, you still use intersections of its vertices (one of its determining behaviors & one of its identifying attribute values) to rule out or otherwise limit the possible solution set of possible identities & degrees of completeness, between the object and adjacent possible objects, where each vertex intersection you check is between the object and the adjacent possible objects, even though you cant see the whole object in every possible way to measure it

    - example: without being able to see a ball in its entirety, you can check:

        - vertex intersection of its shape:
          - vertex 1: that it casts a sphere's shadow
          - vertex 2: that its boundary is circular

        - vertex intersection of its motion:
          - vertex 1: that it bounces instead of falling once
          - vertex 2: that it rolls on a hill in a vacuum of other forces (responds to gravity)

        - vertex intersection of its symmetries:
          - vertex 1: doesnt dissolve in substance that dissolves non-plastics
          - vertex 2: doesnt rest in a square or triangle shape even when compressed to those shapes

      - and you dont have to check other vertices for most probable relevant intents (dodging ball, throwing ball) bc the intersections of these vertices rule out other adjacent classes of objects:
        
        optimization vertexes:
        - 'not made to maximize difficulty on some metric'
        - 'not made to maximize ability to catch'

        symmetry vertexes:
        - 'not aligning weight & size attribute values with hands & strength attribute values' 

        structural vertexes:
        - 'not a block'
        - 'not a disc'

        intent vertexes:
        - 'not a hat'
        - 'functional for intents that are not entertainment intent'
        - 'not used for physical games'
        - 'not manmade' (also a control vertex)

      - you can see that subsets of the vertices and their intersections (like two sides of a triangle forming a point) are sufficient to identify the object for a subset of intents relevant to the object

    - so we can infer whole objects and object identities without documenting their every particle from every angle

    - similarly we can infer other objects we cant fully measure (symmetries, origin, paths) using various vertexes and their intersections:

      - infer symmetries with vertex intersections:
        - radius & origin
        - adjacent symmetries & limits

      - infer origins with vertex intersections:
        - current potential energy and distance from possible origin points
        - current potential energy and patterns of movement

      - infer paths with vertex intersections:
        - destination and alternative selection metrics
        - efficiency or resource conservation priorities

  - financial risk products

    - routes between nodes with different information perspectives can optimize for load-balancing of the payout liability, or maximizing asset flow/trades/distribution, or connecting outcomes (creating a circuit rather than a one-directional rule or decision fork)

      - allocating better decision information, models & tools to those with fewer assets or whose bets dont pay off can offset initial resource imbalances, offering better risk assessments & trades to those who need them

      - distributing info & tools (like probability/causal analysis, fallacy/incentive/insider trading identification) is one way to reduce risk across agents

      - exchange of funds should be tied to value creation by default, unless the funds are invested in useful high-value research, which has inherent necessary valid risk (as opposed to risk of buying a faulty product that cant be converted to something useful in an adjacent way, which happens frequently and is from bad design/implementation rather than necessity because of fast research)

        - value can be created by valuable attributes like convenience, speed, efficiency, minimized cost, quality, relevance, fairness
        - this value can be to agents in the transaction or to society
        - value creation isnt guaranteed, like when you buy property that breaks & is unfixable, but each transaction should have a fallback mechanism to create value, rather than relying on insurance
        - the fallback can be decision analysis tools, tax deductions, or insurance payouts, but it can also be built-in to the transaction contract
          - if your house is burned down, you can have a fallback default transaction designed to sell the data about what caused the fire, sell the property to someone who doesnt want to use any damaged infrastructure (pipelines, electricity) or to researchers who need fire byproducts to test with or a small business thats been trying to move in to that neighborhood but wants to do so at lower cost
          - this transaction would be designed by default with cost profiles for each option (cost/benefit of the transaction) based on specific demand for attributes created by the event - matching the negative event with the highest adjacent profit opportunities in the market by algorithm to minimize their losses
      
    - trading predictions for value & risk isnt always the trade being made

      - sometimes its trading predicting that the price will go up (buyer) for information that the price will go down (insider trading short seller)
      - or trading a prediction of price increase for market demand risk + disaster risk
      - the set of these trades can be optimized to make all agents better off most of the time
      - how do you arrange trades so the insurance company, supply company, homeowner, and future property owners are ok when a house burns down?
        - the attributes created by the fire include (depending on reason for the fire & damage caused):
          - adjacent houses are cheaper
          - land of house is cheaper
          - infrastructure repair costs may occur (power lines, pipelines)
          - byproducts are generated (heat, carbon)
        - there are agents that can benefit from most attributes created by the event
          - if you can find a buyer interested in moving in at lower cost or someone interested in the neighborhood (real estate developer), the fire might not produce a major loss for the homeowner, even without insurance
          - if repair costs are high, repair companies can purchase information of where new opportunities are available & what their possible profit is
          - if the house is near enough to a water source, it can be turned into a well or other natural resource supply, so the neighborhood could buy the property directly or with taxes
          - you can use the fire as a temporary energy source for the neighborhood which can be distributed with markets/taxes (whether they opt-in to buy the equipment to do that)
        - you can sell the risk of the house burning down beforehand, so that if the risk of it happening is 20%, you sell the risk of it not happening to people who need/earn money more (or people with existing incentives against the house burning down if its a higher cost than benefit across all agents, so no one has an incentive for arson) & sell the risk of it happening to people who dont need/earn money (or people with existing incentives against the house burning down)

      - similarly, other sources of systemic risks like natural disasters can be used for creating value 

        - if a flood happens, suppliers & researchers can pivot to using damaged wood as their building material (researchers can experiment on converting it to a useful resource & then its used as a default supply source, or researchers are assigned to convert dead wood into plants more efficiently)

        - product invalidation with tech produces obsolete tech, which can be allocated to neighborhood recycling centers rather than taken away so it can be used as a community supply for engineering & research

        - a pandemic produces either:
          - high demand for protective supplies (so people can still go out), medical service suppliers, cleaning services, delivery services
          - high demand for tech to invalidate protective supplies (buy computers/internet/robots/drones so people dont have to go out, and minimize quarantine time with medical solution tech investments)
          
        - the change in demand isnt likely to be accompanied by climate consciousness (people would rather buy gloves right away than wait for environmentally friendly gloves), so theres some increase in systemic risk bc of the requirements for fast solutions - companies with climate-friendly solutions can benefit if they can scale supply quicker, which will reduce risk for everyone
        - companies that adapt quickly, companies already in manufacturing business, companies building tech, and companies that host/enable trades will already be in position to benefit
        - companies building testing tools are another possible beneficiary, if people still need to interact (if you dont buy computers/internet/robots and medical solutions)

    - there can be a cap on financial instruments with better price/relative value assessments, so that someone cant buy something at a higher price out of irrational fear (someone whos afraid of fire is likelier to pay more for insurance, even if the risk doesnt match their price point given their irrational fear) or as a premium (charged more bc they earn more)

    - you can also calculate the fewest bets that are necessary (invest in a small business like an experiment automator printer, invest in a research project like determining cause of a systemic imbalance, invest in a particular solution like a farming technology) that will trigger the phase shifts necessary to produce an outcome (protect the environment from risk spirals or cascades or toxic causal shapes like tangled networks or one-directional trees)

    - taking the net outcome of all known bets (info/pattern/probability bets, industry bets, product bets, price bets, demand bets, tech bets, uncertainty bets) & bet patterns (diversify risk levels & timing in portfolio) can probably predict future risks (new incentives/priorities, new products/tech, new attribute sets, new alliances, new paradoxes (conflicting products becoming more popular), new trade loops, new financial instruments) - which can be used to adjust recommended price of a bet once the relative value is known, if that risk outcome is good across agents

      - example: 'if 50 people dont take this bet, another alternative tech will probably develop at lower cost' or 'if 100 people invest at this price, this product will enable investment in these fields given these company contracts'

    - load-balancing can also be automated between funds, so one fund isnt overloaded with liabilities - rather than one fund that made a bad bet bearing the cost, they get updates to their decision models (explaining why a price was false or why a natural event occurred), a small percentage is distributed to funds that arent as socially conscious (dont align their incentives with socially conscious intents, or create trades that prioritize value creation, or organize their trades to build loops so incentives are connected across agents)


  - automatic aggregated information formatting queries as an alternative to unstructured/keyword searches pointing to isolated content in manually entered formats like:

    - 'show me stock/financial instrument/cryptocurrency popularity data in graph format' and the output would be a graph of relative usage statistics available, with suggested content links to definitions of the financial instruments since that's a related intent to looking up their popularity, which implies an intent to invest/profit
    - 'show me product search data according to demos in a table with sorts' and the output would be a table with product search data by age group, economic group, in a table format, with sorts to sort each column
    - 'show me insights from language tutorials' would return a list of insights about learning a language, which is a primary implied intent of that search, with suggested content links to music in that language which is one way to learn a language

    - this would be done by:
      - using previous queries & feedback on search results
      - auto-formatting
      - aggregated data from existing content
      - pulling definitions of keywords like 'demos' to determine what supported keyword they mean, or create a new term out of core functions (groups separated by attribute & attribute value)

  - graph search (with queries like 'show me relationship between time and gdp' or show me relationship between using lysol and cancer')

    - could scan studies related to graph for logical fallacies and adjust graph accordingly, then present a composite graph of data found

    - data from searches & product purchases can be integrated into graph (buying lysol followed by searches for cancer symptoms)

    - 'deploy an AI model to do tasks: find/predict relationship, categorize, or rank' option can be included to train on public data based on plain language queries like the above

  - search data + verified purchases can be used to assess the value of a particular product solution for a problem (like a supplement to treat a health condition), to offset fake reviews or faulty recommendation/removal algorithm or account for product fixes over time, as well as customize it to the user (avoid this product if you have condition x, this product has correlation with onset of condition y

    - customization can also be done for user groups like intelligence - so people likelier to believe a story without checking it like anti-vaxx stories can be shown true stories with more repetition

    - example of a system object being useful for customatization (a false categorization):
      - busy can look like stupidity under certain circumstances - what are those circumstances and when are they most important to avoid 
        (if someone's too busy to check a news site, send them a notification about a pandemic so theyre likelier to see it)
 
  - detecting objects of uncertainty

    - for genuinely invisible sub-systems, we may only be able to find related objects (the boundaries containing them, the filters allowing them to develop) rather than their trajectories on the shape/other attribute interfaces

    - we may also be able to predict a finite set that they may be contained in (given the full set of combinations, what is a probability distribution not found in any natural process but still possible, that could describe uncertainty object behavior that we cant measure)

    - we may also be able to derive accurate opposite insights (given the existence of an object, what is impossible to describe, limit, define, etc)

  - minimum information:

    - whats the most efficient way to depict a physical object - as a network graph of:
      - splits & projections
      - splits & limits
      - gaps & limits
      - corners & angles
      - shapes & positions
      - intersections
      https://en.wikipedia.org/wiki/Orthographic_projection#/media/File:Graphical_projection_comparison.png

      - limits may seem like the best object but youd have to list limits of every side or side type
      - positions & shapes may seem ideal too but then youd have to store shape information
      - intersections may seem useful (intersections of functions like planes & lines at certain points) but theyre similar to number of sides in count

  - type of chart: a map of the trajectory between low-to-high dimensional representations of a function

  - false correlation example:

    - false correlation between two functions:
      - parabola representing position from origin around a circle
      - linear function with positive slope representing change rate of motion around circle
    - the first half would seem correlated, the second half would not - it would seem like a false correlation
    - the two variables are related because of the shape they are describing motion around, but not causative of each other unless there are other factors involved like a compounding force/momentum
    - but if you just looked at the first half of the functions, it would look like a similarity

    - how to generate the list of change types to check for when looking for minimum information like pivot points or vertices that could contradict the apparent correlation:
      - find attributes of the change type (inflection point or change in direction at the top of the parabola is a significant factor)
      - find standardized format (compare change rate of the parabola to the linear function which already represents a change rate - which would identify a slowing of the change rate in the parabola that indicates a limit or inflection point)
      - apply change patterns (a curve like the first half of a parabola doesnt normally just drop back to a position of zero after its change rate slows down)

  - what attributes determine symmetries so you could differentiate between symmetries (distortion functions, origin)

  - code should only be used to when there's an unsolved problem in a domain that doesnt respond to algorithmically determined solutions (when optimization of implementation is uncertain), otherwise algorithms should be selecting code

  - cause is determined by:
    - uniqueness of influence (structures that evolve even in very different boundary shapes arent likely to be caused by the boundary shape)
    - difference from randomness
    - difference between actual/possible functions (if an agent doesnt solve a problem, but they could have efficiently solved it, is the problem caused by them or its origin)
    - degree of clarity (is it certain or ambiguous cause)
    - adjacence (is it directly/near to dependence or indirectly/near to independence)

    - these forms manifest as the corresponding assumptions of independent random variables that are emergent outputs (having no agency to interfere with cause) and are resolvable into orthogonal dimensions and reason to believe theyre causative in the pattern (direction of influence established, direct causation, similar object interaction layer, lower-layer symmetry established like DNA being established as a cause of species variation)

    - info objects like games/trade-offs/forced decisions/equivalent alternates can be integrated with algorithms portraying the set of possible info objects and allowing traversal, to identify causal objects

      - once you identify causal info objects of independent variables of a data set, that can be used to select an algorithm or abstract the prediction function
        example: 'tradeoff between efficiency & accuracy creates types with vertices x, y, z which match algorithm or prediction function a'

    - how often is cause determinable given the attribute sets necessary to determine it? 

      - how often is a variable set determinable as uniquely causing a relationship, definitely different from random interactions, adjacent in causal distance, having no agency, and clear? 

      - which systems/vertices generate determinable cause (difference ratios, change rates, alignments, interaction layers, pivot points, causal structures, problem types, trade-offs, symmetries)?

  - vertices: variables where once theyre assigned a value, the rest of the uncertainties are resolved or resolvable


  - data structures:

    - what kind of data structure would look like the original sequence from one angle, but look like its metadata (like the ordered sequence, or average value statistics) from another angle?
      - is the extra storage of a tree, network, or other structure with more than one dimension worth the computation gains
    - is the best storage format of a list where position would be checked later in code a map retaining order, with keys as ordered values & values as positions in original sequence (in case original position is significant and youre not just trying to find if the value is in the sequence)


  - question-answering algorithm

    - example: 'what is the definition of this word', 'what is the best route to destination', 'why does evolution occur', 'how to implement an intent with this tool'

      - question is the set of nodes that should be connected

      - answer is the path between nodes
        - set of steps to take
        - set of possible alternate equivalent routes
        - network query if other networks are needed
        - ordered combination of words & sub-definitions to form a definition

        - the answer can have various forms:
          - moving in the question intent direction 
          - approaching an interim answer to the final destination node or answering a sub-question
          - answering on another node layer (cause) or answering the reason for the question
          - arriving at a non-answer (there is no right answer) or a conclusion of ambiguity or immeasurability
          - taking a sub-optimal route to the final destination node for one metric (accuracy) to fulfill another metric (understandability)


  - for nn:

    - add to explanation: 

      - fully connected network passes weighted versions of different weight sets from all previous inputs 

        - 3rd layer will have weighted sum of weighted sums of input features, so each weighted sum in layer 2 is contributing to a weighted sum with the other layer 2 nodes
        - the fully connected network is like a reverse decision tree, where decisions represent sums rather than path selections & all lower inputs are passed up to each node above it
        - if A, B, C, and D are the second layer nodes representing a particular weight set of the original input features, the third layer nodes is composed of weighted sets of A, B, C, & D, and so on

        - another way to visualize a particular node path is as the successive progression of that path toward a central origin (the target label) where each layer is the weighted summation of all nodes on that circular layer (to do: make a diagram)

    - whats the value of identifying weight path patterns that can generate a clear answer vs. the patterns that can generate a clear 'not' answer or a lack of clarity between alternatives? should you train to identify an alternate type that can be used to determine the other, given that they are alternates in a set?

    - similarities & other system filter objects between weight paths can help predict errors in pooling layers later in the network

    - what happens when one variable is at a different causal level than the others? 

      - example: 

        - proxy variable: the tail of a cat is more constant than that of a dog, so it can be used as a proxy for a categorization function of multiple variables

        - higher cause variable: DNA vertices are a higher-cause variable than any particular feature set, because they generate identification features

          - how do you keep your data at the same level of cause, so DNA data isnt mixed with image data for categorization? 
          - physical conditions can also be higher cause and could interact with image data by default if the condition is visible 
          - a physical condition could override genetic influences to determine differentiating features (if the dog was in fights, that will override their DNA to determine their feature shapes)
          - its especially important to keep variables at similar causal levels because algorithms will aggregate them, and if you can skip analyzing a causal variable set and instead analyze the causal output variable of that variable set, that will reduce training time
          - in this case there should be alternate paths in the network to handle edge cases, or you could restore the overridden data to its original beforehand (restore a torn ear to its original shape)
          - this means different weight path patterns for false similarities, false differences (a torn ear isnt really a species-specific feature, unless cats are in far fewer fights)
          - you can generate default weight paths to handle different cases like that by:
            - ignoring features that are corrupted (distorted) or that override the real causal variables
            - building in paths that lead to deactivated nodes or zero-weighted features in the case of multiple distortions applied to a feature that dont appear to be from the real causal variable level (output distortions like tearing, which arent from the DNA causal layer)
            - allocating these weight paths that found distortions or different causal level variables to other causal structures
              - looking for DNA causal variables in the ear distortion would mean looking for causal concepts like aggression & then looking for signs of that concept in the data, for features that would cause or indicate aggression, like bigger size or sharper teeth - then the output of this causal search, which is the on the target DNA causal level, could be integrated with the original data set & training could continue
              - or it could be determined to be an output variable rather than a causal variable, and restored to its original value & integrated with the original data set to continue training (and all nodes from that point on could apply a weight transform for that feature if the distortion level is found in other weight paths - installing a memory of weight transforms)


    - error types:

      - example: 'training an ai model by remote controlling a drone to pick coconuts' will have some errors with outputs like 'changing direction to account for wind blowing target in other direction', when the inputs were a monkey pushing the leaf to the side which made the drone's model think that a gust of wind moved it and it should turn to maintain its trajectory

      - how would you design an ai algorithm to:

        - account for these error types in algorithm design
          - identify error types in data like 'assign weight to leaf motion to create a direction change' and convert them to the right combination of factors like 'check for other sources of motion first'
          - apply activation functions to node sets to identify patterns that are error types which shouldnt be passed on as valuable information to determine when to change direction
          - look for similarities (mapping to attribute alignments) in weight path patterns and other system filter objects that could be error types
          - error type deactivation in a hybrid network of networks mapped to sub-problem type (solving a sub-conflict between similar alternatives with a standard categorization network), where combinations of objects like an info asymmetry + assumption dependence lead to error types, so activations can be applied to limit the passage of data after these combinations in the hybrid network

        - predict these error types
          - look for attribute alignments (direction changes produced by both monkey pushing leaf & wind but appearing to be similar enough to be difficult to differentiate given that the monkey may not be visible) and other system filters

    - what would the value be of keeping some parameters randomized, some constant, some locally determined or ambiguous until training/run time (parameter or weight superposition)?

      - determining threshold values & aggregation/grouping methods when particular value sets or weight paths are determined to be causative or require disambiguation:

        - to provide a clear distinction in a categorization or feature selection problem

        - to adjust for errors in the algorithm-problem type match or the algorithm-complexity match if they become significant, so changing data is handled better

          - to find the optimal position of a particular function in the network by training
          - to allow for node clusters/paths to solve sub-problems
          - to allow for iteration of a node & other causal shapes to be applied locally 

    - when a decision is increasingly clearly ambiguous/indeterminable during training, what is the sequence of strategies to follow before returning an 'unknown' prediction
      - navigate to previous nodes when decision wasnt clearly ambiguous & distort data to check for adjacent alternate versions that would be clearly differentiable
      - check for randomness (found with corrupted data, false similarities, and other system objects)
      - check for different causal route to features 
        (species with similar features will have different routes to those features, and the route would leave traces, if not in that data point, then in others, so integrate data set statistics)
      
    - certainty networks vs. isolated predictions

      - a network of predictions with certainty rates would be more flexible as an output than a clear answer, where the network could be applied to protect against unforeseen cascading data changes
        - if an edge case value is in a certainty network, the prediction functions will be able to handle cases where edge cases cascade better than storing isolated predictions

        - this would handle a range of 'potential cause' rather than just 'known cause', for variables that are likely to be causative in high degrees (in conditions like if the system is similar to known systems or has a randomness injection point or other system features)

    - when you organize a network so that each node is only doing one task (executing a task function without any other analysis, like a dev creating a service rather than first or regularly analyzing if the service will actually help the business), its less flexible than a network of nodes that can do multiple tasks

      - a node that can answer 'yes/no' should be a node that can answer 'yes/no if' (given a threshold condition) and 'aggregated yes/no if' (asking adjacent nodes a question before deciding) and 'averaged aggregated yes/no if' and so on, to mine differences between weight paths for insights


    - identify vertices such as cases where individual nodes or subsets can totally change the outcome of the training or produce phase shifts or other important system objects and make decisions about thresholds for those cases before training (what do you do when adding a node adds error 60% of the time and more accuracy 10% of the time and neutral impact the rest, given the data (delegate to different network architecture, gather data, use system objects/patterns to make predictions in those cases)

    - how do you check for optimal combinations, causation, feature sets, and system objects reached before the end of training at a particular node, or a feature/weight set that matches the correct prediction function bc its a high-impact feature/weight combination (weight x applied to weight path 1, weight y applied to weight path 2) that happens to identify an important system object (efficiency, interaction, variance gap, causal structure) - you can include a test of original data in each node for predictive potential once you determine the level of complexity needed to create a prediction function (number of terms or level of variation, which can be used as a filter before running tests for system objects or correct prediction function similarity at each node)

    - how do you identify different types of relevance & nodes that supply them for an intent like categorization/prediction?

      - different types of relevance like direct causation (is causative), imminent causation (will be causative, or may be if imminent conditions occur), alternate causation (can be causative)

        - for categorization, direct causation may be identifying/differentiating features - imminent causation may be converging or adjacent features

        - if all a node does is answer yes/no for a particular question, that will be sufficient for that question but not other important questions that could be answerable with the data

        - how to tell which questions are answerable given that one question is answerable

          - meaning you can reach answer node B given a starting point of attribute values, a function set, and a question intent direction identifying an information gap in attribute values, where the answer node may not fully answer the question but it will move toward the question intent direction

            - for example, asking 'is ingredient A toxic' can be answered with filters (probability filter such as 'not likely in moderation') or mapping a trajectory between nodes agent starting position & agent death

            - so the question ('is substance A toxic') can be formmatted like: 
              - a path between nodes agent position and agent death
              - a movement in the direction of node clusters with the high-toxicity attribute (implying toxic intent of the substance)
              
              where the question is the information gap: 
                - will event 'ingest substance A' move in direction of high-toxicity clusters, an adjacent intent direction or state, or toward agent death

            - so other answerable questions would involve a subset of that information (a subset of the path of the answer) or other related object (alternate, opposing, etc)

          - involve a subset or same set of attributes
          - involve opposing/complementary attributes
          - involve direct causative or caused attributes
          - involve abstract attributes

      - node sets/weight paths that arent important for direct causation may be important for imminent causation

  - given that certain algorithms can only add so much certainty for a particular problem type, that should be integrated into output (trust shouldnt be default handler for algorithm output)

  - behavior data, search data, purchase data can be used to link health conditions and train AI to predict conditions from a symptom search

  - statistical tests & hypotheses should be standardized for false objects (false similarities), errors in assumptions (non-normal distribution) or data collection/measurement, change types (about to become another distribution/in the process of being converted)
    
    - 'a hypothesis assuming random independent variables is more likely to include m/n non-random variables or x causal shapes'

    - these insights can be used to adjust test critical values in the absence of data on correctness to use instead:
      - 'a z-value of 1 for a data set like this is likely to be accurate 5% of the time and is likelier to be 1.2 most of the time'

  - structural mismatches of solutions & problems

    1. algorithm structures: theres an inherent structural mismatch between some algorithms (decision tree, neural net) and some problem types (prediction) given the intent & abstraction layer 

      - example: 

        - some algorithms are too specific or have a structural mismatch (decision tree has structural split, specify & direction intents) with the problem type (predicting a set of variables that is about to change)

        - the decision tree occupies a very low-level, neutral, granular, abstract/structural position - that means it can be used for many intents 

        - the decision tree can be hacked with various edge case, phase shift & boundary manipulations

          - if two variables on different layers are about to converge, they should have been on the same layer or in the opposite causal direction or treated as one variable - the model will be increasingly wrong until it's updated or until these ambiguities and likely relationships are accounted for in the design

          - the reason algorithms can be hacked is bc of the structural mismatch between the algorithm and the problem type

        - the intents resulting from the decision tree's layers, direction, and thresholds can only contain so much variation in the data 

        - generated varied data with permutations of data objects (loops, sets, alternates) can be used to make the tree more robust to adjacent/likely changes

    2. test structures: a similar problem is when the test/method will identify false similarities or other objects & return the false version, limiting the potential for the correct version to be identified

      - the structure of the test can be a barrier to the truth, if its over simplified or excludes too much information

      - a test that's done iteratively on new data without changing the function according to new information would fail if the data changes more than the function can handle

    3. brain analysis structures: structural biases in human brains prevent us from seeing the truth - we're biased toward objects we understand or which are simple to derive

    - algorithms should identify mismatches (in complexity, variance, completeness & other metrics) & other problem types between the data/algorithm/problem type

    4. prediction method structures: 

      - bayesian probability has the incorrect structure for solving a problem of predicting the dependent variable if its not actually the dependent variable

      - meaning if the input is actually an output, like a southern dialect is an output of location, so predicting criminal activity for a location based on whether they have a southern dialect may correlate with some location-crime data bc of hot weather increasing emotions, but southerners can move while retaining their manner of speech & can change their emotion-based behavior from the new cold weather

      - probability of (criminal activity | dialect) should be probability of (criminal activity | weather), since weather is the more causative reason why location is a significant factor in criminal activity


  - algorithms should produce a set of solutions (an obvious/simple answer, a pattern-compliant answer, a common answer, a robust answer)

    - example of why youd want an 'obvious solution' tag on the output:

      - how could you build an algorithm that wouldnt overidentify a race attribute as being a potential criminal indicator?

      - query maps & causal shapes:
        - query for data on related word sentiment (if theres a negative association with a word related to that race)
        - query for data on intent ('identifying potential criminals' is the intent of the training process, which needs to be an input to the algorithm)
          - why would identifying an individual by a related attribute to a negative related term be useful for the 'identify potential criminal' intent?
          - if the intent is to 'identify potential criminals', which is a high-stakes intent
          - the algorithm should identify that a causal loop would be dangerous to treat as an input, given that a causal loop (like mistreatment or persecution of minorities leading to poverty which leads to crime) can hide original inputs (root cause being mistreatment)

      - alternatively, it could use interface analysis:

        - use inference & intent (to predict) to arrive at insights like:

          - 'if the answer is obvious, it must not be true bc otherwise I wouldnt have been asked to predict it, unless this is a test situation'
          - 'if the attribute value is common across the population, it must not be a predictor bc there are many people with that attribute value who are not in this data set bc they didnt commit a crime'

        - hard-code insights like above to be consulted if a variable is mistakenly identified as significant

        - treating one involuntary variable as high-impact belies the complexity of social games, which involve learning competitions (who has the best manners, whos the smartest) that would mean there is high potential for economic status variation within nodes having that attribute

        - given the changeability of that variable (easily changed with structural tools & also frequently changed in gene pool), it should not be treated as a predictor

        - it should identify location & economic status as an indicator of criminal activity given the health/drug addictions symptoms of the people whose data is used for training

        - it should also identify social games that are used for criminal activity & skill at those games (making intimidating or emotionless facial expressions)

        - it should identify culture as a key factor in variation in criminal activity, which can be specific to an attribute but has high variation within that attribute (producing gang violence, govt corruption, or a culture of karma/street justice) with low variation in outcomes (kill or be killed) - and identify that if it doesnt have cultural information, it cant make predictions

      - most algorithms dont have the complexity to identify complex sub-systems or related systems like social information games, communities, economies, or cultures

      - a really smart algorithm would immediately identify a few insights like:

        - example of deriving a social game like bullying:
          - with a priority of 'avoiding criticism'
            - most nodes would deserve criticism, because avoiding it is easy, especially at scale
            - this incentivizes criticizing nodes who dont deserve criticism

        - 'there are different reasons people do crimes'
        - 'some reasons they do crimes include need (resource acquisition, asset/reputation protection), goal attainment (enable a career), culture (avoiding crimes isnt a priority), social games (dares, threats, bullying, corruption, group dynamics), enjoyment (test if anyones paying attention, test how fragile the system is, rebel against authority)'
        - 'randomness is the biggest distributor of those reasons' (luck)
        - 'randomness can lead to lack of justice or other types of meaning'
        - 'people who have lack of justice are likelier to do crimes'
        - 'which people dont have justice - unfairly persecuted people'
        - 'which people are unfairly persecuted - different people, excellent people'
        - 'some privileged people with justice/meaning do crimes anyway just for fun to see how much they can get away with' (counterpoint - other reasons to do crimes than need, culture, social games)
        - 'which facial expressions are associated with the criminal reasons we're trying to prevent'
        - 'which factors do we need to complete the prediction function'
        - 'which expressions are often false signals despite being good predictors in subsets'

        - how would you build an algorithm to identify those insights just from maps & from mugshot data? 

          - the point is invalid once you have those insights, bc the answer is clearly not predicting who will do crimes but figuring out:
            - a system of social rules to prevent those situations from happening in society
            - at what point people start trying to have fun rather than contribute
            - deriving intent (either mechanically or with a prediction tool)

          - the algorithm would identify a high-variance data set and derive that its not the obvious visible attributes that will be predictors (except a few like visible signs of crack addiction, which will only predict the ratio that are crack addicts) but the subtle visible attributes, which will necessarily be incomplete without data about each person's specific traversal of the maps

          - the algorithm would identify that certain facial expressions are associated with criminal activity, such as:
            - dead eyed hopeless expression (lost hope of good treatment)
            - crazy expression (went on a rant after putting up with some stressor)
            - shocked expression (cant believe this happened to them)

          - most of them would also have signs of stress destructuring their faces, except resilient criminals, bosses, or some first-time offenders

          - the different routes to each expression should show up in traversals of social game, community, & culture maps as their status & decisions change, and cross concepts like randomness (in which ideas people encounter, which skills they learn, which social games they play), equivalence (in resource distribution), justice (as a counter object to randomness), & meaning (group belonging, goal attainment, success)

          - the output would be a set of paths that leave traces, some of which would show up in mugshots, some of which would require questions

        - identifying a concept (like 'reason to do crime') can be as trivial as exploring combinations of core functions

          - once the algorithm identifies the concept of language (from input data or insight maps) as 'information trades formatted as paths', it should be able to identify the concept of lies, as it will know that 'information can be inaccurately described' from its own errors

          - once it identifies lies, it can identify the concept of social games (reward from coordinating a lie with another node, reward from a successful lie, reward from adding a distant transform to another node's information)

          - once it identifies social games, it can derive their intent (the point of social games is to control other people or to get resources like enjoyment or funds)

          - then it can identify specific social games as anti-societal behavior that hurts the group

          - the counter-object is pro-societal rule compliance that helps the group

          - then it can move on to identifying specific social games (initial crimes, bullying) that remove inputs of pro-societal decisions (complying with rules), creating a 'reason to do crime'

          - then it can look for outputs of those games (stress, difference) and make an attempt to translate that to the data features (facial expressions, signs of addiction, signs of aging)

        - this requires that society-wide data needs to be integrated into the algorithms (like genetic variance) as well as data on criminal activity


  - rather than using data & the training process as an indicator of consensus, they can use patterns (patterns of data, patterns of change, patterns of variables) as an indicator of consensus

  - algorithm based on problematic adaptive systems like cancer bc theyre learning faster than the host system

  - if something can generate a change predictably/consistently, it's a change supply - otherwise it's a change request, so output as well as causal position relative to the output is important when determining category
  
    - time may be a variance gap (a space where change is possible) to resolve a question/problem set - so not resolving it can preserve time, unless resolving it will allow for more potential or moving on to other variance gaps
