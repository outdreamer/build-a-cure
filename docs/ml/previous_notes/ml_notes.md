- like other functions, ml algorithms/structures are only as useful as the potential value added (given their potential to generate certainty structures) by the info formats they create & depend on (subsets, connections), which create errors when used for non-compliant intents with that particular format (like intents requiring a different type/variant of the format)
    - democracy is only useful if the groups that vote represent the population
    - n-degree connections are only useful if the n-degree connections are a possible relevant input to the required target info (like 'n + 1 degree connections') of the function intent, and are processed in a way required to create that target info, and the processing functions are adjacent/available (so creating valuable info from the info format is trivial, making the info format useful)
    - subsets are only useful if their parameters include the important subset capable of producing the required info
    - info formats that are clearly useful for solving the 'find a prediction function' problem, bc they can be adjacently transformed into a prediction function by core functions like 'build' (as in 'build a prediction function out of function components like subset functions or variables')
    	- function subsets, alternate functions, alternate function formats (curved vs. linear functions), general/specific functions, conditional functions, interchangeable functions with equivalent/similar inputs/outputs, approximation functions, base functions, function limits/requirements, variables, variable attributes like weights or position (inputs, outputs, constants, determining variables, causal variables, generative variables, descriptive variables), variable interaction structures (like variable sets & variable types), and data

- problems like volatility ('slight input changes causing volatility in output') can be fixed with some general structures of optimization, like:
	- generalization structures (rather than assigning a specific position, assign an approximate or general position involving the original node & neighboring nodes based on similarity, as in nodes with similar outputs)
	- approximation structures, based on the insight that 'a very specific output (in the form of a 'prediction function') created by non-specific inputs ('network params', which are not perfectly customized/specific to the data set & problem type) is unlikely to be optimal in some way (as in 'robust'), bc the level of certainty achievable with a network is not create-able with the input certainty or the certainty achievable with the learning process given the network, and certainty cant come from nowhere'
	- each node that can be activated in a network should represent a different source of meaning in the form of 'variable interaction structures' with different params/outputs, so that the chance of finding the correct structures is high
	- the potential of a network & params to find certainty structures (like info, insights, patterns, correlations, connections, & structures in general) is calculatable
		- each network can find different interface structures (types, interactions, contributions/influence/changes, patterns, structures like differences/connections/subsets/combinations, examples, concepts like relevance, causes/reasons) with varying success
		- these structures combined as specified in that particular network structure & algorithm offer different types/levels/structures of calculatable certainties in a data set

- "For the CNN-based classifier they observe that units associated with objects and parts emerge in later layers, while earlier layers are largely associated with colours. For the generator network, on the other hand, object/part neurons can be found more frequently in earlier layers, while the later layers focus on colors."
	- https://towardsdatascience.com/four-deep-learning-papers-to-read-in-august-2021-7d98385a378d

- for a cnn, the feature causation moves from adjacently identifiable features (colors being an attribute found in every pixel/feature) to absolutely identifiable features (larger features like objects being identifiable with features representing larger sections)

- for a gan, the feature causation moves from foundation/type/template structures (objects) to apply details to, and specific details (colors) to tune the output with.

- these methods have biases toward priorities that may help or hinder their intents
	- cnn: prioritization of 'adjacent feature' structures (as inputs to 'larger features')
	- gan: prioritization of 'template/type' structures (as inputs to 'detailed type examples' output)
	- gpsa: de-prioritization of 'locality' priority or 'position' structure
	- loss-minimizer using loss landscape: prioritization of 'perspective differentiated by difference from target' as a parameter-selection filter

	- these priorities are conceptual, and the implementation of structuring these priorities may or may not be correct for these intents
		- the structure applied to the concept of 'locality' may or may not be applied in the right form when assigned to the position of 'features' in the structural problem space (compared to other structures like variable/interaction structures, functions in the function network, parameter output difference patterns, etc)
		- these other structures may be more relevant to apply this conceptual priority (or de-prioritization) to, in order to correct associated error types with that priority or contradicting priorities
		- 'parameter output difference patterns' is a particularly useful structure in this problem space that is not being applied optimally
			- the 'threshold' structure in parameter differences is also not being applied optimally, to generate understanding of parameter/algorithm/network/error interactions
			- example: 
				- answering questions like the following would indicate understanding of these interactions:
					- interaction: 
						- 'does a particular parameter/algorithm/network combination generate activation patterns for normal data set that result in an error of "too many 'deactivated' nodes for the possible output of the network to capture the variance of the original output"' (like a structure such as a 'barrier of deactivated nodes preventing a proportion of feature value combinations from contributing to predictions')
					- cause: 
						- 'why does this error occur for this combination'
							- 'parameters combine in this algorithm & network structure in a way that prevents relevant value combinations that are "just below configured & emerging thresholds" from succeeding at contributing to output'
							- 'emerging thresholds & other emerging structures in the "process" object created by data/parameter/algorithm/network/error interaction structures are not accounted for in the input structures'

- identifying the relevant interaction layer or base to act as the symmetry (foundation/type/template to apply variations to) and the variables/attributes that determine the variations from those bases is a workaround that integrates these feature causation structures

	- insights can be used as symmetries or components of them bc theyre units of truth that capture attention, and applying changing to them (using them as a symmetry) will reveal how true they are, in the sense of stability/reliability/consistency

	- causal concepts of useful structures in ml (like how 'difference' is a causal concept of 'attention') are another useful structure that these structures can be standardized to, beyond other metadata like priorities

	- the algorithm would list the identifiable structures (components/attributes/objects/interaction functions/errors) in the data set
		- then it would try to identify default relevant interface structures like types that form the basis of different clusters/subsets in the data set, applied to the structures (error types, component types, variable types), which are relevant for categorization problems (predicting category, generating category example)
		- then it would try to identify variables that determine variations from those types
		
	- this is similar to 'starting in the middle interaction layer and radiating outwards', as opposed to starting from adjacent details or starting from core objects as foundations for change
		- it integrates error types as an 'adversarial guide'

		- this uses a cross-section of interaction layers rather than the raw objects/attributes clearly identifiable in the data set, so there is some derivation work to do before it can be applied

		- another version of 'starting from the middle' would be a cross-section of interaction layer attributes, like:
			- a component of the image + common attribute values for that component + a partial adjacent component
			- pairs of components commonly found together across the image, regardless of adjacence
			- variable & error structures commonly found with attribute values or value patterns

		- this applies a different cross-section as a different interaction layer than the default interaction layers that could act as symmetries, such as a template/foundation/type, or the raw data set attributes, or the default object/attribute/function structures, or an interface layer, where these cross-sections are mixing interface components

		- another variant would be starting from the cross-section definition as the symmetry, & identifying various cross-sections that can act as symmetries & applying them as input data to identify the cross-sections that are most predictive of predictive feature structures or algorithm success

	- applying interface components (like intent/structure/bases) is better than ML (which builds a function from an algorithm & function network that is tuned & designed & applied seemingly by accident) bc it starts from understanding of what is useful for solving the problem, rather than guessing based on a limited list of mental functions & structures (like neurons, activation status, weighted connections, & attention) that drew ML engineers' attention for being simple & known.

	- this is also better bc it starts from the correct interaction layer (problem/solution on the meaning interface) where alternate solutions can be generated/applied and coordinated with other solutions & solution-generating queries for other problems.

	- this is different from a math 'solver' program that just applies known math rules to solve a function with known associated solution rules, which is unaware of any concept of 'meaning' except the definition of 'equivalence' and does not integrate any concept of 'interfaces' like potential, cause, intent, concepts, logic, or information.

- identifying interface components are useful for selecting solutions
	- example: the 'meaning' of this 'optimizer/pooling/regularization function set in these positions' is a 'sub-optimal performance for this intent' because 'the standardizing function removes this info which is required for determining this variable'
	- statements about 'meaning' like this are possible once structures are mapped to 'intent' & 'cause' & other interface components
	- these statements can be used as rules to identify value of & select structures for a context, like for a particular intent
