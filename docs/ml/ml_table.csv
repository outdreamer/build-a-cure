Algorithm,Type,Math Function,Parameters,Explanation or Solution,Advantages,Disadvantages,Accuracy,Generalizability,Robustness,Computational Efficiency,Interpretability,Complexity,Data Requirements,Ethical Considerations,Supervised or Unsupervised,Requirements (scaling, standardization),Loss Function

Linear Regression,Regression,J(θ) = (1/2m) * Σ(h(xᵢ) - yᵢ)²,Objective is to minimize mean squared error,Simple and interpretable,Assumes linearity and low multicollinearity between features and independence and normal distribution of errors and few outliers,Medium,High,Medium,High,High,Low,Medium,Low,Supervised,,Mean Squared Error

Logistic Regression,Classification,J(θ) = -(1/m) * Σ[yᵢ log(h(xᵢ)) + (1 - yᵢ) log(1 - h(xᵢ))],,Probabilistic output,Struggles with non-linear data,Binary cross-entropy loss,,,Medium,High,Medium,High,High,Low,Medium,Low,Supervised,,Cross Entropy Loss
Polynomial Regression,Regression,,,used when the input/output relationship is nonlinear and can be approximated by a polynomial
Support Vector Machine,Classification,J(w, b) = (1/2)||w||² + C Σ max(0, 1 - yᵢ(w·xᵢ + b)),Hinge loss + regularization,Works in high-dimensions,Hard to tune and less scalable,,,High,High,High,Medium,Medium,Medium,High,Medium,Supervised,,Hinge Loss
Support Vector Regression (SVR),Regression,,,Uses margin-based optimization like SVM
SVM Kernel,Algorithm Sub-function
Naive Bayes,Classification,P(y|x) ∝ P(y) * Π P(xᵢ|y),Bayes’ Theorem with conditional independence,Fast and works well with text,Assumes Independence,Low,Low,Low,High,High,Low,Low,Low,Supervised,,No loss (probability-based)

Embedded Method,Feature Selection Method,,,Embedded methods perform feature selection during the model training process and combine the benefits of both filter and wrapper methods, where feature selection is integrated into the model training allowing the model to select the most relevant features based on the training process dynamically,More efficient than wrapper methods because the feature selection process is embedded within model training and is often more scalable than wrapper methods and Wrapper and embedded methods are better for capturing complex feature interactions,Works with a specific learning algorithm so the feature selection might not work well with other models
L1 (Lasso) Regression,Feature Selection Embedded Method and Regression and Regularization,,,A regularized variant of regression that adds a penalty to the loss function to prevent overfitting thereby applying L1 regularization to encourage sparsity in the model where features with non-zero coefficients are considered important and where methods like Lasso are more suitable for certain models like linear models and where lasso can shrink coefficients to zero
L2 (Ridge) Regularization,Feature Selection Embedded Method and Regression and Regularization,,,A regularized variant of regression that adds a penalty to the loss function to prevent overfitting

Decision Tree Classifier,Classification and Feature Selection Embedded Method,Gini = 1 - Σ(pᵢ)² or Entropy = -Σ(pᵢ log₂ pᵢ),Impurity functions to choose best splits,Easy to interpret and fast and performs feature selection by selecting the most important features for splitting nodes based on criteria like Gini impurity or information gain,Prone to overfitting,High,Medium,Low,Medium,Medium,Medium,Low-Medium,Low,Supervised,,Gini Impurity or Information Gain
Decision Tree Regressor,Regression,,,,,,,,,,,,,,Supervised,,Mean Squared Error
Random Forest Classifier,Classification and Feature Selection Embedded Method that is the same as Decision Tree + Bootstrap Aggregation (Bagging) that is an ensemble of decision trees,,,Robust and handles non-linearity and performs feature selection by selecting the most important features for splitting nodes based on criteria like Gini impurity or information gain,Slower and less interpretable,High,High,High,Medium-High,Low,High,Low-Medium,Low,Supervised,,Gini Impurity or Information Gain
Random Forest Regressor,Regression,,,,,,,,,,,,,,Supervised,,Mean Squared Error
K-Nearest Neighbors,Classification and Regression,d(x, xᵢ) = √Σ(xⱼ - xᵢⱼ)² (Euclidean),Distance metric for neighbor selection,No training phase,Slow at prediction time,Medium,Medium,Medium,High,Low,High,Medium,Low,Supervised,,No loss

K-Means,Clustering,Σ||xᵢ - μ_k||² (for each cluster k),Minimize within-cluster variance,Simple and scalable,Needs K specified and spherical classes and is sensitive to noise,Low,Low,Low,High,Low,High,Low,Low,Unsupervised,,Sum of squared errors
DBSCAN,Clustering,ε-neighborhood, density ≥ minPts,Cluster defined by density regions,Handles noise and arbitrary shapes,Struggles with different densities
Hierarchical Clustering,Clustering,d(A,B) = min/max/avg ||a - b||,Agglomerative clustering based on distance linkage,Dendogram visualization and no need to specify k,Computationally expensive,Low,Low,Low,Low,Low,High,Low,Low,Unsupervised,,Distance-metric based

Neural Networks,Classification,J(θ) = (1/m) Σ Loss(yᵢ, ŷᵢ),Backpropagation minimizes loss via gradient descent,,,,,,,,,,,,Cross entropy loss
Neural Networks,Regression,J(θ) = (1/m) Σ Loss(yᵢ, ŷᵢ),Backpropagation minimizes loss via gradient descent,,,,,,,,,,,,Mean squared error
Gradient Descent,Algorithm Sub-function
Backpropagation,Algorithm Sub-function,
Reinforcement Learning,,Q(s, a) ← Q(s, a) + α[r + γ max Q(s’, a’) - Q(s, a)],Bellman equation update
Recurrent Neural Networks (RNNs),,,handle sequential data by maintaining a memory of previous inputs and used for Sequence Modeling and Time Series Prediction and Natural Language Processing,,Struggle with long-term dependencies and have vanishing gradients (solved by LSTM and Gated Recurrent Unit),High,Medium-High,Medium-High,Low-Medium,Low,High,High,Low
Q-Learning,Reinforcement Learning,,,,,,Medium,Medium,Medium,Medium,Medium,High,High,Medium
Deep Q-Network (DQN),Reinforcement Learning,,,,,,High,High,High,Medium,Low,High,High,Medium
Convolutional Neural Networks (CNNs),Neural Networks,,,Image Classification and Object Detection and Image Segmentation,,,High,Medium-High,Medium-High,Low-Medium,Low,High,High,Low
Generative Adversarial Networks (GANs),Neural Networks,,,,,,Medium,Medium,Medium,Low,Low,High,High,High
Long Short-Term Memory (LSTM) networks,Recurrent Neural Network,,,,,,High,High,High,Medium,Low,High,High,Medium

Max Pooling,Neural Network layer type,,,Downsamples feature maps, reduces computation
Flatten,Neural Network layer type,,,Converts 2D to 1D
Dense (Fully Connected),Neural Network layer type,,,All nodes are connected to all nodes


TF-IDF,Vectorization,,,optimizes memory usage by creating a sparse matrix by multiplying importance weight (as in how often a word appears in a document) by how unique a word is (to reduce importance of common words) across the entire collection of documents so words appearing in many documents get a lower IDF score and rare words get a higher score and in the process converts text to vectors handling a large vocabulary in the process
Word2vec,Vectorization,,,uses a two-layer neural network to encode words into embedding vectors for semantic and syntactic similarity using either continuous bag of words or skip gram to infer a word from context or vice versa,skip gram is more effective for infrequent words and CBOW is faster to train
One-Hot Encoding,Vectorization,,,converts a word into a vector with a bit corresponding to the index in the vocabulary with all other bits set to zero
Bag of Words,Vectorization,,,represents text data as a vector of word counts

Dimensionality reduction,Function Type,,,used to simplify datasets by reducing the number of features while retaining the most important information.
Principal Component Analysis (PCA),Signal Processing and Dimensionality Reduction,maximize var(Z) s.t. Z = Xw and ||w||=1,Find eigenvectors of covariance matrix and thereby transforms data into a new set of orthogonal features (principal components) that capture the maximum variance,Reduces overfitting and fast and improves visualization,Components may be hard to interpret and disregards independence of components and misses non-linear patterns,High,High,High,Low,High,Low-Medium,Low,Unsupervised,,Minimizes variance loss
Independent Component Analysis (ICA),Signal Processing and Dimensionality Reduction,,,separates mixed signals into their independent non-Gaussian components thereby finding a linear transformation of data that maximizes statistical independence among the components which is used in signal analysis to isolate distinct sources from mixed signals,doesnt require labeled data and is non-parametric so it doesnt require assumptions about the data probability distribution and can be used for feature extraction to identify important features,assumes decomposed source features are non-gaussian and are independent and are mixed linearly and ICA can be computationally expensive,Unsupervised
Linear Discriminant Analysis (LDA),Dimensionality Reduction,J(w) = |wᵗ S_B w| / |wᵗ S_W w|,Maximize class separation
Autoencoders,Dimensionality Reduction and Anomaly Detection,L = ||x - x'||²,Neural network that compresses and reconstructs data to minimize reconstruction loss,useful for dimensionality reduction and feature learning and anomaly detection,,,,,,Medium,Medium,Medium,Medium
t-Distributed Stochastic Neighbor Embedding (t-SNE),Dimensionality Reduction,Pᵢⱼ ∝ exp(-||xᵢ - xⱼ||²/2σ²),Minimize Kullback-Leibler divergence between distributions and thereby reduces dimensions for visualizing high-dimensional data and preserving local relationships,Captures complex structure unlike PCA,slower and non-deterministic,Low,Low,Low,High,Low,High,Low,Low,Unsupervised,,Minimizes pairwise differences
Non-negative Matrix Factorization (NMF),Dimensionality Reduction,,,Factorizes data into non-negative components,useful for sparse data like text or images
Statistical independence,Concept,,,refers to the idea that two random variables: X and Y are independent if knowing one does not affect the probability of the other so the joint probability of X and Y is equal to the product of their individual probabilities
Isomap,Dimensionality Reduction,,,Preserves geodesic distances to capture non-linear structures in data
Locally Linear Embedding (LLE),Dimensionality Reduction,,,Preserves local relationships by reconstructing data points from their neighbors
Latent Semantic Analysis (LSA),Dimensionality Reduction,,,Reduces the dimensionality of text data to reveal hidden patterns

Scaling
z-score,Evaluation Metric,z = (x - mean)/standard deviation,,Standardizes x value
t-score,Evaluation Metric,,,Uses the student's t-distribution
Variance,Evaluation Metric,variance = (sum of (x - mean) squared)/n,,
Standard deviation,Evaluation Metric,standard deviation = square root of the variance,,
Standard error,Evaluation Metric,standard error = standard deviation / √n,,
Kurtosis,Distribution Metric,,,a statistical measure that describes the shape of a distribution particularly the "peakedness" or "tailedness" of the data
Skewness,Distribution Metric,,,

Accuracy,Classification Evaluation Metric,TP/TP + FP + FP + FN,
Precision,Classification Evaluation Metric,TP/TP + FP,,Weights false positive predictions as important thereby analyzing the positive predictions,Does not consider True Negatives and False Negatives
Recall,Classification Evaluation Metric,TP/TP + FN,,Weights false negative predictions as important thereby analyzing the correct positive examples,Unfortunately it often emphasizes a higher false positive rate
F1 score,Classification Evaluation Metric,2*(Precision * Recall)/(Precision + Recall),,if we increase the precision the recall decreases and vice versa so the harmonic mean of precision and recall combines these two metrics to offset that trade-off,best for uneven classes
Cross-validated F1-score,Classification Evaluation Metric,,,helps avoid overfitting
Mean F1 score,Classification Evaluation Metric
Confusion Matrix,Classification Evaluation Metric,,,Compares counts of actual vs. predicted outputs for each of N target classes to compare predictions (true/false positives and true/false negatives)
AUC curve,Classification Evaluation Metric,,,Evaluates how well the model can separate the classes by analyzing the classification model at different threshold values where a score close to 1 is ideal,good for binary classification
Receiver Operating Characteristic ROC curve,Classification Evaluation Metric,,,Compares true positive rate (recall/sensitivity) and the false positive rate (FP/FP + TN) and determines the model's capacity to distinguish between different classes,good for binary classification
Precision-Recall Curve,Classification Evaluation Metric,,,good when the positive class is rare
Silhouette Score,Clustering Evaluation Metric

Learning Curve,Function Type,,,Compares how well increasingly large training sets perform on the same test set to evaluate if the model would benefit from more data where if the validation accuracy increases with training set size it indicates underfitting and that more data would be useful and where a gap in training and test set accuracy indicates overfitting and where additional data can decrease both underfitting and overfitting

Loss/error functions,Function Type,,,Quantifies how well one prediction of the machine learning algorithm compares to the actual target value during training thereby providing the signal for the model's learning algorithm to update weights and parameters
Log Loss
R-squared loss,Regression Evaluation Metric
Mean Absolute Error,Regression Evaluation Metric
Mean Squared Error,Regression Evaluation Metric
Mean Absolute Percentage Error,Regression Evaluation Metric,,,Used to express the error in terms of percentage, where the smaller the percentage the better the model performance
Root Mean Squared Error,Regression Evaluation Metric,,,Indicates how much the data points are spread around the best line and is the standard deviation of the MSE, where lower value means that the data point lies closer to the best fit line.
Root Log Mean Squared Error,Regression Evaluation Metric
Cross entropy,Function Type,CE(y,p)=−1/n * ∑y(i)×log(p(i)),class label y and predicted probability p,Cross entropy is used to measure the distance between two probability distributions such as using the discrete cross-entropy loss (CE) between class label y and the predicted probability p when training logistic regression or neural network classifiers on a dataset of n examples
Binary Cross Entropy Loss
Multinomial Cross Entropy Loss
Shannon Cross Entropy Loss
Renyi Cross Entropy Loss (generalization of Shannon)
Kullback-Leibler Divergence,Regression Evaluation Metric,,,Evaluates difference between two probability distributions

Entropy,Algorithm Sub-function,Entropy = -Σ(pᵢ log₂ pᵢ),
Gini Impurity,Algorithm Sub-function,Gini = 1 - Σ(pᵢ)²,
Information Gain,Feature Selection Filter Method,,,Measures how well an attribute reduces uncertainty so an algorithm would split data based on which attribute maximizes information gain (reduces uncertainty) in a set S which is calculated using the difference in entropy before and after the split
Gain Ratio,Algorithm Sub-function,,,Improves on information gain by considering the worth of attributes with a wide range of possible values thereby handling the bias of information gain to favor attributes with more pronounced values,
Chi-square test,Feature Selection Filter Method,,,It is generally used to test the relationship between categorical variables. It compares the observed values from different attributes of the dataset to its expected value.
Fisher’s Score,Feature Selection Filter Method,,,Fisher's selects each feature independently according to their scores under Fisher criterion leading to an optimal set of features so a larger the Fisher’s score indicates a better selected feature
Pearson’s Correlation Coefficient,Feature Selection Filter Method,,,pearon's coefficient is a measure of quantifying the association between the two continuous variables and the direction of the relationship with its values ranging from -1 to 1
Variance Threshold,Feature Selection Filter Method,,,variance threshold is an approach where all features are removed whose variance doesn’t meet the specific threshold and by default this method removes features having zero variance and the assumption made using this method is higher variance features are likely to contain more information
Mean Absolute Difference,Feature Selection Filter Method,,,Mean absolute difference is a method is similar to variance threshold method but the difference is there is no square in this method
Dispersion ratio,Feature Selection Filter Method,,,It is defined as the ratio of the Arithmetic mean (AM) to that of Geometric mean (GM) for a given feature whose value ranges from +1 to infinity as AM ≥ GM for a given feature where a higher dispersion ratio implies a more relevant feature
Filter Methods,Feature Selection Method,,,Evaluate each feature independently with target variable where features with high correlation with target variable are selected as it means this feature has some relation and can help make predictions where these methods are used in the preprocessing phase to remove irrelevant or redundant features based on statistical tests (correlation) or other criteria,Quickly evaluate features without training the model and good for removing redundant or correlated features and Filter methods are often preferred for very large datasets due to their speed,These methods don't consider feature interactions so they may miss feature combinations that improve model performance
Wrapper Method,Feature Selection Method,,,Greedy algorithms that train algorithms using different combinations of features and computing relations between these subset features and the target variable and add/remove features with stopping criteria being potentially whenever the model performance decreases or a specific number of features is achieved,Can lead to better model performance since they evaluate feature subsets in the context of the model and can capture feature dependencies and interactions,But are computationally more expensive than filter methods especially for large datasets
Forward selection,Feature Selection Wrapper Method,,,Forward selection is an iterative approach where we initially start with an empty set of features and keep adding a feature which best improves our model after each iteration where the stopping criterion is where the addition of a new variable does not improve the performance of the model
Backward elimination,Feature Selection Wrapper Method,,,Backward elimination is a method that is also an iterative approach where we initially start with all features and after each iteration we remove the least significant feature where the stopping criterion is where no improvement in the performance of the model is observed after the feature is removed
Recursive elimination,Feature Selection Wrapper Method,,,Recursive elimination is a greedy method that selects features by recursively removing the least important ones where it trains a model and ranks features based on importance and eliminates them one by one until the desired number of features is reached

Cost/objective function,Function Type,,,An average of the loss function of an entire training set containing several training examples thereby quantifying the model's performance on the whole training dataset

Cross validation,Function Type,,,Some part of the dataset is reserved for testing the model. There are many types of Cross-Validation out of which K Fold Cross Validation is mostly used
k-fold cross validation,Cross Validation and Generalization Method,,,The original dataset is divided into k subsets (folds) and this is repeated k times where 1 fold is used for testing purposes, the rest k-1 folds are used for training the model which eneralizes the model well and reduces the error rate.
Holdout is a simpler approach,Cross Validation and Generalization Method,,,the dataset is divided into ratios like 80:20 of train and test datasets and is used in neural networks and many classifiers
Adversarial Validation,Validation Type,

Data Augmentation,Function Type,,,Helps generalize and generates new data records/features based on existing data and makes it harder to memorize irrelevant information like over-specific information or noise via training examples or features
Mixup,Data Augmentation and Generalization Method,
Cutout,Data Augmentation and Generalization Method,
CutMix,Data Augmentation and Generalization Method,

Model Interpretability & Explainability,Function Type,,,Understanding how a model makes decisions
SHAP,Model Interpretability
LIME,Model Interpretability
Feature importance plots,Model Interpretability

Activation functions,Function Type,,,Specifies whether a neural network node should be activated thereby allowing that set of weights to contribute to the model
ReLU,Activation function,
Softmax,Activation function,
Tanh,Activation function,

Hyperparameter Tuning,Function Type,,,Process of optimizing model parameters other than data parameters (like learning rate, number of trees).
Grid search,Hyperparameter Tuning
Random Search,Hyperparameter Tuning
Bayesian Optimization,Hyperparameter Tuning

Data/Covariate shift,Problem Type,,,Distribution of x input data changes so detect covariate shift with adversarial validation and fix covariate shift with importance weighting (assign different weights to training examples to emphasize certain examples during training to increase the weight of examples likely to be in the test distribution)
Label/Prior probability shift,Problem Type,,,y class label distribution changes so update the model by adjusting the weights of the weighted loss function according to the new distribution if the new distribution of labels is known which is a type of importance weighting that incentivizes prioritizing certain classes that have become more or less common in the new data
Concept/Conditional shift,Problem Type,,,the conditional p(y|x) distribution has changed which requires continuous monitoring and model retraining
Domain/Joint shift,Problem Type,,,p(x) and p(y|x) both change so its a combination of covariate and concept drift and implies label p(y) shift as well unless the change in p(x) offsets the change in p(y|x) which is detected by monitoring model performance and data statistics and is fixed by collecting more labeled data from the target domain and retraining or adapting the model

Overfitting,Problem Type,,,occurs when the model fits the training data too closely indicating high variance (as opposed to high bias) and learns noise and outliers rather than the pattern so performs well on training data but not on new or test data and is fixed by data augmentation like Mixup/Cutout/CutMix and collecting more data after plotting a learning curve to detect if more data is beneficial and use self-supervised learning to pretrain on large unlabeled datasets to reduce overfitting on small datasets and use transfer learning from highly relevant large labeled datasets and use few-shot learning if additiona labeled data is not feasible and use feature engineering and normalization and include adversarial examples and label/feature noise and label smoothing and smaller batch sizes and regularization techniques like dropout and weight decay and decreasing model size and capacity and building ensemble models
Label smoothing
Regularization,Function Type,,,penalizes complexity by adding a penalty term representing the weights size to the optimizer or the loss function that is minimized during training
L2 Regulation,Regularization and Generalization,RegularizedLoss = Loss + λ/n * ∑ w2,λ is a hyperparameter that controls the regularization strength,penalizes complexity by adding a penalty term of the squared sum of the weights to the loss function that is minimized during training where the optimizer minimizes the modified loss during backpropagation now with the additional penalty term which leads to smaller model weights 
Dropout,Reducing overfitting,,,reduces overfitting by randomly setting some activations of hidden units to zero during training so those neurons cant be relied on and more neurons are used to create multiple independent representations of the same data
Weight Decay,Reducing overfitting ,,,similar to L2 regularization but is applied to the optimizer directly rather than modifying the loss function which has the same effect as L2 regularization
Early Stopping,Reducing overfitting,,,monitor performance on a validation set during training and stop training when performance on the validation set starts to decline (when the validation and training set performance are the most similar which is the point with the least overfitting and which is a good point for early stopping of training iterations)
Smaller models,Reducing overfitting,,,because the smaller the number of model parameters the smaller its capacity to overfit to noise choosing smaller models with reducing layer count/width and pruning and knowledge distillation is good for reducing overfitting,,however double descent and grokking indicate that larger overparameterized models have good generalization if they are trained beyond the point of overfitting
Pruning,Reducing overfitting with smaller models,,,iterative pruning trains a large model and then iteratively removes parameters of the model and retraining it so that it maintains the original performance,improves generalization of the training process as it involves more extended training periods and a replay of learning rate schedules
Knowledge Distillation,Reducing overfitting with smaller models,,,knowledge distillation transfers knowledge from a supervised teacher model (trained with cross-entropy loss between predicted and actual outputs) to a smaller student model (which is trained on the same dataset with the objective of minimizing cross entropy between predicted and actual outputs as well as the difference between student and teacher outputs measured with Kullback-Leibler divergence)
Double Descent,Generalization Observation,,,models with small or very large parameter counts have good generalization performance while models with parameter counts equal to number of training data points have poor generalization performance
Grokking,Generalization Observation,,,as the size of a dataset decreases the need for optimization increases
Large parameter count requirements,Generalization Observation,models with a larger number of parameters require more training data to generalize well.  		
Ensemble models,Reducing overfitting,,,combine predictions from multiple models to improve the overall prediction performance like in random forests and gradient boosting,generalize well,increased computational cost so neural networks are less suitable for ensemble methods
Majority voting,Ensemble method,,,train k different classifiers and collect the predicted class label from each of these k models for a given input and return the most frequent class label as the prediction where ties are resolved using a confidence score or randomly picking a label or picking the class label with the lowest index,can combine different models
Stacking,Generalization and Ensemble method for Classification/Regression,,,a more advanced variant of majority voting that trains a new meta model to combine predictions of other models,can combine different models like a SVM, MLP and a KNN
K-fold ensemble model,Ensemble method,,,after building models using k-fold cross-validation, compute the average performance across all k iterations to estimate the overall performance of the model then combine the individual k models as an ensemble (or train the model on the entire training dataset rather than the k k - 1 subsets) as a majority vote classifier or a stacked ensemble model
Skip-connections,Reducing overfitting with model modifications,,,used in residual networks
Look-ahead optimizers,Reducing overfitting with model modifications
Stochastic weight averaging,Reducing overfitting with model modifications
Multitask learning,Reducing overfitting with model modifications
Snapshot ensemble,Reducing overfitting with model modifications
Bagging,Algorithm Type,
Boosting,Algorithm Type,
Gradient Boosting Regressor,Regression and Feature Selection Embedded Method,L = ΣL(yᵢ, F(xᵢ)),Sequentially minimize loss (e.g., MSE, Log Loss) where like random forests gradient boosting models select important features while building trees by prioritizing features that reduce error the most,,,High,High,High,Medium,Low,High,High,Medium,Supervised,,Mean Squared Error
Gradient Boosting Classification,Classification and Feature Selection Embedded Method,,,,,,High,High,High,Medium,Low,High,High,Medium,Supervised,,Cross Entropy Loss
XGBoost,Classification and Regression,L = Σ l(yᵢ, ŷᵢ) + ΣΩ(fₖ),Loss plus regularization on tree complexity,,,High,High,High,Medium,Low,High,Medium,Medium
AdaBoost,,,,,,,,,,,,,,Exponential Loss

Batch normalization (BatchNorm),Reducing overfitting and normalization and regularization and layer input normalization technique,,,stabilize training and regularize and generalize
Layer normalization (LayerNorm),Reducing overfitting and normalization and regularization and layer input normalization technique,,,stabilize training and regularize and generalize
Weight normalization,Reducing overfitting and normalization,,,normalizes the model weights instead of layer inputs,generalizes and regularizes indirectly

Binomial distribution,,,to calculate probabilities for a process where only one of two possible outcomes may occur on each trial, such as coin tosses.
Hypergeometric distribution,,,to find the probability of k successes in n draws without replacement where both the hypergeometric distribution and the binomial distribution describe the number of times an event occurs in a fixed number of trials and the probability remains the same for every trial for the binomial distribution and in contrast in the hypergeometric distribution each trial changes the probability for each subsequent trial because there is no replacement
Poisson distribution,,,to measure the probability that a given number of events will occur during a given time frame such as the count of library book checkouts per hour where Poisson regression is used when the target variable represents count data (positive integers) and the data is Poisson distributed, which means that the mean and variance are roughly the same and for large means we can use a normal distribution to approximate a Poisson distribution
Ordinal distribution,,,ordinal data is a subcategory of categorical data where the categories have a natural order, such as 1 < 2 < 3 where ordinal regression does not make any assumptions about the distance between the ordered categories. 
Geometric distribution,,,to determine the probability that a specified number of trials will take place before the first success occurs

Curse of dimensionality,Problem Type,
Vanishing Gradient problem,Problem Type,
Bias vs. variance,Problem Type

SMOTE
Hypothesis Testing,,,,commonly used in Outlier Detection
Connected Components,Graph Algorithms
Shortest Path,Graph Algorithms
Pagerank,Graph Algorithms
Centrality Measures,Graph Algorithms
StandardScaler
SARIMA
ARIMA
Bidirectional Encoder Representations from Transformers (BERT)
H2O AutoML
Manifold Learning
FastAI

Sources
https://medium.com/coders-camp/40-machine-learning-algorithms-with-python-3defd764b961
https://sebastianraschka.com/books/ml-q-and-ai/#table-of-contents
https://github.com/Tanu-N-Prabhu/Python/tree/master/Machine%20Learning%20Interview%20Prep%20Questions