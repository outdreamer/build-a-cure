Algorithm	Type	Math Function	Explanation	Advantages	Disadvantages	Supervised or Unsupervised	Loss Function

Linear Regression	Regression	y = ax + b		Objective is to minimize mean squared error to minimize the difference between predicted and actual values, fits a straight line or hyperplane in higher dimensions to the data	Simple and interpretable and good for predicting continuous values and data with linear relationships	Assumes linear relationship between features and target variable, assumes additivity so the impact of one feature on the target isnt dependent on values of other features, assumes features are no correlated (no collinearity), assumes errors are independently and identically normally distributed so confidence intervals arent too wide/narrow and no correlation between errors and constant variance (homoscedasticity) of errors and low multicollinearity between features and independence and normal distribution of errors and few outliers	Supervised		Mean Squared Error
Homoscedasticity Data distribution concept 		Homoscedasticity is constant variance of y as x increases (heteroscedasticity is changing variance of y as x increases)	
Multiple Linear Regression 	Regression 	y = B0 + B1 * x1 + .... + Bn * xN 
Logistic Regression	Classification	p(y = 1|X) = 1/(1 + e^-(B0 + B1*x + other coefficients of logistic regression model))	uses a sigmoid function to output values between 0 and 1 by identifying a classification boundary where the output is interpreted as a probability of a class	Best for binary classification and problems where probabilities are required as it has probabilistic output	Struggles with non-linear data	Supervised		Binary Cross Entropy Loss or Log/Logistic Loss
Polynomial Regression	Regression	y = a0 + a1x + a2x2 + … + anxn + ϵ	ϵ is the error term representing deviations between predicted and actual values	used when the input/output relationship is nonlinear and can be approximated by a polynomial
Ordinary Least Squares Regression (OLSR) 	Regression
Stepwise Regression	Regression
Multivariate Adaptive Regression Splines (MARS)	Regression
Locally Estimated Scatterplot Smoothing (LOESS)	Regression

Hard-margin Support Vector Machine	Classification and Instance-based Algorithm 	The hard-margin SVM objective function is to minimize (1/2)||w||² subject to yᵢ(w dot xᵢ + b) >= 1 with normal vector w	Maximizes the margin between classes with a linear decision boundary, finding the hyperplane that best separates classes with the maximum margin, using kernels to handle non-linear data and using support vectors to define the hyperplane	Robust against overfitting, works in high-dimensions and with complex patterns where clear margins of separation exist and particularly good for binary classification	Hard to tune and less scalable			Supervised		Hinge Loss
Soft-margin (with slack) Support Vector Machine	Classification and Instance-based Algorithm 	Minimize λ * ||w||² + C * 1/n * Σ max(0, 1 - yᵢ(w dot xᵢ - b)) with normal vector w	where choosing a sufficiently small value for λ yields the hard-margin classifier for linearly classifiable input data and the soft-margin SVM behaves similar to hard-margin SVM for large values of C	Hinge loss + regularization	Works in high-dimensions and particularly good for binary classification and useful when data is not linearly separable	Hard to tune and less scalable	Supervised		Hinge Loss
Support Vector Regression (SVR)	Regression and Instance-based Algorithm	Minimize (1/2)||w||² subject to |yᵢ - inner_product(w dot xᵢ) - intercept b) <= ϵ		Uses margin-based optimization like SVM
Kernel Trick	Function Type			Feature mapping function that represents data through a set of pairwise similarity comparisons between original data points with a kernel function which transforms them into coordinates in a higher-dimensional feature space with linearly separable features
Gaussian Radial Basis Function Kernel		k(xi, xj) = exp(-γ||xi - xj||^2)
Polynomial Kernel		k(xi, xj) = (xi * xj) ^ d
Sigmoid kernel (hyperbolic tangent)		k(xi, xj) = tanh(κ * xi * xj + c)

K-Nearest Neighbors	Classification and Regression and Instance-based Algorithm	Uses a distance metric of euclidean distance d(x, Xi) √(Σ for all n (xj - Xij)^2) for continuous neighbor selection, uses overlap distance for discrete data or pearson's correlation coefficient where the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis where it classifies based on nearby neighbors with voronoi-like regions formed by voting among nearest points, classifying new points based on the majority class of the k closest data points using distance metrics to nearest neighbors (majority vote from k nearest neighbors for classification, average value of k nearest neighbors for regression) by finding the k closest samples and predicting the label or value	No training phase and non-parametric and effective for well-separated clusters	Slow at prediction time	Supervised		No loss
Learning Vector Quantization 	Instance-based Algorithm 
Self-Organizing Map (SOM) 	Instance-based Algorithm
Locally Weighted Learning (LWL) 	Instance-based Algorithm

Naive Bayes	Classification and Bayesian Algorithm	The naive Bayes classifier predicted y = argmax p(Ck) Π for all features n p(xi | Ck) combines the probability model with a decision rule where the naive bayes conditional probability model is P(y|x) ∝ P(y) * P(xᵢ|y)/P(x) where the decision boundary indicates the different likelihoods P(x|y = 0) and P(x|y = 1) and where it uses probabilistic regions based on feature likelihoods, applying Bayes rule for class probabilities	Fast and works well with independent features and works well for text classification and high-dimensional input and only requires a small amount of training data to estimate the parameters necessary for classification and good at scaling	Assumes feature independence and information about the target class provided by each variable is unrelated to the information from the others variables with no information shared between the predictors and bad at quantifying uncertainty compared to logistic regression and is outperformed by other approaches like boosted trees or random forests	Supervised		No loss function (probability-based)
Gaussian Naive Bayes	Classification and Bayesian Algorithm	P(x = v|Class k Ck) = 1/√(2 * pi * std dev for k squared) * e ^ -(v - mean) squared/2 * (std dev for k squared)
Multinomial Naive Bayes 	Classification and Bayesian Algorithm
Averaged One-Dependence Estimators (AODE) 	Bayesian Algorithm 
Bayesian Belief Network 	Bayesian Network Type
Bayesian Algor Network (BBN) Bayesian Network Type
Bayesian Network (BN) 	Bayesian Network Type

Class Prior	Probability function	the expected frequency or prior for a given class = number of samples in that class / total number of samples or the class prior can be 1/k if classes are equally probable where prior means before observation so without the influence of other variables (independent probability) where the posterior is with the influence of other variables (conditional probability on some other variable)
Posterior	Bayesian probability function	posterior = prior * likelihood/evidence or probability (Class k | x) = P(Class k) * P(x|Class k)/P(x) or P(b|a) = P(b) * P(a|b)/P(a)
Likelihood function	Probability function	P(x|Y)		
Decision boundary/surface	Decision function			For example comparing the likelihoods P(x|y = 0) and P(x|y = 1) can help identify the decision boundary separating negative and positive predictions between two distributions https://en.wikipedia.org/wiki/Naive_Bayes_classifier#/media/File:ROC_curves.svg where the decision boundary is a line if the distributions are both gaussian with equal covariance and the decision boundary is a curved surface if theyre gaussian with different covariance
Decision Threshold,Probability concept		the decision boundary position (the threshold) is determined by the prior + the loss function and the decision boundary structure is determined by the likelihood function where the decision boundary position for two gaussian distributed classes would be where the distributions intersect at 2⃗x·(µ1−µ−1) = |µ1|^2 −|µ2|^2
Decision rule	Algorithm sub-function	A decision rule takes input x and outputs a decision for example: a decision rule such as y MLE (x) = 1 if x > x MLE, otherwise y MLE (x) = -1
Decision	Probability concept		The Bayes decision is the MAP estimator if the loss function penalizes all errors equally and if the prior is also uniform then the Bayes decision is the MLE
Argmax	Distribution function	the argmax is the input point that maximizes the output point
Log-likelihood ratio	Probability function	for binary classification, the decision depends on the log-likelihood ratio and the threshold T for example: if log(Px|y=1)/(Px|y=-1) > T then y = 1 otherwise y = -1
P-value 	Probability concept 	The p-value is calculated as p = Pr(T >= t | H0) for a one-sided right-tail test-statistic distribution, p = Pr(T <= t| H0) for a one-sided left-tail test-statistic distribution, and p = 2 * min (Pr(T >= t | H0), Pr(T <= t | H0)) for a two-sided test statistic distribution where if the distribution of T is symmetric around zero then p = Pr(|T| >= |t| | H0)	The p-value is the probability of obtaining test results at least as extreme as the result actually observed, assuming the null hypothesis is correct, so a small p-value means such an extreme observed outcome would be unlikely under the null hypothesis, the p-value quantifying the statistical significance of a result, where the result is the observed value of the chosen statistic T, where smaller p-values are evidence against the null hypothesis bc the lower the p-value, the lower the probability of getting that result if the null hypothesis was true, where the distribution of significant p-values is a p-curve which can be used to detect publication bias or p-hacking and has alternative statistics like confidence intervals, likelihood ratios, and bayes factors, and the p-value indicates whether the data falls within the range of what would happen 95% of the time if the null hypothesis is true and if the p-value is greater than the alpha level 0.05
N choose k	Probability Function	n choose k = n!/(k! * (n-k)!)
Likelihood principle 	Statistical concept 		The likelihood principle indicates that given a statistical model, all the evidence in a sample relevant to model parameters is contained in the likelihood function
Likelihood function 	Statistical concept 	The likelihood function L(omega | x) = fX(x | omega) which measures how likely a value of omega is if we know that X has the value x
Coherence
Maximum Likelihood Estimator (MLE)	Probability function	y MLE (x) = argmax P(x|y)
Maximum A posteriori (MAP) Estimator	Probability function	y MAP (x) = argmax P(y|x)
Ordinary Least Squares Estimator
Bayesian Estimator
Bias 	Prediction concept 		Bias is a difference between predicted and actual values, such as the difference between the true value and the mean prediction
Weight 	Prediction concept 		Weight is the change in the output per a unit change in the predictor, which represents the importance of a variable in the predicted output if the variables are normalized

Data and parameter preprocessing	Algorithm Type	Standardize data with scaling then graph data then create relevant features then identify importance/relevance of features then filter features by importance/relevance then select model then select model hyperparameters
Data postprocessing	Algorithm Type	Analyze scores related to model then create a decision surface, then analyze for errors in model preprocessing/training, then analyze for input/concept/output/domain drift

L1 Least Absolute Shrinkage and Selection Operator (Lasso) Regularization	Feature Selection Embedded Method and Regularization	loss = 1/n * Σ (actual yi - predicted yi)^2 for all n examples + lambda * Σ|wi| for all m features		A regularized variant of regression that adds an absolute value of the w coefficient penalty to the loss function to prevent overfitting thereby applying L1 regularization to encourage sparsity in the model where features with non-zero coefficients are considered important and where methods like Lasso are more suitable for certain models like linear models and where lasso can shrink coefficients to zero, where L1 requires feature normalization
Lasso Regression	Regression	A regression model that uses Lasso regularization to select important features
L2 (Ridge) Regularization	Regularization	loss = 1/n * Σ(actual yi - predicted yi)^2 for all n examples + lambda * Σ wi^2 for all m features	A regularized variant of regression that adds a squared w coefficient penalty to the loss function thereby penalizing larger weights which encourages weights to decay to zero to prevent overfitting and handles multicollinearity by shrinking coefficients of correlated features instead of eliminating them by constraining the coefficient norm, where L1 regularization uses the sum of absolute values of weights and L2 uses the sum of weights squared, where L2 requires feature normalization
Elastic Net Regression	Regression	Elastic Net Regression combines L1 and L2 regularization in adding the absolute norm of the weights and the squared measure of the weights with a mixing parameter alpha where 1 is lasso, 0 is ridge and other values are both lasso and ridge: Loss = 1/n Σ(actual yi - predicted yi)^2 + lambda * ((1 - alpha) * Σ|wi| for all m features + alpha * Σ wi^2 for all m features)
Least-Angle Regression (LARS) 	Regularization function
AIC/BIC 	Regularization function
Basis pursuit denoising 	Regularization function
Rudin-Osher-Fatemi model (TV) 	Regularization function
Potts model 	Regularization function
RLAD 	Regularization function
Dantzig Selector 	Regularization function
SLOPE  	Regularization function

Decision Tree Classifier	Classification and Feature Selection Embedded Method	Gini = 1 - Σ(pᵢ)² or Entropy = -Σ(pᵢ log₂ pᵢ)	Uses an entropy/impurity function to choose best splits based on maximization of information gain like Gini Impurity or entropy, where each leaf in the tree is a decision or prediction and each decision node represents a feature test, where each branch represents an outcome of the test, where end leaf nodes provide the final prediction value, constructing a model of decisions made based on actual values of attributes in the data, where the algorithm finds the set S that minimizes the sum of child node impurities and chooses the split (X, S) that gives the minimum compared to all X and S for each variable X, then repeats this at each decision node until a stopping threshold is reached, where the max depth of a tree should be limited as well as limiting the number of test nodes, the minimum number of data points at a node required to split, avoiding splitting when at least one of the subsample sizes is below a threshold and stop splitting a node if it doesnt improve the fit to prevent overfitting 	Best with categorical/numerical data and easy to interpret and fast and performs feature selection by selecting the most important features for splitting nodes based on criteria like Gini impurity or information gain 	Doesn't require scaling or normalizing the data and handles non-linear relationships and missing data	Prone to overfitting if too deep and are often inaccurate and unstable/volatile and can't capture complex interactions and information gain is biased towards features with many categories and computationally expensive	Supervised		Entropy, Gini Impurity or Information Gain
Decision Tree Regressor	Regression		Supervised		Mean Squared Error
Classification and Regression Tree (CART) 	Decision Tree Algorithm
Iterative Dichotomizer (ID3) 	Decision Tree Algorithm
C4.5 and C5.0 	Decision Tree Algorithm
Chi-squared Automatic Interaction Detection (CHAID) 	Decision Tree Algorithm
Decision Stump 	Decision Tree Algorithm
M5 	Decision Tree Algorithm
Conditional Decision Trees 	Decision Tree Algorithm

Blending (Weighted average) 	Ensemble algorithm type

Bagging (bootstrapped aggregation)	Ensemble algorithm type			Trains multiple similar models and averages their predictions to reduce errors where the bootstrapping procedure (repeatedly selecting a random sample with replacement of the training set and fitting a model to these samples then averaging for regression or selecting the majority vote for classification) leads to better model performance because it decreases the variance of the model, without increasing the bias, where subsampling the data and allowing the weak learner trees to overfit slightly by not pruning the trees creates less correlated trees which have the effect of creating weaker but not weak learners 	Bagging leads to improvements for unstable procedures which include artificial neural networks, classification and regression trees, and subset selection in linear regression	Bagging can mildly degrade the performance of stable methods such as k-nearest neighbors

Binning	Bagging/Random Forest function 	Binning involves grouping values together to avoid values that are far apart

Stacking (Stacked Generalization) Ensemble learning algorithm type			An ensemble learning algorithm where the final stacked model combines predictions from multiple strong base models 	Stacking has better accuracy and can combine different models like decision trees, logistic regression, and SVM and reduces overfitting when implemented with cross-validation by balancing out weaknesses of each model and learns from mistakes of base models and is customizable	Stacking is complex, slow, not interpretable, and has a risk of overfitting if the meta-model is too complex or if there's data leakage and requires more data

Mixture of Experts (MoE) 	Ensemble algorithm 		Similar to stacking, MoE creates a strong model from strong learner classifier models (experts in some subset of the feature space) by dividing the task into sub-tasks and training each expert on a sub-task, involving a divide and conquer approach similar to decision trees, and a meta-model similar to stacked generalization, and a gating/routing network to route inputs to the most relevant expert and a combination method of expert predictions, where experts can be any neural network model like a transformer or a simple feedforward network and outputs a probability distribution for experts, and where the gating network is often a simple linear layer followed by a softmax function, and where load balancing can prevent reliance on a subset of experts which also prevents expert collapse, and where hyperparameters include number of experts where too few experts limit the model's capacity and too many experts lead to underutilization and training instability and where expert capacity of how many tokens each expert can process in a batch is more flexible at higher capacity with increased computational cost and where load balancing regularization strength needs to be balanced against task performance since excessively strong regularization can harm performance while weak regularization can lead to expert imbalance and where gating entropy measures routing diversity, where pooling/combination methods include selecting the expert with the largest output or confidence provided by the gating network or using a weighted sum of predictions given confidence level 	Dense expert selection of more experts can improve performance when multiple perspectives on the input is useful and expert parallelization enables parallel computing and dynamic batching of inputs routed to the same expert can increase efficiency and where memory allocation can minimize overhead from expert switching and data movement and where MoE models can maintain approximately constant computational cost while scaling parameters 	Gradients may not flow effectively through unused experts so auxiliary loss functions (additional loss terms to penalize overconfidence in routing or underuse of experts), gradient scaling to ensure all experts receive inputs during training even when selected infrequently, and regularization techniques to prevent overfitting to specific routes and maintain diversity among experts are used to address this problem, and where distributed implementations have to manage data movement between experts and uneven expert utilization can create bottlenecks in distributed systems

Boosting	Ensemble algorithm type		Boosting creates a series of models that correct errors made by previous models

Gradient Boosting Classification	Ensemble and classification and Feature Selection Embedded Method	Objective is to minimize the loss F(x) = Σ for all M gamma_m * h_m(x) where the model at stage m is F_m(x) = F_m-1(x) + learning rate gamma_m * h_m(x) where h_m(x) is a new weak model trained to minimize the gradient of the loss function at step m where predictions of the new model are added to the ensemble and the training iteration is repeated until a stopping threshold is reached which typically uses decision trees which is different from random forest in that gradient boosting builds one tree at a time and gradient boosting combines results iteratively rather than at the end of the process which is what random forest does		Best for structured/tabular data, high performance models where it refines weak learners sequentially with boosted decision boundaries improving error step by step by building an ensemble of weak learners (usually decision trees) sequentially so each new model corrects errors by previous models 	Reduces overfitting	Supervised		Cross Entropy Loss
Gradient Boosting Regressor	Ensemble and regression and Feature Selection Embedded Method	L = ΣL(yᵢ, F(xᵢ))	Sequentially minimize loss (e.g. MSE, Log Loss) where like random forests, gradient boosting models select important features while building trees by prioritizing features that reduce error the most			Supervised		Mean Squared Error Loss
XGBoost	Ensemble classification and Regression	L = Σ l(yᵢ, ŷᵢ) + ΣΩ(fₖ)	Loss plus regularization on tree complexity		advanced version of gradient boosting that includes regularization to prevent overfitting	faster than gradient boosting for large datasets
LightGBM	Ensemble algorithm for classification			Uses a histogram-based approach for faster computation and supports categorical features natively
CatBoost	Ensemble algorithm for classification			Designed for categorical data with built-in encoding, using symmetric trees for faster training and better generalization	
AdaBoost	Ensemble algorithm 					Exponential Loss
Gradient Boosting Machine (GBM) 	Ensemble algorithm
Gradient Boosted Regression Trees (GBRT)	Ensemble algorithm
Random Forest Classifier	Classification and Feature Selection Embedded Method and Ensemble algorithm 		Random Forest Classifier is the same as Decision Tree + Bootstrap Aggregation (Bagging) that is an ensemble of decision trees with more stable boundaries and aggregated decision regions from multiple trees that trains multiple decision trees on different parts of the data with random sampling by considering a random subset of features at each split and with feature selection, then aggregates the predictions of multiple trees (majority vote for classification, average for regression), which applies randomness by selecting random data samples with replacement from the training set to train different models and average their predictions where this random sampling of the training set reduces variance, and also each time a tree split is determined, random forest takes a random sample of m features from the full set of n features without replacement and uses this subset of m features as candidates for the split which decorrelates trees, where a larger value of trees reduces overfitting and typically this parameter is initialized as the square of the number of features, where an excessive number of large trees can lead to overfitting 	High accuracy and reduces overfitting and robust to outliers and handles large datasets with high dimensionality and handles missing data by binning and handles complex data and noise doesnt require scaling or normalization and handles non-linearity and performs feature selection by selecting the most important features for splitting nodes based on criteria like Gini impurity or information gain	Slower and less interpretable and sensitive to imbalanced data and random forests also do not generally perform well when given sparse data with little variability and consumes a lot of memory and is difficult to tune and is likely to pick correlated features bc it samples features to build each tree	Supervised		Entropy, Gini Impurity or Information Gain
Random Forest Regressor	Regression and Feature Selection Embedded Method and Ensemble algorithm		Supervised		Mean Squared Error
Weak learner 	Model Type 		Weak learning models perform slightly better than random guessing and are often combined to create strong learning models, including types of weak learners like a decision tree with one node, k nearest neighbors with k = 1 and operating on one or a subset of input variables, multi-layer perceptron with one node operating on one or a subset of input variables, or naive bayes operating on one input variable, where weak learners are often combined in ensemble learning such as boosting algorithms which combine weak learners to create strong learners by combining the predictions of weak learners by training weak learners sequentially to minimize the errors made by previous models
Strong learner 	Model Type 		Strong learning models learn with good accuracy, examples include logistic regression, SVMs, and k-nearest neighbors with parameters tuned to create strong models

Few-shot learning 			Using a very small example-to-class ratio for small training sets, for example in a 5-way 1-shot problem there are five classes with one example each, uses a support set to sample training tasks that mimic the use-case scenario during prediction, where each training task has a query image to be classified and the model is trained on several training tasks from the support set in an "episode", where the classes in the support and query sets differ from the support and query sets encountered during training, where one few-shot strategy is to learn a model that produces embeddings so we can find the target class with a nearest-neighbor search among the images in the support set, where the model learns how to produce good embeddings from the support set to classify the query image based on finding the most similar embedding vector		Supervised
Meta-learning (few shot)	Few shot learning type 		Training updates the model's parameters so it can adapt well to a new task 			Supervised

Loss/error functions	Function Type			Quantifies how well one prediction of the machine learning algorithm compares to the actual target value during training thereby providing the signal for the model's learning algorithm to update weights and parameters
Mean Absolute Error	Regression Loss Function 	1/n * Σ for all n |actual yj - predicted yj| 		Sensitive to outliers
Mean Squared Error	Regression Loss Function	1/n * Σ for all n of (actual yᵢ - predicted yᵢ)²		Penalizes larger errors heavily
Mean Absolute Percentage Error	Regression Loss Function			Used to express the error in terms of percentage	 where the smaller the percentage the better the model performance
Root Mean Squared Error	Regression Loss Function	√(Σ for all n (actual yi - predicted yi)^2) / N		Indicates how much the data points are spread around the best line and is the standard deviation of the MSE	 where lower value means that the data point lies closer to the best fit line.
Root Mean Squared Log Error	Regression Loss Function 	√ (Σ for all n (log(actual yi + 1) - log(predicted yi + 1))^2 / N)
Huber Loss
Los-cosh Loss 	Regression Loss Function 	Σ log(cosh(predicted yi - actual yi)) 	Log-cosh loss is a smooth alternative to MAE that is less sensitive to outliers

Cross Entropy Loss	Loss Function	-Σ for all n (true label yᵢ * log(predicted probability pi for class i)) where the true label is a one-hot encoded vector which is 1 for the correct class and 0 for incorrect classes	Cross entropy is used to measure the distance between two probability distributions such as using the discrete cross-entropy loss (CE) between class label y and the predicted probability p when training logistic regression or neural network classifiers on a dataset of n examples

Binary Cross Entropy Loss 	Binary Classification Loss Function 	-1/n * Σ for all n [actual yi * log (predicted yi) + (1 - actual yi) * log(1 - predicted yi) 	Used in binomial logistic regression, measures how close predictions are to actual class probabilities
Hinge Loss	SVM Binary Classification Loss Function	max(0, 1 - yᵢ(w·xᵢ + b)) 	Encourages a large margin between classes

Categorical Cross Entropy Loss 	Multiclass Classification Loss Function - Σ for all n actual yi * log(predicted yi) 	Generalization of binary cross entropy for > 2 classes
Sparse Categorical Cross Entropy Loss 	Multiclass Classification Loss Function with sparse labels 			Optimized for memory, labels don't need to be one-hot		
KL Divergence 	Probabilistic Classification 		Measures divergence from true distribution

Focal Loss 	Imbalanced Classification Loss Function 			Focuses learning on hard misclassified examples
Hinge Loss/Margin Ranking Loss 	Ranking Loss Function 			Encourages correct order of relevance
Triplet Loss 	Loss Function used in Face Recognition, Embeddings 			Encourages anchor-positive closeness and anchor-negative separation
Dice Loss/IoU Loss 	Image segmentation Loss Function 			Measures overlap and is robust when classes are imbalanced
Connectionist Temporal Classification Loss 	Sequence model Loss Function 			Enables training without aligned input-output sequences
Wasserstein Loss 	GAN/WGAN Loss Function 			Stabilizes gradients in training generative models
Exponential Loss 	Adaboost Loss Function

Log Loss	Regression Loss Function	-1/n * Σ for all points n Σ for all classes m yij * log(pij)	Used in multinomial logistic regression and neural networks
R2 loss	Regression Loss Function 	Ratio of the sum of squares residual (SSR the total variation of predicted values from the mean) and the sum of squares total (SST the total variation of actual values from the mean) = 1 - Σ for all n (actual yi - predicted yi)^2 / Σ for all n (actual y - y mean)^2 = 1 - Mean squared error/Var(y)	The coefficient of determination R2 measures the proportion of the variation in the dependent variable that can be explained by independent variables	Less sensitive to outliers
Adjusted R2 loss 	Regression Loss Function

Entropy	Information uncertainty function	Entropy H(S) = -Σ for all k (pᵢ log₂ pᵢ) where pᵢ is the proportion of the dataset belonging to class i	entropy encodes the number of info bits required to represent information and a decision tree tries to reduce the entropy by splitting the data on features that provide the most information about the target variable
Shannon Entropy 	Entropy type 	The expected amount of information/surprise in a random variable is information(x) = -log(p(x))	log is the base 2 log or natural log and p(x) is the probability of event x
Cross Entropy 	Entropy type 	The distance between the true distribution p and the predicted distribution q
Conditional Entropy 	Entropy type 	The average uncertainty remaining in Y given that X is known
Kullback-Leibler Divergence (Relative Entropy)	Entropy type 	Indicates how a probability distribution diverges from a second expected distribution
Differential Entropy 	Entropy type 	Differential entropy is entropy for continuous random variables
Joint Entropy 	Entropy type 	The uncertainty of two variables together 
Renyi Entropy (generalization of Shannon) 	Entropy type 	Renyi entropy is a tunable generalization of entropy for different sensitivity levels
Tsallis Entropy 	Entropy type 	Tsallis entropy is another generalization of entropy used in non-extensive systems like complex networks

Gini Impurity	Information impurity function	Gini G(S) = 1 - Σ for all k (pᵢ)²	 The lower the Gini Impurity the better the feature splits the data into distinct categories
Information Gain	Feature Selection Filter Method	IG = entropy H(S) - entropy H(S|a) = entropy H(S) - Σ for all values v in attribute A |Sv|/|S| * H(Sv) information gain = entropy of a node of the decision tree minus the entropy of a candidate split at node t of a decision tree	Measures how well an attribute reduces uncertainty/entropy where a greedy algorithm would split data based on which attribute maximizes information gain (reduces uncertainty/entropy) in a set S which is calculated using the difference in entropy before and after the split	it tends to choose the most impactful features close to the root of the tree and is a good measure for deciding the relevance of some features	One major drawback of information gain is that the feature that is chosen as the next node in the tree tends to have more unique values
Phi function 	Decision tree split function 	phi = 2 * PL * PR * Q(s|t) 	the phi function decides relevance based on goodness of a split at a node and the phi function is maximized when the chosen feature splits the samples in a way that produces homogenous splits and have around the same number of samples in each split
Gain Ratio	Information entropy function 	[-Σ for all n H(T) * log(H(T)) - (-Σ for all n H(T|a) * log(H(T|a))) ]/ -Σ for all n N(ti)/N(t) * log base 2 (N(ti)/N(t)) where N(xi) is the number of times xi occurs and H(T) is the entropy of T		Improves on information gain by considering the worth of attributes with a wide range of possible values thereby handling the bias of information gain to favor attributes with more pronounced values

Centroid-based Clustering Method Clustering Method Type			Represent clusters using central points like centroids or medoids
K-Means	Centroid-based Clustering Method	The objective is to find argmin Σi in K Σx in Si ||xᵢ - μ_k||² = argmin Σi in K |Si| Var Si (for each cluster k) where Si is the size of the set of points in cluster k and μ_k is the centroid of cluster k where each observation is a d-dimensional vector, aiming to partition the n observations into k <= n sets so as to minimize the within-cluster sum of squares WCSS (variance) and which is equivalent to minimizing the pair-wise squared deviations of points in the same cluster, picking k centers, assigning each point to the closest center and then updating centers based on the average of assigned points, repeating until assignments stabilize	Minimizes within-cluster variance by grouping data by proximity with the cluster centroids discovered from unlabeled data	Simple and fast and works well with spherical clusters with similar sizes and scalable	Needs K specified and spherical classes and is sensitive to noise	Unsupervised		Sum of squared errors
k-median clustering 	Clustering Method
K-means++ clustering Centroid-based Clustering Method
K-mode clustering Centroid-based Clustering Method
Fuzzy C-means (FCM) Clustering Centroid-based Clustering Method

Distribution-based Clustering	Clustering Method Type
Gaussian Mixture Models (GMMs)	Distribution-based Clustering Method 			Models clusters as overlapping Gaussian distributions, assigning probabilities for data points' cluster membership
Expectation-Maximization Algorithms Distribution-based Clustering Method
Dirichlet Process Mixture Models (DPMMs)	Distribution-based Clustering Method

Connectivity-based Clustering method 	Clustering Method Type
Hierarchical Clustering	Connectivity-based Clustering	d(A	B) = min/max/avg ||a - b||	Agglomerative clustering based on distance linkage, building a tree-like structure by merging or splitting clusters	Dendogram visualization and no need to specify k	Computationally expensive	Unsupervised		Distance-metric based
Within-Cluster Sum of Squares - WCSS	K-means clustering objective function Σk in K Σx in Sk ||xᵢ - μ_k||² for all Sk where Sk is the set of points in cluster k and μ_k is the centroid of cluster k
Agglomerative Clustering	Connectivity-based Clustering
Divisive Clustering	Connectivity-based Clustering
Affinity Propagation 	Connectivity-based Clustering

Density-based Clustering Method 	Clustering Method Type
DBSCAN (Density-Based Spatial Clustering of Applications with Noise)	Density-based Clustering	ε-neighborhood with density ≥ minimum points	Forms clusters defined by density regions	Handles noise and outliers and arbitrary shapes	Struggles with different densities	Unsupervised
OPTICS (Ordering Points to Identify the Clustering Structure)	Density-based Clustering

Neural Networks	Classification	J(θ) = (1/m) Σ Loss(yᵢ,ŷᵢ)	Backpropagation minimizes loss via gradient descent using layers of neurons that apply weighted transformations followed by non-linear functions, learning by backpropagating error and updating weights by gradient descent	Captures complex patterns and non-linear relationships			Cross entropy loss
Neural Networks	Regression							Mean squared error
Gradient Descent	Optimization function	the update rule θj :=θj − (α ^ ∂/∂θj) * J(θ)
Stochastic Gradient Descent 	Optimization function
Batch Gradient Descent
Adam Gradient Descent
Backpropagation	Neural network weight update function			applies the chain rule to compute the gradient of the loss function with respect to each weight in the network
Multilayer Perception 	Neural Network Type for Classification and Regression 		Neural network with multiple layers of nodes
Deep learning 	Learning Type		Uses neural networks with many layers 	useful for tasks involving large datasets and high-dimensinoal data
Feedforward Neural Networks (FNN) Deep Learning Network Type used for classification and regression 	Basic neural network model 	Easy to train and is useful for simple tasks
Convolutional Neural Networks (CNN)	Deep Learning Network Type 			Good for handling grid-like data and learns spatial hierachies of features
Generative Adversarial Networks (GANs)	Deep Learning Network Type 			Useful for generating synthetic data closely resembling real data
Transformer Networks 	Deep Learning Network Type 			Used in sequence-to-sequence tasks 	Efficiently handles long-range dependencies and parallelizes training

Hopfield Network 	Neural Network Type
Radial Basis Function Network (RBFN) 	Neural Network Type
Deep Boltzmann Machine (DBM) Neural Network Type
Deep Belief Networks (DBN) Neural Network Type

Reinforcement Learning 	Learning Type		Q(s,a) ← Q(s,a) + α[r + γ max Q(s’,a’) - Q(s,a)] uses a Bellman equation update 		The model learns by making a sequence of decisions interacting with an environment and receiving rewards/penalties, with a goal of maximizing cumulative reward, categorized into model-based and model-free methods
Model-based Reinforcement Learning 	Reinforcement Learning Type 		Uses an environment model to predict outcomes and help the agent plan actions by simulating potential results
Markov Decision Processes (MDPs)	Reinforcement Model-based Learning Type 	
Bellman equation	Reinforcement Model-based Learning Type 	
Value iteration algorithm 	Reinforcement Model-based Learning Type 	
Monte Carlo Tree Search 	Reinforcement Model-based Learning Type 	
Model-free Reinforcement Learning 	Reinforcement Learning Type 		Model-free methods dont depend on an environment model but instead learn directly from experience by interacting with the environment and adjusting actions based on feedback, which have value-based and policy-based methods
Value-based Model-free Reinforcement Learning 	Reinforcement Learning Model-free Type 		Value-based methods learn the value of different states or actions where the agent estimates the expected return from each action and selects the one with the highest value
Q-Learning 	Value-based Model-free Reinforcement Learning Type 			Used for decision-making in sequential tasks	Can learn optimal policies even with limited environment knowledge
Deep Q-Networks (DQN) 	Reinforcement Learning Network Type 		Handles more complex reinforcement learning with deep learning, combining q-learning with deep neural networks, allowing it to handle large state spaces
SARSA 	Value-based Model-free Reinforcement Learning Type
Monte Carlo Methods 	Value-based Model-free Reinforcement Learning Type
Policy-based Model-free Reinforcement Learning 	Reinforcement Learning Model-free Type 		Policy-based methods directly learn a policy (a mapping from states to actions) without estimating values where the agent continuously adjusts its policy to maximize rewards
REINFORCE algorithm 	Policy-based Model-free Reinforcement Learning Type
Actor-Critic algorithm 	Policy-based Model-free Reinforcement Learning Type
Asynchronous Advantage Actor-Critic (A3C) 	Policy-based Model-free Reinforcement Learning Type
Policy Gradient Methods	Reinforcement Learning Type 		Learns policies directly for reinforcement learning tasks	Useful for tasks with continuous action spaces or complex environments
Proximal Policy Optimization (PPO) 	Reinforcement Learning Type 		PPO is reinforcement learning with stable and efficient updates 	Useful for large-scale problems with complex environments

Recurrent Neural Networks (RNNs)	Network Type		handles sequential data by maintaining a memory of previous inputs and used for Sequence Modeling and Time Series Prediction and Natural Language Processing		Struggle with long-term dependencies and have vanishing gradients (solved by LSTM and Gated Recurrent Unit)	
Long Short-Term Memory (LSTM) networks	Recurrent Neural Network Type	Models sequences with long-range dependencies	Addresses the vanishing gradient problem in RNNs

Semi-supervised learning 	Learning type 	A hybrid between supervised and unsupervised learning that uses both labeled and unlabeled data for training 	Often used with large datasets when labeling data is expensive or time-consuming
Self-supervised learning 	Learning Type 	Self-supervised learning is a subset of unsupervised learning where the model learns to predict parts of the data from other parts of the data without explicit labels	Is useful when labeling data is not possible

Association rules algorithms 	Algorithm Type 	association rules algorithms find patterns between items in large datasets based on the frequency of item occurrences and co-occurrences
Apriori algorithm 	Association Rule Algorithm Type 		Finds frequent itemsets by iterating through data and pruning non-frequent item combinations
FP-Growth (Frequent Pattern-Growth)	Association Rule Algorithm Type 		Efficiently mines frequent itemsets using a compressed FP-tree structure without candidate generation
ECLAT (Equivalence Class Clustering and bottom-up Lattice Traversal)	Association Rule Algorithm Type 		Uses vertical data format for faster frequent pattern discovery through efficient intersection of itemsets

Max Pooling 	Neural Network layer type			Selects a maximum of inputs in pool, and downsamples feature maps	 Reduces computation
Average Pooling 	Neural Network layer type 		Averages inputs in pool
Flatten 	Neural Network layer type			Converts 2D to 1D
Dense (Fully Connected) 	Neural Network layer type			All nodes in the layer are connected to all nodes in the next layer

TF-IDF	Vectorization			Optimizes memory usage by creating a sparse matrix by multiplying importance weight (as in how often a word appears in a document) by how unique a word is (to reduce importance of common words) across the entire collection of documents so words appearing in many documents get a lower IDF score and rare words get a higher score and in the process converts text to vectors handling a large vocabulary in the process
Word2vec	Vectorization			Uses a two-layer neural network to encode words into embedding vectors for semantic and syntactic similarity using either continuous bag of words or skip gram to infer a word from context or vice versa	skip gram is more effective for infrequent words and CBOW is faster to train
One-Hot Encoding	Vectorization			Converts a word into a vector with a bit corresponding to the index in the vocabulary with all other bits set to zero 		Using one-hot encoding with categorical variables of high cardinality is suboptimal bc of the curse of dimensionality
Binary Encoder 	Vectorization
Label Encoding	Vectorization
Ordinal Encoding	Vectorization
Target Encoding	Vectorization
Bag of Words	Vectorization			Represents text data as a vector of word counts

Dimensionality reduction	Function Type			used to simplify datasets by reducing the number of features while retaining the most important information.
Principal Component Analysis (PCA)	Signal Processing and Dimensionality Reduction to 2d	maximize var(Z) s.t. Z = Xw and ||w||=1, find the covariance matrix, find the eigenvalue decomposition of the covariance matrix and find the projection with the eigenvectors of the covariance matrix, PCA transforms data into a new set of axes or orthogonal features (principal components) that capture the maximum variance, keeping the most important components	Reduces overfitting and reduces the number of features while retaining the variance and fast and improves visualization	Components may be hard to interpret and disregards independence of components and misses non-linear patterns	Unsupervised		Minimizes variance loss
Principal Component Regression (PCR) 	Dimensionality Reduction
Independent Component Analysis (ICA)	Signal Processing and Dimensionality Reduction			separates mixed signals into their independent non-Gaussian components thereby finding a linear transformation of data that maximizes statistical independence among the components which is used in signal analysis to isolate distinct sources from mixed signals	Doesnt require labeled data and is non-parametric so it doesnt require assumptions about the data probability distribution and can be used for feature extraction to identify important features	Assumes decomposed source features are non-gaussian and are independent and are mixed linearly and ICA can be computationally expensive	Unsupervised
Linear Discriminant Analysis (LDA)	Dimensionality Reduction	J(w) = |wᵗ S_B w| / |wᵗ S_W w|	Maximize class separation
Mixture Discriminant Analysis (MDA)
Quadratic Discriminant Analysis (QDA)
Flexible Discriminant Analysis (FDA)
Autoencoders	Dimensionality Reduction and Anomaly Detection	L = ||x - x'||²	Neural network that compresses and reconstructs data to minimize reconstruction loss, learning efficient representations of data	Useful for dimensionality reduction and feature learning and anomaly detection, works well with high-dimensional data
Stacked Auto-Encoders 	Neural Network Type
t-Distributed Stochastic Neighbor Embedding (t-SNE)	Dimensionality Reduction	Pᵢⱼ ∝ exp(-||xᵢ - xⱼ||²/2σ²)	Minimize Kullback-Leibler divergence between high and low dimensional distributions and thereby reduces dimensions for visualizing high-dimensional data and preserving local relationships	Captures complex structure unlike PCA and is effective at reducing the dimensionality of complex data	Slower and non-deterministic	Unsupervised		Minimizes pairwise differences
Non-negative Matrix Factorization (NMF)	Dimensionality Reduction			Factorizes data into non-negative components	useful for sparse data like text or images
Isomap	Dimensionality Reduction			Preserves geodesic distances to capture non-linear structures in data
Locally Linear Embedding (LLE)	Dimensionality Reduction			Preserves local relationships by reconstructing data points from their neighbors
Latent Semantic Analysis (LSA)	Dimensionality Reduction			Reduces the dimensionality of text data to reveal hidden patterns
Partial Least Squares Regression (PLSR) 	Dimensionality Reduction Type
Sammon Mapping	Dimensionality Reduction Type
Multidimensional Scaling (MDS)	Dimensionality Reduction Type
Projection Pursuit	Dimensionality Reduction Type
Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)	Dimensionality Reduction Type
Singular Value Decomposition	Dimensionality Reduction Type
Neighborhood Component Analysis
Relief Algorithm

Statistical independence	Concept			refers to the idea that two random variables: X and Y are independent if knowing one does not affect the probability of the other so the joint probability of X and Y is equal to the product of their individual probabilities
Covariance Matrix	matrix used in PCA			summarizes relationships between different variables
Eigenvectors/eigenvalues in PCA	matrix attributes used in PCA			the principal components are the eigenvectors of the covariance matrix and the eigenvalues represent the variance explained by each component where the objective is to find the eigenvectors of the largest eigenvalues
Eigenvector 	Matrix attribute 	Aftering finding eigenvalues, find the eigenvector (a nonzero vector that changes scale by an eigenvalue when multiplied by a matrix A) associated with eigenvalue_i: (matrix A - eigenvalue_i * identity matrix I) * eigenvector v = 0, where if multiplying A by v scales v by a factor of the scaler lambda then v is an eigenvector of A and lambda is the corresponding eigenvalue https://en.wikipedia.org/wiki/File:Eigenvalue_equation.svg
Eigenvalue 	Matrix attribute 	Find eigenvalues (scalers that scale an eigenvector) by solving for the roots of the equation resulting from the determinant = 0: det(matrix A - eigenvalues λ * identity matrix I) = 0, then find eigenvectors for each eigenvalue
Determinant of a matrix	Matrix attribute 	The 2x2 determinant is ad - bc and the 3x3 determinant is aei + bfg + cdh - ceg - bdi - afh, or can be expressed as the sum of n! signed products of matrix entries, or a linear combination of determinants of sub-matrixes, or can be expressed with gaussian elimination using the row echelon form

z-score	Evaluation Metric	z = (x - mean)/standard deviation		Standardizes x value
t-score	Evaluation Metric	estimated value - assumed value / standard error	Uses the student's t-distribution to calculate the number of standard deviations from the mean
Confidence Interval 	Evaluation Metric 	A range of values used to estimate a population parameter like the mean, where a 95% confidence interval indicates that 95 of 100 repeated samples with the same sampling method overlap with the actual population value of the parameter, where the confidence interval reflects the reliability of the sampling method used to generate the intervals, so the true mean lies in the confidence interval 95% of the time
Bayes factors
Likelihood ratios
Expected value
Covariance
Correlation
Multicollinearity 	Data interaction concept 	Multicollinearity is where features are correlated, which reduces the robustness of a model
Variance	Evaluation Metric	variance = expected value E of (x - mean)^2 for a random variable, or the covariance of a random variable with itself, or the second cumulant of a probability distribution that generates X: E[X^2] - E[X]^2
Standard deviation	Evaluation Metric	standard deviation = square root of the variance
Standard error	Evaluation Metric	standard error = standard deviation / √n
Kurtosis	Distribution Metric			Kurtosis is a statistical measure that describes the shape of a distribution particularly the "peakedness" or "tailedness" of the data
Skewness	Distribution Metric		Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean
Euclidean distance between two points	Continuous distance metric		In one dimension d(x, Xi) = √(Σ for all n (xj - Xij)^2) and in two dimensions, the distance d(p,q) = √((p1 - q1)^2 + (p2 - q2)^2)	A commonly used distance metric for continuous variables is Euclidean distance
Manhattan distance 	Distance metric 	d(x,y) = Σ for all n |xi - yi|
Minkowski distance 	Distance metric 	d(x,y) = (Σ for all n (xi - yi)^p)^p_mean 	Represents Euclidean distance with p = 2 and Manhattan distance with p = 1
Hamming distance (overlap metric)	Discrete distance metric
Norm Vector Concept A norm is a function from a real or complex vector space to the non-negative real numbers that behaves in certain ways like the distance from the origin: it commutes with scaling, obeys a form of the triangle inequality, and zero is only at the origin. In particular, the Euclidean distance in a Euclidean space is defined by a norm on the associated Euclidean vector space, called the Euclidean norm, the 2-norm, or the magnitude or length of the vector. This norm can be defined as the "square root of the inner product of a vector with itself"
Seminorm	Vector concept	Satisfies the first two properties of a norm but can be zero for vectors other than the origin
A normed vector space Vector concept		A vector space with a specified norm

Accuracy	Classification Evaluation Metric	TP + TN/TP + TN + FP + FN	
Precision (Positive Predictive Value)	Classification Evaluation Metric	TP/TP + FP		Weights false positive predictions as important thereby analyzing the positive predictions	Does not consider True Negatives and False Negatives
Recall/sensitivity (True positive rate)	Classification Evaluation Metric	TP/TP + FN		Weights false negative predictions as important thereby analyzing the correct positive examples	Unfortunately it often emphasizes a higher false positive rate
False positive rate 	Classification Evaluation Metric 	FP/FP + TN
Specificity (True negative rate)	Classification Evaluation Metric 	TN/TN + FP
Miss rate (False negative rate)	Classification Evaluation Metric 	FN/FN + TP
False Discovery Rate 	Classification Evaluation Metric 	FP/FP + TP
False Omission Rate 	Classification Evaluation Metric 	FN/FN + TN
F1 score	Classification Evaluation Metric	2*(Precision * Recall)/(Precision + Recall)		if we increase the precision the recall decreases and vice versa so the harmonic mean of precision and recall combines these two metrics to offset that trade-off	best for uneven classes
Cross-validated F1-score	Classification Evaluation Metric			helps avoid overfitting
Mean F1 score	Classification Evaluation Metric
Confusion Matrix	Classification Evaluation Metric			Compares counts of actual vs. predicted outputs for each of N target classes to compare predictions (true/false positives and true/false negatives)
AUC (Area under the ROC curve)	Classification Evaluation Metric 		Represents degree or measure of separability (evaluates how well the model can separate the classes by analyzing the classification model at different threshold values) where a score close to 1 is ideal, 0 is the worst possible score and 0.5 indicates the model has no class separation capacity, which uses the TPR and FPR	Good for binary classification
Receiver Operating Characteristic ROC curve	Classification Evaluation Metric curve			A probability curve that compares true positive rate (recall/sensitivity) and the false positive rate and determines the model's capacity to distinguish between different classes and is used to predict the probability of the binary outcome	Good for binary classification 	Overly optimistic at estimating performance in highly skewed domains
Precision-Recall Curve	Classification Evaluation Metric		Plots the precision vs the recall for different probability thresholds, which are recommended for highly skewed domains where ROC curves can provide an overly optimistic view of the performance	Good when the positive class is rare and in highly skewed domains
PR AUC 	Classification Evaluation Metric 		The PR AUC summarizes the curve with a range of threshold values as a single score, where a high PR AUC indicates high recall and high precision, where high precision is a low false positive rate and high recall is a low false negative rate, where the AU ROC uses TPR and FPR and AUC PR uses positive predictive value PPV and TPR, where AUC PR is more useful if true negatives arent relevant or the positive class is relevant, but AUC ROC is useful where the positive and negative classes are relevant or the dataset is balanced
Silhouette Score	Clustering Evaluation Metric 	

Learning Curve	Function Type			Compares how well increasingly large training sets perform on the same test set to evaluate if the model would benefit from more data where if the validation accuracy increases with training set size it indicates underfitting and that more data would be useful and where a gap in training and test set accuracy indicates overfitting and where additional data can decrease both underfitting and overfitting

Filter Methods	Feature Selection Method			Evaluate each feature independently with target variable where features with high correlation with target variable are selected as it means this feature has some relation and can help make predictions where these methods are used in the preprocessing phase to remove irrelevant or redundant features based on statistical tests (correlation) or other criteria	Quickly evaluate features without training the model and good for removing redundant or correlated features and Filter methods are often preferred for very large datasets due to their speed	These methods don't consider feature interactions so they may miss feature combinations that improve model performance
Chi-square test	Feature Selection Filter Method			It is generally used to test the relationship between categorical variables. It compares the observed values from different attributes of the dataset to its expected value.
Fisher’s Score	Feature Selection Filter Method			Fisher's selects each feature independently according to their scores under Fisher criterion leading to an optimal set of features so a larger the Fisher’s score indicates a better selected feature
Pearson’s Correlation Coefficient	Feature Selection Filter Method			pearson's coefficient is a measure of quantifying the association between the two continuous variables and the direction of the relationship with its values ranging from -1 to 1
Variance Threshold	Feature Selection Filter Method			variance threshold is an approach where all features are removed whose variance doesn’t meet the specific threshold and by default this method removes features having zero variance and the assumption made using this method is higher variance features are likely to contain more information
Mean Absolute Difference	Feature Selection Filter Method			Mean absolute difference is a method is similar to variance threshold method but the difference is there is no square in this method
Dispersion ratio	Feature Selection Filter Method			It is defined as the ratio of the Arithmetic mean (AM) to that of Geometric mean (GM) for a given feature whose value ranges from +1 to infinity as AM ≥ GM for a given feature where a higher dispersion ratio implies a more relevant feature

Hypothesis Testing	Statistical test type			Commonly used in outlier detection, includes t-test, chi-square, ANOVA
One-sample t-test 	Statistical test 		One number predicts y
Wilcoxon signed rank Statistical test 		One number predicts the signed rank of y

Paired-sample t-test 	Statistical test 		One intercept predicts the pairwise y2 - y1 differences
Wilcoxon matched pairs 	Statistical test 		One intercept predicts the signed rank of the pairwise differences y2 - y1

Pearson correlation 	Statistical test 		One intercept plus x multiplied by a number (slope) predicts y with continuous x
Spearman correlation 	Statistical test 		One intercept plus ranked x multipled by a number (slope) predicts ranked y with continuous x

Two-sample t-test 	Statistical test 		An intercept for group 1 (plus a difference if group 2) predicts y with discrete x
Welch's t-test 	Statistical test 		An intercept for group 1 (plus a difference if group 2) predicts y, with one variance per group instead of one common variance with discrete x
Mann-Whitney U 	Statistical test 		An intercept for group 1 (plus a difference if group 2) predicts signed rank of y with discrete x
One-way ANOVA 	Statistical test 		An intercept for group 1 (plus a difference if another group) predicts y with discrete x
Kruskal-Wallis 	Statistical test 		An intercept for group 1 (plus a difference if another group) predicts rank of y with discrete x
Goodness of fit 	Statistical test 	An intercept for group 1 (plus a difference if another group) and a family like the poisson family predicts y with discrete x

One-way ANCOVA 	Statistical test 		An intercept for group 1 (plus a difference is another group) plus a slope on x predicts y, with continuous x

Two-way ANOVA 	Statistical test 		An intercept plus x1 terms and x2 terms plus x1,x2 interaction terms (changing one x term changes the other x-term in an interaction term)
Chi-square test 	Statistical test 	A log(intercept) plus log(x1) terms and log(x2) terms plus log(x1,x2) interaction terms where inputs are proportions and x is discrete and a family like the poisson family

Student's t-test 	Statistical test 	
ANOVA Statistical test 		Provides a statistical test of whether two or more population means are equal, generalizing the t-test beyond two means, and uses an f-test to compare the means of two groups by analyzing variance, comparing amount of variation between each group means to the amount of variation within each group, where if between-group variation is larger, it means the group means are likely different, based on the law of total variance in which the total variation in a dataset can be broken into components from different variation sources, and where the f-test of ANOVA follows the normality, homogeneity of variance, independence of errors and random sampling assumptions
ANCOVA 	Statistical test
MANOVA 	Statistical test
F-test 	Statistical test 	The test statistic in an f-test is the ratio of two scaled sums of squares reflecting different sources of variability, constructed so the statistic is often greater when the null hypothesis is false, where the sums of squares should be independent and should follow a scaled chi-squared distribution which is guaranteed if the data values are independent and normally distributed with a common variance, where the formula for one-way ANOVA f-test statistic is explained variance or between-group variability or Σ for all K ni * (sample mean in ith group Yi - overall mean Y)^2/(K - 1) / unexplained variance or within-group variability or Σ for all K Σ for all ni (jth observation in the ith out of K groups Yij - sample mean in ith group Yi)^2/(N - K) where N is the overall sample size, which follows the f-distribution with degrees of freedom d1 = K - 1 and d2 = N - K under the null hypothesis, which will be a large value if the between group variability is relatively large which is unlikely if the population means of the groups are equal	The f-test is a statistical test that compares variances to determine if the variances of two samples (or a ratio of variances among multiple samples) are significantly different, calculating the F statistic and checking if it follows and F distribution, where f-tests are frequently used to compare different statistical models and find the one that best describes the population the data came from, where when models are created with least squares the resulting f-tests are called exact f-tests, and is used in cases like to test the hypothesis that the means of a set of normally distributed population with the same standard deviation are equal, or the hypothesis that a regression model fits the data well, or the hypothesis that a data set in a regression analysis follows the simpler of two nested linear models, where multiple-comparison testing is done after completing an f-test if the f-test leads to a rejection of the null hypothesis and the factor being studied impacts the dependent variable. The result of the f-test is determined by comparing calculated F value and the critical F value with specific significance level like 5% where the f-table contains critical F values given degrees of freedom for the distribution of the f-statistic under assumption of a true null hypothesis, the f table is designed to determine the threshold beyond which the f statistic is expected to exceed a specific percent of the time (5%) when the null hypothesis is accurate, where if the F statistic < critical F value, the null hypothesis is accepted and there is no significant difference among sample averages and observed differences can be caused by randomness and the result is not significant, where if the F statistic > critical F value, the null hypothesis is rejected and there is significant difference among sample averages and observed differences in sample averages couldnt be reasonably caused by randomness and the result is significant, where when there are only two groups in one-way ANOVA f-test, F = t^2 where t is the student's t statistic, where another use of the f-test is to determine if a nested model is significantly better than the model it's nested in by comparing the residual sum of squares F = (RSS1 - RSS2/p2 - p1) / (RSS2/n - p2) where RSSi is replaced with the weighted sum of squared residuals if the regression model has been calculated with weights, where F will have an F distribution with (p2 - p1, n - p2) degrees of freedom and where the null hypothesis is rejected if F > critical value of the F distribution for a false-rejection probability like 0.05	The f-test is useful for multi-group comparison efficiency, clarity in variance comparison, versatility across disciplines, and allows not specifying which groups should be compared, instead comparing all groups	The f-test is sensitive to homogeneity of variance and non-normality so has alternative tests like Levene's, Bartlett's, and the Brown-Forsythe test and has limited scope to group comparisons specifically comparing variances between groups, and has interpretation challenges by not identifying specific group pairs with distinct variances and if the null hypothesis is rejected, it's not clear which groups significantly differ, nor can it be said that the group pair with the greatest mean difference is significantly different at the level alpha which the f-test was performed at
Likelihood ratio test 	Statistical test 		F is a monotone function of the likelihood ratio statistic so the F-test is a likelihood ratio test
Degrees of freedom 	Statistical test variable 		The number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary

Wrapper Method	Feature Selection Method			Greedy algorithms that train algorithms using different combinations of features and computing relations between these subset features and the target variable and add/remove features with stopping criteria being potentially whenever the model performance decreases or a specific number of features is achieved	Can lead to better model performance since they evaluate feature subsets in the context of the model and can capture feature dependencies and interactions	But are computationally more expensive than filter methods especially for large datasets
Forward selection	Feature Selection Wrapper Method			Forward selection is an iterative approach where we initially start with an empty set of features and keep adding a feature which best improves our model after each iteration where the stopping criterion is where the addition of a new variable does not improve the performance of the model
Backward elimination	Feature Selection Wrapper Method			Backward elimination is a method that is also an iterative approach where we initially start with all features and after each iteration we remove the least significant feature where the stopping criterion is where no improvement in the performance of the model is observed after the feature is removed
Recursive elimination	Feature Selection Wrapper Method			Recursive elimination is a greedy method that selects features by recursively removing the least important ones where it trains a model and ranks features based on importance and eliminates them one by one until the desired number of features is reached

Embedded Method	Feature Selection Method			Embedded methods perform feature selection during the model training process and combine the benefits of both filter and wrapper methods and include Lasso regularization, decision trees, random forests, and gradient boosting which all select important features	 where feature selection is integrated into the model training allowing the model to select the most relevant features based on the training process dynamically	More efficient than wrapper methods because the feature selection process is embedded within model training and is often more scalable than wrapper methods and Wrapper and embedded methods are better for capturing complex feature interactions	Works with a specific learning algorithm so the feature selection might not work well with other models

Loss function	Function Type			The loss function Loss(a(x),y) defines the cost of making decision a(x) when the true state is y for a single example
Cost function	Function Type	1/n * Σ for all n Loss(actual yi, predicted yi) 	An average or total of the loss function for an entire training set containing several training examples thereby quantifying the model's performance on the whole training dataset, the expected value of the loss function across the data
Objective function 	Function Type 	The objective function refers to any function being optimized/minimized/maximized during training, such as the cost function (error term), regularization terms, complexity, sparsity, or accuracy
Risk function	Function Type			The risk function combines the loss function and the decision rule and the probabilities where the risk of a decision rule a(x) is the Σ for x,y of the (expected loss(a(x),y) * probability P(x,y)) where the decision rule that minimizes the risk is selected

Cross validation	Function Type		Some part of the dataset is reserved for testing the model. There are many types of Cross-Validation out of which K Fold Cross Validation is mostly used
K-fold cross validation	Cross Validation and Generalization Method			The original dataset is divided into k subsets (folds) and this is repeated k times where 1 fold is used for testing purposes and the rest k-1 folds are used for training the model which generalizes the model well and reduces the error rate, where the performance on each fold is averaged once all folds are trained 	More cross validation folds reduces the chance of overfitting
Holdout is a simpler approach	Cross Validation and Generalization Method			the dataset is divided into ratios like 80:20 of train and test datasets and is used in neural networks and many classifiers
Adversarial Validation	Validation Type	

Hyperparameters	Variable Type
Learning Rate	Hyperparameter			Learning rate determines the jump interval for gradient descent
Batch size 	Hyperparameter
N estimators 	Random forest hyperparameter 		Number of trees in a random forest
Max depth 	Decision tree hyperparameter 		Maximum number of levels in a decision tree
Minimum samples split 	Decision tree hyperparameter 		Minimum number of data points in a node before the node is split
Minimum samples leaf 	Decision tree hyperparameter 		Minimum number of data points allowed in a leaf node
Max features 	Decision Tree hyperparameter 		Maximum number of features considered in a decision tree when splitting nodes
Bootstrap  	Decision Tree hyperparameter 		Method for sampling data point with or without replacement

Hyperparameter Tuning	Function Type			Process of optimizing model parameters other than data parameters (like learning rate	 number of trees).
Grid search	Hyperparameter Tuning 	Grid search checks every combination of hyperparameters  	Grid search is exhaustive, checking every value within specified ranges	However, grid search only checks within the specified hyperparameter ranges and is computationally expensive and can only check discrete values
Random Search	Hyperparameter Tuning 	Random search checks a random subset of combinations of hyperparameters, either sampling with or without replacement 	Random search can increase hyperparameter ranges without increasing the number of checks tested	Random search like grid search only checks within the specified hyperparameter ranges and can only check discrete values
Bayesian Optimization	Hyperparameter Tuning 	Applies informed changes to hyperparameters checked, based on information from previous checks 	Can reach the optimal configurations quicker than grid or random search 	Mostly only useful for large models since smaller models can be checked quickly with grid search
K-fold cross validation with hyperparameter tuning 	Hyperparameter Tuning 	Its useful to combine k-fold cross validation with hyperparameter search methods, performing k-fold cross validation with each hyperparameter configuration for can example with grid search

Data Augmentation	Function Type 		Can involve scaling, cropping, flipping, or rotating image data or applying changes like random noise or dropping random values to data in general 	Helps generalize and generates new data records/features based on existing data and makes it harder to memorize irrelevant information like over-specific information or noise via training examples or features
Mixup	Data Augmentation and Generalization and Regularization Method 	Randomly mixes data like by overlapping images 	Mixup increases data diversity 	However Mixup also can create blurred images 
Cutout	Data Augmentation and Generalization and Regularization Method 	Randomly removes square sections of images during training 	Cutout can remove noise, making models more robust 	However cutout can also remove important features especially in sparse images	
CutMix	Data Augmentation and Generalization and Regularization Method 	Randomly mixes a subset of input images and mixes labels according to the proportion of the image subsets 	Cutmix increases data diversity 	However cutmix also can create unrealistic images and can remove important features especially in sparse images

Model Interpretability & Explainability	Function Type			Understanding how a model makes decisions
SHAP (shapley additive explanations)	Model Interpretability	The shapley value for feature i is: φᵢ = Σ [|S|!(M-|S|-1)!/M!] × [f(S ∪ {i}) – f(S)] where S represents all possible subsets of features excluding feature i, and f represents the model’s prediction function 	SHAP calculates feature importance by considering all possible combinations of features and measuring how much each feature contributes to the difference between the current and average prediction, ensuring that the sum of all SHAP values equals the difference between the individual and average prediction, where features that contribute equally have equal SHAP values, positively/negatively contributing features have positive/negative values, and features that dont contribute have zero SHAP values and SHAP values can be computed by summing values from individual components 	SHAP is rigorous, has consistent explanations, integrates global and local info, is efficient by optimizing for model types, enables rich visualizations and is additive and is better for complex models 	However SHAP is computationally complex, has errors from approximations, is sensitive to the choice of baseline value, doesnt handle correlated features well, and requires careful selection of datasets
LIME (local interpretable model-agnostic explanations)	Model Interpretability 	Lime explains individual predictions by learning an interpretable model locally around the prediction by perturbing relevant input features around a specific instance and observing how predictions change, first perturbing data, then collecting predictions, then assigning higher weights to closer samples, then training a local model using algorithms like linear regression, then generates explanations based on coefficients of the local model 		LIME is useful bc of its model agnosticism, instance-specific explanations, intuitive output, flexible data type handling, and local fidelity, accurately approximating model behavior in the local neighborhood of the explained instance 	However LIME is unstable/volatile/inconsistent with small input changes leading to very different explanations due to random sampling and is dependent on sampling strategy used for perturbations, uses and reflects limited global info, is computationally expensive and is sensitive to hyperparameters and is better for simpler models
Feature importance plots	Model Interpretability
DeepLIFT	Deep learning interpretability method
Grad-CAM	Deep learning interpretability method
Integrated Gradients	Deep learning interpretability method

Activation functions	Function Type			Specifies whether a neural network node should be activated thereby allowing that set of weights to contribute to the model
Step	Activation function 	1 if x >=0 or 0 if x < 0
Sigmoid 	Activation Function and Squashing Function 	1/(1 + e^-x)	Squashing functions limit output to a range of 0 to 1 which is useful when predicting probabilities	The sigmoid function is useful for binary classification problems because its outputs are values between 0 and 1 and is non-linear so it improves on the step function 	However the sigmoid function is less useful for multi-class classification bc it doesnt normalize outputs to sum to 1 across multiple classes and has the vanishing gradient problem with large or small inputs bc the gradient approaches zero for large or small values
Tanh	Activation function 	tanh(x) = (e^x - e^-x)/(e^x + e^-x) = sinh(x)/cosh(x)	Tanh is an s-curve similar to sigmoid but has outputs ranging from -1 to 1 	Tanh introduces non-linearity which allows learning complex patterns and has output centered around zero which leads to more efficient training and faster convergence bc the mean of the output is closer to zero so its useful for data already centered around zero or where negative input values are significant and should be retained and for shallow neural networks and tanh helps reduce the vanishing gradient problem compared to sigmoid activation bc the gradient of the tanh function is usually higher than the gradient of the sigmoid which enables better weight updates during backpropagation and tanh is differentiable so it can be used with gradient descent 	Like sigmoid, tanh is also not typically used for multi-class problems bc it doesnt handle probability distributions and tanh can also have the vanishing gradient problem with large or small inputs bc the gradient approaches zero for large or small values, just like the sigmoid function and tanh is sensitive to outliers in input
ReLU	Activation function 	f(x) = max(0,x) 		ReLU is often used in hidden layers of deep networks bc of its simplicity and efficiency and doesnt have the vanishing gradient problem 	However ReLU doesnt convert logits to probabilities so its not used in output layers in classification tasks and creates dead neurons from receiving only negative inputs which stops learning for those neurons and ReLU doenst treat negative and positive values equally which can slow down learning and exploding activations can become too large for large positive inputs, which are problems leaky ReLU is designed to solve
Leaky ReLU 	Activation function 	x if x > 0 and 0.01 * x if x <= 0 	Leaky ReLU is designed to fix the problem of dead neurons so it doesnt return a zero value for negative inputs so neurons wont become inactive during training 	Leaky ReLU prevents dead neurons by allowing a small nonzero gradient for negative inputs and improves gradient flow during backpropagation and is faster and more stable compared to ReLU and is useful in deep networks where ReLU can have problems
Softmax 	Activation function 	softmax(zi) = K * e^zi / ∑ for all K * e^zj, where zi is the logit (output of previous layer in network) for the ith class, K is the number of classes, e^zi is the exponential of the logit and ∑ for all K e^zj is the sum of exponentials across all classes = number of classes * exponential of the logit / sum of exponentials across all classes, where the e^zi term increases the difference between logits so that even small increases in logits lead to higher probabilities, while small logits have near-zero probabilities, and where the sum of the exponentials normalizes the values into probabilities, and where softmax and cross-entropy loss are often used together bc the cross-entropy loss can compare the predicted probability distribution resulting from softmax with the true label and cross-entropy loss can penalize the network if the predicted probability for the correct class is low  		Softmax helps transform logits (raw prediction scores) into probabilities of each class being the correct prediction, these probabilities being distributed across classes so that their sum is 1, and softmax is typically applied in the final output layer and is useful for multi-class classification tasks and softmax is differentiable so can be integrated into the backpropagation algorithm involving gradient descent 	However softmax is sensitive to outliers and noise bc it increases differences between logits so large differences like those found in outliers and noise can determine the output and small probabilities can cause small gradients during backpropagation which slows down learning and softmax can assign high probabilities to incorrect classes and requires exponentiation and normalization which is computationally expensive and is not suited to cases where data can belong to multiple classes

Batch normalization (BatchNorm)	Reducing overfitting and regularization and layer input normalization technique		Addresses the problem of internal covariate shift in neural networks by normalizing the data in each mini-batch, by calculating the mean and variance in a batch and then adjusting values to have a similar range, then scaling and shifting the values so the model learns effectively and the inputs to each layer are in a stable range even if outputs of earlier layers change during training, where the normalized activation xi = (non-normalized xi - the mean) /	√ (square root of variance + error term to avoid division by zero), then the normalized activations are scaled by a learnable parameter gamma and shifted by another learnable parameter beta yi = gamma * normalized xi + beta 	Speeds up and stabilizes training and generalizes and allows using higher learning rates and helps avoid vanishing/exploding gradients and can act like a regularizer thereby reducing the need for dropout, allows faster convergence during training
Layer normalization (LayerNorm)	Reducing overfitting and regularization and layer input normalization technique			Stabilizes training and regularizes and generalizes
Weight normalization	Reducing overfitting and normalization			normalizes the model weights instead of layer inputs	generalizes and regularizes indirectly
Feature normalization 	Normalization 		Feature normalization is required for L1 and L2 regularization but not required for linear regression without regularization, and is useful where additional stability is required

Internal covariate shift 	Problem Type 		As input data propagates through the neural network, the distribution of each layer's inputs changes which can slow down the training process, which batch normalization is designed to solve by normalizing inputs of each layer
Data/Covariate shift	Problem Type			Distribution of x input data changes so detect covariate shift with adversarial validation and fix covariate shift with importance weighting (assign different weights to training examples to emphasize certain examples during training to increase the weight of examples likely to be in the test distribution)
Label/Prior probability shift	Problem Type			y class label distribution changes so update the model by adjusting the weights of the weighted loss function according to the new distribution if the new distribution of labels is known which is a type of importance weighting that incentivizes prioritizing certain classes that have become more or less common in the new data
Concept/Conditional shift	Problem Type			the conditional p(y|x) distribution has changed which requires continuous monitoring and model retraining
Domain/Joint shift	Problem Type			p(x) and p(y|x) both change so its a combination of covariate and concept drift and implies label p(y) shift as well unless the change in p(x) offsets the change in p(y|x) which is detected by monitoring model performance and data statistics and is fixed by collecting more labeled data from the target domain and retraining or adapting the model

Overfitting	Problem Type			occurs when the model fits the training data too closely indicating high variance (as opposed to high bias) and learns noise and outliers rather than the pattern so performs well on training data but not on new or test data and is fixed by data augmentation like Mixup/Cutout/CutMix and collecting more data after plotting a learning curve to detect if more data is beneficial and use self-supervised learning to pretrain on large unlabeled datasets to reduce overfitting on small datasets and use transfer learning from highly relevant large labeled datasets and use few-shot learning if additiona labeled data is not feasible and use feature engineering and normalization and include adversarial examples and label/feature noise and label smoothing and smaller batch sizes and regularization techniques like dropout and weight decay and decreasing model size and capacity and building ensemble models
Label smoothing
Regularization	Function Type			penalizes complexity by adding a penalty term representing the weights size to the optimizer or the loss function that is minimized during training
L2 Regularization	Regularization and Generalization	RegularizedLoss = Loss + λ/n * ∑ w2	λ is a hyperparameter that controls the regularization strength	penalizes complexity by adding a penalty term of the squared sum of the weights to the loss function that is minimized during training where the optimizer minimizes the modified loss during backpropagation now with the additional penalty term which leads to smaller model weights 
Dropout	Reducing overfitting			reduces overfitting by randomly setting some activations of hidden units to zero during training so those neurons cant be relied on and more neurons are used to create multiple independent representations of the same data
Weight Decay	Reducing overfitting			similar to L2 regularization but is applied to the optimizer directly rather than modifying the loss function which has the same effect as L2 regularization
Early Stopping	Reducing overfitting			monitor performance on a validation set during training and stop training when performance on the validation set starts to decline (when the validation and training set performance are the most similar which is the point with the least overfitting and which is a good point for early stopping of training iterations)
Smaller models	Reducing overfitting			because the smaller the number of model parameters the smaller its capacity to overfit to noise choosing smaller models with reducing layer count/width and pruning and knowledge distillation is good for reducing overfitting		however double descent and grokking indicate that larger overparameterized models have good generalization if they are trained beyond the point of overfitting
Pruning	Reducing overfitting with smaller models			iterative pruning trains a large model and then iteratively removes parameters of the model and retraining it so that it maintains the original performance	improves generalization of the training process as it involves more extended training periods and a replay of learning rate schedules
Knowledge Distillation	Reducing overfitting with smaller models			knowledge distillation transfers knowledge from a supervised teacher model (trained with cross-entropy loss between predicted and actual outputs) to a smaller student model (which is trained on the same dataset with the objective of minimizing cross entropy between predicted and actual outputs as well as the difference between student and teacher outputs measured with Kullback-Leibler divergence)
Double Descent	Generalization Observation			models with small or very large parameter counts have good generalization performance while models with parameter counts equal to number of training data points have poor generalization performance
Grokking	Generalization Observation			as the size of a dataset decreases the need for optimization increases
Large parameter count requirements	Generalization Observation	models with a larger number of parameters require more training data to generalize well.  		
Ensemble models	Reducing overfitting			combine predictions from multiple models to improve the overall prediction performance like in random forests and gradient boosting	generalize well	increased computational cost so neural networks are less suitable for ensemble methods
Majority voting	Ensemble method			train k different classifiers and collect the predicted class label from each of these k models for a given input and return the most frequent class label as the prediction where ties are resolved using a confidence score or randomly picking a label or picking the class label with the lowest index	can combine different models
Stacking	Generalization and Ensemble method for Classification/Regression			a more advanced variant of majority voting that trains a new meta model to combine predictions of other models	can combine different models like a SVM	 MLP and a KNN
K-fold ensemble model	Ensemble method			after building models using k-fold cross-validation	 compute the average performance across all k iterations to estimate the overall performance of the model then combine the individual k models as an ensemble (or train the model on the entire training dataset rather than the k k - 1 subsets) as a majority vote classifier or a stacked ensemble model
Skip-connections	Reducing overfitting with model modifications			used in residual networks
Look-ahead optimizers	Reducing overfitting with model modifications
Stochastic weight averaging	Reducing overfitting with model modifications
Multitask learning	Reducing overfitting with model modifications
Snapshot ensemble	Reducing overfitting with model modifications

Curse of dimensionality	Problem Type 		Phenomena that occur in high dimensional spaces related to increases in sparsity with increases in dimension, resulting in increased data requirements and where high dimensional spaces make all data seem different resulting in difficulty identifying similarities to organize it
Vanishing Gradient problem	Problem Type		Phenomenon where early layers have smaller weights than later layers due to increased numbers of multiplications during backpropagation, which can introduce instability in training or slow/halt training, occurring mostly in deep networks and specifically recurrent neural networks which use many layers	A solution for RNNs could be a LSTM network, gradient clipping to restrict the gradients within a radius, batch normalization, a multi-level hierarchy of networks pre-trained one level at a time, deep belief networks, residual connections, the ReLU activation function, or weight initialization where distribution of initial weights vary according to activation function used
Bias vs. variance trade-off	Problem type 		In supervised learning, bias error occurs by approximating a simpler model, where the learning algorithm has incorrect assumptions causing it to miss general patterns in data by underfitting, where variance error results from sensitivity to small changes in inputs which can model the noise rather than the general patterns in data by overfitting to noise, where accuracy is a way to quantify bias which can be improved by selecting local info and precision quantifies variance which can be improved by selecting info from a larger space compared to bias/accuracy and which can be optimized or smoothed with regularization to remove excess neighboring info
Data leakage	Problem Type			Data leakage happens when a model has access to info during training that it wont have access to when making predictions
Model collapse 	Problem Type 		Model collapse refers to the phenomenon where a model's performance degrades due to errors from training on uncurated synthetic data output by another model, occurring as a result of learning errors, sampling errors, or functional approximation errors
Bayes error rate 	Problem Type Concept	Bayes error (reproducible error or the minimum possible error) is a combination of latent variables, unmeasurable features, and noise
Bayesian vs. frequentist choice 	Problem type 	The result of bayesian approach can be a probability distribution for what is know about parameters given experiment results, where the result of a frequentist approach is either a decision from a significance test or a confidence interval, and where in a frequentist approach, unknown parameters are usually considered fixed rather than being random variables where a bayesian approach allows probabilities to be associated with unknown parameters, where these probabilities can have a frequency probability interpretation as well as a bayesian interpretation, where the bayesian approach allows these probabilities to represent a belief that given values of the parameter are true, and where bayesian probability treats probability as "certainty", where frequentist treats probability as the "limit of its relative frequency of occurrences of an event in an infinite number of experiment reppetitions" and frequentist inference violates the likelihood principle and where frequentist inference identifies different tail ends of distributions as indicating different levels of statistical significance of the same data with different assumed probability distributions, a difference that does not occur in bayesian inference

Binomial distribution 	Probability Distribution	The probability of getting exactly k successes in n independent Bernoulli trials (with the same rate p) is given by the probability mass function (n choose k) * p^k * (1 - p)^(n-k) with mean np and variance np * (1 - p) 	The binomial distribution is used to calculate probabilities for a process where only one of two possible outcomes may occur on each trial, such as coin tosses, and is a bernoulli distribution when n = 1
Bernoulli Distribution 	Probability Distribution 
Hypergeometric distribution Probability Distribution	The pdf is px(k) = PR(X = k) = (K choose k) * (N - K choose n - k)/(N choose n) with mean n * (K/N) and variance n * (K/N) * (N - K/N) * (N - n/N - 1)		The hypergeometric distribution is used to find the probability of k successes in n draws without replacement where both the hypergeometric distribution and the binomial distribution describe the number of times an event occurs in a fixed number of trials and the probability remains the same for every trial for the binomial distribution and in contrast in the hypergeometric distribution each trial changes the probability for each subsequent trial because there is no replacement
Poisson distribution 	Probability Distribution	The pdf is f(k|lambda) = ((e^-lambda) * (lambda^k))/k! where the mean is lambda and the variance is lambda	The poisson distribution can be used to measure the probability that a given number of events will occur during a given time frame such as the count of library book checkouts per hour where Poisson regression is used when the target variable represents count data (positive integers) and the data is Poisson distributed which means that the mean and variance are roughly the same, and for large means we can use a normal distribution to approximate a Poisson distribution
Ordinal regression 	Regression type 	The cumulative probability function is Pr(y <=i | x) = inverse link function σ(threshold omega_i - w dot x) where σ the inverse link function can be the logistic function or an exponential function for example σ(omega_i - w dot x) = 1/1 + e ^ -(omega_i - w dot x) 	Ordinal data is a subcategory of categorical data where the categories have a natural order such as 1 < 2 < 3 where ordinal regression does not make any assumptions about the distance between the ordered categories
Geometric distribution 	Probability Distribution	The pdf indicates the probability that the kth trial is the first success is Pr(x = k) = (1 - p)^(k -1) * p where the mean is 1/p and the variance is (1 - p)/p^2		The geometric distribution can be used to determine the probability that a specified number of trials will take place before the first success occurs
Exponential Distribution 	Probability Distribution 	The pdf is f(x|lambda) = lambda * e^(-lambda * x) where the mean is 1/lambda and the variance is 1/(lambda^2)
Uniform Distribution 	Probability Distribution 	The pdf is f(x|a,b) = 1/(b-a) for x between a and b and 0 for x < a or x > b where the mean is (a + b)/2 and the variance is (b - a)^2/12
Gaussian Distribution 	Probability Distribution 	The pdf is f(x|mean, variance) = (1/√2 * pi * variance) * e ^ (-1/2 * (x - mean/standard deviation)^2)
Beta Distribution 	Probability Distribution 
Gamma Distribution 	Probability Distribution 
F distribution 	Probability Distribution 	The f-distribution is the distribution of X = (U1/d1) / (U2/d2) where U1 and U2 are independent random variables with chi-square distributions with degrees of freedom d1 and d2 where the pdf is √((d1 * x)^d1 * (d2^d2) / (d1 * x + d2)^(d1 + d2)) / x * beta function B(d1/2, d2/2) with mean d2/(d2 - 2) for d2 > 2 and variance is (2 * d2^2 * (d1 + d2 - 2)) / (d1 * (d2 - 2)^2 * (d2 - 4)) for d2 > 4
Student's t distribution 	Probability Distribution
Chi-squared distribution 	Probability Distribution
Pearson distribution
Laplace distribution
Fisher's z-distribution


SMOTE
Connected Components	Graph Algorithms
Shortest Path	Graph Algorithms
Pagerank	Graph Algorithms
Centrality Measures	Graph Algorithms
StandardScaler
SARIMA
ARIMA
Bidirectional Encoder Representations from Transformers (BERT)
H2O AutoML
Manifold Learning
FastAI
Retrieval Augmented Generation
Attention


Sources
https://medium.com/coders-camp/40-machine-learning-algorithms-with-python-3defd764b961
https://sebastianraschka.com/books/ml-q-and-ai/#table-of-contents
https://github.com/Tanu-N-Prabhu/Python/tree/master/Machine%20Learning%20Interview%20Prep%20Questions
https://machinelearningmastery.com/
https://www.cs.jhu.edu/~ayuille/courses/Stat161-261-Spring14/RevisedLectureNotes2.pdf#:~:text=Hence%20the%20form%20of%20the%20decision%20rule%20%28i.e.	is%20at%20xML%20%3D%201%3D2%28%201%20%2B%201%29.
https://www.geeksforgeeks.org/machine-learning/decision-tree/
https://mljourney.com/ml-model-explainability-shap-vs-lime/
https://www.markovml.com/blog/lime-vs-shap
https://blog.dailydoseofds.com/p/grid-search-vs-random-search-vs-bayesian
https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74/
https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf
https://github.com/alexeygrigorev/data-science-interviews/blob/master/theory.md