Object	Type	Related Math Function	Math Function Explanation 	Advantages	Disadvantages	Supervised or Unsupervised	Related Functions such as Loss Function
Bias 	Prediction concept 		Bias is a difference between predicted and actual values, such as the difference between the true value and the mean prediction; measures how different predictions are from true values in general if the model is trained on different datasets, as in the systematic error not due to randomness
Variance 	Prediction concept 	Variance is the consistency/variability in a model prediction for classifying a training example if the model is trained multiple times like on different subsets of the training data set, which indicates sensitivity to randomness in the training data set
Weight 	Prediction concept 		Weight is the change in the output per a unit change in the predictor, which represents the importance of a variable in the predicted output if the variables are normalized
Π	Math Function 	Product of all inputs
Σ	Math Function	Sum of all inputs
||w|| 	Math Function 	||w|| is the squared root of the sum of w^2 for all vectors w (||w|| is the length of vector w)
Log 	Math Function 	Standardizes values according to a base, reduces possibility of numerical underflow which can occur if inputs are small, converts products into sums via the addition trick which can make it easier to find the derivative of a function
Odds function 	Probability function 	odds of an event happening = probability of event / 1 - probability of event
Logit Function 	Probability Function 	natural log (probability of event / 1 - probability of event) 	Takes input values from 0 to 1 and transforms them into real number values
Logistic Regression Likelihood function 	Function Type 	P(y|x,w) = Π (activation sigmoid(z) output as in conditional probability) ^ yi * (1 - activation sigmoid(z) output as in conditional probability) ^ (1 - yi) 	This likelihood function is maximized in logistic regression, where the activation sigmoid(z) output is the conditional probability that y has a value given x,w			Assumes examples are independent of each other
Log-likelihood function 	Function Type 	log(P(y|x,w)) = Σ yi * log(activation sigmoid(zi) output as in conditional probability) * (1 - yi) * log(1 - activation sigmoid(zi) output as in conditional probability) 	Applies the log to the likelihood function
Log-likelihood function as a cost function 	Function Usage 	Minimizing the negative of the log-likelihood function applies the log-likelihood function as a cost function, where the first term -y * log(activation sigmoid output) is 0 if y = 0 and the second term (1 - y) * log (1 - activation sigmoid output) is zero if y = 1, so the cost function becomes -log(activation sigmoid output) if y = 1 and -log(1 - activation sigmoid output) if y = 0, so maximizing the log-likelihood function is equal to minimizing the cost function
Sigmoid Function 	Function Type 	1/1 + e^-z, where z is the weight vector dot x 	Once the sigmoid function is applied as an activation function, the sigmoid function output is interpreted as the probability of y being a specific class given x and w

Linear Regression	Regression	y = ax + b		Objective is to minimize mean squared error to minimize the difference between predicted and actual values, fits a straight line or hyperplane in higher dimensions to the data	Simple and interpretable and good for predicting continuous values and data with linear relationships	Assumes linear relationship between features and target variable, assumes additivity so the impact of one feature on the target isnt dependent on values of other features, assumes features are no correlated (no collinearity), assumes errors are independently and identically normally distributed so confidence intervals arent too wide/narrow and no correlation between errors and constant variance (homoscedasticity) of errors and low multicollinearity between features and independence and normal distribution of errors and few outliers	Supervised		Mean Squared Error
Homoscedasticity Data distribution concept 		Homoscedasticity is constant variance of y as x increases (heteroscedasticity is changing variance of y as x increases)	
Multiple Linear Regression 	Regression 	y = B0 + B1 * x1 + .... + Bn * xN 

Logistic Regression	Classification	p(y = 1|X) = 1/(1 + e^-(w dot x + b))	Logistic regression tries to maximize the conditional likelihoods of the training data, which makes it more susceptible to outliers than linear SVMs, where SVMs prioritize points near the decision boundary (the support vectors), and logistic regression uses the sigmoid function 1/(1 + e^z) where z is (w dot x + b) to output values between 0 and 1 by identifying a classification boundary where the output is interpreted as a probability of a class. The cost function of logistic regression Σ (-yi * log(activation sigmoid output) - (1 - yi) * log (1 - activation sigmoid output)) can be regularized with the term (lambda/2) * ||w||^2 which will shrink the weights during training, increasing the regularization strength by increasing lambda, where lambda is the inverse of the C parameter, and decreasing C increases the bias and decreases the variance of the model, logistic regression is useful for online learning using SGD and predicts the probability of an event	Best for binary classification and problems where probabilities are required as it has probabilistic output	Struggles with non-linear data	Supervised		Binary Cross Entropy Loss or Log/Logistic Loss
Polynomial Regression	Regression	y = a0 + a1x + a2x2 + … + anxn + ϵ	ϵ is the error term representing deviations between predicted and actual values	used when the input/output relationship is nonlinear and can be approximated by a polynomial
Ordinary Least Squares Regression (OLSR) 	Regression
Stepwise Regression	Regression
Multivariate Adaptive Regression Splines (MARS)	Regression
Locally Estimated Scatterplot Smoothing (LOESS)	Regression
Ordinal regression 	Regression type 	The cumulative probability function is Pr(y <=i | x) = inverse link function σ(threshold omega_i - w dot x) where σ the inverse link function can be the logistic function or an exponential function for example σ(omega_i - w dot x) = 1/1 + e ^ -(omega_i - w dot x) 	Ordinal regression does not make any assumptions about the distance between the ordered categories
Ordinal data 	Data Type 	Ordinal data is a subcategory of categorical data where the categories have a natural order such as 1 < 2 < 3 
Nominal data 	Data Type 	Nominal data is categorical data with no sort/order

Hard-margin Support Vector Machine	Classification and Instance-based Algorithm 	The hard-margin SVM objective function is to minimize (1/2)||w||² subject to yᵢ(w dot xᵢ - b) >= 1 with normal vector w	Maximizes the margin between classes with a linear decision boundary, finding the hyperplane that best separates classes with the maximum margin, using kernels to handle non-linear data and using support vectors to define the hyperplane. The difference between the positive and negative hyperplane equations (w0 + w dot x positive = 1 and w0 + w dot x negative = -1) normalized by the length of the vector w, ||w||, is 2/||w||, although it's easier to minimize the reciprocal term 1/2 * ||w||^2. The negative examples should fall below the negative hyperplane and the positive above the positive hyperplane, which is the condition yi (w dot xi + b) >= 1	Robust against overfitting, works in high-dimensions and with complex patterns where clear margins of separation exist and particularly good for binary classification	SVMs have many parameters to tune and are less scalable			Supervised		Hinge Loss
Soft-margin (with slack) Support Vector Machine	Classification and Instance-based Algorithm 	Minimize 1/2 * ||w||^2  + C * Σ slack term i, subject to yᵢ(w dot xᵢ - b)) >= 1 - slack term i with normal vector w, where C represents the trade-off between increasing the margin and ensuring xi falls on the right side of the margin, which will behave similarly to the hard-margin SVM for large values of C if the input data are linearly separable. The slack variable avoids the linear assumption for nonlinearly separable data, which allows the convergence of the optimization in case of misclassifications, if cost penalizations are appropriate, where large values of C correspond to large error penalties for misclassification errors, so C controls the width of the margin (large C = small margin)	Works in high-dimensions and particularly good for binary classification and useful when data is not linearly separable	Hard to tune and less scalable	Supervised		Hinge Loss
Support Vector Regression (SVR)	Regression and Instance-based Algorithm	Minimize (1/2)||w||² subject to |yᵢ - inner_product(w dot xᵢ) - intercept b) <= ϵ		Uses margin-based optimization like SVM

Kernel Trick	Function Type			Feature mapping function that represents data through a set of pairwise similarity comparisons between original data points with a kernel function which transforms them into coordinates in a higher-dimensional feature space with linearly separable features, in other words it defines the "high dimensional distance between points = the inner product of their coordinates" and computes these "distances between points = the inner product of lower dimensional coordinates" using a kernel function that only uses lower dimensional terms like the kernel k(i,j) = i * j ||i^2|| * ||j^2|| that is equivalent to i as in (xi, yi, xi^2 + yi^2) * j as in (xj, yj, xj^2 + yj^2), where (x^2 + y^2) is the function creating the linear boundary for some non-linearly separable data set like concentric circle categories, and the kernel evaluates the inner/dot product of data points in the higher dimensional space without transforming to that higher dimensional space, where the kernel function is a simplified variant of the inner product function that only uses known terms or trivial transformations of known terms, so the kernel function is a similarity function between a pair of examples that calculates a higher dimensional similarity (like distance) calculated using the inner product by applying a kernel function to the lower dimensional inner product, which is possible with some kernel functions

Gaussian Radial Basis Function Kernel		k(xi, xj) = exp(-γ||xi - xj||^2) where γ = 1/(2 * sigma ^2) which is a free parameter to be optimized, and where the minus sign changes the distance metric into a similarity score and the exponent ensures the similarity score is between 0 and 1, and where the gamma γ parameter determines how much influence the training examples have, so a high gamma can increase variance and overfitting
Polynomial Kernel		k(xi, xj) = (xi transpose dot xj + threshold omega) ^ p power parameter to be specified
Sigmoid kernel (hyperbolic tangent)		k(xi, xj) = tanh(mu * xi transpose dot xj + threshold omega)

Parametric models 	Algorithm Type 	Parametric models estimate parameters from the data set to learn a function that can classify new points without requiring the original training data set (perceptron, logistic regression, linear SVM)
Non-parametric models 	Algorithm Type 	Non-parametric models can't be described with a fixed set of parameters, so their number of parameters grows with the data set (decision tree/random forest, kernel SVM, K nearest neighbors)
Instance-based learning 	Algorithm Type 	Models learn the training data set, where lazy learning is a special case of instance-based learning that involves no cost during learning

K-Nearest Neighbors	Classification and Regression and Instance-based Algorithm	A lazy learning algorithm (learns the dataset instead of an estimator) that uses a distance metric of euclidean distance d(x, Xi) = √(Σ for all n (xj - Xij)^2) for continuous neighbor selection, and uses overlap distance for discrete data or pearson's correlation coefficient, where the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis where it classifies based on nearby neighbors with voronoi-like regions formed by voting among nearest points, classifying new points based on the majority class of the k closest data points using distance metrics to nearest neighbors (majority vote from k nearest neighbors for classification, average value of k nearest neighbors for regression) by finding the k closest samples and predicting the label or value, where the value of k is the number of neighbors the algorithm checks before making a prediction, where a larger k can make predictions more stable but if k is too large the model can become too simple and miss important patterns (underfitting) so cross-validation and the elbow method are useful for selecting k and k should be odd to avoid ties, where the algorithm involves 1. selecting k and a distance metric, 2. finding the k nearest neighbors of the record to classify, 3. assigning the class label first by majority vote, then closer distance, then first label in the training data set to resolve ties, where k determines over vs. underfitting, where knn's compute class probabilities by aggregating the class labels of the k nearest neighbors to return the normalized class label frequencies 	No training phase and non-parametric (only requires k and a distance metric) and effective for well-separated clusters and works well for classification and regression and learns as new data is collected 	Slow at prediction time, struggles with high numbers of features and can overfit when the data is high dimensional bc of the curse of dimensionality or not sanitized, complexity grows linearly with the number of data points unless there are few features and its implemented with k-d trees, and cant discard data points so storage is an issue, need to use feature selection and dimensionality reduction techniques instead of regularization which is not applicable to k-nn to avoid the curse of dimensionality 	Supervised		No loss
Elbow Method 	k-nearest neighbors function 		The graph of the error rate or accuracy for different k values indicates a drop in error as k increases at first, where after a certain point the error stops decreasing quickly, so the point where the curve changes direction and looks like an elbow is usually the best choice for k

Learning Vector Quantization 	Instance-based Algorithm 
Self-Organizing Map (SOM) 	Instance-based Algorithm
Locally Weighted Learning (LWL) 	Instance-based Algorithm

Naive Bayes	Classification and Bayesian Algorithm	The naive Bayes classifier predicted y = argmax p(Ck) Π for all features n p(xi | Ck) combines the probability model with a decision rule where the naive bayes conditional probability model is P(y|x) ∝ P(y) * P(xᵢ|y)/P(x) where the decision boundary indicates the different likelihoods P(x|y = 0) and P(x|y = 1) and where it uses probabilistic regions based on feature likelihoods, applying Bayes rule for class probabilities	Fast and works well with independent features and works well for text classification and high-dimensional input and only requires a small amount of training data to estimate the parameters necessary for classification and good at scaling	Assumes feature independence and information about the target class provided by each variable is unrelated to the information from the others variables with no information shared between the predictors and bad at quantifying uncertainty compared to logistic regression and is outperformed by other approaches like boosted trees or random forests	Supervised		No loss function (probability-based)
Gaussian Naive Bayes	Classification and Bayesian Algorithm	P(x = v|Class k Ck) = 1/√(2 * pi * std dev for k squared) * e ^ -(v - mean) squared/2 * (std dev for k squared)
Multinomial Naive Bayes 	Classification and Bayesian Algorithm
Averaged One-Dependence Estimators (AODE) 	Bayesian Algorithm 
Bayesian Belief Network 	Bayesian Network Type
Bayesian Algor Network (BBN) Bayesian Network Type
Bayesian Network (BN) 	Bayesian Network Type

Class Prior	Probability function	the expected frequency or prior for a given class = number of samples in that class / total number of samples or the class prior can be 1/k if classes are equally probable where prior means before observation so without the influence of other variables (independent probability) where the posterior is with the influence of other variables (conditional probability on some other variable)
Posterior	Bayesian probability function	posterior = prior * likelihood/evidence or probability (Class k | x) = P(Class k) * P(x|Class k)/P(x) or P(b|a) = P(b) * P(a|b)/P(a)
Likelihood function	Probability function	P(x|Y) 	Explains how well a statistical model explains observed data by calculating the probability of seeing that data under different parameter values of the model, constructed from the join probability distribution of the random variable that presumably generated the observations where in contrast in bayesian statistics the estimate of interest is the converse of the likelihood (the posterior probability of the parameter given the observed data, calculated with bayes' rule)
Decision boundary/surface	Decision function			For example comparing the likelihoods P(x|y = 0) and P(x|y = 1) can help identify the decision boundary separating negative and positive predictions between two distributions https://en.wikipedia.org/wiki/Naive_Bayes_classifier#/media/File:ROC_curves.svg where the decision boundary is a line if the distributions are both gaussian with equal covariance and the decision boundary is a curved surface if theyre gaussian with different covariance
Decision Threshold,Probability concept		the decision boundary position (the threshold) is determined by the prior + the loss function and the decision boundary structure is determined by the likelihood function where the decision boundary position for two gaussian distributed classes would be where the distributions intersect at 2⃗x·(µ1−µ−1) = |µ1|^2 −|µ2|^2
Decision rule	Algorithm sub-function	A decision rule takes input x and outputs a decision for example: a decision rule such as y MLE (x) = 1 if x > x MLE, otherwise y MLE (x) = -1
Decision	Probability concept		The Bayes decision is the MAP estimator if the loss function penalizes all errors equally and if the prior is also uniform then the Bayes decision is the MLE
Argmax	Distribution function	the argmax is the input point that maximizes the output point
Log-likelihood ratio	Probability function	for binary classification, the decision depends on the log-likelihood ratio and the threshold T for example: if log(Px|y=1)/(Px|y=-1) > T then y = 1 otherwise y = -1
P-value 	Probability concept 	The p-value is calculated as p = Pr(T >= t | H0) for a one-sided right-tail test-statistic distribution, p = Pr(T <= t| H0) for a one-sided left-tail test-statistic distribution, and p = 2 * min (Pr(T >= t | H0), Pr(T <= t | H0)) for a two-sided test statistic distribution where if the distribution of T is symmetric around zero then p = Pr(|T| >= |t| | H0)	The p-value is the probability of obtaining test results at least as extreme as the result actually observed, assuming the null hypothesis is correct, so a small p-value means such an extreme observed outcome would be unlikely under the null hypothesis, the p-value quantifying the statistical significance of a result, where the result is the observed value of the chosen statistic T, where smaller p-values are evidence against the null hypothesis bc the lower the p-value, the lower the probability of getting that result if the null hypothesis was true, where the distribution of significant p-values is a p-curve which can be used to detect publication bias or p-hacking and has alternative statistics like confidence intervals, likelihood ratios, and bayes factors, and the p-value indicates whether the data falls within the range of what would happen 95% of the time if the null hypothesis is true and if the p-value is greater than the alpha level 0.05
N choose k	Probability Function	n choose k = n!/(k! * (n-k)!)
Likelihood principle 	Statistical concept 		The likelihood principle indicates that given a statistical model, all the evidence in a sample relevant to model parameters is contained in the likelihood function
Likelihood function 	Statistical concept 	The likelihood function L(omega | x) = fX(x | omega) which measures how likely a value of omega is if we know that X has the value x
Coherence
Maximum Likelihood Estimator (MLE)	Probability function	y MLE (x) = argmax P(x|y)
Maximum A posteriori (MAP) Estimator	Probability function	y MAP (x) = argmax P(y|x)
Ordinary Least Squares Estimator
Bayesian Estimator

Data and parameter preprocessing	Algorithm Type	Standardize data with scaling then graph data then create relevant features then identify importance/relevance of features then filter features by importance/relevance then select model then select model hyperparameters
Data postprocessing	Algorithm Type	Analyze scores related to model then create a decision surface, then analyze for errors in model preprocessing/training, then analyze for input/concept/output/domain drift

Feature Scaling 	Feature Engineering Method 	Feature scaling is irrelevant for decision trees/random forests, where feature scaling involves normalization and standardization
Normalization 	Feature Scaling Method 	Normalization involves rescaling features to a range of 0 to 1 (a special case of min-max scaling where the normalized value for xi = (xi - xmin)/(xmax - xmin)
Standardization 	Feature Scaling Method 	Standardization involves centering features to have mean 0 with a standard deviation of 1 to have the same parameters as a standard normal distribution (0 mean and unit variance) which makes it easier to learn the weights of the standardized features which maintains useful info about outliers and makes the algorithm less sensitive to outliers compared to min-max scaling, where the standardized x value = (x - mean)/standard deviation
Robust Scaler 	Feature Scaling Method 	The robust scaler is useful for algorithms that are prone to overfitting, where the robust scaler removes the median and scales the data set according to the 1st and 3rd quartile values so more extreme values and outliers are less powerful
Data Imputation 	Feature Engineering Method 	Imputation methods include mean imputation, median imputation, and most frequent imputation

L1 Least Absolute Shrinkage and Selection Operator (Lasso) Regularization	Feature Selection Embedded Method and Regularization	A loss function with a l1 regularization term (Σ|wi|) = 1/n * Σ (actual yi - predicted yi)^2 for all n examples + λ * Σ|wi| for all m features		A regularized variant of regression that adds an absolute value of the w coefficient penalty to the loss function to prevent overfitting thereby applying L1 regularization to encourage sparsity in the model where features with non-zero coefficients are considered important and where methods like Lasso are more suitable for certain models like linear models and where lasso can shrink coefficients to zero, where L1 requires feature normalization, where in contrast to l2 regularization, l1 regularization usually creates sparse feature vectors and most feature weights will be zero, where sparsity is useful in high dimensional datasets with many irrelevant features especially where there are more irrelevant features than training examples
Lasso Regression	Regression	A regression model that uses Lasso regularization to select important features
L2 (Ridge) Regularization	Regularization	The regularization term is λ/2 * ||w||^2 or λ/2 * Σ wi^2 for all m features, so adding this term to the loss function penalizes extreme parameter/weight values, as in loss = 1/n * Σ(actual yi - predicted yi)^2 for all n examples + λ/2 * ||w||^2, so loss/cost functions can be regularized with the term (λ/2) * ||w||^2 which will shrink the weights during training and penalize complexity by adding the squared sum of the weights as a penalty term, increasing the regularization strength by increasing λ, where λ is the inverse of the C parameter, and decreasing C increases the bias and decreases the variance of the model 	A regularized variant of regression that adds a squared w coefficient penalty to the loss function thereby penalizing larger weights which encourages weights to be less extreme and decay to zero to prevent overfitting and handles multicollinearity by shrinking coefficients of correlated features instead of eliminating them by constraining the coefficient norm, where L1 regularization uses the sum of absolute values of weights and L2 uses the sum of weights squared, where L2 requires feature normalization
Elastic Net Regression	Regression	Elastic Net Regression combines L1 and L2 regularization in adding the absolute norm of the weights and the squared measure of the weights with a mixing parameter alpha where 1 is lasso, 0 is ridge and other values are both lasso and ridge: Loss = 1/n Σ(actual yi - predicted yi)^2 + lambda * ((1 - alpha) * Σ|wi| for all m features + alpha * Σ wi^2 for all m features)
Least-Angle Regression (LARS) 	Regularization function
AIC/BIC 	Regularization function
Basis pursuit denoising 	Regularization function
Rudin-Osher-Fatemi model (TV) 	Regularization function
Potts model 	Regularization function
RLAD 	Regularization function
Dantzig Selector 	Regularization function
SLOPE  	Regularization function

Decision Tree Classifier	Classification and Feature Selection Embedded Method	Gini = 1 - Σ(pᵢ)² or Entropy = -Σ(pᵢ log₂ pᵢ)	Uses an entropy/impurity function to choose best splits based on maximization of information gain like Gini Impurity or entropy, where each leaf in the tree is a decision or prediction and each decision node represents a feature test, where each branch represents an outcome of the test, where end leaf nodes provide the final prediction value, constructing a model of decisions made based on actual values of attributes in the data, where the algorithm finds the set S that minimizes the sum of child node impurities and chooses the split (X, S) that gives the minimum compared to all X and S for each variable X, then repeats this at each decision node until a stopping threshold is reached, where the max depth of a tree should be limited as well as limiting the number of test nodes, the minimum number of data points at a node required to split, avoiding splitting when at least one of the subsample sizes is below a threshold and stop splitting a node if it doesnt improve the fit to prevent overfitting, where decision trees compute class probabilities using a frequency vector created for each node at training time, which collects the frequency values of each class label computed from the class label distribution at that node, then normalizes the frequencies so they add up to 1 	Best with categorical/numerical data and easy to interpret and fast and performs feature selection by selecting the most important features for splitting nodes based on criteria like Gini impurity or information gain, doesn't require scaling or normalizing the data and handles non-linear relationships and missing data 	Prone to overfitting if too deep and are often inaccurate and unstable/volatile and can't capture complex interactions and information gain is biased towards features with many categories and computationally expensive	Supervised		Entropy, Gini Impurity or Information Gain
Decision Tree Regressor	Regression		Supervised		Mean Squared Error
Classification and Regression Tree (CART) 	Decision Tree Algorithm
Iterative Dichotomizer (ID3) 	Decision Tree Algorithm
C4.5 and C5.0 	Decision Tree Algorithm
Chi-squared Automatic Interaction Detection (CHAID) 	Decision Tree Algorithm
Decision Stump 	Decision Tree Algorithm
M5 	Decision Tree Algorithm
Conditional Decision Trees 	Decision Tree Algorithm

Blending (Weighted average) 	Ensemble algorithm type

Bagging (bootstrapped aggregation)	Ensemble algorithm type			Trains multiple similar models and averages their predictions to reduce errors, where instead of using the same training data set on each classifier in the ensemble, the bootstrapping procedure (repeatedly selecting a random sample with replacement of the training set and fitting a model to these samples then averaging for regression or selecting the majority vote for classification) leads to better model performance because it decreases the variance of the model, without increasing the bias, where random forest is a case of bagging in which subsampling the data and allowing the weak learner trees to overfit slightly by not pruning the trees creates less correlated trees which have the effect of creating weaker but not weak learners  	Bagging can decrease overfitting and improves accuracy for unstable models like artificial neural networks, classification and regression trees, and subset selection in linear regression	Bagging can mildly degrade the performance of stable methods such as k-nearest neighbors and is ineffective at reducing bias, which is why its good to apply bagging to an ensemble of classifiers with low bias like unpruned decision trees

Binning	Bagging and Random Forest function 	Binning involves grouping values together to avoid values that are far apart

Stacking (Stacked Generalization) Ensemble learning algorithm type			An ensemble learning algorithm where the final stacked model combines predictions from multiple strong base models, which is a two-level ensemble, where the first level has individual classifiers that feed their predictions into the second level where another classifier (usually logistic regression) is fit to the level-one predictions to make the final predictions 	Stacking has better accuracy and can combine different models like decision trees, logistic regression, and SVM and reduces overfitting when implemented with cross-validation by balancing out weaknesses of each model and learns from mistakes of base models and is customizable	Stacking is complex, slow, not interpretable, and has a risk of overfitting if the meta-model is too complex or if there's data leakage and requires more data
Mixture of Experts (MoE) 	Ensemble algorithm 		Similar to stacking, MoE creates a strong model from strong learner classifier models (experts in some subset of the feature space) by dividing the task into sub-tasks and training each expert on a sub-task, involving a divide and conquer approach similar to decision trees, and a meta-model similar to stacked generalization, and a gating/routing network to route inputs to the most relevant expert and a combination method of expert predictions, where experts can be any neural network model like a transformer or a simple feedforward network and outputs a probability distribution for experts, and where the gating network is often a simple linear layer followed by a softmax function, and where load balancing can prevent reliance on a subset of experts which also prevents expert collapse, and where hyperparameters include number of experts where too few experts limit the model's capacity and too many experts lead to underutilization and training instability and where expert capacity of how many tokens each expert can process in a batch is more flexible at higher capacity with increased computational cost and where load balancing regularization strength needs to be balanced against task performance since excessively strong regularization can harm performance while weak regularization can lead to expert imbalance and where gating entropy measures routing diversity, where pooling/combination methods include selecting the expert with the largest output or confidence provided by the gating network or using a weighted sum of predictions given confidence level 	Dense expert selection of more experts can improve performance when multiple perspectives on the input is useful and expert parallelization enables parallel computing and dynamic batching of inputs routed to the same expert can increase efficiency and where memory allocation can minimize overhead from expert switching and data movement and where MoE models can maintain approximately constant computational cost while scaling parameters 	Gradients may not flow effectively through unused experts so auxiliary loss functions (additional loss terms to penalize overconfidence in routing or underuse of experts), gradient scaling to ensure all experts receive inputs during training even when selected infrequently, and regularization techniques to prevent overfitting to specific routes and maintain diversity among experts are used to address this problem, and where distributed implementations have to manage data movement between experts and uneven expert utilization can create bottlenecks in distributed systems

Boosting	Ensemble algorithm type		Boosting creates a series of models that correct errors made by previous models, where unlike bagging, boosting uses random samples of the training data set without replacement, where boosting trains an ensemble of weak learners which often have a slight performance advantage over random guessing like a decision tree stump, which lets weak learners learn from hard-to-classify misclassified examples to improve ensemble performance, with steps 1. select a random sample from the training data set without replacement to train a weak learner, 2. draw a second random training data set subset without replacement and add 50% of previously misclassified examples to train a second weak learner, 3. find the training examples which the first two weak learners disagree on to train a third weak learner, 4. combine the three weak learners using majority voting 	Boosting can decrease bias and variance, in contrast to bagging, though in practice boosting algorithms like Adaboost tend to overfit (have high variance)
Gradient Boosting Classification	Ensemble and classification and Feature Selection Embedded Method	Objective is to minimize the loss F(x) = Σ for all M gamma_m * h_m(x) where the model at stage m is F_m(x) = F_m-1(x) + learning rate gamma_m * h_m(x) where h_m(x) is a new weak model trained to minimize the gradient of the loss function at step m where predictions of the new model are added to the ensemble and the training iteration is repeated until a stopping threshold is reached which typically uses decision trees which is different from random forest in that gradient boosting builds one tree at a time and gradient boosting combines results iteratively rather than at the end of the process which is what random forest does		Best for structured/tabular data, high performance models where it refines weak learners sequentially with boosted decision boundaries improving error step by step by building an ensemble of weak learners (usually decision trees) sequentially so each new model corrects errors by previous models 	Reduces overfitting	Supervised		Cross Entropy Loss
Gradient Boosting Regressor	Ensemble and regression and Feature Selection Embedded Method	L = ΣL(yᵢ, F(xᵢ))	Sequentially minimize loss (e.g. MSE, Log Loss) where like random forests, gradient boosting models select important features while building trees by prioritizing features that reduce error the most			Supervised		Mean Squared Error Loss
XGBoost	Ensemble classification and Regression	L = Σ l(yᵢ, ŷᵢ) + ΣΩ(fₖ)	Loss plus regularization on tree complexity		advanced version of gradient boosting that includes regularization to prevent overfitting	faster than gradient boosting for large datasets
LightGBM	Ensemble algorithm for classification			Uses a histogram-based approach for faster computation and supports categorical features natively
CatBoost	Ensemble algorithm for classification			Designed for categorical data with built-in encoding, using symmetric trees for faster training and better generalization	
AdaBoost	Ensemble algorithm 	Uses the complete training data set for training weak learners, reweighting the training examples in each iteration to build a strong classifier that learns from weak learner mistakes, with steps 1. set the weight vector to uniform weights where the sum of the weights is 1, 2. for j in m boosting rounds, train a weighted weak learner cj = train(X, y, w), predict class labels yhat = predict(cj, x), compute weighted error rate error = weight vector dot (yhat != y) (a binary vector of 1s and 0s which assigns 1 if the prediction is incorrect and 0 otherwise), compute the coefficient aj = 0.5 log (1 - error/error), update weights = previous weight vector * exp(-coefficient x yhat x y), normalize weights to sum to 1 = previous weight vector/sum of weights), 3. compute the final prediction yhat = (Σ for all j coefficient (aj x predict(cj,X)) > 0)	 					Exponential Loss
Gradient Boosting Machine (GBM) 	Ensemble algorithm
Gradient Boosted Regression Trees (GBRT)	Ensemble algorithm
Random Forest Classifier	Classification and Feature Selection Embedded Method and Ensemble algorithm 		Random Forest Classifier is the same as Decision Tree + Bootstrap Aggregation (Bagging) that is an ensemble of decision trees with more stable boundaries and aggregated decision regions from multiple trees that trains multiple decision trees on different parts of the data with random sampling by considering a random subset of features at each split and with feature selection, then aggregates the predictions of multiple trees (majority vote for classification, average for regression), which applies randomness by selecting random data samples with replacement from the training set to train different models and average their predictions where this random sampling of the training set reduces variance, and also each time a tree split is determined, random forest takes a random sample of m features from the full set of n features without replacement and uses this subset of m features as candidates for the split which decorrelates trees, where a larger value of trees reduces overfitting and typically this parameter is initialized as the square of the number of features, where an excessive number of large trees can lead to overfitting, where the algorithm 1. draws a random bootstrap sample of size n with replacement, 2. create a decision tree from the bootstrap sample, at each node randomly selecting d features without replacement and splitting the node with the feature that provides the best split according to the objective function like maximizing the info gain, using a random subset of features when training individual decision trees, 3. repeats 1 and 2 k times (k is the number of trees), 4. aggregates predictions to assign class label by majority vote for classification, average for regression, where the bootstrap sample size n controls the bias-variance trade-off of the random forest, where increasing the size of the sample may increase overfitting by increasing similarity of decision trees, where n the size of the bootstrap sample is usually selected as the number of training examples, where d the number of features at each split should be less than the total number of features which defaults to the square root of the total number of features, where a random forest measures feature importance by average impurity decrease computed from all decision trees without assuming data is linearly separable 	High accuracy and requires little parameter tuning and reduces overfitting compared to decision trees and robust to outliers and handles large datasets with high dimensionality and handles missing data by binning and handles complex data and noise doesnt require scaling or normalization and handles non-linearity and performs feature selection by selecting the most important features for splitting nodes based on criteria like Gini impurity or information gain 	Slower and less interpretable and sensitive to imbalanced data and random forests also do not generally perform well when given sparse data with little variability and consumes a lot of memory and is difficult to tune and is likely to pick correlated features bc it samples features to build each tree, and one correlated feature may be ranked higher than other correlated features bc info may be lost during training	Supervised		Entropy, Gini Impurity or Information Gain
Random Forest Regressor	Regression and Feature Selection Embedded Method and Ensemble algorithm		Supervised		Mean Squared Error
Weak learner 	Model Type 		Weak learning models perform slightly better than random guessing and are often combined to create strong learning models, including types of weak learners like a decision tree with one node, k nearest neighbors with k = 1 and operating on one or a subset of input variables, multi-layer perceptron with one node operating on one or a subset of input variables, or naive bayes operating on one input variable, where weak learners are often combined in ensemble learning such as boosting algorithms which combine weak learners to create strong learners by combining the predictions of weak learners by training weak learners sequentially to minimize the errors made by previous models
Strong learner 	Model Type 		Strong learning models learn with good accuracy, examples include logistic regression, SVMs, and k-nearest neighbors with parameters tuned to create strong models

Few-shot learning 			Using a very small example-to-class ratio for small training sets, for example in a 5-way 1-shot problem there are five classes with one example each, uses a support set to sample training tasks that mimic the use-case scenario during prediction, where each training task has a query image to be classified and the model is trained on several training tasks from the support set in an "episode", where the classes in the support and query sets differ from the support and query sets encountered during training, where one few-shot strategy is to learn a model that produces embeddings so we can find the target class with a nearest-neighbor search among the images in the support set, where the model learns how to produce good embeddings from the support set to classify the query image based on finding the most similar embedding vector		Supervised
Meta-learning (few shot)	Few shot learning type 		Training updates the model's parameters so it can adapt well to a new task 			Supervised

Loss/error functions	Function Type			Quantifies how well one prediction of the machine learning algorithm compares to the actual target value during training thereby providing the signal for the model's learning algorithm to update weights and parameters
Mean Absolute Error	Regression Loss Function 	1/n * Σ for all n |actual yj - predicted yj| 		Sensitive to outliers
Mean Squared Error	Regression Loss Function	1/n * Σ for all n of (actual yᵢ - predicted yᵢ)²		Penalizes larger errors heavily
Mean Absolute Percentage Error	Regression Loss Function			Used to express the error in terms of percentage	 where the smaller the percentage the better the model performance
Root Mean Squared Error	Regression Loss Function	√(Σ for all n (actual yi - predicted yi)^2) / N		Indicates how much the data points are spread around the best line and is the standard deviation of the MSE	 where lower value means that the data point lies closer to the best fit line.
Root Mean Squared Log Error	Regression Loss Function 	√ (Σ for all n (log(actual yi + 1) - log(predicted yi + 1))^2 / N)
Huber Loss
Los-cosh Loss 	Regression Loss Function 	Σ log(cosh(predicted yi - actual yi)) 	Log-cosh loss is a smooth alternative to MAE that is less sensitive to outliers

Cross Entropy Loss	Loss Function	-Σ for all n (true label yᵢ * log(predicted probability pi for class i)) where the true label is a one-hot encoded vector which is 1 for the correct class and 0 for incorrect classes	Cross entropy is used to measure the distance between two probability distributions such as using the discrete cross-entropy loss (CE) between class label y and the predicted probability p when training logistic regression or neural network classifiers on a dataset of n examples

Binary Cross Entropy Loss 	Binary Classification Loss Function 	-1/n * Σ for all n [actual yi * log (predicted yi) + (1 - actual yi) * log(1 - predicted yi) 	Used in binomial logistic regression, measures how close predictions are to actual class probabilities
Hinge Loss	SVM Binary Classification Loss Function	max(0, 1 - yᵢ(w·xᵢ + b)) 	Encourages a large margin between classes

Categorical Cross Entropy Loss 	Multiclass Classification Loss Function - Σ for all n actual yi * log(predicted yi) 	Generalization of binary cross entropy for > 2 classes
Sparse Categorical Cross Entropy Loss 	Multiclass Classification Loss Function with sparse labels 			Optimized for memory, labels don't need to be one-hot		
KL Divergence 	Probabilistic Classification 		Measures divergence from true distribution

Focal Loss 	Imbalanced Classification Loss Function 			Focuses learning on hard misclassified examples
Hinge Loss/Margin Ranking Loss 	Ranking Loss Function 			Encourages correct order of relevance
Triplet Loss 	Loss Function used in Face Recognition, Embeddings 			Encourages anchor-positive closeness and anchor-negative separation
Dice Loss/IoU Loss 	Image segmentation Loss Function 			Measures overlap and is robust when classes are imbalanced
Connectionist Temporal Classification Loss 	Sequence model Loss Function 			Enables training without aligned input-output sequences
Wasserstein Loss 	GAN/WGAN Loss Function 			Stabilizes gradients in training generative models
Exponential Loss 	Adaboost Loss Function

Log Loss	Regression Loss Function	-1/n * Σ for all points n Σ for all classes m yij * log(pij)	Used in multinomial logistic regression and neural networks
R2 loss	Regression Loss Function 	Ratio of the sum of squares residual (SSR the total variation of actual vs predicted values) and the sum of squares total (SST the total variation of actual values from the mean) = 1 - Σ for all n (actual yi - predicted yi)^2 / Σ for all n (actual yi - y mean)^2 = 1 - Mean squared error/Var(y)	The coefficient of determination R2 measures the proportion of the variance in the dependent variable that can be explained by independent variables, so rhe highest possible value for r-squared is 1, representing a model that captures 100% of the variance, where a negative r-squared means that our model is doing worse (capturing less variance) than a flat line through mean of our data would	Less sensitive to outliers
Adjusted R2 loss 	Regression Loss Function

Entropy	Information uncertainty/impurity function	Entropy H(S) = - Σ for all k (pᵢ log₂ pᵢ) where pᵢ is the proportion of the dataset belonging to class i for a particular node t	Entropy encodes the number of info bits required to represent information and a decision tree tries to reduce the entropy by splitting the data on features that provide the most information about the target variable, where the entropy tries to maximize the mutual info in the tree, where entropy is 0 if all examples belong to the same class and entropy is 1 if the classes are uniformly distributed
Shannon Entropy 	Entropy type 	The expected amount of information/surprise in a random variable is information(x) = -log(p(x))	log is the base 2 log or natural log and p(x) is the probability of event x
Cross Entropy 	Entropy type 	The distance between the true distribution p and the predicted distribution q
Conditional Entropy 	Entropy type 	The average uncertainty remaining in Y given that X is known
Kullback-Leibler Divergence (Relative Entropy)	Entropy type 	Indicates how a probability distribution diverges from a second expected distribution
Differential Entropy 	Entropy type 	Differential entropy is entropy for continuous random variables
Joint Entropy 	Entropy type 	The uncertainty of two variables together 
Renyi Entropy (generalization of Shannon) 	Entropy type 	Renyi entropy is a tunable generalization of entropy for different sensitivity levels
Tsallis Entropy 	Entropy type 	Tsallis entropy is another generalization of entropy used in non-extensive systems like complex networks
Gini Impurity	Information impurity function	Gini G(S) = Σ for all k (pᵢ) * (1 - (pᵢ)) = 1 - Σ for all k (pᵢ)²	 The lower the Gini Impurity, the better the feature splits the data into distinct categories, where the gini impurity tries to minimize the probability of misclassification, where gini impurity and entropy typically have very similar results, where the gini impurity is an intermediate metric between the classification error and the entropy
Information Gain	Feature Selection Filter Method	IG of the parent node in a decision tree = entropy/gini impurity/classification error of the parent - (number of training examples at the left node/number of training examples at the parent node) * entropy/gini impurity/classification error of the left node - (number of training examples at the right node/number of training examples at the parent node) * entropy/gini impurity/classification error of the right node, where entropy/gini impurity/classification error are the three alternative information gain metrics 	Measures how well an attribute reduces uncertainty/entropy where a greedy algorithm would split data based on which attribute maximizes information gain (reduces uncertainty/entropy) in a set S which is calculated using the difference in entropy before and after the split	it tends to choose the most impactful features close to the root of the tree and is a good measure for deciding the relevance of some features	One major drawback of information gain is that the feature that is chosen as the next node in the tree tends to have more unique values
Classification error 	Information impurity function 	1 - max{pᵢ} 	The classification error is useful for pruning a tree but not useful for creating a tree since its less sensitive to changes in class probabilities of nodes

Phi decision tree function 	Decision tree split function 	phi = 2 * PL * PR * Q(s|t) 	The phi function decides relevance based on goodness of a split at a node and the phi function is maximized when the chosen feature splits the samples in a way that produces homogenous splits and have around the same number of samples in each split
Gain Ratio	Information entropy function 	[-Σ for all n H(T) * log(H(T)) - (-Σ for all n H(T|a) * log(H(T|a))) ]/ -Σ for all n N(ti)/N(t) * log base 2 (N(ti)/N(t)) where N(xi) is the number of times xi occurs and H(T) is the entropy of T		Improves on information gain by considering the worth of attributes with a wide range of possible values thereby handling the bias of information gain to favor attributes with more pronounced values

Centroid-based Clustering Method Clustering Method Type			Represent clusters using central points like centroids or medoids
K-Means	Centroid-based Clustering Method	The objective is to find argmin Σi in K Σx in Si ||xᵢ - μ_k||² = argmin Σi in K |Si| Var Si (for each cluster k) where Si is the size of the set of points in cluster k and μ_k is the centroid of cluster k where each observation is a d-dimensional vector, aiming to partition the n observations into k <= n sets so as to minimize the within-cluster sum of squares WCSS (variance) and which is equivalent to minimizing the pair-wise squared deviations of points in the same cluster, picking k centers, assigning each point to the closest center and then updating centers based on the average of assigned points, repeating until assignments stabilize	Minimizes within-cluster variance by grouping data by proximity with the cluster centroids discovered from unlabeled data	Simple and fast and works well with spherical clusters with similar sizes and scalable	Needs K specified and spherical classes and is sensitive to noise	Unsupervised		Sum of squared errors
k-median clustering 	Clustering Method
K-means++ clustering Centroid-based Clustering Method
K-mode clustering Centroid-based Clustering Method
Fuzzy C-means (FCM) Clustering Centroid-based Clustering Method

Distribution-based Clustering	Clustering Method Type
Gaussian Mixture Models (GMMs)	Distribution-based Clustering Method 			Models clusters as overlapping Gaussian distributions, assigning probabilities for data points' cluster membership
Expectation-Maximization Algorithms Distribution-based Clustering Method
Dirichlet Process Mixture Models (DPMMs)	Distribution-based Clustering Method

Connectivity-based Clustering method 	Clustering Method Type
Hierarchical Clustering	Connectivity-based Clustering	d(A	B) = min/max/avg ||a - b||	Agglomerative clustering based on distance linkage, building a tree-like structure by merging or splitting clusters	Dendogram visualization and no need to specify k	Computationally expensive	Unsupervised		Distance-metric based
Within-Cluster Sum of Squares - WCSS	K-means clustering objective function Σk in K Σx in Sk ||xᵢ - μ_k||² for all Sk where Sk is the set of points in cluster k and μ_k is the centroid of cluster k
Agglomerative Clustering	Connectivity-based Clustering
Divisive Clustering	Connectivity-based Clustering
Affinity Propagation 	Connectivity-based Clustering

Density-based Clustering Method 	Clustering Method Type
DBSCAN (Density-Based Spatial Clustering of Applications with Noise)	Density-based Clustering	ε-neighborhood with density ≥ minimum points	Forms clusters defined by density regions	Handles noise and outliers and arbitrary shapes	Struggles with different densities	Unsupervised
OPTICS (Ordering Points to Identify the Clustering Structure)	Density-based Clustering

Neural Networks	Classification	J(θ) = (1/m) Σ Loss(yᵢ,ŷᵢ)	Backpropagation minimizes loss via gradient descent using layers of neurons that apply weighted transformations followed by non-linear functions, learning by backpropagating error and updating weights by gradient descent	Captures complex patterns and non-linear relationships			Cross entropy loss
Neural Networks	Regression							Mean squared error
Multilayer Perception 	Neural Network Type for Classification and Regression 		Neural network with multiple layers of nodes
Perception 	Neural Network Type for Classification and Regression 	The perceptron learning rule is new weight = weight + weight update, where weight update j = learning rate * (actual yi - predicted yi) * xij	Neural network with multiple layers of nodes, compares true class labels with predicted class labels to compute the error and update the weights
Adaline (adaptive linear neuron) 	Neural Network Type 	Adaline uses batch gradient descent, where the Adaline learning rule is new weight = weight + weight update, and where the change in weight j = learning rate * the negative gradient of the cost function, so the weight update j = learning rate * Σ (actual yi - predicted yi) * xij, bc the Adaline cost function is Σ 1/2 * (actual yi - predicted yi) ^ 2 (as in, the sum of squared errors) and the partial derivative of the cost function is - Σ (actual yi - predicted yi) * xij 	Adaline involves a linear activation function (the identity function) instead of the step function of the perceptron, which has continuous output which makes the cost function differentiable, but Adaline still uses a threshold function to make predictions; Adaline compares true class labels to the continuous output of the linear activation function to compute the error and update the weights, as opposed to comparing true to predicted class labels like the perceptron does
Deep learning 	Learning Type		Uses neural networks with many layers 	useful for tasks involving large datasets and high-dimensinoal data
Feedforward Neural Networks (FNN) Deep Learning Network Type used for classification and regression 	Basic neural network model 	Easy to train and is useful for simple tasks
Convolutional Neural Networks (CNN)	Deep Learning Network Type 			Good for handling grid-like data and learns spatial hierachies of features
Generative Adversarial Networks (GANs)	Deep Learning Network Type 			Useful for generating synthetic data closely resembling real data
Transformer Networks 	Deep Learning Network Type 			Used in sequence-to-sequence tasks 	Efficiently handles long-range dependencies and parallelizes training
Hopfield Network 	Neural Network Type
Radial Basis Function Network (RBFN) 	Neural Network Type
Deep Boltzmann Machine (DBM) Neural Network Type
Deep Belief Networks (DBN) Neural Network Type

Gradient Descent	Optimization function	The gradient of the cost function = the partial derivative of the cost function with respect to each weight wj, where the change in weight j = learning rate * the negative gradient of the cost function
Stochastic/iterative/online Gradient Descent 	Optimization function 	The weight update j = learning rate * (actual yi - predicted yi) * xij, where cost is the average cost of training examples in each epoch	SGD updates the weights incrementally for each training example, where learning rate is usually an adaptive learning rate that decreases over time as in c1/(number of iterations + c2), which means SGD doesnt reach the global minimum but approaches it. Typically SGD converges faster than gradient descent bc of the more frequent weight updates. Since each gradient is calculated from one example, errors are noisier than in gradient descent, so SGD can escape shallow local minima easily when using nonlinear cost functions, and SGD can be used for online learning 	Converges faster than gradient descent and can escape shallow local minima 	Requires shuffling training data for every epoch to prevent cycles
Batch Gradient Descent 	Optimization function 		Updates the weights based on the sum of the accumulated errors over all examples of the training dataset
Mini-batch Gradient Descent 	Optimization function 		Applies batch gradient descent to small batches of the training data like 32 examples at a time, which converges faster bc of the more frequent updates
Adam (Adaptive Moment Estimation) Gradient Descent 	Optimization function
Nesterov Accelerated Gradient 	Optimization function
Momentum 	Optimization function
Adagrad 	Optimization function
Adadelta 	Optimization function
Online learning Learning Type 		The model is trained as new data arrives, so the weights aren't re-initialized before each training example is fit, where the weights are updated for each training example

Backpropagation	Neural network weight update function			applies the chain rule to compute the gradient of the loss function with respect to each weight in the network

Reinforcement Learning 	Learning Type		Q(s,a) ← Q(s,a) + α[r + γ max Q(s’,a’) - Q(s,a)] uses a Bellman equation update 		The model learns by making a sequence of decisions interacting with an environment and receiving rewards/penalties, with a goal of maximizing cumulative reward, categorized into model-based and model-free methods
Model-based Reinforcement Learning 	Reinforcement Learning Type 		Uses an environment model to predict outcomes and help the agent plan actions by simulating potential results
Markov Decision Processes (MDPs)	Reinforcement Model-based Learning Type 	
Bellman equation	Reinforcement Model-based Learning Type 	
Value iteration algorithm 	Reinforcement Model-based Learning Type 	
Monte Carlo Tree Search 	Reinforcement Model-based Learning Type 	
Model-free Reinforcement Learning 	Reinforcement Learning Type 		Model-free methods dont depend on an environment model but instead learn directly from experience by interacting with the environment and adjusting actions based on feedback, which have value-based and policy-based methods
Value-based Model-free Reinforcement Learning 	Reinforcement Learning Model-free Type 		Value-based methods learn the value of different states or actions where the agent estimates the expected return from each action and selects the one with the highest value
Q-Learning 	Value-based Model-free Reinforcement Learning Type 			Used for decision-making in sequential tasks	Can learn optimal policies even with limited environment knowledge
Deep Q-Networks (DQN) 	Reinforcement Learning Network Type 		Handles more complex reinforcement learning with deep learning, combining q-learning with deep neural networks, allowing it to handle large state spaces
SARSA 	Value-based Model-free Reinforcement Learning Type
Monte Carlo Methods 	Value-based Model-free Reinforcement Learning Type
Policy-based Model-free Reinforcement Learning 	Reinforcement Learning Model-free Type 		Policy-based methods directly learn a policy (a mapping from states to actions) without estimating values where the agent continuously adjusts its policy to maximize rewards
REINFORCE algorithm 	Policy-based Model-free Reinforcement Learning Type
Actor-Critic algorithm 	Policy-based Model-free Reinforcement Learning Type
Asynchronous Advantage Actor-Critic (A3C) 	Policy-based Model-free Reinforcement Learning Type
Policy Gradient Methods	Reinforcement Learning Type 		Learns policies directly for reinforcement learning tasks	Useful for tasks with continuous action spaces or complex environments
Proximal Policy Optimization (PPO) 	Reinforcement Learning Type 		PPO is reinforcement learning with stable and efficient updates 	Useful for large-scale problems with complex environments

Recurrent Neural Networks (RNNs)	Network Type		handles sequential data by maintaining a memory of previous inputs and used for Sequence Modeling and Time Series Prediction and Natural Language Processing		Struggle with long-term dependencies and have vanishing gradients (solved by LSTM and Gated Recurrent Unit)	
Long Short-Term Memory (LSTM) networks	Recurrent Neural Network Type	Models sequences with long-range dependencies	Addresses the vanishing gradient problem in RNNs

Semi-supervised learning 	Learning type 	A hybrid between supervised and unsupervised learning that uses both labeled and unlabeled data for training 	Often used with large datasets when labeling data is expensive or time-consuming
Self-supervised learning 	Learning Type 	Self-supervised learning is a subset of unsupervised learning where the model learns to predict parts of the data from other parts of the data without explicit labels	Is useful when labeling data is not possible

Association rules algorithms 	Algorithm Type 	association rules algorithms find patterns between items in large datasets based on the frequency of item occurrences and co-occurrences
Apriori algorithm 	Association Rule Algorithm Type 		Finds frequent itemsets by iterating through data and pruning non-frequent item combinations
FP-Growth (Frequent Pattern-Growth)	Association Rule Algorithm Type 		Efficiently mines frequent itemsets using a compressed FP-tree structure without candidate generation
ECLAT (Equivalence Class Clustering and bottom-up Lattice Traversal)	Association Rule Algorithm Type 		Uses vertical data format for faster frequent pattern discovery through efficient intersection of itemsets

Max Pooling 	Neural Network layer type			Selects a maximum of inputs in pool, and downsamples feature maps	 Reduces computation
Average Pooling 	Neural Network layer type 		Averages inputs in pool
Flatten 	Neural Network layer type			Converts 2D to 1D
Dense (Fully Connected) 	Neural Network layer type			All nodes in the layer are connected to all nodes in the next layer

Jaccard similarity 	Similarity metric of sets 	Jaccard(A,B) = | size of intersection between A and B| / | size of union between A and B|
Cosine similarity

Pointwise Mutual Information of token pairs 	Token pair metric 	PMI(t1,t2) = log( P(t1,t2) / (P(t1) * P(t2)) ) = log(N * c(t1,t2)/(c(t1) * c(t2))) where N is the total number of tokens in the text, c(t1,t2) is the number of times t1 and t2 appear together and c(t1) and c(t2) are the number of times they appear separately	PMI is used to find collocations in text
TF-IDF	Vectorization 	IDF(t) = log(N/(1 + n(t))) where t is the token, n(t) is the number of documents that t occurs in and N is the total number of documents		Optimizes memory usage by creating a sparse matrix by multiplying importance weight (as in how often a word appears in a document) by how unique a word is (to reduce importance of common words) across the entire collection of documents so words appearing in many documents get a lower IDF score and rare words get a higher score and in the process converts text to vectors handling a large vocabulary in the process
Word2vec	Vectorization			Uses a two-layer neural network to encode words into embedding vectors for semantic and syntactic similarity using either continuous bag of words or skip gram to infer a word from context or vice versa	skip gram is more effective for infrequent words and CBOW is faster to train
One-Hot Encoding	Vectorization			Converts a word into a vector with a bit corresponding to the index in the vocabulary with all other bits set to zero, or converts categorical labels into vectors with one 1 value		Using one-hot encoding with categorical variables of high cardinality is suboptimal bc of the curse of dimensionality, and one-hot encoding introduces multicollinearity which is an issue with matrix inversion which can lead to unstable estimates, where correlation can be reduced by removing one feature column of the one-hot encoded columns
Binary Encoder 	Vectorization
Label Encoding	Vectorization
Ordinal Encoding	Vectorization
Target Encoding	Vectorization
Bag of Words	Vectorization			Represents text data as a vector of word counts

Covariance 	Statistic 	Covariance of xi and xj = (1/n-1) * Σ for all n (xj_i - sample mean of feature j) (xk_i - sample mean of feature k), where the sample means are zero if the features are standardized, and a positive covariance indicates the features increase or decrease together, and a negative covariance indicates they vary in opposite directions
Explained variance ratio of an eigenvalue 	Math Function 	The ratio of the eigenvalue to the total of all the eigenvalues
Dimensionality reduction	Function Type			used to simplify datasets by reducing the number of features while retaining the most important information.
Principal Component Analysis (PCA)	Signal Processing and Dimensionality Reduction to 2d and Feature Extraction Method	Aims to find the directions of maximum variance in a dataset and project the data into a new subspace with equal or fewer dimensions, so it maximizes var(Z) subject to z = X dot w and ||w|| = 1, which involves creating a d x k transformation matrix w that maps a training example feature vector x onto a new k-dimensional subspace with fewer than d dimensions, resulting in the output vector z, with specific steps 1. standardizing features, 2. finding the symmetric d x d covariance matrix that stores pairwise covariance among features, 3. finding the eigenvalue decomposition of the covariance matrix, and finding the projection with the eigenvectors of the covariance matrix, 4. sorting the eigenvalues by decreasing order (decreasing magnitude) to rank the eigenvectors, 5. select k eigenvectors corresponding to the k largest eigenvalues, 6. construct a projection matrix w from the top k eigenvectors, 7. transform the d dimensional dataset x using the projection matrix w to transform the features to the new k-dimensional feature subspace, where PCA transforms data into a new set of axes or orthogonal features (principal components) that capture the maximum variance, keeping the most important components, as a result of which the component with the largest variance is first, followed by lower variance components, where PCA directions are very sensitive to data scaling so features should be standardized before applying PCA if features have different scales and should have equal importance, where the eigenvectors of the covariance matrix represent the principal components (directions of maximum variance), where their corresponding eigenvalues define their magnitude, and where k the number of principal components is determined by a tradeoff between efficiency and classifier performance	Reduces overfitting and reduces the number of features while retaining the variance and fast and improves visualization 	Components may be hard to interpret and disregards independence of components and misses non-linear patterns	Unsupervised		Minimizes variance loss
Kernel PCA 	Non-linear dimensionality reduction transformation method 	Transforms non-linearly separable data into a lower dimensional subspace that is linearly separable which is useful bc the perceptron requires perfectly linearly separable data to converge and other algorithms like SVM, adaline, and logistic regression assume non-linear separability is bc of noise, where linear transformation techniques like LDA/PCA for dimensionality reduction may not apply to non-linear problems, so Kernel PCA involves performing a nonlinear mapping to transform data into a higher dimensional space, then applies standard PCA to to transform data back into a lower-dimensional space to be separated by a linear classifier, assuming the examples can be separated by density in the input space, which can be avoided with the kernel trick to compute high-dimensionality similarity between feature vectors in the original lower dimensional space without directly computing the pairwise dot products (or their eigenvectors) of the examples, as the kernel calculates the dot product of two low dimensional vectors (the dot product being a measure of similarity), so kernel pca results in examples already projected onto respective components rather than creating a transformation matrix as in PCA, where kernel PCA involves 1. computing the kernel/similarity matrix k for each pair of examples (applying a kernel function like the gaussian kernel to each pair of examples), then 2. centering the kernel matrix with Kprime = k - 1n dot k - k dot 1n + 1n dot K dot 1n where 1n is a n x n dimensional matrix (same as the kernel matrix) where each value is 1/n, where centering the matrix is necessary bc of the assumption that the data is standardized and the mean of each feature is zero when creating the covariance matrix and since the new feature space isnt directly computed so cant be guaranteed to also be centered at zero 3. collect top k eigenvectors of the centered kernel matrix based on their eigenvalues which are ranked by decreasing magnitude, where the eigenvectors are not the principle component axes but the examples already projected onto these axes, so the eigenvectors are obtained for the centered kernel matrix rather than the covariance matrix so the examples are already projected onto the principal component axis v, where adding a new example requires computing phi(x) transpose v (or avoiding computing that with the kernel trick) where kernel PCA is a memory-based method so the whole original dataset needs to be reused to project new examples, unlike standard PCA, by calculating the pairwise similarity between the new example and each existing example, where the Kernel matrix k dot eigenvectors = eigenvalues dot eigenvectors, and where after the similarities are computed for the new example, the eigenvector needs to be normalized by its eigenvalue 			Unsupervised
Principal Component Regression (PCR) 	Dimensionality Reduction
Independent Component Analysis (ICA)	Signal Processing and Dimensionality Reduction			separates mixed signals into their independent non-Gaussian components thereby finding a linear transformation of data that maximizes statistical independence among the components which is used in signal analysis to isolate distinct sources from mixed signals	Doesnt require labeled data and is non-parametric so it doesnt require assumptions about the data probability distribution and can be used for feature extraction to identify important features	Assumes decomposed source features are non-gaussian and are independent and are mixed linearly and ICA can be computationally expensive	Unsupervised
Linear Discriminant Analysis (LDA)	Dimensionality Reduction and Feature Extraction Method	J(w) = |wᵗ Sb w| / |wᵗ Sw w|	Maximizes class separability in a linear feature space using class label info, can be used to increase efficiency and reduce overfitting due to the curse of dimensionality in non-regularized models, where the goal of LDA is to find the feature subspace that maximizes class separability, whereas PCA finds the orthogonal component axes of maximum variance, where LDA has steps 1. standardize the d-dimensional dataset, 2. compute the d-dimensional mean vector for each class with mi = 1/ni Σ xm for each class i which creates a mean vector for each class that stores the mean feature values for that class, 3. create the between-class scatter matrix Sb and the within-class scatter matrix Sw = Σ si for each class i where Si is the individual scatter matrix for class i Si = Σ (x - mi)(x - mi)^T where each Si should be scaled by 1/ni before being summed to create Sw since the classes are unlikely to be uniformly distributed, which leads to computing the covariance matrix which is the same as the normalized scatter matrix, where Sb = Σ for all class i ni * (mi - m)(mi - m)^T where m is the overall mean computed with examples from all classes, 4. compute the eigenvectors/eigenvalues of the matrix Sw inverse dot Sb instead of computing the eigendecomposition of the covariance matrix as in PCA, 5. sort the eigenvalues by decreasing order to rank the corresponding eigenvectors, 6. select the k largest eigenvectors corresponding to the k largest eigenvalues to create a d x k transformation matrix w where the eigenvectors are the columns of this matrix w, 7. project the examples onto the new feature subspace using the transformation matrix w as in Xprime = X dot W, where LDA is similar to PCA in that it involves decomposing a matrix into eigenvectors to form the new lower-dimensional feature subspace, except it takes class label info into account by using the mean vector, where the maximum linear discriminants is c - 1 since the in-between scatter matrix Sb is the sum of c matrices of rank one or less, where in perfect collinearity (all example points are on a straight line) the covariance matrix would have rank one resulting in only one eigenvector with a nonzero eigenvalue	Assumes data is normally distributed, that classes have equal covariance matrices and that training examples are statistically independent, although works reasonably well even when these assumptions are somewhat violated	Supervised
Mixture Discriminant Analysis (MDA)
Quadratic Discriminant Analysis (QDA)
Flexible Discriminant Analysis (FDA)
Autoencoders	Dimensionality Reduction and Anomaly Detection	L = ||x - x'||²	Neural network that compresses and reconstructs data to minimize reconstruction loss, learning efficient representations of data	Useful for dimensionality reduction and feature learning and anomaly detection, works well with high-dimensional data
Stacked Auto-Encoders 	Neural Network Type
t-Distributed Stochastic Neighbor Embedding (t-SNE)	Dimensionality Reduction	Pᵢⱼ ∝ exp(-||xᵢ - xⱼ||²/2σ²)	Minimize Kullback-Leibler divergence between high and low dimensional distributions and thereby reduces dimensions for visualizing high-dimensional data and preserving local relationships	Captures complex structure unlike PCA and is effective at reducing the dimensionality of complex data	Slower and non-deterministic	Unsupervised		Minimizes pairwise differences
Non-negative Matrix Factorization (NMF)	Dimensionality Reduction			Factorizes data into non-negative components	useful for sparse data like text or images
Isomap	Dimensionality Reduction			Preserves geodesic distances to capture non-linear structures in data
Locally Linear Embedding (LLE)	Dimensionality Reduction			Preserves local relationships by reconstructing data points from their neighbors
Latent Semantic Analysis (LSA)	Dimensionality Reduction			Reduces the dimensionality of text data to reveal hidden patterns
Partial Least Squares Regression (PLSR) 	Dimensionality Reduction Type
Sammon Mapping	Dimensionality Reduction Type
Multidimensional Scaling (MDS)	Dimensionality Reduction Type
Projection Pursuit	Dimensionality Reduction Type
Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)	Dimensionality Reduction Type
Singular Value Decomposition (SVD)	Dimensionality Reduction Type
Neighborhood Component Analysis
Relief Algorithm
Fourier and wavelet transforms 

Statistical independence	Concept			refers to the idea that two random variables: X and Y are independent if knowing one does not affect the probability of the other so the joint probability of X and Y is equal to the product of their individual probabilities
Covariance Matrix	matrix used in PCA			Summarizes pairwise covariance relationships between different variables
Eigenvectors/eigenvalues in PCA	matrix attributes used in PCA			the principal components are the eigenvectors of the covariance matrix and the eigenvalues represent the variance explained by each component where the objective is to find the eigenvectors of the largest eigenvalues
Eigenvector 	Matrix attribute 	After finding eigenvalues, find the eigenvector (a nonzero vector that changes scale by an eigenvalue when multiplied by a matrix A) associated with eigenvalue_i: (matrix A - eigenvalue_i * identity matrix I) * eigenvector v = 0, where if multiplying A by v scales v by a factor of the scaler lambda then v is an eigenvector of A and lambda is the corresponding eigenvalue https://en.wikipedia.org/wiki/File:Eigenvalue_equation.svg
Eigenvalue 	Matrix attribute 	Find eigenvalues (scalers that scale an eigenvector) by solving for the roots of the equation resulting from the determinant = 0: det(matrix A - eigenvalues λ * identity matrix I) = 0, then find eigenvectors for each eigenvalue
Determinant of a matrix	Matrix attribute 	The 2x2 determinant is ad - bc and the 3x3 determinant is aei + bfg + cdh - ceg - bdi - afh, or can be expressed as the sum of n! signed products of matrix entries, or a linear combination of determinants of sub-matrixes, or can be expressed with gaussian elimination using the row echelon form

z-score	Evaluation Metric	z = (x - mean)/standard deviation		Standardizes x value
t-score	Evaluation Metric	estimated value - assumed value / standard error	Uses the student's t-distribution to calculate the number of standard deviations from the mean
Confidence Interval 	Evaluation Metric 	A range of values used to estimate a population parameter like the mean, where a 95% confidence interval indicates that 95 of 100 repeated samples with the same sampling method overlap with the actual population value of the parameter, where the confidence interval reflects the reliability of the sampling method used to generate the intervals, so the true mean lies in the confidence interval 95% of the time
Bayes factors
Likelihood ratios
Expected value
Covariance
Correlation
Multicollinearity 	Data interaction concept 	Multicollinearity is where features are correlated, which reduces the robustness of a model
Variance	Evaluation Metric	variance = expected value E of (x - mean)^2 for a random variable, or the covariance of a random variable with itself, or the second cumulant of a probability distribution that generates X: E[X^2] - E[X]^2
Standard deviation	Evaluation Metric	standard deviation = square root of the variance
Standard error	Evaluation Metric	standard error = standard deviation / √n
Kurtosis	Distribution Metric			Kurtosis is a statistical measure that describes the shape of a distribution particularly the "peakedness" or "tailedness" of the data
Skewness	Distribution Metric		Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean
Euclidean distance between two points	Continuous distance metric		In one dimension d(x, Xi) = √(Σ for all n (xj - Xij)^2) and in two dimensions, the distance d(p,q) = √((p1 - q1)^2 + (p2 - q2)^2)	A commonly used distance metric for continuous variables is Euclidean distance, which should be preceded by standardizing values so each feature contributes equally to the distance
Manhattan distance 	Distance metric 	d(x,y) = Σ for all n |xi - yi|
Minkowski distance 	Distance metric 	d(x,y) = (Σ for all n (xi - yi)^p)^p_mean 	Represents Euclidean distance with p = 2 and Manhattan distance with p = 1
Hamming distance (overlap metric)	Discrete distance metric
Norm Vector Concept A norm is a function from a real or complex vector space to the non-negative real numbers that behaves in certain ways like the distance from the origin: it commutes with scaling, obeys a form of the triangle inequality, and zero is only at the origin. In particular, the Euclidean distance in a Euclidean space is defined by a norm on the associated Euclidean vector space, called the Euclidean norm, the 2-norm, or the magnitude or length of the vector. This norm can be defined as the "square root of the inner product of a vector with itself"
Seminorm	Vector concept	Satisfies the first two properties of a norm but can be zero for vectors other than the origin
A normed vector space Vector concept		A vector space with a specified norm

Prediction Error 	Classification Evaluation Metric 	FP + FN/(TP + TN + FP + FN) = 1 - Accuracy
Accuracy	Classification Evaluation Metric	TP + TN/TP + TN + FP + FN, can be interpreted as a threshold point on a ROC curve, although ROC AUC and accuracy metrics usually agree with each other
Precision (Positive Predictive Value)	Classification Evaluation Metric	TP/TP + FP		Weights false positive predictions as important thereby analyzing the positive predictions	Does not consider True Negatives and False Negatives
Recall/sensitivity (True positive rate)	Classification Evaluation Metric	TP/TP + FN		Weights false negative predictions as important thereby analyzing the correct positive examples	Unfortunately it often emphasizes a higher false positive rate
Macro/micro averages 	Classification Evaluation Metric for Multi-class Classification 	The micro average of precision is the sum of all true positives / sum of all positive predictions for all k classes, where the macro average is the sum of all precisions for all k classes / k, where micro-averaging weights each prediction or instance equally, and macro-averaging weights all classes equally to evaluate classifier performance regarding the most frequent class labels, and where the weighted macro-average weights the score of each class label by the number of true instances when calculating the average, where the weighted macro average is useful when classes are imbalanced
False positive rate 	Classification Evaluation Metric 	FP/FP + TN
Specificity (True negative rate)	Classification Evaluation Metric 	TN/TN + FP
Miss rate (False negative rate)	Classification Evaluation Metric 	FN/FN + TP
False Discovery Rate 	Classification Evaluation Metric 	FP/FP + TP
False Omission Rate 	Classification Evaluation Metric 	FN/FN + TN
F1 score	Classification Evaluation Metric	2*(Precision * Recall)/(Precision + Recall)		if we increase the precision the recall decreases and vice versa so the harmonic mean of precision and recall combines these two metrics to offset that trade-off	best for uneven classes
Cross-validated F1-score	Classification Evaluation Metric			helps avoid overfitting
Mean F1 score	Classification Evaluation Metric
Confusion Matrix	Classification Evaluation Metric			Compares counts of actual vs. predicted outputs for each of N target classes to compare predictions (true/false positives and true/false negatives)
AUC (Area under the ROC curve)	Classification Evaluation Metric 		Represents degree or measure of separability (evaluates how well the model can separate the classes by analyzing the classification model at different threshold values) where a score close to 1 is ideal, 0 is the worst possible score and 0.5 indicates the model has no class separation capacity, which uses the TPR and FPR, where the ROC AUC can reflect performance of a classifier regarding imbalanced samples	Good for binary classification
Receiver Operating Characteristic ROC curve	Classification Evaluation Metric curve			A probability curve that compares true positive rate TPR (recall/sensitivity) and the false positive rate FPR and determines the model's capacity to distinguish between different classes, which are computed by shifting the decision threshold of the classifier, and is used to predict the probability of the binary outcome, where the diagonal indicates random guessing and classification models below the diagonal are worse than random guessing, where a perfect classifier would have a TPR of 1 and an FPR of 0	Good for binary classification 	Overly optimistic at estimating performance in highly skewed domains
Precision-Recall Curve	Classification Evaluation Metric		Plots the precision vs the recall for different probability thresholds of a classifier, which are recommended for highly skewed domains where ROC curves can provide an overly optimistic view of the performance	Good when the positive class is rare and in highly skewed domains
PR AUC 	Classification Evaluation Metric 		The PR AUC summarizes the curve with a range of threshold values as a single score, where a high PR AUC indicates high recall and high precision, where high precision is a low false positive rate and high recall is a low false negative rate, where the AUC ROC uses TPR and FPR and AUC PR uses positive predictive value PPV and TPR, where AUC PR is more useful if true negatives arent relevant or the positive class is relevant, but AUC ROC is useful where the positive and negative classes are relevant or the dataset is balanced
Silhouette Score	Clustering Evaluation Metric 	

Learning Curve	Function Type			Compares how well increasingly large training sets perform on the same validation or test set to evaluate if the model would benefit from more data where if the validation/test accuracy increases with training set size it indicates underfitting and that more data would be useful and where a gap in training and validation/test set accuracy indicates high variance/overfitting and where additional data can decrease both underfitting and overfitting
Validation Curve 	Function Type 	Rather than graphing test vs. training accuracy for a number of training examples, validation curves vary the model parameter values like the regularization parameter in logistic regression and plot the training and validation accuracy for different model parameter values

Hypothesis Testing	Statistical test type			Commonly used in outlier detection, includes t-test, chi-square, ANOVA
One-sample t-test 	Statistical test 		One number predicts y
Wilcoxon signed rank Statistical test 		One number predicts the signed rank of y
Paired-sample t-test 	Statistical test 		One intercept predicts the pairwise y2 - y1 differences
Wilcoxon matched pairs 	Statistical test 		One intercept predicts the signed rank of the pairwise differences y2 - y1
Pearson correlation 	Statistical test 		One intercept plus x multiplied by a number (slope) predicts y with continuous x
Spearman correlation 	Statistical test 		One intercept plus ranked x multipled by a number (slope) predicts ranked y with continuous x
Two-sample t-test 	Statistical test 		An intercept for group 1 (plus a difference if group 2) predicts y with discrete x
Welch's t-test 	Statistical test 		An intercept for group 1 (plus a difference if group 2) predicts y, with one variance per group instead of one common variance with discrete x
Mann-Whitney U 	Statistical test 		An intercept for group 1 (plus a difference if group 2) predicts signed rank of y with discrete x
One-way ANOVA 	Statistical test 		An intercept for group 1 (plus a difference if another group) predicts y with discrete x
Kruskal-Wallis 	Statistical test 		An intercept for group 1 (plus a difference if another group) predicts rank of y with discrete x
Goodness of fit 	Statistical test 	An intercept for group 1 (plus a difference if another group) and a family like the poisson family predicts y with discrete x
One-way ANCOVA 	Statistical test 		An intercept for group 1 (plus a difference is another group) plus a slope on x predicts y, with continuous x
Two-way ANOVA 	Statistical test 		An intercept plus x1 terms and x2 terms plus x1,x2 interaction terms (changing one x term changes the other x-term in an interaction term)
Chi-square test 	Statistical test 	A log(intercept) plus log(x1) terms and log(x2) terms plus log(x1,x2) interaction terms where inputs are proportions and x is discrete and a family like the poisson family

Student's t-test 	Statistical test 	T-tests are used to measure whether a given coefficient is significantly different than zero (the null hypothesis that a particular coefficient equals zero), while F tests are used to measure whether any of the terms in a regression model are significantly different from zero
ANOVA Statistical test 		Provides a statistical test of whether two or more population means are equal, generalizing the t-test beyond two means, and uses an f-test to compare the means of two groups by analyzing variance, comparing amount of variation between each group means to the amount of variation within each group, where if between-group variation is larger, it means the group means are likely different, based on the law of total variance in which the total variation in a dataset can be broken into components from different variation sources, and where the f-test of ANOVA follows the normality, homogeneity of variance, independence of errors and random sampling assumptions
ANCOVA 	Statistical test
MANOVA 	Statistical test
F-test 	Statistical test 	The test statistic in an f-test is the ratio of two scaled sums of squares reflecting different sources of variability, constructed so the statistic is often greater when the null hypothesis is false, where the sums of squares should be independent and should follow a scaled chi-squared distribution which is guaranteed if the data values are independent and normally distributed with a common variance, where the formula for one-way ANOVA f-test statistic is explained variance or between-group variability or Σ for all K ni * (sample mean in ith group Yi - overall mean Y)^2/(K - 1) / unexplained variance or within-group variability or Σ for all K Σ for all ni (jth observation in the ith out of K groups Yij - sample mean in ith group Yi)^2/(N - K) where N is the overall sample size, which follows the f-distribution with degrees of freedom d1 = K - 1 and d2 = N - K under the null hypothesis, which will be a large value if the between group variability is relatively large which is unlikely if the population means of the groups are equal	The f-test is a statistical test that compares variances to determine if the variances of two samples (or a ratio of variances among multiple samples) are significantly different, calculating the F statistic and checking if it follows and F distribution, where f-tests are frequently used to compare different statistical models and find the one that best describes the population the data came from, where when models are created with least squares the resulting f-tests are called exact f-tests, and is used in cases like to test the hypothesis that the means of a set of normally distributed population with the same standard deviation are equal, or the hypothesis that a regression model fits the data well, or the hypothesis that a data set in a regression analysis follows the simpler of two nested linear models, where multiple-comparison testing is done after completing an f-test if the f-test leads to a rejection of the null hypothesis and the factor being studied impacts the dependent variable. The result of the f-test is determined by comparing calculated F value and the critical F value with specific significance level like 5% where the f-table contains critical F values given degrees of freedom for the distribution of the f-statistic under assumption of a true null hypothesis, the f table is designed to determine the threshold beyond which the f statistic is expected to exceed a specific percent of the time (5%) when the null hypothesis is accurate, where if the F statistic < critical F value, the null hypothesis is accepted and there is no significant difference among sample averages and observed differences can be caused by randomness and the result is not significant, where if the F statistic > critical F value, the null hypothesis is rejected and there is significant difference among sample averages and observed differences in sample averages couldnt be reasonably caused by randomness and the result is significant, where when there are only two groups in one-way ANOVA f-test, F = t^2 where t is the student's t statistic, where another use of the f-test is to determine if a nested model is significantly better than the model it's nested in by comparing the residual sum of squares F = (RSS1 - RSS2/p2 - p1) / (RSS2/n - p2) where RSSi is replaced with the weighted sum of squared residuals if the regression model has been calculated with weights, where F will have an F distribution with (p2 - p1, n - p2) degrees of freedom and where the null hypothesis is rejected if F > critical value of the F distribution for a false-rejection probability like 0.05	The f-test is useful for multi-group comparison efficiency, clarity in variance comparison, versatility across disciplines, and allows not specifying which groups should be compared, instead comparing all groups	The f-test is sensitive to homogeneity of variance and non-normality so has alternative tests like Levene's, Bartlett's, and the Brown-Forsythe test and has limited scope to group comparisons specifically comparing variances between groups, and has interpretation challenges by not identifying specific group pairs with distinct variances and if the null hypothesis is rejected, it's not clear which groups significantly differ, nor can it be said that the group pair with the greatest mean difference is significantly different at the level alpha which the f-test was performed at
Likelihood ratio test 	Statistical test 		F is a monotone function of the likelihood ratio statistic so the F-test is a likelihood ratio test
Degrees of freedom 	Statistical test variable 		The number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary

Greedy Algorithms 	Algorithm Type 	Greedy Algorithms make locally optimal choices at each stage of a combinatorial search problem and usually create a suboptimal result, in contrast to exhaustive search algorithms which try all possibilities
Feature Extraction Methods	Function Type 	Feature extraction derives info to compress a dataset into a lower dimensional subspace, including methods such as PCA and LDA 	Improves storage space and computational efficiency of a learning algorithm and can improve predictive performance by reducing the curse of dimensionality especially when working with non-regularized models
Feature Selection Methods 	Function Type 		Feature selection methods reduce features and have three main types: Filter, wrapper, and embedded feature selection
Sequential feature selection methods	Feature Selection Method Type 	Greedy search algorithms designed to reduce a d dimensional feature space to a k dimensional feature subspace, with the intent of selecting relevant subsets of features, improving computational efficiency, or reduce the generalization error by removing irrelevant features or noise, which is useful for algorithms that dont support regularization
Sequential Backward selection 	Sequential feature selection type 	Reduces dimensionality of the feature space with a minimum decay in the performance of the classifier to improve computational efficiency, which can improve the predictive power of a model in cases of overfitting, by sequentially removing features based on a criterion like which features cause the least performance loss after removal, until the target number of features is reached
Recursive Backward Elimination 	Feature Selection Method 	Recursively eliminates features based on feature weights
Filter Methods	Feature Selection Method			Evaluate each feature independently with target variable where features with high correlation with target variable are selected as it means this feature has some relation and can help make predictions where these methods are used in the preprocessing phase to remove irrelevant or redundant features based on statistical tests (correlation) or other criteria	Quickly evaluate features without training the model and good for removing redundant or correlated features and Filter methods are often preferred for very large datasets due to their speed	These methods don't consider feature interactions so they may miss feature combinations that improve model performance
Chi-square test	Feature Selection Filter Method			It is generally used to test the relationship between categorical variables. It compares the observed values from different attributes of the dataset to its expected value.
Fisher’s Score	Feature Selection Filter Method			Fisher's selects each feature independently according to their scores under Fisher criterion leading to an optimal set of features so a larger the Fisher’s score indicates a better selected feature
Pearson’s Correlation Coefficient	Feature Selection Filter Method			pearson's coefficient is a measure of quantifying the association between the two continuous variables and the direction of the relationship with its values ranging from -1 to 1
Variance Threshold	Feature Selection Filter Method			variance threshold is an approach where all features are removed whose variance doesn’t meet the specific threshold and by default this method removes features having zero variance and the assumption made using this method is higher variance features are likely to contain more information
Mean Absolute Difference	Feature Selection Filter Method			Mean absolute difference is a method is similar to variance threshold method but the difference is there is no square in this method
Dispersion ratio	Feature Selection Filter Method			It is defined as the ratio of the Arithmetic mean (AM) to that of Geometric mean (GM) for a given feature whose value ranges from +1 to infinity as AM ≥ GM for a given feature where a higher dispersion ratio implies a more relevant feature

Wrapper Method	Feature Selection Method			Wrapper methods are greedy algorithms that train algorithms using different combinations of features and computing relations between these subset features and the target variable and add/remove features with stopping criteria being potentially whenever the model performance decreases or a specific number of features is achieved	Can lead to better model performance since they evaluate feature subsets in the context of the model and can capture feature dependencies and interactions	But are computationally more expensive than filter methods especially for large datasets
Forward selection	Feature Selection Wrapper Method			Forward selection is an iterative approach where we initially start with an empty set of features and keep adding a feature which best improves our model after each iteration where the stopping criterion is where the addition of a new variable does not improve the performance of the model
Backward elimination	Feature Selection Wrapper Method			Backward elimination is a method that is also an iterative approach where we initially start with all features and after each iteration we remove the least significant feature where the stopping criterion is where no improvement in the performance of the model is observed after the feature is removed
Recursive elimination	Feature Selection Wrapper Method			Recursive elimination is a greedy method that selects features by recursively removing the least important ones where it trains a model and ranks features based on importance and eliminates them one by one until the desired number of features is reached

Embedded Method	Feature Selection Method			Embedded methods perform feature selection during the model training process and combine the benefits of both filter and wrapper methods and include Lasso regularization, decision trees, random forests, and gradient boosting which all select important features	 where feature selection is integrated into the model training allowing the model to select the most relevant features based on the training process dynamically	More efficient than wrapper methods because the feature selection process is embedded within model training and is often more scalable than wrapper methods and Wrapper and embedded methods are better for capturing complex feature interactions	Works with a specific learning algorithm so the feature selection might not work well with other models

Loss/error function	Function Type			The loss function Loss(a(x),y) defines the cost of making decision a(x) when the true state is y for a single example
Cost function	Function Type	1/n * Σ for all n Loss(actual yi, predicted yi) 	An average or total of the loss function for an entire training set containing several training examples thereby quantifying the model's performance on the whole training dataset, the expected value of the loss function across the data
Objective function 	Function Type 	The objective function refers to any function being optimized/minimized/maximized during training, such as the cost function (error term), regularization terms, complexity, sparsity, or accuracy
Risk function	Function Type			The risk function combines the loss function and the decision rule and the probabilities where the risk of a decision rule a(x) is the Σ for x,y of the (expected loss(a(x),y) * probability P(x,y)) where the decision rule that minimizes the risk is selected

Cross validation	Function Type		Some part of the dataset is reserved for testing the model. There are many types of Cross-Validation out of which K Fold Cross Validation is mostly used which is used for model tuning, cross validation estimates the variance of the generalization performance
K-fold cross validation	Cross Validation and Generalization Method			The original dataset is randomly divided into k subsets (folds) without replacement and this is repeated k times where 1 fold is used for testing purposes and the rest k-1 folds are used for training the model which generalizes the model well and reduces the error rate, where the performance on each fold is averaged once all folds are trained, which produces k models and k performance estimates, and after k-fold cross validation, retraining the model on the complete training dataset provides more training examples once the optimal hyperparameters are identified using k-fold cross validation, where k = 10 offers a good bias-variance tradeoff for normal sized data sets, although larger values of k are optimal for smaller datasets bc more training data is used in each iteration with a higher k value which reduces bias, however higher k values increase variance by selecting subsets that are similar to each other, where smaller values of k are appropriate for larger data sets, where leave-one-out cross validation which sets k = n and uses one example for testing during each iteration is appropriate for small datasets, where k-fold cross validation is often used with grid search to identify optimal hyperparameter values, where nested cross-validation is useful for selecting between different algorithms 	More cross validation folds reduces the chance of overfitting, and since its a resampling technique without replacement, each example is used once for training and validation which creates a lower-variance estimate of model performance than holdout
Stratified cross-validation 	Cross Validation Method 	Can create better bias and variance estimates especially with unequal class proportions, where class label proportions are preserved in each fold so each fold is representative
Holdout cross-validation	Cross Validation and Generalization Method			The dataset is divided into ratios like 80:20 of train/test or train/validation/test datasets and is used in neural networks and many classifiers, where the validation dataset is better to use for model selection (hyperparameter tuning) than the test dataset to avoid overfitting by performing model selection on the test dataset 		Has the disadvantage that the performance estimate is sensitive to how the dataset is partitioned, so different subsets of the dataset will produce different results, which k-fold cross validation attempts to solve by repeating the holdout method on k subsets k times
Nested cross validation 	Cross Validation Method 	Used to select between different machine learning algorithms, where n outer x k inner cross-validation involves an outer k-fold cross validation loop of n folds to split into training and test folds, and an inner k-fold cross validation loop of k folds to select the model using k-fold cross validation on just the training fold selected by the outer loop, where the test fold is used to evaluate model performance after model selection
Adversarial Validation	Validation Type	

Hyperparameters	Variable Type
Learning Rate	Hyperparameter			Learning rate determines the jump interval for gradient descent
Batch size 	Hyperparameter
N estimators 	Random forest hyperparameter 		Number of trees in a random forest
Max depth 	Decision tree hyperparameter 		Maximum number of levels in a decision tree
Minimum samples split 	Decision tree hyperparameter 		Minimum number of data points in a node before the node is split
Minimum samples leaf 	Decision tree hyperparameter 		Minimum number of data points allowed in a leaf node
Max features 	Decision Tree hyperparameter 		Maximum number of features considered in a decision tree when splitting nodes
Bootstrap  	Decision Tree hyperparameter 		Method for sampling data point with or without replacement

Hyperparameter Tuning	Function Type			Process of optimizing model parameters other than data parameters (like learning rate	 number of trees).
Grid search	Hyperparameter Tuning 	Grid search checks every combination of hyperparameters  	Grid search is exhaustive, checking every value within specified ranges	However, grid search only checks within the specified hyperparameter ranges and is computationally expensive and can only check discrete values
Random Search	Hyperparameter Tuning 	Random search checks a random subset of combinations of hyperparameters, either sampling with or without replacement 	Random search can increase hyperparameter ranges without increasing the number of checks tested	Random search like grid search only checks within the specified hyperparameter ranges and can only check discrete values
Bayesian Optimization	Hyperparameter Tuning 	Applies informed changes to hyperparameters checked, based on information from previous checks 	Can reach the optimal configurations quicker than grid or random search 	Mostly only useful for large models since smaller models can be checked quickly with grid search
K-fold cross validation with hyperparameter tuning 	Hyperparameter Tuning 	Its useful to combine k-fold cross validation with hyperparameter search methods, performing k-fold cross validation with each hyperparameter configuration for can example with grid search

Data Augmentation	Function Type 		Can involve scaling, cropping, flipping, or rotating image data or applying changes like random noise or dropping random values to data in general 	Helps generalize and generates new data records/features based on existing data and makes it harder to memorize irrelevant information like over-specific information or noise via training examples or features
Mixup	Data Augmentation and Generalization and Regularization Method 	Randomly mixes data like by overlapping images 	Mixup increases data diversity 	However Mixup also can create blurred images 
Cutout	Data Augmentation and Generalization and Regularization Method 	Randomly removes square sections of images during training 	Cutout can remove noise, making models more robust 	However cutout can also remove important features especially in sparse images	
CutMix	Data Augmentation and Generalization and Regularization Method 	Randomly mixes a subset of input images and mixes labels according to the proportion of the image subsets 	Cutmix increases data diversity 	However cutmix also can create unrealistic images and can remove important features especially in sparse images

Model Interpretability & Explainability	Function Type			Understanding how a model makes decisions
SHAP (shapley additive explanations)	Model Interpretability	The shapley value for feature i is: φᵢ = Σ [|S|!(M-|S|-1)!/M!] × [f(S ∪ {i}) – f(S)] where S represents all possible subsets of features excluding feature i, and f represents the model’s prediction function 	SHAP calculates feature importance by considering all possible combinations of features and measuring how much each feature contributes to the difference between the current and average prediction, ensuring that the sum of all SHAP values equals the difference between the individual and average prediction, where features that contribute equally have equal SHAP values, positively/negatively contributing features have positive/negative values, and features that dont contribute have zero SHAP values and SHAP values can be computed by summing values from individual components 	SHAP is rigorous, has consistent explanations, integrates global and local info, is efficient by optimizing for model types, enables rich visualizations and is additive and is better for complex models 	However SHAP is computationally complex, has errors from approximations, is sensitive to the choice of baseline value, doesnt handle correlated features well, and requires careful selection of datasets
LIME (local interpretable model-agnostic explanations)	Model Interpretability 	Lime explains individual predictions by learning an interpretable model locally around the prediction by perturbing relevant input features around a specific instance and observing how predictions change, first perturbing data, then collecting predictions, then assigning higher weights to closer samples, then training a local model using algorithms like linear regression, then generates explanations based on coefficients of the local model 		LIME is useful bc of its model agnosticism, instance-specific explanations, intuitive output, flexible data type handling, and local fidelity, accurately approximating model behavior in the local neighborhood of the explained instance 	However LIME is unstable/volatile/inconsistent with small input changes leading to very different explanations due to random sampling and is dependent on sampling strategy used for perturbations, uses and reflects limited global info, is computationally expensive and is sensitive to hyperparameters and is better for simpler models
Feature importance plots	Model Interpretability
DeepLIFT	Deep learning interpretability method
Grad-CAM	Deep learning interpretability method
Integrated Gradients	Deep learning interpretability method

Activation functions	Function Type			Specifies whether a neural network node should be activated thereby allowing that set of weights to contribute to the model
Step	Activation function 	1 if x >=0 or 0 if x < 0
Sigmoid 	Activation Function and Squashing Function 	1/(1 + e^-x)	Squashing functions limit output to a range of 0 to 1 which is useful when predicting probabilities	The sigmoid function is useful for binary classification problems because its outputs are values between 0 and 1 and is non-linear so it improves on the step function 	However the sigmoid function is less useful for multi-class classification bc it doesnt normalize outputs to sum to 1 across multiple classes and has the vanishing gradient problem with large or small inputs bc the gradient approaches zero for large or small values
Tanh	Activation function 	tanh(x) = (e^x - e^-x)/(e^x + e^-x) = sinh(x)/cosh(x)	Tanh is an s-curve similar to sigmoid but has outputs ranging from -1 to 1 	Tanh introduces non-linearity which allows learning complex patterns and has output centered around zero which leads to more efficient training and faster convergence bc the mean of the output is closer to zero so its useful for data already centered around zero or where negative input values are significant and should be retained and for shallow neural networks and tanh helps reduce the vanishing gradient problem compared to sigmoid activation bc the gradient of the tanh function is usually higher than the gradient of the sigmoid which enables better weight updates during backpropagation and tanh is differentiable so it can be used with gradient descent 	Like sigmoid, tanh is also not typically used for multi-class problems bc it doesnt handle probability distributions and tanh can also have the vanishing gradient problem with large or small inputs bc the gradient approaches zero for large or small values, just like the sigmoid function and tanh is sensitive to outliers in input
ReLU	Activation function 	f(x) = max(0,x) 		ReLU is often used in hidden layers of deep networks bc of its simplicity and efficiency and doesnt have the vanishing gradient problem 	However ReLU doesnt convert logits to probabilities so its not used in output layers in classification tasks and creates dead neurons from receiving only negative inputs which stops learning for those neurons and ReLU doenst treat negative and positive values equally which can slow down learning and exploding activations can become too large for large positive inputs, which are problems leaky ReLU is designed to solve
Leaky ReLU 	Activation function 	x if x > 0 and 0.01 * x if x <= 0 	Leaky ReLU is designed to fix the problem of dead neurons so it doesnt return a zero value for negative inputs so neurons wont become inactive during training 	Leaky ReLU prevents dead neurons by allowing a small nonzero gradient for negative inputs and improves gradient flow during backpropagation and is faster and more stable compared to ReLU and is useful in deep networks where ReLU can have problems
Softmax 	Activation function 	softmax(zi) = K * e^zi / ∑ for all K * e^zj, where zi is the logit (output of previous layer in network) for the ith class, K is the number of classes, e^zi is the exponential of the logit and ∑ for all K e^zj is the sum of exponentials across all classes = number of classes * exponential of the logit / sum of exponentials across all classes, where the e^zi term increases the difference between logits so that even small increases in logits lead to higher probabilities, while small logits have near-zero probabilities, and where the sum of the exponentials normalizes the values into probabilities, and where softmax and cross-entropy loss are often used together bc the cross-entropy loss can compare the predicted probability distribution resulting from softmax with the true label and cross-entropy loss can penalize the network if the predicted probability for the correct class is low  		Softmax helps transform logits (raw prediction scores) into probabilities of each class being the correct prediction, these probabilities being distributed across classes so that their sum is 1, and softmax is typically applied in the final output layer and is useful for multi-class classification tasks and softmax is differentiable so can be integrated into the backpropagation algorithm involving gradient descent 	However softmax is sensitive to outliers and noise bc it increases differences between logits so large differences like those found in outliers and noise can determine the output and small probabilities can cause small gradients during backpropagation which slows down learning and softmax can assign high probabilities to incorrect classes and requires exponentiation and normalization which is computationally expensive and is not suited to cases where data can belong to multiple classes

Batch normalization (BatchNorm)	Reducing overfitting and regularization and layer input normalization technique		Addresses the problem of internal covariate shift in neural networks by normalizing the data in each mini-batch, by calculating the mean and variance in a batch and then adjusting values to have a similar range, then scaling and shifting the values so the model learns effectively and the inputs to each layer are in a stable range even if outputs of earlier layers change during training, where the normalized activation xi = (non-normalized xi - the mean) /	√ (square root of variance + error term to avoid division by zero), then the normalized activations are scaled by a learnable parameter gamma and shifted by another learnable parameter beta yi = gamma * normalized xi + beta 	Speeds up and stabilizes training and generalizes and allows using higher learning rates and helps avoid vanishing/exploding gradients and can act like a regularizer thereby reducing the need for dropout, allows faster convergence during training
Layer normalization (LayerNorm)	Reducing overfitting and regularization and layer input normalization technique			Stabilizes training and regularizes and generalizes
Weight normalization	Reducing overfitting and normalization			normalizes the model weights instead of layer inputs	generalizes and regularizes indirectly
Feature normalization 	Normalization 		Feature normalization is required for L1 and L2 regularization but not required for linear regression without regularization, and is useful where additional stability is required

Internal covariate shift 	Problem Type 		As input data propagates through the neural network, the distribution of each layer's inputs changes which can slow down the training process, which batch normalization is designed to solve by normalizing inputs of each layer
Data/Covariate shift	Problem Type			Distribution of x input data changes so detect covariate shift with adversarial validation and fix covariate shift with importance weighting (assign different weights to training examples to emphasize certain examples during training to increase the weight of examples likely to be in the test distribution)
Label/Prior probability shift	Problem Type			y class label distribution changes so update the model by adjusting the weights of the weighted loss function according to the new distribution if the new distribution of labels is known which is a type of importance weighting that incentivizes prioritizing certain classes that have become more or less common in the new data
Concept/Conditional shift	Problem Type			the conditional p(y|x) distribution has changed which requires continuous monitoring and model retraining
Domain/Joint shift	Problem Type			p(x) and p(y|x) both change so its a combination of covariate and concept drift and implies label p(y) shift as well unless the change in p(x) offsets the change in p(y|x) which is detected by monitoring model performance and data statistics and is fixed by collecting more labeled data from the target domain and retraining or adapting the model

Overfitting	Problem Type			occurs when the model fits the training data too closely indicating high variance (as opposed to high bias) and learns noise and outliers rather than the pattern so performs well on training data but not on new or test data and is fixed by data augmentation like Mixup/Cutout/CutMix and collecting more data after plotting a learning curve to detect if more data is beneficial and use self-supervised learning to pretrain on large unlabeled datasets to reduce overfitting on small datasets and use transfer learning from highly relevant large labeled datasets and use few-shot learning if additiona labeled data is not feasible and use feature engineering and normalization and include adversarial examples and label/feature noise and label smoothing and smaller batch sizes and regularization techniques like dropout and weight decay and decreasing model size and capacity and building ensemble models
Label smoothing
Regularization	Function Type			penalizes complexity by adding a penalty term representing the weights size to the optimizer or the loss function that is minimized during training, regularization does not apply to decision trees or k-nearest neighbors
Dropout	Reducing overfitting			reduces overfitting by randomly setting some activations of hidden units to zero during training so those neurons cant be relied on and more neurons are used to create multiple independent representations of the same data
Weight Decay	Reducing overfitting			similar to L2 regularization but is applied to the optimizer directly rather than modifying the loss function which has the same effect as L2 regularization
Early Stopping	Reducing overfitting			monitor performance on a validation set during training and stop training when performance on the validation set starts to decline (when the validation and training set performance are the most similar which is the point with the least overfitting and which is a good point for early stopping of training iterations)
Smaller models	Reducing overfitting			because the smaller the number of model parameters the smaller its capacity to overfit to noise choosing smaller models with reducing layer count/width and pruning and knowledge distillation is good for reducing overfitting		however double descent and grokking indicate that larger overparameterized models have good generalization if they are trained beyond the point of overfitting
Pruning	Reducing overfitting with smaller models			iterative pruning trains a large model and then iteratively removes parameters of the model and retraining it so that it maintains the original performance	improves generalization of the training process as it involves more extended training periods and a replay of learning rate schedules
Knowledge Distillation	Reducing overfitting with smaller models			knowledge distillation transfers knowledge from a supervised teacher model (trained with cross-entropy loss between predicted and actual outputs) to a smaller student model (which is trained on the same dataset with the objective of minimizing cross entropy between predicted and actual outputs as well as the difference between student and teacher outputs measured with Kullback-Leibler divergence)
Double Descent	Generalization Observation			models with small or very large parameter counts have good generalization performance while models with parameter counts equal to number of training data points have poor generalization performance
Grokking	Generalization Observation			as the size of a dataset decreases the need for optimization increases
Large parameter count requirements	Generalization Observation	models with a larger number of parameters require more training data to generalize well.  		
Ensemble models	Reducing overfitting			combine predictions from multiple models to improve the overall prediction performance like in random forests and gradient boosting	generalize well	increased computational cost so neural networks are less suitable for ensemble methods
Majority voting	Ensemble method			Train k different classifiers and collect the predicted class label from each of these k models for a given input and return the most frequent class label as the prediction where ties are resolved using a confidence score or randomly picking a label or picking the class label with the lowest index, where weighted majority voting can be computed as yhat predicted class label = argmax ∑ for all j weight j for classifier j * characteristic/indicator function which returns 1 if the predicted class of the jth classifier Cj(x) is i, which is yhat = mode(C1(x) ... Cm(x)) for equal weights, where using predicted class probabilities instead of class labels for majority voting can be useful if the classifiers are well calibrated which would take the form of yhat = argmax ∑ for all j weight j * predicted probability of the jth classifier for class label i 	Can combine different models
Stacking	Generalization and Ensemble method for Classification/Regression			a more advanced variant of majority voting that trains a new meta model to combine predictions of other models	can combine different models like a SVM	 MLP and a KNN
K-fold ensemble model	Ensemble method			after building models using k-fold cross-validation	 compute the average performance across all k iterations to estimate the overall performance of the model then combine the individual k models as an ensemble (or train the model on the entire training dataset rather than the k k - 1 subsets) as a majority vote classifier or a stacked ensemble model
Skip-connections	Reducing overfitting with model modifications			used in residual networks
Look-ahead optimizers	Reducing overfitting with model modifications
Stochastic weight averaging	Reducing overfitting with model modifications
Multitask learning	Reducing overfitting with model modifications
Snapshot ensemble	Reducing overfitting with model modifications

Class imbalance 	Problem Type 	High accuracy can be achieved with imbalanced data sets by predicting the majority class, so other metrics than accuracy are useful to apply when analyzing classifiers for imbalanced data sets, where algorithms may optimize for the majority class in the data set, where methods to handle imbalanced data include 1. penalizing wrong minority predictions more heavily, 2. upsampling the minority class and downsampling the majority class, 3. generate synthetic training examples 
Curse of dimensionality	Problem Type 		Phenomena that occur in high dimensional spaces related to increases in sparsity with increases in dimension, resulting in increased data requirements and where high dimensional spaces make all data seem different resulting in difficulty identifying similarities to organize it
Vanishing Gradient problem	Problem Type		Phenomenon where early layers have smaller weights than later layers due to increased numbers of multiplications during backpropagation, which can introduce instability in training or slow/halt training, occurring mostly in deep networks and specifically recurrent neural networks which use many layers	A solution for RNNs could be a LSTM network, gradient clipping to restrict the gradients within a radius, batch normalization, a multi-level hierarchy of networks pre-trained one level at a time, deep belief networks, residual connections, the ReLU activation function, or weight initialization where distribution of initial weights vary according to activation function used
Bias vs. variance trade-off	Problem type 		In supervised learning, bias error occurs by approximating a simpler model, where the learning algorithm has incorrect assumptions causing it to miss general patterns in data by underfitting, where variance error results from sensitivity to small changes in inputs which can model the noise rather than the general patterns in data by overfitting to noise, where accuracy is a way to quantify bias which can be improved by selecting local info and precision quantifies variance which can be improved by selecting info from a larger space compared to bias/accuracy and which can be optimized or smoothed with regularization to remove excess neighboring info, where high bias can be handled by gathering/generating more features, and decreasing the degree of regularization, where high variance can be handled by gathering more data, reducing model complexity, decreasing the number of features with feature selection/extraction, or increasing the regularization parameter
Data leakage	Problem Type			Data leakage happens when a model has access to info during training that it wont have access to when making predictions
Model collapse 	Problem Type 		Model collapse refers to the phenomenon where a model's performance degrades due to errors from training on uncurated synthetic data output by another model, occurring as a result of learning errors, sampling errors, or functional approximation errors
Bayes error rate 	Problem Type Concept	Bayes error (reproducible error or the minimum possible error) is a combination of latent variables, unmeasurable features, and noise
Bayesian vs. frequentist choice 	Problem type 	The result of bayesian approach can be a probability distribution for what is know about parameters given experiment results, where the result of a frequentist approach is either a decision from a significance test or a confidence interval, and where in a frequentist approach, unknown parameters are usually considered fixed rather than being random variables where a bayesian approach allows probabilities to be associated with unknown parameters, where these probabilities can have a frequency probability interpretation as well as a bayesian interpretation, where the bayesian approach allows these probabilities to represent a belief that given values of the parameter are true, and where bayesian probability treats probability as "certainty", where frequentist treats probability as the "limit of its relative frequency of occurrences of an event in an infinite number of experiment reppetitions" and frequentist inference violates the likelihood principle and where frequentist inference identifies different tail ends of distributions as indicating different levels of statistical significance of the same data with different assumed probability distributions, a difference that does not occur in bayesian inference

Binomial distribution 	Probability Distribution	The probability of getting exactly k successes in n independent Bernoulli trials (with the same rate p) is given by the probability mass function (n choose k) * p^k * (1 - p)^(n-k) with mean np and variance np * (1 - p) 	The binomial distribution is used to calculate probabilities for a process where only one of two possible outcomes may occur on each trial, such as coin tosses, and is a bernoulli distribution when n = 1
Bernoulli Distribution 	Probability Distribution 
Hypergeometric distribution Probability Distribution	The pdf is px(k) = PR(X = k) = (K choose k) * (N - K choose n - k)/(N choose n) with mean n * (K/N) and variance n * (K/N) * (N - K/N) * (N - n/N - 1)		The hypergeometric distribution is used to find the probability of k successes in n draws without replacement where both the hypergeometric distribution and the binomial distribution describe the number of times an event occurs in a fixed number of trials and the probability remains the same for every trial for the binomial distribution and in contrast in the hypergeometric distribution each trial changes the probability for each subsequent trial because there is no replacement
Poisson distribution 	Probability Distribution	The pdf is f(k|lambda) = ((e^-lambda) * (lambda^k))/k! where the mean is lambda and the variance is lambda	The poisson distribution can be used to measure the probability that a given number of events will occur during a given time frame such as the count of library book checkouts per hour where Poisson regression is used when the target variable represents count data (positive integers) and the data is Poisson distributed which means that the mean and variance are roughly the same, and for large means we can use a normal distribution to approximate a Poisson distribution
Geometric distribution 	Probability Distribution	The pdf indicates the probability that the kth trial is the first success is Pr(x = k) = (1 - p)^(k -1) * p where the mean is 1/p and the variance is (1 - p)/p^2		The geometric distribution can be used to determine the probability that a specified number of trials will take place before the first success occurs
Exponential Distribution 	Probability Distribution 	The pdf is f(x|lambda) = lambda * e^(-lambda * x) where the mean is 1/lambda and the variance is 1/(lambda^2)
Uniform Distribution 	Probability Distribution 	The pdf is f(x|a,b) = 1/(b-a) for x between a and b and 0 for x < a or x > b where the mean is (a + b)/2 and the variance is (b - a)^2/12
Gaussian Distribution 	Probability Distribution 	The pdf is f(x|mean, variance) = (1/√2 * pi * variance) * e ^ (-1/2 * (x - mean/standard deviation)^2)
Beta Distribution 	Probability Distribution 
Gamma Distribution 	Probability Distribution 
F distribution 	Probability Distribution 	The f-distribution is the distribution of X = (U1/d1) / (U2/d2) where U1 and U2 are independent random variables with chi-square distributions with degrees of freedom d1 and d2 where the pdf is √((d1 * x)^d1 * (d2^d2) / (d1 * x + d2)^(d1 + d2)) / x * beta function B(d1/2, d2/2) with mean d2/(d2 - 2) for d2 > 2 and variance is (2 * d2^2 * (d1 + d2 - 2)) / (d1 * (d2 - 2)^2 * (d2 - 4)) for d2 > 4
Student's t distribution 	Probability Distribution
Chi-squared distribution 	Probability Distribution
Pearson distribution
Laplace distribution
Fisher's z-distribution


SMOTE (synthetic minority over-sampling technique) 	Synthetic data generation method
Connected Components	Graph Algorithms
Shortest Path	Graph Algorithms
Pagerank	Graph Algorithms
Centrality Measures	Graph Algorithms
StandardScaler
SARIMA
ARIMA
Bidirectional Encoder Representations from Transformers (BERT)
H2O AutoML
Manifold Learning
FastAI
Retrieval Augmented Generation
Diffusion Models
Residual Networks/skip connections
Knowledge Graphs
Pointer Networks
Attention
FlashAttention
Energy
Inductive Bias
Padding and Packing
F-beta score
Chain Rule
Xavier Initialization


Sources
https://aman.ai/primers/ai/
https://medium.com/coders-camp/40-machine-learning-algorithms-with-python-3defd764b961
https://sebastianraschka.com/books/ml-q-and-ai/#table-of-contents
https://github.com/Tanu-N-Prabhu/Python/tree/master/Machine%20Learning%20Interview%20Prep%20Questions
https://machinelearningmastery.com/plot-a-decision-surface-for-machine-learning/
https://www.cs.jhu.edu/~ayuille/courses/Stat161-261-Spring14/RevisedLectureNotes2.pdf#:~:text=Hence%20the%20form%20of%20the%20decision%20rule%20%28i.e.	is%20at%20xML%20%3D%201%3D2%28%201%20%2B%201%29.
https://www.geeksforgeeks.org/machine-learning/decision-tree/
https://mljourney.com/ml-model-explainability-shap-vs-lime/
https://www.markovml.com/blog/lime-vs-shap
https://blog.dailydoseofds.com/p/grid-search-vs-random-search-vs-bayesian
https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74/
https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf
https://github.com/alexeygrigorev/data-science-interviews/blob/master/theory.md
https://tomasmiskov.com/kernel-trick.html
https://mlu-explain.github.io/linear-regression/

To do list:
- sort by type (regression, machine learning algorithm, probability distribution, dimensionality reduction method, function type, regression evaluation metric, concept, problem type, observation, ensemble method, etc)
- consolidate columns (math function and parameter explanation in the same column, supervised in the type column, loss function in the math function column)
