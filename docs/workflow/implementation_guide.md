# Implementation Instructions

This is the general blueprint for implementing the ideas in this repo, which have more specific blueprints in their own guides.
Use case & tech debt is included where applicable.


## Object Model

	- this is a simple standard information format

	- uses:

		- predict interactions & optimal versions of objects/attributes/types/rules
			- predict emergent objects/attributes/types/rules

		- problem-solving automation method to query objects/attributes/types/rules
		- reduce solution space or identify causative factor in problem

	- tech debt:

		- identify object data sources (code bases defining schema/class definitions, network maps)
		- implement data sanitization & import
		- implement identification functions (objects/attributes/types/rules) to gather more data pre-indexed
		- implement object operation functions (combine, merge, apply, mix, filter)
		- implement object function set (change functions, boundary functions, probability functions)


## System Analysis

	- format accessible once information is standardized to object model
    - what known/potential inputs/outputs available in the system could build the path?
    - "what is the function linking these variables, given the core functions used to build this system?"
    - where are the problem types (gaps, misalignments, mismatches & conflicts)?

	- observes not just rules between objects, but other key system data like:

		- variance (gaps in rule enforcement) & variance sources (gaps in system boundary allowing variance from other systems to leak in)
		- emerging objects
		- system errors & error-handlers
		- vertices (factors that generate or influence the system development)
		- efficiencies, alternatives, & incentives (forces with a built-in reason lending it extra momentum/gravity pulling agents in that direction)
		- interface metadata (intent, pattern, types, function, etc)
		- relationship metadata (related systems, system position in system interface network)


## Intent Analysis

	- indexing objects in a system by intent allows for quick optimization
	- this kind of analysis is useful for finding bugs
		- if a function's lines are indexed by intent, it's clearer when an intent has already been handled or when there's a reversal or gap in logical flow
    - what intents/directions/priorities does this path align with or could be built from?
    - optimization is an example of a general intent, with a specific intent of 'optimizing the metric in question'
    - "what is the function linking these variables, given the variable intents a, b, c and the combination intent matrix ab, bc, ca, and the possible output intents of that matrix, and similarity to output intent of y"


## Pattern Analysis

    - what would the path between inputs/output be, given patterns of other paths?
    - "what is the function linking these variables, given common function patterns between variables of these types/topics/ranges/other metadata?"


## Function Analysis

    - are there multiple directions to approach function derivation from?
      - can patterns in metadata-generators (difference generators) be used to derive the function generators (core functions)?
      - does it loop around to the start, just like lowest math/structural interface (symmetry, equivalence) loops around to highest conceptual interface (balance, power, random)?
      - the shape of the interface nexus should be a circuit feeding itself 
        (fractal inputs to itself, where input interfaces are similar in that they can both generate & be generated by the current interface)
      - this means you can select an optimal interface to start from & a direction to navigate in on the standard order of interface traversal, and an optional interface-organizing metric (like difference) to order interfaces to traverse


## Logic Analysis

	- can include related objects of logic like patterns, functions, cause, and change
	- automating the selection, position, optimization, & implementation of logical rules is possible with this analysis

	- use cases:

		- if the following code appears in this order:

			if variable1 is None:
				return False
			return operation(variable1)

			- variable1 is not checked for False (theres a gap in enforcement between the None & False definitions) so the operation could fail

			if variable1 <= 0:
				return False
			return int(variable1)

			- theres a potential gap in enforcement of data type, where variable1 might not be an integer even if its positive

			if not variable1:
				return False
			if variable1:
			
			- there's an unnecessary condition which is invalidated by prior code (if variable1 is not defined, it would never get to the third line, so the third line is unnecesary)

	- tech debt:

		- analysis & classification of known bug types

		- function to identify new bug types

		- function to identify known bug types in logic
			- gaps in logic enforcement (variance gaps, assumptions)
			- overlapping/repeated logic checks (extraneous validation)
			- side effects that dont match function intent

		- function to identify logic bug resolution methods:
			- bug 'overlapping logic' has bug resolution method:
				- identify isolated logic operations
				- identify scope required of each operation
				- identify required position of each isolated logic operation


## Structural Analysis

	- indexing objects by structure allows clear matching of useful structures to objects/attributes/types/rules
	- this allows objects to be graphed in a standard way, which means translating objects like problems into a computable interface

	- use cases:

		- which objects are chained (cause, risk, variance, errors, gaps, limits)
		- which are dimensions (isolatable attributes of change patterns)
		- which have position
		- that a type stack (which type values on different type layers) and a network/tree (type hierarchy) are useful structures to capture type relationships
    
    - identify shape: chain/stack/network/mix/layer, adjacent shapes, emergent info shapes like alignments/symmetries/gaps/conflicts
    - identify compression functions of shape, given target dimensions
    - identify conversion functions of shape given source/target shape
    - when choosing between nodes & links to model objects in a network:
      - node: many connections to many other objects having a similar property, like having a type in common, usually unique
      - links: usually many connections between two objects at a time, having many possible variations, can be repeated

    - example of structural analysis by applying a particular structure:
      - market analysis
        - the market interface is a standard interface where resources (goods, labor, time, risk, information, expectations, theories, & stored prior labor (currency)) are traded
        - a useful new way to use this is to frame non-resource objects as resources (systems, structures, positions, paths, directions, risk chains, trade loops, markets)
        - then you can apply traditional market analysis (optimal transport) to find, for example, the optimal set of trades to change an industry's position


## Potential Analysis

	- variance is semantically the opposite index (gap/unenforcement/missing information/randomness) to the filter index (limit/structure/information/organization)
    - includes certainty objects (known variance, metrics, change patterns), uncertainty (potential/risk/opportunity) objects, and variance structures like cascades, injection points, accretion points
    - delegation of variance into systems/types/functions/variables/constants

    - example of variance type analysis (difference):

      - what type of variable is it? (object-differentiating/identifying attribute, emergent specific/abstract property, direct function input/output)
      - how does the variable relate to other variables? (decisive metric, substitutable alternative, collinear)
   	  - at what point does a variable become relevant to another variable interaction layer?
      - how do constants accrete between rules, like caps to keep variance from flowing in to corners or creating boundary-invalidating openings in a system/component boundary?
      - what causes variables to cascade across layers, creating fractal variables?
      - what is the path definitely not, based on various maximized measures of similarity?
      - what attributes & attribute sets & attribute dependency trees differ
      - what is transformation cost/potential between objects
      - what is divergence distance between generative paths for each object
      - example: "what is the probable function linking these variables, given that it is an adjacent transform of a square (related function type), & a distant transform of a manifold (unrelated function type)?"
      
	- use case:

		- this is the problem of adding/fitting/reducing structure from a gap in structure, which can be used to solve problems like:

			- prediction
				- which variables are explanatory, given what we can measure

			- causation
				- how alternatives can converge to the same level of variance or change patterns

		- reducing gaps in rule enforcement to shapes or paths has its own set of rules

		- this interface can also be used for specific attribute analysis, of properties that descend from concepts & take form in a specific problem space:
			- the power concept interface (has implementations that look like trust, info, etc)
			- the balance concept interface (has implementations that look like symmetry, justice, etc)


## Change Analysis

	- this regards the potential to break down & format a problem into many different combinations of solved problems (optimal transport, linear algebra, finding prediction function, etc) or known interfaces (type, intent)
	- some sets are more adjacent than more optimal sets & may be a better investment for short-term gains

	- example:
		- when approximating area of an object that is similar to a square but has a convex arc instead of a side (like an opened envelope), it may be more efficient to:
			- calculate the integral of the arc-ed shape and add it to the square area
			- alternatively, if those functions arent available or if the arc is a very low angle and similar enough to a straight line:
				- the arc can be broken into sub-lines & the area of those shapes calculated & then added to the square area

    - example of interface-based change:

      - as change increases, which interfaces are more/less adjacent, where interfaces are represented as a set of filters, each additional filter being a unit of change on the x-axis, and each subsequent filter being one distortion away from the previous filter, where the origin is the most standard filter

    - example of context-based change:

      - as change increases, how does context change (where unit of context are additional conditions)

    - example of structure-based change:

      - example of time-based change:

        - as time increases, what changes:
          - position
          - value (position on a dimension)
          - distance (position from a base point)

        - changing position based on embedded time

      - example of structure-based change:

        - as change increases, what structures change (which structures are stable even in certain change rates)

      - other standard structural bases as alternatives to time, where change is on a y-axis, and these parameters are on the x-axis

        - order: changes are framed based on order - to examine change patterns with respect to order (where unit order is original/standard and highest order is most different order possible)
        - position: changes are framed based on difference from previous position, starting from the standard unit position (default) - for examining change patterns with respect to position distortion
        - distance: changes are framed based on distance type (distance from value, distance from number type, distance from pattern) - for examining change patterns with respect to distance type
        - value: changes are framed based on value type (exponential, constant, pattern value, symmetric value, origin value) - for examining change patterns with respect to value
        - set: changes are framed based on set membership (number type (prime), pattern (progression), distance (adjacent groups)) - for examining change patterns with respect to sets
        - space: changes are framed based on spaces where that change can be framed (topologies, dimensions, vector spaces) - where spaces are formed by adding dimension units

      - example of object-based change:
        - as change increases, what objects (type/variable/inputs/cause) are more/less adjacent 

    - example of concept-based change:

      - as change increases, how does concept (similarity) change

      - example of power-based change:

          - as power (degree of dependency) changes, what else changes:

            - previously distant points become equal to adjacent points as power increases
            - value reverts a concept & the information of the value loses its meaning
            - dimension space can be determined by the degree of dependency
            - does a change increase or reduce power?

          - this can be framed based on potential (bc power can change with respect to options), variance (because power can change with respect to change), and time (bc power can change over time)

      - example of potential-based change:
      
          - as change increases, how does potential (possible change) increase:

            - what probabilities/possibilities become possible (findable/generatable in structural dimensions/on the structural interface)
            - what possibilities become adjacent/distant
            - does a change increase or reduce potential options?

          - as potential changes, how do potential objects/types vary based on the unit of potential (possibility distance, distance between required limits & optional steps)
          - this can be framed on a base of time, because time is a related object to potential (if there is no potential, there is no time)

      - example of variance-based change:

          - changing stabilization based on randomness
          - changing interface development based on randomness
          - changing systematization based on randomness
          - changing object change based on a changeable interface (change stack, like changing orientation of an object within a system that is changing)
          - changing change types (variance leak, variance cascades/activation, variance injection, compounding variance, variance approaching an interface, variance distribution)

          - does a change increase or reduce change sources?

          - as change increases, what change objects (types/rules/rates/direction) alter position/connection/distance/existence?
            - what else changes
            - what aspects of change are altered
            - what core change functions develop or change
            - where does change go if there isnt enough time to contain it
            - what change rates change
            - what stabilizes
            - what patterns emerge
            - what change cascades are triggered
            - what changes develop into randomness
            - what change combinations produce change rate/type/interface changes
          
          - this is a removal of the time parameter, by assigning distance to change types/rates/other metadata, so that any change is framed in terms of a base unit of change (how much change it produces, by making other objects nearer, creating other objects, and connecting with other objects)

          - this can be framed on a base of potential, because potential is a related object to change (if there is no potential, there is no change)
          - this can be framed on a base of time, because time is a related object to change (if there is no time, there is no change)

    - example of function (relationship)-based change:

      - change with respect to function/intent:
        - as change increases, does functionality/intent change and in what direction?

      - example of cause-based change:

        - change with respect to cause

        - the classic parabola of a ball's motion when thrown from the ground has two primary cause-values:
          - origin force until the peak x-value change rate, and gravity force after the peak x-value change rate
          - if the y-value starts changing more from gravity than from origin force, the gravity force becomes determining

        - additional cause values travel farther up the causal stack:
          - forces causing the emergence of gravity & origin forces are other causes


## Concept Analysis

    - deriving unique concepts & their relationships
    - deriving output intents of concept combinations
    - fitting concept combinations to structures
    - describing problem space, deriving strategies & perspectives in that space


## Causal Analysis

    - given the position between these causal factors, which causal patterns are likeliest?
    - "given that a species occupies an interim position between evolution, efficiency, time, and environment, what is the likeliest causal shape linking a species with its environment?"
      - for more evolved organisms, this is a network causal shape, though species with less developed cognitive ability may have simple or uni-directional shapes with environment
    - "what is the function linking these variables, given these functions linking other adjacent generating variables/functions further up/down the causal shape"


## Info Analysis

    - information analysis involves standardizing information formats, like standardizing to the object/attribute/function model (including related objects like state & type) so that information structures are clear & can be mapped to information problem types

    - organization analysis

	    - optimal path/distribution/states
	    - what would the optimal path be, given a certain intent, object identity, & host system?
	    - "what is the function linking these variables that is most efficient/involves fewest variables/involves known constants?"
	    - identify layer to solve a problem at
	    - identify key objects needed to solve a problem
	    - identify structures for information

    - type analysis
      - given a known type stack progression, what is the likeliest position or extension of that stack?
      - "given that these species evolved this way, what level of variance is the missing link between them likely to have?"
      - "what is the function linking these variables, given the type stacks of the function objects (dimensions, adjacent functions, identifiable shapes, etc)"

	- information objects are related to agents & their communication (perspective, strategy, decisions, intent, game, motivation, problems)

	- these objects can be defined as combinations of general interface objects:
	    - perspective: a filter produced by chains of distortions; priority set with object positions & default paths
	    - strategy: efficient path between points
	    - joke: difference between expected & actual position
	    - error: difference between expected & actual decision
	    - argument: position of objects or path between points with supporting connective functions
	    - game: incentivized motion within a system having limits to prevent motion; a system with conflicting/overlapping intents between agents, usually with low-stakes intents
	    - filter: barrier creating a difference between input & output

    - determining position/trajectory on interface

    - selecting interface as best standard for comparison
        - identifying when a particular specific interface will reduce solution set across any possible host system

    - generating specific interfaces (filters) for a problem/space

   	- generating full set of general interfaces (intent, concept, structure)
        - these can be generated by identifying the key differentiating factors across systems, which can be generated as combinations of objects 
          - type is a combination of attributes
          - intent is a combination of function effects
          - concept is a network of networks describing a structural concept (balance, power)
          - structure is a combination of information & rules 

    - identifying all interfaces with variance that cant be captured in other interfaces

    - interface metadata:
          - generatability/common derivable core functions with other interfaces
          - information loss
          - variance focus (what variance is exaggerated for comparison by this interface)
          - position of interface on default interface network (what distortions produce this filter/perspective from unfiltered origin)

    - interface operations:
          - intent / structure interface: assess intent interface by a standard of structure interface (which structures can simplify the intent interface)
          - function + pattern interface: merge networks of functions & patterns into one standard interface definition (input/output/logic + metadata of both objects)
          - function * pattern interface: 
            function patterns (what patterns are there in functions), pattern functions (what functions generate patterns)
            function pattern functions (what functions generate function patterns), pattern function patterns (what patterns are there in functions that generate patterns)
          - cause * type interface: 
            causal type interface (what types of cause are there), type cause interface (what causes types)
            causal type cause (what causes causal types), type cause type (what types of type causes are there)

    - interface analysis requires considering relationships like:
        - patterns in ratios between uncertainty generated by a function combination vs. uncertainty-reduction function patterns & potential (how does it hide information vs. how can information be derived)
        - the relationship between the transformation function converting one space into another, and the transformation function converting a space's objects (like insights) to another space's objects
        - valid/invalid operations in a space
        - set of all possible spaces (fulfilling concept combinations) & link to the objects best described in that space whose differences are relevant to those concepts
        - within a description system, there will be rules linking objects (like a shape & another shape type) that align with inherent system attributes like symmetry: "given any line, an equilateral triangle can be constructed with the line as its base"
        - core operations done on one attribute (length) vs. another attribute (angle)
        - how core operations & objects accrete in a space (multiple, shift, embed) on every interface layer
        - spaces as the intersection of spectrum variables
        - derive object types with attributes useful for a particular operation ("quaternions for 3-d rotation")
        - value accretion into units (integers)
        - what patterns turn into objects that attract/hold (or provide a platform or conduit for) the most variance
        - attributes accrete into aggregate/type/emergent attributes ("equipollent when they are parallel, of the same length, and similarly oriented")


## Problem Analysis

	- on this index, problems are mapped to structure, once problems have been converted to an information problem, which has a clear mapping to the structural interface

	- problems can always be framed as info problems (missing info, conflicting info, unconnected info, mismatches, imbalances, asymmetries)
		- finding a prediction function can be framed as an optimal path in a network of variable nodes

	- once you frame a problem as an info problem, you can map info to structure:
		- conflicts can be vectors with different direction or which overlap

    - ways to map problem:

        - attributes that differentiate problems that are shared with possible solutions
        - mapping intent to direction and assessing progress by movement in that direction
        - networks with clusters & other structures representing decisions
        - system layer graph representing possible steps
        - function sets mapped to sequences given a metric like progression toward goal
        - mapping related/approximate problem or problem higher up causal stack, having lower dimension, like a generative problem
        - mapping change types to dimensions and graphing/calculating dimensions where change types change (an aggregate, interface, or deciding dimension where change type is uncertain but not random)
        - using a layered graph to visualize change of different types/metrics built on a symmetry (vertical axis if horizontal sections are split)
        - mapping language to structure directly ('find' maps to a set of vectors leading from a node indicating possible start positions, with option to use core function vectors to reach target node)
        - a trajectory between low-dimensional problem graphs where each graph is a decision step, and attribute sets & problem of similar type occupy a similar position on an axis depicting all the graphs traversed
        - a metric like size of variable interaction space mapped to length/area/volume to indicate how much of the problem is left, and a metric like number of variables mapped to number of sides of the shape to graph the problem according to structural metrics


    - limits in visualization

        - if you reduce a shape of a subset of problem dimensions, those variables (side length if defined as a cube, or variable set like identities of sides, number of corners/sides, angle of corner, shape identity), cant be used later in the solution, so even though some reductions may seem obviously right, more than one solution should be tried

        - mapping problem types to functions has side effects without limits & standardization applied to the format:
          - removing a problem variable can only be mapped to lowering the number of variables (whether limits, multipliers, or other objects) creating a shape once the problem variables are formatted with the same term set


    - parameters to graph problems

        - number of problem-causing variables/solution metrics fulfilled
        - complexity: 
          - number of core function steps required
          - number of variables
          - number of differences/inefficiencies
          - number of counterintuitive steps (requiring non-standard solutions)
          - number of contrary processes (requiring scoped/nuanced solutions)
        - abstraction (does it solve the same problem when framed on an abstraction layer above)
        - number of steps required to create problem from stable system state, once work is standardized, & adjacence of steps required
        - how much work is required to convert to a particular problem format (route, combination, composition)
        - type/intent ranges/direction (of individual objects or composite stack)
        - similarity (how similar to a standard problem type, or how near to limits within a type dimension)
        - ratio of positive to negative outputs

    - mapping function, to map problems to structures & other problem types

      - problem types

        - find structure

          - find combination (build)
            - of filters
            - of functions
            - of objects
            - of sets
            - of limits

          - find sequence (route)
            - of network nodes representing
              - steps
              - positions
              - sets
              - intents

        - correct imbalance (align)
        
          - in direction
          - in resources
          - in functionality
          - in intent

          - example: find combination of terms to build a prediction function for a data set

            - of filters
              - which filters should be applied to reduce solution space, find relevant objects, or find steps to produce or build the solution

            - of functions
              - which functions are possible solutions to a prediction function problem
                - 'take an average metric of the set of functions predicting x% of the data with fewer than y terms'

            - of objects
                - 'average', 'function set', 'term count', 'accurate prediction ratio'

            - of sets
              - which objects should be grouped (function set, term set)

            - of limits
              - which assumptions are required and which are flexible

            - of matches
              - which objects need to match, to what degree (function terms and data)
              - which set of reductions works the best with a given set of expansions

            - of imbalances/asymmetries (questions)
              - which metric sets are the best filters for a given problem

            - you could graph the problem/solution with any of those objects, if they supply all the info needed to frame the problem
            - navigating on the filter or mismatch section of the network may be faster given the commonness of those objects

          - example: find resources to fulfill a lack of a resource

            - cause of problem: missing resource or its alternatives, or missing resources to generate it or its alternatives, or dependence on resource or its alternative

            1. create missing resource

            - navigate up causal stack: find combinations of functions & objects that generated it
            - navigate sideways: find alternatives or find alternative combinations to generate it

            2. invalidate dependence
            - navigate up causal stack until dependence cause is found: find combinations of functions & objects that generated dependence
            - navigate sideways: find functions to invalidate dependence (generate resource) or correct problem (imbalance, lack, mismatch) causing dependence

            - solution intents 1 & 2 have a 'generate resource' intent in common, which fulfills both solution intents - so if the intent changes between them, the solution involving generating the resource may cover the next problem iteration too, or the intent that invalidates the problem may prevent future iterations


	- example of concept analysis in design of sorting function:

	      - similarity in navigation, equality in split => optimal for target value near initial split points or similar positions to the split points
	      - assumed difference embedded in pre-computation of attributes => optimal for target value with different pre-computed attribute value, or target values in similar position to values with different pre-computed attribute values or adjacent values


	- example of deriving questions to translate into query sequence

	  - stat problem: "Sunrise problem: What is the probability that the sun will rise tomorrow? Very different answers arise depending on the methods used and assumptions made"

	    - interface analysis questions:

	      - what are the shapes & patterns of errors in assumptions & selection/generation of methods? (what ratio of incorrect are people with each additional assumption, given the level of certainty per assumption & complexity of problem)
	      - what are the consequences of not correcting those errors? (how wrong will the predictions be)
	      - what are the shapes of cause in generating/selecting assumptions & methods
	      - what is the usual correct assumption pattern once false assumptions are corrected, and whats the insight path to transform the incorrect to the correct version?
	      - whats the rate of discovery of new sub-systems, objects, or variables in related systems like physics
	      - whats the likelihood we created certainty out of what ratio of our assumptions (over-relying on assumptions to make them conditionally true)
	      - whats the possible causative impact of measurements & application of science knowledge on other knowledge
	      - whats the possibility that a subset/state of physics rules gathers in increasingly isolated space-times, but outside of it, the rules are more flexible
	      - whats the possibility that every science rule we take as certain is a false similarity or other false object?

	- selecting a space (dimension set) for framing changes of a certain type

	    - selecting a space involves selecting a representation of information by intent

	      - various representations of a function between x & y exist, each with their own intent (hide information, emphasize information, distort information)

	        - the selected space should be that which aligns with the intent of the target space (displaying a representative sample, displaying the most diverse variety of samples)

	      - changing the base is a way to change the interface used to represent information

	      - adding dimensions changes intent of switching spaces (adding complexity or clarity)

	      - certain change types have certain intents associated
	        
	        - potential-based change is aligned with intents like prediction (finding patterns like convergence or cascades)

	      - some changes are complex (changing many different things) that cannot be clearly depicted as an increase or decrease, but are still noteworthy as they are adjacent to an increase or decrease (increase the probability of an increase or decrease) even if they dont register on that dimension set

	        - in this case, a set or network of related spaces can be used to represent the change

	        - other spaces can also change what value means in that space, to represent more complex attributes like concepts (potential), where a change alters several metadata attributes of that base concept (potential direction/reversibility/alternative count/probability of alternatives leading to more potential)

	        - for example, a change may not directly/measurably increase the potential of a system, but it could be similar to changes that preceded an increase in potential of a system, so it should be represented on a pattern dimension set (representing the similarity to the other pattern) or a similarity/change dimension set (depicting the similarity to that other preceding pattern's intra-differences within itself), even if it can only be represented as a point rather than a change on the potential dimension set

	        - patterns are related to potential bc if something is too compliant with patterns, its less likely to change, which is an object of potential

	      - with regard to graphing potential itself, it's more useful to use position as a base rather than time
	        - how does potential change with respect to starting position?
	        - if a starting position is isolated, there is less potential for dependence & more potential for independence (the net impact on overall potential is variable)
	        - this reveals more information about actual potential change types by placing useful limits on value ranges

	      - identifying the variance structures (gaps/cascades/filters) in a system will help identify the best dimension sets to frame certain change types in

	    - this allows different representations of a multi-variable dimension like change in visually representable space without using topologies or parallel dimension lines:
	      - vectors representing directions as types of change according to the unit base vectors (units of change types) and scalar as the degree of change
	      - the perimeter of the changes to these vectors as a shape, and the function for that shape as the overall change function, and difference in area calculated the normal way to represent degree of change
	      - the network of change represented as a traditional graph, but the degree of change is treated like momentum and a function is graphed according to the momentum to connect the change nodes
	      - these are ways to graph aggregate change/other bases having different types without resorting to non-standard spaces
	      - the vectorized form of all change type features (a matrix) applied to objects in a standard shape
	        (dimensions are mapped to shape objects like sub-systems, so a dimension like a particular change type would be applied to a shape object like an angle/corner, if their change types align)


## Interface Network

	- this is the set of networks that act as useful filters/standards for comparing metadata 

	- it can refer to a specific set for a specific problem space

		- the specific interface network for the debugging code space could be layers of network filters like:

			- dependencies
			- logic gaps/order/validity
			- side effects
			- types

		- these specific interface networks are often implementations of the general interface network with mapped objects:
			- dependency interface is a combination of the cause/function interface
			- types (data, classes, etc) interface is a subset of the general type interface
			- side effects are a subset of the variance interface (gaps in intent & execution, prediction of emergent attributes after nth iterations of combinations or other operations)

	- whereas the general interface network includes layers of network filters like:

		- intent (priority)
		- perspective (the unit filter object)
		- functions (can include patterns, logic, strategies, rules, and any other set of operations/objects that has order)
		- structure
			- sub interfaces of structure include:
				- difference/position
				- shape
				- direction
		- concepts
		- types
		- variance (change/potential)
		- cause
		- conflict (problem/solution)
		- system

	- a super-interface involves the core functions that can generate the general interface network:

		- filter/find/identify
		- apply/combine
		- build/fill
		- derive/predict
		- change/transform/process

	- like all other sets of objects on an equal interface, any item in the set can be used to find the others
		- in a set of (4, 6, 2, 3) you can start with 4 to find 2 or 3 to find 4

	- each interface network in the set of interfaces (core function interface network, general interface network, specific interface network) can be used to generate the others
		- intent interface can be used to generate the type interface
		- dependency interface can be used to generate the side effect interface
		- interface network can be used to generate the core function interface

	- the filter interface is more clearly usable as a method to generate the others bc most problems can be reduced to a structure that can be filled in different ways for different reasons
		- it can even generate the change interface, by framing each process as a filter between i/o

	- finding the starting interface & direction of traversal across the other interfaces in the network is its own interesting problem, beyond just generating the relevant & useful interfaces in a network
	
	- framing a conflict of type 'competition' as opposing direction/intent or equivalent direction/intent is a calculation that can be automated using any of these kinds of analysis, but the logic & intent interfaces are best at this, and selecting those type of analysis is an important tool to build

	- other uses of the interface network include:

		- finding explanatory variables on multiple interfaces (a trajectory on the interface network) & translating them to a shared interface where possible

			- maybe you can identify that theres an important type, intent, & conceptual variable to identify an object
			- then you can decide if its worth storing that info separately, or standardizing those variables to the same interface
				- if the type variable is explanatory & you need to keep it, you can still standardize it to intent (whats the primary unique function achieved by each type)
				- concepts can also be standardized to other interfaces (what intents do concepts like 'power' achieve in the system & what position do they occupy)
			- which interface to standardize to depends on which use you intend to use the information for
				- if you need to implement it immediately, an interface like intent that is semantically adjacent to the structural & logical interfaces will be more useful
				- if you need to identify new types, standardizing to the type interface will be more useful


  - sometimes you'll be able to skip interim variables/interfaces

      - depict the spine variable & the finger position variable to demonstrate/identify chirality, skipping the connecting functions, because there are multiple connecting functions (endpoint/side selection, extremity development) and they dont determine change in either variable, as the key important relationship is the spine symmetry and the orientation transformed about the finger position interface being reversed according to the spine symmetry

        - the spine isnt symmetric from the side, which implies a bias toward the front, which is a platform where features are concentrated, so the development of limbs (using derivable intents like duplicate, backups, protective, flexible, movement, alternative, balance intents) & their focus toward the front is derivable from the spine features, so we can skip to the finger order interface to identify the concept of chirality or an example of it/its patterns in the system

      - the interim interfaces & variables may not add change to this relationship so they dont need to be depicted or stored in this context

      - this is useful for determining where change can be stored/routed in a system
        - if there is demand for change (stressors demanding new functionality) but all components but one are maximizing their change handlers, then you know theres one potential variable where change will gather/be routed, if its possible to route change from the variance injection point to that variable's causal stack at some layer/point

      - its also useful for determining interface trajectories/adjacent interfaces
