# sub interfaces of primary interface layers

    - example of connection between interfaces:

      - symmetric (efficiency & similarity) interfaces:

        - efficiencies can be found in: 

          - symmetries
            - can be the quickest way to determine relevant/causative variables

          - similarities
            - similarity of shape/position can be a quick way to classify or match

          - attribute alignments
            - alignments between attributes can cause similar systems or emergent rules/attributes, which can provide an interface for comparison or reusing solutions

          - aligned incentives (default rules without exerting force) & intents (structural/conceptual goals)

            - if transporting an object where there is a decrease in height along the trajectory, letting it fall at a rate that wont damage it to take advantage of the gravity incentive built-in that aligns with the intent of 'transportation'

          - an efficiency can be any side effect, assuming each output is potentially useful until proven otherwise, especially if the relationship occurs from system forces like stabilizing incentives rather than costing something to create

      - structural (adjacency & alignment) interfaces:

        - emergent attributes can be illusory & created by:

          - attribute alignments: 

            - example: direction of growth in similar direction by adjacent objects can create the illusion of an emergent border attribute


    - structural interface

      - dimension/attribute/variable/property
        - variables have symmetries with interfaces, since variables are a standard where change can occur within a limit
        - what shapes do attributes take with regard to an object?
          - can be an edge on the outside (output layer) emerging from internal rules (acidity, phenotype)
          - can be a circuit on which the object participates or circulates where current position has meaning (starting interface)
          - can be a set of vectors creating an output intent or a trajectory linking various networks where an attribute shows up (relevance)
          - can be a metadata attribute of intersecting rules (diagonal attribute in a square formed by perpendicular rules)
        - what shapes do attributes take with regard to a system?

      - position
      - distance measures
      - set of possible variable values
      - limit/metric/boundary
      - equivalence is on several interfaces (conceptual, change, & structural) and it ties to the other sub-interfaces of the structural interface, because position is a differentiating attribute that describes difference, so it depends on a definition of equivalence inherent to that space
      - similarity
      - symmetry
      - shape
      - information
      - variable sympathies (cooperation/antagonism/similarity between variables as a way to visualize difference between dimensions, on dimension network)
    - type
      - combination (combination of attributes)
    - priority
      - direction (intent direction)
    - rule: 
      - pattern
      - function
        - filter order (functions as an order of filters)
      - logic
    - change: 
      - variance
      - potential
      - attribute
    - conceptual: 
      - problem
      - communication concepts:
        - question: info asymmetry
      - abstraction (balance, power)
    - causal


# Definitions

  - symmetry: "feature of the system that is preserved under some transformation" (as opposed to lost information/irreversible transform) - https://en.wikipedia.org/wiki/Symmetry_(physics)
  - interface: attribute that highlights differences within a limit when objects are reduced to that attribute, using it as a standard for comparison, that filters out attributes that dont fit its definition
    - gaps in known rules allowing filter to be applied to objects that can occupy the gaps, to identify variance nodes
    - interfaces should produce a network when their variance is exhausted
  - dimension: attribute that describes differences of an isolatable measurable metric in a system that is unique compared to other dimensions 
    (information described in that dimension is not repeated on other dimensions, although it may correlate with or influence information described in other dimensions)
  - network: known rules of known nodes

  - symmetry => interface
    - a limit on change provides a standard for comparison
    - example: 
      - given that a circle is symmetric in every axis that crosses its center, you can compare one side of the axis and expect the other side to reflect that, using one side as a standard for comparison
      - the symmetry is produced by the filter of the set of axes crossing the center, because the center is the key identifying & generative attribute of the object type (radiating outward from center or rotating radius line, holding one point at center - generates the circle shape and can be used as an identifier)
    - this includes object definitions like common shape definitions 
      (transforms that maintain the object type can be considered symmetries to explore change within the object without removing its standards for comparison)
    - therefore key identifying/generative attributes of an object can be used as symmetries 
      - example: core functions, priorities, types

  - can also generate an interface by the combinations of variables that minimize & maximize change (filters to remove common information to leave differentiating information for comparison)


# Questions

  - by navigating the certainty interface (angles, distance, order), can you derive the shape of the corresponding object in the uncertainty interface?
  
  - how do different layers of interaction objects emerge? 
    - why do collections of properties accrete into objects on one layer & other property collections gather on other layers? 
    - do mostly objects with similar types, functions, attributes, attribute values, or complexity interact?
    - how do objects of very different types (an attribute & an object) usually interact in systems, or do they not have behaviors defined outside of similarly typed object interactions?
      example: does a cat respond to teeth (attribute) the same way in any object, or only when attached to certain objects, like a dog, or when it has a certain value (sharpness: high)

  - what happens when a system or space is exhausted and all combinations & interactions have been defined? 
    - does the interface of variance freeze/stabilize into a network
    - does its remaining variance gather on other interfaces/layers? 

  - sub-interface interactions:
    - is it better to standardize to object interface, variable interface, type interface, or function interface?

  - what types of variables can be ignored even when theyre good indicators?
    - output rather than input variables (output is a good indicator even though input variable may be preferable)
    - variables less likely to change

  - give example of framing variance with different interfaces to highlight predictive information or information that can be structured on other interfaces that simplify the problem
    
    - the common goal of 'variance reduction' which maps to 'applying a standard' may not always be the right first step, as 'variance expansion' can offer potential to identify different component interfaces that would highlight hidden variable set ranges
      - example: if you expand or maintain variance, or frame it as a combination of variances simultaneously framable in multiple standards, you can identify hidden variable gaps where variable sets on different interfaces can fill the gap in a way that doesnt disrupt the original variance
      - identify example where variance cant be reduced to a common interface but is still solvable with a set of interface positions/trajectories

  - examine how functions accrete into chains - how does one function get selected when it has the same input/output as another function?
    
    - common metrics like efficiency/availability or something more complex
    - having function metadata similarities beyond input/output
    - having core functions in common
    - having common intents
    - having enough difference to be useful to cooperate with, given inefficiency of regenerating other function on its own

  - what space are these symmetries derivable in? the spectrum interface is derivable from the chaotic evil matrix - what space makes these symmetries derivable?

  - why do some of the most useful interfaces have semantic values (context like identities of nodes, producing interfaces like priority, which is specific to non-random systems that can have intent) 
    rather than structural values (type, pattern, change, structural)

  - give example of variance transforms

    - adding a new interface for variance to expand into
      - changing existing interface used as a filter to hold constant
      - adding new interactions to allow the expansion of new variance

    - moving variance into other interfaces to make the problem more solvable
      - rather than a species evolving into a new set of sub-species, host the variance in epigenetics rather than inherited mutations

  - what does emotion interface map to?
    - emotion functions involve:
      - information indexing
      - assigning cause
      - directing intent 
      - perspective/layer switching
    - emotions are used as a cause, a standard, and an output, allowing for variance injection due to current lack of measurability
    - they can capture sub-nets or trajectory nets on the interface network

  - give example of calculating which variable resolutions can be postponed & solved later with information acquired in subsequent analysis
  - give example of mix & match interface assembly given the problem of 'model identification'
  - choosing bio interfaces including electricity as components with a common language
  - discuss how sub-interfaces can collapse into a super interface (change interfaces) as the default shape of interfaces can be shapes other than networks
  - frame common problems with the standard of variance vs. time (recurrent nn)

  - calculate the set & order of filters/dimensions to use to solve a particular derivation problem
    - 'deriving core functions' can be converted to a set of filters in a particular order, just like a normal finding problem can
    - calculating the set of generative/expanding dimensions or reductive/standardizing filters to use is more valuable than writing the function manually
    - can dimensions act as filters or vice versa? dimensions can capture & highlight information differences, whereas filters can remove information similarities & highlight differences
    - how would expanding dimensions enable deriving a set of core functions in a system?

    - derivation rule examples:
      - start with common core functions across systems
      - apply common dimensions of distortion to expand core function set

    - filter rule examples:
      - replace concrete rules with type versions
      - reduce rules by similarity across attributes that are typical parameters of common core functions

  - algorithm to decide when to use interface query, when to standardize to a particular interface, and when to stack interfaces to isolate variance by applying one interface to another
    - example: analyzing priority direction once you standardize to variance interface

  - algorithm to decide when its time to retire an interface:
    - when the exploit, optimization, & variance opportunities have been exhausted using all possible combinations of components & core functions, so that the interface has fulfilled its potential usage
    - if there is external system variance, another interface may develop to host the remaining variance

  - interfaces may devolve to their abstract forms once a particular implementation is fulfilled
    - type :: combination (combination of attributes)
    - priority :: direction (intent direction)
    - function :: filter order (functions as an order of filters)
    - etc for the other interfaces - which have sub-interfaces created with transforms of the original interface:
      - rule: pattern, function
      - change: change sub-interfaces like variance/potential/attribute interface
      - conceptual: problem, question, abstraction
      - structural: symmetry, shape, similarity, information, limit, metric, boundary
    - do these transforms have patterns in common?

    - the combination, direction & filter order interfaces hold more variance than their specific implementations in the semantic interface nexus
    - this implies there are other interface nexuses using those core interfaces
    - given that the core functions of a space map to the core interfaces, the space can determine this core interface nexus 
      - derive the core functions in a space and you'll have the original interface nexus to start from

    - another example implementation of the core interface nexus:
      - combination :: network, set, cluster, dimensions
      - direction :: potential
      - filter order :: structure (filter orders map to structures that capture variance potential)

    - examine mixed interfaces: interfaces applied on top of interfaces in various combinations to create a topology of applied filters
    - sometimes interfaces should be applied at intervals, so variance can develop in between & adjust interface order & identity as needed
    - embedding layers like parameterization have relevance for choosing interface cycle origin points

    - how would you derive core functions which map to core shapes (combine, direction) from this space?
      - this space allows:
        - freedom to interact (there's room to select between equal alternative rules, all rules are not enforced)
        - freedom to fail (interactions don't have to benefit all entities involved)
        - freedom to calculate (you can check & verify information)
        - limits on calculation (you can know some things at cost of not knowing other things)
        - limits on variance (some rules are enforced but change or assumptions can be concentrated on specific rules to change them or change their enforcement)
        - limits on structure (you can cooperate to create structure but it will decay according to system rules)
        - limits on direction (not all decisions are reversible, and some decisions limit future decision potentials)

      - can you derive the core functions/shapes (combine, direction, filter, position) from those opportunities & limits?
        - filter maps to calculation - when you calculate something, you're reducing your consumption of other information
        - interaction allows for combination
        - interaction allows for direction (choosing a path between equal alternatives)
        - freedoms allow for randomness, which leaves room for variance

      - these freedoms & limits can be framed as a set of filters (rules) which generate the core functions/shapes, which can be used to generates various interface networks, including a key generative interface like concepts
      - freedoms can be framed as gaps in limits, so the limits would represent the interface or network structure
      - there may be an optimal interface network, but allowing them to compete could be the optimal filter
      - what shape do these limits restrict? what filter is applied to another interface to leave these limits remaining, that allow core functions like combine to thrive?
        1. this could be a meta interface (the interface interface, the limit interface) but that leaves the question of the source interface open rather than resolving it
        2. there might not be a calculatable answer (the source is not measurable inside the space) or it's a cyclical answer, where other interfaces generate other interfaces
        - interfaces overlap & influence each other: structural limits form rules, which are classifiable on the function interface, as well as the type interface, structure maps to core functions, concepts map to structure
        - so the network of rules linking interfaces for a particular core interface nexus implementation may be significant, not just as a generator of the nexus but as the primary interface for variance to develop on
        - and the network of these networks may give informtion about the answer to 2

      - standard nn structure has unidirectional causal support 
      - the collapse of sub-networks into a feature on a network with nodes to classify has patterns
      - the transformation of a classification network (containing nodes representing species) into a set of causally connected networks (containing nodes represent symmetries, types, patterns, functions)
        can be done if the data set contains a minimum of information needed to derive variance potentials, symmetry paths, type hierarchies, etc
      - the physics of attribute accretion into types can also be used to derive the set of networks building a classification network
      - if you have x variables on one interface and y variables on another interface, you can derive the network set to build a classification model for the classification network
      - the standard nn supports accreting attributes into types
      - it should also be able to support removing attributes to reveal differentiating factors that are not mimickable with distortion functions or random symmetries possible in the mimicking class
      - distortion functions are the first line of attack in reducing the set of identifying factors

    - what about networks with multidirectional causal support?
      - for example:
        - a classification problem that impacts the classification potential
        - an assumption set that changes the type evolution
        - a type structure that determines the attributes that can accrete in it, rather than only the attributes determining the type
          - example: given that a type has a set of symmetries around which variance accretes up to a limit, which attribute types are possible & which attribute probabilities occur given the level of variance allowed by the initial attribute set?
            - given that a level of variance occurs, what is the likelihood that a backup alternative variable will evolve?
            - "given that a particular sense is inherently limited but the demand for senses is not, what is the likelihood that an alternate way of sensing will evolve"

        - other layers of cause can occur before the type influences the attribute set - like a type that influences filters/symmetries that generate attributes

        - the relationship between attributes & types is multi-directional in cause, not to mention other relationships between interfaces

        - where is the potential for feedback from types-to-attributes in the network structure?
      - a species is overloaded with variance the more its compared to another mimicking species, or the mimicking species adopts variance to become a better mimicker
      - as attributes stabilize into types, its likelier that more interfaces of variance will develop that are not captured by those attributes
      - the relationship between the type/pattern/priority interfaces & variance-capturing variables is not unidirectional (one network builds the other, not the other way around or both)
      - however this network structure can often be used to capture the collapse of attribute sets into types, because types are by definition a collection of attributes, unless:
        - those types are in a state of convergence/divergence that isnt captured by the data or algorithm
        - some attributes have an insignificant symmetry with the identifying attributes (illusory correlation)
        - some attribute variance isnt captured by data set (chihuahua that has been in battle and lost some of its identifying attributes)
      - also there is a trajectory on the interface network that can build the identifying variable set (variance remaining after reduction by distortion functions & other insignificant symmetry types)
        which can take the form of a specific interface built for that problem type (filter out attributes that dont differentiate between these classes, leaving the differentiating attributes for comparison)
        this is what is called the prediction model/function, but usually its built from raw attributes than alternate interface attributes, which may be derivable from a data set that cant capture full identifying variance
      - an interface-building query for a problem type, integrated with a network algorithm, can therefore be a faster approach than using an information filtering network on its own
        similar to standardizing problems to optimal transport problems (how do you get to the target information state given initial information)
      - there may be interim interfaces that are more useful than raw data to determine a collapsed interface
        rather than using feature data to determine type, use feature pattern & feature evolution pattern data to determine type & type state
      - there may also be interfaces that allow variance in likelier patterns than another interface
        rather than using the priority or pattern or type interface, the function interface may capture more variance if the classes being identified are in a state of change


# Symmetry evolution & stacking in progressions of systems

        - as you add equivalently long lines to the system of 'connect these lines at their ends', you can see the evolution of symmetries that can generate the system
          - it starts with creating objects on the explicit system interface (use the 'line' object and the 'angle' object to create shapes connected at line ends)
          - then it adds more interfaces from which the shape can be generated, as the number of lines variable increases
          - as properties emerge in shapes as the progression continues (such as rotation, prior object interface patterns, corner/angle behavior, emerging circle shape as number of lines increases), other interfaces develop, not just on the structural level but other levels

        - the triangle is the first closed shape in the progression of that system, and you can see the symmetry between each corner and the line split by equivalent angles to the other side,
          making three symmetries total (in raw structural symmetries, not counting attribute/rule/change symmetries) 
          - with two connected lines, at a 60 degree angle, you can generate the triangle by duplicating & rotating to connect the duplicate with the other lines to create a 3-sided object
          - the triangle is the first object to have a 'center point' or 'origin' object 
          - the triangle is self-similar (can be used to generate other versions of itself - building triangles out of triangles)
          - the triangle, having the center point object, is also the first object that can be generated with the rotation symmetry:
            - given the center point object and the rotation function, the triangle can be generated by positioning three lines at equal angles from the center point, where the lines will lead to the corners, which can be connected

        - the square has more symmetries than the triangle (not just from corners to opposite corner but side mid-points to opposite side-midpoints) bc of the increase in number of sides, which allows for a corner-to-corner symmetry that the triangle cannot allow given its lack of opposite sides (two corners cant be connected by a shape-crossing line because each line connecting two corners in a triangle is just the side), making four raw structural symmetries total
          - the square can be generated on the triangle interface (four small triangle building a square, one large triangle with a square inside, etc)
          - the square can be generated on the circle interface (using four lines to create a square using a circle as a base)
          - with two connected lines (at a right angle) you can generate the square

        - the pentagon has more symmetries than the square because it can be generated with more shapes
          - the opposite corner-midpoint symmetry exists in the pentagon but also the rotation symmetry
          - with symmetry in angles, you can generate the pentagon from the origin center point & a rotation transform
          - with two connected lines, you can generate the whole five-sided pentagon
          - with transforms on the triangle interface, you can also generate the pentagon
          - so the pentagon can be generated by prior interfaces (triangle, square, line), sub objects (corners, angles, center points) & combinations of them (square plus triangle)
          - you can also generate the pentagon with combinations of prior interfaces & variance interface (square plus opening/expansion of system)
          - shapes generatable with rotation interface & capable of evolving a rotation function trend toward the property of smoothness & curvature, which lead to circle, which is the best rotation shape
          - this implies some attributes like rotation are inherently compounding & cascading throughout a system, just like efficiency can be compounding

        - because of the emerging combination/rotation symmetries, other symmetries emerge:

          - variance interface 
            - permuting the line variable is one way to inject variance, but only using a permutation of an explicit variable
            - you can also inject variance by removing/transforming one sub-object (like a side/corner/angle) into another version
              (opening the square by removing one side allows for two lines to replace the removed side if the other sides' angles are expanded)

          - efficiency interface
            - if two lines carry more functionality than one (like hitting more nodes), the injection of two lines in place of one can produce efficiencies

        - so you can see how these interfaces stack:
          - structural: explicit variables (lines), implicit objects (corners)
          - emergent: rotation, combination
          - abstract: variance, efficiency

        - the interface stack can be rearranged to optimize relationships when generating objects
        

## Interface stack example

      - you might think there are just two dimensions to the chaotic-neutral-lawful good vs. evil matrix:
        - degree of order (chaotic -> neutral -> lawful)
        - degree of evil (good -> neutral -> evil)

        - but this matrix can actually be generated by one dimension, applied to a topic:

          - dimension: extreme - middle - opposite extreme
          - dimension: maximum - balance - zero/minimum
          - topic: ethics, order

        - if you apply either of the dimensions above as one axis, and then add another copy of the dimension but applied to the topic of 'ethics' & the topic of 'order' to create a transformed dimension 'ethical degree', youve generated this matrix using one interface (a general spectrum interface) & one topic (ethics)

        - therefore you have an interface stack inherent to this structure because of their symmetries: 
          foundation: spectrum interface
          application on top of foundation: ethical interface
          extra application on top of application: order interface

        - you can also use order or ethics as the base dimension and create the other by applying one to the other topic:

          - dimension: ethical spectrum
          - dimension order spectrum
          - topic: ethics, order

          - apply order to ethical spectrum: 
            - raw application (alignment): order aligns with good
            - semantic application (context): 'order' in the context of an ethical spectrum aligns with good

          - apply ethics to order spectrum: 
            - raw application (alignment): good aligns with order
            - semantic application (context): 'good' in the context of an order spectrum means 'orderly'

        - not every interface will be so clearly mappable & applicable to another interface - these two interfaces have symmetries because theyre built on the same foundation interface

        - you'll notice there are variations in optimality when applying these dimensions:

          - sometimes chaos aligns with good, and sometimes order aligns with evil - not the usual way we typically see those concepts portrayed - usually we see them portrayed in the opposite alignments, with chaos being evil and order being good
          - but when laws are bad, chaos is good - there is no absolute virtue in chaos or order, just like there's no absolute rule that stays constant when its overloaded with falsehoods like false assumptions

        <img src="chaotic_evil.png" title="https://hotlink.popmartian.com/4B10A5B0/chaotic-evil.jpg"/>

        - those arent the only symmetries in this two-dimensional matrix, given the topics at hand
          - for example, according to these definitions, the neutral evil & chaotic good have a similarity in behavior
            - the neutral evil person will act in their own interest bc of selfishness
            - the chaotic good person will act in their own interest if they think its right, and if they think it will produce other good things or enable other good deeds
            - they act for different reasons (selfishness & ethics) which parse to the same intent (selfish behavior)


## Symmetries

  - symmetries as a source of misclassification error and efficient/cooperative/optimal attributes
    - symmetry as a source of limits to restrict variance with
    - the limits can be common attributes/intents, around which variance develops
      - separation/delegation symmetry
      - retaining backup alternatives
      - organization
      - abstraction 
      - uniqueness 

    - symmetries are also where to look for key differentiating factors
      - the range allowed by a symmetry exists for a reason - to explore possible utility of variance in that range 

    - symmetry types:
      - functional symmetry: only restricted to find an optimal value, after which it's set to that value (tail shape) or the variance is reduced (remove tails with new design)
      - random symmetry: insignificant except in that it displays common patterns or efficiencies (independent evolution of intelligence across diverging species)
      - interface symmetry: allows enough variance that it continues attracting compounding variance (mammal interface, brain interface)
    
    - some symmetry types should be removed from data set bc they dont add variance but allow it to develop

    - you can also calculate the variance types to look for given the structure of a symmetry network
      - given symmetries powering an interface, predict which variable types to look for that could develop in that structure, then try to identify which phase of interface development its at
        - "given uniqueness and functional symmetries powering the identity interface, predict which variables to look for that could develop, then identify phase of development"
          
          - given that you need certain features in order to exist (a way to breathe, a way to sense, a way to communicate, a way to identify other people),
            predict which variables are likely to vary in the human interface, then identify phase of development
            - within the scope of features that can vary without impacting required survival functionality, senses for example can vary while still performing communication functions and are likely to vary to introduce testing of utility in different values, so senses are likely to vary
            - predict which phase of evolution the human interface is at: given the level of variance & optimization achieved on the human interface as well as variance trends, the human interface is likely reaching another explosion of evolution, given how much optimization potential has already been exploited in its existing interfaces (organ differentiation, sense optimization, neuron automation)
          
          - so in order to identify one human from another, look for sense variance (facial identification) as well as variance in the interfaces powering existing variance (organ differentiation, sense optimization, neuron automation, etc) when that expected variance occurs (may take the form of variance on other interfaces with less variance (like cellular structure variance, cellular communication variance, gene repair variance) but are likely to be connected by a functionality interface (reusing/sharing/finding functionality) as an identifying feature of the human interface, which now is struggling to be identified compared to its automatons, so efficiencies in processing are the next likely variance-hosting interface, if humans have to compete with machines for identity for example

    - you can also identify symmetry paths mapping to variance levels

      - in the example of muffin vs. chihuahua, the symmetry paths of a muffin:
        - common shape (humans gravitate toward simple common shapes so theyre likelier to create tools mimicking these shapes)
        - distribution (humans can understand concepts like optimization, and one implementation of optimization in the muffin interface is distribution of differentiating components)
        - humans have sensory preferences in common (texture, flavor), as some flavors are still useful indicators of healthy food (how dogs crave grass & other things when they're sick)
          so sensory attributes like texture & flavor are likely to converge for food intended as comfort food

      - lead to different variance levels than the symmetry paths of a chihuahua
        - distribution (dogs' sensory organs are distributed)
        - communication (dogs sensory organs need to communicate with each other (the mouth can detect if the nose is ok, the nose can detect if the eyes are ok, the ears can tell the eyes if dangerous fists are imminent)
        - backup alternatives (some organs' functions are so important you need a backup - eyes, kidneys, ears, mouth/nose as a possible airway, lungs)
        - uniqueness (domesticated dogs are treated like humans with identities, and sensory organs are likely to produce potentially identifying uniqueness due to the lack of functionality present in some sensory variance)
        - functional variance (some dog sensory organ configurations have utility value)

      - so you can expect variance in the chihuahua that the false chihuahua cannot provide
        - the chihuahua may vary in ways that a muffin cant given their symmetry paths
        - the chihuahua will vary along its facial interface (two eyes, mouth, nose) but the muffin distribution of components will be more random, for texture/flavor optimization
        - the symmetries making it unclear which is which include:
          - size (chihuahua face and muffin are often the same size)
          - the fact that visually measurable ingredients are often used in muffins (chocolate chip) and happen to mimic color scheme
          - the fact that there are only so many positions a chocolate chip can take, given its size, 
          - the fact that the baker will optimize the distribution metric with pretty good accuracy most of the time, meaning its likely to mimic a chihuahua face
        - these symmetries can be used as points for variable reduction (ignore size, ignore color scheme, etc)
        - the routes to these symmetries are insignificant in terms of the identity interface

      - the ways that significant symmetry paths vary can determine possible classification errors
        - chihuahua eyes can be shiny in different light environments
        - muffins may be coated with sugar, which may mimic the attribute 'shiny' used as a way to differentiate between muffin/chihuahua
        - this is a false symmetry, not indicating variance around a common interface, but can still be incorrectly identified as a legitimate symmetry (similarity in shininess means similar class)

      - these provide a point of variance reduction
        - shininess is not significant given that a distortion of an attribute in one class can mimic the naturally developed, identifying attribute in another class, so remove shininess from data set
      
      - therefore given the set of attribute values that can mimic each other, the insignificant symmetries, and the variable potentials left by the differences between significant symmetry paths, you can calculate which variables in a problem space will be capable of having predictive value
        - after removing false mimicking symmetries like shininess, insignificant symmetries like color schema, common symmetries like distribution, you have a limited set of differentiating features left:
          - muffin chocolate chips wont have tear ducts
          - chihuahua eyes wont have chocolate chip texture
        - therefore if you determine that your data set supports the level of detail required for these remaining attributes, you can use them as a predictor

      - given that a classification problem usually gets more difficult in time, you can also predict ways it will change to reduce the differentiating variables
        - they may start using candy that looks more like chihuahua eyes
        - they may start messing with the data set to lure the model into a false sense of security (using raisins which are easier to differentiate) so the actual data is more difficult to differentiate (chocolate chips)
        - this is a problem of identifying intent to misclassify, which means predicting symmetries in other objects that can be used to trick the algorithm into creating an inaccurate model
        - if an evil person wants to fool a movie theatre security ai into believing they are bringing their dog to the movie and not an evil reasonably-priced or homemade snack, there are many ways they can do so, but these ways are predictable & involve insignificant symmetries between classes (similar color, similar shape type)

        - how do you make your algorithm robust against these evil tricksters?
          - you need to be able to identify insignificant symmetries & distortion functions that can exploit them
