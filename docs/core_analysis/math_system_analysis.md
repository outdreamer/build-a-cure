## Points/Questions

  - check other number types for alternate solution paths to set of solutions that are more efficient than across real/complex numbers
  - check what differences are created when core functions start at different core points
  - what is the topology of spaces called?
  - are the best encryption functions those with the fewest symmetries?
  - the layers of possible sets of core functions form the interface network
  - all accreted attribute sets can be collapsed to a point (type) and visualized on a graph to indicate difference between attribute sets, but which attribute set is most informative and should be used as a default base to start from when seeking more attributes (is there a spherical topology where the origin represents this default base attribute set)
  - with the example of an ideal: 
    - given subsets of the integer set having absorption & closure properties, they created an algebraic subtype to encapsulate these properties on that set
    - deriving functions to produce important emergent attributes/objects/rules (emergent attributes of an ideal), given subsets (even numbers) of a set (integers) having attributes (absorption, closure) using a trajectory across function/attribute/set topologies
    - real numbers
      - unit (integers)
        - multiples
          - subset of multiples of 2
            - absorption        
            - closure
              - ideal
          - subset of multiples of 3
            - absorption 
            - closure
              - ideal
    - the progression from real numbers, to the ideal object, to the attributes/rules of the ideal objects on the attribute/rule topology (jacobson radical) & distance to ring & set object attribute sets, should be describable
    - is attribute or rule interface better when condensing objects for comparison
    - if a sphere is symmetric with respect to the origin when rotated around an axis, that means its exact previous state cant be derived once its rotation position is measured, but the previous state space can
      - measuring the output of a symmetric transform cannot reveal specific information about original position (reduce solution space of possible original positions)
      - the output of symmetric transforms (spin, momentum) might disrupt other systems (adjacent objects to sphere), so to reveal information about a symmetric transform history on an object to derive original position, youd need to position that object next to objects that would respond to the output of that symmetric transform, or in spaces where its output would have an impact on objects that could respond as needed
      - the set of adjacent objects & spaces where the symmetric transform output could be used to derive original position can be used as a parameter to obfuscate the original position (algorithm uses different element in the set each time)
  - interface analysis translates geometry & algebra to interface layer objects


## Function conceptual metadata

  - a wave function has metadata:
    - conceptual attributes:
      - repeatability
      - vacillation
      - spectrum
      - limits (extremes)
      - momentum
      - intersections with axes
      - change rule phase shifts
      - parameter change points
      - charge points (attracting motion beyond standard forces, when motion begins to stabilize to constant or no motion, to re-charge it/continue the pattern) 

    - structural attributes
      - inflection points
      - maxima, minima, range
      - zeros
      - continuity

    - adjacent functions
      - polynomials
      - rotation
      - opposing force conflict (propulsion, gravity)

    - equivalent functions (with scalar/shifting transforms)
      - sine function

    - contradictory functions
      - reversal of parabolas with asymptotes rather than intersections with x-axis


## Spaces as attribute generators

  - euclidean space example

    - core inputs
      - position
      - angle

    - core functions
      - in Euclidean space, the core functions operate on the position attribute:
        - shifting (1-d position transform)
        - rotating (2-d position transform)
        - shift & rotate (3-d position transform)

    - definition of equivalence:
      - "if one object can be transformed into the other by some sequence of translations, rotations & reflections, theyre considered equivalent"
      - the definition of equivalence can be framed in terms of the core functions

    - system analysis:

      - how would you derive/reduce the likely/possible emergent attributes of euclidean space given this metadata?
        - identify the possible change types
        - identify which parameters explain change within these change types (position, angle)
        - identify the core functions, given the sequence of dimension numbers (0,1,2,3 in 3-d space) & the change types possible on each dimension or combination of dimensions

      - extension:

        - what is the operation associated with:
          - dimension reduction

        - what is the extreme version of combined attributes/functions?
          - a dimension resulting from 0th dimension with the dimension reduction operation applied (-1th dimension, etc)

        - what is the concept associated with:

          - angle: anchored difference

          - position: unanchored difference

          - both are measures of equivalence

          - angle measures difference in direction starting from same origin

          - what type of object measures difference in equivalence?

            - is there an "angle" object between anchored (position-dependent) & unanchored (position-independent) difference measures?

            - in what space can you frame anchored differences (angles) and unanchored differences (positions) as dimensions so the angle between them reflects their relationships?

      - as you can see the metadata of each space can be calculated from its definition, as can the extensions of that space

      - given a pair of dimensions, how would you generate certain types of motion without the complete space metadata?

        - for example, from euclidean space metadata, how would you derive that:

          - the concept of momentum would intersect with the concept of distance-measuring values (real numbers), to generate the idea of a common shape like a circle or any other space with an arc, without knowing in advance that this shape had extremely relevant attributes to other shapes in that space (pi)

            - why would momentum have an inherent connection to value-anchored distance?

          - the constant forming the natural log (e) is also related to the constant forming the circle (pi * r ^ 2)
            
          - ln a = log (base e) of a = (e^x = a) = integral under x^-1 from 1 to a = x^-1 dx, from 1 to a
          - eulers equation: e ^ (i * pi) + 1 = 0
          - e ^ ix = cos x + i sin x
          - cos = adjacent/hypotenuse
          - sin = opposite/hypotenuse
          - i = -1 ^ 1/2

        - you could start with basic metadata that you do know bc its inherent to the definition of the space:

          - core attributes: 
            - position
            - angle

          - what are some core objects in this space?
            - axis: measurement standard of a variable (in which value difference can be measured)
            - unit: accretion of value into an identifiable object (whole)
            - point: set of parameter values
            - line: connection between points
            - anchor: starting point held fixed
            
            - unanchored difference & anchored difference are two important objects that can be generated from the metadata attributes position & angle
          
          - what are some relationships between core objects/attributes/functions?
            - two axes in euclidean space have an intersection point, where they both have equal value to base their scales on

          - what are some emergent concepts/attributes/functions?
            - the concept of zero, positive & negative are inherent to the definitions, but can also be derived if not included in the definitions (and just the origin point overlapping at zero for both axes is included in the definition)
            - the concept of infinity emerges from identifying the axis as representing the set of real numbers (if your program didnt already know about infinity from the available space metadata)

          - using that intersection point as a starting point, or anchor (position attribute value), what values could the other primary attribute (angle) take, or what core functions (rotation) can be applied to core objects (line) that adjust the other primary attribute (angle)?

            - we are trying to generate the idea of a circle using the metadata we have (position/angle, rotation/shift, point/line)

            - this is where having multiple ways to generate an object comes in handy
              - having more than one way to generate a circle (rotation of anchored line and relationship to a square having overlapping midpoints/corners with the circle) allows you to use fewer assumptions, like if you didnt want to assume knowledge of the rotation function

            - if you rotate an anchored line around a fixed point, what kind of motion can you expect?
              - the length of an anchored line during a rotation would not change, & it also wouldnt form a straight line between two points on the shape traced by the unanchored endpoint of the line

            - if you traversed a square formed by four equivalent lines along the axes, what kind of motion can you expect?
              - this is where momentum should emerge from any prediction model
                - four lines along the axes forming a square whose corners intersected with the circle's boundary would also be able to generate that circle on rotation
                - if you only had the square, and traversed the square with an object, the object would have momentum once it reached a corner that made it difficult to pivot exactly 90 degrees to the next straight line
                - if you allowed deviation from the square at corner pivots, the square would have a bubble shape
                - if you continued allowing deviation from the square, a circle would emerge, as the momentum at each corner decreased with each pivot
                - eventually it might stay constant at a circle, unless the forces overrode it to make it alternate between convex & concave over-corrections to a straight line

            - now that we have the idea of a circle from multiple routes (just by combining inherent & derived core space metadata), can we arrive at either of the above constants or their relationships using these definitions & the shape we've derived?

              - given the idea of a unit and our core operations (add/subtract, multiply/divide), what ways can we generate that unit? 
                - here are some examples:
                  1 * 1   = 1
                  -1 * -1 = 1
                  1 / 1   = 1
                  -1 / -1 = 1
                  1 ^ 1   = 1
                  1 ^ -1  = 1

                - clearly multiple operations executed on various forms of 1 can be done to produce 1

                - but we've also generated the idea of a 'base' in the exponential operations (just like x ^ 2 has x as its base) where various forms of 1 can be used as an exponent to indicate how many times to multiply 1 by itself to generate 1

                - the 'base' concept can be added to the core objects & concepts of the space

                - when we generalize this relationship: x ^ b = y (where b is the target output of the operation, indicating how many times to multiply x by itself to get y), we have a relationship between a generalized cube (shape generated by side of equal length/difference) and a measure of generalized cube volume (value generated by multiplying side length by itself, side_count/dimension_count number of times), so you can derive the number of sides (b) given a base (side length x) and a target volume (y)

                - we can graph several of these functions with different bases, noting that the functions have attributes like:
                  - intersection at y = 0 for x = 1 (log x = 1, log 1 = 0)

                - what other relationships are inherent to the core objects?

                  - with a square (another core object formed with equal side lengths), are any properties related between the side length and the other objects?

                    - a diagonal emerges, connecting opposite corners

                      - the concept of a right angle emerges from the square

                        - at various configurations of a four-sided object and its diagonal (triangle formed by diagonal), the outer corner of the triangle intersects with the unit circle of radius 1
                        - the points of overlap include key angles forming the diagonal (hypotenuse of a right triangle) like pi/2, pi/6, etc
                        - that circle's radius correlates with the ratios between the diagonal-generated (triangular) object's side lengths (sin, cos) - meaning pi/2, pi/6, etc - this correlation produces the constant of pi

                  - now we have a relationship between diagonals, right triangles, angle, and circles - just by varying & combining core objects (square, circle, corner, angle) and core concepts (unit, intersection, distance, origin)

                  - we've also identified a constant (pi) locking the relationship between the diagonal of a four-sided object anchored at the origin & the unit circle anchored at the origin

                - back to our original motion type combination that produced the idea of a circle, how would that idea of a circle interact with a way to calculate the ratio between a side length x and the number of sides of length x to generate a volume y, or a particular configuration of that function using e as a base?

                - the exponent relationship between base x & target y (x ^ b = y) is related to the idea of multiplication (x * y = add x y times), which is related to the idea of an integral (iterate over each point in x range, which is a line of distance y, and add each y-distance value to get the integral, or area under the curve)

                - b in the exponent relationship (x ^ b = y) could represent the separation point to divide the area under the curve into (b being a negative number to allow division of x) so it forms a subset of objects of size x/b, to get the "volume" metric (which just maps to the area in 2-d)

                - now we have a theoretical relationship between angle, circle, pi, generalized cube side length/volume exponent relationship

                - right triangles are important bc they help calculate slope (hypotenuse representing change and opposite & adjacent sides representing components of that change) 

                - at this point, the remaining items to identify are:
                  - the importance of the imaginary unit i, square root of -1
                  - the importance of the natural log base (the constant e)
                  - the relationship between ln a and integral under x^-1 from 1 to a
                  - euler's equation relating natural log constant and pi: e ^ (i * pi) = -1
                  - bonus relationship between pi proxy functions sin/cos and the natural log base e: e ^ ix = cos x + i sin x

                - we should iterate through all the core operations (rotate, root, log), core objects (unit, 1, 0), core attributes (positive/negative), etc, which should produce many concepts, attributes, & functions, among them the idea of the square root of negative 1, the imaginary unit

                - the idea of the imaginary unit is a number which when multiplied by itself (taken as the side of a square) produces -1 (the area of the square with sides of length i is -1)

                - how does this imaginary unit object relate to the other objects?

                  - hypotenuse c (length of diagonal of a right triangle) = (a ^ 2 + b ^ 2) = c ^ 2

                  - the hypotenuse has two solutions, c & -c

                  - the relationship between (a * 1 & b * 1) and (a * i & b * i) has a rotation or reflection operation depending on the axis - rotation being a core operation, so we know this is likely important, given its relationship not just to core objects like the hypotenuse but also the implications of those core objects (two roots possible to produce c ^ 2)

                - area under 1/x from 1 to a) = ln a    (where ln a indicates e ^ y = a)

                - within the function family of (x ^ b = y), what are the patterns with negative values of b? (how many times do you multiply x by itself and divide 1 by that (inverse of generalized volume), to get y) 

                  - this points to the concept of an inverse, which has a neutralizing impact to its original form, when the operation is reversed (multiplication by original distortion)

                  - the generalized cube volume exponentially increases as side length x increases - when you take the inverse of a generalized cube volume, the core object 'generalized cube volume' approaches a limit such as 0

                  - what about the unit version of an inverse generalized cube volume, x ^ -1, or 1/x - how does this relate to the other objects?

                  - a negative exponent produces a reduction of the dimension of the generalized cube, like dividing a volume by the side length to get an area

                  - this is related to the idea of a integral, which increases the exponent by one & multiplies the term by the inverse of the new power, to get a metric of higher dimension (like area under curve function in range)

                    - integral: 1/(power + 1) * coefficient * (variable ^ power + 1) 

                    - the integral is increasing the power in order to get the area of a subcomponent of the area under the curve, by increasing the generalized cube dimension by another side to get the metric of the higher dimension (multiplying x by itself another time to get the generalized volume of x ^ (b + 1))

                    - it multiplies that subcomponent area by the inverse of the new power value (dimension_count/side_count) * the original coefficient because this is a subcomponent of the area, not the whole area, and its proportion needs to be adjusted to represent that

                      - scaling each subcomponent area by the inverse of the increased dimension_count adjusts the subcomponent area's contribution, according to the new dimension

                  - 1/x is the function where the standard changes but the number being evaluated stays the same - it basically measures the power exacted by each standard

                  - one reason the natural log function might be significant is a subset of its metadata:
                    - intersection with y-axis at x = 0
                    - intersection with y = 1 at x = e
                    - aymptote at x = 0
                    - increases slowly to infinity as x increases
                    - decreases quickly to - infinity as x decreases to 0

                  - given these metadata, possible relationships with other objects include:
                    - the key arc of the natural log function between its key metadata points (at x = 0 and x = e) resembles an arc of the unit circle in radius ratio

                  - regardless, we can also derive the relationship between e ^ y = x and area under x ^ - 1 from 1 to a given the relationship between the objects:
                    - inverse
                    - ratio (side_count) between generalized side length & volume
                    - multiplication = add x, b times (shape of side lengths x & b)
                    - multiplication by itself = add x, x times (square of side length x)
                    - multiplication = addition of dimension
                    - intent of multiplication is to get generalized higher-dimension volume (area for 2d)
                    - hypotenuse
                    - hypotenuse roots
                    - circle
                    - pi
                    - imaginary unit
                    - rotation operation

                  - e ^ y = a generalized cube of side length e multiplied by itself y times = area under 1/x

                  - you can either:
                    - compare the area under e ^ y = a to the area under the curve of 1/x from 1 to a (the standard influence measuring function) to check for relationships
                    - compare another metric of e ^ y = a to standardized version of 1/x from 1 to a to check for relationships

                  - area under 1/x = sum of subcomponents of (1/x (the inverse of x, influence of the x standard on the unit value of 1) * change in x at each subcomponent)
                  
                  - why would the y value produced by ln a (e ^ y = a) be equal to the area under 1/x from 1 to a

                    - unit inverse (one-iteration dimension reduction) of a generalized cube volume from 1 to a (from e ^ 0 to e ^ a), where a = e ^ y
                    - 1/x from 1 to a (from e ^ 0 to e ^ x), where a = e ^ y

                    - for ln a, a = multiply e by itself y times (y = side count of the generalized cube volume with side length e) to produce generalized volume a (value of x)
                    - for 1/x, y = inverse of x (x reduced by itself, or 1 applied to the standard of x - to neutralize anything multiplied by x, like a cube volume with side length x)

                    - when you take the integral of 1/x (sum of subcomponents of (1/x * change in x within subcomponent)), raising the exponent of x (starting from x ^ -1) would be x ^ 0, which is 1
                    - this is a hint that the imaginary unit is relevant, which we'll find out later is relevant to e
                    
                    - why would the number of sides y of a cube of side length e & volume a be related to the sum of the inverse of x (applied from e ^ 0 to e ^ a)?

                      e^y = generalized cube volume of side count y & side length e
                      e^y ~ sum of (1/1 + 1/(1 + dx) + ... + 1/(e ^ a))
                      e^y ~ sum of (1/e^0 + ... + 1/(e^a))
                      e^y ~ sum of (inverse of generalized cube volume of side count 0 & side length e + ... + inverse of generalized cube volume of side count a & side length e )
                      e^y ~ sum of (standard influence from generalized cube volume of side count 0 & side length e + ... + standard influence of generalized cube volume of side count a & side length e)
                      e^y ~ sum of (standard influence from 1 to a (the influence of 1 as a standard (trivial) to the influence of a as a standard (non-trivial)))

                      - youre adding standardizing influence of generalized cube volumes with base (side length) e & number of sides y (from x-value 1 to x-value a) to get e^y (ln a)
                      - the inverse of x, x^-1 removes a dimension of x from the standard x
                      - removing a unit from a dimension & multiplying by previous power is done with derivatives to find change in a variable with respect to another variable

                      - removing a unit from dimension x from "e^y = x":
                        e ^ y * x^-1 = x^1 * x^-1
                        e ^ y * x^-1 = 1
                        e ^ y * 1/x = 1

                        = "apply inverse of x to e^y to get 1"
                        = "apply x standard to e^y to get 1"
                        this is a unit relationship (at what point do x & y produce 1, if any): when y = 0, x = 1
                          e ^ 0 = 1
                        - taking this unit relationship-producing step (removing unit from x dimension) across the x-range from 1 to a produces the function to multiply x by when taking subcomponents to add to get the generalized cube volume

                    - so to get the fact that ln a = the area under the curve 1/x from x=1 to x=a, you could:
                      - identify the key attributes of the natural log (including intersections)
                      - identify that the key operation of the natural log is the exponent (dimension increase)
                      - the opposite of 'dimension increase' (adding another side to the cube) is 'dimension reduction'
                      - given the relevance of the concept of 'dimension reduction', apply a dimension reduction to get the unit operation (1/x), so you can multiply it by dx to get area
                      - use that unit operation to relate the function (of the generalized cube volume produced by side length e & side_count y) to the area under the unit operation function 1/x (area up to x = a is y, from e^y = a)

                    - this is an alternative to knowing that ln a = area under 1/x from x = 1 to x = a, and an alternative to deriving the relationship a classical way
                    - this method uses concepts/objects/attributes/functions of the space, their definitions, & combinations of them to generate sets of probable important items and to derive probable important relationships


                - to do:
                  - above we adjust the function a bit to get:
                    e ^ y * x^-1 = 1

                  - how would we get euler's identity from that, without knowing that e & pi are inherently related?

                  - eulers identity:
                    e ^ pi(i) = -1
                    e ^ pi(-1 ^ 1/2) = -1

                  - in a unit circle of radius 1, the hypotenuse of a right triangle whose corner intersects with the circle = 1 ^ 1/2

                  - the inverse of that would be -1 ^ 1/2

                    e ^ pi(inverse (hypotenuse concept)) = -1
                    e ^ pi(inverse (radius concept)) = -1

                  - with the original equation, e ^ 0 = 1

                  - if pi is the y-value in this situation, there may be a relationship between pi and 0 that is relevant here - maybe a relationship between 0 & pi from the unit circle metadata
                  
                  - now that weve established why e ^ (i * pi) = -1, given that we know that the unit circle also intersects with -1 and has pi as a parameter, is there a relationship between e ^ (i * pi) and any combination of the cos & sin functions that can produce the unit circle

        - reference:
          - eulers equation: e ^ (i * pi) + 1 = 0
          - e ^ ix = cos x + i sin x
          - cos = adjacent/hypotenuse, sin = opposite/hypotenuse

## Map of set concepts to system analysis

  - vector: intersection of attributes value & direction
    - vectors are a useful structure to store intents, ordered lists of objects having a common attribute, & variable value sets
  - unit: lower-dimensional composable threshold for differentiation, syncing, & aggregation of filters (has overlap with interface)
  - map: set of possible transform networks connecting two sets
  - value: position (relative difference)
  - boundary: stabilized outer range of combinations produceable with internal components 
  - norm: a function that standardizes (so that vectors can be scaled & added) - may be an interface filtering function
    - a function that assigns value to vectors in a space/set & allows for additivity & scalability
  - neighborhood as a metric of adjacence/similarity/distance (how many steps away) if neighborhoods are determined by core functions
  - core points: vertices, not the same as nodes in a network, more like moments in a moment-generating function
  - core functions
    - basis: the set of core functions
    - identity function as the origin function to start from when applying distortions to get other functions
  - core combinations
    - base: the set of core subsets of a set X that can generate a topology on set X, or whose union can generate set X
  - generalization: removing differentiating attributes to navigate up an abstraction layer


# physical link between 3-d & interface network

  - if there's a physical link to the interface network, its probably activated by or related to symmetry stacks, given that this object keeps appearing in the generated system
    - fractals are a symmetry stack, as the change ratio applies to each successive layer of change
    - this could mean equalizing symmetries (resource distribution) or building a structure that can stack them (independence machine)
    

# questions

  - do concepts like cause emerge logically from a system where interactions (through forces creating adjacence) & time (state changes are persistent & stable till next iteration of time unit) are possible

  - how does a topology of a number's attribute sets (related complex numbers, adjacent functions/relevant formulas, core generating objects, factors, etc) collapse to a number?

  - are there gaps in an incomplete topology depicting numbers, as dictated by physics rules?

  - do components that can be used to build real numbers map to conceptual network (complex numbers), just like structural-conceptual objects have clear mappings, or are complex numbers more like perspective facets of a number? which types of alternate numbers are missing from the type set given combinable attributes?

  - is math causative (rules to describe information of certain types, structures, & potential), or is physics causative of which information can be generated with stability, enabling measurements & therefore allowing it to be described?

    - why are there numerical symmetries that attract information & values? this implies they're true or continue to be true, if they keep generating information

    - which physics filters (forces, particles, states) beyond the set of constants could generate the math system?

    - if the math system is causative, is it also changeable (can you generate different physics rules by changing it)

    - make a number type/attribute layer graph to fill in gaps in combinations

    - if physics generates information like fuel exhaust, and math can decrypt the exhaust into useful patterns, and those patterns have potential to predict & control physics movements, the math system could have power over the physics system

    - can the math system capture every different type of distance, time, value, etc that the physics system can generate? if one lags behind the other that implies some level of causation

      - what definitions of distance, position, value are missing in the network of definitions

    - do physics concepts like cause, time, & mutually exclusive alternatives (true/false, of an attribute captured in a superposition) leak into the math system as sequence/adjacence, position/change, or valid/invalid for that space

      - these can be classified as physics concepts bc logic has rules/physics describing it, and is a unit interface of other change interfaces

      - the physics/rule sets of a system describe its potential to generate information - the physics of math determine what information is describable in the set of system-generated information
      
      - example with the math system:
        - the physics/rule sets of the math system can generate information
        - the physics of math determine what information is describable in the set of math-generatable information (generated information is input to generation functions)
        - there's a logical loop between generatable information rules & describable information rules

      - explore physics of math determining spaces (sets that can be coverted, spaces that are possible) that evolve around symmetries/interfaces enabling those spaces

      - what other outer layers of spaces can be useful/generated with known operations on known spaces?

      - what system would need to be kept in place by these universal constants (which can be inputs or interim tools used to connect/stabilize the other elements of the math space)

      - apply system analysis to math

        - try swapping value definitions (quantity, position, scale, direction) in different spaces to test for symmetries & patterns that can be used as predictors

        - look for system patterns (attribute alignments, etc) and objects (efficiencies, incentives) such as where something is more calculatable

        - intents of function types:

          - wave functions to model a spectrum of stable alts

          - log functions to model a boolean decision

            - if a log function represents each fork and the aggregate output function amounts to the decision tree, are composite log functions reflective of common patterns in aggregated decision tree output functions
            - or the other way around, so a decision tree can usually be modeled with a logistic function once the variables are formatted the right way, like in sets of attribute sets/paths for 0/1 values

          - decision trees 
            - are a good frame for symmetry stacks
            - need dead-end nodes that represent unanswerable decisions (where information doesnt contain minimum required features, is unclear/corrupted, or points to all possible categories with an ambiguous level of equivalence)

          - support vector machine
            - can be applied to causes on various causal layers, after clustering them to differentiate them 
            - similar to the problem of finding a symmetry, also finding an average

          - clustering
            - in addition to centroids/SVMs, you can infer the inherited group data set, & the core functions to get to the original data set groups, and check for variables used in/output by those core functions

          - combining these methods could be used to allocate methods by complexity & type 

            - start with decision trees, use clustering to differentiate between remaining type group features (attribute sets), use inherited type inference & core function derivation to model any remaining features

            - start on the data layer, navigate up to infer types, trace causal functions connecting them as well as output branches that dont become types

            - start with the core objects, combination types, limits, layers, & filters that are most powerful in explaining interactions & design a custom algorithm based on those or other common interaction objects

          - use new data & change patterns to improve the model prediction method - if two clusters tend to evolve sideways (forming a square) and then converge to other corners, that can be used to improve the predictor early on with a small amount of new data once you know what direction the changes are headed in, vs. waiting for a lot of new data to confirm its not just noise

          - noise patterns can be used to reduce data variance to probable isolatable relationships

          - specify a definition of range of degrees of dependence that is classifiable as 'independent' from another variable

          - use probability in selecting assumptions - how often are variables independent (in two separate sub-systems that are isolated & only interact indirectly up to n degrees of interactions away)

          - is there a better standard assumption set that describes more relationships between variables that create complex relationships justifying analysis?

          - is there a variable type that clarifies relationships, which can be applied before gathering data, or which can be removed to generate complexity?

          - univariate polynomials to model isolated motion (same independent variable applied to itself rather than just a constant like linear functions)

            - when does isolated motion (parabola like x^2) look like group motion (normal distribution), and what does this mean?

              - if the motion of the x variable looks like the group motion, either the x variable has the group's information as an input, or its a false similarity, or the motion of an isolated variable can mimic the group motion under certain circumstances, like when physics applies (physics of an isolated variable like throwing a ball up in a system with gravity, and the physics of group decisions differing from a standard averaage decision, where gravity/forces and lack of restrictions both exert a tempering influence on the motion as with a lack of restrictions, trends tend to gravitate toward the average with minor deviations from it in either direction)


    - algorithms

      - when sorting or finding, use vertices & probability to split the list to traverse - a list of length 20 can have vertices at 3 - 5 different points where the value is likely to be within one traversal with x% likelihood - this is related to minimum information to solve

        - 'minimum information to detect pattern' is another key metric - how many nodes do you need to check in a list of length n before you can be reasonably certain if there's a function generating it or if that function applies within subsets or other structures?


# Problems with Current Analysis Methods

## Problems with Set Theory

  - all useful & isolatable spaces/sets should be derived, as well as their attributes (core, emergent & otherwise)

  - spaces (sets with operators) should be auto-generated and intents should be calculated, so that spaces/sets can be used as functions, linked on a network, and chosen programmatically


## Problems with Calculus

  - should be founded on space-generating operations from a core space so that a trajectory between spaces to solve a particular calculus problem like AUC can be calculated from that space-generating operation set, which has intents mapped, rather than looking for calculation spaces to transform to from the function or from the origin space


## Problems with Linear Algebra

  - has inherent limitations due to structure assigned to intersecting concepts of position, relevance, & difference


## Problems with Statistics


### Evaluating variables in isolation

  - the variable interactions usually studied aren't normally indexed by causal metadata like causal adjacency (direct vs. indirect cause) or causal shape

  - same problem as making risk assessment in isolation & evaluating transactions in isolation
    - the risk presented to an insurer is not calculatable from metadata of the insured but from the system the insured exists in
    - the risk in a transaction is calculatable using financial system metadata

  - causal shape of the relationship is assumed to be independent/input vars => dependent/output vars, where in reality the dependence between the output & inputs is likelier to be nonzero

  - the framework for specifying causal distance is not formalized

  - causal shape analysis is crucial for:
    - identifying variables that can be collapsed into other variables
    - which variables that can be replaced with others
    - if there's a one-to-one direct variable relationship in the causal chain
    - if there's an interface variable in the causal chain (a filter where other variables accrue)
    - if there's a set of causal shapes elimination rules that can be applied without gathering more info
    - if there are likely sets of variables that influence the target variable
    - if a variable is an end leaf rather than a causal branch variable
    - if variation is about to converge or expand
    - if variation is concentrated in a sub-system (trade loop) that is largely independent from the host system and can be ignored in many cases

  - statistics is dependent on data & gives a temporary snapshot of a relationship

  - similar problem to game theory, which tries to isolate a game unit with the context of other games


### Checking variables for predictive power instead of predicting variable metadata & sets first

  - system metadata should have a minimum of information that can enable identifying probable variable sets

  - then predictive functions should be quicker to build, because the solution space is drastically reduced by the system analysis identifying probable variable sets

  - first create an interface where the filters allow multiple variable sets & metadata, then once you narrow down which variable sets/metadata are possible, narrow it down by likelihood given system config

  - example of how variables are examined almost at the level of trial & error, when predicting the likely sets of variable sets would drastically reduce computation requirements
    https://phys.org/news/2020-01-simple-sequence.html
    

### Treating non-random processes as random

  - rather than deriving core functions and observing the trajectory of variance from core functions to combinations of them & other normal system rules like variance accretion patterns, they assume each variable is so unrelated to other variables or so impacted by many other variables that it's random, when system analysis would identify it as a clear combination of system functions, gaps, filters, & other components

  - equality is not a default position so it shouldnt be treated as the standard from which to explore distortions
  - other standards like 'path of least resistance' indicating the importance of core functions should be used as defaults to distort instead of random equality

  - the trajectory of heat through a system is predictable like the trajectory of variance through a system is predictable
    https://phys.org/news/2020-01-supercomputers-link-quantum-entanglement-cold.html

  - whether either is the common currency tradeable between system components is another question:
    - is heat or energy or charge the common factor explaining the trajectory?
    - is variance or adjacency or direction the common factor explaining the trajectory?

  - which mechanics explain why other variables accrete into these interfaces
    - variance/energy are both input forces that move into unrestricted spaces (interface/filter gaps) unless artificially restricted
    - does the original position of the input explain its maintenance as an input (early inputs like energy have influence in formation of systems and remain inputs traded in that system)
    - variance/heat are difficult to maintain & retain in the system without the right system structure to handle them, despite being a common output of system inputs
    - if the system isnt closed/independent and doesnt have handlers for variance that allow it to maintain its closure, variance will leak into adjacent systems
    - one way for a system to be independent in a way that doesnt leak in harmful ways to other systems is to have outputs that are inputs or outputs for other systems