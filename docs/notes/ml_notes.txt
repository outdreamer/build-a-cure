"For the CNN-based classifier they observe that units associated with objects and parts emerge in later layers, while earlier layers are largely associated with colours. For the generator network, on the other hand, object/part neurons can be found more frequently in earlier layers, while the later layers focus on colors."
	- https://towardsdatascience.com/four-deep-learning-papers-to-read-in-august-2021-7d98385a378d

- for a cnn, the feature causation moves from adjacently identifiable features (colors being an attribute found in every pixel/feature) to absolutely identifiable features (larger features like objects being identifiable with features representing larger sections)

- for a gan, the feature causation moves from foundation/type/template structures (objects) to apply details to, and specific details (colors) to tune the output with.

- identifying the relevant interaction layer or base to act as the symmetry (foundation/type/template to apply variations to) and the variables/attributes that determine the variations from those bases is a workaround that integrates these feature causation structures.

	- the algorithm would list the identifiable structures (components/attributes/objects/interaction functions/errors) in the data set
	- then it would try to identify default relevant interface structures like types that form the basis of different clusters/subsets in the data set, applied to the structures (error types, component types, variable types), which are relevant for categorization problems (predicting category, generating category example)
	- then it would try to identify variables that determine variations from those types
	
	- this is similar to 'starting in the middle interaction layer and radiating outwards', as opposed to starting from adjacent details or starting from core objects as foundations for change
		- it integrates error types as an 'adversarial guide'

		- this uses a cross-section of interaction layers rather than the raw objects/attributes clearly identifiable in the data set, so there is some derivation work to do before it can be applied

		- another version of 'starting from the middle' would be a cross-section of interaction layer attributes, like:
			- a component of the image + common attribute values for that component + a partial adjacent component
			- pairs of components commonly found together across the image, regardless of adjacence
			- variable & error structures commonly found with attribute values or value patterns

		- this applies a different cross-section as a different interaction layer than the default interaction layers that could act as symmetries, such as a template/foundation/type, or the raw data set attributes, or the default object/attribute/function structures, or an interface layer, where these cross-sections are mixing interface components